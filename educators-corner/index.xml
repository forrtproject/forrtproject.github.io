<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Educators' Corner | FORRT - Framework for Open and Reproducible Research Training</title><link>https://forrt.org/educators-corner/</link><atom:link href="https://forrt.org/educators-corner/index.xml" rel="self" type="application/rss+xml"/><description>Educators' Corner</description><generator>Source Themes Academic (https://sourcethemes.com/academic/)</generator><language>en-us</language><copyright>© 2021 - FORRT Framework for Open and Reproducible Research Training</copyright><image><url>https://forrt.org/media/FORRT_banner.svg</url><title>Educators' Corner</title><link>https://forrt.org/educators-corner/</link></image><item><title>Innovative Mentoring</title><link>https://forrt.org/educators-corner/007-easy-steps-to-elevate-your-mentoring/</link><pubDate>Mon, 13 Sep 2021 10:44:40 -0400</pubDate><guid>https://forrt.org/educators-corner/007-easy-steps-to-elevate-your-mentoring/</guid><description>&lt;p>In academia there are some common ‘good practice’ mentoring things. Many of us do the ‘typical’ activities that good mentors do: weekly group meetings, individual meetings, open door policy, practice presentations, intellectual engagement, iterative writing feedback on theses, grants, and papers, collaborative interactions, and promoting work-life balance. But we can do more –much more– than the basics, without it taking a ton of time. Here are some innovative ways to up your mentoring game.&lt;/p>
&lt;p>Get more out of group meetings. Like many labs, we meet each week. But we rotate what we do, each in approximately equal measure: standard journal article discussion, discussion of a research article on Equity/Diversity/Inclusion (EDI), “Slide Improv”, and Fact or Fiction.&lt;/p>
&lt;p>EDI articles are chosen on a rotating basis by students. They are usually primary research articles. We conscientiously choose papers from diverse areas of EDI. These discussions have been fantastic, and have led to changes in how we do and discuss things. An added benefit is that this is a visible way to show the group that EDI is important to me as their supervisor.&lt;/p>
&lt;p>Slide improv gives experience thinking on the fly. One of the greatest fears students have in presenting is “what if I screw up” or “what if I forget what to say,” and slide improv gives them actual practice in how to respond to, and be more comfortable with, those moments. To do slide improv, a student makes a five-slide presentation (no animations). Another student gives it. The topic is supposed to be on something that isn’t our main research area, but does need to be something in science. They typically are a presentation of the background and results of a single publication. The student who is presenting doesn’t have to be accurate, but what they say has to be believable. Students are nervous doing this the first few times, but over time I’ve seen a marked improvement in confidence and presenting skills.&lt;/p>
&lt;p>Fact or fiction gives experience in critical thinking. A student gives (brief) details and results for three papers. But one of them is completely fabricated. As a group, we try to figure out which is the fake one. This is fun, and seeing the gears turn as people sort things out is great.&lt;/p>
&lt;p>&lt;img src="fig1.png" alt="">&lt;/p>
&lt;p>To get a handle on how my students are doing, and if there are issues in the lab that need addressing, I do an anonymous poll once per year. The poll covers a range of topics on the quality of life, what is working, and what isn’t. This has been VERY HELPFUL in identifying things that need fixing (and some surprises of things that I thought were issues but are fine). I use a variation of this:&lt;/p>
&lt;iframe src="https://docs.google.com/forms/d/e/1FAIpQLScGCi7iACgmVBhFcE7G90oPwuTs-g9CQkrDmOUoQ4FvoT9CfA/viewform?embedded=true" width="100%" height="700px" frameborder="0" marginheight="0" marginwidth="0">Loading...&lt;/iframe>
&lt;p>&lt;br>&lt;br>&lt;/p>
&lt;p>If you would the direct link to the google forms,
&lt;a href="https://docs.google.com/forms/d/e/1FAIpQLScGCi7iACgmVBhFcE7G90oPwuTs-g9CQkrDmOUoQ4FvoT9CfA/viewform" target="_blank" rel="noopener">please follow this link.&lt;/a>&lt;/p>
&lt;p>Over time, I forget which new students I’ve given ‘key advice’ and expectations. So I made a document that I give to all new students. There is a different document for undergraduate Honours Thesis, MSc, and Ph.D. students, since my advice and expectations for those students differ. The document has basic things such as how many weeks ahead to give me drafts and that all students are expected to attend group meetings, and life things like you will work long hours sometimes, but this should be the exception, not the rule.&lt;/p>
&lt;p>Reading papers is important but not urgent, so gets delayed. Every 6 months I require a list of papers the students have read, annotating why they might cite it in their thesis. The expected number of papers is scaled over time. The annotations are really helpful to keep on top of reading, as a searchable document when writing, and for me to see what they are reading. Plus, with a large lab studying diverse topics, this really helps me see some papers I otherwise may miss, too! Every single student I have had make these annotations has said this was incredibly useful when writing their thesis and that they were glad they had done it.&lt;/p>
&lt;p>Lastly, don&amp;rsquo;t be afraid to try out new ways of mentoring, and toss things that don&amp;rsquo;t work. I&amp;rsquo;ve tried a lot of things that just didn&amp;rsquo;t work for my group, or didn&amp;rsquo;t work for &lt;em>this&lt;/em> group. I touch base frequently to see what works and doesn&amp;rsquo;t, and student&amp;rsquo;s like that I ask and it&amp;rsquo;s adaptable.&lt;/p>
&lt;br>
&lt;hr>
&lt;p>&lt;em>Editor&amp;rsquo;s note&lt;/em>: The present text is an adapted version of a widely shared
&lt;a href="https://twitter.com/FlyBehaviour/status/1435285335743864835" target="_blank" rel="noopener">Twitter thread&lt;/a> that resonated with so many of us. We thought it was of general interest and deserved to be memorialized, and hence we approached Amanda to adapt the thread to post it here.&lt;/p></description></item><item><title>Qualitative Open Science Practices</title><link>https://forrt.org/educators-corner/006-qualitative-os-practices/</link><pubDate>Mon, 05 Jul 2021 10:44:40 -0400</pubDate><guid>https://forrt.org/educators-corner/006-qualitative-os-practices/</guid><description>&lt;h3 id="starting-with-qualitative-open-science-practices">Starting with Qualitative Open Science Practices&lt;/h3>
&lt;p>As a person who was trained primarily in research using quantitative methods but needed to do qualitative research to answer the next most pressing question, I was increasingly learning about qualitative research and how to conduct it properly. However, I had not seen much information about how to conduct qualitative research using open science practices. During my PhD, I had been introduced to open science practices as a way to improve educational research, but almost everything I saw did not seem to align with qualitative research. I asked colleagues what existed with regard to open qualitative research resources. Answer: no clue.&lt;/p>
&lt;h3 id="call-to-action">Call to action&lt;/h3>
&lt;p>We thus gathered people interested in qualitative open science research in education at the
&lt;a href="https://www.cos.io/education-reseach-2021-virtual-unconference#:~:text=February%208%2D9%2C%202021&amp;amp;text=The%20unconference%20included%20engaging%20plenary,detailed%20guidance%20for%20education%20researchers" target="_blank" rel="noopener">Virtual Unconference on Open Scholarship Practices in Education Research&lt;/a> sponsored by the
&lt;a href="https://www.cos.io/" target="_blank" rel="noopener">Center for Open Science&lt;/a>. At the conference, we brought other educational researchers and open science fans together to debate what open science qualitative research might look like, put together a list of publicly available publications and tools, and figure out what to do with this information. By the end of a few of these hackathon sessions, we had a list of videos, websites, and publications to help the community understand how to do this work. The full list is located on the
&lt;a href="https://www.oercommons.org/courseware/lesson/80058" target="_blank" rel="noopener">Open Educational Resources website&lt;/a> but will be expanded upon here in this blog. This blog is intended to be a soft entry into this space of qualitative open science research, not a comprehensive journey; take the thoughts below as suggestions &lt;em>if&lt;/em> they align with your philosophy, project, and Institutional Review Board (IRB).&lt;/p>
&lt;h3 id="output">Output&lt;/h3>
&lt;ol>
&lt;li>
&lt;p>We started the resource list with articles that focus on the &lt;strong>basics of open science qualitative research&lt;/strong>. These articles (and Twitter threads) dive into some of the reasons why open science work is important, such as opening the ‘Black box’ of research or helping replication in later studies. And others also touch upon some of the murky issues of doing this work, like the fact that _quant _and _qual _researchers often have different ways of viewing the world and what truth is or can be. This is a good starting point so that you know that there will likely not be consensus on how to do this work well. However, if you read those and still want to try to incorporate open science into your qualitative projects, you can try this in any or all of the following areas: transparency/rigor, open materials, open access, and ethics.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>Transparency &amp;amp; Rigor&lt;/strong>. For transparency and rigor in qual research, this revolves around having enough information in your research so that other researchers A) know about the researcher(s) and their positionality and B) could do a similar study in the future, also known as replicating the study. For example, in the
&lt;a href="https://academic.oup.com/view-large/27217733" target="_blank" rel="noopener">COREQ guidelines&lt;/a>, there is an entire domain dedicated to transparency regarding the researchers and their backgrounds (see picture below). Researchers must also describe the context, questions, use of theory, sampling method, interview techniques, and analysis in a way that someone could imagine themselves performing those tasks. One open science practice that qualitative researchers could use to some degree before even conducting the study is preregistration. Essentially it’s like the dissertation proposal where you lay out your plan in advance. Preregistration (and registered reports, the preregistration done with a specific journal with a publication agreement) helps to ensure that a researcher does not change their stated methods after the study is completed. Explaining every detail of the study might be difficult if you are under space constraints (as with publication length requirements), which brings us to #3: open materials.&lt;/p>
&lt;/li>
&lt;/ol>
&lt;p>&lt;img src="Fig1.png" alt="Criteria for Reporting Qualitative Studies" title="Criteria for Reporting Qualitative Studies">&lt;/p>
&lt;ol start="3">
&lt;li>&lt;strong>Open Materials.&lt;/strong> Thanks to the internet, researchers have websites and repositories where they can upload the tools for others to access. In qualitative research, this might mean interview protocols, memos, coding notebooks, tools (such as Nvivo or R packages), or even the data itself. This provides a sort of audit trail so others can verify the results of the research. There is no all or nothing here; open materials, much like the rest of these open science practices, exist along a spectrum. Not only what researchers share is on a spectrum; researchers can also dictate who may access the open materials. Perhaps it’s the entire public, but it could just be people who want to verify findings (i.e., dissertation committees, participants, reviewers). Below you can see how
&lt;a href="https://www.tandfonline.com/doi/pdf/10.1080/08824096.2018.1513273" target="_blank" rel="noopener">Bowman and Keene (2018)&lt;/a> described open science practices as a layered onion with the innermost layer being the most transparent. However, no matter what or to whom materials are shared, researchers must include their plan within their consent procedures and IRB protocols to not violate any ethical boundaries.&lt;/li>
&lt;/ol>
&lt;p>&lt;img src="Fig2.png" alt="Conceptual Onion of Open Science Practices" title="Conceptual Onion of Open Science Practices">&lt;/p>
&lt;ol start="4">
&lt;li>
&lt;p>&lt;strong>Ethics.&lt;/strong> Ethics must be considered with any research, but given the often close relationships with participants, potentially sensitive data revealed, and the fact that large amounts of data could reveal participants’ identity, ethics are huge when it comes to qualitative research. Resources both for abstract and practical purposes can be found in each section of the resources document (i.e., ethics specific to data management) and general ethical considerations are only within this section.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>Open Access.&lt;/strong> Finally, researchers can consider how to share their research study. Publishing within traditional academic journals can limit who can read their work, so journals have begun providing open access tiers where researchers can pay the publisher so that the article is available to the public. Alternatively, researchers can “publish” their work online in a preprint server before it is published in a traditional publication outlet. Pre-prints can help researchers get their work out and get community feedback, although certain publications will not accept articles that have been put in a preprint server or require authors to change the pre-print or the settings on it. Open access as an open science practice appears to be generally the same for both quantitative and qualitative work, so fewer resources are here as they can be found on most open science pages.&lt;/p>
&lt;/li>
&lt;/ol>
&lt;h3 id="contact-us">Contact us&lt;/h3>
&lt;p>Our team who created this (Rachel Renbarger, Sondra Stegenga, Thomas Lösch, Sebastian Karcher, &amp;amp; Crystal Steltenpohl) hopes that you’ll find this helpful for doing this work! We are continuing to explore this area further, so you can reach out to me at rachelrenbarger (at) gmail.com if you are interested in contributing to the conversation.&lt;/p></description></item><item><title>Making lemonade out of remote teaching</title><link>https://forrt.org/educators-corner/005-remote-teaching-platform/</link><pubDate>Mon, 24 May 2021 10:44:40 -0400</pubDate><guid>https://forrt.org/educators-corner/005-remote-teaching-platform/</guid><description>&lt;h2 id="making-lemonade-out-of-remote-teaching">Making lemonade out of remote teaching&lt;/h2>
&lt;p>2020 was a year of big changes for the whole world. For the academic community, it was not different. The need to rapidly switch from in-person to online teaching represented a big challenge for most educators. However, with new challenges come also new and big opportunities. Assistant Professor of Economics at Stockholm University,
&lt;a href="https://haushofer.ne.su.se/" target="_blank" rel="noopener">Johannes Haushofer&lt;/a>, saw in remote teaching the opportunity to open classes to students from low and middle-income countries.&lt;/p>
&lt;p>&lt;em>Remote Student Exchange&lt;/em> was then created as a volunteer and non-profit initiative to match professors willing to offer spots in their classes with interested students from low and middle-income countries. To facilitate the matching process, a
&lt;a href="https://remotestudentexchange.org/" target="_blank" rel="noopener">website&lt;/a> was launched. It has been a great success so far. Two weeks after its launch, there were 1701 students and 88 professors registered and more than 30 courses offered in varied disciplines such as Economics, Business, Psychology, Political Science, Neuroscience, and many others. You can browse the available courses
&lt;a href="https://remotestudentexchange.org/courses?subject_area=10" target="_blank" rel="noopener">here&lt;/a>.&lt;/p>
&lt;p>At FORRT, we believe that one of the most overlooked benefits of integrating open and reproducible scholarship into higher education is that of social justice. Academia is still a place of privileges that many cannot afford. We highly commend this initiative and in the hopes of helping it grow even further, we have invited Johannes Haushoffer to explain a bit more how this idea came to be and to share his initial experiences with the community.&lt;/p>
&lt;p>&lt;strong>The Remote Student Exchange in a nutshell&lt;/strong>&lt;/p>
&lt;p>If you are teaching a remote course in the next few months, we highly encourage you to consider taking part in this initiative. Students can sign up free of charge, but they do not receive official grades or credits. This means that this initiative is informal and in most cases, it should not require permission from universities or department chairs.&lt;/p>
&lt;p>To participate, visit the website
&lt;a href="https://remotestudentexchange.org/" target="_blank" rel="noopener">https://remotestudentexchange.org/&lt;/a>. In the home page (see figure below) you can sign up as a professor.&lt;/p>
&lt;p>&lt;img src="Fig1.png" alt="Remote Student Exchange" title="Remote Student Exchange">&lt;/p>
&lt;p>After confirming your email account, you will be able to set up the details of the course you want to offer. As shown in the figure below, you will be asked to indicate the subject area, the level of the course (e.g., bachelor, master, Ph.D. level), and any prerequisites for attending the course (e.g., specific courses and areas of knowledge). You should also provide the link to where the classes take place (e.g., Zoom).&lt;/p>
&lt;p>&lt;img src="Fig2.png" alt="Remote Student Exchange: New Course" title="Remote Student Exchange: New Course">&lt;/p>
&lt;p>Importantly, you are flexible to decide how many students you can host and how they can participate (see the figure below showing the settings for course availability). You can accept students to actively participate with questions, discussions, and assignments, and/or you can accept students to audit silently only. You can also decide to review applications to your course and choose which students to accept or to admit enrolled students on a first-come-first-served basis.&lt;/p>
&lt;p>&lt;img src="Fig3.png" alt="Remote Student Exchange: Availability" title="Remote Student Exchange: Availability">&lt;/p>
&lt;p>Interested students also have to sign up on the website. They will be able to browse the courses being currently offered and apply to take part in a class, provided it still has free spots and that they fulfill the prerequisites (if any).&lt;/p>
&lt;p>If you have more questions, make sure to access the
&lt;a href="https://remotestudentexchange.org/help" target="_blank" rel="noopener">FAQ&lt;/a> and watch the video below where Johannes explains a bit more about this initiative. We hope that many more scholars consider joining it. Share this idea widely with other educators and with students that may be interested. You can find a link to the countries contemplated by this initiative
&lt;a href="https://data.worldbank.org/?locations=XM-XP" target="_blank" rel="noopener">here&lt;/a>.&lt;/p>
&lt;br>
&lt;div style="position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;">
&lt;iframe src="https://www.youtube.com/embed/5kyjnPFSmog" style="position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;" allowfullscreen title="YouTube Video">&lt;/iframe>
&lt;/div>
&lt;br>
&lt;center>
&lt;p>
&lt;a href="https://www.youtube.com/watch?v=5kyjnPFSmog" target="_blank" rel="noopener">Video Interview Direct Link to YouTube&lt;/a>&lt;/p>
&lt;/center></description></item><item><title>Teaching the why and how of replication studies</title><link>https://forrt.org/educators-corner/004-teaching-why-how-replication/</link><pubDate>Tue, 04 May 2021 01:44:40 -0400</pubDate><guid>https://forrt.org/educators-corner/004-teaching-why-how-replication/</guid><description>&lt;p>Psychological science is in the midst of a “credibility crisis” in which its practitioners re-examine their practices and re-define what constitutes study rigor. Replication studies have formed a critical role in motivating this sense of crisis – a sense of crisis that has led directly to the current movement to improve psychological science through a “credibility revolution”.&lt;/p>
&lt;p>Despite this important role, when I was invited to hold a virtual two-day, 10-hour workshop on replication studies for the students and faculty of the Department of Social and Organizational Psychology at ISCTE in Lisbon, I realized that I did not have any ready-made teaching materials on this important topic. This blog shares the guiding principles of the workshop and my finished materials so that you, the reader, can learn from my experiences.&lt;/p>
&lt;p>My workshop materials, including a &lt;strong>syllabus&lt;/strong>, &lt;strong>suggested readings&lt;/strong>, and &lt;strong>exercises&lt;/strong>, are freely available at
&lt;a href="https://osf.io/m9bzh/" target="_blank" rel="noopener">https://osf.io/m9bzh/&lt;/a>&lt;/p>
&lt;p>&lt;strong>Workshop overview&lt;/strong>&lt;/p>
&lt;p>My workshop describes the &lt;em>why&lt;/em> and &lt;em>how&lt;/em> of replication studies: &lt;em>why&lt;/em> a researcher might want to conduct a replication study and &lt;em>how&lt;/em> a researcher should go about conducting a replication study. It also emphasizes some issues that are often neglected in discussions of replication studies, at least in my experience, including the importance of choosing good replication targets, the importance of resource constraints in doing good sample size planning, and the use of simulation studies in sample size planning.&lt;/p>
&lt;p>The workshop proceeded in four modules, as shown in this workshop slide:&lt;/p>
&lt;p>&lt;img src="fig1.png" alt="Workshop Organization" title="Workshop Organization">&lt;/p>
&lt;p>Each module is structured around one or two learning goals, or big-picture takeaway points that I wanted the students to understand at the end of the module. Each module also breaks up lecture sections with independent or guided exercises. The exercises are intended to both reinforce the content of the lecture and give the students hands-on experience with a broad swathe of the skills that go into replication research.&lt;/p>
&lt;p>The remaining sections of the blog will describe the content of each module, how this content reinforces the module’s learning goals, and the exercises I used for each module.&lt;/p>
&lt;p>&lt;strong>Module 1: The credibility crisis&lt;/strong>&lt;/p>
&lt;p>&lt;em>Learning goals:&lt;/em> &lt;em>The credibility crisis revealed weaknesses in research; these weaknesses are caused by hidden processes that don’t show up in published reports&lt;/em>&lt;/p>
&lt;p>This module is framed around a schematic version of how empirical psychological science works (adapted from
&lt;a href="https://www.nature.com/articles/s41562-016-0021" target="_blank" rel="noopener">Munafo et al., 2017&lt;/a>).&lt;/p>
&lt;p>&lt;img src="fig2.png" alt="Sometimes your hypothesis will be wrong, Figuring out why helps our theories improve." title="Sometimes your hypothesis will be wrong, Figuring out why helps our theories improve.">&lt;/p>
&lt;p>In the schematic, you generate a hypothesis from theory, design a study to test the hypothesis, collect data based on the study design, analyze the data to test the hypothesis, interpret the results, publish the data, and begin the cycle anew. By using this process to compare discrepancies between your hypothesis and the data, you can identify flaws in your theoretical assumptions, which allows you to revise the theories and improve them.&lt;/p>
&lt;p>However, a variety of events made psychologists aware that something about this cycle wasn’t working. First was the observation that only about 8% of the results published in psychology journals are negative (
&lt;a href="https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0010068" target="_blank" rel="noopener">Fanelli, 2010&lt;/a>) – yet these negative results are necessary to identify flaws in theoretical assumptions. Second was the publication of a paper by the well-respected social psychologist Daryl Bem, who, in 2011, published a somewhat unusual paper in the world’s top journal for social psychology, the &lt;em>Journal of Personality and Social Psychology&lt;/em> (
&lt;a href="https://psycnet.apa.org/record/2011-01894-001" target="_blank" rel="noopener">Bem, 2011&lt;/a>). This paper used methods that met or exceeded the standards of rigor that were typical for the time, but advanced a claim that was patently absurd: that college students (and, by extension, everyday people) could be influenced by future events.&lt;/p>
&lt;p>&lt;img src="fig3.png" alt="Daryl&amp;rsquo;s Bem &amp;ldquo;Feeling the Future&amp;rdquo;" title="Daryl's Bem Feeling the Future">&lt;/p>
&lt;p>This paper suggested either that everything we knew about physics was wrong or (perhaps more likely) that the research methods in social psychology that we used to think were rigorous were somehow flawed.&lt;/p>
&lt;p>I then describe how these observations spurred a &lt;em>credibility crisis&lt;/em> (not a “replication crisis”, as the crisis is broader than just a lack of replicability) in which researchers investigated whether and how research methods in social psychology are flawed. I use two exercises to illustrate some of the problems.&lt;/p>
&lt;p>In
&lt;a href="https://osf.io/58hdp/" target="_blank" rel="noopener">the first&lt;/a>, the students use a
&lt;a href="https://jeanmoneger.shinyapps.io/SizeMatters/" target="_blank" rel="noopener">free shiny app&lt;/a> to simulate how the combination of the selective publication of positive results and low statistical precision create huge distortions in our understanding of the evidence supporting a particular psychological theory. This exercise demonstrates the concepts of statistical power, precision, and publication bias and demonstrates the general method of using simulation studies to understand what happens when some quantity (in the context of a simulation, a &lt;em>parameter&lt;/em>) varies.&lt;/p>
&lt;p>In
&lt;a href="https://osf.io/y3467/" target="_blank" rel="noopener">the second&lt;/a>, the students use FiveThirtyEight’s
&lt;a href="https://projects.fivethirtyeight.com/p-hacking/" target="_blank" rel="noopener">p-hacking app&lt;/a> to illustrate what makes p-hacking possible. I randomly assigned students to either the Republican party or the Democratic party and to either make their assigned party look good or bad. This exercise illustrates how p-hacking can emerge from the combination of flexible definitions / measurement and a desire to obtain a certain result.&lt;/p>
&lt;p>I close the module by describing how published research reports only constitute a subset of what goes into creating a certain research funding and how processes like p-hacking and publication bias, while hidden, can undermine a finding’s credibility. Thus, one way we can uncover the credibility of a finding is to make visible more aspects of the research process. The next module describes one way to achieve this greater level of visibility.&lt;/p>
&lt;p>&lt;img src="fig4.png" alt="What we see and what we don&amp;rsquo;t see" title="What we see and what we don't see">&lt;/p>
&lt;p>&lt;strong>Module 2: Replications as a way of highlighting what is known&lt;/strong>&lt;/p>
&lt;p>&lt;em>Learning goals:&lt;/em> &lt;em>Close replications test the credibility of a finding. They work best with good documentation &amp;amp; structured ways of choosing replication targets&lt;/em>&lt;/p>
&lt;p>This module starts with the question of how to determine whether a particular finding is “credible”. One way is to fix in place the &lt;em>hypothesis&lt;/em> and try to do the study again. When you do the next study, you can either vary certain aspects of the method or duplicate the past method as carefully as possible. Although I don’t yet introduce this terminology, these two sets of scenarios illustrate the ideas behind a &lt;em>close replication&lt;/em> and a &lt;em>distant replication&lt;/em>.&lt;/p>
&lt;p>I then ask the students to complete
&lt;a href="https://osf.io/n243q/" target="_blank" rel="noopener">an exercise&lt;/a> to think about the conclusions that are reasonable in close and distant replications. The goal of this exercise is to illustrate how close replications (where you keep the method similar) are particularly informative when they give you results that differ from a set of past results because they call into question the credibility of the original results. However, they are less informative when you get results that are similar to the original.&lt;/p>
&lt;p>In contrast, distant replications (where you vary some aspect of the method) are particularly informative when you get results similar to the original results because they allow you to generalize across the methodological feature that you varied. However, they are less informative when you get results that differ from the original because, in addition to all the explanations that apply when you do a close replication, the features that you intentionally varied could also have produced the different results.&lt;/p>
&lt;p>&lt;img src="fig5.png" alt="Close vs Distant Replication" title="Close vs Distant Replication">&lt;/p>
&lt;p>I also use this exercise to highlight the importance of minimizing sampling error and fully documenting procedures; both sampling error and undocumented differences in procedure (“hidden moderators”) provide possible explanations for why replication results differ from original results. This highlights a somewhat hidden side benefit of replications: they force you to very carefully document a particular procedure.&lt;/p>
&lt;p>To illustrate how to document the procedure behind a replication study, I introduce the students to the “replication recipe” (
&lt;a href="https://www.sciencedirect.com/science/article/pii/S0022103113001819" target="_blank" rel="noopener">Brandt et al., 2014&lt;/a>). The replication recipe provides a structured set of questions to guide the process of creating a method section for a replication study. As
&lt;a href="https://osf.io/j3pna/" target="_blank" rel="noopener">an exercise&lt;/a>, I ask students to fill out the first section of the replication recipe with an article that I assign (I pre-selected two articles that are short and have a relatively simple research design). After the exercise, we discuss the process of using the replication recipe and identify issues that came up – including the poor reporting standards of most (but not all) psychology articles.&lt;/p>
&lt;p>In the last part of this module, we discuss a way to choose replication targets. I teach a somewhat informal version of a framework developed by
&lt;a href="https://osf.io/preprints/metaarxiv/2gurz/" target="_blank" rel="noopener">Isager and colleagues (2020)&lt;/a>. As
&lt;a href="https://osf.io/zjwcg/" target="_blank" rel="noopener">an exercise&lt;/a>, the students use the framework to rate the value, uncertainty, and cost of doing the replication study that I assigned them.&lt;/p>
&lt;p>&lt;img src="fig6.png" alt="Choosing an effect to replicate" title="Choosing an effect to replicate">&lt;/p>
&lt;p>&lt;strong>Module 3: Resource planning and piloting&lt;/strong>&lt;/p>
&lt;p>&lt;em>Learning goals: Resources are critical for making your replication precise; you should think through the sample your resources allow &amp;amp; use those to calculate your power&lt;/em>&lt;/p>
&lt;p>This module subsumes most of the content that would normally be taught as “power analysis”. The reason I frame power analysis as “resource planning” is to emphasize the critical, and often unrecognized, role that resources (time, money, skills) play in the number of observations a replication study achieves. I teach two workflows for planning resources: a &lt;strong>resource-first workflow&lt;/strong> based on identifying the practical constraints to one’s resources and determining the power those constraints allow to detect different effect sizes, and &lt;strong>smallest-effect-size-of-interest workflow&lt;/strong> based on identifying a smallest effect size of interest and the number of observations (i.e., resources) required to achieve different levels of power to detect that effect.&lt;/p>
&lt;p>&lt;img src="fig7.png" alt="We Live in a Resource-Constrained World" title="We Live in a Resource-Constrained World">&lt;/p>
&lt;p>The module relies heavily on &lt;em>
&lt;a href="https://debruine.github.io/faux/" target="_blank" rel="noopener">faux&lt;/a>&lt;/em>, an R package for simulating fake data that fits a specific set of constraints. I link the idea of simulation studies back to the first module and tell the students that I am giving them a powerful set of tools to conduct their own simulation studies. I do not assume that students know how to use R, but rather wrote two different scripts that make simulating
&lt;a href="https://osf.io/x7upv/" target="_blank" rel="noopener">two-group&lt;/a> and
&lt;a href="https://osf.io/wdqem/" target="_blank" rel="noopener">four-group&lt;/a> designs easy, even for a complete R novice. My goal is to give the students some tools to do basic tasks in R, as well as resources to learn more about simulations in R if they have the time and interest.&lt;/p>
&lt;p>A hidden goal of this module is to demonstrate how under-resourced most past research really is. For example, when I illustrate the resource-first workflow, I assume that we have enough resources to achieve 80 observations per cell in a two-group design – a number of resources that meets or exceeds the sample sizes used during the 2000s in social psychology (
&lt;a href="https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0109019" target="_blank" rel="noopener">Fraley &amp;amp; Vazire, 2014&lt;/a>). This design yields abysmal power to detect most reasonably-sized effects.&lt;/p>
&lt;p>&lt;img src="fig8.png" alt="SESOI smallest-effect-size-of-interest workflow" title="SESOI smallest-effect-size-of-interest workflow">&lt;/p>
&lt;p>As another example, when I illustrate the smallest-effect-size-of-interest workflow, I assume that the target effect is part of an interaction. Interactions require about
&lt;a href="https://statmodeling.stat.columbia.edu/2018/03/15/need-16-times-sample-size-estimate-interaction-estimate-main-effect/" target="_blank" rel="noopener">16 times&lt;/a> the sample size to detect than main effects, a fact that is illustrated vividly in the simulation-based power curve from this part of the module.&lt;/p>
&lt;p>&lt;img src="fig9.png" alt="Notes" title="Notes">&lt;/p>
&lt;p>One last issue that comes out of the simulations is the number of assumptions that one must make in the process of doing a simulation study. This includes both statistical assumptions, such as the size of the standard deviation of the outcome measure, and non-statistical assumptions, such as the length of time it takes for a typical participate in the study (a fact that is necessary to accurately estimate the number of participants who can participate in a lab-based study, for example). I argue that pilot studies are useful for developing good values for these assumptions. Pilot studies are &lt;em>not&lt;/em> useful for directly estimating the value of the target effect size itself (
&lt;a href="https://www.sciencedirect.com/science/article/pii/S002210311630230X?casa_token=OETt_Sm5VFEAAAAA:-9rK8QScds9e0A1siznusvdtvl0-yC2WpBVWe7ztdGkZ8eVILbyqWMC5WmcsAxHWp6X7X7voPeA" target="_blank" rel="noopener">Albers &amp;amp; Lakens, 2018&lt;/a>); in any case it is better to power to a smallest effect size of interest than the expected effect size.&lt;/p>
&lt;p>&lt;img src="fig10.png" alt="Workflow 2" title="Workflow 2">&lt;/p>
&lt;p>&lt;strong>Module 4: Preregistration&lt;/strong>&lt;/p>
&lt;p>&lt;em>Learning goals: To preregister something, create an OSF project &amp;amp; put the replication recipe in the registry. There’s evidence this helps make research more credible&lt;/em>&lt;/p>
&lt;p>The bulk of this module is focused around completing a pre-registration for the article assigned to the students in the previous modules. Because the workshop participants have already completed the first part of the replication recipe (
&lt;a href="https://www.sciencedirect.com/science/article/pii/S0022103113001819" target="_blank" rel="noopener">Brandt et al., 2014&lt;/a>) for this article, they are already familiar with the article’s purpose and materials. For the first part of this module, the students complete the remainder of the replication recipe (or at least, as much as they can) as part of
&lt;a href="https://osf.io/w35fp/" target="_blank" rel="noopener">the last exercise&lt;/a> of the workshop.&lt;/p>
&lt;p>The replication recipe in hand, the students can complete a replication recipe-based preregistration on the Open Science Framework (
&lt;a href="https://osf.io" target="_blank" rel="noopener">OSF&lt;/a>). I walk the students through this process and introduce them to the basics of uploading materials on an OSF page. But the students already completed hard parts of preregistration as part of the previous exercises.&lt;/p>
&lt;p>&lt;img src="fig11.png" alt="Preregistration on the Open Science Framework" title="Preregistration on the Open Science Framework">&lt;/p>
&lt;p>I spend the remainder of this module reviewing evidence of what a well-planned and well-executed preregistration can do for the credibility of research.&lt;/p>
&lt;p>&lt;img src="fig12.png" alt="Preregistration can help on a wide variety of research problems." title="Preregistration can help on a wide variety of research problems.">&lt;/p>
&lt;p>I also briefly cover Registered Reports, which circumvent publication bias through a staged review process. At stage 1, a proposal is reviewed prior to any data collection. At stage 2, the proposal is reviewed again, and as long as the researcher executes their accepted study protocol, published regardless of results. I give the students a
&lt;a href="https://www.cos.io/initiatives/registered-reports" target="_blank" rel="noopener">list of journals&lt;/a> that accepts registered reports if they’re interested in conducting a study with this publication format.&lt;/p>
&lt;p>&lt;img src="fig13.png" alt="Registered Reports" title="Registered Reports">&lt;/p>
&lt;p>&lt;strong>Conclusion: Use my materials!&lt;/strong>&lt;/p>
&lt;p>Replications have played a critical, though I think sometimes misunderstood, role in spurring the credibility revolution: they are one way of investigating the credibility of a particular research finding. The primary way they do this, I think, is by serving as a tool to highlight previously unknown pieces of information, either during the process of documenting the procedure (so it can be successfully executed) or via the results themselves. This role deserves to be underscored in teaching materials. I also think replications can be a useful venue to highlight other parts of the research process, such as decisions about what to research and resource planning. The structure of my finished workshop reflects this general outlook.&lt;/p>
&lt;p>If you find my materials useful, please use them! The materials are freely available in
&lt;a href="https://osf.io/m9bzh/" target="_blank" rel="noopener">this repository&lt;/a>. Let’s use our teaching to pass the lessons of the credibility revolution on to the next generation of behavioral scientists.&lt;/p></description></item><item><title>Developing a comprehensive directory of tools and technologies for social science research methods</title><link>https://forrt.org/educators-corner/003-developing-tools/</link><pubDate>Mon, 04 Jan 2021 10:44:40 -0400</pubDate><guid>https://forrt.org/educators-corner/003-developing-tools/</guid><description>&lt;h2 id="developing-a-comprehensive-directory-of-tools-and-technologies-for-social-science-research-methods">Developing a comprehensive directory of tools and technologies for social science research methods&lt;/h2>
&lt;p>Often the search and exploration of tools and technologies in social science research is not part of the class curriculum in the same way as the systematic review of literature is. This, sadly, leaves the becoming researcher in a place of disadvantage, in my opinion. In their early research career, students will mostly rely on their supervisor or peers to advise on the tools they use, which is still a very limited sample. However, with strides in technological development, researchers could choose from a growing number of multivariate tools for social science methods rising from within the discipline itself, as well as borrowed from other disciplines or coming from the commercial sector.&lt;/p>
&lt;p>Starting from this premise, we decided to build a
&lt;a href="https://ocean.sagepub.com/research-tools-directory#categories" target="_blank" rel="noopener">tools directory&lt;/a> for social scientists, a simple solution for a place where any researcher or student can come and find the right tool for what they need. In this piece, I explain how the tools directory was developed and how it can be used by educators, researchers and students.&lt;/p>
&lt;h3 id="developing-the-tools-directory">Developing the tools directory&lt;/h3>
&lt;p>The initial list was based on software tools and tech platforms that we knew were popular among social science researchers because we’ve commissioned books about them, or they have been prominent within the community. We continued to ask academics, look through papers and other lists like the
&lt;a href="http://dirtdirectory.org/" target="_blank" rel="noopener">DiRT Directory&lt;/a> from the Digital Humanities, the
&lt;a href="https://wiki.digitalmethods.net/Dmi/ToolDatabase" target="_blank" rel="noopener">Digital Methods Initiative&lt;/a> and
&lt;a href="https://sourceforge.net/" target="_blank" rel="noopener">SourceForge&lt;/a>. Soon enough, the directory was growing out of control. What we thought would be a simple scroll down page, organised in a few basic categories, was not serving its purpose any longer.&lt;/p>
&lt;p>With around three hundred different software packages and tools that we knew were used by some or many social science researchers in their work, a new challenge was becoming apparent. It was a paradox-of-choice situation. On one hand, it was increasingly clear why academics often rely solely on recommendations from their peers when choosing a tool. And on the other hand, we knew we needed to explore how one would choose the right tool from a list, and ultimately how to teach others to find the tool that fits their own purposes rather than simply recommending a tool they’ve used.&lt;/p>
&lt;p>As the list grew, we enlisted the help of a few master students, and started collecting more data: who built these tools, were they free or paid, what cluster of similar tools would they belong to, when were they built, based on the information available could we tell whether they were up to date, scaling, or failed, could we find papers that cited these tools, were the creators recommending a citation etc.&lt;/p>
&lt;p>When we hit 400 software packages/tools, we knew we had to promote this list and share it in a way that researchers would actually stumble upon it and have the opportunity to reference it in a lecture or paper. So we wrote a
&lt;a href="https://uk.sagepub.com/en-gb/eur/technologies-for-social-science-research" target="_blank" rel="noopener">whitepaper summarizing the big trends on the development of tools and tech for social science research&lt;/a>. We learned that both commercial and non-commercial tools are popular within the social sciences, but the ones that last longer and are more successful focus beyond the discipline and almost always have a person or teams of people dedicated to raising funds or expanding the community of users and contributors.&lt;/p>
&lt;p>At 400 software packages/tools, we were still not sure the list was big enough. We then focused on specific methods and researched all the tools available to carry out that method or task within the research process. We looked at the evolution of technologies for that method in particular, as well as how it fits within the development of the method itself. We call these ‘deep dives’. We’ve done deep dives on tools for annotation, or tools for transcription, surveying tools, tools for studying social media data, and we kept finding more software applications within each of these areas. We concluded these deep dives to be quite useful, as they enabled sharing slightly more comprehensive sub-lists of tools that could be used in different modules. We have now 543 tools on the list, and the number keeps growing.&lt;/p>
&lt;h3 id="how-to-use-the-tools-directory">How to use the tools directory&lt;/h3>
&lt;p>The full directory is currently available on our
&lt;a href="https://sagepublishing.github.io/sage_tools_social_science/" target="_blank" rel="noopener">GitHub repository&lt;/a> as a csv file. We decided to host it on GitHub, in order to be able to update the directory when we come across new tools or after deep dives; ensure it’s always available for others to reuse in its most up-to-date form, and enable instructors, students and researchers to add tools that might be missing.&lt;/p>
&lt;p>Educators teaching research methods or preparatory courses for students’ theses could present the full tools directory to students, so they are more flexible in finding the right tools for their needs and future projects.Students can browse through the list and filter for tools to find a tool that is most appropriate for a research project they are initiating. For example, a student transcribing interviews might look at the transcription tools to find alternatives. Similarly, educators that are teaching a more specialized course, such as introduction to text mining, data visualization, or social data mining, or running online experiments could filter out a sub-list of tools focusing on the explicit method. They could then share this sub-list as part of the course reference materials or assignments.&lt;/p>
&lt;p>&lt;img src="featured.png" alt="Fig. 1. The spread of 543 tools and technologies across methods and techniques." title="Fig. 1. The spread of 543 tools and technologies across methods and techniques.">&lt;figcaption>Fig. 1. The spread of 543 tools and technologies across methods and techniques.&lt;/figcaption>&lt;/p>
&lt;p>&lt;img src="fig2.png" alt="Fig. 2. Filtering to find transcription tools. A student or instructor could filter by column F (the Competitive cluster which contains the method/technique/task/area that we used to categorize the tool) to get a sub-list of tools that could be broadly used for a particular process. If the cluster is too broad, the student can look through the technique (column E), that breaks it down further. For example for social media tools, the technique would include analysis, collection, visualisation etc. If looking for more recent tools, one can filter by the year the tool was launched (column M); or if the student is interested in something that is free, they can check the charges (column N)." title="Fig. 2. The spread of 543 tools and technologies across methods and techniques.">&lt;figcaption>Fig. 2. Filtering to find transcription tools. A student or instructor could filter by column F (the Competitive cluster which contains the method/technique/task/area that we used to categorize the tool) to get a sub-list of tools that could be broadly used for a particular process. If the cluster is too broad, the student can look through the technique (column E), that breaks it down further. For example for social media tools, the technique would include analysis, collection, visualisation etc. If looking for more recent tools, one can filter by the year the tool was launched (column M); or if the student is interested in something that is free, they can check the charges (column N).&lt;/figcaption>&lt;/p>
&lt;p>While the csv file that contains the tools directory might be easy to update and share, we acknowledge that it might not be that easy to use within a classroom. We are experimenting with a variety of ways that would enable a better display and navigation of the directory, without losing from the ease of updating it.&lt;/p>
&lt;p>In 2019 we did our first deep dive into the tools for social data science to support our
&lt;a href="https://campus.sagepub.com/collecting-social-media-data" target="_blank" rel="noopener">SAGE Campus course on collecting social media data&lt;/a>. We created a sublist to share for this course to help learners find the software that might be most appropriate for their own project, especially given the variety of social media platforms available. To render
&lt;a href="https://airtable.com/shrux4hYwNG1cOyjK/tbl9NiGq87agePZ2M?backgroundColor=cyan&amp;amp;viewControls=on" target="_blank" rel="noopener">the sub-list&lt;/a> in a more friendly way, we used the free version of airtable, which is a no-code app for relational databases with a colorful and modern interface. Students would navigate to this page (Fig 3) to see the sub-list on a single table. They can then find the right tool for their social media project by selecting the platform they want to collect their data from (twitter, instagram, facebook etc), whether they are happy to pay or looking for something that’s free, and the type of task they want to perform: whether they need the tool for collecting the data, analysis, or visualization. Once they have a filtered list, they can also look through the academic papers we’ve linked where each tool has been used, to explore further the potential of the tools.&lt;/p>
&lt;p>&lt;img src="fig3.png" alt="Fig. 3. Screenshot of the sub-list containing social media tools via the free version of airtable. Similar to working with a csv file (as in Fig. 2), this interface lets the student filter the list down to narrow the choices for a tool they could use to either collect or analyse their data. This interface is web-based, and has a more inviting user experience than working with a csv file. A student can easily see the categories of tools, filter by multiple terms or concepts linked within each of the columns." title="Fig. 3. Screenshot of the sub-list containing social media tools via the free version of airtable. Similar to working with a csv file (as in Fig. 2), this interface lets the student filter the list down to narrow the choices for a tool they could use to either collect or analyse their data. This interface is web-based, and has a more inviting user experience than working with a csv file. A student can easily see the categories of tools, filter by multiple terms or concepts linked within each of the columns.">&lt;figcaption>Fig. 3. Screenshot of the sub-list containing social media tools via the free version of airtable. Similar to working with a csv file (as in Fig. 2), this interface lets the student filter the list down to narrow the choices for a tool they could use to either collect or analyse their data. This interface is web-based, and has a more inviting user experience than working with a csv file. A student can easily see the categories of tools, filter by multiple terms or concepts linked within each of the columns.&lt;/figcaption>&lt;/p>
&lt;p>We envision this sub-list of social media tools to be a starting point, as it helps the learner filter down based on a limited number of criteria, such as: the task that can be achieved (collection, analysis), the social media platform that’s integrated, and the fees.&lt;/p>
&lt;p>We’ve reused the same
&lt;a href="https://socialmediatools.pory.app/" target="_blank" rel="noopener">sub-list of social media tools with a different interface&lt;/a> (pory.io, currently in beta) to render this list of tools more akin to a catalogue of records, that the student can search and filter. This rendering was used in a bootcamp on starting off with social media research. Similar to the airtable rendering, a student could filter based on the task they want to achieve and then click into the tool to get more information and explore which one would work better.&lt;/p>
&lt;p>&lt;img src="fig4.png" alt="Fig. 4: Screenshot of the sub-list of social media tools rendered into a catalogue via the pory.io app. The user experience on this interface is friendlier than working with a table as in Fig. 2 &amp; 3. A student can filter the list by the type of tool, which is immediately visible; for example they might be looking for tools to support their data collection. They can then use the search box to enter key terms and narrow down the list further, a process that is more familiar. The student can also browse the list of tools by opening the individual cards to find more information (see next figure)." title="Fig. 4: Screenshot of the [sub-list of social media tools](https://socialmediatools.pory.app/) rendered into a catalogue via the pory.io app. The user experience on this interface is friendlier than working with a table as in Fig. 2 &amp;amp; 3. A student can filter the list by the type of tool, which is immediately visible; for example they might be looking for tools to support their data collection. They can then use the search box to enter key terms and narrow down the list further, a process that is more familiar. The student can also browse the list of tools by opening the individual cards to find more information (see next figure).">&lt;figcaption>Fig. 4: Screenshot of the
&lt;a href="https://socialmediatools.pory.app/" target="_blank" rel="noopener">sub-list of social media tools&lt;/a> rendered into a catalogue via the pory.io app. The user experience on this interface is friendlier than working with a table as in Fig. 2 &amp;amp; 3. A student can filter the list by the type of tool, which is immediately visible; for example they might be looking for tools to support their data collection. They can then use the search box to enter key terms and narrow down the list further, a process that is more familiar. The student can also browse the list of tools by opening the individual cards to find more information (see next figure).&lt;/figcaption>&lt;/p>
&lt;p>&lt;img src="fig5.png" alt="Fig. 5. Fig. 5: Once the student filters a list of tools, they can click one each card to get further information about each tool. Currently this includes a brief description, the platform supported, whether it’s free or not, and several academic papers that have used this tool." title="Fig. 5: Once the student filters a list of tools, they can click one each card to get further information about each tool. Currently this includes a brief description, the platform supported, whether it’s free or not, and several academic papers that have used this tool.">&lt;figcaption>Fig. 5: Once the student filters a list of tools, they can click one each card to get further information about each tool. Currently this includes a brief description, the platform supported, whether it’s free or not, and several academic papers that have used this tool.&lt;/figcaption>&lt;/p>
&lt;p>Airtable and pory.io have different affordances for rendering the sub-lists of tools, and our experience so far is that both have been useful. We are hoping to learn more from these experiments, to understand the student’s journey as well as the data that would inform their exploration process.&lt;/p>
&lt;p>The social media tools sub-list was part of a deep dive that we carried out in 2019. Since then, we dived into
&lt;a href="https://sagepublishing.github.io/sage_tools_social_science/2019/11/11/surveying-tools.html" target="_blank" rel="noopener">surveying tools&lt;/a> and
&lt;a href="https://sagepublishing.github.io/sage_tools_social_science/2020/01/20/text-mining.html" target="_blank" rel="noopener">text mining&lt;/a>. We have not created separate sub-lists for these, and encourage instructors to try other ways of representing these tools within their courses. If you are teaching text mining in the social sciences, for example, you can point your students to this
&lt;a href="https://sagepublishing.github.io/sage_tools_social_science/2020/01/20/text-mining.html" target="_blank" rel="noopener">overview of the text mining tools available&lt;/a> (Fig. 6 &amp;amp; Fig. 7) and share a sub-list of the tools directory filtered for text mining with your students.&lt;/p>
&lt;p>&lt;img src="fig6.png" alt="Fig. 6: Screenshot of the Text Mining section, an overview of tools available." title="Fig. 6: Screenshot of the Text Mining section, an overview of tools available.">&lt;figcaption>Fig. 6: Screenshot of the Text Mining section, an overview of tools available.&lt;/figcaption>&lt;/p>
&lt;p>&lt;img src="fig7.png" alt="Fig. 7: Text mining tools and technologies based on the process they support." title="Fig. 7: Text mining tools and technologies based on the process they support.">&lt;figcaption>Fig. 7: Text mining tools and technologies based on the process they support.&lt;/figcaption>&lt;/p>
&lt;h3 id="going-forward">Going forward&lt;/h3>
&lt;p>Going forward, we are quite interested in finding out what are the criteria people often use to filter down to their top tools, so we can build this list forward and continuously add the data that helps academics and students find the tools that fit their project best.&lt;/p>
&lt;p>We understand that lists follow some form of a hype cycle, where there is a lot of work done at the start and some engagement from the community, and then the whole project slowly dies and it is forgotten. It becomes pretty unusable, because with the pace of research and technology, a lot of the tools are out of date and many new ones have popped up. A person must be dedicated to updating the list and for now we have that covered. Since the publication of the whitepaper in November 2019, we’ve added at least 100 more tools, mostly focusing on text and data mining. While it’s relatively easy to come across new tools, the hardest bit is updating the ones that are already on the list, and that’s where we are open for suggestions from the community. The list with updates to the whitepaper are available in this
&lt;a href="https://sagepublishing.github.io/sage_tools_social_science/" target="_blank" rel="noopener">GitHub repository&lt;/a>.&lt;/p>
&lt;p>Finally, the &lt;em>locus&lt;/em> of software tools and technologies within the research ecosystem remains a big challenge. Software tools are yet to gain the credit of research output. And that is why, among other reasons, software tools are rarely cited or referenced in papers. This is not only bad for
&lt;a href="https://www.slideshare.net/danielskatz/citation-and-reproducibility-in-software" target="_blank" rel="noopener">reproducibility of research, but it also makes it difficult to help other researchers weigh in and compare different tools&lt;/a> used for similar studies. We aim to promote and include the suggested citation of the tools in our list, and strongly encourage anyone to use
&lt;a href="https://citeas.org" target="_blank" rel="noopener">https://citeas.org&lt;/a> when unsure how to give credit to these.&lt;/p>
&lt;p>We remain active and are continuously thinking of better ways to present and re-architecture the information about software tools and technologies we’ve gathered, to make it easier to navigate and explore. We hope these materials will help you and your students become more aware of the diversity of tools and technologies and will open new and potentially easier avenues to decide on the best software tool to use for your research.&lt;/p></description></item><item><title>Using the 'Transparency Checklist Guidelines' as an Educational tool</title><link>https://forrt.org/educators-corner/002-transparencychecklist-balazsaczel/</link><pubDate>Sun, 04 Oct 2020 10:44:40 -0400</pubDate><guid>https://forrt.org/educators-corner/002-transparencychecklist-balazsaczel/</guid><description>&lt;h1 id="transparency-checklist-guideline-for-education">Transparency Checklist Guideline for Education&lt;/h1>
&lt;h2 id="what-is-it-for">What is it for?&lt;/h2>
&lt;p>As an educational tool, the Checklist can be used to teach and improve the standards of transparency and credibility in research reports made by students. The aim is that students are embedded in transparent and open practices from the beginning of their training.&lt;/p>
&lt;p>
&lt;a href="https://docs.google.com/spreadsheets/d/1NxJG5ccRAhvLKngosRVjT8IO2OZ0UDZMN86FW5qGf0Q/edit?usp=sharing" target="_blank" rel="noopener">See here for an adapted version of the original checklist for educational purposes.&lt;/a>&lt;/p>
&lt;p>&lt;img src="Checklist_v2.png" alt="">&lt;/p>
&lt;h2 id="how-to-use-it">How to use it?&lt;/h2>
&lt;p>Supervisors and instructors can require students to create a Transparency Report along with any empirical research assignment.&lt;/p>
&lt;h3 id="steps-to-follow">Steps to follow&lt;/h3>
&lt;ul>
&lt;li>Inform the students about the Transparency Checklist.&lt;/li>
&lt;li>Add the completion of the checklist to the requirements.&lt;/li>
&lt;li>
&lt;a href="http://www.shinyapps.org/apps/TransparencyChecklist/" target="_blank" rel="noopener">Make the checklist app available to students&lt;/a>&lt;/li>
&lt;li>Be ready to answer questions if the students are uncertain about a checklist item.&lt;/li>
&lt;li>Read the Transparency Report along with the manuscript.&lt;/li>
&lt;/ul>
&lt;h3 id="optional-steps">Optional steps&lt;/h3>
&lt;ul>
&lt;li>Explain each item why it is important for transparency.&lt;/li>
&lt;li>In the classroom, ask the students to evaluate some (weakly and strongly transparent) published research papers. This can help them get familiar with the checklist and realize the importance of transparent reporting.&lt;/li>
&lt;/ul>
&lt;h2 id="how-to-evaluate-the-transparency-report">How to evaluate the Transparency Report&lt;/h2>
&lt;ul>
&lt;li>In the requirements, indicate which items should have a “yes” response. Should you want to make it easier for the students, you can require only the Short (12-item) Checklist:
&lt;a href="http://www.shinyapps.org/apps/ShortTransparencyChecklist/" target="_blank" rel="noopener">http://www.shinyapps.org/apps/ShortTransparencyChecklist/&lt;/a>&lt;/li>
&lt;li>When possible, check whether the manuscript is in line with the checklist answers.&lt;/li>
&lt;/ul>
&lt;h2 id="reference">Reference&lt;/h2>
&lt;p>The following paper introduces the Transparency Checklist and describes how an initial set of items was iteratively evaluated by 45 journal editors, as well as 18 open-science advocates until a consensus was reached about the content and form of the checklist.&lt;/p>
&lt;p>Aczel, B., Szaszi, B., Sarafoglou, A. et al. A consensus-based transparency checklist. Nat Hum Behav 4, 4–6 (2020).
&lt;a href="https://www.nature.com/articles/s41562-019-0772-6" target="_blank" rel="noopener">https://www.nature.com/articles/s41562-019-0772-6&lt;/a>&lt;/p></description></item><item><title>Addressing the issue of representation in an undergraduate psych class</title><link>https://forrt.org/educators-corner/001-representation-heatherurry/</link><pubDate>Sat, 03 Oct 2020 10:44:40 -0400</pubDate><guid>https://forrt.org/educators-corner/001-representation-heatherurry/</guid><description>&lt;p>I want to tell y’all about one of my most meaningful teaching experiences. Pull up a chair.&lt;/p>
&lt;p>I’ve been teaching experimental psychology since 2006. It’s the flagship research methods course for undergraduate psychology majors at my institution. We now enroll 120 students each semester and, with the amazing help of 5 TAs, we offer lectures twice per week and a 2.5-hour lab once per week.&lt;/p>
&lt;p>As many of you know, the shit hit the fan when it comes to psychology research during my time teaching this class. I felt pretty dizzy for a while during and after 2011, the year that marks scales falling from my eyes.&lt;/p>
&lt;p>I didn’t really make many substantial changes to my course right away. I needed to feel like I was on firmer ground first. (I expressed that sentiment
&lt;a href="https://twitter.com/HeatherUrry/status/968638314608721921?s=20" target="_blank" rel="noopener">in this tweet thread below.&lt;/a>)&lt;/p>
&lt;p>But then, I came to realize that if I didn’t make changes to my teaching in addition to changes to my research, I was complicit. So, I got to work. Standing on the shoulders of the smart people who brought issues to light and modeled pathways to improvement, I shifted my content.&lt;/p>
&lt;p>Building on traditional topics like internal, external, &amp;amp; construct validity, reliability, measurement, hypothesis testing, etc., I started teaching about transparency, openness, questionable research practices, and effect sizes.&lt;/p>
&lt;p>We started to do IRB-approved, preregistered replication research some of which has seen or will see the light of day in published form (
&lt;a href="https://twitter.com/HeatherUrry/status/1289013799828090880?s=20" target="_blank" rel="noopener">like the example in this tweet).&lt;/a>&lt;/p>
&lt;p>We also started discussing efforts like the ManyLabs studies, the Psychological Science Accelerator (
&lt;a href="http://twitter.com/@PsySciAcc" target="_blank" rel="noopener">@PsySciAcc&lt;/a>), &amp;amp; the Collaborative Replications and Education Project (
&lt;a href="https://twitter.com/CREP_psych" target="_blank" rel="noopener">@CREP_psych&lt;/a>).&lt;/p>
&lt;p>This has all reinvigorated my love of teaching in general and this class specifically. I honestly feel like I’m fulfilling a real need. We need people in our society to have the tools to consume research responsibly, whether they go on to produce research themselves or not.&lt;/p>
&lt;p>But then 2020 happened. After centuries of oppression of Black, Indigenous, People of Color (BIPOC) in the United States, and seeing the disproportionate impact that the coronavirus is having on BIPOC folks, it finally clicked.&lt;/p>
&lt;p>My students need more from me than validity and reliability and transparency and openness. They need to feel they belong. All of them. Every one of them needs to see themselves in psychological science if that’s how they want to spend their time.&lt;/p>
&lt;p>So, this semester I introduced a single class session focused on the importance of representation in science. I borrowed ideas from Jessica Remedios (
&lt;a href="https://twitter.com/jdremedios/status/1303700486277812226?s=20" target="_blank" rel="noopener">see her terrific Twitter thread, which ends with her slides&lt;/a>).&lt;/p>
&lt;p>I had them read this work about
&lt;a href="https://www.pnas.org/content/117/17/9284" target="_blank" rel="noopener">the diversity-innovation paradox by Hofstra and colleagues&lt;/a>.&lt;/p>
&lt;p>I also invited them to watch the
&lt;a href="https://www.pictureascientist.com/" target="_blank" rel="noopener">Picture a Scientist film&lt;/a> and/or listen to the Everything Hertz podcast (
&lt;a href="https://twitter.com/hertzpodcast" target="_blank" rel="noopener">@hertzpodcast&lt;/a>) about Diversity in science
&lt;a href="https://everythinghertz.com/114" target="_blank" rel="noopener">with Jess Wade&lt;/a>.&lt;/p>
&lt;p>I talked about psychology’s positivist tradition of believing there’s some sort of objective reality that we can discover if we cleave to principles like empiricism, transparency, &amp;amp; falsifiability and how that position is marred by biases (e.g., confirmation bias, availability).&lt;/p>
&lt;p>It’s also marred by the fact that scientists are humans with standpoints that affect the questions they ask. And that power structures dominated by men and white people guide what we think is “normal” science. Such ideas aren’t commonly discussed in quantitative research circles.&lt;/p>
&lt;p>I showed them clips from
&lt;a href="https://www.pictureascientist.com/" target="_blank" rel="noopener">Picture a Scientist&lt;/a> so they could appreciate what it’s like to be a woman in science, and the progress that’s been made to remediate gender bias (e.g.,
&lt;a href="http://web.mit.edu/fnl/women/women.html" target="_blank" rel="noopener">the MIT report from 1999&lt;/a>).&lt;/p>
&lt;p>&lt;img src="iceberg.png" alt="">&lt;/p>
&lt;p>We also talked about the idea that some seem to have been left out, women of color, in particular, but also disabled, LGBTQ, first generation, indigenous people, and all their intersections.&lt;/p>
&lt;p>We talked about the loss to science, especially when innovative ideas put forward by women, people of color, and women of color are devalued (Hofstra et al., 2020).&lt;/p>
&lt;p>I showed them powerful clips of one woman of color’s experience as a scientist, professor of chemistry, Raychelle Burks (
&lt;a href="https://twitter.com/DrRubidium" target="_blank" rel="noopener">@DrRubidium&lt;/a>). Dr. Burks captured so perfectly the importance of representation.&lt;/p>
&lt;p>&lt;img src="her1.png" alt=""> &lt;img src="her2.png" alt="">&lt;/p>
&lt;p>I talked about how these problems are not limited to male-dominated sciences but that these issues pervade psychological science too.&lt;/p>
&lt;p>I showed them data from APA demonstrating the move from roughly 78% undergrad psychology majors being female to less than 50% of full professors being female. Lots of factors at play, including gender bias.&lt;/p>
&lt;p>I shared with them an email that a female colleague of color received recently that illustrated the gendered racism she faces in our field. It referenced the hardships faced by white men. (Sorry, what now?)&lt;/p>
&lt;p>I talked about my own standpoint as an educated white woman, a full professor with lots of mentorship from family members and powerful colleagues through the years. I had a lot of help getting where I am in part because of intersecting identities tied to systems of power.&lt;/p>
&lt;p>I expressed my hope that they’ll think about their own intersecting identities and if science is something they love and psychology specifically they should go for it and make room for everybody’s voices.&lt;/p>
&lt;p>I’d never talked about these ideas in my class before this week. I have also never received so much positive feedback from students. They felt seen. I don’t think I’d ever made them feel seen before, not quite like that. I feel good about that.&lt;/p>
&lt;p>I also feel sad. Students feel these things so hard, especially those we’ve marginalized in so many ways. I’ve been in undergrad classrooms for 15 years. I’ve worked with literally hundreds of students. So many missed opportunities.&lt;/p>
&lt;p>I can’t change that. But I can keep it up in future semesters. Urry is ON.&lt;/p>
&lt;p>Meanwhile,
&lt;a href="https://osf.io/597ut/" target="_blank" rel="noopener">here are some of my slides minus film clips and the email&lt;/a>. Maybe they’ll help you or someone you know address representation as a foundational idea of science at one of the earliest career stages.&lt;/p>
&lt;hr>
&lt;br>
&lt;p>&lt;em>Editor&amp;rsquo;s note: The present text is an adapted version of widely shared
&lt;a href="https://twitter.com/HeatherUrry/status/1312104732308115457?s=20" target="_blank" rel="noopener">Twitter thread&lt;/a> which resonated with so many of us. We thought it is of general interest and deserved to be immortalized, and hence we approached Heather to adapt the thread to post it here. Importantly, in addition to writing extremely current and relevant threads, Heather has also inspired the creation of FORRT. Indeed, FORRT was initiated at the 2018 meeting of the
&lt;a href="https://improvingpsych.org/" target="_blank" rel="noopener">Society for the Improvement of Psychological Science (SIPS)&lt;/a> in Heather&amp;rsquo;s
&lt;a href="https://osf.io/x7d45/" target="_blank" rel="noopener">“Teaching replicable and reproducible science”&lt;/a> hackathon with Kristen Lane.&lt;/em>&lt;/p></description></item></channel></rss>