[
    {
        "arabic": [
            {
                "Title": "GPower (برنامج قوة جي) *",
                "Definition": "** Free to use statistical software for performing power analyses. The user specifies the desired statistical test (e.g. t-test, regression, ANOVA), and three of the following: the number of groups/observations, effect size, significance level, or power, in order to calculate the unspecified aspect.  **",
                "Reference": "** Faul et al. (2007); Faul et al. (2009)",
                "Originally drafted by": "** Filip Dechterenko",
                "Reviewed (or Edited) by": "** Thomas Rhys Evans; Kai Krautter; Charlotte R. Pennington",
                "Translated by": "** Ahlam Ahmed",
                "Translation reviewed by": "** Ali H. Al-Hoorie, Alaa M. Saleh, Mai Helmy, Hala Alghamdi, Mohammed Mohsen",
                "Translation": "التَّعريف:** برنامج إحصائي مجاني لإجراء تحليلات القوة. يحدد المستخدم الاختبار الإحصائي المطلوب (مثل اختبار تي ، الانحدار، تحليل التباين (أنوفا)، وثلاثة مما يلي: عدد المجموعات/الملاحظات، أو حجم التأثير، أو مستوى الأهمية، أو القوة، من أجل حساب الجانب غير المحدد.  **المصطلحات ذات الصِّلة:** تحليل القوة، تبرير حجم العينة، تخطيط حجم العينة، القوة الإحصائيَّة.",
                "Related_terms": "** Power analysis; Sample size justification; Sample size planning; Statistical power"
            },
            {
                "Title": "Gaming (the system) (التَّلاعب بالنِّظام) *",
                "Definition": "** Adopting questionable research practices (QRPs, e.g., salami slicing of an academic paper) that would align with academic incentive structures that benefit the academic (e.g. in prestige, hiring, or promotion) regardless of whether they support the process of scholarship. If systems rely on metrics to determine an outcome (e.g. academic credit) those metrics can be subject to intentional manipulation (Naudet et al., 2018\\) or “gamed”. Where promotions, hiring, and tenure are based on flawed metrics they may disfavor openness, rigor, and transparent work (Naudet et al., 2018\\) \\- for example favoring “quantity over quality” \\- and exacerbate existing inequalities.",
                "Reference(s)": "** Moher et al. (2018); Naudet et al. (2018)",
                "Drafted by": "** Adrien Fillon",
                "Reviewed (or Edited) by": "** Sarah Ashcroft-Jones; Helena Hartmann; Sam Parsons; Charlotte R. Pennington  **\\[:ARABIC\\] التَّعريف:** اتباع ممارسات بحثيَّة مشبوهة كالتَّجزئة غير المبررة لورقة أكاديميَّة) والتي من شأنها أن تتماشى مع  أنظمة الحوافز الأكاديميَّة التي تفيد الباحث، مثلًا: في المكانة، أو التَّوظيف، أو التَّرقية، بغض النَّظر عمَّا إذا كانت تدعم المعرفة.  إذا كانت الأنظمة تعتمد على المقاييس لتحديد نتيجة ما (مثل السُّمعة الأكاديميَّة)، فقد تتعرَّض هذه المقاييس للتَّلاعب المتعمَّد (Naudet et al., 2018). وعندما تستند التَّرقيات، والتَّوظيف، والتَّثبيت الوظيفي إلى مقاييس معيبة، فقد لا يحبذ البعض الانفتاح، والصَّرامة، والعمل الشَّفاف (Naudet et al., 2018\\) \\- وذلك كتفضيل \"البحوث الكميَّة على النوعية\" \\- وتؤدي إلى تفاقم أوجه عدم المساواة القائمة.  **المصطلحات ذات الصِّلة**: هيكل الحوافز، معامل تأثير المجلَّة، قرصنة القيمة الاحتماليَّة.",
                "Translated by": "** Ahlam Ahmed",
                "Translation reviewed by": "** Ali H. Al-Hoorie, Alaa M. Saleh, Hiba Alomary, Hala Alghamdi, Mai Helmy, Mohammed Mohsen",
                "Translation": "** Adopting questionable research practices (QRPs, e.g., salami slicing of an academic paper) that would align with academic incentive structures that benefit the academic (e.g. in prestige, hiring, or promotion) regardless of whether they support the process of scholarship. If systems rely on metrics to determine an outcome (e.g. academic credit) those metrics can be subject to intentional manipulation (Naudet et al., 2018\\) or “gamed”. Where promotions, hiring, and tenure are based on flawed metrics they may disfavor openness, rigor, and transparent work (Naudet et al., 2018\\) \\- for example favoring “quantity over quality” \\- and exacerbate existing inequalities.",
                "Related_terms": "** Incentive structure; Journal Impact Factor; *P*\\-hacking"
            },
            {
                "Title": "Garden of Forking Paths (حديقة المسارات المتشعبة ) *",
                "Definition": "** The typically-invisible decision tree traversed during operationalization and statistical analysis given that “there is a one-to-many mapping from scientific to statistical hypotheses” (Gelman & Loken, 2013, p. 6). In other words, even in absence of p-hacking or fishing expeditions and when the research hypothesis was posited ahead of time, there can be a plethora of statistical results that can appear to be supported by theory given data. “The problem is there can be a large number of potential comparisons when the details of data analysis are highly contingent on data, without the researcher having to perform any conscious procedure of fishing or examining multiple p-values” (Gelman & Loken, 2013, p. 1). The term aims to highlight the uncertainty ensuing from idiosyncratic analytical and statistical choices in mapping theory-to-test, and contrasting intentional (and unethical) questionable research practices (e.g. p-hacking and fishing expeditions) versus non-intentional research practices that can, potentially, have the same effect despite not having intent to corrupt their results. The garden of forking paths refers to the decisions during the scientific process that inflate the false-positive rate as a consequence of the potential paths which could have been taken (had other decisions been made). **",
                "Reference(s)": "** Gelman and Loken (2013)",
                "Drafted by": "** Flávio Azevedo; Mahmoud Elsherif",
                "Reviewed (or Edited) by": "** Gisela H. Govaart; Matt Jaquiery; Tamara Kalandadze; Charlotte R. Pennington",
                "Translated by": "** Ahlam Ahmed",
                "Translation reviewed by": "** Ali H. Al-Hoorie, Alaa M. Saleh, Mai Helmy, Hala Alghamdi, Mohammed Mohsen",
                "Translation": "التَّعريف:** شجرة اتخاذ القرار غير المرئيَّة التي يتم اجتيازها عادة أثناء التَّشغيل، والتَّحليل الإحصائي بالنَّظر إلى أن \"هناك طرق متعدِّدة للانتقال من الفرضيَّات العلميَّة إلى الفرضيَّات الإحصائيَّة\" (Gelman & Loken, 2013, p. 6). وبعبارة أخرى، حتى في حالة عدم وجود عمليَّات القرصنة للقيمة الاحتماليَّة، أو التَّصيُّد، وعندما يتم طرح فرضيَّة البحث مسبقًا، يمكن أن يكون هناك عدد كبير من النَّتائج الإحصائيَّة التي يمكن أن تبدو مدعومة بالبيانات النَّظريَّة المعطاة. \"تكمن المشكلة في أنَّه يمكن أن يكون هناك عدد كبير من المقارنات المحتملة عندما تكون تفاصيل التَّحليل معتمدة بشكل كبير على البيانات، دون أن يضطر الباحث إلى اتِّباع أي تصيُّد بشكل واع، أو فحص لقيم احتماليَّة متعدِّدة\" (Gelman & Loken, 2013, p. 1). يهدف المصطلح إلى تسليط الضَّوء على حالة عدم اليقين النَّاتجة عن الخيارات التَّحليليَّة والإحصائيَّة الشَّخصيَّة في الانتقال من النَّظريَّة لاختبارها، والتَّمييز بين الممارسات البحثيَّة المتعمدة (وغير الأخلاقيَّة) المشبوهة (مثل عمليات قرصنة القيمة الاحتماليَّة، والتَّصيُّد) وبين ممارسات البحث غير المقصودة التي من المحتمل أن يكون لها نفس التَّأثير على الرغم من عدم وجود نيّة لإفساد النَّتائج. تشير حديقة المسارات المتشعِّبة إلى القرارات أثناء العمليَّة العلميَّة التي تضخم معدَّل الإيجابيَّة الكاذبة كنتيجة للمسارات المحتملة التي كان من الممكن اتخاذها (في حالة اتخاذ قرارات أخرى). **المصطلحات ذات الصِّلة:** الإيجابيَّة الكاذبة، الخطأ العائلي، تحليل الأكوان المتعدِّدة، التَّسجيل المسبق، درجات حريَّة الباحث، تحليل منحنى المواصفات",
                "Related_terms": "** False-positive; Familywise error; Multiverse Analysis; Preregistration; Researcher degrees of freedom; Specification Curve Analysis"
            },
            {
                "Title": "General Data Protection Regulation (GDPR) (لائحة حماية البيانات العامَّة)*",
                "Definition": "** A legal framework of seven principles implemented across the European Union (EU) that aims to safeguard individuals’ information. The framework seeks to commission citizens with control over their personal data, whilst regulating the parties involved in storing and processing these data. This set of legislation dictates the free movement of individuals’ personal information both within and outside the EU and must be considered by researchers when designing and running studies.  **",
                "Reference": "** Crutzen et al. (2019); [https://ico.org.uk/for-organisations/guide-to-data-protection/guide-to-the-general-data-protection-regulation-gdpr/](https://ico.org.uk/for-organisations/guide-to-data-protection/guide-to-the-general-data-protection-regulation-gdpr/); [https://ec.europa.eu/info/law/law-topic/data-protection\\_en](https://ec.europa.eu/info/law/law-topic/data-protection_en)",
                "Originally drafted by": "** Graham Reid",
                "Reviewed (or Edited) by": "** Elizabeth Collins; Mahmoud Elsherif; Christopher Graham; Sam Parsons",
                "Translated by": "** Sarah Almutairi",
                "Translation reviewed by": "** Ali H. Al-Hoorie, Alaa M. Saleh, Hala Alghamdi, Mai Helmy, Mohammed Mohsen",
                "Translation": "التَّعريف:** إطار عمل قانوني مٌطبق في الاتحاد الأوروبيّ، ويتضمَّن سبعة مبادئ هدفها حماية بيانات الأفراد.  يسعى هذا الإطار إلى تمكين الأفراد من التحَّكم ببياناتهم الشَّخصيَّة وتنظيم عمل الجهات التي تعمل في مجال تحليل وحفظ هذه البيانات. كما تضبط هذه التَّشريعات حريَّة نقل البيانات الشَّخصيَّة للأفراد داخل وخارج الاتحاد الأوروبي والتي يجب على الباحثين الالتزام بها عند تصميم وإجراء الدِّراسات. **المصطلحات ذات الصِّلة:** التّعمية ، خِطة إدارة البيانات، مشاركة البيانات، التكرار، قابليَّة التِّكرار، ،قابلية إعادة الإنتاج.",
                "Related_terms": "** Anonymity; Data Management Plan (DMP); Data sharing; Repeatability; Replicability; Reproducibility"
            },
            {
                "Title": "Generalizability (القابليَّة للتَّعميم ) *",
                "Definition": "** Generalizability refers to how applicable a study’s results are to broader groups of people, settings, or situations they study and how the findings relate to this wider context (Frey, 2018; Kukull & Ganguli, 2012).  **",
                "Reference(s)": "** Esterling et al. (2021); Frey (2018); Kukull and Ganguli (2012); LeBel et al. (2017); Nosek and Errington (2020); Yarkoni (2020)",
                "Drafted by": "** Aoife O’Mahony",
                "Reviewed (or Edited) by": "** Adrien Fillon; Matt Jaquiery; Tina Lonsdorf; Sam Parsons; Julia Wolska",
                "Translated by": "** Sarah Almutairi",
                "Translation reviewed by": "** Ali H. Al-Hoorie, Alaa M. Saleh, Ahlam Ahmed, Hala Alghamdi, Mohammed Mohsen",
                "Translation": "التَّعريف:** هي القدرة على تعميم نتائج دراسة معيّنة على عدد أكبر من المجموعات، أو الظروف، أو المواقف مقارنة بتلك التي شملتها الدِّراسة، وكذلك كيفيَّة ربط هذه النَّتائج بسياق أوسع (Frey, 2018; Kukull & Ganguli, 2012).  **المصطلحات ذات الصِّلة**: التِّكرار المفاهيمي، الصِّدق الخارجي، جمع العيّنات الانتهازي، تحيُّز اختيار العَينة، وِيرد (WEIRD).  **تعريف بديل:** استخدام المواد المعدَّلة، أو تطبيق سلسلة تحليلات لبيانات وعيّنات جديدة  لدراسة الفرضيَّة ذاتها (بمواد مختلفة، بيانات مختلفة) لاختبار مدى قابليَّة تعميم النَّتائج تحت الدِّراسة (The Turing Way Community & Scriberia, 2021).  المصطلحات ذات الصِّلة للتَّعريف البديل: التِّكرار المفاهيميّ.",
                "Related_terms": "** Conceptual replication; External Validity; Opportunistic sampling; Sampling bias; WEIRD **Alternative definition:** Applying modified materials and/or analysis pipelines to new data or samples to answer the same hypothesis (different materials, different data) to test how generalizable the effect under study is (The Turing Way Community & Scriberia, 2021). **Related terms to alternative definition:** (if applicable): Conceptual Replication"
            },
            {
                "Title": "Gift (or Guest) Authorship (التَّأليف المُهدى (أو المؤلف الضّيف) ) *",
                "Definition": "** A software package for tracking changes in a local set of files (local version control), initially developed by Linus Torvalds. In general, it is used by programmers to track and develop computer source code within a set directory, folder or a file system. Git can access remote repository hosting services (e.g. GitHub) for remote version control that enables collaborative software development by uploading contributions from a local system. This process found its way into the scientific process to enable open data, open code and reproducible analyses.  **",
                "Reference": "** Kalliamvakov et al. (2014); Scopatz and Huff (2015); Vuorre and Curley (2018); [https://github.com/git/git/commit/e83c5163316f89bfbde7d9ab23ca2e25604af290](https://github.com/git/git/commit/e83c5163316f89bfbde7d9ab23ca2e25604af290)",
                "Originally drafted by": "** Emma Norris",
                "Reviewed (or Edited) by": "** Adrien Fillon; Bettina M.J. Kern; Dominik Kiersz; Robert M. Ross",
                "Translated by": "** Asma Alzahrani",
                "Translation reviewed by": "** Ali H. Al-Hoorie, Alaa M. Saleh, Hiba Alomary, Ahlam Ahmed, Mohammed Mohsen",
                "Translation": "التَّعريف:** هي حزمة برامج لتتبع التَّغييرات في مجموعة محليَّة من الملفات (التَّحكُّم في الإصدار المحليّ)، طورها في البداية لينوس تورفالدس. وبشكل عام يستخدمه المبرمجون لتتبع وتطوير النُّصوص البرمجيَّة داخل دليل، أو مجلَّد، أو نظام ملفات.  يمكن للمتتبِّع الدُّولي العالميّ الوصول (Git) إلى خدمات استضافة المستودعات عن بُعد (مثل GitHub) للتّحكم في الإصدار عن بُعد الذي يتيح تطوير البرامج التَّعاونيَّة عن طريق تحميل المساهمات من نظام محلي. وجدت هذه العمليَّة طريقها إلى العمليَّة العلميَّة لتمكين البيانات المفتوحة، والنُّصوص البرمجيَّة المفتوحة، والتَّحليلات القابلة للتِّكرار. **المصطلحات ذات الصِّلة:**  Github،مستودع البيانات، التحكم في الإصدار.",
                "Related_terms": "** GitHub; Repository; Version control"
            },
            {
                "Title": "Goodhart’s Law (قانون قودهارت) *",
                "Definition": "** A term coined by economist Charles Goodhart to refer to the observation that measuring something inherently changes user behaviour. In relation to examination performance, Strathern (1997) stated that “when a measure becomes a target, it ceases to be a good measure” (p. 308). Applied to open scholarship, and the structure of incentives in academia, Goodhart’s Law would predict that metrics of scientific evaluation will likely be abused and exploited, as evidenced by Muller (2019).  **",
                "Originally drafted by": "** Adam Parker",
                "Reviewed (or Edited) by": "Sam Parsons; Flávio Azevedo",
                "Translated by": "** Awatif Alruwaili",
                "Translation reviewed by": "** Hiba Alomary, Asma Alzahrani, Ali H. Al-Hoorie, Alaa M. Saleh, Mohammed Mohsen   ###  ### **H** {#h}",
                "Translation": "التَّعريف:** مصطلح صاغه خبير الاقتصاد تشارلز قودهارت للإشارة إلى أنَّ قياس شيء ما يؤدي بطبيعته إلى تغيُّر سلوك المستخدِّم. فعندما يتعلَّق الأمر بتقييم الأداء \"عندما يصبح المقياس هو الهدف فإنه يتوقَّف عن كونه مقياسًا جيِّدًا\" (Strathern, 1997, p. 308). عند تطبيقه على العلم المفتوح وهيكل الحوافز في الأوساط الأكاديميَّة، يتنبأ قانون قودهارت بأنَّه من المرجَّح أن يتم إساءة استخدام مقاييس التَّقييم العلمي واستغلاله (Muller, 2019).  المصطلحات ذات الصِّلة: قانون كامبل، إعلان سان فرانسيسكو بشأن تقييم البحوث، التَّجسيد (المغالطة).",
                "Related_terms": "Campbell's law; DORA; Reification (fallacy) **Reference (s):** Muller (2019); Strathern (1997)"
            },
            {
                "Title": "H-index (مؤشر إتش) *",
                "Definition": "** Hirsch’s index, abbreviated as H-index, intends to measure both productivity and research impact by combining the number of publications and the number of citations to these publications. Hirsch (2005) defined the index as “the number of papers with citation number ≥ *h*” (p. 16569). That is, the greatest number such that an author (or journal) has published at least that many papers that have been cited at least that many times. The index is perceived as a superior measure to measures that only assess, for instance, the number of citations and number of publications but this index has been criticised for the purpose of researcher assessment (e.g. Wendl, 2007).  **",
                "Reference(s)": "** Hirsch (2005); Wendl (2007)",
                "Drafted by": "** Jacob Miranda",
                "Reviewed (or Edited) by": "** Bradley J. Baker; Mahmoud M. Elsherif; Brett J. Gall; Charlotte R. Pennington",
                "Translated by": "** Awatif Alruwaili",
                "Translation reviewed by": "** Hiba Alomary, Asma Alzahrani, Ali H. Al-Hoorie, Alaa M. Saleh, Hala Alghamdi, Mohammed Mohsen",
                "Translation": "التَّعريف:** يهدف مؤشر هيرش (ويختصر اسمه بمؤشر إتش) إلى قياس كل من الإنتاجيَّة وتأثير البحث من خلال الجمع بين عدد المنشورات، وعدد الاستشهادات لهذه المنشورات. يعرّف هذا المؤشر بأنَّه: عدد الأوراق التي تحمل عدد استشهادات ≥ إتش\" (Hirsch, 2005, p. 16569). وهذا يعني أكبر عدد من المنشورات لمؤلف، أو لمجلة تم الاستشهاد بها على الأقل نفس العدد من المرات.  يتفوَّق هذا المؤشر على المقاييس التي تقيم فقط، على سبيل المثال، عدد الاستشهادات وعدد المنشورات ولكن تم انتقاده عندما يستخدم في تقييم باحث ما (مثلًا Wendl, 2007).  **المصطلحات ذات الصِّلة:** الاستشهاد، إعلان سان فرانسيسكو بشأن تقييم البحوث، مؤشر i10، التَّأثير.",
                "Related_terms": "** Citation; DORA; I10-index; Impact"
            },
            {
                "Title": "Hackathon (هاكثون) *",
                "Definition": "** An organized event where experts, designers, or researchers collaborate for a relatively short amount of time to work intensively on a project or problem. The term is originally borrowed from computer programmer and software development events whose goal is to create a fully fledged product (resources, research, software, hardware) by the end of the event, which can last several hours to several days.  **",
                "Reference": "** Kienzler and Fontanesi (2017)",
                "Originally drafted by": "** Flávio Azevedo",
                "Reviewed (or Edited) by": "** Tsvetomira Dumbalska; Brett J. Gall; Emma Norris",
                "Translated by": "** Zainab Alsuhaibani",
                "Translation reviewed by": "** Hiba Alomary, Asma Alzahrani, Ali H. Al-Hoorie, Alaa M. Saleh, Hala Alghamdi, Mohammed Mohsen",
                "Translation": "التَّعريف**: هو حدث منظَّم يتعاون فيه الخبراء، أو المصمِّمون، أو الباحثون لفترة زمنيَّة قصيرة نسبيًا للعمل بشكل مكثَّف على مشروع، أو مشكلة ما.  تم استعارة هذا المصطلح في الأصل من برمجة الحاسب ومسابقات تطوير البرامج التي تهدف إلى الوصول لمنتج كامل (مثل موارد، بحث، برنامج، جهاز) بحلول نهاية المسابقة، والتي يمكن أن تستمر من عدَّة ساعات إلى عدَّة أيام. **المصطلحات ذات الصِّلة:** التَّعاون؛ إديهاتون",
                "Related_terms": "** Collaboration; Edithaton"
            },
            {
                "Title": "HARKing (الافتراض بعد معرفة النَّتائج) *",
                "Definition": "** A questionable research practice termed ‘Hypothesizing After the Results are Known’ (HARKing). “HARKing is defined as presenting a post hoc hypothesis (i.e., one based on or informed by one's results) in a research report as if it was, in fact, *a priori*” (Kerr, 1998, p. 196). For example, performing subgroup analyses, finding an effect in one subgroup, and writing the introduction with a ‘hypothesis’ that matches these results. **",
                "Reference": "Hitchcock and Sober (2014)",
                "Reference(s)": "** Kerr (1998); Nosek and Lakens (2014)",
                "Drafted by": "** Beatrix Arendt",
                "Reviewed (or Edited) by": "** Matt Jaquiery; Charlotte R. Pennington; Martin Vasilev; Flávio Azevedo",
                "Translated by": "** Awatif Alruwaili",
                "Translation reviewed by": "** Asma Alzahrani, Ali H. Al-Hoorie, Mai Helmy, Alaa M. Saleh, Hala Alghamdi, Mohammed Mohsen",
                "Translation": "التَّعريف:** ممارسة بحثيَّة مشبوهة \"تعرّف بتقديم فرضيَّة لاحقة (أي فرضيَّة مبنيَّة على النَّتائج، أو مستنيرة بها) في تقرير بحثي كما لو كانت في الواقع بديهية\" (Kerr, 1998, p. 196). مثال ذلك إجراء تحليلات لمجموعات فرعيَّة، وإيجاد تأثير في مجموعة فرعيَّة واحدة، وكتابة المقدمة مع \"فرضيّة\" تطابق هذه النَّتائج. **المصطلح البديل:** الافتراض التكيفي **المصطلحات ذات الصِّلة:**  المرونة التحليليَّة، النَّقد بعد معرفة النَّتائج، التحليلات التوكيديَّة، تحليل البيانات الاستكشافي، التَّنصُّل، حديقة المسارات المتشعِّبة، قرصنة القيمة الاحتماليَّة، التَّسجيل بعد معرفة النَّتائج، ممارسات البحث المشكوك فيها أو ممارسات إعداد التقارير المشكوك فيها، تخطيط حجم العيِّنة بعد معرفة النَّتائج. **Alternative terms**: accommodational hypothesizing",
                "Related_terms": "** Analytic Flexibility; CARKing; Confirmatory analyses; Exploratory data analysis; Fudging; Garden of forking paths; P-hacking; PARKing; Questionable Research Practices or Questionable Reporting Practices (QRPs); SPARKing"
            },
            {
                "Title": "Hidden Moderators (المتغيِّرات الوسيطة الخفيَّة) *",
                "Definition": "** Contextual conditions that can, unbeknownst to researchers, make the results of a replication attempt deviate from those of the original study. Hidden moderators are sometimes invoked to explain (away) failed replications. Also called hidden assumptions.  **",
                "Reference": "** Zwaan et al. (2018)",
                "Originally drafted by": "** Ali H. Al-Hoorie",
                "Reviewed (or Edited) by": "** Mahmoud Elsherif; Sam Parsons",
                "Translated by": "** Ahmed Hakami",
                "Translation reviewed by": "** Hiba Alomary, Asma Alzahrani, Ali H. Al-Hoorie, Alaa M. Saleh, Hala Alghamdi, Mohammed Mohsen",
                "Translation": "التَّعريف:** الظُّروف السياقيَّة التي يمكن دون علم الباحثين أن تجعل نتائج محاولة التِّكرار تحيد عن نتائج الدِّراسة الأصليَّة. يتم أحيانًا اللُّجوء للمتغيِّرات الوسيطة الخفيَّة؛ لتبرير فشل تكرار النَّتائج. وتسمى أيضًا الافتراضات الخفيَّة.  **المصطلحات ذات الصِّلة:** الفرضيَّة المساعدة.",
                "Related_terms": "** Auxiliary Hypothesis"
            },
            {
                "Title": "Hypothesis (الفرضيَّة) *",
                "Definition": "** A hypothesis is an unproven statement relating the connection between variables (Glass & Hall, 2008\\) and can be based on prior experiences, scientific knowledge, preliminary observations, theory and/or logic. In scientific testing, a hypothesis can be usually formulated with (e.g. a positive correlation) or without a direction (e.g. there will be a correlation). Popper (1959) posits that hypotheses must be falsifiable, that is, it must be conceivably possible to prove the hypothesis false. However, hypothesis testing based on falsification has been argued to be vague, as it is contingent on many other untested assumptions in the hypothesis (i.e., auxiliary hypotheses). Longino (1990, 1992\\) argued that ontological heterogeneity should be valued more than ontological simplicity for the biological sciences, which considers we should investigate differences between and within biological organisms.  **",
                "Reference(s)": "** Beller and Bender (2017); Glass and Hall (2008); Longino (1990, 1992); Popper (1959)",
                "Drafted by": "** Ana Barbosa Mendes",
                "Reviewed (or Edited) by": "** Ali H. Al-Hoorie; Mahmoud Elsherif; Helena Hartmann; Charlotte R. Pennington**;** Graham Reid; Olly Robertson",
                "Translated by": "Awatif Alruwaili",
                "Translation reviewed by": "** Asma Alzahrani, Ali H. Al-Hoorie, Mai Helmy, Alaa M. Saleh, Mohammed Mohsen  ### **I** {#i}",
                "Translation": "التَّعريف**: الفرضيَّة عبارة عن ادِّعاء غير مثبت يتعلَّق بالعلاقة بين المتغيّرات (Glass & Hal, 2008\\) ويمكن أن تستند إلى تجارب سابقة، أو معرفة علميَّة، أو ملاحظات أوليَّة، أو نظريَّة أو منطق.  في الاختبارات العلميَّة، يمكن عادةً صياغة الفرضيَّة مع تحديد الاتجاه، مثلًا ارتباط إيجابي، أو بدون اتجاه، مثلًا سيكون هناك ارتباط ما. يذهب بعض الفلاسفة (Popper, 1959\\) إلى أن الفرضيَّات يجب أن تكون قابلة للدَّحض، أي أنَّه يجب أن يكون من الممكن إثبات خطأ الفرضيَّة. ومع ذلك، فقد قيل إن اختبار الفرضية القائم على التفنيد غامض؛ لأنَّه يعتمد على العديد من الافتراضات الأخرى غير المختبرة في الفرضيَّة (أي الفرضيَّات المساعدة). كما حاجج البعض (Longino, 1990, 1992\\) بأنَّ عدم التَّجانس الوجودي يجب أن يتم تقييمه أكثر من البساطة الوجوديَّة للعلوم البيولوجيَّة، والذي يرى أنَّه يجب علينا دراسة الاختلافات بين الكائنات البيولوجيَّة وداخلها. **المصطلحات ذات الصِّلة:** الفرضيَّة المساعدة، التحليلات التوكيديَّة، نتائج سلبيَّة كاذبة، نتائج إيجابيَّة كاذبة، النَّمذجة، التَّنبؤات، البحث الكميّ، النَّظريَّة، بناء النَّظريَّة، الخطأ من النَّوع الأوّل، الخطأ من النَّوع الثَّاني.",
                "Related_terms": "** Auxiliary Hypothesis; Confirmatory analyses; False negative result; False positive result; Modelling; Predictions; Quantitative research; Theory; Theory building; Type I error; Type II error"
            },
            {
                "Title": "i10-index (مؤشر i10) *",
                "Definition": "** A research metric created by Google Scholar that represents the number of publications a researcher has with at least 10 citations.  **",
                "Reference(s)": "** [https://guides.library.cornell.edu/impact/author-impact-10](https://guides.library.cornell.edu/impact/author-impact-10)",
                "Drafted by": "** Emma Norris",
                "Reviewed (or Edited) by": "** Flávio Azevedo; Sam Parsons",
                "Translated by": "** Nazik Alnour",
                "Translation reviewed by": "** Ali H. Al-Hoorie, Mai Helmy, Alaa M. Saleh, Hala Alghamdi, Mohammed Mohsen",
                "Translation": "ا**لتَّعريف: مؤشر أنشأه الباحث العلمي من قوقل، ويمثل عدد المنشورات لباحث ما التي حصلت على 10 استشهادات على الأقل.  المصطلحات ذات الصِّلة: الاستشهاد، إعلان سان فرانسيسكو بشأن تقييم البحوث، مؤشر إتش، التَّأثير.",
                "Related_terms": "** Citation; DORA; H-index; Impact"
            },
            {
                "Title": "Ideological bias (التَّحيُّز الأيديولوجي) *",
                "Definition": "** The idea that pre-existing opinions about the quality of research can depend on the ideological views of the author(s). One of the many biases in the peer review process, it expects that favourable opinions towards the research would be more likely if friends, collaborators, or scientists agree with an editor or reviewer’s political viewpoints (Tvina et al., 2019). This could potentially lead to a variety of conflicts of interest that undermine diverse perspectives, for example: speeding or delaying peer-review, or influencing the chances of an individual being invited to present their research, thus promoting their work.  **",
                "Reference(s)": "** Tvina et al. (2019)",
                "Drafted by": "** Mahmoud Elsherif",
                "Reviewed (or Edited) by": "** Elizabeth Collins; Flávio Azevedo; Madeleine Ingham; Sam Parsons; Graham Reid",
                "Translated by": "** Nazik Alnour **** Ali H. Al-Hoorie, Ahlam Ahmed, Mai Helmy, Alaa M. Saleh, Mohammed Mohsen",
                "Translation": "التَّعريف:** يمكن أن تعتمد الآراء المسبقة حول جودة البحث على وجهات النَّظر الأيديولوجيَّة للمؤلف (المؤلفين) باعتبارها واحدة من التَّحيُّزات العديدة في عملية تحكيم الأقران.  يُتوقع أن تكون الآراء الإيجابيَّة تجاه البحث أكثر احتماليَّة إذا اتفق الأصدقاء، أو المتعاونون، أو العلماء مع وجهات النَّظر السِّياسيَّة للمحرِّر، أو المراجع (Tvina et al., 2019). وقد يؤدي ذلك إلى مجموعة متنوعة من تضارب المصالح التي تقلِّل من التَّنوع في وجهات النَّظر، على سبيل المثال تسريع أو تأخير تحكيم الأقران، أو التَّأثير على فرص دعوة الفرد لتقديم بحوثه، وبالتَّالي التَّرويج لعمله.  **المصطلحات ذات الصِّلة:** تحيُّز الشَّخصنة، تحكيم الأقران.",
                "Related_terms": "** Ad hominem bias; Peer review"
            },
            {
                "Title": "Inclusion (الشُّمول ) *",
                "Definition": "** Inclusion, or inclusivity, refers to a sense of welcome and respect within a given collaborative project or environment (such as academia) where *diversity* simply indicates a wide range of backgrounds, perspectives, and experiences. Efforts to increase *inclusion* go further to promote engagement and equal valuation among diverse individuals, who might otherwise be marginalized. Increasing inclusivity often involves minimising the impact of, or even removing, systemic barriers to accessibility and engagement.  **",
                "Reference": "Calvert (2019); [Martinez-Acosta and Favero (2018)](https://www-ncbi-nlm-nih-gov.proxy.library.vanderbilt.edu/pmc/articles/PMC6153014/)",
                "Drafted by": "** Ryan Millager",
                "Reviewed (or Edited) by": "** Mahmoud Elsherif; Graham Reid; Kai Krautter; Suzanne L. K. Stewart; Flávio Azevedo",
                "Translated by": "** Ali H. Al-Hoorie",
                "Translation reviewed by": "** Mai Helmy, Ahlam Ahmed, Alaa M. Saleh, Hiba Alomary, Mohammed Mohsen",
                "Translation": "التَّعريف:** يقصد بالشَّمول أو الشُّمولية: الشُّعور بالتَّرحاب، والاحترام داخل مشروع، أو بيئة تعاونيَّة معيَّنة (مثل الأوساط الأكاديميَّة) حيث يشير *التَّنوع* ببساطة إلى وجود مجموعة واسعة من الخلفيَّات، ووجهات النَّظر والخبرات.  كما تسعى الجهود المبذولة  لزيادة *الشُّموليَّة* إلى تعزيز المشاركة، والتَّقييم المتساوي بين مختلف الأفراد الذين قد يتعرضون للتَّهميش.  غالبًا ما تتضمَّن زيادة الشُّمولية تقليل تأثير، أو حتى إزالة الحواجز النِّظاميَّة التي تحول دون إمكانيَّة الوصول والمشاركة. **المصطلحات ذات الصِّلة:** التَّنوع، العدالة، العدالة الاجتماعيَّة.",
                "Related_terms": "** Diversity; Equity; Social Justice"
            },
            {
                "Title": "Incentive structure (هيكل الحوافز) *",
                "Definition": "** The set of evaluation and reward mechanisms (explicit and implicit) for scientists and their work. Incentivised areas within the broader structure include hiring and promotion practices, track record for awarding funding, and prestige indicators such as publication in journals with high impact factors, invited presentations, editorships, and awards. It is commonly believed that these criteria are often misaligned with the telos of science, and therefore do not promote rigorous scientific output. Initiatives like DORA aim to reduce the field’s dependency on evaluation criteria such as journal impact factors in favor of assessments based on the intrinsic quality of research outputs.  **",
                "Reference": "** Koole and Lakens (2012); Nosek et al. (2012); Schonbrodt (2019); Smaldino and McElreath (2016)",
                "Originally drafted by": "** Charlotte R. Pennington; Olmo van den Akker",
                "Reviewed (or Edited) by": "** Helena Hartmann; Flávio Azevedo; Robert M. Ross; Graham Reid; Suzanne L. K. Stewart",
                "Translated by": "Awatif Alruwaili",
                "Translation reviewed by": "** Ahlam Ahmed, Alaa M. Saleh, Ali H. Al-Hoorie, Asma Alzahrani, Mohammed Mohsen",
                "Translation": "التَّعريف:** الهياكل التَّحفيزيَّة هي مجموعة من آليات التَّقييم والمكافأة (الصَّريحة والضِّمنيَّة) للعلماء وأعمالهم. ضمن الهيكل الأوسع.  تتعلَّق المجالات المحفزَّة بممارسات التَّوظيف والتَّرقيات، ووجود سجل حافل في منح التَّمويل، ومؤشرات الاعتبار كالنَّشر في مجلات ذات معامل تأثير عالية، ودعوات العروض التَّقديميَّة، والتَّحرير، والجوائز. يذهب الكثير إلى أنَّ هذه المعايير غالبًا ما تكون غير متوافقة مع غاية العلم وبالتَّالي لا تشجع على الإنتاج العلمي الصَّارم. تهدف مبادرات مثل دورا إلى تقليل الاعتماد على معايير التَّقييم، مثل تقليل الاعتماد على معامل التَّأثير للمجلَّات، والتَّركيز على الجوهر العلمي لمخرجات البحوث. **المصطلحات ذات الصِّلة**: إعلان سان فرانسيسكو بشأن تقييم البحوث، المقاييس، الضَّغط، النشر أو الفناء، الكميَّة، هيكل المكافأة (نظام المكافآت)، المنشورات العلميَّة، العلم البطيء، العوامل الهيكليَّة (البنيويَّة).",
                "Related_terms": "** DORA; Metrics; Pressure; Publish or perish; Quantity; Reward structure; Scientific publications; Slow science; Structural factors"
            },
            {
                "Title": "Induction (الاستقراء) *",
                "Definition": "** “Reasoning by drawing a conclusion not guaranteed by the premises; for example, by inferring a general rule from a limited number of observations. Popper believed that there was no such logical process; we may guess general rules but such guesses are not rendered even more probable by any number of observations. By contrast, Bayesians inductively work out the increase in probability of a hypothesis that follows from the observations” (Dienes, 2008, p. 164).  **",
                "Reference": "Dienes (2008)",
                "Drafted by": "** Alaa Aldoh",
                "Translated by": "** Dr. Nazik Alnour **** Alaa M. Saleh, Ali H. Al-Hoorie, Ahlam Ahmed, Asma Alzahrani, Mohammed Mohsen",
                "Translation": "التَّعريف:** هو الاستدلال باستخلاص نتائج لم تتضمَّنها المقدِّمات المنطقية، مثل استنباط قاعدة عامة من عدد محدود من الملاحظات.  ويعتقد بوبر بأن هذه العمليَّة غير ممكنة، فقد نخمِّن القواعد العامة، لكن مثل هذه التَّخمينات لا تصبح أكثر ترجيحًا عبر زيادة أي عدد من الملاحظات، وعلى نقيض ذلك، يقدّر علماء المنهج البايزي بشكل استقرائي مقدار الزِّيادة في ترجيح الفرضيَّة عبر تتبع الملاحظات\" (Dienes, 2008, p. 164).  **المصطلحات ذات الصِّلة:** الفرضيَّة.",
                "Related_terms": "** Hypothesis"
            },
            {
                "Title": "Interaction Fallacy (مغالطة التَّفاعل) *",
                "Definition": "** A statistical error in which a formal test is not conducted to assess the difference between a significant and non-significant correlation (or other measures, such as Odds Ratio). This fallacy occurs when a significant and non-significant correlation coefficient are assumed to represent a statistically significant difference but the comparison itself is not explicitly tested.  **",
                "Reference": "** Gelman and Stern (2006); Morabia et al. (1997); Nieuwenhuis et al. (2011)",
                "Originally drafted by": "** Graham Reid",
                "Reviewed (or Edited) by": "** Ali H. Al-Hoorie; Mahmoud Elsherif; Kai Krautter; Sam Parsons.",
                "Translated by": "** Ahlam Ahmed",
                "Translation reviewed by": "** Alaa M. Saleh, Ali H. Al-Hoorie, Asma Alzahrani, Mohammed Mohsen",
                "Translation": "التَّعريف:** هو خطأ إحصائي، حيث لا يتم إجراء اختبار منهجي لتقييم الفرق بين الارتباط ذي الدلالة الإحصائيَّة والارتباط الذي لم يحصل على لدلالة الإحصائيَّة، (أو مقاييس أخرى ، مثل نسبة الأرجحيَّة).  تحدث هذه المغالطة عندما يُفترض أنَّ معامل ارتباط دال إحصائياً وآخر غير دال يمثلان فرقًا ذا دلالة إحصائيَّة، ولكن بدون اختبار المقارنة نفسها بشكل صريح. **المصطلحات ذات الصِّلة:** مقارنة الارتباطات، اختبار دلالة الفرضيَّة الصِّفريَّة، الصِّدق الإحصائي، الخطأ من النَّوع الأول، الخطأ من النَّوع الثَّاني.",
                "Related_terms": "** Comparison of Correlations; Null Hypothesis Significance Testing (NHST); Statistical Validity; Type I error; Type II error"
            },
            {
                "Title": "Interlocking (التَّشابك) *",
                "Definition": "** An analysis at the core of intersectionality to analyse power, inequality and exclusion, as efforts to reform academic culture cannot be completed by investigating only one avenue in isolation (e.g. race, gender or ability) but by considering all the systems of exclusion. In contrast to intersectionality (which refers to the individual having multiple social identities), interlocking is usually used to describe the systems that combine to serve as oppressive measures toward the individual based on these identities.  **",
                "Reference(s)": "** Ledgerwood et al. (2021)",
                "Drafted by": "** Christina Pomareda",
                "Reviewed (or Edited) by": "** Ali H. Al-Hoorie; Flávio Azevedo; Mahmoud Elsherif; Eliza Woodward; Gerald Vineyard;",
                "Translated by": "** Ahmed Hakami",
                "Translation reviewed by": "Alaa M. Saleh, Ali H. Al-Hoorie, Ahlam Ahmed, Asma Alzahrani, Mohammed Mohsen",
                "Translation": "التَّعريف:** تحليل في صميم التَّقاطعية لتحليل السُّلطة، واللا مساواة، والإقصاء حيث لا يمكن إكمال الجهود المبذولة لإصلاح الثَّقافة الأكاديميَّة عبر دراسة وسيلة واحدة فقط بمعزل عن الآخر.  على سبيل المثال: العرق، أو الجنس، أو القدرة. ولكن من خلال النَّظر في جميع الأنظمة الإقصائية. وعلى النَّقيض من التَّقاطعيَّة، والتي تشير إلى أنَّ الفرد يمتلك هويات اجتماعيَّة متعدِّدة، عادةً ما يستخدم التَّشابك لوصف الأنظمة التي تتحد لتكون بمثابة تدابير قمعيَّة تجاه الفرد بناءً على هذه الهويات.  **المصطلحات ذات الصِّلة:** بروبينساينس، العدالة؛ التَّنوع، الشُّمول، التَّقاطعية، العلم المفتوح، العدالة الاجتماعيَّة.",
                "Related_terms": "** Bropenscience; Equity; Diversity; Inclusion; Intersectionality; Open Science; Social Justice"
            },
            {
                "Title": "Internal Validity (الصِّدق الدَّاخلي) *",
                "Definition": "** An indicator of the extent to which a study’s findings are representative of the true effect in the population of interest and not due to research confounds, such as methodological shortcomings. In other words, whether the observed evidence or covariation between the independent (predictor) and dependent (criterion) variables can be taken as a bona fide relationship and not a spurious effect owing to uncontrolled aspects of the study’s set up. Since it involves the quality of the study itself, internal validity is a priority for scientific research.  **",
                "Reference": "** Campbell and Stanley (1966) **Alternative definition:** In Psychometrics, the degree of evidence that confirms the internal structure of a psychometric test as compatible with the structure of a psychological construct. **Related terms to alternative definition:** Construct validity",
                "Reference(s)": "** Messick (1995)",
                "Originally drafted by": "** Annalise A. LaPlume",
                "Reviewed (or Edited) by": "** Helena Hartmann; Oscar Lecuona; Meng Liu; Sam Parsons; Graham Reid; Flávio Azevedo",
                "Translated by": "** Hala Alghamdi",
                "Translation reviewed by": "** Alaa M. Saleh, Ali H. Al-Hoorie, Ahlam Ahmed, Asma Alzahrani, Mohammed Mohsen",
                "Translation": "التَّعريف:** هو مؤشر على مدى كون نتائج دراسة ما تمثل التَّأثيرالحقيقي في مجتمع الدِّراسة المستهدف، وليست ناتجة عن متغيرات بحثيَّة دخيلة، مثل القصور في المنهجيَّة، وبعبارة أخرى: يعبّر الصِّدق الدَّاخلي عمَّا إذا كانت الأدلَّة المرصودة، أو التَّباين بين المتغيّرات المستقلة (المتنبِّئة) والمتغيِّرات التَّابعة (المحكية) ناتجة عن علاقة حقيقيَّة وليست نتيجة زائفة؛ بسبب الجوانب التي لا يمكن التَّحكُّم بها عند تصميم الدِّراسة.  وبما أنَّ الأمر يتعلَّق بجودة الدِّراسة نفسها، يعدُّ الصِّدق الدَّاخلي أولويَّة في البحث العلمي. **المصطلحات ذات الصِّلة:** الصِّدق الخارجي، الصِّدق. **التَّعريف البديل:** في القياسات النَّفسيَّة، هو درجة الأدلَّة التي تؤكد على التَّناسق بين الهيكل الدَّاخلي؛ لاختبار قياس نفسي، وبنية البناء النَّفسي. **المصطلحات ذات الصِّلة بالتَّعريف البديل:** الصِّدق البنائي.",
                "Related_terms": "** External validity; Validity"
            },
            {
                "Title": "Intersectionality (التَّقاطعيَّة) *",
                "Definition": "** A term which derives from Black feminist thought and broadly describes how social identities exist within ‘interlocking systems of oppression’ and structures of (in)equalities (Crenshaw, 1989). Intersectionality offers a perspective on the way multiple forms of inequality operate together to compound or exacerbate each other. Multiple concurrent forms of identity can have a multiplicative effect and are not merely the sum of the component elements. One implication is that identity cannot be adequately understood through examining a single axis (e.g., race, gender, sexual orientation, class) at a time in isolation, but requires simultaneous consideration of overlapping forms of identity.  **",
                "Reference(s)": "** Crenshaw (1989); Grzanka (2020); Ledgerwood et al. (2021)",
                "Drafted by": "** Madeleine Pownall",
                "Reviewed (or Edited) by": "** Ali H. Al-Hoorie; Bradley Baker; Mahmoud Elsherif; Wanyin Li; Ryan Millager; Charlotte R. Pennington; Flávio Azevedo",
                "Translated by": "** Ahlam Ahmed",
                "Translation reviewed by": "** Ali H. Al-Hoorie, Alaa M. Saleh, Hiba Alomary, Asma Alzahrani, Mohammed Mohsen   ### **J** {#j}",
                "Translation": "التَّعريف:** مصطلح مستمد من الفكر النَّسوي الأسود، ويصف بشكل عام وجود الهويات الاجتماعيَّة داخل \"أنظمة الاضطهاد المتشابكة\" وهياكل (عدم) المساواة (Crenshaw, 1989). تقدِّم التَّقاطعيَّة منظورًا عن الطَّريقة التي تعمل بها أشكال متعدِّدة من عدم المساواة معًا؛ لمضاعفة بعضها البعض.  يمكن أن يكون لأشكال الهويَّة المتعدِّدة والمتزامنة تأثير مضاعف أكبر من مجرَّد مجموع العناصر المكونة لها.  أحد الآثار المترتِّبة على ذلك هو أنَّه لا يمكن فهم الهوية بشكل كاف من خلال فحص محور واحد (كالعرق، أو الجنس، أو الميول الجنسيَّة، أو الطَّبقية) في وقت ما بمعزل عن غيره، بل يتطلَّب دراسة متزامنة لأشكال الهوية المتداخلة. **المصطلحات ذات الصِّلة**: بروبنساينس، التَّنوع، الشمول، التشابك، العلم المفتوح.",
                "Related_terms": "** Bropenscience; Diversity; Inclusion; Interlocking; Open Science"
            },
            {
                "Title": "JabRef (برنامج جاب ريف ) *",
                "Definition": "** An open-sourced, cross-platform citation and reference management tool that is available free of charge. It allows editing BibTeX files, importing data from online scientific databases, and managing and searching BibTeX files.  **",
                "Reference": "** JabRef Development Team (2021)",
                "Originally drafted by": "** Aleksandra Lazić",
                "Reviewed (or Edited) by": "** Christopher Graham; Michele C. Lim; Sam Parsons; Steven Verheyen",
                "Translated by": "** Ahmed Hakami",
                "Translation reviewed by": "** Awatif Alruwaili, Ali H. Al-Hoorie, Ahlam Ahmed, Asma Alzahrani, Mohammed Mohsen",
                "Translation": "التَّعريف:** أداة لإدارة المراجع، والاستشهادات، مفتوحة المصدر، متعدِّدة المنصَّات، متاحة مجانًا.  تسمح بتحرير ملفات بيب تيكس، واستيراد البيانات من قواعد البيانات العلميَّة عبر الإنترنت، وإدارة ملفات بيب تيكس والبحث فيه**ا.**  **المصطلحات ذات الصِّلة:** برنامج مفتوح المصدر.",
                "Related_terms": "** Open source software"
            },
            {
                "Title": "Jamovi (جاموفي) *",
                "Definition": "** Free and open source software for data analysis based on the R language. The software has a graphical user interface and provides the R code to the analyses. Jamovi supports computational reproducibility by saving the data, code, analyses, and results in a single file.  **",
                "Reference(s)": "** The jamovi project (2020)",
                "Drafted by": "** Amélie Beffara Bret",
                "Reviewed (or Edited) by": "** Adrien Fillon; Alexander Hart; Charlotte R. Pennington",
                "Translated by": "** Hala Alghamdi",
                "Translation reviewed by": "** Awatif Alruwaili, Ali H. Al-Hoorie, Mai Helmy, Alaa M. Saleh, Ahlam Ahmed, Mohammed Mohsen",
                "Translation": "التَّعريف**: هو برنامج  مجاني، ومفتوح المصد؛ لتحليل البيانات، مبني على لغة آر.  يحتوي البرنامج على واجهة مستخدم رسوميَّة، ويوفِّر نصوص برمجيَّة للتَّحليلات بلغة الآر.  يدعم جاموفي إمكانيَّة إعادة الإنتاج الإحصائي من خلال حفظ البيانات، والتَّعليمات البرمجيَّة، والَّتحليلات، والنَّتائج في ملف واحد. **المصطلحات ذات الصِّلة:** برنامج جاسب، المصدر المفتوح، لغة الآر، قابلية إعادة الإنتاج.",
                "Related_terms": "** JASP; Open source; R; Reproducibility"
            },
            {
                "Title": "JASP (برنامج جاسب ) *",
                "Definition": "** Named after Sir Harold Jeffreys, JASP stands for Jeffrey’s Amazing Statistics Program. It is free and open source software for data analysis. JASP relies on a user interface and offers both null hypothesis tests and their Bayesian counterparts. JASP supports computational reproducibility by saving the data, code, analyses, and results in a single file.  **",
                "Reference(s)": "** JASP Team (2020)",
                "Drafted by": "** Amélie Beffara Bret",
                "Reviewed (or Edited) by": "** Adrien Fillon, Adam Parker; Sam Parsons",
                "Translated by": "** Hala Alghamdi",
                "Translation reviewed by": "** Ali H. Al-Hoorie, Ahlam Ahmed, Alaa M. Saleh, Asma Alzahrani, Mohammed Mohsen",
                "Translation": "التَّعريف**: سُمّيَ البرنامج على اسم السير هارولد جيفريز، وهو اختصار لبرنامج \" جيفري المذهل للإحصائيَّات\". وهو برنامج مجاني، ومصدر مفتوح لتحليل البيانات.  يعتمد برنامج جاسب على واجهة مستخدم ويوفِّر كلًا من اختبارات الفرضيَّة الصِّفريَّة، وكذلك نظيرتها من الاختبارات البيزية. يدعم برنامج جاسب إمكانيَّة إعادة الإنتاج الإحصائي من خلال حفظ البيانات، والتَّعليمات البرمجيَّة، والتَّحليلات، والنَّتائج في ملف واحد.  **المصطلحات ذات الصِّلة:** جاموفي، المصدر المفتوح.",
                "Related_terms": "** Jamovi; Open source"
            },
            {
                "Title": "Journal Impact Factor™ (معامل تأثير المجَّلة) *",
                "Definition": "** The mean number of citations to research articles in a journal over the preceding two years. It is a proprietary and opaque calculation marketed by Clarivate**™**. Journal Impact Factors are not associated with the content quality or the peer review process.  **",
                "Reference(s)": "** Brembs et al (2013); Curry (2012); Naudet et al. (2018); Rossner et al. (2008); Sharma et al. (2014)",
                "Drafted by": "** Jacob Miranda",
                "Reviewed (or Edited) by": "** Tsvetomira Dumbalska; Adam Parker",
                "Translated by": "** Ali H. Al-Hoorie",
                "Translation reviewed by": "** Ahlam Ahmed, Mai Helmy, Alaa M. Saleh, Asma Alzahrani, Mohammed Mohsen",
                "Translation": "التَّعريف:** متوسط عدد الاستشهادات للمقالات البحثيَّة في مجلة خلال العامين السَّابقين، وهو عبارة عن حساب خاص ومبهم تسوقه شركة كلاريفات. لا ترتبط عوامل تأثير المجلَّة بجودة المحتوى، أو بعمليَّة تحكيم الأقران.  **المصطلحات ذات العلاقة:** إعلان سان فرانسيسكو بشأن تقييم البحوث، مؤشر إتش.",
                "Related_terms": "** DORA; H-index"
            },
            {
                "Title": "JSON file (ملفات جي سون ) *",
                "Definition": "** JavaScript Object Notation (JSON) is a data format for structured data that can be used to represent attribute-value pairs. Values thereby can contain further JSON notation (i.e., nested information). JSON files can be formally encoded as strings of text and thus are human-readable. Beyond storing information this feature makes them suitable for annotating other content. For example, JSON files are used in Brain Imaging Data Structure (BIDS) for describing the metadata dataset by following a standardized format (dataset\\_description.json).  **",
                "Reference(s)": "** https://bids-specification.readthedocs.io/en/stable/03-modality-agnostic-files.html",
                "Drafted by": "** Tina Lonsdorf",
                "Reviewed (or Edited) by": "** Alexander Hart; Matt Jaquiery; Emma Norris; Charlotte R. Pennington",
                "Translated by": "** Asma Alzahrani",
                "Translation reviewed by": "** Ali H. Al-Hoorie, Ahlam Ahmed, Hiba Alomary, Mohammed Mohsen  ###  ### **K** {#k}",
                "Translation": "التَّعريف:** تدوين كائن جافا النصّي هو تنسيق للبيانات المنظَّمة التي يمكن استخدامها لتمثيل أزواج القيم والسِّمات. وبالتَّالي يمكن أن تحتوي القيم على مزيد من تدوينات جافا النَّصيَّة (مثل المعلومات المتداخلة). يمكن تشفير هذه الملفات رسميًا كسلاسل نصيَّة، وبالتَّالي يمكن للإنسان قراءتها. بالإضافة إلى تخزين المعلومات، فإن هذه الميزة تجعلها مناسبة للتَّعليق على محتوى آخر. فمثلًا تُستخدم هذه الملفات في بنية بيانات تصوير الدِّماغ لوصف مجموعة البيانات الوصفيَّة باتباع تنسيق موحَّد. (dataset\\_description.json).  **المصطلحات ذات الصِّلة:** بنية بيانات تصوير الدّماغ، البيانات الوصفيَّة.",
                "Related_terms": "** BIDS data structure; Metadata"
            },
            {
                "Title": "Knowledge acquisition (اكتساب المعرفة) *",
                "Definition": "** The process by which the mind decodes or extracts, stores, and relates new information to existing information in long term memory. Given the complex structure and nature of knowledge, this process is studied in the philosophical field of epistemology, as well as the psychological field of learning and memory.  **",
                "Reference(s)": "** Brule and Blount (1989)",
                "Drafted by": "** Oscar Lecuona",
                "Reviewed (or Edited) by": "** Bradley Baker; Helena Hartmann; Kai Krautter; Graham Reid",
                "Translated by": "** Sarah Almutairi",
                "Translation reviewed by": "** Awatif Alruwaili, Ali H. Al-Hoorie, Ahlam Ahmed, Alaa M. Saleh, Mohammed Mohsen  ###  ### **L** {#l}",
                "Translation": "التَّعريف:** هي عملية ذهنيَّة يتم من خلالها تفسير، أو استخلاص وتخزين وربط المعلومات الجديدة بالمعلومات المخزَّنة في الذَّاكرة طويلة المدى. ونظرًا لصعوبة وتعقيد المعرفة، وتركيبها فإنَّ هذه العلميَّة الذِّهنيَّة تتم دراستها في المجال الفلسفي لنظرية المعرفة، وكذلك المجال النَّفسي للتعلُّم و الذَّاكرة.  **المصطلحات ذات الصِّلة:** نظرية المعرفة، المعلومات، التَّعلمُّ",
                "Related_terms": "** Epistemology; Information; Learning"
            },
            {
                "Title": "Likelihood function (دالة الاحتماليَّة) *",
                "Definition": "** A statistical model of the data used in frequentist and Bayesian analyses, defined up to a constant of proportionality. A likelihood function represents the likeliness of different parameters for your distribution given the data. Given that probability distributions have unknown population parameters, the likelihood function indicates how well the sample data summarise these parameters. As such, the likelihood function gives an idea of the goodness of fit of a model to the sample data for a given set of values of the unknown population parameters. **Alternative definition:** For a more statistically-informed definition, given a parametric model specified by a probability (density) function f(x|theta), a likelihood *for* a statistical model is defined by the same formula as the density except that the roles of the data *x* and the parameter *theta* are interchanged, and thus the likelihood can be considered a function of *theta* for fixed data *x*. Here, then, the likelihood function would describe a curve or hypersurface whose peak, if it exists, represents the combination of model parameter values that maximize the probability of drawing the sample obtained.  **",
                "Reference(s)": "** Dienes (2008); Hogg et al. (2010); van de Schoot et al. (2021); Geyer (2003); Geyer (2007); https://blog.stata.com/2016/11/01/introduction-to-bayesian-statistics-part-1-the-basic-concepts/",
                "Drafted by": "** Alaa AlDoh",
                "Reviewed (or Edited) by": "** Dominik Kiersz; Graham Reid; Sam Parsons; Flávio Azevedo",
                "Translated by": "** Ahlam Ahmed",
                "Translation reviewed by": "** Ali H. Al-Hoorie, Hiba Alomary, Asma Alzahrani, Mohammed Mohsen",
                "Translation": "التَّعريف**: نموذج إحصائي للبيانات المستخدمة في التَّحليلات التِّكراريَّة والبايزية محدد حتى معامِل ثابت التَّناسب.  تمثل دالة الاحتماليَّة مدى احتمال المؤشرات  المختلفة للتَّوزيع في ضوء البيانات. بالنَّظر إلى أنَّ التَّوزيعات الاحتماليَّة لها مؤشرات غير معروفة تتعلَّق بمجتمع الدِّراسة. تشير دالة الاحتماليَّة إلى جودة بيانات العيِّنة في تلخيص هذه المؤشرات. وعلى هذا النَّحو، تعطي دالة الاحتماليَّة فكرة عن جودة ملاءمة نموذج بيانات العيِّنة لمجموعة معيَّنة من القيم لمؤشرات المجتمع غير المعروفة. **تعريف بديل:** من النَّاحية الإحصائيَّة، بالنَّظر إلى نموذج حدودي محدَّد بواسطة دالة الاحتماليَّة، أو الكثافة (س | ثيتا)، يتم تعريف احتماليَّة النّموذج الإحصائي بنفس صيغة الكثافة باستثناء أن أدوار البيانات ***س*** والمؤشر ***ثيتا*** متبادلة، وبالتَّالي يمكن اعتبار الاحتماليَّة دالة ثيتا للبيانات الثابتة س. في هذه الحالة ستصف دالة الاحتمال منحنى أو سطحا فائقا تمثل ذروته، إن وجدت، مزيجًا من قيم معلمات النّموذج التي تزيد من احتماليَّة رسم العيِّنة التي تم الحصول عليها. **المصطلحات ذات الصِّلة**: معامل بايز، الاستدلال البايزي، تقدير المعاملات باستخدام النهج البايزي، التَّوزيع اللَّاحق، التَّوزيع المسبق.",
                "Related_terms": "** Bayes factor; Bayesian inference; Bayesian parameter estimation; Posterior distribution; Prior distribution"
            },
            {
                "Title": "Likelihood Principle (مبدأ الاحتماليَّة) *",
                "Definition": "** The notion that all information relevant to inference contained in data is provided by the likelihood. The principle suggests that the likelihood function can be used to compare the plausibility of various parameter values. While Bayesians and likelihood theorists subscribe to the likelihood principle, Neyman-Pearson theorists do not, as significance tests violate the likelihood principle because they take into account information not in the likelihood.  **",
                "Reference(s)": "** Dienes (2008); Geyer (2003; 2007);",
                "Drafted by": "** Alaa Aldoh",
                "Reviewed (or Edited) by": "** Sam Parsons; Flávio Azevedo",
                "Translated by": "** Ali H. Al-Hoorie",
                "Translation reviewed by": "** Ahlam Ahmed, Hiba Alomary, Ali H. Al-Hoorie, Asma Alzahrani, Mohammed Mohsen",
                "Translation": "التَّعريف:** فكرة أن جميع المعلومات ذات الصِّلة بالاستدلال الواردة في البيانات يتم توفيرها من خلال الاحتماليَّة. يشير المبدأ إلى أنَّه يمكن استخدام دالة الاحتماليَّة لمقارنة مدى معقوليَّة قيم المعلمات المختلفة. في حين يؤيد البايزيون ومنظرو الاحتماليَّة مبدأَ الاحتماليَّة، فإن منظري نيمان-بيرسون لا يؤيدونه؛ لأن الاختبارات الدّلالية تنتهك مبدأ الاحتماليّة لأنَّهم يأخذون في الاعتبار معلومات ليست في الاحتمال نفسه.  **المصطلحات ذات الصلة:** الاستدلال البايزي؛ دالة الاحتماليَّة",
                "Related_terms": "** Bayesian inference; Likelihood Function"
            },
            {
                "Title": "Literature Review (مراجعة الأدبيَّات) *",
                "Definition": "** Researchers often review research records on a given topic to better understand effects and phenomena of interest before embarking on a new research project, to understand how theory links to evidence or to investigate common themes and directions of existing study results and claims. Different types of reviews can be conducted depending on the research question and literature scope. To determine the scope and key concepts in a given field, researchers may want to conduct a scoping literature review. Systematic reviews aim to access and review all available records for the most accurate and unbiased representation of existing literature. Non-systematic or focused literature reviews synthesise information from a selection of studies relevant to the research question although they are uncommon due to susceptibility to biases (e.g. researcher bias; Siddaway et al., 2019).  **",
                "Reference(s)": "** Huelin et al., (2015); Munn et al., (2018); Pautasso (2013); Siddaway et al. (2019)",
                "Drafted by": "** Marta Topor",
                "Reviewed (or Edited) by": "** Jamie P. Cockcroft; Mahmoud Elsherif; Helena Hartmann; Flávio Azevedo; Meng Liu; Charlotte R. Pennington",
                "Translated by": "** Ali H. Al-Hoorie",
                "Translation reviewed by": "** Ahlam Ahmed, Alaa M. Saleh, Hiba Alomary, Asma Alzahrani, Mohammed Mohsen   ### **Looking for something else?**  We have separated the glossary project across several documents, see links below:  Landing page:\t\t\t[Glossary | Phase 2 | Landing page](https://docs.google.com/document/d/1BKzztg7srUeC_2Yn0b7cMbxp_vYMDlOnEYpxg_S2hWs/edit?usp=sharing) Letters A \\- F:\t\t\t[Glossary | Phase 2 | A-F](https://docs.google.com/document/d/1ob__Alxsnx9yqeTpurEY_N_kv56c1iiZyrg0xxf0ftg/edit?usp=sharing) Letters G \\- L:\t\t\t[Glossary | Phase 2 | G-L](https://docs.google.com/document/d/1LP0cEletpNumDmYHFv3seV3gL6OLQHJ7ZDLQ7T3fzNE/edit?usp=sharing) Letters M \\- R: \t\t\t[Glossary | Phase 2 | M - R](https://docs.google.com/document/d/1FVIgUx717G3pBwI17LGS7y22nKMde-zg0I6AKgz4SeQ/edit?usp=sharing) Letters S \\- Z:\t\t\t[Glossary | Phase 2 | S - Z](https://docs.google.com/document/d/1-5CKFciwhB-WfC1DK8Sajyh8a8CgNOYvOoeZpODux8Y/edit?usp=sharing) References: \t\t\t[Glossary | Phase 2 | References](https://docs.google.com/document/d/12_F8kbnl2GP66iBkIejJeX2KnkZ13iC_jj7VVYZMxpA/edit?usp=sharing) Terms not yet defined: \t\t[Glossary | Phase 2 | Terms not yet defined](https://docs.google.com/document/d/16FGodUkNoNrBYyq2JqHJMNzYuZf-QbsigooMpB_ns_E/edit?usp=sharing) Contributors spreadsheet: \t[Glossary Phase 2 tenzing contributions \\[FORRT\\]](https://docs.google.com/spreadsheets/d/1pkvQ2-h_Hr_ZnlfmKYkuJTuYrxlqS2oSqKqoEDYlKlk/edit?usp=sharing)  [FORRT slack](https://join.slack.com/t/forrt/shared_invite/zt-alobr3z7-NOR0mTBfD1vKXn9qlOKqaQ) \\- join us\\! [FORRT email](mailto:FORRTproject@gmail.com) \\- contact us\\!",
                "Translation": "التَّعريف:** غالبًا ما يقوم الباحثون بمراجعة سجلات البحث عن موضوع معين؛ لفهم التَّأثيرات، والظَّواهر محل الاهتمام بشكل أفضل قبل االبدء في مشروع بحثي جديد؛ لفهم كيفية ارتباط النَّظرية بالأدلَّة، أو للتَّحقيق في الموضوعات والاتِّجاهات المشتركة لنتائج وادعاءات الدِّراسة الحالية. يمكن إجراء أنواع مختلفة من المراجعات اعتمادًا على سؤال البحث ونطاق الأدبيَّات.  يمكن للباحثين في حقل معرفي ما أن يقوموا بإجراء مراجعة أدبية استطلاعية لغرض تحديد النِّطاق، والمفاهيم الأساسيَّة في حقلهم المعرفي. تهدف المراجعات المنهجيَّة إلى الوصول إلى جميع السجلَّات المتاحة، ومراجعتها للحصول على تمثيل أكثر دقّة وغير متحيِّز للأدبيَّات الموجودة، بينما تهدف المراجعات غير المنهجيَّة، أو المركّزة على تجميع المعلومات من مجموعة مختارة من الدِّراسات ذات الصِّلة بسؤال البحث وإن كانت غير شائعة بسبب القابليَّة للتَّحيُّزات (مثل تحيُّز الباحث؛ Siddaway et al., 2019). **المصطلحات ذات الصِّلة:** توليف الأدلَّة، العلوم البعدية/التلوية أو البحث البعديَّ/التلوي، المراجعات السَّرديَّة، المراجعة المنهجيَّة.",
                "Related_terms": "** Evidence synthesis; Meta-science or Meta-research; Narrative reviews; Systematic reviews"
            },
            {
                "Title": "Abstract Bias (انحياز المستخلص) *",
                "Definition": "** The tendency to report only significant results in the abstract, while reporting non-significant results within the main body of the manuscript (not reporting non-significant results altogether would constitute selective reporting). The consequence of abstract bias is that studies reporting non-significant results may not be captured with standard meta-analytic search procedures (which rely on information in the title, abstract and keywords) and thus biasing the results of meta-analyses. **",
                "Reference": "** Duyx et al. (2019)",
                "Originally drafted by": "** Ali H. Al-Hoorie",
                "Reviewed (or Edited) by": "Mahmoud Elsherif; Bethan Iley; Sam Parsons; Gerald Vineyard; Eliza Woodward; Flávio Azevedo",
                "Translated by": "Ali H. Al-Hoorie",
                "Translation reviewed by": "Amani Aloufi, Ahlam Ahmed, Hiba Alomary, Asma alzahrani, Hala Alghamdi, Mohammed Mohsen",
                "Translation": "التَّعريف**: الميل نحو الإشارة إلى النَّتائج ذات الدِّلالة الإحصائيَّة في المستخلص فقط، والاكتفاء بالنَّتائج التي لا تصل بالدِّلالة الإحصائيَّة إلى داخل النَّص الرَّئيسيّ. للبحث (حيث إنَّ عدم الإبلاغ عن هذه النَّتائج يعدُّ تقريرًا انتقائيًا). يؤدي هذا التَّحيُّز إلى عدم اكتشاف الدِّراسات التي تحوي نتائج لا تصل للدِّلالة الإحصائيَّة باستخدام إجراءات الأبحاث البعديَّة المعتادة (التي تعتمد على المعلومات الموجودة في العنوان والملخَّص والكلمات الرَّئيسيَّة)، وبالتَّالي إلى التَّحيُّز في نتائج التَّحليلات البعديَّة. **المصطلحات ذات الصِّلة:** قطف الكرز (الانتقائيّة)، تحيُّز النشر (مشكلة درج الملفَّات)، التقرير الانتقائي.",
                "Related_terms": "** Cherry-picking; Publication bias (File Drawer Problem); Selective reporting"
            },
            {
                "Title": "Academic Impact (التَّأثير الأكاديميّ)",
                "Definition": "** The contribution that a research output (e.g., published manuscript) makes in shifting understanding and advancing scientific theory, method, and application, across and within disciplines. Impact can also refer to the degree to which an output or research programme influences change outside of academia, e.g. societal and economic impact (cf. ESRC: [https://esrc.ukri.org/research/impact-toolkit/what-is-impact/](https://esrc.ukri.org/research/impact-toolkit/what-is-impact/)). **",
                "Reference(s)": "** Anon (2021)",
                "Drafted by": "** Connor Keating",
                "Reviewed (or Edited) by": "** Myriam A. Baum; Adam Parker; Charlotte R. Pennington; Suzanne L. K. Stewart; Flávio Azevedo",
                "Translated by": "Abdulsamad Humaidan",
                "Translation reviewed by": "Amani Aloufi, Ahlam Ahmed, Hiba Alomary, Asma Alzahrani, Hala Alghamdi, Mohammed Mohsen",
                "Translation": "التَّعريف**: يشير هذا المصطلح إلى الإسهام الذي يحدثه النَّتاج البحثي، مثل: مقالة منشورة في التحوُّل المفاهيمي، وتطوير النَّظريات، والأساليب، والتَّطبيقات العمليَّة، سواء داخل التَّخصص  المعرفي الواحد، أو عبر التَّخصُّصات  المعرفيَّة المختلفة. ويدلُّ التَّأثير الأكاديمي كذلك على درجة التَّغيير الذي يحدثه أي نتاج، أو برنامج بحثي خارج المجال الأكاديمي، مثل التَّأثير الاجتماعي والاقتصادي. (راجع: [https://esrc.ukri.org/research/impact-toolkit/what-is-impact/](https://esrc.ukri.org/research/impact-toolkit/what-is-impact/)). **المصطلحات ذات الصّلة:** المستفيدين، **إعلان سان فرانسيسكو بشأن تقييم البحوث** ; REF",
                "Related_terms": "** Beneficiaries; DORA; Reach; REF"
            },
            {
                "Title": "Accessibility (إمكانيَّة الوصول)",
                "Definition": "** Accessibility refers to the ease of access and re-use of materials (e.g., data, code, outputs, publications) for academic purposes, particularly the ease of access is afforded to people with a chronic illness, disability and/or neurodivergence. These groups face numerous financial, legal and/or technical barriers within research, including (but not limited to) the acquisition of appropriately formatted materials and physical access to spaces. Accessibility also encompasses structural concerns about diversity, equity, inclusion, and representation (Pownall et al., 2021). Interfaces, events and spaces should be designed with accessibility in mind to ensure full participation, such as by ensuring that web-based images are colorblind friendly and have alternative text, or by using live captions at events (Brown et al., 2018; Pollet & Bond, 2021; World Wide Web Consortium, 2021). **",
                "Reference(s)": "** Brown et al. (2018); Pollet and Bond (2021); Pownall et al. (2021); Suber (2004); World Wide Web Consortium (2021)",
                "Drafted by": "** Kai Krautter",
                "Reviewed (or Edited) by": "** Valeria Agostini; Myriam A. Baum; Mahmoud Elsherif; Bethan Iley; Tamara Kalandadze; Ryan Millager; Sara Middleton; Charlotte R. Pennington;  Madeleine Pownall; Robert M. Ross; Flávio Azevedo",
                "Translated by": "Hala Alghamdi",
                "Translation reviewed by": "** Amani Aloufi, Ahlam Ahmed, Hiba Alomary, Asma Alzahrani, Mohammed Mohsen",
                "Translation": "الَّتعريف:**  يُعرّف هذا المصطلح بسهولة الوصول، أو إمكانيَّة إعادة استخدام المواد، كالبيانات، والتَّعليمات البرمجيَّة، والمطبوعات، والمنشورات لأغراض أكاديمية، وتحديدًا للأشخاص ذوي الأمراض المزمنة، و الإعاقات، أو الاضطرابات العصبيَّة. تواجه هذه المجموعات في عملية البحث العديد من العوائق الماديَّة، والقانونيَّة، والتقنية، فمثلًا، هناك معوّق الحصول على مواد ملائمة لحالتهم، وكذلك معوّق الدخول البدني إلى المرافق. يشمل مصطلح إمكانية الوصول أيضًا: اعتبارات هيكليَّة تتعلَّق بالتَّنوع، والمساواة، والشُّمولَّية والَّتمثيل  (Pownall et al., 2021\\). عند تجهيز الواجهات، والمناسبات، والمرافق يجب أخذ إمكانية الوصول بعين الاعتبار؛ لضمان المشاركة الكاملة، فمثلًا يجب التأكد من أنَّ الصُّور الإلكترونيَّة موائمة لذوي عمى الألوان، وأن تحتوي هذه الصورعلى نص بديل يوضح محتوى الصورة، أو استخدام السطرجة الفورية، أو التَّرجمة الحيَّة في المناسبات. (Brown et al., 2018; Pollet & Bond, 2021; World Wide Web Consortium, 2021). **المصطلحات ذات الصِّلة:** التَّوفر، بيان توفر البيانات، الشُّمول، الوصول المفتوح، نقص التَّمثيل، التصميم الشامل للتَّعليم.",
                "Related_terms": "** Availability; Data availability statements; Inclusion; Open Access; Under-representation; Universal design for learning (UDL)"
            },
            {
                "Title": "Ad hominem bias (تحيَّز الشَّخصنة)",
                "Definition": "** From Latin meaning “to the person”; Judgment of an argument or piece of work influenced by the characteristics of the person who forwarded it, not the characteristics of the argument itself. Ad hominem bias can be negative, as when work from a competitor or target of personal animosity is viewed more critically than the quality of the work merits, or positive, as when work from a friend benefits from overly favorable evaluation.  **",
                "Reference(s)": "** Barnes et al. (2018); Tvina et al. (2019)",
                "Drafted by": "** Mahmoud Elsherif",
                "Reviewed (or Edited) by": "** Bradley Baker; Filip Dechterenko; Bethan Iley; Madeleine Ingham; Graham Reid",
                "Translated by": "Ali H. Al-Hoorie",
                "Translation reviewed by": "Amani Aloufi, Ahlam Ahmed, Hiba Alomary, Hala Alghamdi, Asma Alzahrani, Mohammed Mohsen",
                "Translation": "التَّعريف:** مأخوذ من المعنى اللاتيني \"إلى الشَّخص\"، وهو حينما يتأثر الحكم على فكرة، أو  عمل بخصائص الشَّخص الذي قام به، وليس بخصائص العمل نفسه. يمكن أن يكون هذا التَّحيز سلبيًا، كما هو الحال عندما يقوم منافس، أو شخص له عداء شخصي بتوجيه انتقاد أقسى مما تستوجبه جودة العمل، وقد يكون إيجابيًا، كما هو الحال عندما يحصل العمل من صديق على تقييم إيجابي للغاية.  **المصطلحات ذات الصلة:** تحكيم الأقران.",
                "Related_terms": "** Peer review"
            },
            {
                "Title": "Adversarial collaboration (التَّشارك العدائي)*",
                "Definition": "** A collaboration where two or more researchers with opposing or contradictory theoretical views —and likely diverging predictions about study results— work together on one project. The aim is to minimise biases and methodological weaknesses as well as to establish a shared base of facts for which competing theories must account. **",
                "Reference(s)": "** Bateman et al. (2005); Cowan et al. (2020); Kerr et al. (2018); Mellers et al. (2001); Rakow et al. (2014)",
                "Drafted by": "** Siu Kit Yeun",
                "Reviewed (or Edited) by": "** Matt Jaquiery; Aoife O’Mahony; Charlotte R. Pennington; Flávio Azevedo; Madeleine Pownall**;** Martin Vasilev",
                "Translated by": "Ali H. Al-Hoorie",
                "Translation reviewed by": "** Amani Aloufi, Ahlam Ahmed, Hiba Alomary, Hala Alghamdi, Mohammed Mohsen",
                "Translation": "التَّعريف:** أن يشترك في مشروع واحد باحثان أو أكثر من مؤيدي نظريات مختلفة أو متناقضة، والذي يؤدي إلى تنبؤات متباينة محتملة حول نتائج الدِّراسة؛ والهدف من ذلك هو تقليل التَّحيزات، ونقاط الضعف المنهجيَّة إلى الحد الأدنى، وكذلك إنشاء قاعدة مشتركة من الحقائق التي يجب أن تأخذها النَّظريات المنافسة في الاعتبار.  **المصطلحات ذات الصلة:** التَّعاون، العديد من المحللين، المعامل المتعددة، التسجيل المسبق، تحيز النَّشر (مشكلة درج الملفات)",
                "Related_terms": "** Collaboration; Many Analysts; Many Labs; Preregistration; Publication bias (File Drawer Problem)"
            },
            {
                "Title": "Adversarial (collaborative) commentary (التَّعليق (التَّعاوني) العدائي)",
                "Definition": "** A commentary in which the original authors of a work and critics of said work collaborate to draft a consensus statement. The aim is to draft a commentary that is free of ad hominem attacks and communicates a common understanding or at least identifies where both parties agree and disagree. In doing so, it provides a clear take-home message and path forward, rather than leaving the reader to decide between opposing views conveyed in separate commentaries.  **",
                "Reference(s)": "** Heyman et al. (2020); Rabagliati et al. (2019); Silberzahn et al. (2014)",
                "Drafted by": "** Steven Verheyen",
                "Reviewed (or Edited) by": "** Mahmoud Elsherif; Sam Parsons; Emma Henderson; Michele C. Lim; Flávio Azevedo",
                "Translated by": "Ali H. Al-Hoorie",
                "Translation reviewed by": "Amani Aloufi, Ahlam Ahmed, Hiba Alomary, Asma Alzahran, Mohammed Mohsen",
                "Translation": "الَّتعريف:** تعليق يتعاون فيه المؤلفون الأصليون للعمل مع نُقّاده؛ لصياغة بيان تتوافق فيه الآراء. ويهدف الَّتعليق التَّعاوني إلى صياغة تعليق خالٍ من تحيُّز الشَّخصنة، وينقل فهمًا مشتركًا، أو على الأقل يُحدد أين يتفق الطَّرفان، وأين يختلفان. تقدّم هذه الطَّريقة رسالة واضحة ومسارًا إلى الأمام، بدلًا من ترك القارئ يختار بين وجهات النَّظر المتعارضة التي يتم نقلها في تعليقات منفصلة.  **المصطلحات ذات الصِّلة:** التَّشارك العدائي، التَّعليق التَّعاوني.",
                "Related_terms": "** Adversarial collaboration; Collaborative commentary"
            },
            {
                "Title": "Affiliation bias (تحيُّز الانتماء) *",
                "Definition": "** This bias occurs when one’s opinions or judgements about the quality of research are influenced by the affiliation of the author(s). When publishing manuscripts, a potential example of an affiliation bias could be when editors prefer to publish work from prestigious institutions. **",
                "Reference(s)": "** Tvina et al. (2019)",
                "Drafted by": "** Mahmoud Elsherif",
                "Reviewed (or Edited) by": "** Christopher Graham; Madeleine Ingham; Adam Parker; Graham Reid",
                "Translated by": "** Abdulsamad Humaidan",
                "Translation reviewed by": "** Amani Aloufi, Ali H. Al-Hoorie, Ahlam Ahmed, Hiba Alomary, Hala Alghamdi",
                "Translation": "التَّعريف**: يحدث هذا النُّوع من التَّحيُّز عندما يتأثر حكم أو وجهة نظر الفرد حول جودة البحوث بالمؤسسة التي ينتمي إليها الباحث. ومثال هذا النُّوع من التَّحيُّز هو عندما يفضل محررو المجلات العلميَّة نشر بحوث قادمة من مؤسسات مرموقة. **المصطلحات ذات الصِّلة:** تحكيم الأقران.",
                "Related_terms": "** Peer review"
            },
            {
                "Title": "Aleatoric uncertainty (الاحتمال العشوائي)*",
                "Definition": "** Variability in outcomes due to unknowable or inherently random factors. The stochastic component of outcome uncertainty that cannot be reduced through additional sources of information. For example, when flipping a coin, there is uncertainty about whether it will land on heads or tails.  **",
                "Reference(s)": "** Der Kiureghian and Ditlevsen (2009)",
                "Drafted by": "** Bradley Baker",
                "Reviewed (or Edited) by": "** Nihan Albayrak-Aydemir**;** Brett Gall; Magdalena Grose-Hodge; Bethan Iley; Charlotte R. Pennington",
                "Translated by": "** Ali H. Al-Hoorie",
                "Translation reviewed by": "** Amani Aloufi, Ahlam Ahmed، Asma Alzahrani, Hiba Alomary, Hala Alghamdi, Mohammed Mohsen",
                "Translation": "التَّعريف:** التَّباين في النَّتائج بسبب عوامل غير معروفة، أو عشوائيَّة بطبيعتها، وهذه العشوائيَّة في النَّتائج لا يمكن تقليصها من خلال مصادر إضافيَّة للمعلومات. فعلى سبيل المثال: عند قلب عملة معدنيَّة، سيكون هناك احتمال عشوائي بشأن ما إذا كانت العملة ستستقر على الصُّورة، أم الكتابة.  **مصطلحات ذات صلة:** اللايقين المعرفي; عدم اليقين النايتي (نسبة إلى العالم فرانك نايت)",
                "Related_terms": "** Epistemic uncertainty; Knightian uncertainty"
            },
            {
                "Title": "Altmetrics (المقاييس البديلة)",
                "Definition": "** Departing from traditional citation measures, altmetrics (short for “alternative metrics”) provide an assessment of the attention and broader impact of research work based on diverse sources such as social media (e.g. X), digital news media, number of preprint downloads, etc. Altmetrics have been criticized in that sensational claims usually receive more attention than serious research (Ali, 2021).  **",
                "Reference(s)": "** Ali (2021); Galligan and Dyas-Correia (2013)",
                "Originally drafted by": "** Mirela Zaneva",
                "Reviewed (or Edited) by": "** Ali H. Al-Hoorie; Charlotte R. Pennington; Birgit Schmidt; Flávio Azevedo",
                "Translated by": "** Ali H. Al-Hoorie",
                "Translation reviewed by": "** Amani Aloufi, Asma Alzahrani, Hiba Alomary, Hala Alghamdi, Mohammed Mohsen  ### ---",
                "Translation": "التَّعريف:** بالابتعاد عن مقاييس الاستشهاد التَّقليديَّة، توفر المقاييس البديلة تقييمًا للاهتمام والتَّأثير الأوسع للعمل البحثي استنادًا إلى مصادر متنوعة، مثل: وسائل التَّواصل الاجتماعي (مثل X)، ووسائل الإعلام الإخباريَّة الرَّقميَّة، وعدد تنزيلات مقالات الطِّباعة الأولية، وما إلى ذلك.  أحد أوجه النَّقد لهذه المقاييس البديلة  هو أنَّ الإدِّعاءات المثيرة عادة ما تحظى باهتمام أكبر من البحث الجاد (Ali, 2021).  **المصطلحات ذات الصِّلة:** التَّأثير الأكاديميّ، المقاييس البديلة، الببليومتريّات، مؤشر إتش، معامل تأثير المجلَّة.",
                "Related_terms": "** Academic impact; Alternative metrics; Bibliometrics; H-index; Impact assessment; Journal impact factor"
            },
            {
                "Title": "AMNESIA (أمنيجا)",
                "Definition": "** AMNESIA is a free anonymization tool to remove identifying information from data. After uploading a dataset that contains personal data, the original dataset is transformed by the tool, resulting in a dataset that is anonymized regarding personal and sensitive data.  **",
                "Reference(s)": "** [https://amnesia.openaire.eu/](https://amnesia.openaire.eu/)",
                "Originally drafted by": "** Norbert Vanek",
                "Reviewed (or Edited) by": "** Ali H. Al-Hoorie; Myriam A. Baum; Charlotte R. Pennington",
                "Translated by": "Hala Alghamdi",
                "Translation reviewed by": "Amani Aloufi, Ali H. Al-Hoorie, Asma Alzahrani, Hiba Alomary, Hala Alghamdi, Mohammed Mohsen",
                "Translation": "التَّعريف**: أمنيجا (Amnesia) هي أداة مجانيَّة لإخفاء الهويَّة، تقوم بإزالة المعلومات التَّعريفيَّة من البيانات، بعد تحميل مجموعة البيانات التي تحتوي على معلومات شخصيَّة، يتم تحويلها عن طريق الأداة فينتج عن ذلك بيانات خالية من المعلومات الشَّخصيَّة والحساسَّة.  **المصطلحات ذات الصِّلة:** التّعمية، السِّريَّة، أخلاقيَّات البحث.",
                "Related_terms": "** Anonymity; Confidentiality; Research ethics"
            },
            {
                "Title": "Analytic Flexibility (المرونة التَّحليليَّة)",
                "Definition": "** Analytic flexibility is a type of researcher degrees of freedom (Simmons, Nelson, & Simonsohn, 2011\\) that refers specifically to the large number of choices made during data preprocessing and statistical analysis. “\\[T\\]he range of analysis outcomes across different acceptable analysis methods” (Carp, 2012, p. 1). Analytic flexibility can be problematic, as this variability in analytic strategies can translate into variability in research outcomes, particularly when several strategies are applied, but not transparently reported (Masur, 2021).  **",
                "Reference(s)": "** Breznau et al. (2021); Carp (2012); Jones et al. (2020); Masur (2021); Simmons et al. (2011)",
                "Originally drafted by": "** Mariella Paul",
                "Reviewed (or Edited) by": "** Adrien Fillon; Bettina M. J . Kern; Adam Parker; Charlotte R. Pennington; Flávio Azevedo",
                "Translated by": "** Naif Masrahi",
                "Translation reviewed by": "** Ahlam Ahmed, Ali H. Al-Hoorie, Asma Alzahrani, Hiba Alomary, Hala Alghamdi, Mohammed Mohsen",
                "Translation": "الَّتعريف**: نوع من درجات حريَّة الباحثين (Simmons, Nelson, & Simonsohn, 2011\\) التي تشير تحديدًا إلى العدد الكبير من الخيارات التي يتم إجراؤها أثناء المعالجة المسبقة للبيانات، والتَّحليل الإحصائيّ. وتعرف كذلك بأنها \"مجموعة من نتائج التَّحليل عبر طرق تحليل مختلفة، و مقبولة\" (Carp, 2012, p. 1). يمكن أن تكون المرونة التَّحليليَّة مشكلة، حيث يمكن أن يُترجم هذا التَّباين في الاستراتيجيات التَّحليليَّة إلى تباين في نتائج البحث تحديدًا عند تطبيق العديد من الاستراتيجيَّات دون الإفصاح عنها بشفافية (Masur, 2021).  **المصطلحات ذات الصِّلة:** حديقة المسارات المتشعِّبة،  تحليل الأكوان المتعدِّدة ، درجات حرية الباحث.",
                "Related_terms": "** Garden of forking paths; Multiverse analysis; Researcher degrees of freedom"
            },
            {
                "Title": "Anonymity (التّعمية)",
                "Definition": "** Anonymising data refers to removing, generalising, aggregating or distorting any information which may potentially identify participants, peer-reviewers, and authors, among others. Data should be anonymised so that participants are not personally identifiable. The most basic level of anonymisation is to replace participants’ names with pseudonyms (fake names) and remove references to specific places. Anonymity is particularly important for open data and data may not be made open for anonymity concerns. Anonymity and open data has been discussed within qualitative research which often focuses on personal experiences and opinions, and in quantitative research that includes participants from clinical populations.  **",
                "Reference": "** Braun and Clarke (2013)",
                "Originally drafted by": "** Claire Melia",
                "Reviewed (or Edited) by": "** Tsvetomira Dumbalska; Bethan Iley; Tamara Kalandadze; Bettina M.J. Kern; Sam Parsons; Charlotte R. Pennington; Flávio Azevedo; Madeleine Pownall; Birgit Schmidt",
                "Translated by": "** Amani Aloufi",
                "Translation reviewed by": "Ahlam Ahmed,  Ali H. Al-Hoorie, Asma Alzahrani, Hiba Alomary, Hala Alghamdi, Mohammed Mohsen",
                "Translation": "التَّعريف**: تشير عملية تعمية البيانات إلى مسح وحجب، أو تعميم، أو تغيير أي معلومات تعريفيَّة قد تشير إلى هوية المشاركين، والمحكِّمين، والمؤلِّفين وغيرهم إذ إنَّه يجب إخفاء البيانات الشَّخصيَّة لأي مشارك، حتى لا يتم التَّعرف على هويته.  تتمثل واحدة من أبسط الطُّرق في إخفاء هُوية المشاركين في تبديل أسمائهم بأسماء وهميَّة وحجب ما يشير إلى أي موقع جغرافي. تعد عملية التّعمية مهمَّة في مصادر البيانات المفتوحة، حيث إن بعض مصادر البيانات قد تبقى محجوبة؛ لأسباب تتعلق بالهُوية. ونوقش موضوع التّعمية ومصادر البيانات المفتوحة في البحوث الكيفيَّة التي تركز على التَّجارب والآراء الشَّخصيَّة، وأيضًا في البحوث الكميَّة التي تتضمَّن عينتها مشاركين في الدِّراسات السَّريريَّة. **المصطلحات ذات الصِّلة:** مجهول الهُوية، عيِّنة الدِّراسات السَّريريَّة، السِّريَّة ، أخلاقيات البحث، عيِّنة البحث، العيّنة الأكثر عُرضة للضَّرر.",
                "Related_terms": "** Anonymising; Clinical populations; Confidentiality; Research ethics; Research participants; Vulnerable population"
            },
            {
                "Title": "ARRIVE Guidelines ( إرشادات أرايف) *",
                "Definition": "** The ARRIVE guidelines (Animal Research: Reporting of In Vivo Experiments) are a checklist-based set of reporting guidelines developed to improve reporting standards, and enhance replicability, within living (i.e. *in vivo*) animal research. The second generation ARRIVE guidelines, ARRIVE 2.0, were released in 2020\\. In these new guidelines, the clarity has been improved, items have been prioritised and new information has been added with an accompanying “Explanation” and “Elaboration” document to provide a rationale for each item and a recommended set to add context to the study being described.  **",
                "Reference(s)": "** Percie du Sert et al. (2020)",
                "Drafted by": "** Ben Farrar",
                "Reviewed (or Edited) by": "** Jamie P. Cockcroft; Mahmoud Elsherif; Elias Garcia-Pelegrin; Helena Hartmann; Wanyin Li; Charlotte R. Pennington",
                "Translated by": "Ali H. Al-Hoorie",
                "Translation reviewed by": "** Amani Aloufi, Ahlam Ahmed, Asma Alzahrani, Hiba Alomary, Hala Alghamdi, Mohammed Mohsen",
                "Translation": "التَّعريف**: إرشادات أرايف (اختصار للبحوث الحيوانيَّة: الإبلاغ عن التَّجارب في الجسم الحي) هي مجموعة من المبادئ التَّوجيهيَّة لإعداد التقارير بناء على قائمة تحقق تم تطويرها؛ لتحسين معايير الإبلاغ، وتعزيز قابليَّة التِّكرار في بحوث الحيوانات الحية.  صدر الجيل الثَّاني من هذه الإرشادات في عام 2020\\. وفي هذه المبادئ التَّوجيهيَّة الجديدة، تم إيضاح هذه البنود أكثر، وأُعطي لكلٍ منها الأولويَّة، وأُضيفت معلومات جديدة في وثيقتي \"الشَّرح\" و \"التَّفصيل\" المصاحبتين لها؛ لتوفير الأساس المنطقي لكل بند، ومجموعة من التَّوصيات لإضفاء سياق على الدراسة التي يجري وصفها.  **المصطلحات ذات الصِّلة:** إرشادات الإعداد، دليل إعداد التّقارير، سترينج:الخلفية الاجتماعية والقابلية للتتبع والاختيار الذاتي وتارخ التربية والتأقلم والتعود.",
                "Related_terms": "** PREPARE Guidelines; Reporting Guideline; STRANGE"
            },
            {
                "Title": "Article Processing Charge (APC) (رسوم النَّشر) *",
                "Definition": "** An article (sometimes author) processing charge (APC) is a fee charged to authors by a publisher in exchange for publishing and hosting an open access article. APCs are often intended to compensate for a potential loss of revenue the journal may experience when moving from traditional publication models, such as subscription services or pay-per-view, to open access. While some journals charge only about US$300, APCs vary widely, from US$1000 (Advances in Methods and Practice in Psychological Science) or less to over US$10,000 (Nature). While some publishers offer waivers for researchers from certain regions of the world or who lack funds, some APCs have been criticized for being disproportionate compared to actual processing and hosting costs (Grossmann & Brembs, 2021\\) and for creating possible inequities with regard to which scientists can afford to make their works freely available (Smith et al., 2020).  **",
                "Reference": "** Grossmann and Brembs (2021); Smith et al. (2020)",
                "Originally drafted by": "** Nick Ballou",
                "Reviewed (or Edited) by": "** Ali H. Al-Hoorie; Bethan Iley; Flávio Azevedo; Robert Ross; Tobias Wingen",
                "Translated by": "** Mahdi Aben Ahmed",
                "Translation reviewed by": "** Amani Aloufi, Ahlam Ahmed, Ali H. Al-Hoorie, Hala Alghamdi, Mohammed Mohsen  \\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_",
                "Translation": "التَّعريف**: رسوم النَّشر هي رسوم يفرضها النَّاشر على المؤلفين مقابل نشر واستضافة مقال مفتوح الوصول. غالبًا ما تهدف رسوم النَّشر إلى التَّعويض عن الخسارة المحتملة في الإيرادات التي قد تتعرَّض لها المجلة عند الانتقال من نماذج النَّشر التَّقليديَّة، مثل خدمات الاشتراك، أو الدَّفع لكل عرض،  والوصول المفتوح أيضًا. في حين أنَّ بعض المجلات تتقاضى حوالي 300 دولار أمريكي فقط، فإنَّ رسوم النَّشر تتباين كثيرًا، من 1000 دولار أمريكي (مثل مجلة \"التَّقدم في الأساليب، والممارسات في العلوم النَّفسيَّة\") أو أقل، إلى أكثر من 10000 دولار أمريكي (مثل مجلة \"الطَّبيعة\"). في حين أنَّ بعض النَّاشرين يقدمون إعفاءات للباحثين من مناطق معيَّنة من العالم، أو الذين لا يملكون الدَّعم، فقد تم انتقاد بعض رسوم النشر كونها لا تتناسب مع المعالجة الفعلية للبحوث والاستضافة الفعليَّة (Grossmann & Brembs, 2021\\) ولكونهم قد يخلقون عدم مساواة  بين العلماء حيث أن البعض منهم  يمكنهم تحمل تكلفة إتاحة أعمالهم مجانًا (Smith et al., 2020).  **المصطلحات ذات الصِّلة:**  الوصول المفتوح، نقص التَّمثيل.",
                "Related_terms": "** Open Access; Under-representation"
            },
            {
                "Title": "Authorship (التَّأليف)",
                "Definition": "Authorship assigns credit for research outputs (e.g. manuscripts, data, and software) and accountability for content (McNutt et al. 2018; Patience et al. 2019). Conventions differ across disciplines, cultures, and even research groups, in their expectations of what efforts earn authorship, what the order of authorship signifies (if anything), how much accountability for the research the corresponding author assumes, and the extent to which authors are accountable for aspects of the work that they did not personally conduct. **",
                "Reference(s)": "** ALLEA (2017); German Research Foundation (2019); McNutt et al. (2018); Patience et al. (2019)",
                "Drafted by": "** Jacob Miranda",
                "Reviewed (or Edited) by": "** Bradley Baker; Brett J. Gall; Matt Jaquiery; Charlotte R. Pennington; Flávio Azevedo; Birgit Schmidt; Yuki Yamada",
                "Translated by": "** Asma Alzahrani",
                "Translation reviewed by": "Ali H. Al-Hoorie, Hala Alghamdi, Ahlam Ahmed, Mohammed Mohsen",
                "Translation": "التَّعريف**: التَّأليف هو نسبة المخرجات البحثيَّة (مثل المخطوطات، والبيانات، والبرامج) لشخص أو أكثر، وكذلك تحمّل مسؤوليَّة محتواها (McNutt et al., 2018; Patience et al., 2019). تختلف الأعراف بين التَّخصصات، والثَّقافات وحتى المجموعات البحثيَّة في توقعاتها بشأن ماهية الجهد الذي يستحق أن يندرج تحت \"التَّأليف\"، وما يعنيه ترتيب المؤلفين \\- إن وجد \\- وفي مدى المسؤوليَّة التي يتحملُّها المؤلِّف عن البحث ،وعن جوانب العمل التي لم يقم بتنفيذها شخصيًا. **المصطلحات ذات الصِّلة:** التَّأليف المشترك، التحالف التَّأليفي، الإسهام، تصنيف أدوار المؤلفين، مبدأ التأكيد على المؤلِّفين الأول والأخير، التَّأليف المُهدى (أو المؤلف الضيف)، نهج التسلسل الذي يحدد المساهمة.",
                "Related_terms": "** Co-authorship; Consortium authorship; Contributorship; CRediT; First-last-author-emphasis norm (FLAE); Gift (or Guest) Authorship; Sequence-determines-credit approach (SDC)"
            },
            {
                "Title": "Auxiliary Hypothesis (الفرضية المساعدة) *",
                "Definition": "** All theories contain assumptions about the nature of constructs and how they can be measured. However, not all predictions are derived from theories and assumptions can sometimes be drawn from other premises. Additional assumptions are made to deduce a prediction and tested by making links to observable data. These auxiliary hypotheses are sometimes invoked to explain why a replication attempt has failed.  **",
                "Reference(s)": "** Dienes (2008); Lakatos (1978)",
                "Drafted by": "** Alaa Aldoh",
                "Reviewed (or Edited) by": "** Ali H. Al-Hoorie; Nihan Albayrak-Aydemir; Mahmoud Elsherif; Bethan Iley; Sam Parsons; Flávio Azevedo",
                "Translated by": "Asma Alzahrani",
                "Translation reviewed by": "Ali H. Al-Hoorie, Hala Alghamdi, Ahlam Ahmed, Mohammed Mohsen   ### **B** {#b}",
                "Translation": "التعريف**: تحتوي جميع النظريات على افتراضات حول طبيعة المفاهيم وكيفية قياسها. ومع ذلك  لايتم اشتقاق جميع التنبؤات  من النظريات والافتراضات، فقد يتم استنتاج الافتراضات أحيانًا من مبادئ أخرى. يتم وضع افتراضات إضافية لاستنتاج التنبؤ ثم اختبارها عن طريق ربطها بالبيانات التي يمكن ملاحظتها. كما أنه يتم في بعض الأحيان تبرير سبب فشل محاولة تكرار النتائج بهذه الفرضيات المساعدة.  **المصطلحات ذات الصلة:** اللايقين المعرفي؛ الفرضية؛ الافتراضات الإحصائية؛ المتغيرات الوسيطة الخفية.",
                "Related_terms": "** Epistemic uncertainty; Hypothesis; Statistical assumptions; Hidden moderators"
            },
            {
                "Title": "Badges (Open Science) ((الشَّارات (العلم المفتوح) *",
                "Definition": "** Badges are symbols that editorial teams add to published manuscripts to acknowledge open science practices and act as incentives for researchers to share data, materials, or to embed study preregistration. As clearly-visible symbols, they are intended to signal to the reader that content has met the standard of open research required to receive the badge (typically from that journal). Different badges may be assigned for different practices, such as research having been made available and accessible in a persistent location (“open material badge” and “open data badge”), or study preregistration (“preregistration badge”).  **",
                "Reference(s)": "** Hardwicke et al. (2020); Kidwell et al. (2016); Rowhani-Farid et al. (2020); Science (n.d.)",
                "Drafted by": "** Jacob Miranda",
                "Reviewed (or Edited) by": "** Brett Gall; Helena Hartmann; Mariella Paul; Charlotte R. Pennington; Lisa Spitzer; Suzanne L. K. Stewart",
                "Translated by": "** Asma Alzahrani",
                "Translation reviewed by": "** Amani Aloufi, Hiba Alomary, Hala Alghamdi, Ali H. Al-Hoorie, Mohammed Mohsen",
                "Translation": "التعريف:** هي رموز تضيفها فرق التَّحرير للمخطوطات المنشورة؛ للإقرار بممارسات العلم المفتوح. وتكون بمثابة حوافز للباحثين؛ لمشاركة البيانات، والمواد، أو لتضمين التَّسجيل المسبق للدِّراسة، ولكونها رموزًا واضحة فإنَّ هذه الشَّارات تهدف إلى التَّوضيح للقارئ بأنَّ المحتوى قد استوفى معايير البحث المفتوح المطلوب لتلقي الشَّارة \\- المقدمة عادة من تلك المجلة- قد يتم تعيين شارات مختلِّفة لممارسات مختلِّفة، مثل إتاحة البحث وإمكانيَّة الوصول للمحتوى في موقع ثابت (\"شارة المحتوى المفتوح\" و \"شارة البيانات المفتوحة\") أو التَّسجيل المسبق للدِّراسة (شارة التَّسجيل المسبق).  **المصطلحات ذات الصِّلة**: حوافز، شارة البيانات المفتوحة، التَّسجيل المسبق، شارة ثلاثيَّة.",
                "Related_terms": "** Incentives; Open Data badge; Preregistration; Triple badge"
            },
            {
                "Title": "Bayes Factor (معامل بايز) *",
                "Definition": "** A continuous statistical measure for model selection used in Bayesian inference, describing the *relative* evidence for one model over another, regardless of whether the models are correct. Bayes factors (BF) range from 0 to infinity, indicating the relative strength of the evidence, and where 1 is a neutral point of no evidence. In contrast to *p*\\-values, Bayes factors allow for 3 types of conclusions: a) evidence for the alternative hypothesis, b) evidence for the null hypothesis, and c) no sufficient evidence for either. Thus, BF are typically expressed as BF10 for evidence regarding the alternative compared to the null hypothesis, and as BF01 for evidence regarding the null compared to the alternative hypothesis.  **",
                "Reference": "** Hoijtink et al. (2019) Makowski et al. (2019)",
                "Originally drafted by": "** Meng Liu",
                "Reviewed (or Edited) by": "** Alaa AlDoh; Helena Hartmann; Connor Keating; Kai Krautter; Michele C. Lim; Suzanne L. K. Stewart; Ana Todorovic",
                "Translated by": "Ahlam Ahmed",
                "Translation reviewed by": "** Amani Aloufi, Asma Alzahrani, Ali H. Al-Hoorie, Alaa M. Saleh, Mohammed Mohsen",
                "Translation": "التَّعريف**: مقياس إحصائيّ مستمر؛ لاختيار النَّموذج المستخدم في الاستدلال البايزي، يصف الأدلَّة النِّسبيَّة لنموذج ما على آخر، بغض النَّظر عما إذا كانت النَّماذج صحيحة أم لا. تتراوح معاملات بايز من 0 إلى ما لا نهاية، ممَّا يشير إلى القوة النِّسبيَّة للأدلَّة، وحيث 1 هي نقطة محايدة لعدم وجود دليل. وعلى النَّقيض من القيمة الاحتماليَّة، تسمح معاملات بايز بثلاثة أنواع  من الاستنتاجات:  (أ) دليل على الفرضيَّة البديلة. (ب) دليل على الفرضيَّة الصِّفريَّة. (ج) لا يوجد دليل كاف لأي منهما. وعليه يعبر BF10 عن وجود دليل لصالح الفرضيَّة البديلة مقارنة بالفرضيَّة الصِّفريَّة، و BF01 عن دليل لصالح الفرضيَّة الصِّفرية مقارنة بالفرضيَّة البديلة. **المصطلحات ذات الصِّلة:** الاستدلال البايزي، إحصاءات بايزية، دالة الاحتمالية، اختبار دلالة الفرضيَّة الصِّفريَّة، القيمة الاحتماليَّة.",
                "Related_terms": "** Bayesian inference; Bayesian statistics; Likelihood function; Null Hypothesis Significance Testing (NHST); *p*\\-value"
            },
            {
                "Title": "Bayesian Inference (الاستدلال البايزي) *",
                "Definition": "** A method of statistical inference based upon Bayes’ theorem, which makes use of epistemological (un)certainty using the mathematical language of probability. Bayesian inference is based on allocating (and reallocating, based on newly-observed data or evidence) credibility across possibilities. Two existing approaches to Bayesian inference include “Bayes factors” (BF) and Bayesian parameter estimation.  **",
                "Reference": "** Dienes (2011; 2014; 2016); Etz et al. (2018); Kruschke (2015); McElreath (2020); Wagenmakers et al. (2018)",
                "Originally drafted by": "** Charlotte R. Pennington",
                "Reviewed (or Edited) by": "** Alaa AlDoh; Bradley Baker; Robert Ross; Markus Weinmann; Tobias Wingen; Steven Verheyen",
                "Translated by": "** Ahlam Ahmed",
                "Translation reviewed by": "** Amani Aloufi, Asma Alzahrani, Ali H. Al-Hoorie, Mohammed Mohsen",
                "Translation": "التعريف:** طريقة للاستدلال الإحصائي تعتمد على نظرية بايز، والتي تستخدم (عدم) اليقين المعرفي باستخدام اللغة الرياضية للاحتمالات. يعتمد الاستدلال البايزي على تخصيص (وإعادة تخصيص ، بناء على البيانات أو الأدلة التي تمت ملاحظتها حديثا) والمصداقية عبر الاحتمالات. هناك طريقتان للاستدلال البايزي يشملان \"عوامل بايز\" وتقدير معامل بايزي.  **المصطلحات ذات الصلة:** معامل بايز;  إحصاءات بايزية، تقدير المعاملات باستخدام النهج البايزي.",
                "Related_terms": "** Bayes Factor; Bayesian statistics; Bayesian Parameter Estimation"
            },
            {
                "Title": "Bayesian Parameter Estimation (تقدير المعاملات باستخدام النَّهج البايزي) *",
                "Definition": "** A Bayesian approach to estimating parameter values by updating a prior belief about model parameters (i.e., prior distribution) with new evidence (i.e., observed data) via a likelihood function, resulting in a posterior distribution. The posterior distribution may be summarised in a number of ways including: point estimates (mean/mode/median of a posterior probability distribution), intervals of defined boundaries, and intervals of defined mass (typically referred to as a credible interval). In turn, a posterior distribution may become a prior distribution in a subsequent estimation. A posterior distribution can also be sampled using Monte-Carlo Markov Chain methods which can be used to determine complex model uncertainties (e.g. Foreman-Mackey et al., 2013).  **",
                "Reference": "** Foreman-Mackey et al. (2013); McElreath (2020); Press (2007); [https://blog.stata.com/2016/11/15/introduction-to-bayesian-statistics-part-2-mcmc-and-the-metropolis-hastings-algorithm/](https://blog.stata.com/2016/11/15/introduction-to-bayesian-statistics-part-2-mcmc-and-the-metropolis-hastings-algorithm/)",
                "Originally drafted by": "** Alaa AlDoh",
                "Reviewed (or Edited) by": "** Mahmoud Elsherif; Helena Hartmann; Dominik Kiersz; Meng Liu; Ana Todorovic; Markus Weinmann",
                "Translated by": "** Ahlam Ahmed",
                "Translation reviewed by": "** Amani Aloufi, Ali H. Al-Hoorie, Asma Alzahrani, Mohammed Mohsen",
                "Translation": "التَّعريف:** يقوم النَّهج البايزي بتقدير قيم المعاملات من خلال تحديث اعتقاد مسبق حول معاملات النَّموذج \\_ وبعبارة أخرى حول التَّوزيع المسبق\\_ وذلك لوجود أدلَّة جديدة (أي البيانات الملاحظة) وذلك باستخدام دالة الإمكان؛ ممَّا يؤدي إلى التَّوزيع اللَّاحق.  يمكن تلخيص التَّوزيع اللَّاحق بعدَّة طرق بما في ذلك:  تقدير دقيق لمتوسط، ووسيط، ومنوال التَّوزيع البعدي المحتمل، والفترات المتقطِّعة للحدود، وللكتلة المحدَّدة (يُشار للفترات المتقطِّعة للكتلة المحدَّدة على أنها الفترات الموثوق بها)، وفي المقابل قد يصبح التَّوزيع اللَّاحق توزيعًا مسبقًا في مرحلة تالية، كما يمكن أخذ عيّنات من التَّوزيع اللَّاحق باستخدام سلسلة ماركوف مونتي كارلو والتي يمكن استخدامها لتحديد أوجه الشَّك المعقّدة في النَّموذج (Foreman-Mackey et al., 2013). **المصطلحات ذات الصِّلة:** معامل بايز، الاستدلال البايزي، الإحصاءات البايزية، اختبار دلالة الفرضية الصِّفريَّة.",
                "Related_terms": "** Bayes Factor; Bayesian inference; Bayesian statistics; Null Hypothesis Significance Testing (NHST)"
            },
            {
                "Title": "BIDS data structure (بنية بيانات تصوير الدِّماغ) *",
                "Definition": "** The Brain Imaging Data Structure (BIDS) describes a simple and easy-to-adopt way of organizing neuroimaging, electrophysiological, and behavioral data (i.e., file formats, folder structures). BIDS is a community effort developed *by* the community *for* the community and was inspired by the format used internally by the OpenfMRI repository known as [OpenNeuro](https://openneuro.org). Having initially been developed for fMRI data, the BIDS data structure has been extended for many other measures, such as EEG (Pernet et al., 2019).  **",
                "Reference(s)": "** Gorgolewski et al. (2016); [https://bids.neuroimaging.io/](https://bids.neuroimaging.io/)",
                "Drafted by": "** Tina Lonsdorf",
                "Reviewed (or Edited) by": "** Ali H. Al-Hoorie; David Moreau; Mariella Paul; Charlotte R. Pennington",
                "Translated by": "** Ahlam Ahmed",
                "Translation reviewed by": "** Amani Aloufi, Hala Alghamdi, Ali H. Al-Hoorie, Asma Alzahrani, Mohammed Mohsen",
                "Translation": "التَّعريف**: تصف بنية بيانات تصوير الدِّماغ طريقة بسيطة، وسهلة التَّبني؛ لتنظيم التَّصوير العصبيّ و الفيزيولوجيا الكهربيّة، والبيانات السُّلوكيَّة (أي تنسيقات الملفات، وهياكل المجلَّدات).  وهذه البنية جهد مجتمعي طوَّره المجتمع من أجل المجتمع، وكان مستوحى من التَّنسيق المستخدم داخليًا بواسطة مستودع OpenfMRI المعروف باسم OpenNeuro. بعد أن تم تطويره في البداية لبيانات التَّصوير بالرَّنين المغناطيسيّ الوظيفيّ، تم توسيع البنية للعديد من التَّدابير الأخرى، مثل مخطط كهربية الدِّماغ (Pernet et al., 2019). **المصطلحات ذات الصِّلة**: البيانات المفتوحة.",
                "Related_terms": "** Open Data"
            },
            {
                "Title": "BIZARRE (بيزار) *",
                "Definition": "** This acronym refers to Barren, Institutional, Zoo, and other Rare Rearing Environments (BIZARRE). Most research for chimpanzees is conducted on this specific sample. This limits the generalizability of a large number of research findings in the chimpanzee population. The BIZARRE has been argued to reflect the universal concept of what is a chimpanzee (see also WEIRD, which has been argued to be a universal concept for what is a human).  **",
                "Reference(s)": "** Clark et al. (2019); Leavens et al. (2010)",
                "Originally drafted by": "** Mahmoud Elsherif",
                "Reviewed (or Edited) by": "** Zoe Flack; Charlotte R. Pennington",
                "Translated by": "** Asma Alzahrani",
                "Translation reviewed by": "Amani Aloufi, Ahlam Ahmed, Hala Alghamdi, Ali H. Al-Hoorie, Mohammed Mohsen**  ### ---",
                "Translation": "التَّعريف**: يشير مصطلح (BIZARRE) إلى اختصار البيئات القاحلة، المؤسساتيَّة، وحدائق الحيوان، وغيرها من البيئات المناخيَّة لتربية الحيوانات.  يتم إجراء معظم البحوث في هذه العينة بالتَّحديد عن الشَّمبانزي، والذي يحد بدوره من قابليَّة تعميم عدد كبير من نتائج البحوث على مجتمع الشَّمبانزي. يُعتقد أن بيزار يعكس المفهوم العالمي لكلِّ ماهو شمبانزي (راجع أيضًا وِيرد، والذي قيل إنه مفهوم عالمي لما هو إنسان). **المصطلحات ذات الصِّلة**: المجتمع، سترينج: الخلفية الاجتماعية والقابلية للتتبع والاختيار الذاتي وتاريخ التربية والتأقلم والتّعود، وِيرد",
                "Related_terms": "** Populations; STRANGE; WEIRD"
            },
            {
                "Title": "Bottom-up approach (to Open Scholarship) (النَّهج التَّصاعديّ) *",
                "Definition": "** Within academic culture, an approach focusing on the intrinsic interest of academics to improve the quality of research and research culture, for instance by making it supportive, collaborative, creative and inclusive. Usually indicates leadership from early-career researchers acting as the changemakers driving shifts and change in scientific methodology through enthusiasm and innovation, compared to a “top-down” approach initiated by more senior researchers Bottom-up approaches take into account the specific local circumstances of the case itself, often using empirical data, lived experience, personal accounts, and circumstances as the starting point for developing policy solutions.  **",
                "Reference(s)": "** Button et al. (2016); Button et al. (2020); Hart and Silka (2020); Meslin (2010); Moran et al. (2020); [https://www.cos.io/blog/strategy-for-culture-change](https://www.cos.io/blog/strategy-for-culture-change)",
                "Drafted by": "** Catherine Laverty",
                "Reviewed (or Edited) by": "** Helena Hartmann; Michele C. Lim; Adam Parker; Charlotte R. Pennington; Birgit Schmidt; Marta Topor; Flávio Azevedo",
                "Translated by": "** ِAbdulsamad Humaidan",
                "Translation reviewed by": "** Amani Aloufi, Ahlam Ahmed, Hala Alghamdi, Ali H. Al-Hoorie, Mohammed Mohsen",
                "Translation": "التَّعريف:** في الثَّقافة الأكاديميَّة يركز النَّهج التَّصاعديّ على الاهتمام الجوهريّ للأكاديميين بتحسين جودة البحوث وثقافة البحث، عبر جعلها داعمة، وتعاونيّة، ومبدعة، وشاملة.  وفي الغالب يشير النَّهج التَّصاعديّ لقيادة الباحثين حديثي الانضمام للحقل البحثيّ في بداية حياتهم المهنيَّة كصانعي التَّغيير الذين يقودون التَّحولات، والتَّغيير في المنهجيَّة العلميَّة من خلال الحماس والابتكار. ومقارنة بالنَّهج التنازلي والذي بدأه كبار الباحثين، فإنَّ النَّهج التَّصاعدي من أسفل إلى أعلى يأخذ بالحسبان الظُّروف المحليَّة والمحدَّدة بكلِّ حالة، والتي في كثير من الأحيان تستخدم البيانات التَّطبيقيَّة، والخبرات الحياتيَّة المعاشة، والاعتبارات الشَّخصية، والظُّروف كنقطة بداية  لتطوير حلول السِّياسات. **المصطلحات ذات الصِّلة:** الباحثون المبتدئون، المبادرات الشَّعبيَّة.",
                "Related_terms": "** Early Career Researchers (ECRs); Grassroot initiatives"
            },
            {
                "Title": "Boundary condition (الشَّرط الحدّي) *",
                "Definition": "Theories provide answers to what, how and why questions of a specific phenomenon. The ‘What’ refers to the variables involved in a causal model, The ‘how’  describes how the effects relate the variables to one another and ‘Why’ identifies the mechanisms explaining the relationship between these variables. However, boundary condition refers to the who, where and when features of a theory (Whetten, 1989). Put simply, these conditions relate to the values and positionality of the researcher, spatial and temporal boundaries (Bacharach, 1989\\) and the limits of generalisability of a theory (Whetten, 1989).  **",
                "Originally drafted by": "Mahmoud Elsherif",
                "Reviewed (or Edited) by": "Shijun Yu",
                "Translated by": "** Ahlam Ahmed",
                "Translation reviewed by": "** Hala Alghamdi, Ali H. Al-Hoorie, Asma Alzahrani",
                "Translation": "التَّعريف:** تقدم النَّظريّات إجابات عن أسئلة ماذا وكيف ولماذا حول ظاهرة معيّنة.  يشير \"ماذا\" إلى المتغيّرات المتضمّنة في النَّموذج السَّببي، ويصف \"كيف\" كيفيَّة ربط التَّأثيرات بين المتغيّرات ببعضها البعض. ويحدد \"لماذا\"  الآليّات التي تشرح العلاقة بين هذه المتغيرات. ومع ذلك، يشير الشَّرط الحدّيّ إلى سمات النَّظريّة مَن وأين ومتى  (Whetten, 1989). ببساطة، تتعلَّق هذه الشُّروط بقيم الباحث، وموقعه، والحدود المكانيَّة والزَّمانيَّة (Bacharach, 1989\\) وحدود قابليَّة تعميم النَّظريَّة (Whetten, 1989). **المصطلحات ذات الصِّلة:** أزمة التَّعميم، الموضعيَّة، النظريّة.",
                "Related_terms": "Generalisability crisis; Positionality; Theory **Reference (s)**: Bacharach, (1989); Busse et.al, (2017); Dubin (1978), Whetten (1989)"
            },
            {
                "Title": "Bracketing Interviews (تأطير المقابلات ) *",
                "Definition": "Bracketing interviews are commonly used within qualitative approaches. During these interviews researchers explore their personal subjectivities and assumptions surrounding their ongoing research. This allows researchers to be aware of their own interests and helps them to become both more reflective and critical about their research, considering how their own experiences may impact the research process. Bracketing interviews can also be subject to qualitative analysis.  **",
                "Originally drafted by": "Claire Melia",
                "Reviewed (or Edited) by": "Tamara Kalandadze; Charlotte R. Pennington; Graham Reid; Marta Topor",
                "Translated by": "** ِAli H. Al-Hoorie",
                "Translation reviewed by": "** Ahlam Ahmed, Asma Alzahrani, Mohammed Mohsen",
                "Translation": "التَّعريف:** يستخدم تأطير المقابلات على نحوِ شائع في المنهج النَّوعي.  يحاول الباحثون خلالها استكشاف تحيزاتهم الشَّخصيَّة، وافتراضاتهم المتعلِّقة بالبحوث التي يجرونها. ويتيح ذلك للباحثين أن يكونوا على دراية باهتماماتهم الخاصّة، ويساعدهم على أن يصبحوا أكثر تأملًا في بحوثهم وأكثر انتقادًا لها، مع الأخذ في الاعتبار كيفيَّة تأثير تجاربهم الخاصَّة على عمليَّة البحث. ويمكن أن يخضع تأطير المقابلات نفسه للتَّحليل النَّوعي. **المصطلحات ذات الصِّلة**: البحث النَّوعي، الانعكاسيَّة، تحيُّز الباحث.",
                "Related_terms": "Qualitative research; Reflexivity; Researcher bias **Reference (s)**: Rolls and Relf (2006); Sorsa et al. (2015)"
            },
            {
                "Title": "Bropenscience (بروبنساينس) *",
                "Definition": "A tongue-in-cheek expression intended to raise awareness of the lack of diverse voices in open science (Bahlai, Bartlett, Burgio et al., 2019; Onie, 2020), in addition to the presence of behaviour and communication styles that can be toxic or exclusionary. Importantly, not all bros are men; rather, they are individuals who demonstrate rigid thinking, lack self-awareness, and tend towards hostility, unkindness, and exclusion (Pownall et al., 2021; Whitaker & Guest, 2020). They generally belong to dominant groups who benefit from structural privileges. To address \\#bropenscience, researchers should examine and address structural inequalities within academic systems and institutions.  **",
                "Originally drafted by": "Zoe Flack",
                "Reviewed (or Edited) by": "Magdalena Grose-Hodge; Helena Hartmann; Bethan Iley; Tamara Kalandadze; Adam Parker; Charlotte R. Pennington; Flávio Azevedo; Bradley Baker; Mahmoud Elsherif",
                "Translated by": "** Ahlam Ahmed",
                "Translation reviewed by": "** Hala Alghamdi, Asma Alzahrani, Mohammed Mohsen  ### **C** {#c}",
                "Translation": "التَّعريف**: تعبير ساخر يهدف إلى زيادة الوعي بنقص الأصوات المتنوِّعة في العلوم المفتوحة (Bahlai, Bartlett, Burgio et al., 2019; Onie, 2020)، بالإضافة إلى وجود أنماط سلوكيَّة، وتواصليَّة يمكن أن تكون سامّة، أو إقصائيَّة.  الأهم من ذلك، ليس بالضَّرورة أن يكون كل هؤلاء الأفراد ذكورًا، بل هم أفراد يظهرون تفكيرًا جامدًا، ويفتقرون إلى الوعي الذَّاتي، ويميلون إلى العداء، والقسوة، والإقصاء (Pownall et al., 2021; Whitaker & Guest, 2020). و ينتمون عمومًا إلى مجموعات مهيمنة تستفيد من الامتيازات الهيكليَّة. لمعالجة هذه الظاهرة يجب على الباحثين دراسة وفحص ومعالجة التباينات واللامساواة الهيكليَّة داخل الأنظمة، والمؤسسات الأكاديميَّة. **المصطلحات ذات الصِّلة**: التَّنوع، الشمول، التَّقاطعيَّة، العلم المفتوح.",
                "Related_terms": "Diversity; Inclusion; Intersectionality; Open Science **Reference (s)**: Guest (2017); Whitaker and Guest (2020); Pownall et al. (2021)"
            },
            {
                "Title": "CARKing (النقد بعد معرفة النتائج) *",
                "Definition": "Critiquing After the Results are Known (CARKing) refers to presenting a criticism of a design as one that you would have made in advance of the results being known. It usually forms a reaction or criticism to unwelcome or unfavourable results, results whether the critic is conscious of this fact or not.  **",
                "Reference(s)": "** Bardsley (2018); Nosek and Lakens (2014)",
                "Originally drafted by": "** Mahmoud Elsherif",
                "Reviewed (or Edited) by": "** Ali H. Al-Hoorie; Ashley Blake; Adrien Fillon; Charlotte R. Pennington",
                "Translated by": "** Ruwayshid",
                "Translation reviewed by": "** Amani Aloufi, Ruwayshid, Asma Alzahrani, Hala Alghamdi, Ali H. Al-Hoorie, Mohammed Mohsen",
                "Translation": "التَّعريف:** يشير هذا المصطلح إلى تقديم نقد لمنهجيَّة الدِّراسة، كما لو كان الشَّخص سيقدمه قبل معرفة النَّتائج، وعادة ما يشكِّل رد فعل، أو نقدًا لنتائج غير مرحَّب بها، أو غير مرغوب فيها، سواء كان النَّاقد واعيًا بهذه الحقيقة أم لا.  **المصطلحات ذات الصِّلة:** الافتراض بعد معرفة النتائج، التسجيل بعد معرفة النتائج، التَّسجيل المسبق، التَّقرير المسجَّل، السباركينج",
                "Related_terms": "** HARKing; PARKing; Preregistration; Registered Report; SPARKing"
            },
            {
                "Title": "Center for Open Science (COS) (مركز العلم المفتوح) *",
                "Definition": "** A non-profit technology organization based in Charlottesville, Virginia with the mission to increase openness, integrity, and reproducibility of research. Among other resources, the COS hosts the Open Science Framework (OSF) and the Open Scholarship Knowledge Base.  **",
                "Reference(s)": "** cos.io",
                "Drafted by": "** Beatrix Arendt",
                "Reviewed (or Edited) by": "** Ali H. Al-Hoorie; Mariella Paul; Charlotte R. Pennington; Lisa Spitzer",
                "Translated by": "** Mahdi Aben Ahmed",
                "Translation reviewed by": "** Amani Aloufi, Ruwayshid, Asma Alzahrani, Hala Alghamdi, Ali H. Al-Hoorie, Mohammed Mohsen",
                "Translation": "التَّعريف:** منظمة غير ربحيَّة،  تعنى بالتُّكنولوجيا، ومقرها في شارلوتسفيل بولاية فيرجينيا، وتهتم بزيادة الشَّفافيَّة، والنَّزاهة، وإمكانيَّة تكرار البحوث. ويستضيف المركز إطار العلوم المفتوحة وقاعدة المعرفة المفتوحة بالإضافة إلى مصادر أخرى.  **المصطلحات ذات الصِّلة**: شارات العلوم المفتوحة، إطار العلوم المفتوحة، مجموعات OSF ، مؤسسات OSF، اجتماعات OSF، طبعات أولية OSF، سجلات OSF، الَّتسجيلات (التسجيلات المسبقة، والتَّقارير المسجلة)، إرشادات تعزيز الشَّفافيَّة والانفتاح (TOP)",
                "Related_terms": "** Open Science badges; Open Science Framework; OSF collections; OSF institutions; OSF meetings; OSF preprints; OSF registries; Registrations (Preregistrations & Registered Reports); Transparency and Openness Promotion Guidelines (TOP)"
            },
            {
                "Title": "Citation bias (تحيُّز الاستشهاد) *",
                "Definition": "** A biased selection of papers or authors cited and included in the references section. When citation bias is present, it is often in a way which would benefit the author(s) or reviewers, over-represents statistically significant studies, or reflects pervasive gender or racial biases (Brooks, 1985; Jannot et al., 2013; Zurn et al., 2020). One proposed solution is the use of Citation Diversity Statements, in which authors reflect on their citation practices and identify biases which may have emerged (Zurn et al., 2020).  **",
                "Reference(s)": "** Brooks (1985); Jannot et al. (2013); Thombs et al. (2015); Zurn et al. (2020)",
                "Originally drafted by": "** Bettina M. J. Kern",
                "Reviewed (or Edited) by": "** Mahmoud Elsherif; Annalise A. LaPlume; Helena Hartmann; Bethan Iley; Charlotte R. Pennington; Timo Roettger; Tobias Wingen",
                "Translated by": "** Ruwayshid",
                "Translation reviewed by": "** Asma Alzahrani; Awatif Alruwaili , Naif Masrahi, Hala Alghamdi, Ali H. Al-Hoorie",
                "Translation": "التَّعريف:** هو اختيار متحيِّز للبحوث أو المؤلفين الذين يتم الاستشهاد بهم، وإدراجهم في قسم المراجع. عندما يكون هناك تحيُّز في الاستشهاد، فغالبًا ما يكون ذلك بطريقة تفيد المؤلِّفين، أو المحكِّمين، أو تبالغ في تمثيل الدِّراسات ذات الدِّلالة الإحصائيَّة، أو تعكس التَّحيُّزات المتعلقة بالنوع البشري أو التحيزات العرقيَّة (Brooks, 1985; Jannot et al., 2013; Zurn et al., 2020). ولعل أحد الحلول المقترحة هو استخدام \"بيان تنوع الاستشهاد\" بحيث ينظر المؤلِّفون لممارساتهم في الاستشهاد، ويحدِّدون مواطن التَّحيُّز التي قد تظهر.  **المصطلحات ذات الصلة:** بيان تنُّوع  الاستشهادات، تقرير التَّحيُّز.",
                "Related_terms": "** Citation diversity statement; Reporting bias"
            },
            {
                "Title": "Citation Diversity Statement (بيان تنوُّع الاستشهادات) *",
                "Definition": "** A current effort trying to increase awareness and mitigate the citation bias in relation to gender and race is the Citation Diversity Statement, a short paragraph where “the authors consider their own bias and quantify the equitability of their reference lists. It states: (i) the importance of citation diversity, (ii) the percentage breakdown (or other diversity indicators) of citations in the paper, (iii) the method by which percentages were assessed and its limitations, and (iv) a commitment to improving equitable practices in science” (Zurn et al., 2020, p. 669). **",
                "Reference": "** Zurn et al. (2020)",
                "Originally drafted by": "** Helena Hartmann",
                "Reviewed (or Edited) by": "** Mahmoud Elsherif; Magdalena Grose-Hodge; Sam Parsons; Timo Roettger",
                "Translated by": "** Naif Masrahi",
                "Translation reviewed by": "**  Ruwayshid, Asma Alzahrani; Awatif Alruwaili, Ali H. Al-Hoorie, Mohammed Mohsen",
                "Translation": "التَّعريف:**  هو جهود جارية لزيادة الوعي، وتخفيف تحيُّز الاستشهاد فيما يتعلَّق بالنوع البشري والعرق، وهو عبارة عن فقرة قصيرة، حيث \"يأخذ المؤلِّفون في الاعتبار تحيُّزهم الشَّخصي، ويقيسون مدى إنصاف قوائمهم المرجعيَّة. تنصُّ الفقرة على: (1) أهميَّة تنوُّع الاستشهادات، (2) النِّسبة المئويَّة للتَّفصيل (أو مؤشرات التَّنوع الأخرى) للاستشهادات في الورقة، (3) الطَّريقة التي تم من خلالها تقييم النِّسب المئويَّة وقيودها،(4) الالتزام بتحسين الممارسات العادلة في العلوم\" (Zurn et al., 2020, p. 669). **المصطلحات ذات الصِّلة:** تحيُّز الاستشهاد، التنوُّع،  نقص التَّمثيل.",
                "Related_terms": "** Citation bias; Diversity; Under-representation"
            },
            {
                "Title": "Citizen Science (علم المواطن ) *",
                "Definition": "** Citizen science refers to projects that actively involve the general public in the scientific endeavour, with the goal of democratizing science. Citizen scientists can be involved in all stages of research, acting as collaborators, contributors or project leaders. An example of a major citizen science project involved individuals identifying astronomical bodies (Lintott, 2008).  **",
                "Reference(s)": "** Cohn (2008); European Citizen Science Association (2015); Lintott (2008)",
                "Drafted by": "** Mahmoud Elsherif; Ana Barbosa Mendes",
                "Reviewed (or Edited) by": "** Gisela H. Govaart; Tamara Kalandadze; Dominik Kiersz; Charlotte R. Pennington; Robert M. Ross",
                "Translated by": "** Ruwayshid",
                "Translation reviewed by": "** Amani Aloufi, Asma Alzahrani, Ahlam Ahmed, Ali H. Al-Hoorie, Mohammed Mohsen",
                "Translation": "التَّعريف:** يشير مصطلح علم المواطن إلى المشاريع العلميَّة التي تتضمَّن مشاركة فعَّالة من قبل الجمهور في المساعي العلميَّة بهدف إضفاء الطَّابع الدِّيمقراطي على العلوم.  يمكن أن يشارك العلماء المواطنون في جميع مراحل البحث، حيث يعملون كمتعاونين، أو مساهمين أو قادة مشاريع. من الأمثلة على مشروع علمي كبير للمواطنين هو تحديد الأفراد للأجسام الفلكيَّة (Lintott, 2008). ال**مصطلحات ذات الصِّلة:** علم الحشود، توظيف الجماهير. التَّعريف البديل: (إن أمكن) في الماضي كان علم المواطن يشير في الغالب إلى المتطوعين الذين يشاركون كمساعدين ميدانيين في الدِّراسات العلميَّة (Cohn, 2008, p. 193).",
                "Related_terms": "** Crowd science; Crowdsourcing **Alternative definition:** (if applicable) In the past, citizen science mostly referred to volunteers who participate as field assistants in scientific studies (Cohn, 2008, p. 193)."
            },
            {
                "Title": "CKAN (شبكة أرشيف المعرفة الشَّاملة) *",
                "Definition": "** The Comprehensive Knowledge Archive Network (CKAN) is an open-source data platform and free software that aims to provide tools to streamline publishing and data sharing. CKAN supports governments, research institutions and other organizations in managing and publishing large amounts of data.  **",
                "Reference": "** https://ckan.org/",
                "Originally drafted by": "** Tsvetomira Dumbalska",
                "Reviewed (or Edited) by": "** Ali H. Al-Hoorie; Kai Krautter; Charlotte R. Pennington",
                "Translated by": "** Asma Alzahrani",
                "Translation reviewed by": "** Awatif Alruwaili, Naif Masrahi , Ruwayshid, Ali H. Al-Hoorie, Mohammed Mohsen",
                "Translation": "التَّعريف:** توفِّر شبكة أرشيف المعرفة الشَّاملة منصَّة مفتوحة المصدر، وبرمجيات مجانيَّة تهدف إلى توفير أدوات من شأنها تسهيل نشر البيانات، ومشاركتها، كما تدعم الشَّبكة الحكومات، ومؤسَّسات البحث، ومنظَّمات أخرى في إدارة ونشر كميَّات هائلة من البيانات.  **المصطلحات ذات الصِّلة:** منصَّات البيانات، مشاركة البيانات.",
                "Related_terms": "** Data platforms; Data sharing"
            },
            {
                "Title": "COAR Community Framework for Good Practices in Repositories (الإطار المجتمعي للممارسات الجيِّدة في المستودعات) *",
                "Definition": "** A framework which identifies best practices for scientific repositories and evaluation criteria for these practices. Its flexible and multidimensional approach means that it can be applied to different types of repositories, including those which host publications or data, across geographical and thematic contexts.  **",
                "Reference": "** Confederation of Open Access Repositories (2020, October 8\\)",
                "Originally drafted by": "** Aleksandra Lazić",
                "Reviewed (or Edited) by": "** Ashley Blake; Jamie P. Cockcroft; Bethan Iley; Sam Parsons",
                "Translated by": "** Ahlam Ahmed",
                "Translation reviewed by": "** Amani Aloufi, Asma Alzahrani, Hala Alghamdi, Ali H. Al-Hoorie, Mohammed Mohsen",
                "Translation": "التَّعريف**: إطار يحدد أفضل الممارسات للمستودعات العلميَّة، ومعايير التَّقييم لهذه الممارسات. ويتميَّز نهجه \\- المرن والمتعدد الأبعاد \\- بإمكانيَّة تطبيقه على أنواع مختلفة من المستودعات، بما في ذلك تلك التي تستضيف منشورات، أو بيانات، وذلك باختلاف السِّياقات الجغرافيَّة، والموضوعيَّة.  **المصطلحات ذات الصِّلة:** البيانات الوصفيَّة، الوصول المفتوح، البيانات المفتوحة، المواد المفتوحة، مستودع البيانات، مبادئ الثِّقة.",
                "Related_terms": "** Metadata; Open Access; Open Data; Open Material; Repository; TRUST principles"
            },
            {
                "Title": "Codebook (دليل الرُّموز) *",
                "Definition": "** A codebook is a high-level summary that describes the contents, structure, nature and layout of a data set. A well-documented codebook contains information intended to be complete and self-explanatory for each variable in a data file, such as the wording and coding of the item, and the underlying construct. It provides transparency to researchers who may be unfamiliar with the data but wish to reproduce analyses or reuse the data.  **",
                "Reference": "** Arslan et al. (2019);[https://www.icpsr.umich.edu/icpsrweb/content/shared/ICPSR/faqs/what-is-a-codebook.html](https://www.icpsr.umich.edu/icpsrweb/content/shared/ICPSR/faqs/what-is-a-codebook.html)",
                "Originally drafted by": "** Tina Lonsdorf",
                "Reviewed (or Edited) by": "** Ali H. Al-Hoorie; Ashley Blake, Kai Krautter; Charlotte R. Pennington; Flávio Azevedo",
                "Translated by": "** Naif Masrahi",
                "Translation reviewed by": "** Asma Alzahrani, Ahlam Ahmed, Hala Alghamdi, Ali H. Al-Hoorie, Mohammed Mohsen",
                "Translation": "التَّعريف:** يُعد دليل الرُّموز ملخصًا عالي المستوى، يصف محتويات مجموعة البيانات، وهيكلها وطبيعتها، وشكلها.  يحتوي دليل الرُّموز الموثَّق جيداً على معلومات تهدف إلى أن تكون كاملة، وواضحة في حدِّ ذاتها لكلِّ متغيِّر في ملف البيانات، مثل تسمية وترميز العنصر، والمفهوم الذي يمثِّله. يوفر دليل الرُّموز الشَّفافيَّة للباحثين الذين قد لا يكونون على دراية بالبيانات، ولكنَّهم يرغبون في إعادة إنتاج التَّحليلات، أو إعادة استخدام البيانات. **المصطلحات ذات الصِّلة**: قاموس البيانات،  البيانات الوصفيَّة.",
                "Related_terms": "** Data dictionary; Metadata"
            },
            {
                "Title": "Code review (مراجعة النَّص البرمجي) *",
                "Definition": "** The process of checking another researcher's programming (specifically, computer source code) including but not limited to statistical code and data modelling. This process is designed to detect and resolve mistakes, thereby improving code quality. In practice, a modern peer review process may take place via a hosted online repository such as GitHub, GitLab or SourceForge. **",
                "Reference(s)": "** Petre and Wilson (2014); Scopatz and Huff (2015)",
                "Originally drafted by": "** Filip Dechterenko",
                "Reviewed (or Edited) by": "** Ali H. Al-Hoorie; Dominik Kiersz; Sam Parsons; Charlotte R. Pennington",
                "Translated by": "** Naif Masrahi",
                "Translation reviewed by": "** Asma Alzahrani; Awatif Alruwaili, Ahlam Ahmed, Ali H. Al-Hoorie",
                "Translation": "التَّعريف:** عمليَّة التَّحقق من برمجة باحث آخر (على وجه التَّحديد، شفرة مصدر الحاسوب) بما في ذلك على سبيل المثال لا الحصر، نص البرمجي الإحصائي، ونمذجة البيانات. تم تصميم هذه العمليَّة؛ لاكتشاف الأخطاء وحلها، مما يحسّن جودة التَّعليمات البرمجية من النَّاحية العمليَّة ، قد تتم عملية مراجعة النُّظراء الحديثة عبر مستودع مستضاف على الإنترنت مثل GitHub أو GitLab أو SourceForge. **المصطلحات ذات الصِّلة:** قابلية إعادة الإنتاج، التَّحكم في الإصدار",
                "Related_terms": "** Reproducibility; Version control"
            },
            {
                "Title": "Collaborative Replication and Education Project (CREP) (مشروع التِّكرار والتَّعليم التَّعاوني) *",
                "Definition": "** The Collaborative Replication and Education Project (CREP) is an initiative designed to organize and structure replication efforts of highly-cited empirical studies in psychology to satisfy the dual needs for more high-quality direct replications and more training in empirical research techniques for psychology students. CREP aims to address the need for replications of highly cited studies, and to provide training, support and professional growth opportunities for academics completing replication projects.  **",
                "Reference(s)": "** Wagge et al. (2019)",
                "Drafted by": "** Connor Keating",
                "Reviewed (or Edited) by": "** Bradley Baker; Mahmoud Elsherif; Zoe Flack; Charlotte R. Pennington",
                "Translated by": "** Asma Alzahrani",
                "Translation reviewed by": "** Awatif Alruwaili; Ahlam Ahmed, Hala Alghamdi, Ali H. Al-Hoorie, Mohammed Mohsen",
                "Translation": "التَّعريف:** مبادرة مصمَّمة لتنظيم، وهيكلة جهود التِّكرار للدراسات التَّطبيقية ذات الاستشهاد العالي في علم النَّفس وذلك لتلبية الاحتياج لكل من الدراسات التكريرية المباشرة ذات الجودة العالية والمزيد من التَّدريب على تقنيات البحوث التجريبية لطلبة علم النَّفس.  يهدف المشروع إلى تلبية الحاجة؛ لتكرار الدِّراسات التي تم الاستشهاد بها بشكل كبير، وتوفير التَّدريب والدَّعم، وفرص النُّمو المهني للأكاديميين الذين يكملون مشاريع البحوث التِّكرارية.  **المصطلحات ذات الصِّلة:** التِّكرار المباشر، التِّكرار الدَّقيق.",
                "Related_terms": "** Direct replication; Exact replication"
            },
            {
                "Title": "Committee on Best Practices in Data Analysis and Sharing (COBIDAS) (لجنة أفضل الممارسات في تحليل البيانات ومشاركتها) *",
                "Definition": "** The Organization for Human Brain Mapping (OHBM) neuroimaging community has developed a guideline for best practices in neuroimaging data acquisition, analysis, reporting, and sharing of both data and analysis code. It contains eight elements that should be included when writing up or submitting a manuscript in order to improve reporting methods and the resulting neuroimages in order to optimize transparency and reproducibility. **Alternative definition:** (if applicable) Checklist for data analysis and sharing  **",
                "Reference(s)": "** Nichols et al. (2017); Pernet et al. (2020)",
                "Originally drafted by": "** Yu-Fang Yang",
                "Reviewed (or Edited) by": "** Jamie P. Cockcroft; Helena Hartmann; Adam Parker; Sam Parsons",
                "Translated by": "** Asma Alzahrani",
                "Translation reviewed by": "** Awatif Alruwaili; Naif Masrahi, Ahlam Ahmed, Ali H. Al-Hoorie, Mohammed Mohsen",
                "Translation": "التَّعريف**: طور مجتمع التَّصوير العصبي التابع لمنظمة رسم خرائط الدِّماغ البشري دليلًا لأفضل الممارسات في جمع بيانات التَّصوير العصبي، وتحليلها وإعداد التَّقارير الخاصّة بها، ومشاركة كلّ من البيانات والنَّص البرمجي للتَّحليل.  يتضمن هذا المبدأ ثمانية عناصر يجب استيفاؤها عند كتابة أو تقديم مخطوطة من أجل تحسين طرق إعداد التَّقارير، والصُّور العصبيَّة النَّاتجة لتحقيق أقصى درجات الشَّفافيَّة وإعادة الإنتاج.  **تعريف بديل: (إن أمكن)** قائمة مرجعية لتحليل البيانات ومشاركتها."
            },
            {
                "Title": "Communality (التَّشاركيَّة) *",
                "Definition": "** The common ownership of scientific results and methods and the consequent imperative to share both freely. Communality is based on the fact that every scientific finding is seen as a product of the effort of a number of agents. This norm is followed when scientists openly share their new findings with colleagues.  **",
                "Reference(s)": "** Anderson et al. (2010); Hardwicke (2014); Merton (1938, 1942\\)",
                "Drafted by": "** David Moreau",
                "Reviewed (or Edited) by": "** Ashley Blake; Mahmoud Elsherif; Charlotte R. Pennington; Beatrice Valentini",
                "Translated by": "** Sarah Almutairi",
                "Translation reviewed by": "** Amani Aloufi, Asma Alzahrani, Ahlam Ahmed, Ali H. Al-Hoorie, Mohammed Mohsen",
                "Translation": "التَّعريف:** يشير هذا المصطلح للملكيَّة المشتركة للنتائج والأساليب العلميَّة، وما يترتَّب على ذلك من ضرورة مشاركتها بحريّة.  تؤمن التَّشاركيَّة  بأنَّ كلَّ اكتشافٍ علميّ هو نتيجة جهد عدد من أفراد المجتمع، وبناءً عليه، فإنَّ على الباحثين مشاركة نتائج بحوثهم بشكل علني  مع زملائهم.  **المصطلحات ذات الصِّلة: ا**لمعايير الميرتونية، الموضوعيَّة.  **تعريف بديل:** الاشتراكيَّة (Merton, 1942)  تعتبر الطرق والنتائج العلمية في البحث ملكية عامة وإرثًا مشتركًا لذا فإنه من الضرورة ان يتشارك الجميع هذه الطرق والنتائج  بحرية ومصداقية.",
                "Related_terms": "** Mertonian norms; Objectivity **Alternative definition:** Communism (in Merton, 1942\\) **Related terms to alternative definition** (if applicable)"
            },
            {
                "Title": "Community Projects (المشاريع المجتمعيَّة)*",
                "Definition": "** Collaborative projects that involve researchers from different career levels, disciplines, institutions or countries. Projects may have different goals including peer support and learning, conducting research, teaching and education. They can be short-term (e.g., conference events or hackathons) or long-term (e.g., journal clubs or consortium-led research projects). Collaborative culture and community building are key to achieving project goals.  **",
                "Reference(s)": "** Ellemers (2021); Orben (2019); Shepard (2015)",
                "Drafted by": "** Marta Topor",
                "Reviewed (or Edited) by": "** Ali H. Al-Hoorie; Jamie P. Cockcroft; Mahmoud Elsherif; Kai Krautter; Gerald Vineyard",
                "Translated by": "** Sarah Almutairi",
                "Translation reviewed by": "** Amani Aloufi, Asma Alzahrani, Ahlam Ahmed, Ali H. Al-Hoorie, Alaa M. Saleh, Mohammed Mohsen",
                "Translation": "التَّعريف:** هي المشاريع التَّعاونية التي يتشارك فيها باحثون من مختلف المستويات المهنيَّة، أو التَّخصُّصات، أو المؤسسات، أو البلدان.  وتختلف هذه المشاريع باختلاف الأهداف التي تُقام من أجلها، مثل: مشاريع مساعدة الأقران على التَّعلم، إجراء البحوث ووالتعليم والتَّربية.  وتختلف مدَّة هذه المشاريع بين مشاريع قصيرة المدى (كالمؤتمرات، أو فعاليات الهاكاثون)، أو طويلة المدى كأنشطة نوادي المجلات، أو المشاريع البحثيَّة التي تنظمها التَّحالفات. وتعتمد هذه المشاريع المجتمعيَّة على الثَّقافة التَّعاونيَّة، والسَّعي لبناء المجتمع كأساس لتحقيق أهدافها.  **المصطلحات ذات الصِّلة:** النَّهج التَّصاعدي، البحث الجماعيَّ، هاكثون، المعامل المتعدِّدة، نوادي الأبحاث التكرارية.",
                "Related_terms": "** Bottom-up approach (to Open Scholarship); Crowdsourced research; Hackathon; Many Labs; ReproducibiliTea"
            },
            {
                "Title": "Compendium (خلاصة وافية) *",
                "Definition": "** A collection of files prepared by a researcher to support a report or publication that include the data, metadata, programming code, software dependencies, licenses, and other instructions necessary for another researcher to independently reproduce the findings presented in the report or publication.  **",
                "Originally drafted by": "** Ben Marwick",
                "Reviewed (or Edited) by": "** Ali H. Al-Hoorie; Charlotte R. Pennington",
                "Translated by": "** Asma Alzahrani",
                "Translation reviewed by": "**Awatif Alruwaili; Ahlam Ahmed, Hala Alghamdi, Ali H. Al-Hoorie, Mohammed Mohsen",
                "Translation": "التَّعريف:** هي مجموعة من الملفّات التي يعدها الباحث؛ لدعم تقرير، أو بحث منشور. تتضمَّن البيانات الأصليَّة، والبيانات الوصفيَّة، والنَّص البرمجي، والبرامج التَّابعة، والتَّراخيص، والتَّعليمات الأخرى اللّازمة لباحث آخر؛ لإعادة إنتاج النَّتائج المذكورة في التَّقرير، أو البحث بشكل مستقل.  **المصطلحات ذات الصلة:** موسوعات؛ تكرار؛ قابلية إعادة الإنتاج، خلاصة البحوث",
                "Related_terms": "** Compendia; Replication; Reproducibility; Research compendium; **References:** Claerbout and Karrenfach (1992); Gentleman (2005); Marwick et al. (2018); Nüst et al. (2018)"
            },
            {
                "Title": "Computational reproducibility (الاستنساخ الحسابي) *",
                "Definition": "** Ability to recreate the same results as the original study (including tables, figures, and quantitative findings), using the same input data, computational methods, and conditions of analysis. The availability of code and data facilitates computational reproducibility, as does preparation of these materials (annotating data, delineating software versions used, sharing computational environments, etc). Ideally, computational reproducibility should be achievable by another second researcher (or the original researcher, at a future time), using only a set of files and written instructions. Also referred to as analytic reproducibility (LeBel et al., 2018).  **",
                "Reference(s)": "** Committee on Reproducibility and Replicability in Science et al. (2019); Kitzes et al (2017, p. xxii); LeBel et al. (2018); Nosek and Errington (2020); Obels et al. (2020)",
                "Drafted by": "** Tina Lonsdorf",
                "Reviewed (or Edited) by": "** Sarah Ashcroft-Jones; Helena Hartmann; Annalise A. LaPlume; Adam Parker; Charlotte R. Pennington; Eike Mark Rinke",
                "Translated by": "** Asma Alzahrani",
                "Translation reviewed by": "** Awatif Alruwaili; Ahlam Ahmed, Hala Alghamdi,. Ali H. Al-Hoorie, Mohammed Mohsen",
                "Translation": "التَّعريف**: القدرة على إعادة إنتاج نفس نتائج الدِّراسة الأصليَّة، بما في ذلك الجداول، والرُّسوم البيانيَّة، والنَّتائج الكميَّة باستخدام نفس البيانات، والأساليب الحسابيَّة، وظروف التَّحليل.  يسهم توافر التَّعليمات البرمجيَّة والبيانات في إعادة التِّكرار الحسابي، وكذلك إعداد هذه الأدوات (تحليل البيانات، تحديد إصدارات البرامج المستخدمة، مشاركة البيئات الحسابيَّة، وغيرها).  من النَّاحية المثاليَّة، يجب أن تكون إمكانيَّة التِّكرار الحسابيّ قابلة للتَّحقيق من قبل باحث ثانٍ \\_أو الباحث الأصلي في وقت لاحق\\_ باستخدام مجموعة من الملفّات، والتَّعليمات المكتوبة فقط.  يعرف الاستنساخ الحسابي أيضًا باسم الاستنساخ التَّحليليّ (LeBel et al., 2018).  **المصطلحات ذات الصِّلة**: مبادئ فير، ، قابليَّة التِّكرار، قابليَّة إعادة الإنتاج.",
                "Related_terms": "** FAIR principles; Replicability; Reproducibility"
            },
            {
                "Title": "Conceptual replication (التِّكرار المفاهيمي) *",
                "Definition": "** A replication attempt whereby the primary effect of interest is the same but tested in a different sample and captured in a different way to that originally reported (i.e., using different operationalisations, data processing and statistical approaches and/or different constructs; LeBel et al., 2018). The purpose of a conceptual replication is often to explore what conditions limit the extent to which an effect can be observed and generalised (e.g., only within certain contexts, with certain samples, using certain measurement approaches) towards evaluating and advancing theory (Hüffmeier et al., 2016).  **",
                "Reference(s)": "** Crüwell et al. (2019); Hüffmeier et al. (2016); LeBel et al. (2018)",
                "Drafted by": "** Mahmoud Elsherif; Thomas Rhys Evans",
                "Reviewed (or Edited) by": "** Adrien Fillon; Helena Hartmann; Matt Jaquiery; Tina B. Lonsdorf; Catia M. Oliveira; Charlotte R. Pennington; Graham Reid; Timo Roettger; Lisa Spitzer; Suzanne L. K. Stewart; Flávio Azevedo",
                "Translated by": "** Asma Alzahrani",
                "Translation reviewed by": "** Awatif Alruwaili; Ahlam Ahmed, Ali H. Al-Hoorie, Mohammed Mohsen",
                "Translation": "التَّعريف**: محاولة تكرار بحيث يكون التَّأثير الأساسيّ المراد اختباره هو نفسه، ولكنَّه يتم اختباره في عينة مختلفة، وتحليله بطريقة مختلفة عن تلك التي بُحثت في الدراسة الأصلية ، كاستعمال طرق تقنين مختلفة أو معالجة بيانات، أو أساليب إحصائيَّة، أو مفاهيم مختلفة (LeBel et al., 2018).  غالبًا ما يكون الهدف من تكرار المفاهيم هو استكشاف الظُّروف التي تحدُّ من مدى إمكانيَّة  ملاحظة التَّأثير وتعميمه في سياقات معيّنة، وعيّنات معيّنة، وباستعمال أساليب قياس معيّنة نحو تقييم وتطوير النَّظريَّة. (Hüffmeier et al., 2016).  **المصطلحات ذات الصِّلة**: التِّكرار المباشر، القابليَّة للتَّعميم.",
                "Related_terms": "** Direct replication; Generalizability"
            },
            {
                "Title": "Confirmation bias (**ا**لانحياز التَّوكيديّ) *",
                "Definition": "** The tendency to seek out, interpret, favor and recall information in a way that supports one’s prior values, beliefs, expectations, or hypothesis. **التَّعريف:** الميل للبحث عن المعلومات، وتفسيرها، وتمييزها، واسترجاعها بطريقة تدعم قيم الشَّخص ومعتقداته، وتوقعاته، أو فرضياته. **المصطلحات ذات الصِّلة:** الانحياز التَّأكيدي، الانحياز الاجتماعيّ (المتوافق)، التَّحيُّز الشَّخصي",
                "Reference(s)": "** Bishop (2020); Nickerson (1998); Spencer and Heneghan (2018); Wason (1960)",
                "Drafted by": "** Barnabas Szaszi; Jenny Terry",
                "Reviewed (or Edited) by": "** Mahmoud Elsherif; Tamara Kalandadze; Sam Parsons; Charlotte R. Pennington",
                "Translated by": "** Asma Alzahrani",
                "Translation reviewed by": "** Awatif Alruwaili; Ali H. Al-Hoorie, Hala Alghamdi, Mohammed Mohsen",
                "Translation": "** The tendency to seek out, interpret, favor and recall information in a way that supports one’s prior values, beliefs, expectations, or hypothesis. **التَّعريف:** الميل للبحث عن المعلومات، وتفسيرها، وتمييزها، واسترجاعها بطريقة تدعم قيم الشَّخص ومعتقداته، وتوقعاته، أو فرضياته. **المصطلحات ذات الصِّلة:** الانحياز التَّأكيدي، الانحياز الاجتماعيّ (المتوافق)، التَّحيُّز الشَّخصي",
                "Related_terms": "** Confirmatory bias; Congeniality bias; Myside bias"
            },
            {
                "Title": "Confirmatory analyses (التَّحليلات التَّوكيديَّة) *",
                "Definition": "** Part of the confirmatory-exploratory distinction (Wagenmakers et al., 2012), where confirmatory analyses refer to analyses that were set *a priori* and test existent hypotheses. The lack of this distinction within published research findings has been suggested to explain replicability issues and is suggested to be overcome through study preregistration which clearly distinguishes confirmatory from exploratory analyses. Other researchers have questioned these terms and recommended a replacement with ‘discovery-oriented’ and ‘theory-testing research’ (Oberauer & Lewandowsky, 2019; see also Szollosi & Donkin, 2019).  **",
                "Reference(s)": "** Box (1976); Oberauer and Lewandowsky (2019); Szollosi and Donkin (2019); Tukey (1977); Wagenmakers et al. (2012)",
                "Drafted by": "** Jenny Terry",
                "Reviewed (or Edited) by": "** Mahmoud Elsherif; Eduardo Garcia-Garzon; Helena Hartmann; Mariella Paul; Charlotte R. Pennington; Timo Roettger; Lisa Spitzer",
                "Translated by": "** Hala Alghamdi",
                "Translation reviewed by": "** Amani Aloufi, Asma Alzahrani, Ali H. Al-Hoorie, Mohammed Mohsen",
                "Translation": "التَّعريف:** هي جزء من التَّمييز بين البحوث التَّوكيديّة، والاستكشافيَّة (Wagenmakers et al., 2012)، حيث تشير التَّحاليل التَّوكيديَّة إلى التَّحاليل التي تم تحديدها مسبقًا، وتختبر الفرضيّات الموجودة.  يرى البعض أن عدم الالتفات لهذا التَّمييز في نتائج البحوث المنشورة يفسر سبب مشاكل القابليَّة للتِّكرار، ويمكن التغلب عليها من خلال التَّسجيل المسبق للدِّراسة والذي يمِّيز بوضوح التَّحاليل التَّوكيديّة عن الاستكشافيّة. وقد شكك بعض الباحثين في هذه المصطلحات وأوصوا باستبدالها بـ \"بحوث استكشافية\" و\"بحوث اختبار النَّظريات\". (Oberauer & Lewandowsky, 2019; Szollosi & Donkin, 2019).  **المصطلحات ذات الصِّلة:** تحليل البيانات الاستكشافي، التَّسجيل المسبق",
                "Related_terms": "** Exploratory data analysis; Preregistration"
            },
            {
                "Title": "Conflict of interest (تعارض المصالح) *",
                "Definition": "** A conflict of interest (COI, also ‘competing interest’) is a financial or non-financial relationship, activity or other interest that might compromise objectivity or professional judgement on the part of an author, reviewer, editor, or editorial staff. The *Principles of Transparency and Best Practice in Scholarly Publishing* by the Committee on Publication Ethics (COPE), the Directory of Open Access Journals (DOAJ), the Open Access Scholarly Publishers Association (OASPA), and the World Association of Medical Editors (WAME) states that journals should have policies on publication ethics, including policies on COI (DOAJ, 2018). COIs should be made transparent so that readers can properly evaluate research and assess for potential or actual bias(es). Outside publishing, academic presenters, panel members and educators should also declare COIs. Purposeful failure to disclose a COI may be considered a form of misconduct.  **",
                "Reference(s)": "** [http://www.icmje.org/recommendations/browse/roles-and-responsibilities/author-responsibilities--conflicts-of-interest.html](http://www.icmje.org/recommendations/browse/roles-and-responsibilities/author-responsibilities--conflicts-of-interest.html); DOAJ, 2018: [https://doaj.org/apply/transparency/](https://doaj.org/apply/transparency/)",
                "Drafted by": "** Christopher Graham",
                "Reviewed (or Edited) by": "** Flávio Azevedo",
                "Translated by": "** Asma Alzahrani",
                "Translation reviewed by": "** Awatif Alruwaili, Ali H. Al-Hoorie, Hala Alghamdi, Mohammed Mohsen",
                "Translation": "التَّعريف**: تعارض المصالح ويطلق عليه أيضًا \"المصالح المتنافسة\": هو علاقة ماليَّة، أو غير ماليَّة أو نشاط، أو مصلحة أخرى قد تضعف الموضوعية أو الحكم المهني من جانب المؤلِّف، أو المراجع، أو المحرِّر، أو فريق التَّحرير.  تنصُّ مبادئ الشَّفافيَّة \\_وأفضل الممارسات في النَّشر العلميّ التي وضعتها لجنة أخلاقيَّات النَّشر، ودليل المجلَّات المفتوحة، وجمعيَّة النَّاشرين الأكاديميين بالوصول المفتوح، وجمعيَّة محرِّري الطِّب العالميَّة\\_ على أنَّ الدَّوريّات يجب أنْ تتبنَّى سياسات حول أخلاقيَّات النَّشر، بما في ذلك سياسات حول تعارض المصالح (DOA, 2018). يجب أن يتم الإفصاح عن تعارضات المصالح بشفافية حتى يتمكَّن القراء من تقييم البحث بشكل صحيح، وتقييم الانحياز المحتمل أو الفعلي،  بالإضافة إلى النَّشر، يجب أيضًا على مقدمي العروض الأكاديميَّة وأعضاء اللِّجان والمعلِّمين أن يعلنوا عن تعارضات المصالح.  يمكن اعتبار عدم الكشف عن تعارض المصالح عمدًا شكلًا من أشكال سوء السُّلوك.  **المصطلحات ذات الصِّلة**: الموضوعية، تحكيم الأقران، ثقة الجمهور في العلوم، أخلاقيَّات النَّشر، الشَّفافية",
                "Related_terms": "** Objectivity; Peer review; Public Trust in Science; Publication ethics; Transparency"
            },
            {
                "Title": "Consortium authorship (التَّحالف التَّأليفي) *",
                "Definition": "** Only the name of the consortium or organization appears in the author column, and the individuals' names do not appear in the literature: For example, ‘FORRT’ as an author. This can be seen in the products of collaborative projects with a very large number of collaborators and/or contributors. Depending on the journal policy, individual researchers may be recorded as one of the authors of the product in literature databases such as ORCID and Scopus. Consortium authorship can also be termed group, corporate, organisation/organization or collective authorship (e.g.[https://www.bmj.com/about-bmj/resources-authors/article-submission/authorship-contributorship](https://www.bmj.com/about-bmj/resources-authors/article-submission/authorship-contributorship)), or collaborative authorship (e.g. [https://support.jmir.org/hc/en-us/articles/115001449591-What-is-a-group-author-collaborative-author-and-does-it-need-an-ORCID](https://support.jmir.org/hc/en-us/articles/115001449591-What-is-a-group-author-collaborative-author-and-does-it-need-an-ORCID))  **",
                "Reference(s)": "** Open Science Collaboration (2015); Tierney et al. (2020, 2021\\)",
                "Drafted by": "** Yuki Yamada",
                "Reviewed (or Edited) by": "** Adam Parker; Charlotte R. Pennington; Beatrice Valentini; Qinyu Xiao; Flávio Azevedo",
                "Translated by": "** Asma Alzahrani",
                "Translation reviewed by": "** Awatif Alruwaili; Naif Masrahi, Ali H. Al-Hoorie, Mohammed Mohsen",
                "Translation": "التَّعريف:** في التَّحالف التَّأليفي يظهر اسم الاتحاد، أو المنظمة فقط في عمود المؤلِّف، ولا تظهر أسماء الأفراد، فعلى سبيل المثال: يكون \"فورت\" هو مؤلف.  يمكن ملاحظة هذا النمط في منتجات المشاريع التَّعاونيَّة التي تضم عددًا كبيرًا من المساهمين، اعتمادًا على سياسة المجلَّة، يمكن تسجيل الباحثين الأفراد كمؤلفين للمنتج البحثيّ في قواعد بيانات الأدبيَّات مثل: أوركيد وسكوبس. يمكن أيضًا تسمية التَّحالف التَّأليفيّ بتأليف المجموعة، أو الشَّركة، أو المنظمة، أو التأليف الجماعي، مثل: [https://www.bmj.com/about-bmj/resources-authors/article-submission/authorship-contributorship](https://www.bmj.com/about-bmj/resources-authors/article-submission/authorship-contributorship) ) أو التَّأليف التَّعاوني مثل: ([https://support.jmir.org/hc/en-us/articles/115001449591-What-is-a-group-author-collaborative-author-and-does-it-need-an-ORCID](https://support.jmir.org/hc/en-us/articles/115001449591-What-is-a-group-author-collaborative-author-and-does-it-need-an-ORCID) ) **المصطلحات ذات الصِّلة:** التأليف، تصنيف أدوار المؤلفين.",
                "Related_terms": "** Authorship; CRediT"
            },
            {
                "Title": "Constraints on Generality (COG) (قيود التَّعميم) *",
                "Definition": "** A statement that explicitly identifies and justifies the target population, and conditions, for the reported findings. Researchers should be explicit about potential boundary conditions for their generalisations (Simons et al., 2017). Researchers should provide detailed descriptions of the sampled population and/or contextual factors that might have affected the results such that future replication attempts can take these factors into account (Brandt et al., 2014). Conditions not explicitly listed are assumed not to have theoretical relevance to the replicability of the effect.  **",
                "Reference(s)": "** Busse et al. (2017); Brandt et al. (2014); Simons et al. (2017); Yarkoni (2020)",
                "Drafted by": "** Mahmoud Elsherif",
                "Reviewed (or Edited) by": "** Ali H. Al-Hoorie; Jamie P. Cockcroft; Sam Parsons; Charlotte R. Pennington; Timo Roettger",
                "Translated by": "** Mahdi Aben Ahmed",
                "Translation reviewed by": "** Amani Aloufi, Asma Alzahrani, Awatif Alruwaili, Ali H. Al-Hoorie, Mohammed Mohsen  ### ---",
                "Translation": "التَّعريف:** عبارة تحدّد بشكل صريح، وتبرِّر المجتمع المستهدف، وظروف النَّتائج التي دوِّنت. يجب أن يكون الباحثون صريحين بشأن القيود المحتملة لتعميم نتائج بحوثهم (Simons et al., 2017). وينبغي على الباحثين تقديم أوصاف تفصيليَّة لعيِّنة المجتمع والعوامل السِّياقيَّة التي قد تؤثرعلى النَّتائج بحيث يمكن لمحاولات التِّكرار المستقبليَّة أن تأخذ هذه العوامل بعين الاعتبار (Brandt et al., 2014).  من المفترض ألا يكون للشروط غير المدرجة صراحةً صلة نظريَّة بإمكانيَّة تكرار التَّأثير. **المصطلحات ذات الصِّلة:** بيزار، التَّنوع، العدالة، القابلية للتَّعميم، الشمول، قابليَّة إعادة الإنتاج، التِّكرار، سترينج: الخلفية الاجتماعية والقابلية للتتبع والاختبار الذاتي وتاريخ التربية والتأقلم والتّعود، وِيرد",
                "Related_terms": "** BIZARRE; Diversity; Equity; Generalizability; Inclusion; Reproducibility; Replication; STRANGE; WEIRD"
            },
            {
                "Title": "Construct validity (الصِّدق البنائي) *",
                "Definition": "** When used in the context of measurement and testing, construct validity refers to the degree to which a test measures what it claims to be measuring. In fields that study hypothetical unobservable entities, construct validation is essentially theory testing, because it involves determining whether an objective measure (a questionnaire, lab task, etc.) is a valid representation of a hypothetical construct (i.e., conforms to a theory). When used in a broader sense about a research study, or a claim, conclusion, or observed effect in a research study, construct validity concerns the extent to which the sampling particulars used in the study (participants, settings, treatments, and dependent variables) map onto the higher order constructs the study, the claim, the conclusion is about. According to Shadish et al. (2002), construct validity can be defined as “the degree to which inferences are warranted from the observed persons, settings, and cause and effect operations included in a study to the constructs that these instances might represent” (p. 38). **",
                "Reference": "** Cronbach and Meehl (1955); Shadish et al. (2002); Smith (2005)",
                "Originally drafted by": "** Annalise A. LaPlume",
                "Reviewed (or Edited) by": "** Ali H. Al-Hoorie; Mahmoud Elsherif; Zoltan Kekecs; Charlotte R. Pennington",
                "Translated by": "** Zainab Alsuhaibani",
                "Translation reviewed by": "** Asma Alzahrani; Awatif Alruwaili, Ali H. Al-Hoorie, Mohammed Mohsen",
                "Translation": "التَّعريف**: عند استخدامه في سياق القياس والاختبار، يشير الصِّدق البنائي إلى مدى قياس اختبار معيّن ما يدّعي قياسه. أمّا في المجالات التي تُعنى بكيانات فرضيّة غير قابلة للقياس، يعد الصِّدق البنائي في الأساس اختبارًا نظريًا،لأنَّه يتضمَّن تحديد ما إذا كان المقياس الموضوعي (استبانة، مهمة معمليَّة، وما إلى ذلك) ممثلًا صادقًا للبنية الافتراضية (أي يتوافق مع نظريّة ما). وعند استخدامه بمعنى أوسع حول دراسة بحثيَّة، أو ادعاء، أو استنتاج، أو تأثير ملحوظ في دراسة بحثيَّة، فإنَّ صدق البناء يتعلَّق بمدى اتساق تفاصيل العيِّنة المستخدمة في الدِّراسة (المشاركون، والبيئات والتَّجارب، والمتغيِّرات التَّابعة) مع ~~البنى و~~المفاهيم العليا، والادعاء، والاستنتاج. ووفقًا لبعض العلماء، يمكن تعريف الصِّدق البنائي على أنه: \"الدَّرجة التي يتم فيها ضمان صحّة نسبة الاستدلالات من الأفراد، والسِّياقات، والنَّتائج ومسبباتها في دراسة معيّنة للمفاهيم التي قد تمثلها هذه الحالات\" (Shadish et al., 2002., p. 38). **المصطلحات ذات الصِّلة:** أزمة القياس، صدق القياس، ممارسات القياس المشكوك فيها، النظريّة، الصِّدق، التَّصديق.",
                "Related_terms": "** Measurement crisis; Measurement validity; Questionable Measurement Practices (QMP); Theory; Validity; Validation"
            },
            {
                "Title": "Content validity (صدق المحتوى ) *",
                "Definition": "** The degree to which a measurement includes all aspects of the concept that the researcher claims to measure; “A qualitative type of validity where the domain of the concept is made clear and the analyst judges whether the measures fully represent the domain” (Bollen, 1989, p.185). It is a component of *construct validity* and can be established using both quantitative and qualitative methods, often involving expert assessment.  **",
                "Reference": "** Bollen (1989); Brod et al. (2009); Drost (2011); Haynes et al. (1995)",
                "Originally drafted by": "** Annalise A. LaPlume",
                "Reviewed (or Edited) by": "** Mahmoud Elsherif; Wanyin Li; Aoife O’Mahony; Eike Mark Rinke; Sam Parsons; Graham Reid",
                "Translated by": "** Zainab Alsuhaibani",
                "Translation reviewed by": "** Asma Alzahrani; Awatif Alruwaili, Naif Masrahi, Ali H. Al-Hoorie, Mohammed Mohsen",
                "Translation": "التَّعريف**: يشير هذا المصطلح إلى مدى شموليَّة القياس لجميع جوانب المفهوم التي يدعي الباحث قياسها: \"هو نوع من أنواع الصِّدق الكيفي الذي يكون فيه مجال المفهوم واضحًا بحيث يحكم المحلّل على ما إذا كانت المقاييس تمثل المجال كليةً \" (Bollen, 1989, p.185).  وهو أحد مكونات الصِّدق البنائي ويمكن تنفيذه باستخدام الطُّرق الكميَّة والكيفيّة، والتي غالبًا ما تتضمَّن تقييم الخبراء.  **المصطلحات ذات الصِّلة:** الصِّدق البنائي، الصِّدق",
                "Related_terms": "** Construct validity; Validity"
            },
            {
                "Title": "Contribution (الإسهام ) *",
                "Definition": "** A formal addition or activity in a research context. Contribution and contributor statements, including acknowledgments sections in journal articles, are attached to research products to better classify and recognize the variety of labor beyond “authorship” that any intellectual pursuit requires. Contribution is an evolving “source of data for understanding the relationship between authorship and knowledge production” (Lariviere et al., p.430). In open source software development, a contribution may count as changes committed onto a project's software repository following a peer-review (known technically as a pull request). An example of an open-source project accepting contributions is NumPy (Harris et al., 2020).  **",
                "Reference": "** Knoth and Herrmannova (2014); Larivière et al. (2016); Holcombe (2019)",
                "Originally drafted by": "** Micah Vandegrift",
                "Reviewed (or Edited) by": "** Jamie P. Cockcroft; Dominik Kiersz; Michele C. Lim; Leticia Micheli; Sam Parsons; Gerald Vineyard",
                "Translated by": "** Naif Masrahi",
                "Translation reviewed by": "** Asma Alzahrani; Awatif Alruwaili, Ali H. Al-Hoorie, Mohammed Mohsen",
                "Translation": "التَّعريف**: هو إضافة رسميَّة، أو نشاط في سياق البحث. يتم إرفاق بيانات المساهمين، بما في ذلك أقسام الشُّكر في مقالات المجلات، بمنتجات البحث؛ لتصنيفها بشكل أفضل، والاعتراف بتنوع العمل الذي يتجاوز \"التَّأليف\" الذي يتطلبه أي عمل، أو إنتاج فكري.  إن المساهمة تتطوَّر باعتبارها \"مصدر بيانات لفهم العلاقة بين التَّأليف، والإنتاج المعرفي\" (Lariviere et al., p.430).  يُمكن اعتبار المساهمة في مجال تطوير البرمجيات مفتوحة المصدر بمثابة تغييرات يتم إدراجها في مخزن برامج المشروع بعد مراجعة الأقران المعروف تقنيًا باسم \"طلب السَّحب\". يعتبر NumPy مثالًا على مشروع مفتوح المصدر يقبل المساهمات (Harris et al., 2020). **المصطلحات ذات الصِّلة:** التَّأليف، تصنيف أدوار المؤلفين، القياسات الدِّلالية.",
                "Related_terms": "** authorship; CRediT; Semantometrics"
            },
            {
                "Title": "Corrigendum (البيان التصحيحي) *",
                "Definition": "** A corrigendum (pl. corrigenda, Latin: 'to correct') documents one or multiple errors within a published work that do not alter the central claim or conclusions and thus does not rise to the standard of requiring a retraction of the work. Corrigenda are typically available alongside the original work to aid transparency. Some publishers refer to this document as an erratum (pl. errata, Latin: 'error'), while others draw a distinction between the two (corrigenda as author-errors and errata as publisher-errors).  **",
                "Reference": "** Correction or retraction? (2006)",
                "Originally drafted by": "** Charlotte R. Pennington",
                "Reviewed (or Edited) by": "** Bradley Baker; Nick Ballou; Wanyin Li; Adam Parker; Emily A. Williams",
                "Translated by": "** Asma Alzahrani",
                "Translation reviewed by": "** Awatif Alruwaili; Naif Masrahi, Ali H. Al-Hoorie, Mohammed Mohsen",
                "Translation": "التعريف:** تُوثِّق التصحيحات خطأ ما أو أكثر في البحث المنشور والتي لا تغير جوهر البحث وفكرته الأساسية ولا استنتاجاته وبالتالي لا يرقى إلى مستوى سحب العمل. غالبًا ما تكون التصحيحات بجانب العمل الأصلي لغرض الشفافية. كما يطلق بعض الناشرين على هذا المستند مصلح \"خطأ\"، بينما يميز آخرون بين الاثنين بحيث تتعلق \"التصحيحات\" بأخطاء المؤلف  بينما تتعلق \"الأخطاء\" بأخطاء الناشر.  **المصطلحات ذات الصلة:** التصحيح، خطأ، التراجع",
                "Related_terms": "** Correction; Errata; Retraction"
            },
            {
                "Title": "Co-production (الإنتاج المشترك) *",
                "Definition": "** An approach to research where stakeholders who are not traditionally involved in the research process are empowered to collaborate, either at the start of the project or throughout the research lifecycle. For example, co-produced health research may involve health professionals and patients, while co-produced education research may involve teaching staff and pupils/students. This is motivated by principles such as respecting and valuing the experiences of non-researchers, addressing power dynamics, and building mutually beneficial relationships. **",
                "Reference": "** Filipe et al. (2017); Graham et al. (2019); NIHR (2021); Co-production Collective (2021)",
                "Originally drafted by": "** Emma Norris",
                "Reviewed (or Edited) by": "** Gisela H. Govaart; Magdalena Grose-Hodge; Helena Hartmann;Charlotte R. Pennington; Sonia Rishi; Emily A. Williams",
                "Translated by": "** Ahlam Ahmed",
                "Translation reviewed by": "** Asma Alzahrani; Awatif Alruwaili, Ali H. Al-Hoorie, Mohammed Mohsen  ### ---",
                "Translation": "التَّعريف**: ‏نهج للبحث يمكِّن أصحاب المصلحة الذين لا يشاركون عادةً في عملية البحث من التَّعاون، إما في بداية المشروع، أو طوال دورة فترة البحث. على سبيل المثال: قد يتضمَّن الإنتاج المشترك في البحث الصِّحي التَّعاون بين المهنيين الصِّحيين والمرضى، بينما قد يشمل البحث التربوي أعضاء هيئة التَّدريس والطُّلاب. والدافع وراء هذا هو بعض المبادئ مثل: احترام وتقدير تجارب غير الباحثين، ومعالجة ديناميكيات السُّلطة، وبناء علاقات متبادلة المنفعة. **المصطلحات ذات الصلة**: علم المواطن.  التعاون؛  البحوث التعاونية،  علم الحشود،  منحة دراسية،  الترجمة المتكاملة للمعرفة (IKT)؛  النمط 2 لإنتاج المعرفة؛  البحوث التشاركية، عيّنة ومجتمع الدّراسة المساهمين",
                "Related_terms": "** Citizen science; Collaboration; Collaborative research; Crowd science; Engaged scholarship; Integrated Knowledge Translation (IKT); Mode 2 of knowledge production; Participatory research; Patient and Public Involvement (PPI)"
            },
            {
                "Title": "Creative Commons (CC) license (رخصة المشاع الإبداعيّ) *",
                "Definition": "** A set of free and easy-to-use copyright licences that define the rights of the authors and users of open data and materials in a standardized way. CC licenses enable authors or creators to share copyright-law-protected work with the public and come in different varieties with more or less clauses. For example, the Attribution-NonCommercial-ShareAlike 4.0 International (CC BY-NC-SA 4.0) allows you to share and adapt the material, under the condition that you; give credit to the original creators, indicate if changes were made, and share under the same license as the original, and you cannot use the material for commercial purposes. **",
                "Reference(s)": "** https://creativecommons.org/about/cclicenses/",
                "Originally drafted by": "** Tina Lonsdorf",
                "Reviewed (or Edited) by": "** Adrien Fillon; Gisela H. Govaart; Annalise A. LaPlume; Sam Parsons; Charlotte R. Pennington",
                "Translated by": "** Ahlam Ahmed",
                "Translation reviewed by": "** Asma Alzahrani, Awatif Alruwaili, Ali H. Al-Hoorie, Mohammed Mohsen",
                "Translation": "التَّعريف**: ‏مجموعة من التَّراخيص لحقوق النَّشر المجانيَّة سهلة الاستخدام، حيث تحدِّد حقوق المؤلِّفين، ومستخدمي البيانات، والمواد المفتوحة بطريقة موحّدة. تُمكِّن تراخيص المشاع الإبداعي المؤلِّفين من مشاركة العمل المحمي بموجب قانون حقوق الطَّبع والنَّشر مع الجمهور، ويأتي في أشكال مختلفة مع بنود أكثر أو أقل. فعلى سبيل المثال: تتيح لك رخصة \"الإسناد-غير تجاري-المشاركة بالمثل 4.0 الدولية\" مشاركة المواد وتعديلها بشرط أن ينسب العمل للكُتّاب الأصليين، والإشارة إلى تغييرات إن وجدت، والمشاركة بموجب نفس التَّرخيص، وأن لا تستخدم  المواد لأغراض تجاريَّة. **المصطلحات ذات الصِّلة**: ‏حقوق الطَّبع والنَّشر، التَّرخيص. **تعريف بديل:** (إن أمكن) المشاع الإبداعي هي منظمة دولية غير ربحية توفر تراخيص المشاع الإبداعي بهدف تقليل العقبات القانونية أمام مشاركة المعرفة والإبداع.",
                "Related_terms": "** Copyright; Licence **Alternative definition:** (if applicable) Creative Commons is an international nonprofit organization that provides Creative Commons licences, with the goal to minimize legal obstacles to the sharing of knowledge and creativity."
            },
            {
                "Title": "Credibility revolution (ثورة المصداقيَّة ) *",
                "Definition": "** The problems and the solutions resulting from a growing distrust in scientific findings, following concerns about the credibility of scientific claims (e.g., low replicability). The term has been proposed as a more positive alternative to the term replicability crisis, and includes the many solutions to improve the credibility of research, such as preregistration, transparency, and replication. **",
                "Reference": "** Angrist and Pischke (2010); Vazire (2018); Vazire et al. (2020)",
                "Originally drafted by": "** Tamara Kalandadze",
                "Reviewed (or Edited) by": "** Bradley Baker; Mahmoud Elsherif; Helena Hartmann; Kai Krautter; Annalise A. LaPlume; Oscar Lecuona; Charlotte R. Pennington; Robert Ross; Tobias Wingen; Flávio Azevedo",
                "Translated by": "** Ahlam Ahmed",
                "Translation reviewed by": "** Asma Alzahrani; Awatif Alruwaili, Ali H. Al-Hoorie",
                "Translation": "التَّعريف**: تتعلَّق بالمشاكل والحلول النَّاتجة عن تزايد عدم الثِّقة في النَّتائج العلميَّة، على إثر المخاوف المتعلِّقة بمصداقيّة الادِّعاءات العلميَّة (مثل انخفاض قابليَّة التِّكرار). تم اقتراح هذا المصطلح كبديل أكثر إيجابيَّة لمصطلح أزمة قابليَّة التِّكرار، إذ يتضمَّن العديد من الحلول؛ لتحسين مصداقيَّة البحث، مثل التَّسجيل المسبق، والشَّفافيَّة والتِّكرار. **المصطلحات ذات الصِّلة:** مصداقيَّة الادِّعاءات العلميَّة، معايير عالية من الأدِّلَّة، العلم المفتوح، أزمة إعادة الإنتاج (أو أزمة التكرار)، ، الشَّفافيَّة. ‏",
                "Related_terms": "** Credibility of scientific claims; High standards of evidence; Openness; Open Science;Reproducibility crisis (aka Replicability or replication crisis); Transparency"
            },
            {
                "Title": "Creative destruction approach to replication (نهج التَّدمير الإبداعي للتِّكرار) *",
                "Definition": "** Replication efforts should seek not just to support or question the original findings, but also to replace them with revised, stronger theories with greater explanatory power. This approach therefore involves ‘pruning’ existing theories, comparing all the alternative theories, and making replication efforts more generative and engaged in theory-building (Tierney et al., 2020, 2021). **",
                "Reference(s)": "** Tierney et al. (2020, 2021\\)",
                "Drafted by": "** Mahmoud Elsherif",
                "Reviewed (or Edited) by": "** Magdalena Grose-Hodge; Aoife O’Mahony; Adam Parker; Charlotte R. Pennington; Sonia Rishi; Beatrice Valentini",
                "Translated by": "** Ahlam Ahmed",
                "Translation reviewed by": "** Asma Alzahrani, Nabil Sayed; Awatif Alruwaili, Ali H. Al-Hoorie, Mohammed Mohsen",
                "Translation": "التَّعريف**: ‏يجب أن تسعى جهود تكرار البحوث ليس فقط لدعم النَّتائج الأصليَّة، أو التَّشكيك فيها، بل أيضّا لاستبدالها بنظريات أقوى، وذات قوّة تفسيريّة أكبر. وبالتَّالي، فإنَّ هذا النَّهج يتضمَّن \"تشذيب\" النَّظريات الحاليَّة، ومقارنة جميع النَّظريَّات البديلة، وجعل جهود تكرار الأبحاث توليديَّة، ومشاركة في بناء النَّظريَّة أكثر (Tierney et al., 2020, 2021). **المصطلحات ذات الصِّلة**: البحث الجماعي، التَّزوير، النُّسخ المتماثلة، النظريّة.",
                "Related_terms": "** Crowdsourced research; Falsification; Replication; Theory"
            },
            {
                "Title": "CRediT (تصنيف أدوار المؤلِّفين) *",
                "Definition": "** The Contributor Roles Taxonomy (CRediT; [https://casrai.org/credit/](https://casrai.org/credit/)) is a high-level taxonomy used to indicate the roles typically adopted by contributors to scientific scholarly output. There are currently 14 roles that describe each contributor’s specific contribution to the scholarly output. They can be assigned multiple times to different authors and one author can also be assigned multiple roles. CRediT includes the following roles: Conceptualization, Data curation, Formal Analysis, Funding acquisition, Investigation, Methodology, Project administration, Resources, Software, Supervision, Validation, Visualization, Writing – original draft, Writing – review & editing. A description of the different roles can be found in the work of Brand et al., (2015). **",
                "Reference(s)": "** Brand et al. (2015); Holcombe (2019); [https://casrai.org/credit/](https://casrai.org/credit/)",
                "Drafted by": "** Sam Parsons",
                "Reviewed (or Edited) by": "** Myriam A. Baum; Matt Jaquiery; Tamara Kalandadze; Connor Keating; Charlotte R. Pennington; Yuki Yamada; Flávio Azevedo",
                "Translated by": "** Ahlam Ahmed",
                "Translation reviewed by": "** Asma Alzahrani; Awatif Alruwaili, Ali H. Al-Hoorie, Mohammed Mohsen",
                "Translation": "التَّعريف**: تصنيف أدوار المساهمين (https://casrai.org/credit) هو تصنيف رفيع المستوى يُستخدَّم للإشارة إلى الأدوار التي يعتمدها عادة المساهمون في الإنتاج العلمي. ويوجد حاليًا 14 دورًا يصف المساهمة المحدَّدة لكلِّ مساهم في الإنتاج العلمي. يمكن تعيين كل دور لمؤلِّفين مختلفين، ويمكن أيضًا تعيين أدوار متعدِّدة لمؤلف واحد يتضمَّن هذا التَّصنيف الأدوار الآتية: تصوّر الفكرة، وتنظيم البيانات، والتَّحليل الرَّسمي، والحصول على التَّمويل، والتَّحقيق، والمنهجيَّة، وإدارة المشروع، والموارد ، والبرامج، والإشراف، والتَّحقق من الصِّحة، والتَّصور، والكتابة \\- المسودة الأصليَّة، والكتابة \\- المراجعة والتَّحرير. وهناك وصف لكل من هذه الأدوار المختلفة (Brand et al., 2015). **المصطلحات ذات الصِّلة:** التَّأليف، الإسهام.",
                "Related_terms": "** Authorship; Contributions"
            },
            {
                "Title": "Criterion validity (صدق المحك ) *",
                "Definition": "** The degree to which a measure corresponds to other valid measures of the same concept. Criterion validity is usually established by calculating regression coefficients or bivariate correlations estimating the direction and strength of relation between test measure and criterion measure. It is often confused with *construct validity* although it differs from it in intent (merely predictive rather than theoretical) and interest (predicting an observable outcome rather than a latent construct). Unreliability in either test or criterion scores usually diminishes criterion validity. Also called criterion-related or concrete validity.  **",
                "Reference": "** DeVellis (2017); Drost (2011)",
                "Originally drafted by": "** Annalise A. LaPlume",
                "Reviewed (or Edited) by": "** Helena Hartmann; Kai Krautter; Sam Parsons; Eike Mark Rinke",
                "Translated by": "** Zainab Alsuhaibani",
                "Translation reviewed by": "** Asma Alzahrani; Awatif Alruwaili, Ali H. Al-Hoorie, Mohammed Mohsen",
                "Translation": "التَّعريف**: يشير هذا المصطلح إلى مدى توافق المقياس مع المقاييس الصَّادقة الأخرى للمفهوم ذاته.  وعادة ما يتم بيان صدق المحك عن طريق حساب معاملات الانحدار، أو الارتباطات ثنائيَّة المتغيِّر؛ لتقدير اتجاه وقوة العلاقة بين المقياس والمحك .  وغالبًا ما يتم الخلط بينه وبين الصِّدق البنائي على الرُّغم من اختلافه عنه في الغرض (فهو مجرد تنبؤي وليس نظريًا) والفائدة (التَّنبؤ بنتيجة يمكن ملاحظتها بدلًا من البنية الكامنة). وعادةً ما يؤدي عدم الموثوقيَّة في درجات المقياس أو المحك إلى تقليل صدق المحك . ويُطلق عليه أيضًا الصِّدق المتعلِّق بالمحكات أو الصِّدق الملموس. **المصطلحات ذات الصِّلة:** الصدق البنائي، الصِّدق.",
                "Related_terms": "** Construct validity; Validity"
            },
            {
                "Title": "Crowdsourced Research (البحث الجماعي) *",
                "Definition": "** Crowdsourced research is a model of the social organisation of research as a large-scale collaboration in which one or more research projects are conducted by multiple teams in an independent yet coordinated manner. Crowdsourced research aims at achieving efficiency and scalability gains by pooling resources, promoting transparency and social inclusion, as well as increasing the rigor, reliability, and trustworthiness by enhancing statistical power and mutual social vetting. It stands in contrast to the traditional model of academic research production, which is dominated by the independent work of individual or small groups of researchers (‘small science’). Examples of crowdsourced research include so-called ‘many labs replication’ studies (Klein et al., 2018), ‘many analysts, one dataset’ studies (Silberzahn et al., 2018), distributive collaborative networks (Moshontz et al., 2018\\) and open collaborative writing projects such as Massively Open Online Papers (MOOPs) (Himmelstein et al., 2019; Tennant et al., 2019). Alternatively, crowdsourced research can refer to the use of a large number of research “crowdworkers” in data collection hired through online labor markets like Amazon Mechanical Turk or Prolific, for example in content analysis (Benoit et al., 2016; Lind et al., 2017\\) or experimental research (Peer et al., 2017). Crowdsourced research that is both open for participation and open through shared intermediate outputs has been referred to as *crowd science* (Franzoni & Sauermann, 2014). **",
                "Reference": "** Benoit et al. (2016); Breznau (2021); Franzoni and Sauermann (2014); Himmelstein et al. (2019); Klein et al. (2018); Lind et al. (2017); Moshontz et al. (2018); Peer et al. (2017); Silberzahn et al. (2018); Stewart et al. (2017); Tennant et al. (2019); Uhlmann et al. (2019); [https://psysciacc.org/](https://psysciacc.org/); [https://crowdsourcingweek.com/what-is-crowdsourcing/](https://crowdsourcingweek.com/what-is-crowdsourcing/#:~:text=Crowdsourcing%20is%20the%20practice%20of,levels%20and%20across%20various%20industries)",
                "Originally drafted by": "** Eike Mark Rinke",
                "Reviewed (or Edited) by": "** Ali H. Al-Hoorie; Sam Parsons; Charlotte R. Pennington; Suzanne L. K. Stewart; Flávio Azevedo",
                "Translated by": "** Naif Masrahi",
                "Translation reviewed by": "** Asma Alzahrani; Awatif Alruwaili, Ali H. Al-Hoorie, Mohammed Mohsen",
                "Translation": "التعريف:** البحث الجماعي هو نموذج للتَّنظيم الاجتماعي؛ للبحث كتعاون واسع النِّطاق يتم فيه تنفيذ مشروع بحثي واحد، أو أكثر من قبل فرق متعدِّدة بطريقة مستقلَّة ولكن منسَّقة. يهدف البحث الجماعي إلى تحقيق مكاسب الكفاءة، وقابليَّة التَّوسع من خلال تجميع الموارد، وتعزيز الشَّفافيَّة، والاندماج الاجتماعي، بالإضافة إلى زيادة الدِّقة والموثوقية والجدارة من خلال تعزيز القوة الإحصائيَّة، والتَّحقق الاجتماعي المتبادل. يتعارض البحث الجماعي مع النَّموذج التَّقليدي لإنتاج البحث الأكاديمي، والذي يهيمن عليه العمل المستقلّ للأفراد، أو مجموعات صغيرة من الباحثين (\"العلوم الصغيرة\"). من أمثلة البحث الجماعي ما يسمى بدراسات \"التِّكرار المتعدِّد المعامل'' (Klein et al., 2018)، ودراسات \"العديد من المحلِّلين لمجموعة بيانات واحدة '' (Silberzahn et al., 2018)، والشَّبكات التَّعاونيَّة التَّوزيعيَّة (Moshontz et al., 2018\\) ومشاريع الكتابة التَّعاونية المفتوحة مثل: أوراق الإنترنت المفتوحة الضَّخمة (Himmelstein et al., 2019; Tennant et al., 2019). وكذلك يمكن أن يشير البحث الجماعي إلى استخدام عدد كبير من \"حشود العمال\" في جمع البيانات من خلال أسواق العمل عبر الإنترنت مثل Amazon Mechanical Turk أو Prolific، مثلًا في تحليل المحتوى (Benoit et al., 2016; Lind et al., 2017\\) أو البحث التَّجريبي (Peer et al., 2017). يُشار إلى البحث الجماعي المفتوح للمشاركة، والمفتوح من خلال المخرجات الوسيطة المشتركة باسم علم الحشود (Franzoni & Sauermann, 2014). **المصطلحات ذات الصِّلة:** علم المواطن، التَّعاون، توظيف الحشود، علم الفريق",
                "Related_terms": "** Citizen science; Collaboration; Crowdsourcing; Team science"
            },
            {
                "Title": "Cultural taxation (الضَّريبة الثَّقافيَّة) *",
                "Definition": "** The additional labor expected or demanded of members of underrepresented or marginalized minority groups, particularly scholars of color. This labor often comes from service roles providing ethnic, cultural, or gender representation and diversity. These roles can be formal or informal, and are generally unrewarded or uncompensated. Such labor includes providing expertise on matters of diversity, educating members of majority groups, acting as a liaison to minority communities, and formal and informal roles as mentor and support system for minority students. **",
                "Reference(s)": "** Joseph and Hirschfeld (2011); Ledgerwood et al. (2021); Padilla (1994)",
                "Originally drafted by": "** Bradley Baker",
                "Reviewed (or Edited) by": "** Helena Hartmann; Bethan Iley; Aoife O’Mahony; Charlotte R. Pennington; Flávio Azevedo",
                "Translated by": "** Ahlam Ahmed",
                "Translation reviewed by": "** Asma Alzahrani; Awatif Alruwaili, Ali H. Al-Hoorie, Mohammed Mohsen",
                "Translation": "التَّعريف**: العبء الإضافي المتوقع، أو المطلوب من أفراد الأقليَّات الممثَّلة تمثيلًا ناقصًا، أو المهمّشة، وخاصّة العلماء من غير العرق الأبيض. غالبًا ما يأتي هذا العبء من الأدوار الخدميّة التي توفر التَّمثيل، والتَّنوع العرقي، أو الثَّقافي، أو الجندري. يمكن أن تكون هذه الأدوار رسميَّة أو غير رسميَّة، وعادة ما تكون غير مجزية، أو غير مدفوعة الأجر. يشمل هذا العبء توفير الخبرة في مسائل التَّنوع الثَّقافي، وتثقيف أعضاء مجتمعات الأغلبيَّة، والعمل كحلقة وصل مع مجتمعات الأقليَّات، والأدوار الرَّسميَّة، وغير الرَّسميَّة كموجّه ونظام دعم لطلبة الأقليَّات. **المصطلحات ذات الصِّلة**: العمل غير المرئي، اختلالات السُّلطة، علاقات السُّلطة.",
                "Related_terms": "** Invisible labor; Power imbalances; Power relations"
            },
            {
                "Title": "Cumulative science (العلم التَّراكمي) *",
                "Definition": "** Goal of any empirical science, it is the pursuit of “the construction of a cumulative base of knowledge upon which the future of the science may be built” (Curran, 2009, p. 1). The idea that science will create more complete and accurate theories as a function of the amount of evidence and data that has been collected. Cumulative science develops in gradual and incremental steps, as opposed to one abrupt discovery. While revolutionary science occurs scarcely, cumulative science is the most common form of science. **",
                "Reference": "** Curran (2009); d’Espagnat (2008); Kuhn (1962); Mischel (2008)",
                "Originally drafted by": "** Beatrice Valentini",
                "Reviewed (or Edited) by": "** Sarah Ashcroft-Jones; Mahmoud Elsherif; Helena Hartmann; Oscar Lecuona; Wanyin Li; Sonia Rishi; Flávio Azevedo",
                "Translated by": "** Hala Alghamdi",
                "Translation reviewed by": "** Asma Alzahrani; Awatif Alruwaili, Ali H. Al-Hoorie, Mohammed Mohsen  ###  ### **D** {#d}",
                "Translation": "التَّعريف:** إنَّ غاية أي علم تطبيقي هي السَّعي نحو بناء قاعدة معرفيَّة تراكميَّة، تُمكّن مستقبل العلم أن يبنى عليها. (Curran, 2009, p. 1). وتعني هذه الفكرة أنَّ العلم سيقوم بخلق نظريَّات أكثر اكتمالًا ودقة كلَّما زادت كميَّة الأدلَّة والبيانات التي تم جمعها. يتطوَّر العلم التَّراكمي بخطى تدريجيَّة ومتزايدة، وليس عبر اكتشاف واحد مفاجئ. في حين أنَّ العلم الثَّوري نادر الحدوث، يعد العلم التَّراكمي أكثر أشكال العلوم شيوعًا. **المصطلحات ذات الصِّلة:** العلم البطيء.",
                "Related_terms": "** Slow Science"
            },
            {
                "Title": "Data Access and Research Transparency (DA-RT) (الوصول للبيانات وشفافيَّة البحث) *",
                "Definition": "** Data Access and Research Transparency (DA-RT) is an initiative aimed at increasing data access and research transparency in the social sciences. It is a multi-epistemic and multi-method initiative, created in 2014 by the Council of the American Political Science Association (APSA), to bolster the rigor of empirical social inquiry. In addition to other activities, DA-RT developed the Journal Editors' Transparency Statement (JETS), which requires subscribing journals to (a) making relevant data publicly available if the study is published, (b) following a strict data citation policy, (c) transparently describing the analytical procedures and, if possible, providing public access to analytical code, and (d) updating their journal style guides and codes of ethics to include improved data access and research transparency requirements. **",
                "Reference": "** Carsey (2014); Monroe (2018)",
                "Originally drafted by": "** Eike Mark Rinke",
                "Reviewed (or Edited) by": "** Filip Dechterenko; Kai Krautter; Flávio Azevedo",
                "Translated by": "** Hala Alghamdi",
                "Translation reviewed by": "** Ali H. Al-Hoorie, Ahlam Ahmed, Mohammed Mohsen",
                "Translation": "التَّعريف:** مبادرة تهدف إلى زيادة الوصول إلى البيانات وشفافيَّة البحوث في العلوم الاجتماعيَّة. وهي مبادرة متعدِّدة المعارف والأساليب، أنشأها مجلس جمعيَّة العلوم السِّياسيَّة الأمريكيَّة في عام 2014 لتعزيز دقة البحث الاجتماعي التجريبي . وبالإضافة إلى أنشطتها الأخرى، طورت هذه المبادرة بيان الشَّفافيَّة لمحرِّري المجلات والذي يتطلَّب التزام المجلات المشتركة بهذه المبادرة بالآتي: (أ) إتاحة البيانات ذات الصِّلة للعموم إذا تم نشر الدِّراسة. (ب) اتِّباع سياسة صارمة للاستشهاد بالبيانات. (ج) وصف الإجراءات التَّحليليَّة بشفافيَّة، وإن أمكن، إتاحة الوصول للنَّص البرمجي للتَّحليل. (د) تحديث أدلّة نمط المجلّة، وقواعد الأخلاقيَّات؛ لتشمل تسهيل الوصول إلى البيانات ومتطلّبات الشَّفافيَّة البحثيَّة. **المصطلحات ذات الصِّلة: إمكانيَّة الوصول، مشاركة البيانات، قابلية التكرار، قابليَّة إعادة الإنتاج.**",
                "Related_terms": "** Accessibility; Data sharing; Replicability; Reproducibility"
            },
            {
                "Title": "Data management plan (DMP) (خطة إدارة البيانات) *",
                "Definition": "** A structured document that describes the process of data acquisition, analysis, management and storage during a research project. It also describes data ownership and how the data will be preserved and shared during and upon completion of a project. Data management templates also provide guidance on how to make research data FAIR and where possible, openly available. **",
                "Reference(s)": "** Burnette et al. (2016); Michener (2015); Research Data Alliance (2020); https://library.stanford.edu/research/data-management-services/data-management-plans\\#:\\~:text=A%20data%20management%20plan%20(DMP,share%20and%20preserve%20your%20data.",
                "Drafted by": "** Dominique Roche",
                "Reviewed (or Edited) by": "** Charlotte R. Pennington; Sam Parsons; Birgit Schmidt; Flávio Azevedo",
                "Translated by": "** Hala Alghamdi",
                "Translation reviewed by": "** Ahlam Ahmed, Ali H. Al-Hoorie, Mohammed Mohsen",
                "Translation": "التَّعريف:** يشير هذا المصطلح إلى ملف منظم يصف عمليَّة الحصول على البيانات وتحليلها وإدارتها وتخزينها أثناء قيام بمشروع بحثي ما ، كما تصف ملكيَّة هذه البيانات وكيف سيتم حفظها ونشرها أثناء وبعد الانتهاء من المشروع. توفِّر قوالب إدارة البيانات إرشادات حول كيفيَّة جعل بيانات البحث متوافقة مع مبادئ فير ومتاحة للجميع إن أمكن ذلك. **المصطلحات ذات الصِّلة:** أرشفة البيانات، مشاركة البيانات، تخزين البيانات، مبادئ فير، البيانات المفتوحة.",
                "Related_terms": "** Data archiving; Data sharing; Data storage; FAIR principles; Open data"
            },
            {
                "Title": "Data sharing (مشاركة البيانات) *",
                "Definition": "** collection of practices, technologies, cultural elements and legal frameworks that are relevant to the practice of making data used for scholarly research available to other investigators. Gollwitzer et al. (2020) describe two types of data sharing: Type 1: Data that is necessary to reproduce the findings of a published research article. Type 2: data that have been collected in a research project but have not (or only partly) been analysed or reported after the completion of the project and are hence typically shared under a specified embargo period. **",
                "Reference(s)": "** Abele-Brehm et al. (2019); Gollwitzer et al. (2020); https://eudatasharing.eu/what-data-sharing",
                "Drafted by": "** Mahmoud Elsherif",
                "Reviewed (or Edited) by": "** Helena Hartmann; Tina Lonsdorf; Charlotte R. Pennington; Timo Roettger; Flávio Azevedo",
                "Translated by": "Naif Masrahi**",
                "Translation reviewed by": "** Ahlam Ahmed, Ali H. Al-Hoorie, Hala Alghamdi, Mohammed Mohsen",
                "Translation": "التَّعريف:** مجموعة من الممارسات، والتِّقنيَّات، والعناصر الثَّقافيَّة، والأطر القانونيَّة التي تتعلَّق بجعل البيانات المستخدمة في البحث العلمي متاحة للباحثين الآخرين. وصف بعض الباحثين (Gollwitzer et al., 2020\\) نوعين من مشاركة البيانات: النَّوع 1/ البيانات الضَّروريَّة لإعادة إنتاج نتائج مقالة بحثيَّة منشورة. النوع 2/ البيانات التي تم جمعها في مشروع بحثي ولكن لم يتم تحليلها (أو تم تحليلها جزئيًا فقط) أو لم يتم كتابة تقرير عنها بعد الانتهاء من المشروع، ومن ثم تتم مشاركتها عادةً في ظل فترة حظر محدَّدة. **المصطلحات ذات الصِّلة:** مبادئ فير؛ البيانات المفتوحة.",
                "Related_terms": "** FAIR principles; Open data"
            },
            {
                "Title": "Data visualisation (تصوير البيانات) *",
                "Definition": "** Graphical representation of data or information. Data visualisation takes advantage of humans’ well-developed visual processing capacity to convey insight and communicate key information. Data visualisations often display the raw data, descriptive statistics, and/or inferential statistics. **",
                "Reference(s)": "** Healy (2018); Tufte (1983)",
                "Drafted by": "** Bradley Baker",
                "Reviewed (or Edited) by": "** Mahmoud Elsherif; Charlotte R. Pennington; Suzanne L. K. Stewart;",
                "Translated by": "** Hala Alghamdi",
                "Translation reviewed by": "** Ahlam Ahmed, Ali H. Al-Hoorie, Mohammed Mohsen",
                "Translation": "التَّعريف:** هو التَّمثيل الرُّسومي للبيانات أو المعلومات. تستند فعاليَّة تصوير البيانات إلى قدرة المعالجة البصريَّة المتطوِّرة لدى البشر في نقل الأفكار، وتوصيل المعلومات الأساسيَّة. يبرز تصوير البيانات غالبًا الرُّسوماتِ والبيانات الأوليَّة، والإحصائيّات الوصفيَّة، و الإحصائيَّات الاستدلاليَّة. **المصطلحات ذات الصِّلة:** شكل بياني، رسم بياني، مخطَّط بياني.",
                "Related_terms": "** Figure; Graph; Plot"
            },
            {
                "Title": "Demarcation criterion (معيار التَّمييز) *",
                "Definition": "** A criterion for distinguishing science from non-science which aims to indicate an optimal way for knowledge of the world to grow. In a Popperian approach, the demarcation criterion was falsifiability and the application of a falsificationist attitude. Alternative approaches include that of Kuhn, who believed that the criterion was puzzle solving with the aim of understanding nature, and Lakatos, who argued that science is marked by working within a progressive research programme. **",
                "Reference(s)": "** Dienes (2008)",
                "Drafted by": "** Alaa AlDoh",
                "Reviewed (or Edited) by": "** Bethan Iley; Sara Middleton",
                "Translated by": "Ruwayshid",
                "Translation reviewed by": "Ali H. Al-Hoorie, Ahlam Ahmed, Mohammed Mohsen",
                "Translation": "التَّعريف:** معيار لتمييز العلم عن غير العلم، يهدف إلى بيان الطريقة المثلى لنمو المعرفة في العالم. في النَّهج البوبري، يعدُّ معيار التمييز هو القابلية للتزييف، وتطبيق موقف التَّكذيب. تشمل الأساليب البديلة نهج \"كُون\"، الذي اعتقد بأنَّ المعيار هو حل الألغاز بهدف فهم الطبيعة، وكذلك \"لاكاتوس\" الذي رأى العلم يتميّز بالعمل ضمن برنامج بحث تقدمي. ال**مصطلحات ذات الصِّلة:** الفرضيَّة، التَّزييف.",
                "Related_terms": "** Hypothesis; Falsification"
            },
            {
                "Title": "DOI (digital object identifier) (معرّف الكائن الرَّقمي) *",
                "Definition": "** Digital Object Identifiers (DOI) are alpha-numeric strings that can be assigned to any entity, including: publications (including preprints), materials, datasets, and feature films \\- the use of DOIs is not restricted to just scholarly or academic material. DOIs “provides a system for persistent and actionable identification and interoperable exchange of managed information on digital networks” ([https://doi.org/hb.html](https://doi.org/hb.html)). There are many different DOI registration agencies that operate DOIs, but the two that researchers would most likely encounter are [Crossref](https://www.crossref.org/) and [Datacite](https://datacite.org/). **",
                "Reference": "** Bilder (2013); Morgan (1998); [https://www.doi.org/hb.html](https://www.doi.org/hb.html)",
                "Originally drafted by": "** Tina Lonsdorf",
                "Reviewed (or Edited) by": "** Ashley Blake; Helena Hartmann; Sam Parsons; Charlotte R. Pennington; Flávio Azevedo",
                "Translated by": "** Ahlam Ahmed",
                "Translation reviewed by": "** Ali H. Al-Hoorie, Hala Alghamdi, Mohammed Mohsen",
                "Translation": "التَّعريف:** معرفات الكائنات الرَّقميَّة هي سلاسل أبجديَّة رقميَّة يمكن تخصيصها لأي كيان بما في ذلك: المنشورات (ومنها المطبوعات الأوليَّة)، والمواد، ومجموعات البيانات، والأفلام. ولا يقتصر استخدام المعرّفات الرَّقميَّة على المواد الأكاديميَّة فقط. إنَّ معرفات الكائنات الرَّقميَّة توفِّر نظامًا لتحديد دائم، وقابل للتَّنفيذ، وتبادل قابل للتَّشغيل البيني للمعلومات المدارة على الشَّبكات الرَّقميَّة  (https://doi.org/hb.html). هناك العديد من وكالات لتسجيل معرّفات الكائنات الرَّقميَّة وإدارتها، ولكن من المرجَّح أنْ يواجه أغلب الباحثين اثنتين هما: Crossref و Datacite. :**المصطلحات ذات الصِّلة** arXiv and BibTex; Crossref, Datacite, ISBN, ISO, الأوركيد; Permalink.",
                "Related_terms": "** arXiv and BibTex; Crossref, Datacite, ISBN, ISO, ORCID; Permalink"
            },
            {
                "Title": "Double-blind peer review (التحكيم مزدوج التَّعمية) *",
                "Definition": "** Evaluation of research products by qualified experts where both the author(s) and reviewer(s) are anonymous to each other. “This approach conceals the identity of the authors and their affiliations from reviewers and would, in theory, remove biases of professional reputation, gender, race, and institutional affiliation, allowing the reviewer to avoid bias and to focus on the manuscript’s merit alone” (Tvina et al., 2019, p. 1082). Like all types of peer-review, double-blind peer review is not without flaws. Anonymity can be difficult, if not impossible, to achieve for certain researchers working in a niche area. **",
                "Reference(s)": "** Largent and Snodgrass (2016); Tvina et al. (2019)",
                "Drafted by": "** Mahmoud Elsherif",
                "Reviewed (or Edited) by": "** Bradley Baker; Helena Hartmann; Meng Liu; Emma Norris",
                "Translated by": "** Asma Alzahrani",
                "Translation reviewed by": "** Ali H. Al-Hoorie, Hala Alghamdi, Ahlam Ahmed, Mohammed Mohsen",
                "Translation": "التَّعريف:** هو تقييم نتائج البحوث من قِبل خبراء مؤهلين حيث يكون المؤلِّف (أو المؤلِّفون) والمحكّم (أو المحكّمون) مجهولًا (مجهولين) بالِّنسبة لبعضهم البعض، حيث يخفي هذا النَّهج هوية المؤلِّفين وانتماءاتهم عن المحكّمين ومن شأنه، من النَّاحية النَّظريَّة، إزالة التَّحيُّزات المتعلِّقة بالسُّمعة المهنيَّة، والجنس، والعِرق، والانتماء المؤسسي، ممّا يسمح للمحكّم بتجنب التَّحيُّز والتَّركيز على جودة النَّص وحده. (Tvina et al., 2019, p. 1082).  وكما هو الحال في جميع أنواع تحكيم الأقران فإنَّ التحكيم مزدوج التَّعمية لا يخلو من العيوب، فقد يكون إخفاء الهوية أمرًا صعبًا، إن لم يكن مستحيلًا، بالنِّسبة لبعض الباحثين العاملين في مجال متخصِّص.  **المصطلحات ذات الصِّلة:** تَّحيُّز الشَّخصنة، تحيُّز الانتماء ، المراجعة المجهولة، المراجعة المخفيَّة، التّحكيم المفتوح، تحكيم الأقران، التّحكيم أحاديَّ التَّعمية، المراجعة التَّقليديَّة، التّحكيم ثُّلاثيّ التعمية.",
                "Related_terms": "** Ad hominem bias; Affiliation bias; Anonymous review; Masked review; Open peer review; Peer review; Single-blind peer review; Traditional peer review; Triple-Blind peer review"
            },
            {
                "Title": "Double consciousness (الوعي المزدوج) *",
                "Definition": "** An identity confusion, as the individual feels like they have two distinct identities. One is to assimilate to the dominant culture at university when the individual is with colleagues and professors, while the other is when the individual is with their families. This continuous shift may cause a lack of certainty about the individual’s identity and a belief that the individual does not fully belong anywhere. This lack of belonging can lead to poor social integration within the academic culture that can manifest in less opportunities and more mental health issues in the individual (Rubin, 2021; Rubin et al., 2019).  **",
                "Reference(s)": "** Albayrak and Okoroji (2019); Du Bois (1968); Gilroy (1993)",
                "Drafted by": "** Nihan Albayrak-Aydemir",
                "Reviewed (or Edited) by": "** Mahmoud Elsherif; Wanyin Li; Michele C. Lim; Adam Parker",
                "Translated by": "** Sarah Almutairi",
                "Translation reviewed by": "** Ali H. Al-Hoorie, Hala Alghamdi, Ahlam Ahmed, Mohammed Mohsen",
                "Translation": "التَّعريف:** حالة ارتباك في الهُويَّة، والتي يشعر خلالها الفرد بأنَّ لديه هويتين مختلفتين، هوية تسعى للاندماج مع الثَّقافة السَّائدة في المحيط الجامعي عندما يكون الفرد بين زملائه وأساتذته، وهوية أخرى عندما يكون بين أفراد عائلته.  قد يتسبب هذا التَّبادل المستمرّ بين الهويتين إلى ضعف ثقة الفرد بهويته واعتقاده بأنَّه لا ينتمي  إلى أي جهة.  ويمكن أن يؤدِّي شعور الفرد بضعف الانتماء إلى عدم قدرته على الاندماج الاجتماعي في الثَّقافة الأكاديميَّة، وهذا بدوره قد يؤدِّي  إلى تقليل فرص الفرد ويؤثر على صحته العقليَّة (Rubin, 2021; Rubin et al., 2019).  **المصطلحات ذات الصِّلة:** الطَّبقة الاجتماعيَّة، الاندماج الاجتماعي",
                "Related_terms": "** Social class; Social integration"
            },
            {
                "Title": "DORA (إعلان سان فرانسيسكو بشأن تقييم البحوث) *",
                "Definition": "** The San Francisco Declaration on Research Assessment (DORA) is a global initiative aiming to reduce dependence on journal-based metrics (e.g. journal impact factor and citation counts) and, instead, promote a culture which emphasises the intrinsic value of research. The DORA declaration targets research funders, publishers, research institutes and researchers and signing it represents a commitment to aligning research practices and procedures with the declaration’s principles. **",
                "Reference(s)": "** Health Research Board (n.d.); [https://sfdora.org/](https://sfdora.org/)",
                "Drafted by": "** Aoife O’Mahony",
                "Reviewed (or Edited) by": "** Connor Keating; Charlotte R. Pennington",
                "Translated by": "** Ruwayshid",
                "Translation reviewed by": "** Ali H. Al-Hoorie, Hiba Alomary, Hala Alghamdi, Mohammed Mohsen",
                "Translation": "التَّعريف:** اختصار لإعلان سان فرانسيسكو لتقييم البحث العلمي، وهو مبادرة عالميَّة تهدف إلى تقليل الاعتماد على مقاييس المجلات العلميَّة، مثل: معامل التأثير أو عدد الاستشهادات وتعزيز الثَّقافة التي تركِّز على القيمة الفعليَّة للبحث. يستهدف إعلان \"دورا\" كلًا من المانحين، والنَّاشرين، والمراكز البحثيَّة، وكذلك الباحثين. والَّتوقيع عليه يمثل تعهدًا بمواءمة الإجراءات والممارسات البحثية مع مبادئ الإعلان. **المصطلحات ذات الصِّة:** القابلية للتَّعميم، معامل تَّأثير المجلَّة، العلم المفتوح.",
                "Related_terms": "** Generalizability; Journal Impact Factor; Open Science"
            },
            {
                "Title": "Direct replication (التِّكرار المباشر) *",
                "Definition": "** As ‘direct replication’ does not have a widely-agreed technical meaning nor there is no clear cut distinction between a direct and conceptual replication, below we list several contributions towards a consensus. Rather than debating the ‘exactness’ of a replication, it is more helpful to discuss the relevant differences between a replication and its target, and their implications for the reliability and generality of the target’s results. Generally, direct replication refers to a new data collection that attempts to replicate original studies’ methods as closely as possible. A replication attempt that “seek(s) to duplicate the necessary elements that produced the original finding” (Cruwell et al., 2019, p. 243). The purpose of a direct replication can be to identify type 1 errors and/or experimenter effects, determine the replicability of an effect using the same or improved practices, or to create more specific estimates of effect size (Hűffmeier et al., 2016). Directness of replication is a continuum between repeating specific observations (data) and observing generalised effects (phenomena). How closely a replication replicates an original study is often a matter for debate, often with differences being cited as hidden moderators of effects. Furthermore, there can be debate over the relevant importance of technical equivalence (i.e., using identical materials) versus psychological equivalence (i.e., realizing the identical psychological conditions) to the original study (Schwarz and Strack, 2014). For example, consider a study on Trust in the US- President conducted in 2018\\. A technical equivalent replication would use Trump as stimulus (he was president in 2018\\) a psychological equivalent study would use Biden (he is the current president).  **",
                "Reference(s)": "** Crüwell et al. (2019); Hüffmeier et al. (2016); LeBel et al. (2019); Schwarz and Strack (2014)",
                "Drafted by": "**Mahmoud Elsherif (original); Thomas Rhys Evans (alternative); Tina Lonsdorf (alternative)",
                "Reviewed (or Edited) by": "** Beatrix Arendt; Adrien Fillon; Matt Jaquiery; Charlotte R. Pennington; Graham Reid; Lisa Spitzer; Tobias Wingen; Flávio Azevedo",
                "Translated by": "** Hala Alghamdi",
                "Translation reviewed by": "** Ali H. Al-Hoorie, Ahlam Ahmed, Mohammed Mohsen Term coined: No",
                "Translation": "التَّعريف**: نظرًا لأنَّ مصطلح \"التِّكرار المباشر\" ليس له معنى تقني محدَّد متَّفق عليه على نطاق واسع، كما أنَّه لا يوجد تمييز واضح بين التِّكرار المباشر، والتِّكرار المفاهيمي، فإنَّنا ندرج أدناه العديد من المساهمات التي بُذلت للوصول إلى معنى متّفق عليه.  بدلًا من مناقشة \"دقّة\" التِّكرار، من المفيد أكثر مناقشة الاختلافات ذات الصِّلة بين الدِّراسة المكرَّرة والدِّراسة الأصليَّة، وتأثيرات ذلك على موثوقيَّة، وعموميَّة نتائج الدِّراسة الأصليَّة. بشكل عام، يشير التِّكرار المباشر إلى عملية جمع بيانات جديدة بهدف تكرار الطُّرق البحثيَّة للدِّراسات الأصليَّة بأكبر قدر ممكن، وهي محاولة \"تسعى إلى تكرار العناصر الضَّروريَّة التي أنتجت النَّتيجة الأصليَّة (Cruwell et al., 2019, p. 243). قد يكون الغرض من التِّكرار المباشر تحديد أخطاء النَّوع الأول، أو التَّأثيرات المتعلِّقة بالمُختبِر، أو تحديد إمكانيَّة تكرار نتيجة ما باستخدام نفس الممارسات البحثية، أو باستخدام ممارسات أفضل، أو إيجاد تقديرات أكثر دِقَّة لحجم التَّأثير (Hűffmeier et al., 2016). إنَّ مدى كون الدِّراسة التِّكراريَّة مباشرة تتراوح بين مجرَّد تكرار لملاحظات (بيانات) محدَّدة وبين ملاحظة التَّأثيرات المعمَّمة الظواهر. إنَّ مدى قرب الدِّراسة التِّكراريَّة للدِّراسة الأصليَّة هو غالبًا محل نقاش، وغالبًا ما يتم ذكر أنَّ الاختلافات تمثل عوامل وسيطة مخفيَّة. إضافة إلى ذلك، قد يكون هناك جدل حول الأهميَّة التَّقريبية للتَّكافؤ المنهجي (أي استخدام مواد متطابقة) للدِّراسة الأصليَّة مقابل التَّكافؤ النَّفسي (أي إدراك الظُّروف النَّفسيَّة المتطابقة) للدِّراسة الأصليَّة (Schwarz and Strack, 2014). على سبيل المثال، لننظر إلى دراسة حول الثِّقة في \\-رئيس الولايات المتَّحدة والتي أجريت في عام 2018\\. إنَّ التِّكرار المنهجي المتطابق سيقوم باستخدام ترامب كمحفز (كان رئيسًا في عام 2018)، أما التِّكرار النَّفسي المتطابق سيقوم باستخدام بايدن (وهو الرئيس الحالي). **المصطلحات ذات الصِّلة:** التِّكرار الوثيق، التِّكرار المفاهيمي، التِّكرار، العوامل الوسيطة الخفيَّة.",
                "Related_terms": "** close replication; Conceptual replication; exact replication; hidden moderators"
            },
            {
                "Title": "Diversity (التَّنوع) *",
                "Definition": "** Diversity refers to between-person (i.e., interindividual) variation in humans, e.g. ability, age, beliefs, cognition, country, disability, ethnicity, gender, language, race, religion or sexual orientation. Diversity can refer to diversity of researchers (who do the research), the diversity of participant samples (who is included in the study), and diversity of perspectives (the views and beliefs researchers bring into their work; Syed & Kathawalla, 2020).  **",
                "Reference(s)": "** Syed and Kathawalla (2020)",
                "Drafted by": "** Ryan Millager; Mariella Paul",
                "Reviewed (or Edited) by": "** Nihan Albayrak-Aydemir; Mahmoud Elsherif; Helena Hartmann; Madeleine Ingham; Annalise A. LaPlume; Wanyin Li; Charlotte R. Pennington; Olly Robertson; Flávio Azevedo",
                "Translated by": "** Naif Masrahi",
                "Translation reviewed by": "** Ali H. Al-Hoorie, Hala Alghamdi, Ahlam Ahmed, Mohammed Mohsen  ### **E** {#e}",
                "Translation": "التَّعريف:** يشير التَّنوع إلى الاختلاف بين الأشخاص (أي الأفراد من البشر)، على سبيل المثال من حيث القدرة، أو العمر، أو المعتقدات، أو الإدراك، أو البلد، أو الإعاقة، أو الثَّقافة، أو الجنس، أو اللُّغة، أو العِرق، أو الدِّين، أو الميول الجنسيَّة.  يمكن أنْ يشيرَ التَّنوع إلى تنوع الباحثين (الذين يقومون بالبحث)، وتنوع عيّنات المشاركين (الذين يتم تضمينهم في الدِّراسة)، وتنوع وجهات النَّظر (الآراء والمعتقدات التي يجلبها الباحثون في عملهم (Syed & Kathawalla, 2020).  **المصطلحات ذات الصِّلة:** بروبينساينس، بيزار، مقاومة الاستعمار، الوعي المزدوج، العدالة، الشُّمول،  سترينج: الخلفية الاجتماعية والقابلية للتتبع والاختيار الذاتي وتاريخ التربية والتأقلم والتّعود، وِيرد",
                "Related_terms": "** Bropenscience; BIZARRE; Decolonisation; Double Consciousness; Equity; Inclusion; STRANGE; WEIRD"
            },
            {
                "Title": "Early career researchers (ECRs) (الباحثون المبتدئون) *",
                "Definition": "** A label given to researchers who “range from senior doctoral students to postdoctoral workers who may have up to 10 years postdoctoral education; the latter group may therefore include early career or junior academics” (Eley et al., 2012, p. 3). What specifically (e.g. age, time since PhD inclusive or exclusive of career breaks and leave, title, funding awarded) constitutes an ECR can vary across funding bodies, academic organisations, and countries.  **",
                "Reference(s)": "** Bazeley (2003); Eley et al. (2012); Pownall et al (2021)",
                "Drafted by": "** Micah Vandegrift",
                "Reviewed (or Edited) by": "** Thomas Rhys Evans; Sam Parsons; Olly Robertson; Suzanne L. K. Stewart; Flávio Azevedo **Translated by** Naif Masrahi **** Ali H. Al-Hoorie, Hala Alghamdi, Ahlam Ahmed, Mohammed Mohsen",
                "Translation": "التَّعريف:** هو تصنيف يُمنح للباحثين الذين هم من الطَّلبة في السِّنين الأخيرة من الدُّكتوراة إلى العاملين في مرحلة ما بعد الدُّكتوراة والذين قد تصل خبرتهم إلى 10 سنوات من التَّعليم بعد الدُّكتوراة؛ ولذلك قد تشمل المجموعة الأخيرة الأكاديميين في بداية حياتهم المهنيَّة، أو الأكاديميين المبتدئين\" (Eley et al., 2012, p. 3).  يختلف تحديد من ينطبق عليهم هذا التَّصنيف باختلاف هيئات التَّمويل، والمؤسسات الأكاديميَّة، والبلدان، مثلًا: فيما يتعلَّق بالعمر، وما إذا كانت الفترة منذ الحصول على درجة الدُّكتوراة شاملة فترات الإجازات، والتَّفرُّغ، أم لا، والمسمّى الوظيفي، والتَّمويل الممنوح. **المصطلحات ذات الصِّلة:** باحث في بداية حياته المهنيَّة.",
                "Related_terms": "** Early Career Investigator"
            },
            {
                "Title": "Economic and societal impact (الأثر الاقتصادي والاجتماعي) *",
                "Definition": "** The contribution a research item makes to the broader economy and society. It also captures the benefits of research to individuals, organisations, and/or nations.  **",
                "Reference": "** https://esrc.ukri.org/research/impact-toolkit/what-is-impact/",
                "Originally drafted by": "** Adam Parker",
                "Reviewed (or Edited) by": "** Helena Hartmann; Aoife O’Mahony; Charlotte R. Pennington",
                "Translated by": "** Asma Alzahrani",
                "Translation reviewed by": "** Ali H. Al-Hoorie, Hala Alghamdi, Ahlam Ahmed, Mohammed Mohsen",
                "Translation": "التَّعريف**: المساهمة التي يقدمها بحث ما للاقتصاد، والمجتمع بشكل عام.  كما يُجسد هذا المصطلح فوائد البحث عن الأفراد والمؤسسات و الدُّول. **المصطلحات ذات الصِّلة**: التَّأثير الأكاديمي.",
                "Related_terms": "** Academic Impact"
            },
            {
                "Title": "Embargo Period (فترة الحظر) *",
                "Definition": "** Applied to Open Scholarship, in academic publishing, the period of time after an article has been published and before it can be made available as Open Access. If an author decides to self-archive their article (e.g., in an Open Access repository) they need to observe any embargo period a publisher might have in place. Embargo periods vary from instantaneous up to 48 months, with 6 and 12 months being common (Laakso & Björk, 2013). Embargo periods may also apply to pre-registrations, materials, and data, when authors decide to only make these available to the public after a certain period of time, for instance upon publication or even later when they have additional publication plans and want to avoid being scooped (Klein et al., 2018).  **",
                "Reference": "** Klein et al. (2018), Laakso and Björk (2013); [https://en.wikipedia.org/wiki/Embargo\\_(academic\\_publishing)](https://en.wikipedia.org/wiki/Embargo_\\(academic_publishing\\))",
                "Originally drafted by": "** Aleksandra Lazić",
                "Reviewed (or Edited) by": "** Bradley Baker; Adam Parker; Sam Parsons; Steven Verheyen; Flávio Azevedo",
                "Translated by": "** Asma Alzahrani",
                "Translation reviewed by": "** Ali H. Al-Hoorie, Hala Alghamdi, Ahlam Ahmed , Mohammed Mohsen",
                "Translation": "التَّعريف**: في سياق العلم المفتوح في النَّشر الأكاديمي، تُعرّف فترة الحظر بالفترة الزَّمنيَّة التي تلي نشر المقالة وقبل أنْ تصبح متاحةً للوصول المفتوح.  إذا قرَّر المؤلِّف أرشفة مقالته ذاتيًا،على سبيل المثال: في مستودع للوصول المفتوح، فيتوجَّب عليه مراعاة أي فترة حظر يفرضها النَّاشر.  وتختلف فترات الحظر من فورية إلى 48 شهرًا، وتتراوح أغلبها بين 6 و12 شهرًا (Laakso & Björk, 2013). يمكن أن تنطبق فترات الحظر أيضًا على التَّسجيلات المسبقة، والمواد، والبيانات.  عندما يقرر المؤلِّفون إتاحتها للجمهور فقط بعد فترة زمنية معيَّنة، على سبيل المثال: عند النَّشر، أو حتى في وقت لاحق عندما يكون لدى المؤلِّف خطط نشر إضافية ويرغب في تجنب أن يتم سبقه (Klein et al., 2018).  **المصطلحات ذات الصِّلة**: الوصول المفتوح، حاجز مالي، الطّباعة الأوليَّة.",
                "Related_terms": "** Open access; Paywall; Preprint"
            },
            {
                "Title": "Epistemic uncertainty (اللايقين المعرفي) *",
                "Definition": "** Systematic uncertainty due to limited data, measurement precision, model or process specification, or lack of knowledge. That is, uncertainty due to lack of knowledge that could, in theory, be reduced through conducting additional research to increase understanding. Such uncertainty is said to be personal, since knowledge differs across scientists, and temporary since it can change as new data become available. **",
                "Reference(s)": "** Der Kiureghian and Ditlevsen (2009); Ferson et al., (2004) **Alternative terms:** Epistemic uncertainty is also known as knowledge uncertainty, subjective uncertainty, or type B uncertainty.",
                "Drafted by": "** Bradley Baker",
                "Reviewed (or Edited) by": "** Jamie P. Cockcroft; Elizabeth Collins; Charlotte R. Pennington; Graham Reid",
                "Translated by": "** Ahlam Ahmed",
                "Translation reviewed by": "** Ali H. Al-Hoorie, Hiba Alomary, Hala Alghamdi, Asma Alzahrani, Mohammed Mohsen",
                "Translation": "التَّعريف:** عدم اليقين المنهجي ناتج عن محدوديَّة البيانات، أو دِقَّة القياس، أو مواصفات النَّموذج، أو العملية، أو نقص المعرفة. ويمكن نظريًا تقليل عدم اليقين النَّاتج بسبب نقص المعرفة من خلال إجراء بحوث إضافيَّة لزيادة الفهم. يذهب البعض إلى أنَّ عدم اليقين أمر شخصي؛ لأنَّ المعرفة تختلف بين العلماء، وكما أنَّه مؤقت؛ لأنَّه يمكن أنْ يتغيَّر مع توفُّر بيانات جديدة. **المصطلحات ذات الصِّلة:** الاحتمال العشوائي، عدم اليقين النَّايتي. **مصطلحات بديلة:** يعرف اللا يقين المعرفي أيضًا باسم اللا يقين العلمي، أو اللا يقين الذَّاتي، أو اللا يقين من النَّوع (ب)",
                "Related_terms": "** Aleatoric uncertainty; Knightian uncertainty"
            },
            {
                "Title": "Epistemology (نظريَّة المعرفة) *",
                "Definition": "** Alongside ethics, logic, and metaphysics, epistemology is one of the four main branches of philosophy. Epistemology is largely concerned with nature, origin, and scope of knowledge, as well as the rationality of beliefs.  **",
                "Reference": "** Steup et al. (2020)",
                "Originally drafted by": "** Amélie Beffara Bret",
                "Reviewed (or Edited) by": "** Emma Norris; Adam Parker; Robert M Ross; Steven Verheyen",
                "Translated by": "** Mahdi Aben Ahmed",
                "Translation reviewed by": "** Amani Aloufi, Ali H. Al-Hoorie, Hiba Alomary, Asma Alzahrani, Mohammed Mohsen",
                "Translation": "التَّعريف:** تعدُّ نظريَّة المعرفة أحد الفروع الأربعة الرَّئيسيَّة للفلسفة، إلى جانب أخلاقيات البحث والمنطق والميتافيزيا. تهتم نظرية المعرفة إلى حد كبير بطبيعة المعرفة، وأصلها، ونطاقها، إضافة إلى عقلانيَّة المعتقدات.  **المصطلحات ذات الصِّلة:** العلوم البعدية/التلوية أو البحث البعدي/التلوي، علم الوجود (الذَّكاء الاصطناعي)",
                "Related_terms": "** Meta-science or Meta-research ; Ontology (Artificial Intelligence)"
            },
            {
                "Title": "Equity (العدالة) *",
                "Definition": "** Different individuals have different starting positions (cf. “opportunity gaps”) and needs. Whereas equal treatment focuses on treating all individuals *equally*, equitable treatment aims to level the playing field by actively increasing opportunities for under-represented minorities. Equitable treatment aims to attain equality through “fairness”: taking into account different needs for support for different individuals, instead of focusing merely on the needs of the majority.  **",
                "Reference(s)": "** Albayrak-Aydemir (2020); Posselt (2020)",
                "Drafted by": "** Gisela H. Govaart",
                "Reviewed (or Edited) by": "** Nihan Albayrak-Aydemir; Mahmoud Elsherif; Ryan Millager; Charlotte R. Pennington; Beatrice Valentini; Flávio Azevedo",
                "Translated by": "Asma Alzahrani**",
                "Translation reviewed by": "** Ali H. Al-Hoorie, Hala Alghamdi, Ahlam Ahmed, Mohammed Mohsen",
                "Translation": "التَّعريف:** الأفراد المختلفون لديهم بدايات مختلفة (راجع \"فجوات الفرص\") واحتياجات مختلفة. ففي حين تركز المعاملة المتساوية على معاملة جميع الأفراد بنفس الدرجة، فإنَّ المعاملة المنصفة تهدف إلى تحقيق تكافؤ الفرص من خلال زيادة الفرص المتاحة للأقليات الممثَّلة تمثيلًا ناقصًا.  تهدف المعاملة المنصفة إلى تحقيق المساواة من خلال \"الإنصاف\"، مع مراعاة الاحتياجات المختلفة لدعم الأفراد المختلفين، بدلًا من التَّركيز فقط على احتياجات الأغلبيَّة. **المصطلحات ذات الصِّلة:** التَّنوع، العدالة، الإنصاف، الشُّمول، العدالة الاجتماعية.",
                "Related_terms": "** Diversity; Equality; Fairness; Inclusion; Social justice"
            },
            {
                "Title": "Equivalence Testing (اختبار التَّكافؤ) *",
                "Definition": "** Equivalence tests statistically assess the null hypothesis that a given effect exceeds a minimum criterion to be considered meaningful. Thus, rejection of the null hypothesis provides evidence of a lack of (meaningful) effect. Based upon frequentist statistics, equivalence tests work by specifying equivalence bounds: a lower and upper value that reflect the smallest effect size of interest. Two one-sided *t*\\-tests are then conducted against each of these equivalence bounds to assess whether effects that are deemed meaningful can be *rejected* (see Schuirmann, 1972; Lakens et al., 2018; 2020).  **",
                "Reference": "** Lakens et al. (2018); Lakens et al. (2020); Schuirmann (1987)",
                "Originally drafted by": "** Charlotte R. Pennington",
                "Reviewed (or Edited) by": "** Bradley Baker; James E. Bartlett; Jamie P. Cockcroft; Tobias Wingen; Flávio Azevedo",
                "Translated by": "** Asma Alzahrani",
                "Translation reviewed by": "** Mai Helmy, Ali H. Al-Hoorie, Ahlam Ahmed, Mohammed Mohsen",
                "Translation": "التَّعريف**: تقوم اختبارات التَّكافؤ بتقييم الفرضيَّة الصِّفريَّة إحصائيًا، وهل  لها تأثير معين يتجاوز الحد الأدنى من المعايير التي تعدُّ ذات معنى، وبالتَّالي فإنَّ رفض الفرضيَّة الصِّفرية يعدُّ دليلًا على عدم وجود تأثير (ذي معنى). بناءً على الإحصائيات المتكرِّرة.  تعمل اختبارات التَّكافؤ عبر تحديد حدود التكافؤ، قيمة دنيا وعليا تعكس أصغر حجم تأثير محل الاهتمام.  يتم بعد ذلك إجراء اختبارين \"تي\" أحادي الجانب مقابل كل من حدود التَّكافؤ هذه؛ لتقييم ما إذا كان من الممكن رفض التَّأثيرات التي تعدُّ ذات معنى (Schuirmann, 1972; Lakens et al., 2018; 2020).  **المصطلحات ذات الصِّلة:** حدود التَّكافؤ، التَّزوير، التَّحليلات المتكرِّرة، الاستدلال بفترات الثِّقة، اختبار دلالةالفرضيَّة الصِّفريَّة، أصغر حجم تأثير موضع اهتمام (SESOI)، (TOSTER)، إجراء TOST.",
                "Related_terms": "** Equivalence bounds; Falsification; Frequentist analyses; Inference by confidence intervals; Null Hypothesis Significance Testing (NHST); Smallest effect size of interest (SESOI); TOSTER; TOST procedure."
            },
            {
                "Title": "Error detection (اكتشاف الاخطاء) *",
                "Definition": "** Broadly refers to examining research data and manuscripts for mistakes or inconsistencies in reporting. Commonly discussed approaches include: checking inconsistencies in descriptive statistics (e.g. summary statistics that are not possible given the sample size and measure characteristics; Brown & Heathers, 2017; Heathers et al. 2018), inconsistencies in reported statistics (e.g. *p*\\-values that do not match the reported *F* statistics and accompanying degrees of freedom; Epskamp, & Nuijten, 2016; Nuijten et al. 2016), and image manipulation (Bik et al., 2016). Error detection is one motivation for data and analysis code to be openly available, so that peer review can confirm a manuscript’s findings, or if already published, the record can be corrected. Detected errors can result in corrections or retractions of published articles, though these actions are often delayed, long after erroneous findings have influenced and impacted further research.  **",
                "Reference(s)": "** Bik et al. (2016); Brown and Heathers (2017); Epskamp and Nuijten (2016); Heathers et al. (2018); Nuijten et al. (2016); [https://retractionwatch.com/](https://retractionwatch.com/)",
                "Drafted by": "** William Ngiam",
                "Reviewed (or Edited) by": "** Ali H. Al-Hoorie; Jamie P. Cockcroft; Dominik Kiersz; Sam Parsons; Suzanne L. K. Stewart; Marta Topor",
                "Translated by": "** Ruwayshid",
                "Translation reviewed by": "** Asma Alzahrani, Ali H. Al-Hoorie, Hiba Alomary, Mohammed Mohsen",
                "Translation": "التَّعريف:**  يشير هذا المصطلح بشكل عام إلى فحص البيانات، والنُّصوص البحثيَّة؛ لاكتشاف الأخطاء والتَّناقضات فيها.  من الأساليب الشَّائعة في هذا السِّياق فحص تناقضات الإحصاء الوصفي مثل أنْ يكون الموجز الإحصائي غير ممكنٍ؛ نظرًا لحجم العيّنة، أو خصائص القياس (Brown & Heathers, 2017; Heathers et al. 2018\\) ، أو فحص التَّناقضات في التَّقارير الإحصائيَّة مثل ألّا تتوافق القيمة الاحتمالية مع قيمة F ودرجات الحرّيّة المصاحبة (Epskamp, & Nuijten, 2016; Nuijten et al. 2016), أو التَّلاعب بالصُّور (Bik et al., 2016). يعد اكتشاف الأخطاء أحد الدَّوافع؛ لأنْ تكون البيانات ونص التَّحليل البرمجي متاحًا للجميع؛ لتتمكن عمليّة تحكيم الأقران من التَّأكد من نتائج البحث، أو تصحيح الأخطاء في حال كان البحث منشورًا.  كما أنَّه من الممكن أن تساهم الأخطاء المكتشفة في تصحيح، أو سحب البحوث المنشورة، بالرُّغم من أنَّ هذه الإجراءات عادة ما تتأخر، ولا تحدث إلا بعدما تكون هذة النَّتائج المغلوطة قد أثّرت في البحوث اللَّاحقة.  **المصطلحات ذات الصِّلة:** نزاهة البحث، التَّصحيح، سحب (البحث)",
                "Related_terms": "** Research integrity; correction; retraction"
            },
            {
                "Title": "Evidence Synthesis (توليف الأدلة) *",
                "Definition": "** This is a type of research method which aims to draw general conclusions to address a research question on a certain topic, phenomenon or effect by reviewing research outcomes and information from a range of different sources. Information which is subject to synthesis can be extracted from both qualitative and quantitative studies. The method used to synthesise the gathered information can be qualitative (narrative synthesis), quantitative (meta-analysis) or mixed (meta-synthesis, systematic mapping). Evidence synthesis has many applications and is often used in the context of healthcare, public policy as well as understanding and advancement of specific research fields. **",
                "Reference": "** Centre for Evaluation (n.d.); James et al., (2016); Siddaway et al. (2019)",
                "Originally drafted by": "** Marta Topor",
                "Reviewed (or Edited) by": "** Aoife O’Mahony; Tamara Kalandadze; Adam Parker; Charlotte R. Pennington",
                "Translated by": "** Naif Masrahi",
                "Translation reviewed by": "** Hiba Alomary, Ali H. Al-Hoorie, Hala Alghamdi, Asma Alzahrani, Mohammed Mohsen",
                "Translation": "التعريف**: هو نوع من مناهج   البحث و يهدف إلى استخلاص استنتاجات عامة لمعالجة سؤال بحثي حول موضوع أو ظاهرة أو تأثير معين من خلال مراجعة النتائج البحثية والمعلومات من مجموعة من المصادر المختلفة كما يمكن استخلاص المعلومات الخاضعة للتوليف من الدراسات النوعية والكمية. يمكن توليف هذه المعلومات بشكل كيفي (توليف سردي)، أو كمي (تحليل بعدي) أو بشكل مختلط (توليف بعدي، رسم خرائط منهجي). يحتوي توليف (تجميع) الأدلة على العديد من التطبيقات وغالبًا ما يستخدم في سياق الرعاية الصحية والسياسة العامة، ويُستخدم أيضًا لفهم وتطوير مجالات بحثية محددة. **المصطلحات ذات الصلة**: مراجعة الأدبيات،  التحليل البعدي،  التركيب البعدي،  العلوم البعدية/التلوية  أو البحث البعدي/التلوي؛  مراجعة السرد، مراجعة النطاق، خريطة منهجية، المراجعة المنهجية",
                "Related_terms": "** Literature Review; Meta-analysis; Meta-synthesis; Meta-science or Meta-research; Narrative review; Scoping review; Systematic map; Systematic review"
            },
            {
                "Title": "Exploratory data analysis (تحليل البيانات الاستكشافيّ)",
                "Definition": "** Exploratory Data Analysis (EDA) is a well-established statistical tradition that provides conceptual and computational tools for discovering patterns in data to foster hypothesis development and refinement. These tools and attitudes complement the use of hypothesis tests used in confirmatory data analysis (CDA). Even when well-specified theories are held, EDA helps one interpret the results of CDA and may reveal unexpected or misleading patterns in the data.  **",
                "Reference(s)": "** Behrens (1997); Box (1976); Tukey (1977); Wagenmakers (2012)",
                "Drafted by": "** Jenny Terry",
                "Reviewed (or Edited) by": "** Helena Hartmann; Timo Roettger; Charlotte R. Pennington; Flávio Azevedo",
                "Translated by": "** Hala Alghamdi",
                "Translation reviewed by": "** Amani Aloufi, Hiba Alomary, Ali H. Al-Hoorie, Asma Alzahrani, Mohammed Mohsen",
                "Translation": "التَّعريف:** هو تقليد إحصائي راسخ، يوفر أدوات مفاهيميَّة وحسابيَّة؛ لاستكشاف الأنماط في البيانات، وذلك لتعزيز تطوير الفرضيّات وتحسينها.  تُكمّل هذه الأدوات والتَّوجهات اختبار الفرضيَّات التي تستعمل في تحليل البيانات التّأكيدي، وحتى في سياق النَّظريات المحدَّدة والمعتبرة.  يساعد تحليل البيانات الاستكشافي في تفسير نتائج تحليل البيانات التَّأكيدي، و قد يكشف عن أنماط غير متوقّعة، أو مضّللة في البيانات.  **المصطلحات ذات الصِّلة:** التَّحليلات التَّوكيديَّة، البحث القائم على البيانات، البحث الاستكشافيّ.",
                "Related_terms": "** Confirmatory analyses; Data-driven research; Exploratory research"
            },
            {
                "Title": "External Validity (الصّدق الخارجي) *",
                "Definition": "** Whether the findings of a scientific study can be generalized to other contexts outside the study context (different measures, settings, people, places, and times). Statistically, threats to external validity may reflect interactions whereby the effect of one factor (the independent variable) depends on another factor (a confounding variable). External validity may also be limited by the study design (e.g., an artificial laboratory setting or a non-representative sample).  **",
                "Reference": "** Cook and Campbell (1979); Lynch (1982); Steckler and McLeroy (2008) **Alternative definition:** (if applicable) In Psychometrics, the degree of evidence that confirms the relations of a tested psychological construct with external variables **Related terms to alternative definition:** Criterion validity; Convergent validity; Divergent validity",
                "Reference(s)": "** Messick (1995)",
                "Originally drafted by": "** Annalise A. LaPlume",
                "Reviewed (or Edited) by": "** Mahmoud Elsherif; Helena Hartmann; Kai Krautter; Oscar Lecuona; Flávio Azevedo",
                "Translated by": "** Hala Alghamdi, Asma Alzahrani **** Amani Aloufi, Hiba Alomary, Ali H. Al-Hoorie, Mohammed Mohsen  ###  ### **F** {#f}",
                "Translation": "التعريف:** يشير إلى إمكانية تعميم نتائج دراسة علمية ما على سياقات أخرى خارج سياق الدراسة (مثلا مقاييس مختلفة، بيئات مختلفة، أشخاص مختلفين، بيئات مختلفة، أوقات مختلفة). من الناحية الإحصائية، قد يكون ما يهدد الصدق الخارجي تفاعلات معينة مثل أن يكون تأثير عامل معين (المتغير المستقل) معتمدا على عامل آخر (المتغير الدخيل). قد يكون الصدق الخارجي محدوداً بسبب تصميم الدراسة (ككونها في بيئة معملية، كون عينة الدراسة لا تمثل المجتمع).  **المصطلحات ذات العلاقة:**  قيود التعميم، الصدق الداخلي، القابلية للتعميم، التمثيل، الصدق **التعريف البديل:** في سياق علم القياس النفسي، فإن الصدق الخارجي هو درجة الأدلة التي تؤكد العلاقات بين بنية نفسية تم اختبارها والمتغيرات الخارجية. **المصطلحات المرتبطة بالتعريف البديل:** صدق المحك؛ الصلاحية التقاربية؛ الصلاحية الانفصالية",
                "Related_terms": "** Constraints on Generality (COG); Internal validity; Generalizability; Representativity; Validity"
            },
            {
                "Title": "Face validity (الصِّدق الظاهري) *",
                "Definition": "** A subjective judgement of how suitable a measure appears to be on the surface, that is, how well a measure is operationalized. For example, judging whether questionnaire items should relate to a construct of interest at face value. Face validity is related to construct validity, but since it is subjective/informal, it is considered an easy but weak form of validity.  **",
                "Reference": "** Holden (2010)",
                "Originally drafted by": "** Annalise A. LaPlume",
                "Reviewed (or Edited) by": "** Helena Hartmann; Kai Krautter; Adam Parker; Sam Parsons",
                "Translated by": "** Hala Alghamdi",
                "Translation reviewed by": "** Amani Aloufi, Hiba Alomary, Mahdi Aben Ahmed, Ali H. Al-Hoorie, Mohammed Mohsen",
                "Translation": "التَّعريف**: هو الحكم الشَّخصي على مدى ملاءمة مقياس معيّن بحسب ظاهره، وتحديداً، إلى أي مدى يمكن تقنينه. فعلى سبيل المثال، يشمل هذا الحكم على ما إذا كانت أسئلة الاستبيان في ظاهرها ترتبط بالظاهرة المراد دراستها.  يعدُّ الصِّدق الظَّاهري \\- وإن كان له علاقة بالصِّدق البنائي \\- شكلًا سهلًا وضعيفًا من أشكال الصِّدق لأنَّه حكم شخصي، وغير رسمي.  **المصطلحات ذات الصِّلة:** الصدق البنائي، صدق المحتوى، الصِّدق المنطقي، قابليَّة التَّنفيذ،  الصِّدق.",
                "Related_terms": "** Construct Validity; Content Validity; Logical Validity; Operationalization; Validity"
            },
            {
                "Title": "FAIR principles (مبادئ فير) *",
                "Definition": "** Describes making scholarly materials Findable, Accessible, Interoperable and Reusable (FAIR). ‘Findable’ and ‘Accessible’ are concerned with where materials are stored (e.g. in data repositories), while ‘Interoperable’ and ‘Reusable’ focus on the importance of data formats and how such formats might change in the future.  **",
                "Reference(s)": "** Crüwell et al. (2019); Wilkinson et al. (2016); [https://www.go-fair.org/fair-principles/](https://www.go-fair.org/fair-principles/)",
                "Drafted by": "** Sonia Rishi",
                "Reviewed (or Edited) by": "** Mahmoud Elsherif; Sam Parsons; Charlotte R. Pennington",
                "Translated by": "** Sarah Almutairi",
                "Translation reviewed by": "** Amani Aloufi, Hiba Alomary, Ali H. Al-Hoorie, Asma Alzahrani, Mohammed Mohsen",
                "Translation": "تعريف:** الاختصار \"فير\" مستمد من أربعة مبادئ تصف المواد العلميَّة على كونها يمكن العثور عليها، سهلة الوصول، قابلة للتَّكامل، ولإعادة الاستعمال.  يركز المبدآن الأوليان على أهميَّة المكان الذي تٌحفظ فيه المواد (مثل مستودعات البيانات)، بينما يركز المبدآن الأخيران على أهميَّة ترتيب وتنسيق البيانات، وكيفيَّة خضوع هذه التَّنسيقات للتَّغير مستقبلًا.  **المصطلحات ذات الصِّلة:** البيانات الوصفيَّة، الوصول المفتوح، النص البرمجي المفتوح، البيانات المفتوحة، المواد المفتوحة، مستودع البيانات.",
                "Related_terms": "** Metadata; Open Access; Open Code; Open Data; Open Material; Repository"
            },
            {
                "Title": "Feminist psychology (علم النَّفس النَّسوي) *",
                "Definition": "** With a particular focus on gender and sexuality, feminist psychology is inherently concerned with representation, diversity, inclusion, accessibility, and equality. Feminist psychology initially grew out out of a concern for representing the lived experiences of girls and women, but has since evolved into a more nuanced, intersectional and comprehensive concern for all aspects of equality (e.g., Eagly & Riger, 2014). Feminist psychologists have advocated for more rigorous consideration of equality, diversity, and inclusion within Open Science spaces (Pownall et al., 2021). **",
                "Reference": "** Eagly and Riger (2014); Grzanka (2020); Pownall et al (2021)",
                "Originally drafted by": "** Madeleine Pownall",
                "Reviewed (or Edited) by": "** Mahmoud Elsherif; Helena Hartmann; Kai Krautter; Charlotte R. Pennington",
                "Translated by": "** Hiba Alomary",
                "Translation reviewed by": "** Mahdi Aben Ahmed, Ahlam Ahmed, Ali H. Al-Hoorie, Mohammed Mohsen",
                "Translation": "تعريف**: يهتم علم النَّفس النَّسوي بالتَّمثيل، والتَّنوع، والشُّموليَّة، والمساواة، ويركز على الجنسانيَّة والجندرية. نشأ علم النَّفس النَّسوي ليمثّل التَّجارب الحياتيَّة للفتيات والنِّساء، ولكنَّه تطوَّر منذ ذلك الحين إلى اهتمام أكثر دقَّة وتقاطعًا وشمولاً لجميع جوانب المساواة (Eagly & Riger, 2014). وقد دعا علماء النَّفس النَّسويون إلى النَّظر بشكل أكثر صرامة في المساواة، والتَّنوع، والشُّمول في مجالات العلوم المفتوحة (Pownall et al., 2021). **المصطلحات ذات الصِّلة**: الشُّمول، الموضعيَّة، الانعكاسيَّة، نقص التَّمثيل، العدالة.",
                "Related_terms": "** Inclusion; Positionality; Reflexivity; Under-representation; Equity"
            },
            {
                "Title": "First-last-author-emphasis norm (FLAE) (مبدأ التَّأكيد على المؤلفَين الأول والأخير ) *",
                "Definition": "** An authorship system that assigns the order of authorship depending on the contributions of a given author while simultaneously valuing the first and last position of the authorship order most. According to this system, the two main authors are indicated as the first and last author \\- the order of the authors between the first and last position is determined by contribution in a descending order.  **",
                "Reference": "** Tscharntke et al. (2007)",
                "Originally drafted by": "** Myriam A. Baum",
                "Reviewed (or Edited) by": "** Charlotte R. Pennington",
                "Translated by": "** Ahlam Ahmed",
                "Translation reviewed by": "** Amani Aloufi, Mahdi Aben Ahmed, Hiba Alomary, Ali H. Al-Hoorie, Mohammed Mohsen",
                "Translation": "التَّعريف:** يحدِّد هذا المبدأ ترتيب المؤلِّفين بناءً على مساهمات كل مؤلِّف، ولكن مع تقدير أكبر للمؤلّفَين الأول والأخير. فحسب هذا المبدأ، يكون المؤلفان الرئيسيان هما الأول والأخير، بينما يُحدّد ترتيب باقي المؤلفِين الواقعين بينهما بحسب إسهاماتهم في التَّأليف.  **المصطلحات ذات اِّلصلة:** التأليف،  مساهمات المؤلِّف، تصنيف أدوار المؤلِّفين",
                "Related_terms": "** Authorship; Author contributions; CreDit taxonomy"
            },
            {
                "Title": "FORRT (فورت) *",
                "Definition": "** Framework of Open Reproducible Research and Teaching. It aims to provide a pedagogical infrastructure designed to recognize and support the teaching and mentoring of open and reproducible research in tandem with prototypical subject matters in higher education. FORRT strives to be an effective, evolving, and community-driven organization raising awareness of the pedagogical implications of open and reproducible science and its associated challenges (i.e., curricular reform, epistemological uncertainty, methods of education). FORRT also advocates for the opening of teaching and mentoring materials as a means to facilitate access, discovery, and learning to those who otherwise would be educationally disenfranchised.  **",
                "Reference(s)": "** [FORRT \\- Framework for Open and Reproducible Research Training](https://forrt.org/); F (2019)",
                "Drafted by": "** Tamara Kalandadze",
                "Reviewed (or Edited) by": "** Mahmoud Elsherif; Charlotte R. Pennington; Flávio Azevedo",
                "Translated by": "** Mahdi Aben Ahmed",
                "Translation reviewed by": "** Hiba Alomary, Ahlam Ahmed, Ali H. Al-Hoorie , Asma Alzahrani, Mohammed Mohsen",
                "Translation": "التَّعريف:**  اختصار لإطار البحث، والتَّدريس المفتوح القابل للتِّكرار الذي يهدف إلى توفير بنية تحتية تربويَّة مصمَّمة للتَّعرُّف على البحوث المفتوحة، والقابلة للتِّكرار ودعم تدريسها وتوجيهها، جنبًا إلى جنب مع الموضوعات المتعارف عليها في التَّعليم العالي.  تسعى فورت إلى أن تكون منظَّمة فعَّالة، ومتطوِّرة ومجتمعيَّة تزيد الوعي بالآثار التَّربوية للعلوم المفتوحة والقابلة للتِّكرار والتَّحديّات المرتبطة بها (أي إصلاح المناهج الدِّراسيَّة، واللاليقين المعرفي، وأساليب التَّعليم).  تدعو فورت أيضًا إلى إتاحة مواد التَّدريس، والتّوجيه كوسيلة لتسهيل الوصول، والاكتشاف والتَّعلم للجميع بما فيهم من لا يستطيع  الوصول للمعرفة أو التَّعلم.  **مصطلحات ذات صلة:** دمج مبادئ العلوم المفتوحة والقابلة للتكرار في التعليم العالي",
                "Related_terms": "** Integrating open and reproducible science tenets into higher education"
            },
            {
                "Title": "Free Our Knowledge Platform (منصَّة حرِّر معرفتنا) *",
                "Definition": "** A collective action platform aiming to support the open science movement by obtaining pledges from researchers that they will implement certain research practices (e.g., pre-registration, pre-print). Initially pledges will be anonymous until a sufficient number of people pledge, upon which names of pledges will be released. The initiative is a grassroots movement instigated by early career researchers.  **",
                "Reference(s)": "** [https://freeourknowledge.org/about/](https://freeourknowledge.org/about/)",
                "Drafted by": "** Jamie P. Cockcroft",
                "Reviewed (or Edited) by": "** Ashley Blake; Elizabeth Collins; Mahmoud Elsherif; Sam Parsons; Flávio Azevedo",
                "Translated by": "** Mahdi Aben Ahmed",
                "Translation reviewed by": "** Hiba Alomary, Ahlam Ahmed, Ali H. Al-Hoorie , Asma Alzahrani, Mohammed Mohsen  ### **Looking for something else?**  We have separated the glossary project across several documents, see links below:  Landing page:\t\t\t[Glossary | Phase 2 | Landing page](https://docs.google.com/document/d/1BKzztg7srUeC_2Yn0b7cMbxp_vYMDlOnEYpxg_S2hWs/edit?usp=sharing) Letters A \\- F:\t\t\t[Glossary | Phase 2 | A-F](https://docs.google.com/document/d/1ob__Alxsnx9yqeTpurEY_N_kv56c1iiZyrg0xxf0ftg/edit?usp=sharing) Letters G \\- L:\t\t\t[Glossary | Phase 2 | G-L](https://docs.google.com/document/d/1LP0cEletpNumDmYHFv3seV3gL6OLQHJ7ZDLQ7T3fzNE/edit?usp=sharing) Letters M \\- R: \t\t\t[Glossary | Phase 2 | M - R](https://docs.google.com/document/d/1FVIgUx717G3pBwI17LGS7y22nKMde-zg0I6AKgz4SeQ/edit?usp=sharing) Letters S \\- Z:\t\t\t[Glossary | Phase 2 | S - Z](https://docs.google.com/document/d/1-5CKFciwhB-WfC1DK8Sajyh8a8CgNOYvOoeZpODux8Y/edit?usp=sharing) References: \t\t\t[Glossary | Phase 2 | References](https://docs.google.com/document/d/12_F8kbnl2GP66iBkIejJeX2KnkZ13iC_jj7VVYZMxpA/edit?usp=sharing) Terms not yet defined: \t\t[Glossary | Phase 2 | Terms not yet defined](https://docs.google.com/document/d/16FGodUkNoNrBYyq2JqHJMNzYuZf-QbsigooMpB_ns_E/edit?usp=sharing) Contributors spreadsheet: \t[Glossary Phase 2 tenzing contributions \\[FORRT\\]](https://docs.google.com/spreadsheets/d/1pkvQ2-h_Hr_ZnlfmKYkuJTuYrxlqS2oSqKqoEDYlKlk/edit?usp=sharing)  [FORRT slack](https://join.slack.com/t/forrt/shared_invite/zt-alobr3z7-NOR0mTBfD1vKXn9qlOKqaQ) \\- join us\\! [FORRT email](mailto:FORRTproject@gmail.com) \\- contact us\\!",
                "Translation": "التَّعريف:** منصَّة عمل جماعيَّة تهدف إلى دعم حركة العلوم المفتوحة من خلال الحصول على تعهُّدات من الباحثين بتنفيذ ممارسات بحثيَّة معيَّنة،مثل: التَّسجيل المسبق، والطِّباعة الأوليَّة).  تكون التَّعهدات في البداية مجهولة المصدر حتى يتعهَّد عدد كاف من الأشخاص، وعندها يتم إعلان أسماء المتعهدين.  وهذه المبادرة عبارة عن حركة بحثيَّة أنشأها مجموعة من الباحثين في بداية حياتهم المهنيَّة.  **المصطلحات ذات الصِّلة**: العلم المفتوح، تعهد التَّسجيل المسبق.",
                "Related_terms": "** Open Science; Preregistration Pledge"
            },
            {
                "Title": "Salami slicing (التَّقطيع غير المبرَّر) *",
                "Definition": "** A questionable research/reporting practice strategy, often done *post hoc*, to increase the number of publishable manuscripts by ‘slicing’ up the data from a single study \\- one example of a method of ‘gaming the system’ of academic incentives. For instance, this may involve publishing multiple studies based on a single dataset, or publishing multiple studies from different data collection sites without transparently stating where the data originally derives from. Such practices distort the literature, and particularly meta-analyses, because it is unclear that the findings were obtained from the same dataset, thereby concealing the dependencies across the separately published papers.  **",
                "Reference(s)": "** Fanelli (2018)",
                "Drafted by": "** Mahmoud Elsherif",
                "Reviewed (or Edited) by": "** Sarah Ashcroft-Jones; Adrien Fillon; Helena Hartmann; Matt Jaquiery; Tamara Kalandadze; Charlotte R. Pennington; Graham Reid; Suzanne L. K. Stewart",
                "Translated by": "** Mai Helmy.",
                "Translation reviewed by": "** Ahlam Ahmed, Ali H. Al-Hoorie, Hala Alghamdi, Mohammed Mohsen",
                "Translation": "التَّعريف:** استراتيجيَّة مشبوهة في إجراء البحوث أو إعداد تقارير تهدف لزيادة عدد النُّصوص القابلة للنَّشر عن طريق \"تقسيم\" البيانات من دراسة واحدة. غالبًا ما يتم اتِّخاذ قرار التَّقسيم في وقت متأخِّر من العمليَّة البحثيَّة، وهو أحد الأمثلة على التَّلاعب بنظام الحوافِّز الأكاديميَّة. فمثلًا قد يتضمَّن ذلك نشر دراسات متعدِّدة تستند إلى مجموعة بيانات واحدة، أو نشر دراسات متعدِّدة من مواقع مختلفة لجمع البيانات دون الإشارة بشفافيَّة إلى مصدر البيانات في الأصل. مثل هذه الممارسات تشوِّه الأدبيَّات العلميَّة، وخاصّة التَّحليلات البعديَّة؛ لأنَّه ليس واضحًا ما إذا كانت  النَّتائج مستخلصة من نفس مجموعة البيانات، وبالتَّالي تخفي تبعيَّة هذه الأوراق ببعض كونها منشورة بشكل منفصل.  **المصطلحات ذات الصِّلة:** التَّلاعب بالنِّظام، ممارسات البحث المشكوك فيها، أو ممارسات إعداد التَّقارير المشكوك فيها ، نشر جزئي.",
                "Related_terms": "** Gaming (the system); Questionable Research Practices or Questionable Reporting Practices (QRPs); Partial publication"
            },
            {
                "Title": "Scooping (السَّبق) *",
                "Definition": "** The act of reporting or publishing a novel finding prior to another researcher/team. Survey-based research indicates that fear of being scooped is an important fear-related barrier for data sharing in psychology, and agent-based models suggest that competition for priority harms scientific reliability (Tiokhin et al. 2021).  **",
                "Reference(s)": "** Houtkoop et al. (2018); Laine (2017); Tiokhin et al. (2021)",
                "Drafted by": "** William Ngiam",
                "Reviewed (or Edited) by": "** Ashley Blake; Thomas Rhys Evans; Connor Keating; Graham Reid; Timo Roettger; Robert M. Ross; Flávio Azevedo",
                "Translated by": "** Mai Helmy.",
                "Translation reviewed by": "** Ahlam Ahmed, Ali H. Al-Hoorie, Mohammed Mohsen",
                "Translation": "التَّعريف:** الإبلاغ عن اكتشاف نتيجة جديدة، أو نشرها قبل باحث أو فريق آخر. وتشير الأبحاث المسحيَّة إلى أنَّ الخوف من السَّبق هو عائق مهم مرتبط بالخوف أمام مشاركة البيانات في علم النَّفس، وكما تشير البحوث التي تتبع منهجيَّة النمذجة القائمة على الوكلاء إلا أنَّ التَّنافس على الأولويَّة يضرُّ بالموثوقيَّة العلميَّة (Tiokhin et al., 2021).  **المصطلحات ذات الصِّلة:** الحداثة، البيانات المفتوحة، التَّسجيل المسبق.",
                "Related_terms": "** Novelty; Open data; Preregistration"
            },
            {
                "Title": "Semantometrics (القياسات الدِّلاليَّة) *",
                "Definition": "** A class of metrics for evaluating research using full publication text to measure semantic similarity of publications and highlighting an article’s contribution to the progress of scholarly discussion. It is an extension of tools such as bibliometrics, webometrics, and altmetrics.  **",
                "Reference": "** Herrmannova and Knoth (2016); Knoth and Herrmannova (2014)",
                "Originally drafted by": "** Alaa AlDoh",
                "Reviewed (or Edited) by": "** Jamie P. Cockcroft; Mahmoud Elsherif; Christopher Graham; Charlotte R. Pennington",
                "Translated by": "** Mai Helmy.",
                "Translation reviewed by": "** Ahlam Ahmed, Ali H. Al-Hoorie, Hala Alghamdi, Mohammed Mohsen",
                "Translation": "التَّعريف:** نوع من المقاييس لتقييم البحوث العلميَّة باستخدام نص النَّشر الكامل؛ لقياس التَّشابه الدِّلالي بين البحوث وتسليط الضَّوء على مساهمة البحث في التَّقدُّم العلمي. وهو امتداد لأدوات شبيهة مثل القياسات الببليومتريَّة وقياسات الويب، والمقاييس البديلة.  **المصطلحات ذات الصِّلة:** القياسات الببليومتريَّة، الإسهام",
                "Related_terms": "** Bibliometrics; Contribution(p)"
            },
            {
                "Title": "Sensitive research (البحوث الحسَّاسة) *",
                "Definition": "** Research that poses a threat to those who are or have been involved in it, including the researchers, the participants, and the wider society. This threat can be physical danger (e.g. suicide) or a negative emotional response (e.g. depression) to those who are involved in the research process. For instance, research conducted on victims of suicide, the researcher might be emotionally traumatised by the descriptions of the suicidal behaviours. Indeed, the communication with the victims might also make them re-experience the traumatic memories, leading to negative psychological responses.  **",
                "Reference": "** Lee (1993); Albayrak-Aydemir (2019)",
                "Originally drafted by": "** Nihan Albayrak-Aydemir",
                "Reviewed (or Edited) by": "** Valeria Agostini; Mahmoud Elsherif; Helena Hartmann; Graham Reid",
                "Translated by": "** Awatif Alruwaili",
                "Translation reviewed by": "** Ahlam Ahmed, Ali H. Al-Hoorie، Hala Alghamdi, Mohammed Mohsen",
                "Translation": "التَّعريف:** البحوث التي تشكل تهديدًا لأولئك الذين شاركوا فيها، بما في ذلك الباحثين، والمشاركين والمجتمع بشكل واسع. يمكن أن يكون هذا التَّهديد خطرًا جسديًا (مثل الانتحار) أو استجابة عاطفيَّة سلبيَّة (مثل الاكتئاب) للمشاركين في عملية البحث. على سبيل المثال، في البحوث التي أجريت على ضحايا الانتحار، قد يتعرَّض الباحث لصدمة عاطفيَّة بسبب وصف السُّلوكيَّات الانتحاريَّة، كما أنَّ التَّواصل مع الضَّحايا قد يجعلهم أيضًا يعيدون تجربة الذِّكريات المؤلمة، ممَّا يؤدِّي إلى استجابات نفسيَّة سلبيَّة.  **المصطلحات ذات الصِّلة:** التّعمية.",
                "Related_terms": "** Anonymity"
            },
            {
                "Title": "Sequence-determines-credit approach (SDC) (نهج التَّسلسل الذي يحدِّد المساهمة ) *",
                "Definition": "** An authorship system that assigns authorship order based on the contribution of each author. The names of the authors are listed according to their contribution in descending order with the most contributing author first and the least contributing author last.  **",
                "Reference": "** Schmidt (1987); Tscharntke et al. (2007)",
                "Originally drafted by": "** Myriam A. Baum",
                "Reviewed (or Edited) by": "** Sam Parsons; Charlotte R. Pennington",
                "Translated by": "Mai Helmy.",
                "Translation reviewed by": "** Ahlam Ahmed, Ali H. Al-Hoorie, Hala Alghamdi, Mohammed Mohsen",
                "Translation": "التَّعريف:** نظام تأليف يحدِّد ترتيب التَّأليف بناءً على مساهمة كلِّ مؤلِّف. ويتمّ سرد أسماء المؤلِّفين وفقًا لمساهمتهم بترتيب تنازلي  بدءًا بالمؤلِّف الأكثر مساهمةً وانتهاءً بالمؤلِّف الأقل مساهمة.  **المصطلحات ذات الصِّلة:** التَّأليف، مبدأ التَّأكيد على  المؤلِّفين الأول والأخير",
                "Related_terms": "** Authorship; First-last-author-emphasis norm (FLAE)"
            },
            {
                "Title": "Sherpa Romeo (شيربا روميو) *",
                "Definition": "** An online resource that collects and presents open access policies from publishers, from across the world, providing summaries of individual journal's copyright and open access archiving policies.  **",
                "Reference": "** [https://v2.sherpa.ac.uk/romeo/](https://v2.sherpa.ac.uk/romeo/)",
                "Originally drafted by": "** Aleksandra Lazić",
                "Reviewed (or Edited) by": "** Mahmoud Elsherif; Christopher Graham; Sam Parsons; Martin Vasilev",
                "Translated by": "** Mai Helmy.",
                "Translation reviewed by": "** Ahlam Ahmed, Ali H. Al-Hoorie , Hala Alghamdi, Mohammed Mohsen",
                "Translation": "التَّعريف:** مصدر عبر الإنترنت يجمع، ويعرض سياسات الوصول المفتوح للنَّاشرين من جميع أنحاء العالم، ويقدِّم ملخَّصات لحقوق الطَّبع، والنَّشر، والأرشفة الخاصّة بكلِّ المجلَّة.  **المصطلحات ذات الصِّلة**: فترة الحظر، الوصول المفتوح، حاجز مالي، الطّباعة الأوليَّة، مستودع  البيانات.",
                "Related_terms": "** Embargo period; Open access; Paywall; Preprint; Repository"
            },
            {
                "Title": "Single-blind peer review (التحكيم أحاديَّ التَّعمية) *",
                "Definition": "** Evaluation of research products by qualified experts where the reviewer(s) knows the identity of the author(s), but the reviewer(s) remains anonymous to the author(s).  **",
                "Reference": "** Largent and Snodgrass (2016)",
                "Originally drafted by": "** Bradley Baker",
                "Reviewed (or Edited) by": "** Ashley Blake; Christopher Graham; Helena Hartmann; Graham Reid",
                "Translated by": "** Mai Helmy.",
                "Translation reviewed by": "** Ahlam Ahmed, Ali H. Al-Hoorie, Hala Alghamdi, Mohammed Mohsen",
                "Translation": "التَّعريف:** تقييم الأعمال البحثيَّة من قبل خبراء مؤهلين حيث يعرف المحكّم (أو المحكّمون) هويَّة المؤلِّف (أو المؤلفين)، لكن تبقى هوية المحكّم مجهولة بالنَّسبة للمؤلِّف.  **المصطلحات ذات الصِّلة:** تحكيم مجهول ، التّحكيم مزدوج التَّعمية، التحكيم المخفي ، التّحكيم المفتوح، تحكيم الأقران ، التّحكيم ثَّلاثيَّ التَّعمية.",
                "Related_terms": "** Anonymous review; Double-blind peer review; Masked review; Open Peer Review; Peer review; Triple-blind peer review"
            },
            {
                "Title": "Slow science (العلم البطيء) *",
                "Definition": "** Adopting Open Scholarship practices leads to a longer research process overall, with more focus on transparency, reproducibility, replicability and quality, over the quantity of outputs. Slow Science opposes publish-or-perish culture and describes an academic system that allows time and resources to produce fewer higher-quality and transparent outputs, for instance prioritising researcher time towards collecting more data, more time to read the literature, think about how their findings fit the literature and documenting and sharing research materials instead of running additional studies.  **",
                "Reference(s)": "** http://slow-science.org/; Nelson et al., (2012); Frith (2020)",
                "Drafted by": "** Sonia Rishi",
                "Reviewed (or Edited) by": "** Adrien Fillon; Tamara Kalandadze; Sam Parsons Charlotte R. Pennington; Robert M Ross; Timo Roettger",
                "Translated by": "** Mai Helmy. **** Ahlam Ahmed, Ali H. Al-Hoorie, Hala Alghamdi, Mohammed Mohsen",
                "Translation": "التَّعريف:** يؤدِّي تبني ممارسات المعرفة المفتوحة إلى عمليَّة بحث أطول مدةً في المجمل، مع تركيز أكبر  على الشَّفافيَّة، وقابليَّة التِّكرار، وإعادة الإنتاج، والجودة، بدلًا من كميَّة المخرجات، وتعارض عمليَّة العلم البطيء ثقافة \"النَّشر أو الفناء \" وتصف نظامًا أكاديميًا يوفِّر الوقت، والموارد لإنتاج عدد أقل من المخرجات، ولكنَّها عالية الجودة والشَّفافية. مثلًا تعطى الأولويَّة لوقت الباحث لجمع المزيد من البيانات، والمزيد من الوقت لقراءة الأدبيَّات، والتَّفكير في كيفيَّة تناسب النَّتائج التي توصَّلوا إليها مع الأدبيَّات، وتوثيق ومشاركة المواد البحثيَّة، بدلًا من إجراء دراسات إضافيَّة.  **المصطلحات ذات الصِّلة:** التَّعاون، هيكل الحوافز، النَّشر أو الفناء، ثقافة البحث، جودة البحث.",
                "Related_terms": "** collaboration; Incentive structure; Publish or Perish; research culture; research quality"
            },
            {
                "Title": "Society for Open, Reliable, and Transparent Ecology and Evolutionary biology (SORTEE) (جمعيَّة علم البيئة والبيولوجيا التَّطوريَّة المنفتحة والموثوقة والشَّفافة ) *",
                "Definition": "** SORTEE ([https://www.sortee.org/](https://www.sortee.org/)) is an international society with the aim of improving the transparency and reliability of research results in the fields of ecology, evolution, and related disciplines through cultural and institutional changes. SORTEE was launched in December 2020 to anyone interested in improving research in these disciplines, regardless of experience. The society is international in scope, membership, and objectives. As of May 2021, SORTEE comprises of over 600 members.  **",
                "Reference(s)": "** https://www.sortee.org/",
                "Drafted by": "** Brice Beffara Bret; Dominique Roche",
                "Reviewed (or Edited) by": "** Sarah Ashcroft-Jones; Mahmoud Elsherif; Charlotte R. Pennington; Graham Reid",
                "Translated by": "** Mai Helmy. **** Ahlam Ahmed, Ali H. Al-Hoorie, Mohammed Mohsen",
                "Translation": "التَّعريف:** مجتمع دولي يهدف إلى تحسين شفافيَّة وموثوقيَّة نتائج البحوث في مجالات البيئة والتَّطور والتَّخصُّصات ذات الصِّلة من خلال التَّغييرات الثَّقافية والمؤسّسيَّة (https://www.sortee.org/). تم إطلاق الجمعية في ديسمبر 2020 وهي متاحة لأي شخص مهتم بتحسين البحث في هذه التَّخصُّصات، بغض النَّظر عن الخبرة، فالجمعيَّة دوليَّة في نطاقها وعضويتها وأهدافها. تضم الجمعيَّة أكثر من 600 عضوا اعتبارًاًمن مايو 2021\\.  **المصطلحات ذات الصِّلة:** جمعيَّة تطوير العلوم النَّفسيَّة",
                "Related_terms": "** Society for the Improvement of Psychological Science (SIPS)"
            },
            {
                "Title": "Society for the Improvement of Psychological Science (SIPS) (جمعية تطويرالعلوم النَّفسيَّة) *",
                "Definition": "** A membership society founded to further promote improved methods and practices in the psychological research field. The society aims to complete its mission statement by enhancing the training of psychological researchers; by promoting research cultures that are more conducive to better quality research; by quantifying and empirically assessing the impact of such reforms; and by leading outreach events within and outside psychology to better the current state of research norms.  **",
                "Reference(s)": "** https://improvingpsych.org/",
                "Drafted by": "** Mahmoud Elsherif",
                "Reviewed (or Edited) by": "** Ashley Blake; Jade Pickering; Graham Reid; Flávio Azevedo",
                "Translated by": "** Mai Helmy.",
                "Translation reviewed by": "** Ahlam Ahmed, Ali H. Al-Hoorie, Hala Alghamdi, Mohammed Mohsen",
                "Translation": "التَّعريف:** جمعيَّة تأسَّست لتعزيز الأساليب، والممارسات التي تحسِّن البحوث النَّفسيَّة. تهدف الجمعيَّة إلى تحقيق رسالتها من خلال تدريب الباحثين في علم النَّفس، وتعزيز الثَّقافة البحثيَّة التي تساعد على تحسين جودة البحوث، وتقييم وقياس تأثير هذه الإصلاحات تجريبيًا، وتنظيم أنشطة توعية داخل وخارج علم النَّفس؛ لتحسين معايير البحث العلمي الحاليَّة.  **المصطلحات ذات الصِّلة:** جمعيَّة علم البيئة والبيولوجيا التَّطوريَّة المنفتحة والموثوقة والشَّفافة",
                "Related_terms": "** Society for Open, Reliable, and Transparent Ecology and Evolutionary biology (SORTEE)"
            },
            {
                "Title": "Social class (الطَّبقة الاجتماعيّة) *",
                "Definition": "** Social class is usually measured using both objective and subjective measurements, as recommended by the American Psychological Association (American Psychological Association,Task Force on Socioeconomic Status, 2007). Unlike the conventional concept, which only considers one factor, either education or income (e.g., economic variables), an individual's social class is considered to be a combination of their education, income, occupational prestige, subjective social status, and self-identified social class. Social class is partly a cultural variable, as it is a stable variable and likely to change slowly over the years. Social class can have important implications to academic outcomes. An individual may have a high socio-economic status yet identify as a working class individual. Working class students tend to have different life circumstances and often more restrictive commitments than middle-class students, which make their integration with other students more difficult (Rubin, 2021). The lack of time and money is obstructive to their social experience at university. Working class students are more likely to work to support themselves, resulting in less time for academic activities and for socializing with other students as well as less money to purchase items linked to social experiences (e.g. food).  **",
                "Reference(s)": "** Evans and Rubin (2021); Rubin et al. (2019); Rubin (2021); Saegert et al. (2007)",
                "Drafted by": "** Mahmoud Elsherif",
                "Reviewed (or Edited) by": "** Leticia Micheli; Eliza Woodward; Julika Wolska; Gerald Vineyard**;** Yu-Fang Yang",
                "Translated by": "Mai Helmy.",
                "Translation reviewed by": "** Ahlam Ahmed, Ali H. Al-Hoorie, Asma Alzahrani, Mohammed Mohsen",
                "Translation": "التَّعريف:** عادةً ما يتم قياس الطبقة الاجتماعيَّة باستخدام القياسات الموضوعيَّة والذَّاتيَّة، على النَّحو الذي توصي به جمعيَّة علم النَّفس الأمريكيَّة (American Psychological Association,Task Force on Socioeconomic Status, 2007). وعلى عكس المفهوم التَّقليدي، الذي يأخذ بعين الاعتبار عاملًا واحدًا فقط، إما التَّعليم أو الدَّخل (مثلا المتغيرات الاقتصادية)، تعتبر الطبقة الاجتماعية للفرد مزيجًا من التعليم والدخل والمكانة المِهنية والحالة الاجتماعية الذاتية والوضع الاجتماعي المحدد ذاتيًا. وتعد الطَّبقة الاجتماعيَّة، جزئيًا، متغيرًا ثقافيًا؛ حيث إنَّها متغير مستقر قد يتغيَّر ببطء على مرِّ السِّنين. ويمكن أن يكون للطَّبقة الاجتماعيَّة آثارًا مهمَّة على النَّتائج الأكاديميَّة، فقد يتمتَّع الفرد بمكانة اجتماعيَّة واقتصاديَّة عالية مع تعريفه كفرد من الطَّبقة العاملة. ويميل طلاب الطَّبقة العاملة إلى أن تكون لديهم ظروف حياتيَّة مختلفة، وغالبًا ما تكون التزاماتهم أكثر تقييدًا من طلاب الطَّبقة المتوسِّطة، مما يجعل اندماجهم مع الطُّلاب الآخرين أكثر صعوبة (Rubin, 2021). كما إن ضيق الوقت والمال قد يعيق تجربتهم الاجتماعيَّة في الجامعة. وعادةً يعمل طلاب الطَّبقة العاملة؛ لأجل دعم أنفسهم، ممَّا يؤدي إلى تقليل وقت الأنشطة الأكاديميَّة، والتَّواصل الاجتماعي مع الطُّلاب الآخرين وكذلك امتلاك مال أقل لشراء العناصر المرتبطة بالتَّجارب الاجتماعيَّة (مثل الطَّعام).  **المصطلحات ذات الصِّلة:** الاندماج الاجتماعي",
                "Related_terms": "** Social integration"
            },
            {
                "Title": "Social integration (الاندماج الاجتماعيّ) *",
                "Definition": "** Social integration is a multi-dimensional construct. In an academic context, social integration is related to the quantity and quality of the social interactions with staff and students, as well as the sense of connection and belonging to the university and the people within the institute. To be more specific, social support, trust, and connectedness are all variables that contribute to social integration. Social integration has important implications for academic outcomes and mental wellbeing (Evans & Rubin, 2021). Working class students are less likely to integrate with other students, since they have differing social and economic backgrounds and less disposable income. Thus they are not able to experience as many educational and fiscal opportunities than others. In turn, this can lead to poor mental health and feelings of ostracism (Rubin, 2021).  **",
                "Reference(s)": "** Evans and Rubin (2021); Rubin et al. (2019); Rubin (2021)",
                "Drafted by": "** Mahmoud Elsherif",
                "Reviewed (or Edited) by": "** Leticia Micheli; Eliza Woodward; Julika Wolska**;** Gerald Vineyard; Yu-Fang Yang; Flávio Azevedo",
                "Translated by": "** Mai Helmy.",
                "Translation reviewed by": "** Ali H. Al-Hoorie, Alaa M. Saleh, Asma Alzahrani, Mohammed Mohsen",
                "Translation": "التَّعريف:** الاندماج الاجتماعي هو مفهوم متعدِّد الأبعاد. في السِّياق الأكاديمي، يرتبط الاندماج الاجتماعي بكميَّة ونوعيَّة التَّفاعلات الاجتماعيَّة مع الموظفين والطُّلاب، إضافة إلى الشُّعور بالارتباط والانتماء إلى الجامعة والأفراد فيها. ولكي نكون أكثر تحديدًا ، فإنَّ الدَّعم الاجتماعي والثِّقة والتَّرابط كلها متغيرات تساهم في الاندماج الاجتماعي. للاندماج الاجتماعي آثار مهمة على النَّتائج الأكاديميَّة والصِّحة العقليَّة (Evans & Rubin, 2021)؛ فيقل احتمال اندماج طلاب الطَّبقة العاملة مع الطُّلاب الآخرين؛ نظرًا لأنَّ لديهم خلفيَّات اجتماعيَّة واقتصاديَّة مختلفة ودخلًا أقل، وبالتَّالي فإنَّهم غير قادرين على تجربة عدد مماثل من الفرص التَّعليميَّة والماليَّة. وهذا بدوره يمكن أن يؤدي إلى تدهور الصِّحة العقليَّة والشُّعور بالنبذ ​​(Rubin, 2021).  **المصطلحات ذات الصِّلة:** الطَّبقة الاجتماعيَّة",
                "Related_terms": "** Social class"
            },
            {
                "Title": "Specification Curve Analysis (تحليل منحنى المواصفات) *",
                "Definition": "** An analytic approach that consists of identifying, calculating, visualising and interpreting results (through inferential statistics) for *all* reasonable specifications for a particular research question (see Simonsohn et al., 2015). Specification curve analysis helps make transparent the influence of presumably arbitrary decisions during the scientific progress (e.g., experimental design, construct operationalization, statistical models or several of these) made by a researcher by comprehensively reporting all non-redundant, sensible tests of the research question. Voracek et al. (2019) suggest that SCA differs from multiverse analysis with regards to the graphical displays (a specification curve plot rather than a histogram and tile plot) and the use of inferential statistics to interpret findings.  **",
                "Reference(s)": "** Simonsohn et al. (2015); Simonsohn (2020); Voracek et al. (2019)",
                "Drafted by": "** Bradley Baker",
                "Reviewed (or Edited) by": "** Tina B. Lonsdorf; Sam Parsons; Charlotte R. Pennington",
                "Translated by": "** Mai Helmy.",
                "Translation reviewed by": "Ali H. Al-Hoorie, Alaa M. Saleh, Asma Alzahrani, Mohammed Mohsen",
                "Translation": "التَّعريف:** هو نهج تحليلي يتكوَّن من تحديد النَّتائج وحسابها وتصوّرها وتفسيرها (من خلال الإحصائيَّات الاستدلاليَّة) *لجميع* المواصفات المعقولة لسؤال بحث معين (see Simonsohn et al., 2015). ويساعد تحليل منحنى المواصفات على جعل تأثير القرارات العشوائيَّة المفترضة شفافة أثناء التَّقدُّم العلمي (كالتَّصميم التَّجريبي، وتوظيف المفاهيم، والنَّماذج الإحصائيَّة ،أو خليط من هذا كله) والتي يقوم بها الباحث من خلال الإبلاغ الشَّامل عن جميع الاختبارات المعقولة، وغير الزَّائدة عن الحاجة لسؤال البحث.  يشير بعض الباحثين (Voracek et al., 2019\\) إلى أن تحليل منحنى المواصفات يختلف عن تحليل الأكوان المتعدِّدة فيما يتعلَّق بالعروض الرُّسوميَّة (مخطَّط منحنى المواصفات بدلًا من الرَّسم البياني، ومخطَّط توزيع البيانات) واستخدام الإحصائيَّات الاستدلاليَّة لتفسير النَّتائج.  **المصطلحات ذات الصِّلة:** تحليل الأكوان المتعدِّدة، توليف البحث، المتانة (في التَّحليلات)، التَّقارير الانتقائيَّة، اهتزاز التَّأثيرات.",
                "Related_terms": "** Multiverse analysis; Research synthesis; Robustness (analyses); Selective reporting; Vibration of effects"
            },
            {
                "Title": "Statistical Assumptions (الافتراضات الإحصائيَّة) *",
                "Definition": "** Analytical approaches and models assume certain characteristics of one’s data (e.g., statistical independence, random samples, normality, equal variance,...). Before running an analysis, these assumptions should be checked since their violation can change the results and conclusion of a study. Good practice in open and reproducible science is to report assumption testing in terms of the assumptions verified and the results of such checks or corrections applied.  **",
                "Reference": "** Garson (2012); Hahn and Meeker (1993); Hoekstra et al. (2012); Nimon (2012)",
                "Originally drafted by": "** Graham Reid",
                "Reviewed (or Edited) by": "** Jamie P. Cockcroft; Sam Parsons; Martin Vasilev; Julia Wolska",
                "Translated by": "** Mai Helmy.",
                "Translation reviewed by": "** Ali H. Al-Hoorie, Alaa M. Saleh, Asma Alzahrani, Mohammed Mohsen",
                "Translation": "التَّعريف:** تفترض الأساليب والنَّماذج التَّحليليَّة خصائص معيَّنة للبيانات (كالاستقلال الإحصائي، وعشوائيَّة العيّنات، والانحناء الطبيعي، والتَّباين المتساوي وغيرها). قبل إجراء تحليل، فإنَّه يجب التَّحقُّق من هذه الافتراضات؛ لأنَّ انتهاكها يمكن أن يغيِّر نتائج الدِّراسة واستنتاجها. وتتمثَّل الممارسة الجيِّدة في العلوم المفتوحة والقابلة للتِّكرار في الإبلاغ عن اختبار الافتراضات، بذكر الافتراضات التي تم فحصها ونتائج هذه الفحوصات، أو التَّصحيحات المطبقة.  **المصطلحات ذات الصِّلة:** اختبار دلالة الفرضيَّة الصِّفرية ، الدِّلالة الإحصائيَّة، الصِّدق الإحصائي، الشَّفافيَّة ، الخطأ من النُّوع الأوّل، الخطأ من النَّوع الثَّاني ، الخطأ من نوع إم، الخطأ  من نوع إس.",
                "Related_terms": "** Null Hypothesis Significance Testing (NHST); Statistical Significance; Statistical Validity; Transparency; Type I error; Type II error; Type M error; Type S error"
            },
            {
                "Title": "Statistical power (القوّة الإحصائيّة) *",
                "Definition": "** Statistical power is the long-run probability that a statistical test correctly rejects the null hypothesis if the alternative hypothesis is true. It ranges from 0 to 1, but is often expressed as a percentage. Power can be estimated using the significance criterion (alpha), effect size, and sample size used for a specific analysis technique. There are two main applications of statistical power. A priori power where the researcher asks the question “given an effect size, how many participants would I need for X% power?”. Sensitivity power asks the question “given a known sample size, what effect size could I detect with X% power?”.  **",
                "Reference(s)": "** Carter et al. (2021); Cohen (1962); Cohen (1988); Dienes (2008); Giner-Sorolla et al. (2019); Ioannidis (2005); Lakens (2021a)",
                "Drafted by": "** Thomas Rhys Evans",
                "Reviewed (or Edited) by": "** James E. Bartlett; Jamie P. Cockcroft; Adrien Fillon; Emma Henderson; Tamara Kalandadze; William Ngiam; Catia M. Oliveira; Charlotte R. Pennington; Graham Reid; Martin Vasilev; Qinyu Xiao; Flávio Azevedo",
                "Translated by": "** Mai Helmy.",
                "Translation reviewed by": "** Ali H. Al-Hoorie, Alaa M. Saleh, Ahlam Ahmed, Mohammed Mohsen",
                "Translation": "التَّعريف:** القوة الإحصائيَّة هي الاحتماليَّة على المدى الطَّويل بأنَّ الاختبار الإحصائي يرفض بشكل صحيح الفرضيَّة الصِّفريَّة إذا كانت الفرضيَّة البديلة صحيحة.  يتراوح من 0 إلى 1، ولكن غالبًا ما يتم التَّعبير عنه كنسبة مئويَّة. يمكن تقدير القدرة باستخدام معيار الأهميَّة (ألفا)، وحجم التَّأثير، وحجم العيِّنة المستخدمة في تقنيَّة تحليل محدَّدة. هناك نوعان من التَّطبيقات الرَّئيسيَّة للقوّة الإحصائيَّة. قوة مسبقة حيث يطرح الباحث السُّؤال \"بالنَّظر إلى حجم التَّأثير، كم عدد المشاركين الذين سأحتاجهم للحصول على قوة معينة؟\". أما قوّة الحساسيَّة فتطرح السُّؤال التَّالي: \"نظرًا لحجم عيّنة معروف، ما هو حجم التَّأثير الذي يمكنني اكتشافه بقوّة معيّنة؟\".  **المصطلحات ذات الصِّلة:** حجم التَّأثير ، التَّحليل البعدي، اختبار دلالة الفرضيَّة الصِّفريَّة ، تحليل القوّة، القيمة التَّنبؤيَّة الإيجابيَّة، البحث الكمي، حجم العيِّنة، معيار الأهميَّة (ألفا) ، الخطأ من النَّوع الأوّل ،الخطأ من النُّوع الثَّاني.  **المصطلحات ذات الصِّلة بالتَّعريف البديل:** الخطأ من النُّوع الثَّاني.",
                "Related_terms": "** Effect Size; Meta-analysis; Null Hypothesis Significance Testing (NHST); Power Analysis; Positive Predictive Value; Quantitative research; Sample size; Significance criterion (alpha); Type I error; Type II error **Related terms to alternative definition:** Type II Error"
            },
            {
                "Title": "Statistical significance (الدَّلالة الإحصائيَّة) *",
                "Definition": "** A property of a result using Null Hypothesis Significance Testing (NHST) that, given a significance level, is deemed unlikely to have occurred given the null hypothesis. Tenny and Abdelgawad (2017) defined it as “a measure of the probability of obtaining your data or more extreme data assuming the null hypothesis is true, compared to a pre-selected acceptable level of uncertainty regarding the true answer” (p. 1). Conventions for determining the threshold vary between applications and disciplines but ultimately depend on the considerations of the researcher about an appropriate error margin. The American Statistical Association’s statement (Wasserstein & Lazar, 2016\\) notes that “Researchers often wish to turn a p-value into a statement about the truth of a null hypothesis, or about the probability that random chance produced the observed data. The p-value is neither. It is a statement about data in relation to a specified hypothetical explanation, and is not a statement about the explanation itself” (p. 131).  **",
                "Reference(s)": "** Cassidy et al. (2019); Tenny and Abdelgawad (2021); Wasserstein and Lazar (2016)",
                "Drafted by": "** Alaa AlDoh; Flávio Azevedo",
                "Reviewed (or Edited) by": "** James E. Bartlett; Alexander Hart**;** Annalise A. LaPlume; Charlotte R. Pennington; Graham Reid; Timo Roettger; Suzanne L. K. Stewart",
                "Translated by": "** Zainab Alsuhaibani",
                "Translation reviewed by": "** Ali H. Al-Hoorie, Alaa M. Saleh, Mohammed Mohsen Term coined: No",
                "Translation": "التَّعريف**: هي وصف لنتيجة ما باستخدام اختبار دلالة الفرضيَّة الصِّفريَّة التي لا يرجَّح الحصول عليها باعتبار مستوى معين من الدَّلالة الإحصائيَّة في النَّظرية الصِّفريَّة. وقد عرفها بعض الباحثين بأنَّها \"مقياس لاحتماليَّة الحصول على بياناتك، أو بيانات أكثر تطرفًا على افتراض أنَّ الفرضيَّة الصِّفرية صحيحة، مقارنة بمستوى مقبول محدَّد مسبقًا من عدم اليقين فيما يتعلَّق بالإجابة الحقيقية\" (Tenny & Abdelgawad, 2017, p. 1). وتختلف الأعراف في تحديد بدايتها بين التَّطبيقات، والتَّخصُّصات، ولكنَّها تعتمد في نهاية الأمر على اعتبارات الباحث حول هامش الخطأ المناسب.  ويشير بيان الجمعيَّة الإحصائيَّة الأمريكيَّة (Wasserstein & Lazar, 2016\\) إلى أنَّ \"الباحثون غالبًا ما يرغبون في تحويل القيمة الاحتماليَّة إلى دليل صحة الفرضيَّة الصِّفريَّة، أو حول احتمال أن تكون الصُّدفة العشوائيَّة قد سبَّبت البيانات المرصودة. لكن القيمة الاحتماليَّة ليست كذلك، حيث إنَّها إشارة حول البيانات المتعلِّقة بتفسير افتراضيَّة معيّنة، وليست إشارة عن التَّفسير نفسه\" (ص. 131). **المصطلحات ذات الصِّلة:** خطأ ألفا، الإحصاءات المتكرِّرة؛ فرضيَّة الصِّفر، اختبار دلالة الفرضيَّة الصِّفريَّة؛ القيمة الاحتماليَّة؛ الخطأ من النُّوع الأوَّل.",
                "Related_terms": "** Alpha error; Frequentist statistics; Null hypothesis; Null Hypothesis Significance Testing (NHST); *P*\\-value; Type I error **Incorrect definition:** Statistical significance describes the likelihood of the observed result against chance (regardless of the null hypotheses)"
            },
            {
                "Title": "Statistical validity (الصِّدق الإحصائي) *",
                "Definition": "** The extent to which conclusions from a statistical test are accurate and reflective of the true effect found in nature. In other words, whether or not a relationship exists between two variables and can be accurately detected with the conducted analyses. Threats to statistical validity include low power, violation of assumptions, reliability of measures, etc, affecting the reliability and generality of the conclusions.  **",
                "Reference(s)": "** Cook and Campbell (1979); Drost (2011)",
                "Drafted by": "** Annalise A. LaPlume",
                "Reviewed (or Edited) by": "** Jamie P. Cockcroft, Zoltan Kekecs; Graham Reid",
                "Translated by": "** Zainab Alsuhaibani",
                "Translation reviewed by": "** Ali H. Al-Hoorie, Ahlam Ahmed, Mohammed Mohsen",
                "Translation": "التَّعريف**: مدى دقّة استنتاجات الاختبار الإحصائي وعكاسها للتَّأثير الحقيقي الموجود في الواقع. بمعنى آخر، هو معرفة ما إذا كان هناك علاقة بين متغيرين أم لا، ويمكن اكتشافها بدقَّة من خلال التَّحليلات التي تم إجراؤها. وتشمل مهدِّدات الصِّدق الإحصائي الآتي: القوة المنخفضة، انتهاك الفرضيَّات، ثبات المقاييس، وغير ذلك ممَّا يؤثِّر على موثوقيَّة النَّتائج وعموميّتها.  **المصطلحات ذات الصِّلة:** القوّة (الإحصائيَّة)، الصِّدق، الافتراضات الإحصائيَّة",
                "Related_terms": "** Power; Validity; Statistical assumptions"
            },
            {
                "Title": "STRANGE (سترينج: الخلفيَّة الاجتماعيَّة والقابليَّة للتَّتبع والاختيار الذَّاتي وتاريخ التَّربية والتَّأقلم والتَّعود) *",
                "Definition": "** The STRANGE “framework” is a proposal and series of questions to help animal behaviour researchers consider sampling biases when planning, performing and interpreting research with animals. STRANGE is an acronym highlighting several possible sources of sampling bias in animal research, such as the animals’ Social background; Trappability and self-selection; Rearing history; Acclimation and habituation; Natural changes in responsiveness; Genetic make-up, and Experience.  **",
                "Reference(s)": "** Webster and Rutz (2020)",
                "Drafted by": "** Mahmoud Elsherif",
                "Reviewed (or Edited) by": "** Ben Farrar; Zoe Flack; Elias Garcia-Pelegrin; Charlotte R. Pennington; Graham Reid",
                "Translated by": "** Mai Helmy.",
                "Translation reviewed by": "** Ali H. Al-Hoorie, Ahlam Ahmed, Alaa M. Saleh, Mohammed Mohsen",
                "Translation": "التَّعريف:** إطار يقدم مقترحًا وسلسلة من الأسئلة لمساعدة الباحثين في مجال سلوك الحيوان على النَّظر في تحيُّزات أخذ العيِّنات عند التَّخطيط، وإجراء وتفسير البحوث على الحيوانات، وهو اختصار يسلِّط الضَّوء على العديد من المصادر المحتملة لتحيُّز أخذ العيِّنات في بحوث الحيوانات، مثل: الخلفيَّة الاجتماعيَّة للحيوانات، والقابليَّة للتَّتبع والاختيار الذَّاتي، وتاريخ التَّربية، و التَّأقلم والتَّعوُّد، والتَّغيُّرات الطَّبيعيَّة في الاستجابة، والتَّكوين الجيني والخبرة.  **المصطلحات ذات الصِّلة:** التَّحيُّز، قيود التَّعميم، العيِّنة، تحيّز أخذ العيِّنات، ويرد",
                "Related_terms": "** Bias; Constraints on Generality (COG); Populations; Sampling bias; WEIRD"
            },
            {
                "Title": "StudySwap (مبادلة الدِّراسات) *",
                "Definition": "** A free online platform through which researchers post brief descriptions of research projects or resources that are available for use (“haves”) or that they require and another researcher may have (“needs”). StudySwap is a crowdsourcing approach to research which can ensure that fewer research resources go unused and more researchers have access to the resources they need.  **",
                "Reference": "** Chartier et al. (2018); [https://osf.io/view/StudySwap](https://osf.io/view/StudySwap)",
                "Originally drafted by": "** Charlotte R. Pennington",
                "Reviewed (or Edited) by": "** Mahmoud Elsherif; Helena Hartmann; Emma Henderson; Graham Reid",
                "Translated by": "** Mai Helmy.",
                "Translation reviewed by": "** Ali H. Al-Hoorie, Ahlam Ahmed, Asma Alzahrani, Mohammed Mohsen",
                "Translation": "التَّعريف:** منصَّة مجانيَّة على الإنترنت ينشر من خلالها الباحثون وصفًا موجزًا للمشاريع البحثيَّة، أو الموارد المتاحة للاستخدام (الممتلكات) أو التي يحتاجون  إليها وقد يكون لدى باحث آخر (الاحتياجات). مبادلة الدِّراسات هو نهج للتَّعهيد الجماعي للبحث والذي يمكن أن يضمن بقاء موارد بحثيَّة غير مستخدمة وحصول المزيد من الباحثين على الموارد التي يحتاجون إليها.  **المصطلحات ذات الصِّلة:** التَّعاون ،التَّعهيد الجماعي، علم الفريق",
                "Related_terms": "** Collaboration; Crowdsourcing; Team science"
            },
            {
                "Title": "Systematic Review (المراجعة المنهجيّة)",
                "Definition": "** A form of literature review and evidence synthesis. A systematic review will usually include a thorough, repeatable (reproducible) search strategy including key terms and databases in order to find relevant literature on a given topic or research question. Systematic reviewers follow a process of screening the papers found through their search, until they have filtered down to a set of papers that fit their predefined inclusion criteria. These papers can then be synthesised in a written review which may optionally include statistical synthesis in the form of a meta-analysis as well. A systematic review should follow a standard set of guidelines to ensure that bias is kept to a minimum for example PRISMA (Moher et al., 2009; Page et al., 2021), Cochrane Systematic Reviews (Higgins et al., 2019), or NIRO-SR (Topor et al., 2021).  **",
                "Reference": "** Higgins et al. (2019); Moher et al. (2009); Page et al. (2021); Topor et al. (2021)",
                "Originally drafted by": "** Jade Pickering",
                "Reviewed (or Edited) by": "** Mahmoud Elsherif; Adam Parker; Charlotte R. Pennington; Timo Roettger; Marta Topor; Emily A. Williams; Flávio Azevedo",
                "Translated by": "** Hala Alghamdi",
                "Translation reviewed by": "** Ali H. Al-Hoorie, Ahlam Ahmed, Asma Alzahrani, Mohammed Mohsen  ### ---  ###  ### **T** {#t}",
                "Translation": "التَّعريف:** تعدُّ المراجعة المنهجيَّة شكلًا من أشكال مراجعة الأدبيَّات وتجميع الأدلَّة. عادة ما تتضمَّن المراجعة المنهجيَّة استراتيجيَّة بحث شاملة وقابلة للتِّكرار، وتشمل: المصطلحات الرَّئيسيَّة، وقواعد البيانات المستخدمة للوصول إلى الأدبيَّات ذات العلاقة بموضوع ، أو سؤال بحثي معين. يقوم المراجعون المنهجيُّون بعمليَّة فحص الدِّراسات التي تم العثور عليها من خلال بحثهم، وتصفيتها حتى الوصول إلى مجموعة من الدِّراسات التي تتطابق مع معايير التَّضمين المحدَّدة مسبقًا. يمكن بعد ذلك جمع هذه الدِّراسات في مراجعة مكتوبة والتي قد تحتوي بدورها- ولكن ليس بالضَّرورة- على تجميع احصائي باستخدام التَّحليل البعدي. يجب أن تتبع المراجعة المنهجيَّة مجموعة محدَّدة من الإرشادات؛ لضمان إبقاء التَّحيُّز عند الحدِّ الأدنى مثل إرشادات بريزما (Moher et al., 2009; Page et al., 2021) ومراجعات كوكرين المنهجيَّة (Higgins et al. 2019\\) أو قائمة نيرو للمراجعات المنهجيَّة (Topor et al., 2021).  **المصطلحات ذات الصِّلة:** التَّحليل البعدي ، كونسورت CONSORT، المراجعات المنهجيَّة غير التَّدخليَّة  المفتوحة والقابلة للتِّكرار",
                "Related_terms": "** Meta-analysis; CONSORT; Non-Intervention, Reproducible, and Open Systematic Reviews (NIRO-SR); PRISMA"
            },
            {
                "Title": "Tenzing (تينزينج) *",
                "Definition": "** *tenzing* is an online webapp and R package that helps researchers to track and report the contributions of each team member using the CRediT taxonomy in an efficient way. Team members of a research project can indicate their contributions to each CRediT role using an online spreadsheet template, and provide any additional authors' information (e.g., name, affiliation, order in publication, email address, and ORCID iD). Upon writing the manuscript, *tenzing* can automatically create a list of contributors belonging to each CRediT role to be included in the contributions section and create the manuscript’s title page.  **",
                "Reference(s)": "** Holcombe et al. (2020)",
                "Drafted by": "** Marton Kovacs",
                "Reviewed (or Edited) by": "** Balazs Aczel; Mahmoud Elsherif; Helena Hartmann; Charlotte R. Pennington; Graham Reid; Flávio Azevedo",
                "Translated by": "** Mai Helmy.",
                "Translation reviewed by": "** Ali H. Al-Hoorie, Asma Alzahrani, Ahlam Ahmed, Hiba Alomary, Mohammed Mohsen  ### ---",
                "Translation": "التَّعريف:** حزمة تطبيقات ويب وحزمة آر على الإنترنت تساعد الباحثين على تتبُّع مساهمات كلِّ عضوٍ في الفريق، والإبلاغ عنها باستخدام تصنيف أدوار المؤلِّفين بطريقة فعَّالة.  ويمكن لأعضاء فريق المشروع البحثي الإشارة إلى مساهماتهم في كلِّ دورٍ من تصنيف أدوار المؤلِّفين باستخدام قالب جدول بيانات عبر الإنترنت، وتقديم أي معلوماتٍ إضافيَّة للمؤلِّفين (كالاسم، والانتماء، وترتيب النَّشر، وعنوان البريد الإلكتروني، ورقم الأوركيد). وعند كتابة المخطوطة، يمكن أنْ ينشئ تينزينج تلقائيًا قائمة بالمساهمين المنتمين إلى كلِّ دورٍ بحسب تصنيف أدوار المؤلِّفين؛ ليتم تضمينهم في قسم المساهمات وإنشاء صفحة عنوان المخطوطة.  **المصطلحات ذات الصِّلة:** التَّأليف، التَّحالف التَّأليفي، الإسهام، تصنيف أدوار المؤلِّفين.",
                "Related_terms": "** Authorship; Consortium authorship; Contributions; CRediT"
            },
            {
                "Title": "Theory (النَّظريَّة) *",
                "Definition": "** A theory is a unifying explanation or description of a process or phenomenon, which is amenable to repeated testing and verifiable through scientific investigation, using various experiments led by several independent researchers. Theories may be rejected or deemed an unsatisfactory explanation of a phenomenon after rigorous testing of a new hypothesis that explains the phenomena better or seems to contradict them but is more generalisable to a wider array of findings.  **",
                "Reference(s)": "** Schafersman (1997); Wacker (1998)",
                "Drafted by": "** Aoife O’Mahony",
                "Reviewed (or Edited) by": "** Mahmoud Elsherif; Helena Hartmann; Charlotte R. Pennington; Graham Reid",
                "Translated by": "** Hala Alghamdi",
                "Translation reviewed by": "** Ali H. Al-Hoorie, Ahlam Ahmed, Asma Alzahrani, Mohammed Mohsen",
                "Translation": "التَّعريف**: هي تفسير، أو وصف موحّد لعملية أو ظاهرة ما، تكون قابلة للاختبارات المتكرِّرة، ويمكن التَّحقُّق منها من خلال البحث العلمي باستعمال تجارب مختلفة بقيادة العديد من الباحثين المستقلين.  من الممكن رفض النَّظريات أو اعتبارها تفسيرًا غير مرضٍ لظاهرة ما بعد الاختبار الصَّارم لفرضيَّة جديدة.  تفسِّر الظَّاهرة بشكل أفضل أو تبدو متعارضة معها، ولكنَّها مع ذلك أكثر قابليَّة للتَّعميم على مجموعة أوسع من النَّتائج. **المصطلحات ذات الصِّلة:** الفرضيَّة، النّموذج (الفلسفي)، بناء النَّظريَّة.",
                "Related_terms": "** Hypothesis; Model (philosophy); Theory building"
            },
            {
                "Title": "Theory building (بناء النَّظريَّة) *",
                "Definition": "** The process of creating and developing a statement of concepts and their interrelationships to show how and/or why a phenomenon occurs. Theory building leads to theory testing.  **",
                "Reference(s)": "** Borsboom et al. (2020); Corley and Gioia (2011); Gioia and Pitrie (1990); Wacker (1998)",
                "Drafted by": "** Filip Dechterenko",
                "Reviewed (or Edited) by": "** Mahmoud Elsherif; Helena Hartmann; Charlotte R. Pennington",
                "Translated by": "** Mahdi Aben Ahmed",
                "Translation reviewed by": "** Ali H. Al-Hoorie, Ahlam Ahmed, Asma Alzahrani, Mohammed Mohsen",
                "Translation": "التَّعريف:** هي عمليَّة إنشاء وتطوير بيان للمفاهيم وعلاقاتها المتبادلة؛ لإظهار كيف أو لماذا تحدث ظاهرةٌ ما، فبناء الَّنظريَّة يقودنا إلى اختبارها.  **المصطلحات ذات الصِّلة:** الفرضيَّة، النَّموذج الفلسفي، النَّظريَّة، المساهمة النَّظريَّة، النَّموذج النَّظري.",
                "Related_terms": "** Hypothesis; Model (philosophy); Theory; Theoretical contribution; Theoretical model"
            },
            {
                "Title": "The Troubling Trio (الثّلاثي المقلق) *",
                "Definition": "** Described as a combination of low statistical power, a surprising result, and a *p*\\-value only slightly lower than .05.  **",
                "Reference(s)": "** Lindsay (2015)",
                "Drafted by": "** Halil Emre Kocalar",
                "Reviewed (or Edited) by": "**; Catia M. Oliveira; Adam Parker; Sam Parsons;Charlotte R. Pennington",
                "Translated by": "** Hala Alghamdi",
                "Translation reviewed by": "** Ali H. Al-Hoorie, Ahlam Ahmed, Asma Alzahrani, Mohammed Mohsen",
                "Translation": "التَّعريف:** يوصف بأنَّه مزيج من القوة الإحصائيَّة المنخفضة، والنَّتيجة المفاجئة، والقيمة الاحتماليَّة التي تقل عن 0.05 بقليل فقط.  **المصطلحات ذات الصِّلة**: التِّكرار، قابليَّة إعادة الإنتاج، اختبار دلالة الفرضيَّة الصِّفرية، قرصنة القيمة الاحتماليَّة، ممارسات البحث المشكوك فيها أو ممارسات إعداد التَّقارير المشكوك فيها.",
                "Related_terms": "** Replication; Reproducibility; Null Hypothesis Significance Testing (NHST); *P*\\-hacking; Questionable Research Practices or Questionable Reporting Practices (QRPs)"
            },
            {
                "Title": "Transparency (الشَّفافيَّة) *",
                "Definition": "** Having one’s actions open and accessible for external evaluation. Transparency pertains to researchers being honest about theoretical, methodological, and analytical decisions made throughout the research cycle. Transparency can be usefully differentiated into “scientifically relevant transparency” and “socially relevant transparency”. While the former has been the focus of early Open Science discourses, the latter is needed to provide scientific information in ways that are relevant to decision makers and members of the public (Elliott & Resnik, 2019).  **",
                "Reference(s)": "** Elliott and Resnik (2019); Lyon (2016); [Syed (2019)](https://psyarxiv.com/cteyb/)",
                "Drafted by": "** William Ngiam",
                "Reviewed (or Edited) by": "** Tamara Kalandadze; Aoife O’Mahony; Eike Mark Rinke; Flávio Azevedo",
                "Translated by": "** Sarah Almutairi",
                "Translation reviewed by": "** Ali H. Al-Hoorie, Ahlam Ahmed, Asma Alzahrani, Mohammed Mohsen",
                "Translation": "التَّعريف:** تشير إلى جعل أعمال الفرد متاحة، وقابلة للإطِّلاع ويمكن الوصول إليها؛ لغرض التَّقييم الخارجي. تتعلَّق الشَّفافيَّة بأن يكون الباحثون صادقين بشأن القرارات النَّظريَّة، و المنهجيَّة، والتَّحليلية التي يتم اتخاذها طوال دورة البحث. ويمكن الفصل بين نوعين من الشَّفافيَّة: الأول يتعلَّق \"بالشَّفافيَّة العلميَّة\" والتي تركز على النِّقاشات المتعلِّقة بالعلم المفتوح، بينما النَّوع الثَّاني يتعلَّق \"بالشَّفافيَّة المجتمعيَّة\" والتي تركِّز على ضرورة توفير المعلومات العلميَّة بطرق لها علاقة بصنّاع القرار والمجتمع العام (Elliott & Resnik, 2019).  **المصطلحات ذات الصِّلة:** المصداقيَّة في الادعِّاءات العلميَّة، العلم المفتوح، التَّسجيل المسبق، قابليَّة إعادة الإنتاج، استحقاق الثِّقة.",
                "Related_terms": "** Credibility of scientific claims; Open science; Preregistration; Reproducibility; Trustworthiness"
            },
            {
                "Title": "Transparency Checklist (قائمة مراجعة الشَّفافيَّة) *",
                "Definition": "** The transparency checklist is a consensus-based, comprehensive checklist that contains 36 items that cover the prepregistration, methods, results and discussion and data, code and materials availability. A shortened 12-item version of the checklist is also available. Checklist responses can be submitted alongside a manuscript for review. While the checklist can also work for educational purposes, it mainly aims to support researchers to identify concrete actions that can increase the transparency of their research while a disclosed checklist can help the readers and reviewers gain critical information about different aspects of transparency of the submitted research.  **",
                "Reference(s)": "** Aczel et. al. (2021)",
                "Drafted by": "** Barnabas Szaszi",
                "Reviewed (or Edited) by": "** Sarah Ashcroft-Jones; Mahmoud Elsherif; Helena Hartmann; Graham Reid; Flávio Azevedo",
                "Translated by": "** Mai Helmy.",
                "Translation reviewed by": "** Ali H. Al-Hoorie, Ahlam Ahmed, Asma Alzahrani, Mohammed Mohsen  ### ---",
                "Translation": "التَّعريف:** قائمة مراجعة الشَّفافيَّة عبارة عن قائمة مراجعة شاملة مبنية على الإجماع تحتوي على 36 عنصرًا و تغطِّي التَّسجيل المسبق، والطُّرق، والنَّتائج، والمناقشة، والبيانات، والنَّص البرمجي، وتوافر المواد. تتوفَّر أيضًا نسخة مختصرة من 12 عنصرًا من قائمة المراجعة. يمكن تقديم الرُّدود على قائمة المراجعة جنبًا إلى جنب مع مخطوطة للمراجعة. و على الرَّغم من أنَّه يمكن أن تُستعمل هذه القائمة للأغراض التَّعليميَّة، إلا أنَّها تهدف بشكل أساسي إلى دعم الباحثين لتحديد الإجراءات الملموسة التي يمكن أن تزيد من شفافيَّة أبحاثهم بينما يمكن لقائمة المراجعة التي تم الكشف عنها أن تساعد القراء والمراجعين في الحصول على معلومات مهمَّة حول الجوانب المختلفة لشفافيَّة البحث المُقدم.  **المصطلحات ذات الصِّلة:** مصداقيَّة الادِّعاءات العلميَّة ،العلم المفتوح، التَّسجيل المسبق، قابليَّة إعادة الإنتاج، الجدارة بالثِّقة",
                "Related_terms": "** Credibility of scientific claims; Open science; Preregistration; Reproducibility; Trustworthiness"
            },
            {
                "Title": "Triple-blind peer review (التّحكيم ثلاثيّ التَّعمية) *",
                "Definition": "** Evaluation of research products by qualified experts where the author(s) are anonymous to both the reviewer(s) and editor(s). **“**Blinding of the authors and their affiliations to both editors and reviewers. This approach aims to eliminate institutional, personal, and gender biases” (Tvina et al., 2019, p. 1082).  **",
                "Reference(s)": "** Largent and Snodgrass (2016); Tvina et al. (2019)",
                "Drafted by": "** Mahmoud Elsherif",
                "Reviewed (or Edited) by": "** Bradley Baker; Helena Hartmann; Charlotte R. Pennington; Christopher Graham",
                "Translated by": "** Awatif Alruwaili",
                "Translation reviewed by": "Ali H. Al-Hoorie, Ahlam Ahmed, Asma Alzahrani, Mohammed Mohsen",
                "Translation": "التَّعريف:** تقييم المنتجات البحثيَّة بعرضها على خبراء مؤهلين بحيث يكون المؤلِّف (أو المؤلِّفون) مجهول الهوية لكلِّ من المحكّمين والمحرِّرين. \"إخفاء المؤلِّفين وانتمائهم لكلٍّ من المحرِّرين والمراجعين. هذه المنهجيَّة تهدف الى استبعاد التَّحيُّزات الشَّخصيَّة، والمؤسَّسيَّة والجندريَّة\" (Tvina et al., 2019, p. 1082).  **المصطلحات ذات الصِّلة:** التَّحكيم مزدوج التَّعمية، التَّحكيم المفتوح، التَّحكيم أحاديَّ التَّعمية.",
                "Related_terms": "** Double-blind peer review; Open Peer Review; Single-blind peer review"
            },
            {
                "Title": "Trim-and-fill method (طريقة التَّقليم والتَّعبئة) ! New term - comments needed ! *",
                "Definition": "** The trim-and-fill method estimates missing studies as a result of publication bias in the funnel plot and adjusts the overall magnitude of the effect size. The studies with the most extreme effect sizes on the left and on the right side are suppressed. The number of studies excluded is conducted based on a case-by-case basis.  This method is to first trim the studies producing an asymmetrical funnel plot in order to ensure the overall magnitude of the effect size influenced by the remaining studies is not influenced by the remaining studies by publication bias. However, if there is no true overall effect size, it is infeasible to identify the studies to be trimmed at the first step. Following this, the missing studies are imputed in the funnel plot based on the overall estimate that is corrected in terms of bias.  **",
                "Reference(s)": "**",
                "Drafted by": "** Mahmoud Elsherif",
                "Reviewed (or Edited) by": "**",
                "Translated by": "** Mai Helmy.",
                "Translation reviewed by": "Ali H. Al-Hoorie, Ahlam Ahmed, Asma Alzahrani, Mohammed Mohsen",
                "Translation": "التَّعريف:** تقوم طريقة التَّقليم والتَّعبئة بتقدير الدِّراسات المفقودة نتيجة تحيُّز النَّشر في مخطَّط القمع، وتضبط الحجم الإجمالي لحجم التَّأثير. يتم استبعاد الدِّراسات ذات أحجام التَّأثير الأكثر تطرفًا على الجانب الأيسر، والجانب الأيمن، ثم يتمُّ تقدير عدد الدِّراسات المستبعدة على أساس كلِّ حالة على حدة.  تهدف هذه الطَّريقة أولًا إلى تقليم الدِّراسات التي تنتج مخطّطًا قمعيًا غير متماثل من أجل ضمان عدم تأثر الحجم الإجمالي لحجم التَّأثير بالدِّراسات المتبقية بتحيُّز النَّشر. ومع ذلك، إذا لم يكن هناك حجم تأثير إجمالي حقيقي، فمن غير الممكن تحديد الدِّراسات التي سيتم استبعادها في الخطوة الأولى. بعد ذلك، يتم احتساب الدِّراسات المفقودة في مخطَّط القمع بناءً على التَّقدير العام الذي تم تصحيحه من حيث التَّحيُّز.  **المصطلحات ذات الصِّلة:** التَّحليل البعدي.",
                "Related_terms": "** meta analysis"
            },
            {
                "Title": "TRUST Principles (مبادئ الثِّقة) *",
                "Definition": "** A set of guiding principles that consider Transparency, Responsibility, User focus, Sustainability, and Technology (TRUST) as the essential components for assessing, developing, and sustaining the trustworthiness of digital data repositories (especially those that store research data). They are complementary to the FAIR Data Principles.  **",
                "Reference": "** Lin et al. (2020)",
                "Originally drafted by": "** Aleksandra Lazić",
                "Reviewed (or Edited) by": "** Jamie P. Cockcroft; Mahmoud Elsherif; Helena Hartmann; Sam Parsons",
                "Translated by": "** Mai Helmy.",
                "Translation reviewed by": "** Ali H. Al-Hoorie, Ahlam Ahmed, Asma Alzahrani, Mohammed Mohsen",
                "Translation": "التَّعريف:** مجموعة من المبادئ التَّوجيهيَّة التي تَعتبر الشَّفافيَّة، والمسؤوليَّة، والتَّركيز على المستخدم والاستدامة والتّكنولوجيا مكونات أساسيَّة لتقييم، وتطوير، واستدامة موثوقيَّة مستودعات البيانات الرَّقميَّة (خاصّة تلك التي تخزِّن بيانات البحث). وهي مُكمّلة لمبادئ بيانات فير.  **المصطلحات ذات الصِّلة:** مبادئ فير ،البيانات الوصفيَّة، الوصول المفتوح، البيانات المفتوحة، المواد المفتوحة، مستودع  البيانات.",
                "Related_terms": "** FAIR principles; Metadata; Open Access; Open Data; Open Material; Repository"
            },
            {
                "Title": "Type I error (الخطأ من النَّوع الأوّل) *",
                "Definition": "** “Incorrect rejection of a null hypothesis” (Simmons et al., 2011, p. 1359), i.e. finding evidence to reject the null hypothesis that there is no effect when the evidence is actually in favouring of retaining the null that there is no effect (For example, a judge imprisoning an innocent person). Concluding that there is a significant effect and rejecting the null hypothesis when your findings actually occured by chance.  **",
                "Reference": "** Simmons et al., (2011)",
                "Originally drafted by": "** Lisa Spitzer",
                "Reviewed (or Edited) by": "** Mahmoud Elsherif; Adrien Fillon; Helena Hartmann; Matt Jaquiery; Mariella Paul; Charlotte R. Pennington; Graham Reid; Olly Robertson; Mirela Zaneva",
                "Translated by": "** Ali H. Al-Hoorie",
                "Translation reviewed by": "** Ahlam Ahmed, Asma Alzahrani, Alaa M. Saleh, Hiba Alomary, Mohammed Mohsen",
                "Translation": "التَّعريف:** \"الرَّفض غير الصَّحيح للفرضيَّة الصِّفريّة \" (Simmons et al., 2011, p. 1359)، هو الوصول إلى دليل لرفض الفرضيَّة الصِّفريَّة  (القائلة بأنَّه لا يوجد أي تأثير) عندما يكون الدَّليل في الواقع لصالح الإبقاء على فرضيَّة العدم الذي لا أثر له  (مثلًا، قاض يسجن شخصًا بريئًا). وهو استنتاج أنَّ هناك تأثيرًا حقيقيًا، ورفض الفرضية الصِّفرية  حينما تكون النَّتائج قد حدثت بالفعل عن طريق الصُّدفة.  **المصطلحات ذات الصِّلة:** الإحصاءات التِّكراريَّة، اختبار دلالة الفرضيَّة الصِّفريَّة، نتيجة عدميَّة، القيمة الاحتماليَّة، ممارسات البحث المشكوك فيها، أو ممارسات إعداد التَّقارير المشكوك فيها، أزمة إعادة الإنتاج  (أو أزمة التِّكرار)؛ النَّزاهة العلميَّة، القوة الإحصائيَّة،  نتيجة إيجابيَّة حقيقيَّة، الخطأ من النُّوع الثَّاني.",
                "Related_terms": "** Frequentist statistics; Null Hypothesis Significance Testing (NHST); Null Result; *P* value; Questionable Research Practices or Questionable Reporting Practices (QRPs); Reproducibility crisis (aka Replicability or replication crisis); Scientific integrity; Statistical power; True positive result; Type II error"
            },
            {
                "Title": "Type II error (الخطأ من النَّوع الثَّاني) *",
                "Definition": "** A false negative result occurs when the alternative hypothesis is true in the population but the null hypothesis is accepted as part of the analysis (Hartgerink et al., 2017). That is, finding a non-significant statistical result when the effect is true (For example, a judge passing an innocent verdict on a guilty person). False negatives are less likely to be the subject of replications than positive results (Fiedler et al., 2012), and remain an unresolved issue in scientific research (Hartgerink et al., 2017).  **",
                "Reference(s)": "** Fiedler et al. (2012); Hartgerink et al. (2017)",
                "Originally drafted by": "** Olly Robertson",
                "Reviewed (or Edited) by": "** Mahmoud Elsherif; Charlotte R. Pennington",
                "Translated by": "** Ali H. Al-Hoorie",
                "Translation reviewed by": "** Ahlam Ahmed, Asma Alzahrani, Alaa M. Saleh, Hiba Alomary, Mohammed Mohsen",
                "Translation": "التَّعريف:** تحدث النَّتيجة السَّلبيَّة الخاطئة عندما تكون الفرضيَّة البديلة صحيحة في مجتمع الدِّراسة، ولكن يتم قبول الفرضيَّة الصِّفريَّة كجزء من التَّحليل (Hartgerink et al.، 2017). أي العثور على نتيجة إحصائيَّة ليست ذات دلالة إحصائيَّة عندما يكون التَّأثير حقيقيًا (على سبيل المثال، قاض يصدر حكمًا بريئًا على شخص مذنب). عادةً ما تجذب النَّتائج السَّلبيَّة الخاطئة اهتمامًا أقل من الأبحاث التِّكرارية مقارنةً بالنَّتائج الإيجابيَّة (Fiedler et al.، 2012\\) ، وتظل هذه مشكلة لم يتم إيجاد حل لها في البحث العلمي (Hartgerink et al.، 2017).  **المصطلحات ذات الصِّلة:** حجم الأثر، اختبار دلالة الفرضيَّة الصِّفريَّة،  ممارسات البحث المشكوك فيها، أو ممارسات إعداد التَّقارير المشكوك فيها، أزمة إعادة الإنتاج  (أو أزمة التَّكرار،)، النَّزاهة العلميَّة، القوة الإحصائيَّة، نتيجة إيجابيَّة حقيقيَّة، الخطأ من النَّوع الأوّل.",
                "Related_terms": "** Effect size; Null Hypothesis Significance Testing (NHST); Questionable Research Practices or Questionable Reporting Practices (QRPs); Reproducibility crisis (aka Replicability or replication crisis); Scientific integrity; Statistical power; True positive result; Type I error"
            },
            {
                "Title": "Type M error (الخطأ من النَّوع إم ) *",
                "Definition": "** A Type M error occurs when a researcher concludes that an effect was observed with magnitude lower or higher than the real one. For example, a type M error occurs when a researcher claims that an effect of small magnitude was observed when it is large in truth or vice versa.  **",
                "Reference(s)": "** Gelman and Carlin (2014); Lu et al.(2018)",
                "Originally drafted by": "** Eduardo Garcia-Garzon",
                "Reviewed (or Edited) by": "** Mahmoud Elsherif; Helena Hartmann; Graham Reid; Mirela Zaneva",
                "Translated by": "** Ali H. Al-Hoorie",
                "Translation reviewed by": "** Ahlam Ahmed, Asma Alzahrani, Alaa M. Saleh, Mohammed Mohsen",
                "Translation": "التَّعريف:** يحدث خطأ من النَّوع (إم) عندما يستنتج الباحث أنَّه تم ملاحظة تأثير بحجم أقل، أو أعلى من التَّأثير الحقيقي. على سبيل المثال: يحدث خطأ من النَّوع (إم) عندما يدَّعي الباحث أنَّه تم ملاحظة تأثير صغير الحجم عندما يكون كبيرًا في الحقيقة أو العكس.  **المصطلحات ذات الصِّلة:** القوة الإحصائيَّة، الخطأ من النَّوع إس، الخطأ من النَّوع الأوّل، الخطأ من النَّوع الثَّاني.",
                "Related_terms": "** Statistical power; Type S error; Type I error; Type II error"
            },
            {
                "Title": "Type S error (الخطأ من النَّوع إس ) *",
                "Definition": "** A Type S error occurs when a researcher concludes that an effect was observed with an opposite sign than real one. For example, a type S error occurs when a researcher claims that a positive effect was observed when it is negative in reality or vice versa.  **",
                "Reference(s)": "** Gelman and Carlin (2014); Lu et al. (2018)",
                "Originally drafted by": "** Eduardo Garcia-Garzon",
                "Reviewed (or Edited) by": "** Helena Hartmann; Sam Parsons; Graham Reid; Mirela Zaneva",
                "Translated by": "** Ali H. Al-Hoorie",
                "Translation reviewed by": "** Ahlam Ahmed, Asma Alzahrani, Alaa M. Saleh, Hiba Alomary, Mohammed Mohsen  ### **U** {#u}",
                "Translation": "التَّعريف:** يحدث خطأ من النَّوع (إس) عندما يستنتج الباحث أنَّه تم ملاحظة تأثير بعلامة معاكسة للعلامة الحقيقيَّة. على سبيل المثال: يحدث خطأ من النَّوع إس عندما يدَّعي الباحث أنَّه تم ملاحظة تأثير إيجابي عندما يكون سلبيًا في الواقع أو العكس.  **المصطلحات ذات الصِّلة:** القوة الإحصائيَّة، الخطأ من النَّوع إم، الخطأ من النَّوع الأوّل، الخطأ من النَّوع الثَّاني.",
                "Related_terms": "** Statistical power; Type M error; Type I error; Type II error"
            },
            {
                "Title": "Under-representation (نقص التَّمثيل) *",
                "Definition": "** Not all voices, perspectives, and members of the community are adequately represented. Under-representation typically occurs when the voices or perspectives of one group dominate, resulting in the marginalization of another. This often affects groups who are a minority in relation to certain personal characteristics.  **",
                "Drafted by": "** Madeleine Pownall",
                "Reviewed (or Edited) by": "** Mahmoud Elsherif; Helena Hartmann; Bethan Iley; Adam Parker; Charlotte R. Pennington, Mirela Zaneva",
                "Translated by": "** Sarah Almutairi",
                "Translation reviewed by": "Ali H. Al-Hoorie, Asma Alzahrani, Alaa M. Saleh, Ahlam Ahmed, Hiba Alomary, Mohammed Mohsen",
                "Translation": "التَّعريف:** يصف هذا المصطلح النَّقص أو القصور في تمثيل كافّة الأصوات، ووجهات النَّظر، والفئات الموجودة في المجتمع بشكل كافٍ.  ويحدث نقص التَّمثيل عادةً عندما يسيطر صوت ووجهات نظر مجموعة ما على المجتمع ممّا يؤدِّي إلى تهميش للفئات الأخرى، ويؤثِّر هذا غالبًا على الأقليّات التي تمتلك خصائص فرديّة معيّنة.  **المصطلحات ذات الصِّلة:** العدالة، عدالة، عدم مساواة، ويرد",
                "Related_terms": "** Equity; Fairness; Inequality; WEIRD"
            },
            {
                "Title": "Universal design for learning (UDL) (التَّصميم الشَّامل للتَّعلُّم) *",
                "Definition": "** A framework for improving learning and optimising teaching based upon scientific insights of how humans learn. It aims to make learning inclusive and transformative for all people in which the focus is on catering to the differing needs of different students. It is often regarded as an evidence-based and scientifically valid framework to guide educational practice, consisting of three key principles: engagement, representation, and action and expression. In addition, UDL is included in the Higher Education Opportunity Act of 2008 (Edyburn, 2010).  **",
                "Reference(s)": "** Hitchcock et al. (2002); Rose (2000); Rose and Meyer (2002)",
                "Drafted by": "** Charlotte R. Pennington",
                "Reviewed (or Edited) by": "** Valeria Agostini; Mahmoud Elsherif; Graham Reid; Mirela Zaneva; Flávio Azevedo",
                "Translated by": "Mai Helmy.  Ali H. Al-Hoorie, Asma Alzahrani, Alaa M. Saleh, Ahlam Ahmed, Hiba Alomary, Mohammed Mohsen  ### **V** {#v}",
                "Translation": "التَّعريف:** هو إطار عمل لتحسين التَّعلُّم وتحسين التَّدريس بناءً على الرُّؤى العلميَّة لكيفيَّة تعلُّم البشر.  يهدف إلى جعل التَّعلُّم شاملًا وتحويليًا لجميع الأشخاص حيث ينصبُّ التَّركيز على تلبية الاحتياجات المختلفة للطُّلاب المختلفين. وغالبًا ما يُنظر إليه على أنَّه إطار عمل قائم على الأدلَّة و صحيح علميًا لتوجيه الممارسة التَّعليميَّة، حيث يتألَّف من ثلاثة مبادئ رئيسيَّة وهي: المشاركة، والتَّمثيل، والعمل والتعبير. بالإضافة إلى ذلك، تم تضمين التَّصميم الشَّامل للتَّعلُّم في قانون فرص التَّعليم العالي لعام 2008 (Edyburn, 2010). **المصطلحات ذات الصِّلة:** تكافؤ الفرص، الشُّمولية، أصول التَّربية، ممارسة التَّدريس.",
                "Related_terms": "** Equal opportunities; Inclusivity; Pedagogy; Teaching practice"
            },
            {
                "Title": "Validity (الصِّدق) *",
                "Definition": "** Validity refers to the application of statistical principles to arrive at well-founded —i.e., likely corresponding accurately to the real world— concepts, conclusions or measurement. In psychometrics, validity refers to the extent to which something measures what it intends to or claims to measure. Under this generic term, there are different types of validity (e.g., internal validity, construct validity, face validity, criterion validity, diagnostic validity, discriminant validity, concurrent validity, convergent validity, predictive validity, external validity).  **",
                "Reference(s)": "** Campbell (1957); Boorsboom et al. (2004); Kelley (1927)",
                "Drafted by": "** Tamara Kalandadze; Madeleine Pownall; Flávio Azevedo",
                "Reviewed (or Edited) by": "** Eduardo Garcia-Garzon; Halil E. Kocalar; Annalise A. LaPlume; Joanne McCuaig; Adam Parker; Charlotte R. Pennington",
                "Translated by": "** Zainab Alsuhaibani",
                "Translation reviewed by": "** Ali H. Al-Hoorie, Asma Alzahrani, Ahlam Ahmed, Alaa M. Saleh, Hiba Alomary, Mohammed Mohsen",
                "Translation": "التَّعريف:** يشير الصِّدق إلى تطبيق المبادئ الإحصائيَّة للوصول إلى مفاهيم، أو استنتاجات، أو مقاييس ذات أساس جيد؛ أي من المحتمل أن تتطابق بدقَّة مع الواقع.  وفي القياس النَّفسي، يشير الصِّدق إلى المدى الذي يقيس فيه شيء ما، و يهدف أو يدّّعي قياسه. يشمل هذا المصطلح العام عدَّة أنواع مختلفة من الصِّدق، منها على سبيل المثال: الصِّدق الدَّاخلي، صدق البناء، الصِّدق الظَّاهري، صدق المحك، الصِّدق التَّشخيصي، الصِّدق التَّمييزي، الصِّدق التَّلازمي، الصِّدق التَّقاربي، الصِّدق التَّنبؤي، الصِّدق الخارجي.  **المصطلحات ذات الصِّلة:** السَّببية، الصِّدق البنائي، صدق المحتوى، صدق المحك، الصِّدق الخارجي، الصِّدق الظَّاهري، الصِّدق الدَّاخلي، القياس، ممارسات القياس المشكوك فيها، القياس النَّفسي، الثَّبات، القوة الإحصائيَّة، الصِّدق الإحصائي، الاختبار.",
                "Related_terms": "** Causality; Construct validity; Content validity; Criterion validity; External validity; Face validity; Internal validity; Measurement; Questionable Measurement Practices (QMP); Psychometry; Reliability; Statistical power; Statistical validity; Test"
            },
            {
                "Title": "Version control (التَّحكم في الإصدار)",
                "Definition": "** The practice of managing and recording changes to digital resources (e.g. files, websites, programmes, etc.) over time so that you can recall specific versions later. Version control systems are designed to record the history of changes (who, what and when), and help to avoid human errors (e.g. working on the wrong version). For example, the Git version control system is a widely used software tool that originally helped software developers to version control shared code and is now used across many scientific disciplines to manage and share files.  **",
                "Reference": "** [https://git-scm.com/book/en/v2/Getting-Started-About-Version-Control](https://git-scm.com/book/en/v2/Getting-Started-About-Version-Control)",
                "Drafted by": "** Mahmoud Elsherif",
                "Reviewed (or Edited) by": "** Sarah Ashcroft-Jones; Thomas Rhys Evans; Helena Hartmann; Matt Jaquiery; Adam Parker; Charlotte R. Pennington; Robert M. Ross; Timo Roettger; Andrew J. Stewart",
                "Translated by": "** Ahlam Ahmed",
                "Translation reviewed by": "** Ali H. Al-Hoorie, Asma Alzahrani, Mohammed Mohsen    ### **W** {#w}",
                "Translation": "التَّعريف:** ممارسة إدارة وتسجيل التَّغيُّرات التي تطرأ على الموارد الرَّقميَّة،مثل: الملفَّات، والمواقع الإلكترونيَّة، والبرامج وما إلى ذلك…  بمرور الوقت بحيث يمكن مراجعة إصدارات معيَّنة لاحقًا. تم تصميم أنظمة التَّحكم في الإصدار لتسجيل محفوظات التَّغييرات (من، وماذا، ومتى)، والمساعدة على تجنُّب الأخطاء البشريَّة، مثل العمل على إصدار خاطئ، على سبيل المثال: يعد نظام التَّحكم في إصدار Git أداة برمجيَّة مستخدمة على نطاق واسع حيث ساعدت مطوري البرامج في الأصل على التَّحكُّم في إصدار التَّعليمات البرمجيَّة المشتركة، ويستخدم الآن في العديد من التَّخصُّصات العلميَّة لإدارة الملفات ومشاركتها.  **المصطلحات ذات الصِّلة: ا**لمتتبِّع الدُّولي العالمي، قابليَّة إعادة الإنتاج، إدارة تكوين البرامج، إدارة شفرة المصدر، التَّحكم في المصدر.",
                "Related_terms": "** Git; Reproducibility; Software configuration management; Source code management; Source control"
            },
            {
                "Title": "Webometrics (قياسات الويب) *",
                "Definition": "** Webometrics involves the study of online content. Webometrics focuses on the numbers and types of hyperlinks between different online sites. Such approaches have been considered as a type of altmetrics. “The study of the quantitative aspects of the construction and use of information resources, structures and technologies on the Web drawing on [bibliometric](https://en.wikipedia.org/wiki/Bibliometrics) and [informetric](https://en.wikipedia.org/wiki/Informetrics) approaches” (Björneborn & Ingwersen, 2004).  **",
                "Reference(s)": "** Björneborn and Ingwersen (2004)",
                "Drafted by": "** Charlotte R. Pennington",
                "Reviewed (or Edited) by": "** Christopher Graham; Mirela Zaneva",
                "Translated by": "** Mai Helmy.",
                "Translation reviewed by": "** Asma Alzahrani, Ali H. Al-Hoorie, Mohammed Mohsen",
                "Translation": "التَّعريف:** تتضمَّن قياسات الويب دراسة المحتوى عبر الإنترنت، وتركِّز على أعداد، وأنواع الارتباطات التَّشعبيَّة بين مواقع الإنترنت المختلفة. وتعدُّ هذه الأساليب كنوع من المقاييس البديلة. \"دراسة الجوانب الكميَّة لبناء واستخدام موارد المعلومات، والهياكل، والتِّقنيَّات على الويب بالاعتماد على المناهج الببليومتريَّة والمعلوماتيَّة\" (Björneborn & Ingwersen, 2004).  **المصطلحات ذات الصِّلة:** المقاييس البديلة، القياسات الببليومتريَّة.",
                "Related_terms": "** Altmetrics; Bibliometrics"
            },
            {
                "Title": "WEIRD (وِيرد) *",
                "Definition": "**",
                "Reference(s)": "**",
                "Drafted by": "**",
                "Reviewed (or Edited) by": "**  ###  ### **Z** {#z}",
                "Translated by": "** Ahlam Ahmed",
                "Translation reviewed by": "** Ali H. Al-Hoorie, Asma Alzahrani, Alaa M. Saleh, Mohammed Mohsen  ###  ### **X** {#x}",
                "Translation": "**",
                "Related_terms": "** **Alternative definition:** (if applicable) **Related terms to alternative definition:** (if applicable)"
            },
            {
                "Title": "Z-Curve (منحنى زد) *",
                "Definition": "** Computing a Z-score is a statistical approach mainly used to obtain the ‘Estimated Replication Rate’ (ERR) and ‘Expected Discovery Rate’ (EDR) for a set of reported studies. Calculating a *z*\\-curve for a set of statistically significant studies involves converting reported *p*\\-values to *z*\\-scores, fitting a finite mixture model to the distribution of *z*\\-scores, and estimating mean power based on the mixture model. The Z-curve analysis can be performed in R through a dedicated package \\- [https://cran.r-project.org/web/packages/zcurve/index.html](https://cran.r-project.org/web/packages/zcurve/index.html).  **",
                "Reference(s)": "** Bartoš and Schimmack (2020); Brunner and Schimmack (2020)",
                "Drafted by": "** Bradley J. Baker",
                "Reviewed (or Edited) by": "** Kamil Izydorczak; Sam Parsons; Charlotte R. Pennington; Mirela Zaneva",
                "Translated by": "** Ahlam Ahmed",
                "Translation reviewed by": "** Ali H. Al-Hoorie, Asma Alzahrani, Alaa M. Saleh, Mohammed Mohsen",
                "Translation": "التَّعريف:** حساب درجة زد هو نهج إحصائي يستخدم بشكل أساسي للحصول على \"معدل النَّسخ المقدر\" و \"معدل الاكتشاف المتوقَّع\" لمجموعة من الدِّراسات التي تم إعداد تقارير نتائجها. يتضمَّن حساب منحنى زد لمجموعة من الدِّراسات ذات الدِّلالة الإحصائيَّة تحويل القيم الاحتماليَّة التي أعدت سلفاً إلى درجات زد، وتثبيت نموذج خليط محدود لتوزيع درجات زد، وتقدير متوسط ​​القوة بناءً على نموذج الخليط. يمكن إجراء تحليل منحنى زد في لغة الآر من خلال حزمة مخصَّصة \\- [https://cran.r-project.org/web/packages/zcurve/index.html](https://cran.r-project.org/web/packages/zcurve/index.html).  **المصطلحات ذات الصِّلة:** المقاييس البديلة، نسبة درج الملفَّات، منحنى القيمة الاحتماليَّة، قرصنة القيمة الاحتماليَّة ، تكرار أو نسخ متماثل، القوة الإحصائيَّة.",
                "Related_terms": "** Altmetrics; File drawer ratio; P-curve; P-hacking; Replication; Statistical power"
            },
            {
                "Title": "Zenodo (زينودو) *",
                "Definition": "** An open science repository where researchers can deposit research papers, reports, data sets, research software, and any other research-related digital artifacts. Zenodo creates a persistent digital object identifier (DOI) for each submission to make it citable. This platform was developed under the European OpenAIRE program and operated by CERN.  **",
                "Reference": "** www.zenodo.org",
                "Originally drafted by": "** Ali H. Al-Hoorie",
                "Reviewed (or Edited) by": "** Sara Middleton",
                "Translated by": "** Mai Helmy",
                "Translation reviewed by": "** Ali H. Al-Hoorie, Asma Alzahrani, Ahlam Ahmed, Alaa M. Saleh, Mohammed Mohsen  ### **Looking for something else?**  We have separated the glossary project across several documents, see links below:  Landing page:\t\t\t[Glossary | Phase 2 | Landing page](https://docs.google.com/document/d/1BKzztg7srUeC_2Yn0b7cMbxp_vYMDlOnEYpxg_S2hWs/edit?usp=sharing) Letters A \\- F:\t\t\t[Glossary | Phase 2 | A-F](https://docs.google.com/document/d/1ob__Alxsnx9yqeTpurEY_N_kv56c1iiZyrg0xxf0ftg/edit?usp=sharing) Letters G \\- L:\t\t\t[Glossary | Phase 2 | G-L](https://docs.google.com/document/d/1LP0cEletpNumDmYHFv3seV3gL6OLQHJ7ZDLQ7T3fzNE/edit?usp=sharing) Letters M \\- R: \t\t\t[Glossary | Phase 2 | M - R](https://docs.google.com/document/d/1FVIgUx717G3pBwI17LGS7y22nKMde-zg0I6AKgz4SeQ/edit?usp=sharing) Letters S \\- Z:\t\t\t[Glossary | Phase 2 | S - Z](https://docs.google.com/document/d/1-5CKFciwhB-WfC1DK8Sajyh8a8CgNOYvOoeZpODux8Y/edit?usp=sharing) References: \t\t\t[Glossary | Phase 2 | References](https://docs.google.com/document/d/12_F8kbnl2GP66iBkIejJeX2KnkZ13iC_jj7VVYZMxpA/edit?usp=sharing) Terms not yet defined: \t\t[Glossary | Phase 2 | Terms not yet defined](https://docs.google.com/document/d/16FGodUkNoNrBYyq2JqHJMNzYuZf-QbsigooMpB_ns_E/edit?usp=sharing) Contributors spreadsheet: \t[Glossary Phase 2 tenzing contributions \\[FORRT\\]](https://docs.google.com/spreadsheets/d/1pkvQ2-h_Hr_ZnlfmKYkuJTuYrxlqS2oSqKqoEDYlKlk/edit?usp=sharing)  [FORRT slack](https://join.slack.com/t/forrt/shared_invite/zt-alobr3z7-NOR0mTBfD1vKXn9qlOKqaQ) \\- join us\\! [FORRT email](mailto:FORRTproject@gmail.com) \\- contact us\\!",
                "Translation": "التَّعريف:** مستودع علمي مفتوح حيث يمكن للباحثين إيداع الأوراق البحثيَّة، والتَّقارير، ومجموعات البيانات، وبرامج البحث، وأي أدوات رقميَّة أخرى متعلِّقة بالبحوث، وينشئ زينودو معرفًا رقميًا ثابتًا لكلِّ عمليَّة إرسال لجعلها قابلة للاستشهاد. وقد تم تطوير هذه المنصَّة في إطار برنامج OpenAIRE أوبن إيرالأوروبي وتشغيلها بواسطة منظمة سيرن.  **المصطلحات ذات الصِّلة:** DOI (معرف الكائن الرقمي)، مشاركة، البيانات المفتوحة، إطار العلوم المفتوحة، الطّباعة الأولية",
                "Related_terms": "** DOI (digital object identifier); figshare; Open data; Open Science Framework; Preprint"
            },
            {
                "Title": "Manel (اللَّجنة الذُّكوريَّة) *",
                "Definition": "** Portmanteau for ‘male panel’, usually to refer to speaker panels at conferences entirely composed of (usually caucasian) males. Typically discussed in the context of gender disparities in academia (e.g., women being less likely to be recognised as experts by their peers and, subsequently, having fewer opportunities for career development).  **",
                "Reference(s)": "** Bouvy and Mujoomdar (2019); Goodman and Pepinsky (2019); Nittrouer et al. (2018); Rodriguez and Günther (2020)",
                "Drafted by": "** Sam Parsons",
                "Reviewed (or Edited) by": "** Mahmoud Elsherif; Thomas Rhys Evans; Beatrice Valentini; Christopher Graham; Flávio Azevedo",
                "Translated by": "** Mai Helmy.",
                "Translation reviewed by": "** Awatif Alruwaili, Ahlam Ahmed, Alaa M. Saleh, Ali H. Al-Hoorie, Mohammed Mohsen",
                "Translation": "التَّعريف:** يستخدم هذا التَّعبير- المأخوذ من كلمتي رجل ولجنة من الخبراء في اللُّغة الإنجليزيَّة \\- عادةً للإشارة إلى مجموعات المتحدثين في المؤتمرات المكوَّنة بالكامل من الذُّكور (وهي عادة مجالس مغلقة). وعادة ما يناقش في سياق الفوارق بين الجنسين في الأوساط الأكاديميَّة (على سبيل المثال، تقلُّ احتماليَّة الاعتراف بالنِّساء كخبيرات من قبل أقرانهن، وبالتَّالي تقلُّ فرصهنَّ في التَّطور الوظيفي).  **المصطلحات ذات الصِّلة:** بروبنساينس، التَّنوع، العدالة،علم النَّفس النَّسوي، الشُّمول، نقص التَّمثيل",
                "Related_terms": "** Bropenscience; Diversity; Equity; Feminist psychology; Inclusion; Under-representation"
            },
            {
                "Title": "Many authors (العديد من المؤلِّفين) *",
                "Definition": "** Large-scale collaborative projects involving tens or hundreds of authors from different institutions. This kind of approach has become increasingly common in psychology and other sciences in recent years as opposed to research carried out by small teams of authors, following earlier trends which have been observed e.g. for high-energy physics or biomedical research in the 1990s. These large international scientific consortia work on a research project to bring together a broader range of expertise and work collaboratively to produce manuscripts.  **",
                "Reference(s)": "** Cronin (2001); Moshontz et al. (2021); Wuchty et al. (2007)",
                "Drafted by": "** Yu-Fang Yang",
                "Reviewed (or Edited) by": "** Christopher Graham; Adam Parker; Charlotte R. Pennington; Birgit Schmidt; Beatrice Valentini",
                "Translated by": "** Awatif Alruwaili",
                "Translation reviewed by": "** Mahdi Aben Ahmed , Ahlam Ahmed, Alaa M. Saleh, Ali H. Al-Hoorie, Mohammed Mohsen",
                "Translation": "التَّعريف:** هي مشاريع تعاونيَّة واسعة النِّطاق يشترك فيها العشرات، أو المئات من الباحثين من مؤسّسات مختلفة، أصبحت هذه المنهجيَّة شائعة جدا في علم النَّفس، وبعض العلوم الأخرى خلال السَّنوات الماضية مقارنة بالبحوث التي تجريها فِرق صغيرة من المؤلِّفين، اتباعًا لاتجاهات في بحوث الطَّاقة الفيزيائيَّة العالية، أو البحوث الطبيَّة الحيويَّة في أوائل التِّسعينيات. تعمل هذه الائتلافات العلميَّة الدُّوليَّة على جمع مجموعة واسعة من الخبرات، والعمل بشكل تعاونيّ لإنتاج البحوث.  **المصطلحات ذات الصِّلة:** تعاون، الائتلافات، التَّحالف التَّأليفي ، عدد الكتاب مرتفع (فرط الباحثين)، حشد المصادر، تعدُّد الباحثين، فريق بحث.",
                "Related_terms": "** Collaboration; Consortia; Consortium authorship; Crowdsourcing; Hyperauthorship; Multiple-authors; Team science"
            },
            {
                "Title": "Many Labs (المعامل المتعدِّدة) *",
                "Definition": "** A crowdsourcing initiative led by the Open Science Collaboration (2015) whereby several hundred separate research groups from various universities run replication studies of published effects. This initiative is also known as “Many Labs I” and was subsequently followed by a “Many Labs II” project that assessed variation in replication results across samples and settings. Similar projects include ManyBabies, EEGManyLabs, and the Psychological Science Accelerator.  **",
                "Reference(s)": "** Ebersole et al. (2016); Frank et al. (2017); Klein et al. (2014); Klein et al. (2018); Moshontz et al. (2018); Open Science Collaboration (2015); Pavlov et al. (2020)",
                "Drafted by": "** Sam Parsons",
                "Reviewed (or Edited) by": "** Helena Hartmann; Charlotte R. Pennington; Mirela Zaneva",
                "Translated by": "** Dr.Nazik Alnour",
                "Translation reviewed by": "** Ahlam Ahmed, Alaa M. Saleh, Ali H. Al-Hoorie, Mohammed Mohsen",
                "Translation": "التّعريف:** مبادرة بقيادة فريق التَّعاون العلميّ المفتوح عام 2015 حيث إنَّ مئات المجموعات البحثيَّة من مختلف الجامعات كرَّرت عددًا من الدِّراسات المنشورة ذات نسبة تأثير عاليَة.  عرفت هذه المبادرة بـ \"المعامل المتعدِّدة رقم 1\" وتلاها مشروع \"المعامل المتعدِّدة رقم 2\" الذي قام بتقييم التَّباين في نتائج الدِّراسات المكرَّرة عبر العيّنات، والسِّياقات المختلفة.  وهناك مشاريع مشابهة مثل \"العديد من الأطفال\" و \"العديد من تخطيطات الدِّماغ الكهربائيَّة\" ومسرع العلوم النَّفسيَّة. **المصطلحات ذات الصِّلة: التَّعاون، العديد من المحلِّلين، المعامل المتعدِّدة رقم 1، المعامل المتعدِّدة رقم 2، التَّعاون العلميّ المفتوح، التِّكرار.**",
                "Related_terms": "** Collaboration; Many analysts; Many Labs I; Many Labs II; Open Science Collaboration; Replication"
            },
            {
                "Title": "Massive Open Online Courses (MOOCs) (مقرَّرات التَّعلُّم الضَّخمة المفتوحة عن بعد) *",
                "Definition": "** Exclusively online courses which are accessible to any learner at any time, are typically free to access (while not necessarily openly licensed), and provide video-based instructions and downloadable data sets and exercises. The “massive” aspect describes the high volume of students that can access the course at any one time due to their flexibility, low or no cost, and online nature of the materials.  **",
                "Reference(s)": "** Baturay (2015); [https://opensciencemooc.eu/](https://opensciencemooc.eu/)",
                "Drafted by": "** Elizabeth Collins",
                "Reviewed (or Edited) by": "** Tsvetomira Dumbalska; Mahmoud Elsherif; Helena Hartmann; Sam Parsons; Charlotte R. Pennington",
                "Translated by": "** Mahdi Aben Ahmed",
                "Translation reviewed by": "** Awatif Alruwaili, Alaa M. Saleh, Ali H. Al-Hoorie , Mohammed Mohsen",
                "Translation": "التَّعريف:** هي الدَّورات التَّدريبيَّة التي تنفذ عبر الإنترنت حصريًا، والتي يمكن الوصول إليها من قبل أي متعلِّم في أي وقت، عادة ما يكون الوصول إليها مجانًا (على الرُّغم من أنَّها ليست بالضَّرورة ذات رخصة استعمال علني)، وتوفِّر إرشادات قائمة على الفيديو، ومجموعات وتمارين بيانات قابلة للتَّنزيل.  يصف الجانب \"الضَّخم\" منها الحجم الكبير من الطُّلاب الذين يمكنهم الوصول إلى الدُّورة التَّدريبيَّة في أي وقت بسبب مرونتها، وتكلفتها المنخفضة، أو كونها مجانيَّة، وطبيعة المواد عبر الإنترنت.  **المصطلحات ذات الصِّلة:** إمكانيَّة الوصول،  التَّعليم عن بعد، الشُّمول، التَّعلم المفتوح",
                "Related_terms": "** Accessibility; Distance education; Inclusion; Open learning"
            },
            {
                "Title": "Massively Open Online Papers (MOOPs) (الأوراق الضخمة والمفتوحة على الإنترنت) *",
                "Definition": "** Unlike the traditional collaborative article, a MOOP follows an open participatory and dynamic model that is not restricted by a predetermined list of contributors.  **",
                "Reference(s)": "** Himmelstein et al. (2019); Tennant et al. (2019)",
                "Drafted by": "** Ali H. Al-Hoorie",
                "Reviewed (or Edited) by": "**",
                "Translated by": "** Nazik Alnour",
                "Translation reviewed by": "** Ahlam Ahmed, Alaa M. Saleh, Hiba Alomary, Ali H. Al-Hoorie, Mohammed Mohsen",
                "Translation": "التَّعريف:** تتبع الأوراق الضَّخمة المفتوحة على الإنترنت نموذجًا مفتوحًا تشاركيًا يمتاز بالمرونة، ولا تحكمه قائمة محدَّدة من المشاركين، على عكس المقال التَّقليدي المشترك.  **المصطلحات ذات الصِّلة:** علم المواطن، البحث الجماعي، العديد من المؤلِّفين.",
                "Related_terms": "** Citizen science; Collaboration; Crowdsourced Research; Many authors; Team science"
            },
            {
                "Title": "Matthew effect (in science) (تأثير ماثيو (في العلوم)) *",
                "Definition": "** Named for the ‘rich get richer; poor get poorer’ paraphrase of the Gospel of Matthew. Eminent scientists and early-career researchers with a prestigious fellowship are disproportionately attributed greater levels of credit and funding for their contributions to science while relatively unknown or early-career researchers without a prestigious fellowship tend to get disproportionately little credit for comparable contributions. The impact is a substantial cumulative advantage that results from modest initial comparative advantages (and vice versa). **",
                "Reference(s)": "** Bol et al. (2018); Bornmann et al. (2019); Merton (1968)",
                "Drafted by": "** Tamara Kalandadze",
                "Reviewed (or Edited) by": "** Bradley Baker; Tsvetomira Dumbalska; Mahmoud Elsherif; Matt Jaquiery; Charlotte R. Pennington",
                "Translated by": "** Ahlam Ahmed",
                "Translation reviewed by": "** Awatif Alruwaili, Alaa M. Saleh, Hiba Alomary, Ali H. Al-Hoorie, Mohammed Mohsen",
                "Translation": "التَّعريف:** هو إعادة صياغة لمبدأ ماثيو الإنجيلي (الأثرياء يصبحون أكثر ثراءً؛ الفقراء يصبحون أكثر فقرًا)، حيث يُنسب إلى العلماء البارزين، والباحثين في بداية حياتهم المهنيَّة الحاصلين على زمالة مرموقة مستوياتٍ أعلى من السُّمعة، والتَّمويل بشكل غير متناسب مع مساهماتهم العلميَّة، في حين أنَّ الباحثين غير المعروفين نسبيًا أو الباحثين في بداية حياتهم المهنيَّة دون زمالة مرموقة لا يحصلون على سمعة تتناسب مع مثل هذه المساهمات. ويمثل هذا التَّأثير ميزة تراكميَّة كبيرة تنتج عن تراكم مزايا أوليَّة متواضعة (والعكس صحيح). . **المصطلحات ذات الصِّلة:** تأثبر ماثيو في (العلوم) ؛ قانون ستيجلر للتَّسمية .",
                "Related_terms": "** Matthew effect in education; Stigler’s law of eponymy"
            },
            {
                "Title": "Meta-analysis (التَّحليل البعدي) *",
                "Definition": "** A meta-analysis is a statistical synthesis of results from a series of studies examining the same phenomenon. A variety of meta-analytic approaches exist, including random or fixed effects models or meta-regressions, which allow for an examination of moderator effects. By aggregating data from multiple studies, a meta-analysis could provide a more precise estimate for a phenomenon (e.g. type of treatment) than individual studies. Results are usually visualized in a forest plot. Meta-analyses can also help examine heterogeneity across study results. Meta-analyses are often carried out in conjunction with systematic reviews and similarly require a systematic search and screening of studies. Publication bias is also commonly examined in the context of a meta-analysis and is typically visually presented via a funnel plot.  **",
                "Reference(s)": "** Borenstein et al. (2011); [Yeung et al. (2021)](https://mgto.org/exp-ma-rr-template-folder)",
                "Drafted by": "** Martin Vasilev; Siu Kit Yeung",
                "Reviewed (or Edited) by": "** Thomas Rhys Evans; Tamara Kalandadze; Charlotte R. Pennington; Mirela Zaneva",
                "Translated by": "** Zainab Alsuhaibani",
                "Translation reviewed by": "** Ahlam Ahmed, Alaa M. Saleh, Ali H. Al-Hoorie, MOhammed Mohsen",
                "Translation": "التَّعريف**: التَّحليل البعدي (أو التَّلوي) هو تجميع إحصائي لنتائج عدد من الدِّراسات التي تبحث في نفس الظَّاهرة.  توجد طرق متنوعة من أساليب التَّحليل البعدي، بما في ذلك نماذج التَّأثيرات العشوائيَّة، أو الثَّابتة، أو الانحدارات البعديَّة، والتي تسمح بفحص التَّأثيرات الوسيطة. ومن خلال تجميع  البيانات من دراسات متعدِّدة، يمكن أن يوفِّر التَّحليل البعدي تقديرًا أكثر دِقّة لظاهرة ما، مثل: معاملة معيّنة من الدِّراسات الفرديَّة. عادة ما يتم تصوير النَّتائج في مخطط غابي. ويمكن أن تساعد التَّحليلات البعديَّة أيضًا في فحص غياب التَّجانس في نتائج الدِّراسات. وغالبًا ما يتم إجراء التَّحليلات البعديَّة بالاقتران مع المراجعات المنهجيَّة التي تتطلَّب كذلك بحثًا، وفحصًا منهجيًا للدِّراسات. ومن الشَّائع أيضًا فحص تحيُّز النَّشر في سياق التَّحليل البعدي، وعادة ما يتم تقديمه بشكل مرئي عبر مخطط قمعي. **المصطلحات ذات الصِّلة:** كونسرتCONSORT ، التَّحليل التَّلوي الارتباطي ، حجم الأثر ، توليف الأدلَّة ، المراجعات المنهجيَّة غير التدخلية المفتوحة والقابلة للتِّكرار؛ بريزما  PRISMA ، تحيُّز النَّشر (مشكلة درج الملفات) ؛ ستروب STROBE ؛ المراجعة المنهجيَّة",
                "Related_terms": "** CONSORT; Correlational Meta-Analysis; Effect size; Evidence synthesis; Non-Intervention, Reproducible, and Open Systematic Reviews (NIRO-SR); PRISMA; Publication bias (File Drawer Problem); STROBE; Systematic Review"
            },
            {
                "Title": "Metadata (البيانات الوصفيَّة) *",
                "Definition": "** Structured data that describes and synthesises other data. Metadata can help find, organize, and understand data. Examples of metadata include creator, title, contributors, keywords, tags, as well as any kind of information necessary to verify and understand the results and conclusions of a study such as codebook on data labels, descriptions, the sample and data collection process.  **",
                "Reference(s)": "** Gollwitzer et al. (2020); [https://schema.datacite.org/](https://schema.datacite.org/)",
                "Drafted by": "** Matt Jaquiery",
                "Reviewed (or Edited) by": "** Helena Hartmann; Tina Lonsdorf; Charlotte R. Pennington; Mirela Zaneva",
                "Translated by": "** Hiba Alomary",
                "Translation reviewed by": "** Ahlam Ahmed, Alaa M. Saleh, Ali H. Al-Hoorie, Mohammed Mohsen",
                "Translation": "التَّعريف**: بيانات مرتَّبة تجمع وتصف بيانات أخرى.  تساعد البيانات الوصفيَّة في العثور على البيانات، وترتيبها وفهمها. تتضمَّن أمثلة البيانات الوصفيَّة على معلومات المنتج،  والعنوان، والمساهمين، والكلمات المفتاحية والعلامات، بالإضافة إلى أي نوع من المعلومات اللَّازمة للتَّحقق وفهم  نتائج الدِّراسات واستنتاجاتها مثل كتاب الرُّموز حول تسميات البيانات، والأوصاف، ونوع العيِّنة وطريقة جمع البيانات. **المصطلحات ذات الصِّلة**: بيانات، البيانات المفتوحة. تعريف بديل: (إن أمكن) بيانات حول البيانات.",
                "Related_terms": "** Data; Open Data **Alternative definition:** (if applicable) Data about data"
            },
            {
                "Title": "Meta-science or Meta-research (العلوم البعديَّة/التَّلوية أو البحث البعدي/التَّلوي) *",
                "Definition": "** The scientific study of science itself with the aim to describe, explain, evaluate and/or improve scientific practices. Meta-science typically investigates scientific methods, analyses, the reporting and evaluation of data, the reproducibility and replicability of research results, and research incentives.  **",
                "Reference(s)": "** Ioannidis et al. (2015); Peterson and Panofsky (2020)",
                "Drafted by": "** Elizabeth Collins",
                "Reviewed (or Edited) by": "** Tamara Kalandadze; Lisa Spitzer; Olmo van den Akker",
                "Translated by": "** Ruwayshid",
                "Translation reviewed by": "** Ahlam Ahmed, Alaa M. Saleh, Ali H. Al-Hoorie, Mohammed Mohsen",
                "Translation": "التَّعريف:**  يشير المصطلح إلى الدِّراسة العلميَّة للعلم نفسه بهدف وصف وشرح وتقييم، وتطوير الممارسات العلميَّة. يبحث علم \"ماوراء العلوم \" عادة في الأساليب العلميَّة والتَّحليل وإعداد التَّقارير عن البيانات وتقييمها وإعادة إنتاج، أو تكرار نتائج البحث، وكذلك في حوافز البحث. المصطلحات ذات الصِّلة:",
                "Related_terms": "**"
            },
            {
                "Title": "Model (computational) (النُّموذج الحاسوبيّ) *",
                "Definition": "** Computational models aim to mathematically translate the phenomena under study to better understand, communicate and predict complex behaviours.  **",
                "Reference(s)": "** Guest and Martin (2020); Wilson and Collins (2019)",
                "Drafted by": "** Charlotte R. Pennington",
                "Reviewed (or Edited) by": "** Jamie P. Cockcroft; Mahmoud Elsherif; Meng Liu; Yu-Fang Yang; Michele C. Lim",
                "Translated by": "** Hiba Alomary",
                "Translation reviewed by": "** Ahlam Ahmed, Alaa M. Saleh, Ali H. Al-Hoorie, Mohammed Mohsen",
                "Translation": "التَّعريف**: تهدف النَّماذج الحاسوبيَّة إلى ترجمة الظَّواهر قيد  الدِّراسة رياضيًاً بشكل حسابي؛ لفهم السُّلوكيَّات المعقَّدة وتفسيرها، والتَّنبؤ بها بشكلٍ أفضل.  **المصطلحات ذات الصِّلة:** خوارزميّات، محاكاة البيانات، الفرضيَّة، النَّظريَّة، بناء النَّظريَّة",
                "Related_terms": "** algorithms; data simulation; hypothesis; theory; theory building"
            },
            {
                "Title": "Model (statistical) (النَّموذج الإحصائيّ ) *",
                "Definition": "** A mathematical representation of observed data that aims to reflect the population under study, allowing for the better understanding of the phenomenon of interest, identification of relationships among variables and predictions about future instances. A classic example would be the application of Chi square to understand the relationship between smoking and cancer (Doll & Hill, 1954\\)**.**  **",
                "Reference(s)": "** Doll and Hill (1954)",
                "Drafted by": "** Jamie P. Cockcroft",
                "Reviewed (or Edited) by": "** Alaa AlDoh; Mahmoud Elsherif; Meng Liu; Catia M. Oliveira; Charlotte R. Pennington",
                "Translated by": "** Ruwayshid",
                "Translation reviewed by": "** Ahlam Ahmed, Alaa M. Saleh, Hiba Alomary, Ali H. Al-Hoorie, Mohammed Mohsen",
                "Translation": "التَّعريف:**  تمثيل رياضي للبيانات المرصودة؛ بهدف عكس المجتمع البحثي قيد الدِّراسة، ممَّا يسمح بفهم أفضل للظَّاهرة محل الاهتمام، وكذلك تحديد العلاقات بين المتغيِّرات، أو التَّنبؤ بالحالات المستقبليَّة.  مثال شائع هو استخدام اختبار مربع كاي (Chi square) لفهم العلاقة بين التَّدخين والسَّرطان (Doll & Hill, 1954\\)**.**  **المصطلحات ذات الصِّة:** الاستدلال البايزي، النَّموذج الحاسوبي  ، النَّموذج الفلسفي ، اختبار دلالة الفرضيَّة الصِّفريَّة. **التَّعريف البديل:** نموذج رياضيّ يجسِّد مجموعة من الافتراضات الإحصائيَّة المتعلِّقة بتوليد بيانات العيِّنة ويستخدم في تطبيق التَّحليل الإحصائيّ",
                "Related_terms": "** Bayesian Inference; Model (computational); Model (philosophy); Null Hypothesis Significance Testing (NHST) **Alternative definition:** A mathematical model that embodies a set of statistical assumptions concerning the generation of sample data and is used to apply statistical analysis."
            },
            {
                "Title": "Model (philosophy) (النَّموذج الفلسفيّ) *",
                "Definition": "** The process by which a verbal description is formalised to remove ambiguity, while also constraining the dimensions a theory can span. The model is thus data derived. “Many scientific models are representational models: they represent a selected part or aspect of the world, which is the model’s target system” (Frigg & Hartman, 2020).  **",
                "Reference(s)": "** Frigg and Hartman, (2020); Glass and Martin (2008); Guest and Martin (2020)",
                "Drafted by": "** Mahmoud Elsherif",
                "Reviewed (or Edited) by": "Charlotte R. Pennington; Michele C. Lim",
                "Translated by": "** Ahlam Ahmed",
                "Translation reviewed by": "** Alaa M. Saleh, Hiba Alomary, Ali H. Al-Hoorie, Mohammed Mohsen",
                "Translation": "التَّعريف:** هو العمليَّة التي يتم من خلالها إضفاء الطَّابع الرَّسمي على الوصف اللَّفظي؛ لإزالة الغموض، مع تقييد الأبعاد التي يمكن أنْ تمتدَّ إليها النَّظريَّة؛ مما ينتج عنه اشتقاق النَّموذج من البيانات.  \"إنَّ العديد من النَّماذج العلميَّة هي نماذج تمثيليَّة: فهي تمثل جزءًا أو جانبًا مختارًا من العالم، وهو النِّظام المستهدف للنَّموذج\" (Frigg & Hartman, 2020). **المصطلحات ذات الصِّلة:** الفرضيَّة، النَّظريَّة،  بناء النَّظريَّة.",
                "Related_terms": "** Hypothesis; Theory; Theory building"
            },
            {
                "Title": "Multi-Analyst Studies (الدِّراسات متعدِّدة المحلّلين) *",
                "Definition": "** In typical empirical studies, a single researcher or research team conducts the analysis, which creates uncertainty about the extent to which the choice of analysis influences the results. In multi-analyst studies, two or more researchers independently analyse the same research question or hypothesis on the same dataset. According to Aczel and colleagues (2021), a multi-analyst approach may be beneficial in increasing our confidence in a particular finding; uncovering the impact of analytical preferences across research teams; and highlighting the variability in such analytical approaches.  **",
                "Reference(s)": "** Aczel et. al. (2021); Silberzahn et al. (2018)",
                "Drafted by": "** Sam Parsons",
                "Reviewed (or Edited) by": "** Tsvetomira Dumbalska; Mahmoud Elsherif; William Ngiam; Charlotte R. Pennington; Graham Reid; Barnabas Szaszi; Flávio Azevedo",
                "Translated by": "** Alaa M. Saleh",
                "Translation reviewed by": "** Hiba Alomary, Ali H. Al-Hoorie , Mohammed Mohsen",
                "Translation": "التَّعريف**: يجري التَّحليل في الدِّراسات الإحصائيَّة النَّموذجيَّة باحث واحد أو مجموعة بحثيَّة، وهذا يشكك في مدى تأثير اختيار نوع التَّحليل على النَّتائج.  في الدِّراسات متعدِّدة المحلّلين يقوم باحثان، أو أكثر بتحليل نفس سؤال، أو فرضيَّة البحث اعتمادًا على نفس مجموعة البيانات بشكل مستقل، ويري أكزل وزملاؤه (2021) أنَّ أسلوب تعدُّد محللي البيانات قد يفيد في زيادة ثقتنا بنتيجة معيّنة، وكشف تأثير التَّفضيلات التَّحليليَّة بين المجموعات البحثيَّة، وتسليط الضَّوء على الاختلافات بين أساليب التَّحليل. **المصطلحات ذات الصِّلة:** المرونة التَّحليليَّة، العلوم الجماعية، تحليل البيانات، حديقة المسارات المتشعِّبة، تحليل الأكوان المتعدِّدة، درجات حريَّة الباحث، الشَّفافيَّة العلمية.",
                "Related_terms": "** Analytic flexibility; Crowdsourcing science; Data Analysis; Garden of Forking Paths; Multiverse Analysis; Researcher Degrees of Freedom; Scientific Transparency"
            },
            {
                "Title": "Multiplicity (التَّعدديَّة) *",
                "Definition": "** Potential inflation of Type I error rates (incorrectly rejecting the null hypothesis) because of multiple statistical testing, for example, multiple outcomes, multiple follow-up time points, or multiple subgroup analyses. To overcome issues with multiplicity, researchers will often apply controlling procedures (e.g., Bonferroni, Holm-Bonferroni; Tukey) that correct the alpha value to control for inflated Type I errors. However, by controlling for Type I errors, one can increase the possibility of Type II errors (i.e., incorrectly accepting the null hypothesis).  **",
                "Reference(s)": "** Sato (1996); Schultz and Grimes (2005)",
                "Drafted by": "** Aidan Cashin",
                "Reviewed (or Edited) by": "** Jamie P. Cockcroft; Mahmoud Elsherif; Meng Liu; Charlotte R. Pennington",
                "Translated by": "** Mai Helmy.",
                "Translation reviewed by": "** Ahlam Ahmed, Alaa M. Saleh, Ali H. Al-Hoorie, Mohammed Mohsen",
                "Translation": "التَّعريف: ه**و التَّضخُّم المحتمل لمعدلات الخطأ من النُّوع الأوّل (رفض الفرضيَّة الصِّفريَّة بطريقة غير صحيحة) بسبب الاختبارات الإحصائيَّة المتعدِّدة، مثلًا حين استعمال النَّتائج المتعدِّدة، أو نقاط المتابعة المتعدِّدة، أو تحليلات المجموعات الفرعيَّة المتعدِّدة.  وللتَّغلب على مشكلات التَّعدديَّة، غالبًا ما يطبِّق الباحثون إجراءات التَّحكم (مثل  Bonferroni ، Holm-Bonferroni ، Tukey) التي تصحِّح قيمة ألفا للتَّحكم في أخطاء النُّوع الأول المتضخِّمة. ومع ذلك، ومن خلال التَّحكم في أخطاء النَّوع الأول ، قد تزداد احتمالية حدوث أخطاء من النَّوع الثَّاني للباحث (أي قبول الفرضيَّة الصِّفريَّة بطريقة غير صحيحة).  **المصطلحات ذات الصِّلة:** ألفا، معدل الاكتشاف الخاطئ، مشكلة المقارنات المتعدِّدة، اختبار متعدِّد، اختبار دلالة الفرضيَّة الصِّفريَّة",
                "Related_terms": "** Alpha; False Discovery Rate; Multiple comparisons problem; Multiple testing; Null Hypothesis Significance Testing (NHST)"
            },
            {
                "Title": "Multiverse analysis (تحليل الأكوان المتعدِّدة) *",
                "Definition": "** Multiverse analyses are based on all potentially equally justifiable data processing and statistical analysis pipelines that can be employed to test a single hypothesis. In a data multiverse analysis, a single set of raw data is processed into a multiverse of data sets by applying all possible combinations of justifiable preprocessing choices. Model multiverse analyses apply equally justifiable statistical models to the same data to answer the same hypothesis. The statistical analysis is then conducted on all data sets in the multiverse and all results are reported which enhances promoting transparency and illustrates the robustness of results against different data processing (data multiverse) or statistical (model multiverse) pipelines. Multiverse analysis differs from Specification curve analysis with regards to the graphical displays (a histogram and tile plota rather than a specification curve plot). **",
                "Reference(s)": "** Del Giudice and Gangestad (2021); Steegen et al. (2016)",
                "Drafted by": "** Tina Lonsdorf; Flávio Azevedo",
                "Reviewed (or Edited) by": "** Mahmoud Elsherif; Adrien Fillon; William Ngiam; Sam Parsons",
                "Translated by": "Ahlam Ahmed",
                "Translation reviewed by": "** Alaa M. Saleh, Ali H. Al-Hoorie, Mohammed Mohsen  ###  ### **N** {#n}",
                "Translation": "التَّعريف**: تستند التَّحليلات متعدِّدة الأكوان إلى عمليَّات معالجة البيانات، والتَّحليل الإحصائي التي يمكن تبريرها بشكل متساوٍ، والتي يمكن استخدامها لاختبار فرضيَّة واحدة في تحليل البيانات متعدِّدة الأكوان. تتم معالجة مجموعة واحدة من البيانات الأوليَّة (الخام) في أكوان متعدِّدة من مجموعات البيانات من خلال تطبيق جميع الطُّرق الممكنة من خيارات المعالجة المسبقة المبرَّرة. تُطبّق تحليلات نموذج الأكوان المتعدِّدة نماذج إحصائيَّة مبرّرة بنفس القدر على نفس البيانات للإجابة على نفس الفرضيَّة. ثم يتم إجراء التَّحليل الإحصائي على جميع مجموعات البيانات في الكون المتعدِّد، ويعد تقريراً عن  جميع النَّتائج ممَّا يعزِّز الشَّفافيَّة، ويوضح متانة النَّتائج مقابل معالجة البيانات المختلِفة (البيانات متعدِّدة الأكوان) أو المعالجة الإحصائيَّة (النَّماذج متعدِّدة الأكوان). يختلف تحليل الأكوان المتعدِّدة عن تحليل منحنى المواصفات فيما يتعلَّق بالعروض الرُّسوميَّة (الرَّسم البياني، أو المدرج التِّكراري، ومخطَّط التَّجانب بدلًا عن مخطَّط منحنى المواصفات). **المصطلحات ذات الصِّلة:** حديقة المسارات المتشعِّبة، المتانة (في التَّحليلات)،  تحليل منحنى المواصفات،  اهتزاز التَّأثيرات.",
                "Related_terms": "** Garden of forking paths; Robustness (analyses); Specification curve analysis; Vibration of effects"
            },
            {
                "Title": "Name Ambiguity Problem (مشكلة ازدواجية الاسم) *",
                "Definition": "** An attribution issue arising from two related problems: authors may use multiple names or monikers to publish work, and multiple authors in a single field may share full names. This makes accurate identification of authors on names and specialisms alone a difficult task. This can be addressed through the creation and use of unique digital identifiers that act akin to digital fingerprints such as ORCID.  **",
                "Reference(s)": "** Wilson and Fenner (2012)",
                "Drafted by": "** Shannon Francis",
                "Reviewed (or Edited) by": "** Tsvetomira Dumbalska; Mahmoud Elsherif; Helena Hartmann; Wanyin Li; Charlotte R. Pennington",
                "Translated by": "** Dr.Nazik Alnour",
                "Translation reviewed by": "** Alaa M. Saleh, Hiba Alomary, Ali H. Al-Hoorie, Mohammed Mohsen",
                "Translation": "التَّعريف:** هي مشكلة  الإسناد والتي تنشأ من مشكلتين ذات صلة بالموضوع: وهي أنَّ المؤلفين ربَّما يستخدمون أسماء، أو ألقاب متعدِّدة، لنشر أعمالهم، وربَّما يشترك عدَّة مؤلفين في مجال واحد في الاسم كاملًا. وهذا يجعل إثبات شخصيَّة المؤلِّفين بالأسماء والتَّخصُّصات أمرًا صعبًا. ويمكن تجاوز هذه المشكلة بإنشاء معرِّفات رقميَّة متميزة مطابقة للبصمة الرَّقمية مثل أوركيد (ORCID).  **المصطلحات ذات الصِّلة:** التَّأليف، معرّف الكائن الرَّقمي، الأوركيد",
                "Related_terms": "** Authorship; DOI (digital object identifier); ORCID (Open Researcher and Contributor ID)"
            },
            {
                "Title": "Named entity-based Text Anonymization for Open Science (NETANOS) (إخفاء هُويَّة النَّص المعتمد على الكيان للعلم المفتوح) *",
                "Definition": "** A free, open-source anonymisation software that identifies and modifies named entities (e.g. persons, locations, times, dates). Its key feature is that it preserves critical context needed for secondary analyses. The aim is to assist researchers in sharing their raw text data, while adhering to research ethics.  **",
                "Reference(s)": "** Kleinberg et al. (2017)",
                "Originally drafted by": "** Norbert Vanek",
                "Reviewed (or Edited) by": "** Jamie P. Cockcroft; Aleksandra Lazić; Charlotte R. Pennington; Sam Parsons Elif Bastan**,**",
                "Translated by": "** Mai Helmy.",
                "Translation reviewed by": "** Alaa M. Saleh, Ali H. Al-Hoorie, Mohammed Mohsen",
                "Translation": "التَّعريف:** هو برنامج مجاني مفتوح المصدر؛ لإخفاء الهويَّة ويعمل على تحديد وتعديل الكيانات المذكورة (مثل الأشخاص، والمواقع، والأوقات، والتَّواريخ). وتتمثَّل ميزته الرَّئيسيَّة في أنَّه يحافظ على السِّياق الدَّقيق اللَّازم للتَّحليلات الثَّانويَّة.  والهدف هو مساعدة الباحثين في مشاركة بياناتهم النَّصيَّة الأوليَّة ، مع الالتزام بأخلاقيات البحث.  **المصطلحات ذات الصِّلة:** التّعمية، السِّريَّة، مشاركة البيانات، أخلاقيَّات البحث",
                "Related_terms": "** Anonymity; Confidentiality; Data sharing; Research ethics"
            },
            {
                "Title": "Non-Intervention, Reproducible, and Open Systematic Reviews (NIRO-SR) (المراجعات المنهجيَّة غير التَّدخليَّة المفتوحة، والقابلة للتِّكرار) *",
                "Definition": "** A comprehensive set of tools to facilitate the development, preregistration and dissemination of systematic literature reviews for non-intervention research. Part A represents detailed guidelines for creating and preregistering a systematic review protocol in the context of non-intervention research whilst preparing for transparency. Part B represents guidelines for writing up the completed systematic review, with a focus on enhancing reproducibility.  **",
                "Reference(s)": "** Topor et al. (2021)",
                "Drafted by": "** Asma Assaneea",
                "Reviewed (or Edited) by": "** Tsvetomira Dumbalska; Thomas Rhys Evans; Tamara Kalandadze; Jade Pickering; Mirela Zaneva",
                "Translated by": "** Ahlam Ahmed",
                "Translation reviewed by": "** Alaa M. Saleh, Ali H. Al-Hoorie, Mohammed Mohsen",
                "Translation": "التَّعريف:** مجموعة شاملة من الأدوات؛ لتسهيل التَّطوير، والتَّسجيل المسبق، ونشر مراجعات الأدبيَّات المنهجيَّة للبحوث غير التَّدخليَّة. يمثل الجزء (أ) مبادئ توجيهيَّة (إرشادات) مفصَّلة لإنشاء إجراءات مراجعة منهجيَّة وتسجيله مسبقًا في سياق بحث غيرتدخلي أثناء التَّحضير للشَّفافيَّة، ويمثِّل الجزء (ب) مبادئ توجيهيَّة (إرشادات) لكتابة المراجعة المنهجيَّة المنجزة، مع التَّركيز على تعزيز إمكانيَّة التِّكرار.  **المصطلحات ذات الصِّلة:** تراكم المعرفة، المراجعة المنهجيَّة، بروتوكول المراجعة المنهجيَّة.",
                "Related_terms": "** Knowledge accumulation; Systematic review; Systematic Review Protocol"
            },
            {
                "Title": "Null Hypothesis Significance Testing (NHST) (اختبار دلالة الفرضيَّة الصِّفريَّة) *",
                "Definition": "** A frequentist approach to inference used to test the probability of an observed effect against the null hypothesis of no effect/relationship (Pernet, 2015). Such a conclusion is arrived at through use of an index called the *p*\\-value. Specifically, researchers will conclude an effect is present when an a priori alpha threshold, set by the researchers, is satisfied; this determines the acceptable level of uncertainty and is closely related to Type I error.  **",
                "Reference": "** Lakens et al. (2018); Pernet (2015); Spence and Stanley (2018)",
                "Originally drafted by": "** Alaa AlDoh",
                "Reviewed (or Edited) by": "** Jamie P. Cockcroft; Annalise A. LaPlume; Charlotte R. Pennington; Sonia Rishi",
                "Translated by": "** Ruwayshid",
                "Translation reviewed by": "** Alaa M. Saleh, Mohammed Mohsen  ### **O** {#o}",
                "Translation": "التَّعريف**: يستخدم المنهج التِّكراري للاستنتاج لفحص احتماليَّة أن يكون هناك تأثير مرصود  ضد الفرضيَّة الصِّفرية التي تفترض عدم وجود علاقة أو تأثير (Pernet, 2015). ويتم الوصول لهذا الاستنتاج من خلال رقم استدلالي يسمى قيمة ألفا. وعلى وجه التَّحديد، يستنتج الباحثون وجود تأثير ما إذا تحقَّقت شروط القيمة الحدية لألفا التي وضعت مسبقا من قبلهم، وهذا ما يحدِّد المستوى المقبول من الاحتمالية وكذلك يكون قريبًا جدًا من الخطأ من النَّوع الأوّل.  ال**مصطلحات ذات الصِّلة:** الاستدلال، قيمة ألفا (p)، الدِّلالة الإحصائية، الخطأ من النَّوع الأوّل.",
                "Related_terms": "** Inference; P-value; Statistical significance; Type I error"
            },
            {
                "Title": "Objectivity (الموضوعيَّة) *",
                "Definition": "** The idea that scientific claims, methods, results and scientists themselves should remain value-free and unbiased, and thus not be affected by cultural, political, racial or religious bias as well as any personal interests (Merton, 1942).  **",
                "Reference(s)": "** Macfarlane and Cheng (2008); Merton (1942)",
                "Originally drafted by": "** Ryan Millager",
                "Reviewed (or Edited) by": "** Mahmoud Elsherif; Madeleine Ingham; Kai Krautter; Sam Parsons; Charlotte R. Pennington",
                "Translated by": "** Zainab Alsuhaibani",
                "Translation reviewed by": "** Awatif Alruwaili, Alaa M. Saleh, Ali H. Al-Hoorie, Mohammed Mohsen",
                "Translation": "التَّعريف:** تشير الموضوعيَّة إلى أنَّه يجب أنْ تكون الادِّعاءات، والطُّرق، والنَّتائج العلميَّة غير منحازة، وذلك يشمل عدم انحياز العلماء أنفسهم، وبالتَّالي عدم تأثرهم بالتَّحيُّز الثَّقافي، أو السِّياسي، أو العرقي، أو الدِّيني وكذلك أي مصالح شخصيَّة (ميرتون ، 1942).  **المصطلحات ذات الصِّة:** التَّشاركيَّة، قواعد ميرتون، الحياديَّة",
                "Related_terms": "** Communality; Mertonian norms; Neutrality"
            },
            {
                "Title": "Ontology (Artificial Intelligence) (علم الوجود (الذَّكاء الاصطناعي)) *",
                "Definition": "** A set of axioms in a subject area that help classify and explain the nature of the entities under study and the relationships between them.  **",
                "Reference": "** Noy and McGuinness (2001)",
                "Originally drafted by": "** Emma Norris",
                "Reviewed (or Edited) by": "** Charlotte R. Pennington; Graham Reid",
                "Translated by": "** Hiba Alomary",
                "Translation reviewed by": "** Awatif Alruwaili, Alaa M. Saleh, Hiba Alomary, Ali H. Al-Hoorie, Mohammed Mohsen",
                "Translation": "التَّعريف:** مجموعة من البديهيات في أي مجال، والتي  تساعد على تصنيف طبيعة العناصر تحت الدِّراسة، وشرح العلاقة بينها.  **المصطلحات ذات الصِّلة**: علم وظائف العناصر، نظريَّة المعرفة، علم التَّصنيف.",
                "Related_terms": "** Axiology; Epistemology; Taxonomy"
            },
            {
                "Title": "Open access (الوصول المفتوح) *",
                "Definition": "** “Free availability of scholarship on the public internet, permitting any users to read, download, copy, distribute, print, search, or link to the full texts of these research articles, crawl them for indexing, pass them as data to software, or use them for any other lawful purpose, without financial, legal, or technical barriers other than those inseparable from gaining access to the internet itself” (Boai, 2002). Different methods of achieving open access (OA) are often referred to by color, including Green Open Access (when the work is openly accessible from a public repository), Gold Open Access (when the work is immediately openly accessible upon publication via a journal website), and Platinum (or Diamond) Open Access (a subset of Gold OA in which all works in the journal are immediately accessible after publication from the journal website without the authors needing to pay an article processing fee \\[APC\\]).  **",
                "Reference(s)": "** [Budapest Open Access Initiative (2002)](https://www.budapestopenaccessinitiative.org/read); Suber (2015)",
                "Drafted by": "** Mahmoud Elsherif",
                "Reviewed (or Edited) by": "** Nick Ballou; Helena Hartmann; Aoife O’Mahony; Ross Mounce; Mariella Paul; Charlotte R. Pennington",
                "Translated by": "** Zainab Alsuhaibani",
                "Translation reviewed by": "Alaa M. Saleh, Ali H. Al-Hoorie, Mohammed Mohsen",
                "Translation": "التَّعريف:** هو \"التَّوفر المجّاني للمعرفة على الإنترنت، ممَّا يسمح لأي مستخدم بقراءة النُّصوص الكاملة للمقالات البحثيَّة، أو تنزيلها، أو نسخها، أو توزيعها، أو طباعتها، أو البحث عنها، أو الارتباط بها، أو فهرستها، أو استخدامها كبيانات في البرامج، أو استخدامها لأي غرض قانوني آخر بدون عوائق ماليَّة، أو قانونيَّة، أو تقنيَّة، بخلاف من لا يمكنه الوصول إلى الإنترنت بحد ذاته\" (Boai, 2002). وغالبًا ما تستخدم الألوان للإشارة للطُّرق المختلفة للوصول المفتوح، بما في ذلك: الوصول المفتوح الأخضر (وذلك عندما يكون العمل متاحًا بشكل مفتوح من مستودع عام)، والوصول المفتوح الذَّهبي (وذلك عندما يكون العمل متاحًا مباشرة عند النَّشر عبر موقع المجلة على الويب)، والوصول المفتوح البلاتينيوم (أو الألماسي) (وهو فرع من الوصول المفتوح الذَّهبي حيث يمكن الوصول إلى جميع الأعمال في المجلَّة فورًا بعد النَّشر من موقع المجلَّة على الويب دون حاجة المؤلِّفين إلى دفع رسوم نشر المقالة).  **المصطلحات ذات الصِّلة:** رسوم النَّشر، مبادئ فير، حاجز مالي، الطّّباعة الأوليَّة، مستودع البيانات.",
                "Related_terms": "** Article Processing Charge; FAIR principles; Paywall; Preprint; Repository"
            },
            {
                "Title": "Open Code (النَّص البرمجي المفتوح) *",
                "Definition": "** Making computer code (e.g., programming, analysis code, stimuli generation) freely and publicly available in order to make research methodology and analysis transparent and allow for reproducibility and collaboration. Code can be made available via open code websites, such as GitHub, the Open Science Framework, and Codeshare (to name a few), enabling others to evaluate and correct errors and re-use and modify the code for subsequent research.  **",
                "Reference": "** Easterbrook (2014)",
                "Originally drafted by": "** Charlotte R. Pennington",
                "Reviewed (or Edited) by": "** Elizabeth Collins; Mahmoud Elsherif; Christopher Graham; Emma Henderson",
                "Translated by": "Mai Helmy.",
                "Translation reviewed by": "Alaa M. Saleh, Hiba Alomary, Ali H. Al-Hoorie, Ahlam Ahmed, Mohammed Mohsen",
                "Translation": "التَّعريف:** إتاحة النَّص البرمجي للحاسوب (مثل البرمجة، والنص البرمجي للتَّحليل، وتوليد المحفِّزات) مجانًا للجمهور من أجل جعل منهجيَّة البحث، والتَّحليل واضحة، والسَّماح بإعادة الإنتاج والتشارك البحثي.  ويمكن إتاحة  الرمز (الكود) عبر مواقع الويب ذات البرمجيات المفتوحة، مثل GitHub و Open Science Framework و Codeshare (على سبيل المثال لا الحصر) ، مما يتيح للآخرين تقييم الأخطاء وتصحيحها وإعادة استخدام وتعديل الكود للبحث اللاحق.  المصطلحات ذات الصِّلة: الاستنساخ الحسابي، الوصول المفتوح، التَّراخيص المفتوحة، المواد المفتوحة، المصدر المفتوح، برنامج مفتوح المصدر، قابليَّة إعادة الإنتاج، بناء الجملة.",
                "Related_terms": "** Computational Reproducibility; Open Access; Open Licensing; Open Material; Open Source; Open Source Software; Reproducibility; Syntax"
            },
            {
                "Title": "Open Data (البيانات المفتوحة) *",
                "Definition": "** Open data refers to data that is freely available and readily accessible for use by others without restriction, “Open data and content can be freely used, modified, and shared by anyone for any purpose” ([https://opendefinition.org/](https://opendefinition.org/)). Open data are subject to the requirement to attribute and share alike, thus it is important to consider appropriate Open Licenses. Sensitive or time-sensitive datasets can be embargoed or shared with more selective access options to ensure data integrity is upheld.  **",
                "Reference": "** [https://opendefinition.org/](https://opendefinition.org/) (version 2.1); [https://opendatahandbook.org/guide/en/what-is-open-data/](https://opendatahandbook.org/guide/en/what-is-open-data/)",
                "Originally drafted by": "** Lisa Spitzer",
                "Reviewed (or Edited) by": "** Sarah Ashcroft-Jones; Mahmoud Elsherif; Helena Hartmann; Matt Jaquiery; Flávio Azevedo; Ross Mounce; Charlotte R. Pennington; Steven Verheyen",
                "Translated by": "** Zainab Alsuhaibani",
                "Translation reviewed by": "** Alaa M. Saleh, Ali H. Al-Hoorie, Asma Alzahrani, Mohammed Mohsen",
                "Translation": "التَّعريف:** تشير البيانات المفتوحة إلى البيانات المتاحة مجانًا، والتي تمكّن الآخرون من الوصول إليها بسهولة للاستخدام دون قيود.  \"يمكن استخدام البيانات والمحتوى المفتوح وتعديله ومشاركته مجانًا من قبل أي شخص لأي غرض\" (https://opendefinition.org/). وتخضع البيانات المفتوحة لمتطلَّبات السِّمة، والمشاركة على حد سواء. ولذلك فإنَّه من المهم النَّظر في التَّراخيص المفتوحة المناسبة. ويمكن حظر مجموعة البيانات الحساسة، أو الحساسة للوقت، أو مشاركتها مع خيارات وصول أكثر انتقائيَّة لضمان سلامة البيانات. **المصطلحات ذات الصِّلة:** الشَّارات (العلم المفتوح)، توافر البيانات، مبادئ فير، البيانات الوصفيَّة، التَّراخيص المفتوحة، المواد المفتوحة، قابليَّة إعادة النَّتائج، تحليل البيانات الثَّانويَّة",
                "Related_terms": "** Badges (Open Science); Data availability; FAIR principles; Metadata; Open Licenses; Open Material; Reproducibility; Secondary data analysis"
            },
            {
                "Title": "Open Educational Resources (OERs) (المصادر التَّعليميَّة المفتوحة) *",
                "Definition": "** Learning materials that can be modified and enhanced because their creators have given others permission to do so. The individuals or organizations that create OERs—which can include materials such as presentation slides, podcasts, syllabi, images, lesson plans, lecture videos, maps, worksheets, and even entire textbooks—waive some (if not all) of the copyright associated with their works, typically via legal tools like Creative Commons licenses, so others can freely access, reuse, translate, and modify them.  **",
                "Reference": "** [https://opensource.com/resources/what-open-education](https://opensource.com/resources/what-open-education); [https://en.unesco.org/themes/building-knowledge-societies/oer](https://en.unesco.org/themes/building-knowledge-societies/oer)",
                "Originally drafted by": "** Aleksandra Lazić",
                "Reviewed (or Edited) by": "** Sam Parsons; Charlotte R. Pennington; Steven Verheyen; Elizabeth Collins",
                "Translated by": "** Mahdi Aben Ahmed",
                "Translation reviewed by": "** Alaa M. Saleh, Ali H. Al-Hoorie, Asma Alzahrani, Mohammed Mohsen",
                "Translation": "التَّعريف:** هي المواد التَّعليميَّة التي يمكن تعديلها، وتحسينها؛ لأنَّ منشئيها منحوا الإذن للآخرين للقيام بذلك. يتنازل الأفراد، أو المنظّمات التي تنشئ موارد تعليميَّة مفتوحة، والتي يمكن أن تتضمَّن مواد مثل شرائح العرض التَّقديمي، والبودكاست، والمناهج الدِّراسيَّة، والصُّور، وخطط الدُّروس، ومقاطع فيديوهات المحاضرات، والخرائط، وأوراق العمل، وحتى الكتب المدرسيَّة بأكملها \\- عن بعض (إن لم يكن كل) حقوق النشر المرتبطة بأعمالهم- عادةً عن طريق الأدوات القانونية مثل تراخيص المشاع الإبداعي، بحيث يمكن للآخرين الوصول إليها وإعادة استخدامها وترجمتها وتعديلها بحُرية.  **المصطلحات ذات الصِّلة:** إمكانية الوصول، فورت، الوصول المفتوح، التَّراخيص المفتوحة، المواد المفتوحة",
                "Related_terms": "** Accessibility; FORRT; Open access; Open Licenses; Open Material"
            },
            {
                "Title": "Open Educational Resources (OER) Commons (مصادر التعَّلُّم المفتوحة العامّة)  *",
                "Definition": "** OER Commons (with OER standing for open educational resources) is a freely accessible online library allowing teachers to create, share and remix educational resources. The goal of the OER movement is to stimulate “collaborative teaching and learning” ([https://www.oercommons.org/about](https://www.oercommons.org/about)) and provide high-quality educational resources that are accessible for everyone.  **",
                "Reference(s)": "** www.oercommons.org",
                "Drafted by": "** Ali H. Al-Hoorie",
                "Reviewed (or Edited) by": "** Mahmoud Elsherif, Gisela H. Govaart",
                "Translated by": "** Asma Alzahrani",
                "Translation reviewed by": "** Mahdi Aben Ahmed, Alaa M. Saleh, Ali H. Al-Hoorie, Mohammed Mohsen",
                "Translation": "التَّعريف**: مصادر التَّعلُّم المفتوحة هي مكتبة مجانيَّة على الإنترنت تُمكِّن المعلِّمين من إنشاء، ومشاركة ودمج المصادر التَّعليميَّة.  تهدف حركة مصادر التَّعلُّم المفتوح إلى تحفيز\"التعليم والتَّعلُّم التَّعاوني\" ([https://www.oercommons.org/about](https://www.oercommons.org/about)) كما تقدِّم موارد تعليميَّة ذات جودة عالية متاحة للجميع. **المصطلحات ذات الصِّلة:** العدالة، فورت، الشمول، قاعدة المعرفة  للمنح الدِّراسية المفتوحة، إطار العلوم المفتوحة.",
                "Related_terms": "** Equity; FORRT; Inclusion; Open Scholarship Knowledge Base; Open Science Framework"
            },
            {
                "Title": "Open Licenses (التَّراخيص المفتوحة) *",
                "Definition": "** Open licenses are provided with open data and open software (e.g., analysis code) to define how others can (re)use the licensed material. In setting out the permissions and restrictions, open licenses often permit the unrestricted access, reuse and retribution of an author’s original work. Datasets are typically licensed under a type of open licence known as a Creative Commons license (e.g., MIT, Apache, and GPL). These can differ in relatively subtle ways with GPL licenses (and their variants) being Copyleft licenses that require that any derivative work is licensed under the same terms as the original. **",
                "Reference(s)": "** [https://opensource.org/licenses](https://opensource.org/licenses)",
                "Originally drafted by": "** Andrew J. Stewart",
                "Reviewed (or Edited) by": "** Elizabeth Collins; Sam Parsons; Graham Reid; Steven Verheyen",
                "Translated by": "** Ahlam Ahmed",
                "Translation reviewed by": "** Alaa M. Saleh, Ahlam Ahmed, Ali H. Al-Hoorie, Mohammed Mohsen",
                "Translation": "التَّعريف:** تُلحق التَّراخيص المفتوحة بالبيانات، والبرامج المفتوحة (مثل النَّص البرمجي للتَّحليل)؛ لتحديد كيف يمكن للآخرين (إعادة) استخدام المواد المرخّصة. فعند تحديد الأذونات والقيود، غالبًا ما تسمح التَّراخيص المفتوحة بالوصول غير المقيد إلى العمل الأصلي للمؤلف وإعادة استخدامه. عادةً ما يتم ترخيص مجموعات البيانات بموجب نوع من التَّرخيص المفتوح المعروف باسم ترخيص المشاع الإبداعي، على سبيل المثال: معهد ماساتشوستس للتكنولوجيا، وأباتشي، والتَّرخيص العام). يمكن أنْ تختلف هذه التَّراخيص في بعض التَّفاصيل، فمثلًا تلزم التَّراخيص العاملة (وأشكالها المختلفة) كونها تراخيص الحقوق المتروكة التي تتطلَّب أن يتم ترخيص أي عمل مشتق بموجب نفس شروط العمل الأصلي. **المصطلحات ذات الصِّلة:** رخصة المشاع الإبداعي،  الحقوق المتروكة  حقوق النَّشر،  رخصة،  البيانات المفتوحة،  المصدر المفتوح.",
                "Related_terms": "** Creative Commons (CC) License; Copyleft; Copyright; Licence; Open Data; Open Source"
            },
            {
                "Title": "Open Material (المواد المفتوحة) *",
                "Definition": "** Author’s public sharing of materials that were used in a study, “such as survey items, stimulus materials, and experiment programs” (Kidwell et al., 2016, p. 3). Digitally-shareable materials are posted on open access repositories, which makes them publicly available and accessible. Depending on licensing, the material can be reused by other authors for their own studies. Components that are not digitally-shareable (e.g. biological materials, equipment) must be described in sufficient detail to allow reproducibility. **",
                "Reference": "** Blohowiak et al. (2020); Kidwell et al. (2016)",
                "Originally drafted by": "** Lisa Spitzer",
                "Reviewed (or Edited) by": "** Sam Parsons; Charlotte R. Pennington; Olly Robertson; Emily A. Williams; Flávio Azevedo",
                "Translated by": "** Ahlam Ahmed",
                "Translation reviewed by": "** Alaa M. Saleh, Ali H. Al-Hoorie, Asma Alzahrani, Mohammed Mohsen",
                "Translation": "التَّعريف:** مشاركة المؤلِّف للمواد التي تم استخدامها في الدِّراسة، مثل أسئلة الاستبيان، والمواد التَّحفيزيَّة وبرامج التَّجارب. (Kidwell et al., 2016, p. 3). يتم نشر المواد القابلة للمشاركة رقميًا في مستودعات الوصول المفتوح، ممَّا يجعلها متاحة للجمهور، ويمكن الوصول إليها. وبحسب التَّرخيص، فإنَّه يمكن إعادة استخدام المواد من قبل مؤلِّفين آخرين في دراساتهم. يجب وصف المكونات غير القابلة للمشاركة رقميًا (مثل المواد البيولوجيَّة والمعدَّات) بتفاصيل كافية للسَّماح بإمكانيَّة التِّكرار. **المصطلحات ذات الصِّلة:** الشَّارات (العلم المفتوح)، مصداقيَّة الادِّعاءات العلميَّة، مبادئ فير،  الوصول المفتوح، النَّص البرمجي المفتوح، البيانات المفتوحة، قابليَّة إعادة الإنتاج ، الشَّفافيَّة.",
                "Related_terms": "** Badges (Open Science); Credibility of scientific claims; FAIR principles; Open Access; Open Code; Open Data; Reproducibility; Transparency"
            },
            {
                "Title": "OpenNeuro (المنصَّة العصبيَّة المفتوحة) *",
                "Definition": "** A free platform where researchers can freely and openly share, browse, download and re-use brain imaging data (e.g., MRI, MEG, EEG, iEEG, ECoG, ASL, and PET data).  **",
                "Reference(s)": "** Poldrack et al. (2013); Poldrack and Gorgolewski (2014) https://openneuro.org/",
                "Drafted by": "** Ali H. Al-Hoorie",
                "Reviewed (or Edited) by": "** Leticia Micheli, Gisela H. Govaart",
                "Translated by": "** Mai Helmy.",
                "Translation reviewed by": "** Alaa M. Saleh, Ahlam Ahmed, Ali H. Al-Hoorie, Mohammed Mohsen",
                "Translation": "التَّعريف:** هذه منصَّة مجانيَّة تمكِّن الباحثين من مشاركة بيانات تصوير الدِّماغ وتصفُّحها وتنزيلها وإعادة استخدامها بحريَّة وبشكل مفتوح، مثل: بيانات التَّصوير بالرَّنين المغناطيسي و MEG و EEG و iEEG و ECoG و ASL و PET).  **المصطلحات ذات الصِّلة:** بنية بيانات تصوير الدِّماغ، البيانات المفتوحة، التَّصوير بالرَّنين المغناطيسي المفتوح.",
                "Related_terms": "** BIDS data structure; Open data; OpenfMRI"
            },
            {
                "Title": "Open Peer Review (التّحكيم المفتوح) *",
                "Definition": "** A scholarly review mechanism providing disclosure of any combination of author and referee identities, as well as peer-review reports and editorial decision letters, to one another or publicly at any point during or after the peer review or publication process. It may also refer to the removal of restrictions on who can participate in peer review and the platforms for doing so. Note that ‘open peer review’ has been used interchangeably to refer to any, or all, of the above practices.  **",
                "Reference(s)": "Ross-Hellauer (2017)",
                "Originally drafted by": "** Sonia Rishi",
                "Reviewed (or Edited) by": "** Mahmoud Elsherif; Sam Parsons; Charlotte R. Pennington; Yuki Yamada; Flávio Azevedo",
                "Translated by": "Ahlam Ahmed",
                "Translation reviewed by": "** Alaa M. Saleh, Ali H. Al-Hoorie, Asma Alzahrani, Mohammed Mohsen",
                "Translation": "التَّعريف**: آليَّة تحكيم علميَّة يُكشف فيها عن هويات المؤلِّف، أو المحكّم، أو كليهما، بالإضافة إلى تقارير المحكّمين، وخطابات قرار المحررين لبعضهم البعض، علنًا في أي وقت أثناء، أو بعد عمليَّة التحكيم أو النَّشر.  وقد يشير أيضًا إلى إزالة القيود المفروضة على من يمكنه المشاركة في التحكيم والمنصَّات اللَّازمة للقيام بذلك. وتجدر الإشارة أنَّ التحكيم المفتوح  قد استخدم بالتَّبادل للإشارة إلى أي من الممارسات المذكورة أعلاه أو جميعها. **المصطلحات ذات الصِّلة:** التحكيم من دون تعمية، العلم المفتوح، مبادرة انفتاح تحكيم الأقران؛ التحكيم الشَّفاف.",
                "Related_terms": "** Non-anonymised peer review; Open science; PRO (peer review openness) initiative; Transparent peer review"
            },
            {
                "Title": "Open Scholarship (المعرفة المفتوحة) *",
                "Definition": "‘**Open scholarship’ is often used synonymously with ‘open science’, but extends to all disciplines, drawing in those which might not traditionally identify as science-based. It reflects the idea that knowledge of all kinds should be openly shared, transparent, rigorous, reproducible, replicable, accumulative, and inclusive (allowing for all knowledge systems). Open scholarship includes all scholarly activities that are not solely limited to research such as teaching and pedagogy. **",
                "Reference(s)": "** Tennant et al. (2019) Foundations for Open Scholarship Strategy Development https://www.researchgate.net/publication/330742805\\_Foundations\\_for\\_Open\\_Scholarship\\_Strategy\\_Development",
                "Drafted by": "** Gerald Vineyard",
                "Reviewed (or Edited) by": "** Mahmoud Elsherif; Zoe Flack; Sam Parsons; Charlotte R. Pennington",
                "Translated by": "** Mahdi Aben Ahmed",
                "Translation reviewed by": "** Awatif Alruwaili, Alaa M. Saleh, Ali H. Al-Hoorie, Mohammed Mohsen",
                "Translation": "التَّعريف:** غالبًا ما يتم استخدام المعرفة المفتوحة بشكل مرادف لـ \"العلوم المفتوحة\" ، ولكنَّها تمتدُّ إلى جميع التَّخصُّصات ، مع الأخذ في الاعتبار تلك التي قد لا يتم تحديدها تقليديًا على أنها من ضمن العلوم. وتعكس مفهوم المعرفة المفتوحة أنَّ المعرفة بجميع أنواعها يجب أن تكون مشتركة بشكل مفتوح، وشفاف، وصارم، وقابل لإعادة الإنتاج، والتِّكرار، وتراكميَّة، وشاملة (تسمح لجميع أنظمة المعرفة). تشمل المعرفة المفتوحة جميع الأنشطة العلميَّة التي لا تقتصر فقط على البحث مثل التَّدريس، والبيداغوجيا. **المصطلحات ذات الصِّلة:** بروبنساينس، مقاومة الاستعمار، المعرفة، البحث المفتوح، العلم المفتوح",
                "Related_terms": "** Bropenscience; Decolonisation; Knowledge; Open Research; Open Science"
            },
            {
                "Title": "Open Scholarship Knowledge Base (قاعدة المعرفة للمنح الدِّراسيَّة المفتوحة ) # *",
                "Definition": "** The Open Scholarship Knowledge Base (OSKB) is a collaborative initiative to share knowledge on the what, why and how of open scholarship to make this knowledge easy to find and apply. Information is curated and created by the community. The OSKB is a community under the Center for Open Science (COS).  **",
                "Reference(s)": "** www.oercommons.org/hubs/OSKB",
                "Drafted by": "** Ali H. Al-Hoorie",
                "Reviewed (or Edited) by": "** Mahmoud Elsherif; Samuel Guay; Tamara Kalandadze",
                "Translated by": "** Asma Alzahrani",
                "Translation reviewed by": "Alaa M. Saleh, Ahlam Ahmed, Ali H. Al-Hoorie, Mohammed Mohsen",
                "Translation": "التَّعريف**: قاعدة المعرفة المفتوحة (OSKB) هي مبادرة تعاونيَّة لمشاركة المعرفة بماذا، ولماذا، وكيف يكون العلم المفتوح؛ وذلك لتسهيل المعرفة بالعلم المفتوح وبتطبيقه، كما يتم اختيار، وإنشاء المعلومات وتنظيمها بدقَّة من قِبل المجتمع.  وتعدُّ قاعدة المعرفة المفتوحة مجتمعًا تابعًا لمركز العلم المفتوح. **المصطلحات ذات الصِّلة:** مركز العلم المفتوح ، المصادر التَّعليميَّة  المفتوحة، المعرفة المفتوحة،  العلم المفتوح.",
                "Related_terms": "** Center for Open Science (COS), Open Educational Resources (OERs); Open scholarship; Open Science"
            },
            {
                "Title": "Open Science (العلم المفتوح) *",
                "Definition": "** An umbrella term reflecting the idea that scientific knowledge of all kinds, where appropriate, should be openly accessible, transparent, rigorous, reproducible, replicable, accumulative, and inclusive, all which are considered fundamental features of the scientific endeavour. Open science consists of principles and behaviors that promote transparent, credible, reproducible, and accessible science. Open science has six major aspects: open data, open methodology, open source, open access, open peer review, and open educational resources.  **",
                "Reference(s)": "** Abele-Brehm et al. (2019); Crüwell et al. (2019); Kathawalla et al. (2020); Syed (2019); Woelfe et al. (2011)",
                "Drafted by": "** Mahmoud Elsherif",
                "Reviewed (or Edited) by": "** Zoe Flack; Tamara Kalandadze; Charlotte R. Pennington; Qinyu Xiao",
                "Translated by": "** Zainab Alsuhaibani",
                "Translation reviewed by": "Ahlam Ahmed, Ali H. Al-Hoorie, Asma Alzahrani, Mohammed Mohsen",
                "Translation": "التَّعريف:** مصطلح شامل يعكس فكرة أنَّ المعرفة العلميَّة بجميع أنواعها، وحيثما كان ذلك مناسبًا، لابد أن تكون متاحة بشكل مفتوح، وشفاف، ودقيق، وقابل للتِّكرار، وإعادة الإنتاج، ومتراكم، وشامل.  وتعدُّ جميع هذه السِّمات أساسيَّة في المسعى العلمي. ويتكون العلم المفتوح من مبادئ وسلوكيات تعزِّز شفافية العلوم، والمصداقيَّة، والقابليَّة  للتِّكرار، وسهولة الوصول. يشتمل العلم المفتوح على ستة جوانب رئيسيَّة: البيانات المفتوحة، والمنهجيَّة المفتوحة، والمصدر المفتوح، والوصول المفتوح، والتّحكيم المفتوح، والموارد التَّعليميَّة المفتوحة. **المصطلحات ذات الصِّلة:** إمكانيَّة الوصول، المصداقيَّة، البيانات المفتوحة، المواد المفتوحة، تحكيم الأقران المفتوحة، البحث المفتوح، الممارسات العلميَّة  المفتوحة، المعرفة المفتوحة، أزمة إعادة الإنتاج (أو أزمة التِّكرار)، قابليَّة إعادة الإنتاج ، الشَّفافيَّة",
                "Related_terms": "** Accessibility; Credibility; Open Data; Open Material; Open Peer Review; Open Research; Open Science Practices; Open Scholarship; Reproducibility crisis (aka Replicability or replication crisis); Reproducibility; Transparency"
            },
            {
                "Title": "Open Science Framework (إطار العلوم المفتوحة) *",
                "Definition": "** A free and open source platform for researchers to organize and share their research project and to encourage collaboration. Often used as an open repository for research code, data and materials, preprints and preregistrations, while managing a more efficient workflow. Created and maintained by the Center for Open Science.  **",
                "Reference(s)": "** Foster and Deardorff (2017); https://osf.io/",
                "Drafted by": "** William Ngiam",
                "Reviewed (or Edited) by": "** Mahmoud Elsherif; Charlotte R. Pennington; Lisa Spitzer",
                "Translated by": "** Mai Helmy.",
                "Translation reviewed by": "** Asma Alzahrani, Ahlam Ahmed, Ali H. Al-Hoorie, Mohammed Mohsen",
                "Translation": "التَّعريف:** منصَّة مجانيَّة مفتوحة المصدر للباحثين؛ لتنظيم ومشاركة مشاريعهم البحثيَّة، ولتشجيع التَّعاون بينهم. وغالبًا ما تستخدم كمستودع مفتوح للنُّصوص البرمجيَّة للبحوث والبيانات، والمواد، والمطبوعات، والتَّسجيلات المسبقة مع إدارة أكثر كفاءة لسير العمل. وقد أنشأها وطوَّرها مركز العلوم المفتوحة.  **المصطلحات ذات الصِّلة:** أرشيف، مركز العلم المفتوح، النَّص البرمجي المفتوح، البيانات المفتوحة، الطّباعة الأوليَّة، التَّسجيل المسبق.",
                "Related_terms": "** Archive; Center for Open Science (COS); Open Code; Open Data; Preprint; Preregistration"
            },
            {
                "Title": "Open Source software (برنامج مفتوح المصدر) *",
                "Definition": "** A type of computer software in which source code is released under a license that permits others to use, change, and distribute the software to anyone and for any purpose. Open source is more than openly accessible: the distribution terms of open-source software must comply with 10 specific criteria (see: [https://opensource.org/osd](https://opensource.org/osd)).  **",
                "Reference": "** [https://opensource.org/osd](https://opensource.org/osd); [https://www.fosteropenscience.eu/foster-taxonomy/open-source-open-science](https://www.fosteropenscience.eu/foster-taxonomy/open-source-open-science)",
                "Originally drafted by": "** Connor Keating",
                "Reviewed (or Edited) by": "** Jamie P. Cockcroft; Helena Hartmann; Charlotte R. Pennington; Andrew J. Stewart",
                "Translated by": "** Mai Helmy.",
                "Translation reviewed by": "** Ahlam Ahmed, Ali H. Al-Hoorie, Asma Alzahrani, Mohammed Mohsen",
                "Translation": "التَّعريف:** نوع من البرامج الحاسوبيَّة يتم فيه إصدار تعليمات برمجيَّة بموجب ترخيص يسمح للآخرين باستخدام البرنامج، وتغييره، وتوزيعه لأي شخص، ولأي غرض.  ما يميّز هذه البرامج ليس الوصول المفتوح فحسب، بل يجب أيضًا أن تتوافق شروط توزيع البرامج مفتوحة المصدر مع 10 معايير محدَّدة (انظر: https://opensource.org/osd).  **المصطلحات ذات الصِّلة:** GITHUB. الوصول المفتوح، النَّص البرمجي مفتوح، البيانات المفتوحة، التَّراخيص المفتوحة، بايثون، لغة الآر، مستودع البيانات",
                "Related_terms": "** Github; Open Access; Open Code; Open Data; Open Licenses; Python; R; Repository"
            },
            {
                "Title": "Open washing (الغسل المفتوح) *",
                "Definition": "** Open washing, termed after “greenwashing”, refers to the act of claiming openness to secure perceptions of rigor or prestige associated with open practices. It has been used to characterise the marketing strategy of software companies that have the appearance of open-source and open-licensing, while engaging in proprietary practices. Open washing is a growing concern for those adopting open science practices as their actions are undermined by misleading uses of the practices, and actions designed to facilitate progressive developments are reduced to ‘ticking the box’ without clear quality control.  **",
                "Reference": "** Farrow (2017); Moretti (2020); Villum (2016); Vlaeminck and Podkrajac (2017)",
                "Originally drafted by": "** Meng Liu",
                "Reviewed (or Edited) by": "** Thomas Rhys Evans; Sam Guay; Sam Parsons; Charlotte R. Pennington; Beatrice Valentini",
                "Translated by": "** Mai Helmy.",
                "Translation reviewed by": "** Ahlam Ahmed, Ali H. Al-Hoorie, Asma Alzahrani, Mohammed Mohsen",
                "Translation": "التَّعريف:** يشير الغسل المفتوح إلى ادِّعاء الانفتاح لتقمّص الصَّرامة، والهيبة المرتبطة بالممارسات المفتوحة، وتمَّ استخدام هذا التَّعبير لتوصيف استراتيجيَّة التَّسويق لدى شركات البرمجيَّات التي تبدو مفتوحة المصدر، وذات تراخيص مفتوحة، ولكنَّها متورِّطة في ممارسات الملكيَّة.  ويعدُّ الغسيل المفتوح مصدر قلقٍ متزايدٍ لأولئك الذين يتبنَّون ممارسات علميَّة مفتوحة حيث يتم تقويض جهودهم من خلال هذه الاستخدامات المضلِّلة، بينما يتم اختزال الإجراءات المصمَّمة؛ لتسهيل التَّطورات التَّقدميَّة إلى \"وضع علامة على المربع\" دون مراقبة واضحة للجودة.  **المصطلحات ذات الصِّلة:** الوصول المفتوح ،البيانات المفتوحة، المصدر المفتوح",
                "Related_terms": "Open Access; Open Data; Open Source"
            },
            {
                "Title": "Optional Stopping (الإيقاف الاختياري) *",
                "Definition": "** The practice of (repeatedly) analyzing data during the data collection process and deciding to stop data collection if a statistical criterion (e.g. *p*\\-value, or bayes factor) reaches a specified threshold. If appropriate methodological precautions are taken to control the type 1 error rate, this can be an efficient analysis procedure (e.g. Lakens, 2014). However, without transparent reporting or appropriate error control the type 1 error can increase greatly and optional stopping could be considered a Questionable Research Practice (QRP) or a form of p-hacking.  **",
                "Reference(s)": "** Beffara Bret et al. (2021); Lakens (2014); Sagarin et al. (2014); Schönbrodt et al. (2017)",
                "Originally Drafted by": "** Brice Beffara Bret; Bettina M. J. Kern",
                "Reviewed (or Edited) by": "** Ali H. Al-Hoorie; Helena Hartmann; Catia M. Oliveira; Sam Parsons",
                "Translated by": "** Mai Helmy.",
                "Translation reviewed by": "** Ahlam Ahmed, Ali H. Al-Hoorie, Asma Alzahrani, Mohammed Mohsen",
                "Translation": "التَّعريف:** ممارسة تحليل البيانات بشكلٍ متكرِّر أثناء عمليَّة جمع البيانات، واتِّخاذ قرار بإيقاف جمع البيانات إذا وصل معيار إحصائي (مثل قيمة الاحتماليَّة، أو عامل بايز) إلى حدٍّ معيّن.  وإذا تم اتِّخاذ الاحتياطات المنهجيَّة المناسبة للتَّحكُّم في معدَّل الخطأ من النُّوع الأوَّل، فقد يكون هذا إجراءً فعالًا (مثل Lakens ، 2014). ومع ذلك ، بدون الإبلاغ الشَّفاف، أو التَّحكُّم المناسب في الخطأ ، يمكن أن يزيد الخطأ من النُّوع الأوَّل بشكل كبير، ويمكن اعتبار التوقف الاختياري ممارسة بحث مشكوك فيها، أو شكلًا من أشكال قرصنة القيمة الاحتماليَّة.  **المصطلحات ذات الصِّلة:** قرصنة القيمة الاحتماليَّة، ممارسات البحث المشكوك فيها، أو ممارسات إعداد التَّقارير المشكوك فيها، الاختبار المتسلسل",
                "Related_terms": "** *P*\\-hacking; Questionable Research Practices or Questionable Reporting Practices (QRPs); Sequential testing"
            },
            {
                "Title": "ORCID (Open Researcher and Contributor ID) (الأوركيد) *",
                "Definition": "** A organisation that provides a registry of persistent unique identifiers (ORCID iDs) for researchers and scholars, allowing these users to link their digital research documents and other contributions to their ORCID record. This avoids the name ambiguity problem in scholarly communication. ORCID iDs provide unique, persistent identifiers connecting researchers and their scholarly work. It is free to register for an ORCID iD at [https://orcid.org/register](https://orcid.org/register).  **",
                "Reference(s)": "** Haak et al. (2012); [https://orcid.org/](https://orcid.org/)",
                "Drafted by": "** Martin Vasilev",
                "Reviewed (or Edited) by": "** Bradley Baker; Mahmoud Elsherif; Shannon Francis; Charlotte R. Pennington; Emily A. Williams; Flávio Azevedo",
                "Translated by": "** Awatif Alruwaili",
                "Translation reviewed by": "** Ahlam Ahmed, Ali H. Al-Hoorie, Asma Alzahrani, Mohammed Mohsen",
                "Translation": "التَّعريف:** هي منظمة توفِّر سجل ثابت لمعرِّفات فريدة للباحثين، والعلماء بما يسمح لهم بربط وثائق بحوثهم الرَّقميَّة، ومساهماتهم الأخرى بسجل الأوركيد.  يساعد هذا الإجراء في تجنُّب التباس الأسماء في مجال الاتِّصال العلمي. مُعرِّفات الأوركيد توفِّر معرِّفًا فريدًا ثابتًا دائمًا؛ لربط الباحثين بأعمالهم. يمكن الاشتراك مجانًا عن طريق : [https://orcid.org/register](https://orcid.org/register) **المصطلحات ذات الصِّلة:**  التَّأليف، معرِّف الكائن الرَّقميَّ، مشكلة ازدواجيَّة الاسم.",
                "Related_terms": "** Authorship; DOI (digital object identifier); Name Ambiguity Problem"
            },
            {
                "Title": "Overlay Journal (المجلَّات التَّركيبيَّة) *",
                "Definition": "** Open access electronic journals that collect and curate articles available from other sources (typically preprint servers, such as arXiv). Article curation may include (post-publication) peer review or editorial selection. Overlay journals do not publish novel material; rather, they organize and collate articles available in existing repositories.  **",
                "Reference": "** Ginsparg (1997, 2001); [https://discovery.ucl.ac.uk/id/eprint/19081/](https://discovery.ucl.ac.uk/id/eprint/19081/)",
                "Originally drafted by": "** Bradley Baker",
                "Reviewed (or Edited) by": "** Christopher Graham; Helena Hartmann; Sam Parsons; Charlotte R. Pennington",
                "Translated by": "** Alaa Saleh",
                "Translation reviewed by": "** Ali H. Al-Hoorie, Ahlam Ahmed, Hala Alghamdi, Mohammed Mohsen   ###  ### **P** {#p}",
                "Translation": "التَّعريف**: هي مجلَّات علميَّة إلكترونيَّة ذات الوصول المفتوح، تقوم بتجميع، وتنظيم المقالات المتاحة من مصادر أخرى (عادةً خوادم ما قبل الطِّباعة مثل arXiv). قد تتضمَّن معالجة  المقالات تحكيم الأقران بعد النَّشر، أو لجان التَّحكيم.  لا تهدف المجلَّات التَّركيبيَّة المفتوحة إلى نشر مواد جديدة، ولكنَّها تهدف إلى تنظيم، وتجميع المقالات المتاحة الموجودة في المستودعات الرَّقميَّة. **المصطلحات ذات الصِّلة:** الوصول المفتوح ، الطّباعة الأوليَّة",
                "Related_terms": "** Open access; Preprint"
            },
            {
                "Title": "P-curve (منحنى القيم الاحتماليَّة) *",
                "Definition": "** P-curve is a tool for identifying potential publication bias and makes use of the distribution of significant *p*\\-values in a series of independent findings. The deviation from the expected right-skewed distribution can be used to assess the existence and degree of publication bias: if the curve is right-skewed, there are more low, highly significant *p*\\-values, reflecting an underlying true effect. If the curve is left-skewed, there are many barely significant results just under the 0.05-threshold. This suggests that the studies lack evidential value and may be underpinned by questionable research practices (QRPs; e.g., p-hacking). In the case of no true effect present (true null hypothesis) and unbiased *p*\\-value reporting, the *p*\\-curve should be a flat, horizontal line, representing the typical distribution of *p*\\-values.  **",
                "Reference": "** Bruns and Ioannidis (2016); Simonsohn et al. (2014a); Simonsohn et al.(2014b); Simonsohn et al. (2019)",
                "Originally drafted by": "** Bettina M. J. Kern",
                "Reviewed (or Edited) by": "** Sam Guay; Kamil Izydorczak; Charlotte R. Pennington; Robert M. Ross; Olmo van den Akker",
                "Translated by": "** Alaa Saleh",
                "Translation reviewed by": "** Ali H. Al-Hoorie, Ahlam Ahmed, Asma Alzahrani, Mohammed Mohsen",
                "Translation": "التَّعريف:** أداة لتحديد تحيُّز النَّشر المحتمل، والتي تستعمل توزيع القيم الاحتماليَّة ذات الدِّلالة الإحصائيَّة في سلسلة من النَّتائج المستقلَّة.  يمكن استعمال الانحراف عن التَّوزيع المتوقَّع لتقييم وجود ودرجة تحيُّز النَّشر، فإذا كان المنحنى منحرفًا لليمين، فهناك قيم احتماليَّة أكثر انخفاضًا ذات دلالة إحصائيَّة، ممَّا يعكس وجود تأثير حقيقي، أمَّا إذا كان المنحنى منحرفًا إلى اليسار، فهناك العديد من النَّتائج ذات الدِّلالة الإحصائيَّة التي تقل عن مستوى 0.05 بقليل فقط.  يشير هذا إلى أنَّ الدِّراسات تفتقر إلى القيمة الإثباتيَّة وقد تكون مدعومة بممارسات بحثيَّة مشكوك فيها، مثل: قرصنة القيمة الاحتماليَّة  في حالة عدم وجود تأثير حقيقي (الفرضيَّة الصِّفريَّة حقيقيَّة) وعندما يكون الإبلاغ عن القيمة الاحتماليَّة غير متحيِّز، فيجب أن يكون منحنى القيم الاحتماليَّة خطًا أفقيًا مسطَّحًا، يمثل التَّوزيع النَّموذجي للقيم الاحتماليَّة.  **المصطلحات ذات الصِّلة:**  درج حفظ الملفَّات، الفرضيَّة، قرصنة القيمة الاحتماليَّة، القيمة الاحتماليَّة، تَّحيُّز النَّشر (مشكلة درج البيانات)، ممارسات البحث المشكوك فيها أو ممارسات إعداد التَّقارير المشكوك فيها، تقارير انتقائيَّة، منحني زد",
                "Related_terms": "** File-drawer; Hypothesis; *P*\\-hacking; *p*\\-value; Publication bias (File Drawer Problem); Questionable Research Practices or Questionable Reporting Practices (QRPs); Selective reporting; Z-curve"
            },
            {
                "Title": "*p*****-hacking (قرصنة القيمة الاحتماليَّة) *",
                "Definition": "** Exploiting techniques that may artificially increase the likelihood of obtaining a statistically significant result by meeting the standard statistical significance criterion (typically α \\= .05). For example, performing multiple analyses and reporting only those at *p* \\< .05, selectively removing data until *p* \\< .05, selecting variables for use in analyses based on whether those parameters are statistically significant. **",
                "Reference(s)": "** Hardwicke et al. (2014); Neuroskeptic (2012)",
                "Drafted by": "** Mahmoud Elsherif",
                "Reviewed (or Edited) by": "** Tamara Kalandadze; William Ngiam; Sam Parsons; Martin Vasilev",
                "Translated by": "** Ruwayshid",
                "Translation reviewed by": "** Ahlam Ahmed, Ali H. Al-Hoorie, Asma Alzahrani, Mohammed Mohsen",
                "Translation": "التَّعريف:**  هو أسلوب استغلالي قد يزيد بنحو غير طبيعي من احتماليَّة  الحصول على نتيجة ذات دلالة إحصائيَّة من خلال تحقيق معيار درجة الدلالة الإحصائيَّة اللَّازمة (عادة α \\= .05). مثال ذلك، القيام بعدة تحليلات، ونشر تلك التي تكون فيها القيمة الاحتماليَّة أقل من 0.05 فقط، أو إزالة بعض البيانات بشكل انتقائي حتى تكون القيمة الاحتماليَّة أقل من 0.05، أو اختيار بعض العوامل للتَّحليل بناء على ما إذا كانت المؤشِّرات ذات دلالة إحصائيَّة. **المصطلحات ذات الصِّلة:**  المرونة التَّحليلية ، التَّصيد ، حديقة المسارات المتشعِّبة ، الافتراض بعد معرفة النَّتائج، ممارسات البحث المشكوك فيها أو ممارسات إعداد التَّقارير المشكوك فيها ، تقرير انتقائي",
                "Related_terms": "** Analytic flexibility; Fishing; Garden of forking paths; HARKing; Questionable Research Practices or Questionable Reporting Practices (QRPs); Selective reporting"
            },
            {
                "Title": "*p*****-value (القيمة الاحتماليّة) *",
                "Definition": "** A statistic used to evaluate the outcome of a hypothesis test in Null Hypothesis Significance Testing (NHST). It refers to the probability of observing an effect, or more extreme effect, assuming the null hypothesis is true (Lakens, 2021b). The American Statistical Association’s statement on p-values (Wasserstein & Lazar, 2016\\) notes that p-values are not an indicator of the truth of the null hypothesis and instead defines p-values in this way: “Informally, a p-value is the probability under a specified statistical model that a statistical summary of the data (e.g., the sample mean difference between two compared groups) would be equal to or more extreme than its observed value” (p. 131).  **",
                "Reference(s)": "** [https://psyteachr.github.io/glossary/p.html](https://psyteachr.github.io/glossary/p.html); Lakens (2021b); Wasserstein and Lazar (2016)",
                "Drafted by": "** Alaa AlDoh; Flávio Azevedo",
                "Reviewed (or Edited) by": "** Jamie P. Cockcroft; Charlotte R. Pennington; Suzanne L. K. Stewart; Robbie C.M. van Aert; Marcel A.L.M. van Assen; Martin Vasilev",
                "Translated by": "Mai Helmy.",
                "Translation reviewed by": "Ahlam Ahmed, Ali H. Al-Hoorie, Asma Alzahrani, Mohammed Mohsen",
                "Translation": "التعريف:** رقم إحصائي يستخدم لتقييم نتائج الفرضيات في اختبار دلالة الفرضية الصفرية يشير إلى احتمال ملاحظة تأثير ما أو تأثير أكثر تطرفاً بافتراض صحة الفرضية الصفرية (Lakens, 2021b). ويشير بيان الجمعية الإحصائية الأمريكية (Wasserstein & Lazar, 2016) إلى أن القيم الاحتمالية ليست مؤشرًا على حقيقة الفرضية الصفرية؛ بل تعرف القيم الاحتمالية بهذه الطريقة: \"بشكل غير رسمي، القيمة الاحتمالية هي الاحتمالية بموجب نموذج إحصائي محدد ويكون الملخص الإحصائي للبيانات (على سبيل المثال، متوسط الفرق في ​​العينة بين مجموعتين مقارنتين) مساوياً أو أكثر تطرفاً من قيمته المرصودة\" (ص 131).  **المصطلحات ذات الصِّلة:** اختبار دلالة الفرضيَّة الصِّفريَّة، الدِّلالة الإحصائيَّة",
                "Related_terms": "** Null Hypothesis Statistical Testing (NHST); statistical significance"
            },
            {
                "Title": "Papermill (مصانع  الورق) *",
                "Definition": "** An organization that is engaged in scientific misconduct wherein multiple papers are produced by falsifying or fabricating data, e.g. by editing figures or numerical data or plagiarizing written text. Papermills are “alleged to offer products ranging from research data through to ghostwritten fraudulent or fabricated manuscripts and submission services” (Byrne & Christopher, 2020, p. 583). A papermill relates to the fast production and dissemination of multiple allegedly new papers. These are often not detected in the scientific publishing process and therefore either never found or retracted if discovered (e.g. through plagiarism software).  **",
                "Reference(s)": "** Byrne and Christopher (2020); Hackett and Kelly (2020)",
                "Originally drafted by": "** Helena Hartmann",
                "Reviewed (or Edited) by": "** Sarah Ashcroft-Jones; Elizabeth Collins; Mahmoud Elsherif; Charlotte R. Pennington",
                "Translated by": "** Mai Helmy.",
                "Translation reviewed by": "** Ahlam Ahmed, Ali H. Al-Hoorie, Asma Alzahrani , Mohammed Mohsen",
                "Translation": "التَّعريف:** منظمة متورِّطة في سوء السُّلوك العلمي حيث يتم إنتاج بحوث متعدِّدة عن طريق تزوير، أو تلفيق البيانات، على سبيل المثال: عن طريق تعديل الأرقام، أو البيانات الرَّقميَّة، أو سرقة النُّصوص المكتوبة.  ومصانع الورق \"تُتهم بأنَّها تقدِّم منتجات تتراوح من بيانات البحث إلى المخطوطات المكتوبة الاحتياليَّة، أو المزِّيفة وخدمات التَّقديم\" (Byrne & Christopher, 2020, p. 583). تقوم مصانع الورق بالإنتاج السَّريع ونشر العديد من الأوراق التي تدَّعي بأنَّها جديدة.  غالبًا لا يتم اكتشافها في عملية النَّشر العلمي، وبالتَّالي لا يتم العثور عليها مطلقًا، أو يتم سحبها إذا تم اكتشافها، مثلًا: من خلال برامج الانتحال.  **المصطلحات ذات الصِّلة:** تزييف البيانات، تزوير البيانات، احتيال، إنتحال ، ممارسات البحث المشكوك فيها أو ممارسات إعداد التَّقارير المشكوك فيها، سوء السُّلوك العلمي، النَّشر العلمي.",
                "Related_terms": "** Data fabrication; Data falsification; Fraud; Plagiarism; Questionable Research Practices or Questionable Reporting Practices (QRPs); Scientific misconduct; Scientific publishing"
            },
            {
                "Title": "Paradata (أشباه البيانات) *",
                "Definition": "** Data that are captured about the characteristics and context of primary data collected from an individual \\- distinct from metadata. Paradata can be used to investigate a respondent’s interaction with a survey or an experiment on a micro-level. They can be most easily collected during computer mediated surveys but are not limited to them. Examples include response times to survey questions, repeated patterns of responses such as choosing the same answer for all questions, contextual characteristics of the participant such as injuries that prevent good performance on tasks, the number of premature responses to stimuli in an experiment. Paradata have been used for the investigation and adjustment of measurement and sampling errors.  **",
                "Reference": "** Kreuter (2013)",
                "Originally drafted by": "** Alexander Hart; Graham Reid",
                "Reviewed (or Edited) by": "** Helena Hartmann; Charlotte R. Pennington; Marta Topor; Flávio Azevedo",
                "Translated by": "** Mai Helmy.",
                "Translation reviewed by": "** Ahlam Ahmed, Ali H. Al-Hoorie, Asma Alzahrani, Mohammed Mohsen",
                "Translation": "التَّعريف:** البيانات التي يتم التقاطها حول خصائص وسياق البيانات الأوليَّة التي تم جمعها من شخص ما،  وتختلف عن البيانات الوصفيَّة.  ويمكن استخدامها للتَّحقيق في تفاعل المجيب مع استطلاع ما، أو تجربة على المستوى الجزئي.  ويمكن جمعها بسهولة أكبر أثناء الاستطلاعات التي يتم إجراؤها بواسطة الحاسوب، ولكنَّها لا تقتصر عليها. وتشمل الأمثلة أوقات الاستجابة لأسئلة الاستطلاع، والأنماط المتكرِّرة للاستجابات، مثل اختيار نفس الإجابة لجميع الأسئلة، والخصائص السِّياقيَّة للمشارك، مثل: الإصابات التي تمنع الأداء الجيِّد في المهام، وعدد الاستجابات المبكِّرة للمحفِّزات في التَّجربة.  وتم استخدام أشباه البيانات للتَّحقيق، وتصحيح أخطاء القياس، وأخذ العيِّنات.  **المصطلحات ذات الصِّلة:** البيانات المساعدة، جمع البيانات، جودة البيانات، البيانات الوصفيَّة، قبول المعلومات.",
                "Related_terms": "** Auxiliary data; Data collection; Data quality; Metadata; Process information"
            },
            {
                "Title": "PARKing (التَّسجيل بعد معرفة النَّتائج) *",
                "Definition": "** PARKing (preregistering after results are known) is defined as the practice where researchers complete an experiment (possibly with infinite re-experimentation) before preregistering. This practice invalidates the purpose of preregistration, and is one of the QRPs (or, even scientific misconduct) that try to gain only \"credibility that it has been preregistered.\"  **",
                "Reference": "** [Ikeda et al. (2019)](https://www.jstage.jst.go.jp/article/sjpr/62/3/62_281/_pdf/-char/ja); [Yamada (2018)](https://www.frontiersin.org/articles/10.3389/fpsyg.2018.01831/full)",
                "Originally drafted by": "** Qinyu Xiao",
                "Reviewed (or Edited) by": "Helena Hartmann; Sam Parsons; Yuki Yamada",
                "Translated by": "** Mai Helmy. **** Ruwayshid, Ali H. Al-Hoorie, Ahlam Ahmed, Mohammed Mohsen",
                "Translation": "التَّعريف:** ممارسة يكمل فيها الباحثون تجربة، وربَّما عدد غير محدود من التَّجارب قبل التَّسجيل المسبق.  وهذه الممارسة تبطل الغرض من التَّسجيل المسبق، وهي واحدة من ممارسات إعداد التَّقارير المشكوك (أو حتى سوء السُّلوك العلمي) الذي يحاول اكتساب مصداقيَّة التَّسجيل المسبق.  **المصطلحات ذات الصِّلة:** النَّقد بعد معرفة النَّتائج، ممارسات البحث المشكوك فيها، أو ممارسات إعداد التَّقارير المشكوك فيها ، سباركنغ",
                "Related_terms": "** CARKing; HARKing; Preregistration; Questionable Research Practices or Questionable Reporting Practices (QRPs); SPARKing"
            },
            {
                "Title": "Participatory Research (البحوث التَّشاركيَّة) *",
                "Definition": "** Participatory research refers to incorporating the views of people from relevant communities in the entire research process to achieve shared goals between researchers and the communities. This approach takes a collaborative stance that seeks to reduce the power imbalance between the researcher and those researched through a “systematic cocreation of new knowledge” (Andersson, 2018).  **",
                "Reference": "** Cornwall and Jewkes (1995); Fletcher-Watson et al. (2019); Kiernan (1999); Leavy (2017); Ottmann et al. (2011); Rose (2018)",
                "Originally drafted by": "** Tamara Kalandadze",
                "Reviewed (or Edited) by": "** Jamie P. Cockcroft; Bethan Iley; Halil E. Kocalar; Michele C. Lim",
                "Translated by": "** Ahlam Ahmed",
                "Translation reviewed by": "** Ali H. Al-Hoorie, Ahlam Ahmed, Asma Alzahrani, Mohammed Mohsen",
                "Translation": "التَّعريف:** يشير البحث التَّشاركي إلى دمج آراء الأشخاص من المجتمعات ذات الصِّلة في عملية البحث بأكملها؛ لتحقيق أهداف مشتركة بين الباحثين والمجتمعات.  يتَّخذ هذا النَّهج موقفًا تعاونيًا يسعى إلى تقليل اختلال توازن القوى بين الباحث وأولئك الذين يتم بحثهم من خلال \"الإبداع المشترك المنهجي للمعرفة الجديدة\" (Andersson, 2018). **المصطلحات ذات الصِّلة**: البحث التَّعاوني؛ الشُّمول؛  التَّنوع العصبيّ،  عيِّنة ومجتمع الدِّراسة المساهمين؛  النَّموذج التَّحويليّ.",
                "Related_terms": "** Collaborative research; Inclusion; Neurodiversity; Patient and Public Involvement (PPI); Transformative paradigm"
            },
            {
                "Title": "Patient and Public Involvement (PPI) (عيِّنة ومجتمع الدِّراسة المساهمين ) *",
                "Definition": "** Active research collaboration with the population of interest, as opposed to conducting research “about” them. Researchers can incorporate the lived experience and expertise of patients and the public at all stages of the research process. For example, patients can help to develop a set of research questions, review the suitability of a study design, approve plain English summaries for grant/ethics applications and dissemination, collect and analyse data, and assist with writing up a project for publication. This is becoming highly recommended and even required by funders (Boivin et al., 2018).  **",
                "Reference": "** Boivin et al. (2018); [https://www.invo.org.uk/](https://www.invo.org.uk/)",
                "Originally drafted by": "** Jade Pickering",
                "Reviewed (or Edited) by": "** Mahmoud Elsherif; Sam Parsons; Catia M. Oliveira",
                "Translated by": "** Hiba Alomary.",
                "Translation reviewed by": "** Ahlam Ahmed, Ali H. Al-Hoorie, Asma Alzahrani, Mohammed Mohsen",
                "Translation": "التَّعريف**: التَّعاون البحثي النَّشط مع مجتمع البحث الذي لديه اهتمام بحثي ما فضلا عن تطبيق البحث عليهم، يمكن للباحثين دمج الخبرة الحياتية وخبرة العينة ومجتمع البحث في جميع مراحل عملية البحث.  على سبيل المثال، يمكن لعيّنة الدِّراسة المساعدة في تطوير مجموعة من الأسئلة البحثيَّة، ومراجعة تصميم الدِّراسة، واعتماد الملخَّصات الإنجليزيَّة للتَّقديم على المنح، ونماذج الأخلاقياَّت البحثيَّة ونشرها، وجمع البيانات وتحليلها، والمساعدة في كتابة مشروع بحثي للنَّشر.  يوصى بتطبيق هذه الممارسات والتي تعد واحدة من متطلبات جهات تمويل البحوث (Boivin et al., 2018).  **المصطلحات ذات الصِّلة**: الإنتاج المشترك، البحوث التَّشاركيَّة.",
                "Related_terms": "** Co-production; Participatory research"
            },
            {
                "Title": "Paywall (حاجز مالي) *",
                "Definition": "** A technological barrier that permits access to information only to individuals who have paid \\- either personally, or via an organisation \\- a designated fee or subscription.  **",
                "Reference": "** Day et al. (2020); [https://casrai.org/term/closed-access/](https://casrai.org/term/closed-access/);",
                "Originally drafted by": "** Bradley Baker",
                "Reviewed (or Edited) by": "** Mahmoud Elsherif; Sam Parsons; Charlotte R. Pennington; Julia Wolska",
                "Translated by": "** Hiba Alomary.",
                "Translation reviewed by": "** Awatif Alruwaili, Ali H. Al-Hoorie, Ahlam Ahmed, Mohammed Mohsen",
                "Translation": "التَّعريف:** حاجز تقني يتيح الوصول إلى المعلومات للأشخاص الذين دفعوا رسوم الاشتراك فقط إما شخصيًا، أو عن طريق منظَّمة.  ال**مصطلحات ذات الصِّلة:** إمكانيَّة الوصول، الوصول المفتوح.",
                "Related_terms": "** Accessibility; Open Access"
            },
            {
                "Title": "PCI (Peer Community In) (منظَّمة الأقران) *",
                "Definition": "** PCI is a non-profit organisation that creates communities of researchers who review and recommend unpublished preprints based upon high-quality peer review from at least two researchers in their field. These preprints are then assigned a DOI, similarly to a journal article. PCI was developed to establish a free, transparent and public scientific publication system based on the review and recommendation of preprints. **",
                "Reference(s)": "** https://peercommunityin.org/",
                "Originally drafted by": "** Emma Henderson",
                "Reviewed (or Edited) by": "** Jamie P. Cockcroft; Christopher Graham; Bethan Iley; Aleksandra Lazić; Charlotte R. Pennington",
                "Translated by": "** Mai Helmy.",
                "Translation reviewed by": "** Ahlam Ahmed, Ali H. Al-Hoorie, Asma Alzahrani, Mohammed Mohsen",
                "Translation": "التَّعريف:** منظّمة غير ربحيَّة تنشئ مجتمعات من الباحثين الذين يراجعون المطبوعات الأوليَّة، ويوصون بنشرها، بحيث تُبنى التَّوصية على مراجعة عالية الجودة للأقران.  يقوم بها باحثَين متخصِّصين على الأقل، ويتم بعد ذلك تخصيص معرّف كائن رقمي لهذه المطبوعات الأوليَّة كأنَّه مقال في مجلة، وقد تم تطوير منظمة الأقران؛ لإنشاء نظام نشر علمي مجاني، وشفَّاف، وعام يعتمد على مراجعة المطبوعات الأوليَّة والتَّوصية بها.  **المصطلحات ذات الصِّلة:** الوصول المفتوح، المحفوظات المفتوحة، التّحكيم المفتوح، مجتمع الأقران للتَّقارير المسجَّلة، تحكيم الأقران، الطّباعة الأوليَّة",
                "Related_terms": "** Open Access; Open Archives; Open Peer Review; PCI Registered Reports; Peer review; Preprints"
            },
            {
                "Title": "PCI Registered Reports (مجتمع الأقران للتَّقارير المسجَّلة) *",
                "Definition": "** An initiative launched in 2021 dedicated to receiving, reviewing, and recommending Registered Reports (RRs) across the full spectrum of Science, technology, engineering, and mathematics (STEM), medicine, social sciences and humanities. Peer Community In (PCI) RRs are overseen by a ‘Recommender’ (equivalent to an Action Editor) and reviewed by at least two experts in the relevant field. It provides free and transparent pre- (Stage 1\\) and post-study (Stage 2\\) reviews across research fields. A network of PCI RR-friendly journals endorse the PCI RR review criteria and commit to accepting, without further peer review, RRs that receive a positive final recommendation from PCI RR.  **",
                "Reference(s)": "** [https://rr.peercommunityin.org/about/about](https://rr.peercommunityin.org/about/about)",
                "Originally drafted by": "** Charlotte R. Pennington",
                "Reviewed (or Edited) by": "** Sarah Ashcroft-Jones; Jamie P. Cockcroft; Mahmoud Elsherif; Helena Hartmann",
                "Translated by": "** Mai Helmy.",
                "Translation reviewed by": "** Ali H. Al-Hoorie, Ahlam Ahmed, Asma Alzahrani , Mohammed Mohsen",
                "Translation": "التَّعريف:** مبادرة تم إطلاقها في عام 2021 مخصَّصة لتلقي التقارير المسجَّلة، ومراجعتها، والتَّوصية بها لمختلف التَّخصُّصات من العلوم والتّكنولوجيا، والهندسة، والرِّياضيَّات، والطِّب، والعلوم الاجتماعيَّة، والعلوم الإنسانيَّة.  ويشرف على هذه العمليَّة صاحب التَّزكية (أي ما يعادل المحرّر) ويتم مراجعة البحث من قبل خبيرَين على الأقل في مجال التَّخصُّص. وتوفِّر هذه العمليَّة مراجعات مجانيَّة، وشفَّافة لما قبل (المرحلة الأولى) وما بعد إجراء الدِّراسة (المرحلة الثَّانية). يعتمد عدد من المجلَّات الصَّديقة لمجموعة الأقران للتَّقارير المسجّلة معايير المراجعة هذه، وتلتزم بقبول الأبحاث التي تنال توصية نهائيَّة إيجابيَّة منها.  **المصطلحات ذات الصِّلة:** القبول المبدئي (IPA) ، الوصول المفتوح، منظَّمة الأقران، تحيُّز النَّشر (مشكلة درج الملفَّات)، التَّقرير المسجَّل، تعمية النَّتائج ، مراجعة دراسة المرحلة الأولى، مراجعة دراسة المرحلة الثَّانية، الشَّفافيَّة.",
                "Related_terms": "** In Principle Acceptance (IPA); Open Access; PCI (Peer Community In); Publication bias (File Drawer Problem); Registered Report; Results blind; Stage 1 study review; Stage 2 study review; Transparency"
            },
            {
                "Title": "Plan S (خطة الصَّدمة) *",
                "Definition": "** Plan S is an initiative, launched in September 2018 by cOAlition S, a consortium of research funding organisations, which aims to accelerate the transition to full and immediate Open Access. Participating funders require recipients of research grants to publish their research in compliant Open Access journals or platforms, or make their work openly and immediately available in an Open Access repository, from 2021 onwards. cOAlition S funders have commited to not financially support ‘hybrid’ Open Access publication fees in subscription venues. However, authors can comply with plan S through publishing Open Access in a subscription journal under a “transformative arrangement” as further described in the implementation guidance. The “S” in Plan S stands for shock.  **",
                "Reference": "** [https://www.coalition-s.org](https://www.coalition-s.org/)",
                "Originally drafted by": "** Olmo van den Akker",
                "Reviewed (or Edited) by": "** Jamie P. Cockcroft; Helena Hartmann; Halil E. Kocalar; Birgit Schmidt",
                "Translated by": "Mai Helmy.",
                "Translation reviewed by": "** Ali H. Al-Hoorie, Ahlam Ahmed, Asma Alzahrani, Mohammed Mohsen",
                "Translation": "التَّعريف:** مبادرة تم إطلاقها في سبتمبر 2018 من قبل ائتلاف الصَّدمة (cOAlition S)، وهو اتِّحاد من منظَّمات تمويل البحوث يهدف إلى تسريع الانتقال إلى الوصول المفتوح الكامل والفوري.  ويطالب المموّلون المشاركون من متلقي المنح البحثيَّة نشر بحوثهم في مجلَّات، أو منصَّات ذات وصول مفتوح متوافق، أو جعل أعمالهم متاحة بشكل مفتوح وفوري في مستودع الوصول المفتوح اعتبارًا من عام 2021 فصاعدًا، ويلتزم ممولو ائتلاف الصَّدمة بعدم تقديم الدَّعم الماليّ لرسوم الَّنشر ذات الوصول المفتوح المختلط، أو الهجين.  ومع ذلك، يمكن للمؤلفين الامتثال للخطّة من خلال نشر الوصول المفتوح في مجلة اشتراك بموجب \"ترتيب تحويلي\" كما هو موضح بمزيد من التَّفصيل في إرشادات التَّنفيذ. يشير الحرف \"S\" في الخطة S إلى الصَّدمة.  **المصطلحات ذات الصِّلة:** الوصول المفتوح ، إعلان سان فرانسيسكو بشأن تقييم البحوث، مستودع البيانات.",
                "Related_terms": "** Open Access; DORA; Repository"
            },
            {
                "Title": "Positionality (الموضعيَّة) *",
                "Definition": "** The contextualization of both the research environment and the researcher, to define the boundaries within the research was produced (Jaraf, 2018). Positionality is typically centred and celebrated in qualitative research, but there have been recent calls for it to also be used in quantitative research as well. Positionality statements, whereby a researcher outlines their background and ‘position’ within and towards the research, have been suggested as one method of recognising and centring researcher bias.  **",
                "Reference(s)": "** Jafar (2018); Oxford Dictionaries (2017)",
                "Originally drafted by": "** Joanne McCuaig",
                "Reviewed (or Edited) by": "** Helena Hartmann; Aoife O’Mahony; Madeleine Pownall; Graham Reid",
                "Translated by": "Ruwayshid",
                "Translation reviewed by": "Ali H. Al-Hoorie, Ahlam Ahmed, Asma Alzahrani, Mohammed Mohsen",
                "Translation": "التَّعريف:** تحديد سياق كلٌّ من بيئة البحث والباحث، لتحديد الحدود التي أُنتِج البحث داخلها (Jaraf, 2018). عادة ما يتم التَّركيز على الموضعيَّة والاحتفاء بها في البحث النُّوعي، ولكن هناك دعوات حديثة لاستخدامها في البحث الكمي أيضًا. تم اقتراح صياغات لبيان الموضعيَّة، حيث يحدِّد الباحث خلفيته و\"موقعه\" داخل البحث واتجاهه كطريقة للتَّعرُّف على تحيُّز الباحث وتركيزه.  **المصطلحات ذات الصِّلة:**  التَّحيز، الانعكاسيَّة، وجهات النَّظر.",
                "Related_terms": "** Bias; Reflexivity; Perspective"
            },
            {
                "Title": "Positionality Map (خريطة الموضعية) *",
                "Definition": "** A reflexive tool for practicing explicit positionality in critical qualitative research. The map is to be used “as a flexible starting point to guide researchers to reflect and be reflexive about their social location. The map involves three tiers: the identification of social identities (Tier 1), how these positions impact our life (Tier 2), and details that may be tied to the particularities of our social identity (Tier 3).” (Jacobson & Mustafa 2019, p. 1). The aim of the map is “for researchers to be able to better identify and understand their social locations and how they may pose challenges and aspects of ease within the qualitative research process.” **",
                "Reference": "** Jacobson and Mustafa (2019)",
                "Originally drafted by": "** Joanne McCuaig",
                "Reviewed (or Edited) by": "** Helena Hartmann; Michele C. Lim; Charlotte R. Pennington; Graham Reid",
                "Translated by": "** Ahlam Ahmed",
                "Translation reviewed by": "** Ali H. Al-Hoorie, Hiba Alomary, Asma Alzahrani, Mohammed Mohsen",
                "Translation": "التَّعريف:** أداة انعكاسيَّة لممارسة الموضعيَّة بشفافيَّة في البحوث النُّوعية النَّقديَّة. يجب استخدام الخريطة \"كنقطة انطلاق مرنة لتوجيه الباحثين للتَّفكير والتَّأمل حول موقعهم الاجتماعي. تتضمَّن الخريطة ثلاثة مستويات: تحديد الهُويَّات الاجتماعيَّة (المستوى 1\\) ، وكيف تؤثر هذه المواقف على حياتنا (المستوى 2)، والتَّفاصيل التي قد تكون مرتبطة بخصائص هويتنا الاجتماعيَّة (المستوى 3)\" (Jacobson & Mustafa 2019, p. 1). الهدف من الخريطة هو \"أن يتمكَّن الباحثون من تحديد وفهم مواقعهم الاجتماعيَّة بشكل أفضل، وكيف يمكن أن يطرحوا التَّحديَّات، وجوانب السُّهولة في عمليَّة البحث النَّوعي.\" **المصطلحات ذات الصِّلة:** الموضعيَّة،  البحث النَّوعي، خريطة الهُويَّة الاجتماعيَّة،  الشَّفافيَّة.",
                "Related_terms": "** Positionality; Qualitative research; Social identity map; Transparency"
            },
            {
                "Title": "Post Hoc (التَّحليل البعدي) *",
                "Definition": "** Post hoc is borrowed from Latin, meaning “after this”. In statistics, post hoc (or post hoc analysis) refers to the testing of hypotheses not specified prior to data analysis. In frequentist statistics, the procedure differs based on whether the analysis was planned or post-hoc, for example by applying more stringent error control. In contrast, Bayesian and likelihood approaches do not differ as a function of when the hypothesis was specified.  **",
                "Reference(s)": "** Dienes (p.166, 2008\\)",
                "Drafted by": "** Alaa Aldoh",
                "Reviewed (or Edited) by": "** Sam Parsons; Jamie P. Cockcroft; Bethan Iley; Halil E. Kocalar; Graham Reid; Flávio Azevedo",
                "Translated by": "Mai Helmy.",
                "Translation reviewed by": "** Awatif Alruwaili, Ali H. Al-Hoorie, Asma Alzahrani, Mohammed Mohsen",
                "Translation": "التَّعريف:** تم استعارة المصطلح الإنجليزي من اللاتينيَّة، ويعني \"بعد هذا\". وفي الإحصاء يشير التَّحليل البعدي لاختبار الفرضيَّات التي لم يتم تحديدها بشكل مسبق. ففي الإحصاء التِّكراري، تختلف طريقة التَّحليل بناءً على ما إذا كان مخططًا له أم بعديًا، مثلًا: من خلال تطبيق تحكُّم أكثر صرامة بالخطأ، وفي المقابل، لا تختلف طريقة التَّحليل في المنهجيَّة الاحتماليَّة والبايزيَّة عندما يتم تحديد الفرضيَّة مسبقًا أو لا.  **المصطلحات ذات الصِّلة:** قبليَّة ، بعديَّة، الافتراض بعد معرفة النَّتائج، قرصنة القيمة الاحتماليَّة",
                "Related_terms": "** A priori, Ad hoc; HARKing; P-hacking"
            },
            {
                "Title": "Post Publication Peer Review (مراجعة الأقران بعد النَّشر) ## *",
                "Definition": "** Peer review that takes place after research has been published. It is typically posted on a dedicated platform (e.g., PubPeer). It is distinct from the traditional commentary which is published in the same journal and which is itself usually peer reviewed.  **",
                "Drafted by": "** Ali H. Al-Hoorie",
                "Reviewed (or Edited) by": "** Mahmoud Elsherif; Sam Parsons",
                "Translated by": "** Mai Helmy.",
                "Translation reviewed by": "** Awatif Alruwaili, Ali H. Al-Hoorie, Asma Alzahrani, Mohammed Mohsen",
                "Translation": "التَّعريف:** مراجعة تتم بعد نشر البحث وعادةً ما يتم نشرها على منصَّة مخصَّصة (مثل منصة PubPeer). وهو يختلف عن التَّعليق التَّقليدي الذي ينشر في نفس المجلَّة، والذي عادة ما يخضع لتحكيم الأقران.  **المصطلحات ذات الصِّلة:** التّحكيم المفتوح، منصة بابّير (PubPeer)، تحكيم الأقران",
                "Related_terms": "** Open Peer Review; PeerPub; Peer review"
            },
            {
                "Title": "Posterior distribution (التَّوزيع اللَّاحق) *",
                "Definition": "** A way to summarize one’s updated knowledge in Bayesian inference, balancing prior knowledge with observed data. In statistical terms, posterior distributions are proportional to the product of the likelihood function and the prior. A posterior probability distribution captures (un)certainty about a given parameter value.  **",
                "Reference(s)": "** Dienes (2014); Lüdtke et al. (2020); van de Schoot et al. (2021)",
                "Drafted by": "** Alaa AlDoh",
                "Reviewed (or Edited) by": "** Adam Parker; Jamie P. Cockcroft; Julia Wolska; Yu-Fang Yang; Charlotte R. Pennington",
                "Translated by": "** Ahlam Ahmed",
                "Translation reviewed by": "** Ali H. Al-Hoorie, Hala Alghamdi, Asma Alzahrani, Mohammed Mohsen",
                "Translation": "التَّعريف:** طريقة لتلخيص المعرفة المحدَّثة في الاستدلال البايزي، تحقِّق التَّوازن بين المعرفة السَّابقة والبيانات المرصودة من النَّاحية الإحصائيَّة، توازن التَّوزيعات اللَّاحقة بين ناتج دالة الاحتمال، والافتراضات السَّابقة. يوضِّح التَّوزيع الاحتمالي اللَّاحق وجود اليقين (أو عدمه) حول قيمة مؤشِّر معيَّن.  **المصطلحات ذات الصِّلة:** معامل بايز، الاستدلال البايزي، تقدير المعاملات باستخدام النهج البايزي، دالة الاحتماليَّة، التَّوزيع المسبق.ِ",
                "Related_terms": "** Bayes Factor; Bayesian inference; Bayesian parameter estimation; Likelihood function; Prior distribution"
            },
            {
                "Title": "Predatory Publishing ((النَّشر المفترس (الاستغلاليّ) *",
                "Definition": "** Predatory (sometimes “vanity”) publishing describes a range of business practices in which publishers seek to profit, primarily by collecting article processing charges (APCs), from publishing scientific works without necessarily providing legitimate quality checks (e.g., peer review) or editorial services. In its most extreme form, predatory publishers will publish any work, so long as charges are paid. Other less extreme strategies, such as sending out high numbers of unsolicited requests for editing or publishing in fee-driven special issues, have also been accused as predatory (Crosetto, 2021).  **",
                "Reference": "** Crosetto (2021); Xia et al. (2015)",
                "Originally drafted by": "** Nick Ballou",
                "Reviewed (or Edited) by": "** Olmo van den Akker; Helena Hartmann; Aleksandra Lazić; Graham Reid; Flávio Azevedo",
                "Translated by": "** Mahdi Aben Ahmed",
                "Translation reviewed by": "** Awatif Alruwaili, Ali H. Al-Hoorie, Asma Alzahrani, Mohammed Mohsen",
                "Translation": "التَّعريف:** يصف النَّشر الاستغلالي مجموعة من الممارسات التِّجاريَّة التي يسعى النَّاشرون من خلالها إلى الرِّبح، غالبًا من خلال تحصيل رسوم نشر المقالات، عبر نشر الأعمال العلميَّة بدون فحص معتبر للجودة، مثل تحكيم الأقران، أو خدمات التَّحرير. في أكثر أشكاله تطرفًا، ينشر النَّاشرون الاستغلاليون أي عمل طالما تم دفع رسومه. ومن الاستراتيجيات الأخرى الأقل تطرفًا التي يعدَّها البعض بأنَّها أيضًا استغلاليَّة هي إرسال أعداد كبيرة من الطَّلبات للتَّحرير، أو النَّشر في أعداد خاصّة مدفوعة الأجر. (Crosetto ، 2021).  **المصطلحات ذات الصِّلة:** رسوم النَّشر، التَّلاعب بالنِّظام",
                "Related_terms": "** Article Processing Charge (APC); Gaming (the system)"
            },
            {
                "Title": "PREPARE Guidelines (إرشادات الإعداد) *",
                "Definition": "** The PREPARE guidelines and checklist (Planning Research and Experimental Procedures on Animals: Recommendations for Excellence) aim to help the planning of animal research, and support adherence to the 3Rs (Replacement, Reduction or Refinement) and facilitate the reproducibility of animal research.  **",
                "Reference(s)": "** Smith et al. (2018)",
                "Drafted by": "** Ben Farrar",
                "Reviewed (or Edited) by": "** Mahmoud Elsherif; Gilad Feldman; Elias Garcia-Pelegrin",
                "Translated by": "** Mai Helmy.",
                "Translation reviewed by": "** Awatif Alruwaili, Ali H. Al-Hoorie, Asma Alzahrani, Mohammed Mohsen",
                "Translation": "التَّعريف:** تهدف وثيقة \"تخطيط البحوث، والإجراءات التَّجريبيَّة على الحيوانات: توصيات للتَّميز\" إلى إعداد إرشادات وقائمة للمراجعة تساعد في التَّخطيط لأبحاث الحيوانات، ودعم الالتزام بالاستبدال، والتَّقليل، والصَّقل (المعروفة بالرَّاءات الثَّلاث)، وتسهيل إعادة إنتاج البحوث الحيوانيَّة.  **المصطلحات ذات الصِّلة**: إرشادات أرايف، دليل الإبلاغ، سترينج: الخلفيَّة الاجتماعيَّة والقابليَّة للتَّتبع والاختيار الذَّاتي، وتاريخ التَّربية، والتَّأقلم، والتَّعود.",
                "Related_terms": "** ARRIVE Guidelines; Reporting Guideline; STRANGE"
            },
            {
                "Title": "Preprint (الطّباعة الأوليّة) *",
                "Definition": "** A publicly available version of any type of scientific manuscript/research output preceding formal publication, considered a form of Green Open Access. Preprints are usually hosted on a repository (e.g. arXiv) that facilitates dissemination by sharing research results more quickly than through traditional publication. Preprint repositories typically provide persistent identifiers (e.g. DOIs) to preprints. Preprints can be published at any point during the research cycle, but are most commonly published upon submission (i.e., before peer-review). Accepted and peer-reviewed versions of articles are also often uploaded to preprint servers, and are called postprints.  **",
                "Reference(s)": "** Bourne et al. (2017); Elmore (2018)",
                "Drafted by": "** Mariella Paul",
                "Reviewed (or Edited) by": "** Gisela H. Govaart; Helena Hartmann; Sam Parsons; Tobias Wingen; Flávio Azevedo",
                "Translated by": "** Mai Helmy.",
                "Translation reviewed by": "** Ali H. Al-Hoorie, Ahlam Ahmed, Asma Alzahrani , Mohammed Mohsen",
                "Translation": "التَّعريف:** نسخة متاحة للجمهور من أي نوع من المخطوطات العلميَّة، أو مخرجات البحث التي تسبق النَّشر الرَّسمي، وتعدُّ شكلًا من أشكال الوصول المفتوح الأخضر.  وعادةً ما يتم استضافة هذه النُّسخ في مستودع (مثل arXiv) والذي يسهل النَّشر من خلال مشاركة نتائج البحث بشكل أسرع من النَّشر التَّقليدي.  عادةً ما توفر  مستودعات الطباعة الأولية معرِّفات ثابتة (مثل معرِّفات الكائن الرَّقمي) للمطبوعات الأوليَّة. يمكن نشر النُّسخ الأوليَّة في أي وقت خلال دورة البحث، ولكن يتم نشرها بشكل شائع عند تقديمها للنَّشر (أي قبل تحكيم الأقران). وغالبًا ما يتم \\_أيضًا\\_ تحميل إصدارات المقالات المقبولة والتي تمت مراجعتها من قِبل الأقران إلى مستودعات ما قبل الطِّباعة، وتسمى هذه نسخ ما بعد الطِّباعة.  **المصطلحات ذات الصِّلة:** الوصول المفتوح ،معرِّف الكائن الرَّقمي، ما بعد الطِّباعة، ورقة عمل",
                "Related_terms": "** Open Access; DOI (digital object identifier); Postprint; Working Paper"
            },
            {
                "Title": "Preregistration (التَّسجيل المسبق) *",
                "Definition": "** The practice of publishing the plan for a study, including research questions/hypotheses, research design, data analysis before the data has been collected or examined. It is also possible to preregister secondary data analyses (Merten & Krypotos, 2019). A preregistration document is time-stamped and typically registered with an independent party (e.g., a repository) so that it can be publicly shared with others (possibly after an embargo period). Preregistration provides a transparent documentation of what was planned at a certain time point, and allows third parties to assess what changes may have occurred afterwards. The more detailed a preregistration is, the better third parties can assess these changes and with that the validity of the performed analyses. Preregistration aims to clearly distinguish confirmatory from exploratory research.  **",
                "Reference(s)": "** Haven and van Grootel (2019); Lewandowsky and Bishop (2016); Merten and Krypotos (2019); Navarro (2020); Nosek et al. (2018); Simmons et al. (2021)",
                "Drafted by": "** Mahmoud Elsherif",
                "Reviewed (or Edited) by": "** Gisela H. Govaart; Helena Hartmann; Tina Lonsdorf; William Ngiam; Eike Mark Rinke; Lisa Spitzer; Olmo van den Akker; Flávio Azevedo",
                "Translated by": "** Mai Helmy",
                "Translation reviewed by": "** Ali H. Al-Hoorie, Ahlam Ahmed, Asma Alzahrani, Mohammed Mohsen",
                "Translation": "التَّعريف:** ممارسة نشر خطّة الدِّراسة بما في ذلك أسئلة، وفرضيَّات البحث، وتصميم البحث، وطريقة تحليل البيانات قبل جمع البيانات، أو فحصها، ومن الممكن أيضًا التَّسجيل المسبق لتحليلات البيانات الثَّانويَّة (Merten & Krypotos, 2019). يتم ختم وثيقة التَّسجيل المسبق بختم زمني، وعادة ما يتم تسجيلها لدى طرف مستقل (مثل مستودع بيانات) بحيث يمكن مشاركتها علنًا مع الآخرين (أحيانًا بعد انتهاء فترة حظر اختياريَّة). ويوفِّر التَّسجيل المسبق توثيقًا شفافًا لما تم التَّخطيط له في وقت معين، ويسمح لأطراف ثالثة بتقييم التَّغييرات التي قد تكون حدثت بعد ذلك. وكلَّما كان التَّسجيل المسبق أكثر تفصيلًا، كان بإمكان الطَّرف الثَّالث تقييم هذه التَّغييرات بشكل أفضل، وبالتَّالي صحة التَّحليلات التي تم إجراؤها، ويهدف التَّسجيل المسبق إلى التَّمييز بوضوح بين البحث التَّوكيدي والاستكشافي.  **المصطلحات ذات الصِّلة:** الانحياز التَّوكيدي، التَّحليلات التَّوكيدية، تحليل البيانات الاستكشافيَّ، الافتراض بعد معرفة النَّتائج، ترقيم خطة التَّحليل المسبق، ممارسات البحث المشكوك فيها، أو ممارسات إعداد التَّقارير المشكوك فيها، التَّقرير المسجَّل، مراسم البحث، الشَّفافيَّة.",
                "Related_terms": "** Confirmation bias; Confirmatory analyses; Exploratory Data Analysis; HARKing; Pre-analysis plan; Questionable Research Practices or Questionable Reporting Practices (QRPs); Registered Report; Research Protocol; Transparency"
            },
            {
                "Title": "Preregistration Pledge (تّعهد التَّسجيل المسبق) *",
                "Definition": "** In a “collective action in support of open and reproducible research practices'', the preregistration pledge is a campaign from the Project Free Our Knowledge that asks a researcher to commit to preregistering at least one study in the next two years ([https://freeourknowledge.org/about/](https://freeourknowledge.org/about/)). The project is a grassroots movement initiated by early career researchers (ECRs).  **",
                "Reference(s)": "** https://freeourknowledge.org/2020-12-03-preregistration-pledge/",
                "Originally drafted by": "** Helena Hartmann",
                "Reviewed (or Edited) by": "** Jamie P. Cockcroft; Mahmoud Elsherif; Aleksandra Lazić, Steven Verheyen",
                "Translated by": "** Mai Helmy.",
                "Translation reviewed by": "** Ali H. Al-Hoorie, Ahlam Ahmed, Asma Alzahrani, Mohammed Mohsen",
                "Translation": "التَّعريف**: يعد تعهُّد التَّسجيل المسبق \"إجراءً جماعيًا؛ لدعم ممارسات البحث المفتوحة، والقابلة للتِّكرار\" وهو حملة تابعة لمنصَّة (حرِّر معرفتنا) الذي يطلب من الباحث الالتزام بالتَّسجيل المسبق لدراسة واحدة على الأقل في العامين المقبلين ([https://freeourknowledge.org/about/](https://freeourknowledge.org/about/)). وهذا المشروع عبارة عن حركة شعبيَّة بدأها مجموعة من الباحثين في بداية حياتهم المهنيَّة.  **المصطلحات ذات الصلة:** التَّسجيل المسبق",
                "Related_terms": "** Preregistration"
            },
            {
                "Title": "PRO (peer review openness) initiative (مبادرة انفتاح تحكيم الأقران) *",
                "Definition": "** The agreement made by several academics that they will not provide a peer review of a manuscript unless certain conditions are met. Specifically, the manuscript authors should ensure the data and materials will be made publically available (or give a justification as to why they are not freely available or shared), provide documentation detailing how to interpret and run any files or code and detail where these files can be located via the manuscript itself.  **",
                "Reference": "** Morey et al. (2016)",
                "Originally drafted by": "** Jamie P. Cockcroft",
                "Reviewed (or Edited) by": "** Sarah Ashcroft-Jones; Mahmoud Elsherif; Helena Hartmann; Steven Verheyen",
                "Translated by": "** Mai Helmy.",
                "Translation reviewed by": "** Awatif Alruwaili, Ali H. Al-Hoorie, Asma Alzahrani, Mohammed Mohsen",
                "Translation": "التَّعريف:** هي اتفاقيّة تم التَّوصل إليها من قبل عدد من الأكاديميين بأنَّهم لن يقوموا بمراجعة لأي مخطوطة ما لم تستوفِ شروط معيّنة، وعلى وجه التَّحديد يجب على مؤلفي المخطوطات التَّأكد من إتاحة البيانات، والمواد للجمهور (أو تقديم مبرِّر لعدم توفّرها، أو مشاركتها مجانًا)، وتقديم وثائق توضِّح بالتَّفصيل كيفيَّة تفسير، وتشغيل أي ملفات، أو نصوص برمجيَّة، وتفاصيل مكان هذه الملفَّات، وتحديد موقعها داخل المخطوطة نفسها.  **المصطلحات ذات الصِّلة:** تحكيم الأقران غير مجهولة المصدر، العلم المفتوح، التحكيم المفتوح، تحكيم الأقران الشفاف",
                "Related_terms": "** Non-anonymised peer review; Open Science; Open Peer Review; Transparent peer review"
            },
            {
                "Title": "Prior distribution (التَّوزيع المسبق) *",
                "Definition": "** Beliefs held by researchers about the parameters in a statistical model before further evidence is taken into account. A ‘prior’ is expressed as a probability distribution and can be determined in a number of ways (e.g., previous research, subjective assessment, principles such as maximising entropy given constraints), and is typically combined with the likelihood function using Bayes’ theorem to obtain a posterior distribution.  **",
                "Reference(s)": "** van de Schoot et al. (2021)",
                "Drafted by": "** Alaa AlDoh",
                "Reviewed (or Edited) by": "** Charlotte R. Pennington; Martin Vasilev",
                "Translated by": "** Mai Helmy.",
                "Translation reviewed by": "** Ali H. Al-Hoorie, Ahlam Ahmed, Asma Alzahrani, Mohammed Mohsen",
                "Translation": "التَّعريف:** معتقدات الباحثين حول مؤشِّرات النّموذج الإحصائي قبل جمع البيانات، ويتم التَّعبير عنها على شكل توزيع احتمالي، ويمكن تحديده بعدَّة طرق (مثل البحث السَّابق، أو التَّقييم الذَّاتي، أو بعض المبادئ مثل \"ازدياد الإنتروبية بسبب القيود\")، وعادة ما يتم دمجها مع دالة الاحتماليَّة باستخدام نظرية بايز للحصول على التَّوزيع اللَّاحق.  **المصطلحات ذات الصِّلة:** معامل بايز، الاستدلال البايزي، تقدير المعاملات باستخدام النَّهج البايزي، دالة الاحتماليَّة، التَّوزيع اللَّاحق",
                "Related_terms": "** Bayes Factor; Bayesian inference; Bayesian Parameter Estimation; Likelihood function; Posterior distribution"
            },
            {
                "Title": "Pseudonymisation (التَّسمية المستعارة) *",
                "Definition": "** Pseudonymisation refers to a technique that involves replacing or removing any information that could lead to identification of research subjects’ identity whilst still being able to make them identifiable through the use of the combination of code number and identifiers. This process comprises the following steps: removal of all identifiers from the research dataset; attribution of a specific identifier (pseudonym) for each participant and using it to label each research record; and maintenance of a cipher that links the code number to the participant in a document physically separate from the dataset. Pseudonymisation is typically a minimum requirement from ethical committees when conducting research, especially on human participants or involving confidential information, in order to ensure upholding of data privacy.  **",
                "Reference": "** Mourby et al. (2018); UKRI ([https://mrc.ukri.org/documents/pdf/gdpr-guidance-note-5-identifiability-anonymisation-and-pseudonymisation/](https://mrc.ukri.org/documents/pdf/gdpr-guidance-note-5-identifiability-anonymisation-and-pseudonymisation/))",
                "Originally drafted by": "** Catia M. Oliveira",
                "Reviewed (or Edited) by": "** Helena Hartmann; Sam Parsons; Charlotte R. Pennington; Birgit Schmidt",
                "Translated by": "Mai Helmy.",
                "Translation reviewed by": "** Ali H. Al-Hoorie, Ahlam Ahmed, Asma Alzahrani, Mohammed Mohsen",
                "Translation": "التَّعريف:** تشير إلى تقنيَّة تتضمَّن استبدال، أو إزالة أي معلومات يمكن أن تؤدِّي إلى تحديد هويّة الأشخاص الخاضعين للبحث مع الاستمرار في جعل التَّعرُّف عليهم ممكنًا من خلال استخدام مجموعة من الأرقام، والمعرِّفات الرَّمزيَّة، وتتضمَّن هذه العمليَّة الخطوات الآتية:  إزالة جميع ما يمكن أن يُعرف عن الهوية من بيانات البحث، إسناد معرّف اسم مستعار لكلِّ مشارك واستخدامه؛ لتسميته في كل سجل بحث، وحفظ هذه الأسماء المستعارة مع ما ترمز إليه في مستند منفصل عن البيانات البحثيَّة.  وعادةً ما يكون استخدام الأسماء المستعارة هو الحد الأدنى من  متطلبات اللِّجان الأخلاقيَّة عند إجراء البحوث، خاصةً فيما يتعلَّق بالمشاركين البشر والمعلومات السِّريَّة، من أجل ضمان الحفاظ على خصوصيَّة البيانات.  **المصطلحات ذات الصِّلة:** التّعمية، السِّريَّة، خصوصيَّة البيانات، إزالة الهوية، التَّسمية المستعارة، أخلاقيَّات البحث",
                "Related_terms": "** Anonymity; Confidentiality; Data privacy; De-identification; Pseudonymisation; Research ethics"
            },
            {
                "Title": "Pseudoreplication (التِّكرار الكاذب) *",
                "Definition": "** When there is a lack of statistical independence presented in the data and thus artificially inflating the number of samples (i.e. replicates). For instance, collecting more than one data point from the same experimental unit (e.g. participant or crops). Numerous methods can overcome this, such as averaging across replicates (e.g., taking the mean RT for a participant) or implementing mixed effects models with the random effects structure accounting for the pseudoreplication (e.g., specifying each individual RT as belonging to the same subject). Note, the former option would be associated with a loss of information and statistical power.  **",
                "Reference(s)": "** Davies and Gray (2015); Hurlbert (1984); Lazic (2019)",
                "Drafted by": "** Ben Farrar",
                "Reviewed (or Edited) by": "** Jamie P. Cockcroft; Mahmoud Elsherif; Elias Garcia-Pelegrin; Annalise A. LaPlume",
                "Translated by": "** Mai Helmy.",
                "Translation reviewed by": "** Ali H. Al-Hoorie, Ahlam Ahmed, Asma Alzahrani, Mohammed Mohsen",
                "Translation": "التَّعريف:** يقع التِّكرار الكاذب عندما لا يكون هناك استقلاليَّة إحصائيَّة في البيانات، والذي يؤدي إلى تضخيم عدد العيِّنات بشكل مصطنع، مثلًا عند جمع أكثر من نقطة بيانات واحدة من نفس الوحدة التَّجريبيَّة (مثلًا مأخوذة من مشارك في الدِّراسة أو من محصول؟).  هناك طرق للتَّغلُّب على هذه المشكلة مثل: استخراج المتوسِّط من هذه التِّكرارات (كأخذ متوسِّط وقت الاستجابة للمشارك)، أو اللُّجوء لنماذج التَّأثيرات المختلطة مع حساب التَّأثيرات العشوائيَّة للتِّكرار الكاذب (كتحديد أنَّ أوقات الاستجابة لكلِّ فرد تنتمي إلى ذلك الفرد).  ونلاحظ أنَّ الخيار الأوّل سيكون مرتبطًا بفقدان المعلومات، والقوة الإحصائيَّة.  **المصطلحات ذات الصِّلة:** مربك ،القابليَّة للتَّعميم، التكرار، الصدق",
                "Related_terms": "** Confounding; Generalizability; Replication; Validity"
            },
            {
                "Title": "Psychometric meta-analysis (التَّحليل البعديّ للمقاييس النَّفسيّة) *",
                "Definition": "** Psychometric meta-analyses aim to correct for attenuation of the effect sizes of interest due to measurement error and other artifacts by using procedures based on psychometric principles, e.g. reliability of the measures. These procedures should be implemented before using the synthesised effect sizes in correlational or experimental meta-analysis, as making these corrections tends to lead to larger and less variable effect sizes.  **",
                "Reference(s)": "** Borenstein et al. (2009); Schmidt and Hunter (2014)",
                "Drafted by": "** Adrien Fillon",
                "Reviewed (or Edited) by": "** Mahmoud Elsherif; Eduardo Garcia-Garzon; Helena Hartmann; Catia M. Oliveira; Flávio Azevedo",
                "Translated by": "** Mai Helmy.",
                "Translation reviewed by": "** Ali H. Al-Hoorie, Hala Alghamdi, Asma Alzahrani, Mohammed Mohsen",
                "Translation": "التَّعريف:** تهدف التَّحليلات البعديَّة للمقاييس النَّفسيَّة إلى تصحيح حجم تأثير معيّن، والحاصل بسبب خطأ في القياس وغيره من الأخطاء؛ وذلك باستخدام إجراءات تعتمد على مبادئ القياس النَّفسي، كثبات المقاييس.  ينبغي تنفيذ هذه الإجراءات قبل تحليل حجم الأثر في التَّحليل البعدي الارتباطي، أو التَّجريبي، حيث إن إجراء هذه التَّصحيحات قد يؤدِّي إلى أحجام تأثير أكبر، وأكثر ثباتًا.  **المصطلحات ذات الصِّلة:** التَّحليل التَّالي التَّرابطي، التَّحليل التَّالي لهنتر شميدت، التَّحليل البعدي، المراجعات المنهجيَّة غير التَّدخليَّة المفتوحة، وقابلة للتِّكرار، تحيُّز النَّشر (مشكلة درج الملفَّات)، تعميم الصَّلاحيَّة",
                "Related_terms": "** Correlational meta-analysis; Hunter-Schmidt meta-analysis; Meta-analysis; Non-Intervention, Reproducible, and Open Systematic Reviews (NIRO-SR); Publication bias (File Drawer Problem); Validity generalization"
            },
            {
                "Title": "Publication bias (File Drawer Problem) ((تحيُّز النَّشر (مشكلة درج الملفَّات) *",
                "Definition": "** The failure to publish results based on the \"direction or strength of the study findings\" (Dickersin & Min, 1993, p. 135). The bias arises when the evaluation of a study’s publishability disproportionately hinges on the outcome of the study, often with the inclination that novel and significant results are worth publishing more than replications and null results. This bias typically materializes through a disproportionate number of significant findings and inflated effect sizes. This process leads to the published scientific literature not being representative of the full extent of all research, and specifically underrepresents null finding. Such findings, in turn, land in the so called “file drawer”, where they are never published and have no findable documentation.  **",
                "Reference(s)": "** Dickersin and Min (1993); Devito and Goldacre (2019); Duval and Tweedie (2000a, 2000b); Franco et al. (2014); Lindsay (2020); Rothstein et al. (2005)",
                "Drafted by": "** Mahmoud Elsherif",
                "Reviewed (or Edited) by": "** Jamie P. Cockcroft; Gilad Feldman; Adrien Fillon; Helena Hartmann; Tamara Kalandadze; William Ngiam; Martin Vasilev; Olmo van den Akker; Flávio Azevedo",
                "Translated by": "** Ahlam Ahmed",
                "Translation reviewed by": "** Ali H. Al-Hoorie, Hala Alghamdi, Asma Alzahrani, Mohammed Mohsen",
                "Translation": "التَّعريف:** هو عدم نشر النَّتائج بناءً على \"اتِّجاه أو قوّة نتائج الدِّراسة\" (Dickersin & Min, 1993, p. 135). ينشأ التَّحيُّز عندما يتوقَّف تقييم قابليَّة نشر الدِّراسة بشكل كبير على نتائج الدِّراسة، وغالبًا ما يكون ذلك مع الميل إلى أنَّ النَّتائج الجديدة، وذات الدّلالة الإحصائيَّة تستحق النَّشر أكثر من الدِّراسات التِّكراريَّة، والنَّتائج التي لا تظهر دلالة إحصائيَّة.  يتضح هذا التَّحيُّز عادةً من خلال وجود عدد كبير من الدِّراسات ذات الدِّلالة الإحصائيَّة، وأحجام التَّأثير المتضخِّمة فيها.  تؤدي هذه العمليَّة إلى كون الأدبياَّت العلميَّة المنشورة لا تمثِّل المدى الكامل لجميع الأبحاث، خصوصًا الدِّراسات التي لا تظهر نتائجها دلالة إحصائيَّة، يؤدِّي هذا بدوره إلى ما يسمَّى \"درج الملفَّات\" للأبحاث التي لا يتم نشرها أبدًا، وليس لها وثائق يمكن من خلالها العثور عليها.  **المصطلحات ذات الصِّلة:** تحيُّز النَّشر؛  منحنى القيم الاحتماليَّة،  قرصنة القيمة الاحتماليَّة،  تقارير انتقائيَّة؛  الدِّلالة الإحصائيَّة؛  طريقة التَّقليم والتَّعبئة **تعريف بديل:** في سياق التَّحليل التَّلوي، يحدث تحيُّز النَّشر \"... عندما يكون البحث الذي يظهر في الأدبيَّات المنشورة غير مُمثل بشكل منهجي لمجموعة الدِّراسات المكتملة. ببساطة، عندما يختلف البحث المتاح بسهولة في نتائجه عن نتائج جميع البحوث التي تم إجراؤها في مجال ما، فإنَّ القراء والمراجعين لهذا البحث معرَّضون لخطر التَّوصل إلى استنتاج خاطئ حول ما تظهره مجموعة الأبحاث هذه  \".  (روثستين وآخرون، 2005، ص 1\\) **المصطلحات ذات الصِّلة بالتَّعريف البديل:** التَّحليل البعدي",
                "Related_terms": "** Dissemination bias; P-curve; P-hacking; Selective reporting; Statistical significance; Trim and fill method **Alternative definition:** In the context of meta-analysis, publication bias “...occurs whenever the research that appears in the published literature is systematically unrepresentative of the population of completed studies. Simply put, when the research that is readily available differs in its results from the results of all the research that has been done in an area, readers and reviewers of that research are in danger of drawing the wrong conclusion about what that body of research shows.” (Rothstein et al., 2005, p. 1\\) **Related terms to alternative definition:** meta-analysis"
            },
            {
                "Title": "Public Trust in Science (ثقة الجمهور في العلوم) *",
                "Definition": "** Trust in the knowledge, guidelines and recommendations that has been produced or provided by scientists to the benefit of civil society (Hendriks et al., 2016). These may also refer to trust in scientific-based recommendations on public health (e.g., universal health-care, stem cell research, federal funds for women’s reproductive rights, preventive measures of contagious diseases, and vaccination), climate change, economic policies (e.g., welfare, inequality- and poverty-control) and their intersections. The trust a member of the public has in science has been shown to be influenced by a vast number of factors such as age (Anderson et al., 2012), gender (Von Roten, 2004), rejection of scientific norms (Lewandowsky & Oberauer, 2021), political ideology (Azevedo & Jost, 2021; Brewer & Ley, 2012; Leiserowitz et al., 2010), right-wing authoritarianism and social dominance (Kerr & Wilson, 2021), education (Bak, 2001; Hayes & Tariq, 2000), income (Anderson et al., 2012), science knowledge (Evans & Durant, 1995; Nisbet et al., 2002), social media use (Huber et al., 2019), and religiosity (Azevedo, 2021; Brewer & Ley, 2013; Liu & Priest, 2009). **",
                "Reference(s)": "** Anderson et al. (2012); Azevedo (2021); Azevedo and Jost (2021); Bak (2001); Brewer and Ley (2013); Evans and Durant (1995); Hayes and Tariq (2000); Hendriks et al. (2016); Huber et al. (2019); Kerr and Wilson (2021); Lewandowsky and Oberauer (2021); Liu and Priest (2009); Nisbet et al. (2002); Schneider et al., (2019); Wingen et al. (2020)",
                "Originally drafted by": "** Tobias Wingen; Flávio Azevedo",
                "Reviewed (or Edited) by": "** Elias Garcia-Pelegrin; Helena Hartmann; Catia M. Oliveira; Olmo van den Akker",
                "Translated by": "** Mai Helmy.",
                "Translation reviewed by": "** Ali H. Al-Hoorie, Ahlam Ahmed, Asma Alzahrani, Mohammed Mohsen",
                "Translation": "التَّعريف:** الثِّقة في المعرفة والمبادئ التَّوجيهيَّة، والتَّوصيات التي أنتجها العلماء، أو قدَّموها للمجتمع المدني (Hendriks et al., 2016). وقد تشير أيضًا إلى الثِّقة في التَّوصيات القائمة على أساس علمي  بشأن الصِّحة العامّة (كالرِّعاية الصِّحيَّة الشَّاملة، وبحوث الخلايا الجذعيَّة، والصَّناديق الفيدراليَّة المخصَّصة للحقوق الإنجابيَّة للمرأة، والتَّدابير الوقائيَّة للأمراض المعدية، والتَّطعيم)، وتغيُّر المناخ، والسِّياسات الاقتصاديَّة ( مثل الرَّفاهيَّة، وعدم المساواة، ومكافحة الفقر) وتقاطعاتها.  ولقد تبيّن أن ثقة الجمهور في العلم تتأثَّر بعدد كبير من العوامل مثل العمر (Anderson et al., 2012\\) ، والجنس (Von Roten, 2004\\) ، ورفض المعايير العلميَّة (Lewandowsky & Oberauer, 2021\\) ، والأيديولوجيَّة السِّياسيَّة (Azevedo & Jost, 2021; Brewer & Ley, 2012; Leiserowitz et al., 2010\\) ، والاستبداد اليميني والهيمنة الاجتماعيَّة (Kerr & Wilson, 2021\\) ، والتَّعليم (Bak, 2001; Hayes & Tariq, 2000\\) والدَّخل (Anderson et al., 2012\\) والمعرفة العلميَّة (Evans & Durant, 1995; Nisbet et al., 2002\\) واستخدام وسائل التَّواصل الاجتماعي (Huber et al., 2019\\) والتدين (Azevedo, 2021; Brewer & Ley, 2013; Liu & Priest, 2009).  **المصطلحات ذات الصِّلة:** مصداقية الادِّعاءات العلميَّة، الثِّقة المعرفيَّة",
                "Related_terms": "** Credibility of scientific claims; Epistemic Trust"
            },
            {
                "Title": "Publish or Perish (النَّشر أو  الفناء ) *",
                "Definition": "** An aphorism describing the pressure researchers feel to publish academic manuscripts, often in high prestige academic journals, in order to have a successful academic career. This pressure to publish a high quantity of manuscripts can go at the expense of the quality of the manuscripts. This institutional pressure is exacerbated by hiring procedures and funding decisions strongly focusing on the number and impact of publications.  **",
                "Reference(s)": "** Case (1928); Fanelli (2010)",
                "Drafted by": "** Eliza Woodward",
                "Reviewed (or Edited) by": "** Nick Ballou; Mahmoud Elsherif; Helena Hartmann; Annalise A. LaPlume; Sam Parsons; Timo Roettger; Olmo van den Akker",
                "Translated by": "** Hiba Alomary.",
                "Translation reviewed by": "** Awatif Alruwaili, Ali H. Al-Hoorie, Asma Alzahrani, Mohammed Mohsen",
                "Translation": "التَّعريف:** قول مأثور يصف الضَّغط الذي يتعرَّض له الباحثون لنشر عدد كبير من الأبحاث في مجلات أكاديميَّة مرموقة بغرض النَّجاح في مهنهم الأكاديميَّة، يؤثِّر هذا الضَّغط على جودة البحوث لتركيزه على كمية عوضاً عن جودتها، كما يتفاقم هذا الضَّغط المؤسَّسي باعتماد إجراءات التَّوظيف، وقرارات التَّمويل المؤسَّسيَّة على عدد البحوث المنشورة ومعامل تأثيرها.  **المصطلحات ذات الصِّلة:** هيكل الحوافز، معامل تأثير المجلَّة، أزمة إعادة الإنتاج (أو أزمة التِّكرار)، التَّقطيع غير المبرَّر، العلم البطيء.",
                "Related_terms": "** Incentive structure; Journal Impact Factor; Reproducibility crisis (aka Replicability or replication crisis); Salami slicing; Slow Science"
            },
            {
                "Title": "PubPeer (منصة بابّير (PubPeer))  *",
                "Definition": "** A website that allows users to post anonymous peer reviews of research that has been published (i.e. post-publication peer review).  **",
                "Reference(s)": "** www.pubpeer.com",
                "Drafted by": "** Ali H. Al-Hoorie",
                "Reviewed (or Edited) by": "** Mahmoud ELsherif",
                "Translated by": "** Mai Helmy.",
                "Translation reviewed by": "** Ahlam Ahmed, Ali H. Al-Hoorie, Asma Alzahrani, Mohammed Mohsen",
                "Translation": "التَّعريف:** موقع ويب يسمح للمستخدمين بنشر مراجعات أقران مجهولة المصدر للبحوث ا المنشورة (أي مراجعة الأقران بعد النَّشر).  **المصطلحات ذات الصِّلة:** التّحكيم المفتوح.",
                "Related_terms": "** Open Peer Review"
            },
            {
                "Title": "Python (بايثون) *",
                "Definition": "** An interpreted general-purpose programming language, intended to be user-friendly and easily readable, originally created by Guido van Rossum in 1991\\. Python has an extensive library of additional features with accessible documentation for tasks ranging from data analysis to experiment creation. It is a popular programming language in data science, machine learning and web development. Similar to R Markdown, Python can be presented in an interactive online format called a Jupyter notebook, combining code, data, and text.  **",
                "Reference": "** Lutz (2001)",
                "Originally drafted by": "** Shannon Francis",
                "Reviewed (or Edited) by": "** James E. Bartlett; Alexander Hart; Helena Hartmann; Dominik Kiersz; Graham Reid; Andrew J. Stewart",
                "Translated by": "** Mai Helmy.",
                "Translation reviewed by": "** Ali H. Al-Hoorie, Ahlam Ahmed, Asma Alzahrani, Mohammed Mohsen  ### **Q** {#q}",
                "Translation": "التَّعريف:** لغة برمجة عامَّة الغرض، تهدف إلى أن تكون سهلة الاستخدام وسهلة القراءة.  تم إنشاؤها في الأصل بواسطة غويدو فإنَّ روسوم في عام 1991\\. و تمتلك بايثون مكتبة واسعة من المميّزات الإضافيَّة مع وثائق يمكن الوصول إليها للمهام التي تتراوح من تحليل البيانات إلى إنشاء التَّجارب. تعد بايثون لغة برمجة شائعة في علوم البيانات، والتَّعلُّم الآلي وتطوير الشبكة. على غرار موقع آر مارك داون (R Markdown)، يمكن تقديم بايثون بتنسيق تفاعلي عبر الإنترنت يسمى مشروع جوبتر نوت بوك (Jupyter notebook)، والذي يجمع بين التَّعليمات البرمجيَّة، والبيانات والنُّصوص.  **المصطلحات ذات الصِّلة:** جوبترJupyter، مات بلوت ليب (Matplotlib)،  نمباي (NumPy، اوبن سيسمي (OpenSesame)، بيسكو بي (PsychoPy)، لغة الآر",
                "Related_terms": "** Jupyter; Matplotlib; NumPy; OpenSesame; PsychoPy; R"
            },
            {
                "Title": "Qualitative research (البحث النَّوعي) *",
                "Definition": "** Research which uses non-numerical data, such as textual responses, images, videos or other artefacts, to explore in-depth concepts, theories, or experiences. There are a wide range of qualitative approaches, from micro-detailed exploration of language or focusing on personal subjective experiences, to those which explore macro-level social experiences and opinions. **",
                "Reference(s)": "** Aspers and Corte (2019); Levitt et al. (2017)",
                "Drafted by": "** Madeleine Pownall",
                "Reviewed (or Edited) by": "** Mahmoud Elsherif; Helena Hartmann; Oscar Lecuona; Claire Melia; Flávio Azevedo.",
                "Translated by": "** Ahlam Ahmed",
                "Translation reviewed by": "** Ali H. Al-Hoorie, Hala Alghamdi, Asma Alzahrani, Mohammed Mohsen",
                "Translation": "التَّعريف:** البحث الذي يستخدم بيانات غير رقميَّة، مثل الإجابات النَّصيَة، أو الصُّور، أو مقاطع الفيديو وما شابه، لاستكشاف المفاهيم، أو النَّظريَّات، أو التَّجارب بعمق. هناك مجموعة واسعة من الأساليب النَّوعيَّة بدءًا من الاستكشاف التَّفصيلي الدَّقيق للغة، أو التَّركيز على التَّجارب الشَّخصيَّة إلى تلك التي تستكشف التَّجارب، والآراء الاجتماعيَّة على المستوى الأكبر. **المصطلحات ذات الصِّلة:** تأطير المقابلات؛  الموضعيَّة؛ البحث الكمي؛ الانعكاسيَّة **تعريف بديل:** (إن وُجِد) في علم النَّفس، عادةً ما تهتم نظرية المعرفة للبحث النوعي بفهم وجهات نظر النَّاس. تقترح نظريَّة المعرفة هذه افتراض المساواة بين الباحثين، والمشاركين كبشر، وبالتَّالي الحاجة إلى فهم إنساني متعاطف بدلًا من الاستنتاجات المستندة إلى البيانات.",
                "Related_terms": "** Bracketing Interviews; Positionality; Quantitative research; Reflexivity **Alternative definition:** (if applicable) In Psychology, the **epistemology** of qualitative research is typically concerned with understanding people’s perspectives. Such epistemology proposes assuming the equity of researchers and participants as human beings, and in consequence, the need of sympathetic human understanding instead of data-driven conclusions"
            },
            {
                "Title": "Quantitative research (البحث الكميّ) *",
                "Definition": "** Quantitative research encompasses a diverse range of methods to systematically investigate a range of phenomena via the use of numerical data which can be analysed with statistics.  **",
                "Reference(s)": "** Goertzen (2017)",
                "Drafted by": "** Aoife O’Mahony",
                "Reviewed (or Edited) by": "** Valeria Agostini; Tamara Kalandadze; Adam Parker.",
                "Translated by": "Ahlam Ahmed",
                "Translation reviewed by": "** Ali H. Al-Hoorie, Hala Alghamdi, Asma Alzahrani, Mohammed Mohsen",
                "Translation": "التَّعريف**: يشمل البحث الكمي مجموعة متنوِّعة من الأساليب للبحث بشكل منهجي في مجموعة من الظَّواهر من خلال استخدام البيانات الرَّقمية التي يمكن تحليلها إحصائيًا.  **المصطلحات ذات الصِّلة:** القياس، البحث النَّوعي، حجم العيّنة، القوة الإحصائيَّة، إحصائيّات.",
                "Related_terms": "** Measuring; Qualitative research; Sample size; Statistical power; Statistics"
            },
            {
                "Title": "Questionable Research Practices or Questionable Reporting Practices (QRPs) (ممارسات البحث المشكوك فيها أو ممارسات إعداد التَّقارير المشكوك فيها) *",
                "Definition": "** A range of activities that intentionally or unintentionally distort data in favour of a researcher’s own hypotheses \\- or omissions in reporting such practices \\- including; selective inclusion of data, hypothesising after the results are known (HARKing), and *p*\\-hacking. Popularized by John et al. (2012).  **",
                "Reference(s)": "** Banks et al. (2016); Fiedler and Schwartz (2016); Hardwicke et al. (2014); John et al. (2012); Neuroskeptic (2012); Sijtsma (2016); Simonsohn et al. (2011)",
                "Drafted by": "** Mahmoud Elsherif",
                "Reviewed (or Edited) by": "** Tamara Kalandadze; William Ngiam; Sam Parsons; Mariella Paul; Eike Mark Rinke; Timo Roettger; Flávio Azevedo",
                "Translated by": "** Mai Helmy.",
                "Translation reviewed by": "** Ali H. Al-Hoorie, Ahlam Ahmed, Asma Alzahrani, Mohammed Mohsen",
                "Translation": "التَّعريف:** مجموعة من الأنشطة التي تشوه البيانات عن قصد، أو عن غير قصد لصالح فرضيَّات الباحث نفسه \\- أو إغفال الإبلاغ عن مثل هذه الممارسات \\- بما في ذلك التَّضمين الانتقائي للبيانات، والافتراض بعد معرفة النَّتائج، وقرصنة القيمة الاحتماليَّة، وشاعت هذه التَّسمية بعد بحث جون وزملاؤه (John et al., 2012).  **المصطلحات ذات الصِّلة:** الاستخدام الإبداعي للقيم المتطرِّفة. التَّزوير ، درج الملفَّات، حديقة المسارات المتشعِّبة، الافتراض بعد معرفة النَّتائج، ترقيم عدم نشر البيانات، قرصنة القيمة الاحتماليَّة  ، صيد القيمة الاحتماليَّة P ، النَّشر الجزئي للبيانات، رواية القصص اللَّاحقة، التَّسجيل المسبق، ممارسات القياس المشكوك فيها، درجات حرية الباحث، القرصنة العكسيَّة للقيمة الاحتماليَّة، التَّقطيع غير المبرَّر",
                "Related_terms": "** Creative use of outliers; Fabrication; File-drawer; Garden of forking paths; HARKing; Nonpublication of data; *P*\\-hacking; *P*\\-value fishing; Partial publication of data; Post-hoc storytelling; Preregistration; Questionable Measurement Practices (QMP); Researcher degrees of freedom; Reverse *p*\\-hacking; Salami slicing"
            },
            {
                "Title": "Questionable Measurement Practices (QMP) (ممارسات القياس المشكوك فيها ) *",
                "Definition": "** Decisions researchers make that raise doubts about the validity of measures used in a study, and ultimately the study’s final conclusions (Flake & Fried, 2020). Issues arise from a lack of transparency in reporting measurement practices, a failure to address construct validity, negligence, ignorance, or deliberate misrepresentation of information.  **",
                "Reference": "** Flake and Fried (2020)",
                "Originally drafted by": "** Halil Emre Kocalar",
                "Reviewed (or Edited) by": "** Jamie P. Cockcroft; Annalise A. LaPlume; Sam Parsons; Mirela Zaneva; Flávio Azevedo",
                "Translated by": "** Mai Helmy.",
                "Translation reviewed by": "Ali H. Al-Hoorie, Hiba Alomary, Asma Alzahrani, Mohammed Mohsen  ###  ### **R** {#r}",
                "Translation": "التَّعريف:** القرارات التي يتَّخذها الباحثون، والتي تثير الشُّكوك حول صحة المقاييس المستخدمة في الدِّراسة، وبالتَّالي التَّشكيك في صحة نتائج هذه الدِّراسة (Flake & Fried, 2020). وتنشأ المشكلات من انعدام الشَّفافيَّة في الإبلاغ عن ممارسات القياس، أو الفشل في معالجة الصِّدق البنائي، أو الإهمال، أو الجهل، أو التَّحريف المتعمَّد للمعلومات.  **المصطلحات ذات الصِّلة:** الصِّدق البنائي، القياس،  قرصنة القيمة الاحتماليَّة ، القياس النَّفسي، ممارسات البحث المشكوك فيها، أو ممارسات إعداد التَّقارير المشكوك فيها، الصدق",
                "Related_terms": "** Construct validity; Measurement schmeasurement; *P*\\-hacking; Psychometrics; Questionable Research Practices or Questionable Reporting Practices (QRPs); Validity"
            },
            {
                "Title": "R (لغة الآر) *",
                "Definition": "** R is a free, open-source programming language and software environment that can be used to conduct statistical analyses and plot data. R was created by Ross Ihaka and Robert Gentleman at the University of Auckland. R enables authors to share reproducible analysis scripts, which increases the transparency of a study. Often, R is used in conjunction with an integrated development environment (IDE) which simplifies working with the language, for example RStudio or Visual Studio Code, or Tinn-R .  **",
                "Reference": "** [https://www.r-project.org/](https://www.r-project.org/); R Core Team (2020)",
                "Originally drafted by": "** Lisa Spitzer",
                "Reviewed (or Edited) by": "** Bradley Baker; Alexander Hart; Joanne McCuaig; Andrew J. Stewart",
                "Translated by": "** Awatif Alruwaili",
                "Translation reviewed by": "** Ali H. Al-Hoorie, Hala Alghamdi, Ahlam Ahmed, Mohammed Mohsen",
                "Translation": "التَّعريف:** هي لغة برمجة مجانيَّة ومفتوحة المصدر وبيئة برمجيَّة يمكن استخدامها لإجراء التَّحليلات الإحصائيَّة ورسم البيانات.  أنشأ هذه اللُّغة روس إسحاق، وروبرت جنتلمان في جامعة أوكلاند. يساعد برنامج الآر المؤلفين من مشاركة البرامج النَّصية للتَّحليل القابلة للتِّكرار، مما يزيد من شفافيَّة الدِّراسة. غالبًا ما يتم استخدام الار جنبًا إلى جنب مع بيئة التَّطوير المتكاملة التي تبسط العمل مع اللُّغة، مثل RStudio  أو Visual Studio Code، أو Tinn-R. **مصطلحات ذات العلاقة:** مفتوح المصدر, التحليلات الإحصائية.",
                "Related_terms": "** Open-source; Statistical analysis"
            },
            {
                "Title": "Red Teams (الفرق الحمراء) *",
                "Definition": "** An approach that integrates external criticism by colleagues and peers into the research process. Red teams are based on the idea that research that is more critically and widely evaluated is more reliable. The term originates from a military practice: One group (the red team) attacks something, and another group (the blue team) defends it. The practice has been applied to open science, by giving a red team (designated critical individuals) financial incentives to find errors in or identify improvements to the materials or content of a research project (in the materials, code, writing, etc.; Coles et al., 2020).  **",
                "Reference": "** Coles et al. (2020); Lakens (2020)",
                "Originally drafted by": "** Annalise A. LaPlume",
                "Reviewed (or Edited) by": "** Nick Ballou; Mahmoud Elsherif**;** Thomas Rhys Evans; Helena Hartmann; Timo Roettger",
                "Translated by": "** Ahmed Hakami",
                "Translation reviewed by": "** Ali H. Al-Hoorie, Hala Alghamdi, Asma Alzahrani, Mohammed Mohsen",
                "Translation": "التَّعريف:** نهج يدمج بين النَّقد الخارجي، ونقد الأقران في عمليَّة البحث. تعتمد الفرق الحمراء على فكرة أنَّ البحث الذي يتم تقييمه بشكل نقدي، وواسع يكون أكثر موثوقيَّة.  نشأ هذا المصطلح من التَّدريبات العسكريَّة: مجموعة (الفريق الأحمر) تهاجم شيئًا ما، ومجموعة أخرى (الفريق الأزرق) تدافع عنه. تم تطبيق هذه الممارسة على العلم المفتوح، من خلال إعطاء الفريق الأحمر (الأفراد الناقدين) حوافز ماليَّة للعثور على أخطاء في البحث، أو لتحديد تحسينات على البحث ومحتواه، مثلًا في المواد، أو النُّصوص البرمجيَّة، أو الكتابة، إلى آخره (Coles et al., 2020).  **المصطلحات ذات الصِّلة:** التَّشارك العدائي",
                "Related_terms": "** Adversarial collaboration"
            },
            {
                "Title": "Reflexivity (الانعكاسيَّة) *",
                "Definition": "** The process of reflexivity refers to critically considering the knowledge that we produce through research, how it is produced, and our own role as researchers in producing this knowledge. There are different forms of reflexivity; personal reflexivity whereby researchers consider the impact of their own personal experiences, and functional whereby researchers consider the way in which our research tools and methods may have impacted knowledge production. Reflexivity aims to bring attention to underlying factors which may impact the research process, including development of research questions, data collection, and the analysis.  **",
                "Reference(s)": "** Braun and Clarke (2013); Finlay and Gough (2008)",
                "Drafted by": "** Claire Melia",
                "Reviewed (or Edited) by": "** Gilad Feldman; Annalise A. LaPlume",
                "Translated by": "** Ahlam Ahmed",
                "Translation reviewed by": "** Ali H. Al-Hoorie, Hala Alghamdi, Asma Alzahrani, Mohammed Mohsen",
                "Translation": "التَّعريف:** تشير عمليَّة الانعكاسيَّة إلى النَّظر النَّقدي في المعرفة التي ننتجها من خلال البحث، وكيفيَّة إنتاجها، ودورنا كباحثين في إنتاج هذه المعرفة.  هناك أشكال مختلفة من الانعكاسيَّة. ففي الانعكاسيَّة الشَّخصية، ينظر الباحثون في تأثير تجاربهم الشَّخصيَّة على الإنتاج المعرفي، وفي الانعكاسيَّة الوظيفية ينظر الباحثون في الطَّريقة التي قد تؤثر بها أدوات وأساليب البحث لدينا على إنتاج المعرفة.  تهدف الانعكاسيَّة إلى لفت الانتباه إلى العوامل الأساسيَّة التي قد تؤثر على عمليَّة البحث، بما في ذلك تطوير أسئلة البحث وجمع البيانات والتَّحليل. ا**لمصطلحات ذات الصِّلة:** تأطير المقابلات،  البحث النَّوعي",
                "Related_terms": "** Bracketing Interviews; Qualitative Research"
            },
            {
                "Title": "Registered Report (التَّقرير المسجَّل ) *",
                "Definition": "** A scientific publishing format that includes an initial round of peer review of the background and methods (study design, measurement, and analysis plan); sufficiently high quality manuscripts are accepted for in-principle acceptance (IPA) at this stage. Typically, this stage 1 review occurs before data collection, however secondary data analyses are possible in this publishing format. Following data analyses and write up of results and discussion sections, the stage 2 review assesses whether authors sufficiently followed their study plan and reported deviations from it (and remains indifferent to the results). This shifts the focus of the review to the study’s proposed research question and methodology and away from the perceived interest in the study’s results.  **",
                "Reference(s)": "** Chambers (2013); Chambers et al. (2015); Chambers and Tzavella (2020); Findley et al. (2016); [https://www.cos.io/initiatives/registered-reports](https://www.cos.io/initiatives/registered-reports)",
                "Drafted by": "** Madeleine Pownall",
                "Reviewed (or Edited) by": "** Gilad Feldman; Emma Henderson; Aoife O’Mahony; Sam Parsons; Mariella Paul; Charlotte R. Pennington; Eike Mark Rinke; Timo Roettger; Olmo van den Akker; Yuki Yamada; Flávio Azevedo",
                "Translated by": "** Ahlam Ahmed",
                "Translation reviewed by": "** Ali H. Al-Hoorie, Hiba Alomary, Asma Alzahrani, Mohammed Mohsen",
                "Translation": "التَّعريف:** شكل من أشكال النَّشر العلمي يتضمَّن جولة أوليَّة من تحكيم الأقران للخلفيَّة المعرفية، وطرق البحث (تصميم الدِّراسة والقياس وخطة التّحليل)، يتم بعدها قبول المخطوطات عالية الجودة مبدئيًا. عادةً ما تحدث هذه المراجعة الأولى قبل جمع البيانات، ولكن من الممكن اقتراح تحليل بيانات تم جمعها بهذه الطَّريقة. وبعد الانتهاء من تحليل البيانات، وكتابة النَّتائج والمناقشة، تركِّز المرحلة الثَّانية من المراجعة على ما إذا كان المؤلفون قد اتبعوا خطة مقترحة مسبقًا بشكلٍ كافٍ، وأبلغوا عن الانحرافات عنها دون التَّركيز على النَّتائج.  يؤدِّي هذا إلى تحويل تركيز المراجعة إلى سؤال ومنهجيَّة البحث المقترحة للدِّراسة، وبعيدًا عن الاهتمام المتصوَّر بنتائج الدِّراسة.  **المصطلحات ذات الصِّلة:** التَّسجيل المسبق، تحيُّز النَّشر (مشكلة درج الملفَّات)، مراجعة خالية من النّتائج ، منظمةالأقران، مراسم البحث.",
                "Related_terms": "** Preregistration; Publication bias (File Drawer Problem); Results-free review; PCI (Peer Community In); Research Protocol"
            },
            {
                "Title": "Registry of Research Data Repositories (سجّل مستودعات بيانات البحث) *",
                "Definition": "** A global registry of research data repositories from different academic disciplines. It includes repositories that enable permanent storage of, description via metadata and access to, data sets by researchers, funding bodies, publishers, and scholarly institutions.  **",
                "Reference": "** [https://www.re3data.org/](https://www.re3data.org/) \\- Registry of Research Data Repositories.",
                "Originally drafted by": "** Aleksandra Lazić",
                "Reviewed (or Edited) by": "** Mahmoud Elsherif; Sam Parsons; Charlotte R. Pennington; Helena Hartmann",
                "Translated by": "** Ahmed Hakami",
                "Translation reviewed by": "** Ali H. Al-Hoorie, Hala Alghamdi, Asma Alzahrani, Mohammed Mohsen",
                "Translation": "التَّعريف:** سجل عالمي لمستودعات البيانات البحثيَّة من مختلف التَّخصُّصات الأكاديميَّة، ويشمل المستودعات التي تمكِّن من التَّخزين الدَّائم، والتَّوصيف عبر البيانات الوصفيَّة، والوصول إلى مجموعات البيانات من قبل الباحثين، وهيئات التَّمويل، والنَّاشرين، والمؤسسات العلميَّة.  **المصطلحات ذات الصِّلة:** البيانات الوصفيَّة، الوصول المفتوح، البيانات المفتوحة، المواد المفتوحة، مستودع البيانات",
                "Related_terms": "** Metadata; Open Access; Open Data; Open Material; Repository"
            },
            {
                "Title": "Reliability (الثَّبات) *",
                "Definition": "** The extent to which repeated measurements lead to the same results. In psychometrics, reliability refers to the extent to which respondents have similar scores when they take a questionnaire on multiple occasions. Noteworthy, reliability does not imply validity. Furthermore, additional types of reliability besides internal consistency exist, including: test-retest reliability, parallel forms reliability and interrater reliability.  **",
                "Reference": "** Bollen (1989); Drost (2011)",
                "Originally drafted by": "** Annalise A. LaPlume",
                "Reviewed (or Edited) by": "** Mahmoud Elsherif**;** Eduardo Garcia-Garzon; Kai Krautter; Olmo van den Akker",
                "Translated by": "** Zainab Alsuhaibani",
                "Translation reviewed by": "** Ahlam Ahmed, Ali H. Al-Hoorie, Asma Alzahrani, Mohammed Mohsen",
                "Translation": "التَّعريف:** يشير الثَّبات إلى مدى تكرار القياس، والحصول على نفس النَّتائج، وفي القياس النَّفسي يشير الثَّبات إلى مدى حصول المستجيبين على درجات مماثلة عندما يقومون بتعبئة الاستبانة في أوقات مختلفة، ومن الجدير بالذِّكر أنَّ الثَّبات لا يعني الصِّدق بالإضافة إلى ذلك، توجد أنواع أخرى من الثَّبات إلى جانب الاتسِّاق الدَّاخلي، وتشمل: ثبات إعادة الاختبار، وثبات النَّماذج المتكافئة، وثبات المقيمين/المحكمين.  **المصطلحات ذات الصِّلة:** الاتِّساق، الاتِّساق الدَّاخلي، معايير الجودة، قابلية التِّكرار، قابليَّة إعادة الإنتاج، الصِّدق.",
                "Related_terms": "** Consistency; Internal consistency; Quality Criteria; Replicability; Reproducibility; Validity"
            },
            {
                "Title": "Repeatability (التِّكرار) *",
                "Definition": "** Synonymous with *test-retest* *reliability*. It refers to the agreement between the results of successive measurements of the same measure. Repeatability requires the same experimental tools, the same observer, the same measuring instrument administered under the same conditions, the same location, repetition over a short period of time, and the same objectives (Joint Committee for Guidelines in Metrology, 2008\\)  **",
                "Reference(s)": "** ISO (1993); Stodden (2011)",
                "Drafted by": "** Mahmoud Elsherif, Adam Parker",
                "Reviewed (or Edited) by": "** Gilad Feldman; Helena Hartmann; Joanne McCuaig; Sam Parsons",
                "Translated by": "** Ahmed Hakami",
                "Translation reviewed by": "** Ahlam Ahmed, Ali H. Al-Hoorie, Asma Alzahrani, Mohammed Mohsen",
                "Translation": "التَّعريف:** مرادف لثبات إعادة الاختبار، ويشير إلى الاتِّفاق بين نتائج القياسات المتتالية لنفس المقياس، يتطلّب التِّكرار نفس الأدوات التَّجريبيَّة، ونفس المراقب، ونفس أداة القياس التي تدار في ظل نفس الظُّروف، ونفس الموقع، والتِّكرار على مدى فترة زمنيَّة قصيرة، ونفس الأهداف (Joint Committee for Guidelines in Metrology, 2008).  **المصطلحات ذات الصِّلة:** الثَّبات",
                "Related_terms": "** Reliability"
            },
            {
                "Title": "Replicability (قابليَّة التِّكرار) *",
                "Definition": "** An umbrella term, used differently across fields, covering concepts of: direct and conceptual replication, computational reproducibility/replicability, generalizability analysis and robustness analyses. Some of the definitions used previously include: a different team arriving at the same results using the original author's artifacts (Barba 2018); a study arriving at the same conclusion after collecting new data (Claerbout & Karrenbach, 1992); as well as studies for which any outcome would be considered diagnostic evidence about a claim from prior research (Nosek & Errington, 2020).  **",
                "Reference(s)": "** Barba (2018); Crüwell et al. (2019); King (1996); National Academies of Sciences et al. (2011); Nosek and Errington (2020)",
                "Drafted by": "** Mahmoud Elsherif",
                "Reviewed (or Edited) by": "** Jamie P. Cockcroft; Adrien Fillon; Gilad Feldman; Annalise A. LaPlume; Tina B. Lonsdorf; Sam Parsons; Eike Mark Rinke; Tobias Wingen",
                "Translated by": "** Mai Helmy.",
                "Translation reviewed by": "** Ahlam Ahmed, Ali H. Al-Hoorie, Hiba Alomary, Mohammed Mohsen",
                "Translation": "التَّعريف:** مصطلح شامل، يستخدم بشكل مختلف عبر المجالات، ويغطي مفاهيم: التِّكرار المباشر والمفاهيمي، إعادة الإنتاج الحسابي، وتحليل قابليَّة التَّعميم، وتحليلات القوّة، وتتضمَّن بعض التَّعريفات المستخدمة سابقًا: وصول فريق مختلف إلى نفس النَّتائج باستخدام أدوات  المؤلِّف الأصليَّة (Barba, 2018)، وصول دراسة إلى نفس النَّتيجة بعد جمع بيانات جديدة (Claerbout & Karrenbach, 1992)، بالإضافة إلى الدِّراسات التي يمكن اعتبار نتائجها دليلًا تشخيصيًا حول مصداقيَّة نتائج بحوث سابقة (Nosek & Errington, 2020).  **المصطلحات ذات الصِّلة:** التِّكرار المفاهيمي، التِّكرار المباشر ، الابليَّة للتَّعميم، قابليَّة إعادة الإنتاج، الثَّبات، المتانة (في التَّحليلات)",
                "Related_terms": "** Conceptual replication; Direct Replication; Generalizability; Reproducibility; Reliability; Robustness (analyses)"
            },
            {
                "Title": "Replication Markets (أسواق التِّكرار) *",
                "Definition": "** A replication market is an environment where users bet on the replicability of certain effects. Forecasters are incentivized to make accurate predictions and the top successful forecasters receive monetary compensation or contributorship for their bets. The rationale behind a replication market is that it leverages the collective wisdom of the scientific community to predict which effect will most likely replicate, thus encouraging researchers to channel their limited resources to replicating these effects.  **",
                "Reference": "** Liu et al. (2020); Tierney et al. (2020); Tierney et al. (2021); www.replicationmarkets.com",
                "Originally drafted by": "** Ali H. Al-Hoorie",
                "Reviewed (or Edited) by": "** Mahmoud Elsherif; Leticia Micheli; Sam Parsons",
                "Translated by": "** Mai Helmy.",
                "Translation reviewed by": "** Ali H. Al-Hoorie, Ahlam Ahmed, Hiba Alomary, Mohammed Mohsen",
                "Translation": "التَّعريف:** سوق التِّكرار هو بيئة يراهن فيها المستخدمون على إمكانيَّة تكرار تأثيرات معيَّنة. يحفّز المتنبؤن لإجراء تنبؤات دقيقة بتلقي تعويضًا ماليًا أو مساهمة معيّنة مقابل رهاناتهم في حال نجاح توقعاتهم.  المنطق وراء سوق التِّكرار هو الاعتماد على الحكمة الجماعيَّة للمجتمع العلمي للتنبؤ بالتأثير الذي من المرجح أن يتكرِّر، وبالتَّالي تشجيع الباحثين على توجيه مواردهم المحدودة لتكرار هذه التَّأثيرات.  **المصطلحات ذات الصِّلة:** علم المواطن. التَّعهيد الجماعي، قابليَّة التِّكرار، قابليَّة إعادة الإنتاج",
                "Related_terms": "** Citizen science; Crowdsourcing; Replicability; Reproducibility"
            },
            {
                "Title": "Reporting Guideline (دليل إعداد التَّقارير) *",
                "Definition": "** A reporting guideline is a “checklist, flow diagram, or structured text to guide authors in reporting a specific type of research, developed using explicit methodology.” (EQUATOR Network, n.d.). Reporting guidelines provide the minimum guidance required to ensure that research findings can be appropriately interpreted, appraised, synthesized and replicated. Their use often differs per scientific journal or publisher.  **",
                "Reference": "** Moher et al. (2009) Schulz et al. (2010); Torpor et al. (2021); Von Elm et al. (2007); [https://www.equator-network.org/about-us/what-is-a-reporting-guideline/](https://www.equator-network.org/about-us/what-is-a-reporting-guideline/)",
                "Originally drafted by": "** Aidan Cashin",
                "Reviewed (or Edited) by": "** Gilad Feldman; Helena Hartmann; Joanne McCuaig",
                "Translated by": "** Mai Helmy",
                "Translation reviewed by": "** Ali H. Al-Hoorie, Hala Alghamdi, Asma Alzahrani, Mohammed Mohsen",
                "Translation": "التَّعريف:** الدَّليل الإرشادي لإعداد التَّقارير هو \"قائمة مراجعة، أو مخطَّط، أو نص منظَّم لتوجيه المؤلِّفين في طريقة كتابة التقرير عن نوع معيَّن من البحث. تم تطويره باستخدام منهجيَّة واضحة\" (EQUATOR Network, n.d.). وتوفِّر إرشادات إعداد التَّقارير الحد الأدنى من التَّوجيه المطلوب لضمان إمكانيَّة تفسير نتائج البحث وتقييمها وتوليفها وتكرارها بشكل مناسب. وغالبَا ما يختلف استخدامها باختلاف المجلَّة العلميَّة، أو النَّاشر.  **المصطلحات ذات الصِّلة:** موقع CONSORT ، المراجعات المنهجيَّة غير التَّدخليَّة المفتوحة والقابلة للتِّكرار ،PRISMA ،STROBE",
                "Related_terms": "** CONSORT; Non-Intervention, Reproducible, and Open Systematic Reviews (NIRO-SR); PRISMA; STROBE"
            },
            {
                "Title": "Repository (مستودع البيانات) *",
                "Definition": "** An online archive for the storage of digital objects including research outputs, manuscripts, analysis code and/or data. Examples include preprint servers such as bioRxiv, MetaArXiv, PsyArXiv, institutional research repositories, as well as data repositories that collect and store datasets including zenodo.org, PsychData, and code repositories such as Github, or more general repositories for all kinds of research data, such as the Open Science Framework (OSF). Digital objects stored in repositories are typically described through metadata which enables discovery across different storage locations.  **",
                "Reference(s)": "** [https://www.nature.com/sdata/policies/repositories](https://www.nature.com/sdata/policies/repositories)",
                "Originally drafted by": "** Tina Lonsdorf",
                "Reviewed (or Edited) by": "** Gilad Feldman; Connor Keating; Mariella Paul; Charlotte R. Pennington; Flávio Azevedo",
                "Translated by": "** Mai Helmy.",
                "Translation reviewed by": "** Ali H. Al-Hoorie, Ahlam Ahmed, Hala Alghamdi, Mohammed Mohsen",
                "Translation": "التَّعريف:** أرشيف على الإنترنت لتخزين المواد الرَّقميَّة بما في ذلك مخرجات البحث والُّنصوص،  والنَّص البرمجي للتَّحليل والبيانات.  وتتضمَّن الأمثلة خوادم ما قبل الطِّباعة مثل bioRxiv و MetaArXiv و PsyArXiv ومستودعات البحث المؤسسيَّة، بالإضافة إلى مستودعات البيانات التي تجمع، وتخزِّن مجموعات البيانات بما في ذلك zenodo.org و PsychData ومستودعات النُّصوص البرمجيَّة  مثل Github أو مستودعات أكثر عموميَّة لجميع أنواع بيانات البحث، مثل إطار العلوم المفتوحة، ويتم عادةً وصف الكائنات الرَّقميَّة المخزَّنة في المستودعات من خلال البيانات التَّعريفيَّة التي يتاح اكتشافها عبر مواقع التَّخزين المختلفة.  **المصطلحات ذات الصِّلة:** مشاركة البيانات، Github البيانات الوصفيَّة، الوصول المفتوح، البيانات المفتوحة، المواد المفتوحة ، إطار العلوم المفتوحة، المصدر المفتوح، نسخة أوليَّة",
                "Related_terms": "** Data sharing; Github; Metadata; Open Access; Open data; Open Material; Open Science Framework; Open Source; Preprint"
            },
            {
                "Title": "ReproducibiliTea (نوادي الأبحاث التِّكراريَّة) *",
                "Definition": "A grassroots initiative that helps researchers create local journal clubs at their universities to discuss a range of topics relating to open research and scholarship. Each meeting usually centres around a specific paper that discusses, for example, reproducibility, research practice, research quality, social justice and inclusion, and ideas for improving science.  **",
                "Reference": "** [https://reproducibilitea.org/](https://reproducibilitea.org/); Orben (2019)",
                "Originally drafted by": "** Emma Norris",
                "Reviewed (or Edited) by": "** Mahmoud Elsherif; Gilad Feldman; Connor Keating; Charlotte R. Pennington; Sam Parsons; Flávio Azevedo",
                "Translated by": "** Asma Alzahrani",
                "Translation reviewed by": "** Ali H. Al-Hoorie, Ahlam Ahmed, Mohammed Mohsen",
                "Translation": "التَّعريف:** هي مبادرة شعبيَّة تساعد الباحثين على إنشاء نوادي محليَّة في الجامعات؛ لمناقشة مجموعة من الموضوعات المتعلِّقة بالبحوث، والمعرفة المفتوحة.  عادة ما يتمحور كل اجتماع حول مناقشة ورقة علميَّة محدَّدة، مثل: إعادة الإنتاج، وممارسة البحث،وجودته، والعدالة الاجتماعيَّة، والشُّموليَّة، والأفكار المتعلِّقة بتحسين العلوم. **المصطلحات ذات الصِّلة:** مبادرة شعبيَّة، نادي المجلَّة، العلم المفتوح، قابليَّة إعادة الإنتاج",
                "Related_terms": "** Grassroots initiative; Journal club; Open science; Reproducibility"
            },
            {
                "Title": "Reproducibility (قابليَّة إعادة الإنتاج) *",
                "Definition": "** A minimum standard on a spectrum of activities (\"reproducibility spectrum\") for assessing the value or accuracy of scientific claims based on the original methods, data, and code. For instance, where the original researcher's data and computer codes are used to regenerate the results (Barba, 2018), often referred to as computational reproducibility. Reproducibility does not guarantee the quality, correctness, or validity of the published results (Peng, 2011). In some fields, this meaning is, instead, associated with the term “replicability” or ‘repeatability’.  **",
                "Reference(s)": "** Barba (2018); Cruwell et al. (2019); Peng (2011), Stodden (2011); Syed (2019); National Academies of Sciences, Engineering, and Medicine (2019)",
                "Drafted by": "** Mahmoud Elsherif",
                "Reviewed (or Edited) by": "** Helena Hartmann; Annalise A. LaPlume; Tina B. Lonsdorf; Sam Parsons; Charlotte R. Pennington; Suzanne L. K. Stewart",
                "Translated by": "** Hala Alghamdi",
                "Translation reviewed by": "** Ahlam Ahmed, Ali H. Al-Hoorie, Asma Alzahrani, Mohammed Mohsen",
                "Translation": "التَّعريف:** يشير إلى الحد الأدنى من نطاق قابليَّة إعادة الإنتاج؛ لتقييم قيمة، أو دقّة الادِّعاءات العلميَّة بناءً على طرق البحث، والبيانات، والتَّعليمات البرمجيَّة الأصليَّة.  يشتمل ذلك على سبيل المثال حالات استخدام البيانات، والتَّعليمات البرمجيَّة للباحث الأصلي؛ لغرض إعادة إنتاج النَّتائج، والتي يشار إليها غالبًا بقابليَّة إعادة الإنتاج الحسابي.  لا تضمن قابليَّة إعادة الإنتاج جودة، أو دقّة، أو صدق النَّتائج المنشورة (Peng, 2011). و في بعض المجالات، يتم ربط هذه الفكرة بمصطلح \"قابليَّة التِّكرار\" أو \"قابليَّة الإعادة\".  **المصطلحات ذات الصِّلة:** الاستنساخ الحسابي، قابليّة التِّكرار، التِّكرار.",
                "Related_terms": "** Computational reproducibility; Replicability; repeatability"
            },
            {
                "Title": "Reproducibility crisis (aka Replicability or replication crisis) (أزمة إعادة الإنتاج (أو أزمة التِّكرار) ) *",
                "Definition": "** The finding, and related shift in academic culture and thinking, that a large proportion of scientific studies published across disciplines do not replicate (e.g. Open Science Collaboration, 2015). This is considered to be due to a lack of quality and integrity of research and publication practices, such as publication bias, QRPs and a lack of transparency, leading to an inflated rate of false positive results. Others have described this process as a ‘Credibility revolution’ towards improving these practices.  **",
                "Reference(s)": "** Fanelli (2018); Open Science Collaboration (2015)",
                "Drafted by": "** Mahmoud Elsherif",
                "Reviewed (or Edited) by": "** Helena Hartmann; Annalise A. LaPlume; Mariella Paul; Sonia Rishi; Lisa Spitzer",
                "Translated by": "** Ahlam Ahmed",
                "Translation reviewed by": "** Ali H. Al-Hoorie, Hala Alghamdi, Asma Alzahrani, Mohammed Mohsen",
                "Translation": "التَّعريف:** هي اكتشاف، وما تتعلق بها من تحوُّل في التَّفكير، والثَّقافة الأكاديميَّة، إنَّ نسبة كبيرة من الدِّراسات العلميَّة المنشورة في مختلف التَّخصُّصات غير قابلة للتِّكرار (Open Science Collaboration, 2015). ويعزى ذلك للافتقار إلى النَّزاهة، ونقص جودة وسلامة ممارسات البحث والنَّشر- مثل تحيُّز النَّشر- وممارسات البحث المشكوك فيها، والافتقار إلى الشَّفافيَّة؛ ممَّا يؤدِّي إلى تضخم معدَّل النّتائج الإيجابيَّة الخاطئة. ووصف آخرون هذه العمليَّة بأنَّها \"ثورة المصداقيَّة\" نحو تحسين هذه الممارسات.  **المصطلحات ذات الصِّلة**: أزمة المصداقيَّة، تحيُّز النَّشر (مشكلة درج الملفَّات) ، ممارسات البحث المشكوك فيها،أو ممارسات إعداد التَّقارير المشكوك فيها، قابليَّة التِّكرار، قابليَّة إعادة الإنتاج.",
                "Related_terms": "** Credibility crisis; Publication bias (File Drawer Problem); Questionable Research Practices or Questionable Reporting Practices (QRPs); Replicability; Reproducibility"
            },
            {
                "Title": "Reproducibility Network (شبكة التكرار) *",
                "Definition": "** A reproducibility network is a consortium of open research working groups, often peer-led. The groups operate on a wheel-and-spoke model across a particular country, in which the network connects local cross-disciplinary researchers, groups, and institutions with a central steering group, who also connect with external stakeholders in the research ecosystem. The goals of reproducibility networks include; advocating for greater awareness, promoting training activities, and disseminating best-practices at grassroots, institutional, and research ecosystem levels. Such networks exist in the UK, Germany, Switzerland, Slovakia, and Australia (as of March 2021).  **",
                "Reference": "** [https://www.ukrn.org/](https://www.ukrn.org/) ; [https://reproducibilitynetwork.de/](https://reproducibilitynetwork.de/); [https://www.swissrn.org/](https://www.swissrn.org/); [https://slovakrn.wixsite.com/skrn](https://slovakrn.wixsite.com/skrn); [https://www.aus-rn.org/](https://www.aus-rn.org/)",
                "Originally drafted by": "** Suzanne L. K. Stewart",
                "Reviewed (or Edited) by": "** Annalise A. LaPlume; Sam Parsons; Flávio Azevedo",
                "Translated by": "** Mai Helmy.",
                "Translation reviewed by": "** Ahlam Ahmed, Ali H. Al-Hoorie, Asma Alzahrani, Mohammed Mohsen",
                "Translation": "التعريف:** شبكة التِّكرار عبارة عن اتِّحاد يتكَّون من مجموعات تهتم بالبحوث المفتوحة، غالبًا ما يقودها الأقران. وتتبع هذه المجموعات نموذج العجلة، والشُّعاع في بلد معين، حيث تقوم الشَّبكة بربط الباحثين، والمجموعات والمؤسَّسات المحليَّة في تخصُّصات مختلفة بمجموعة توجيهيَّة مركزيَّة، والتي تتواصل أيضًا مع أصحاب المصلحة الخارجيين.  وتشمل أهداف شبكات التِّكرار الدَّعوة إلى مزيد من الوعي، وتعزيز أنشطة التَّدريب، ونشر أفضل الممارسات على المستوى الشَّعبي، والمؤسّسي، والبحثي، وتوجد مثل هذه الشَّبكات في المملكة المتَّحدة وألمانيا وسويسرا وسلوفاكيا وأستراليا (اعتبارًا من مارس 2021)."
            },
            {
                "Title": "Research Contribution Metric (p) (مقياس المساهمة البحثيَّة (نشر)) *",
                "Definition": "** Type of semantometric measure assessing similarity of publications connected in a citation network. This method uses a simple formula to assess authors’ contributions. Publication *p* can be estimated based on the semantic distance from the publications cited by *p* to publications citing *p*.  **",
                "Reference": "** Knoth and Herrmannova (2014); Holcombe (2019); Larivière et al. (2016)",
                "Originally drafted by": "** Alaa AlDoh",
                "Reviewed (or Edited) by": "** Michele C. Lim; Jamie P. Cockcroft; Micah Vandegrift; Dominik Kiersz",
                "Translated by": "** Mai Helmy.",
                "Translation reviewed by": "** Ali H. Al-Hoorie, Ahlam Ahmed , Asma Alzahrani, Mohammed Mohsen",
                "Translation": "التَّعريف:** طريقة للقياس الدّلالي الذي يقيّم تشابه المنشورات المرتبطة  بشبكة الاستشهاد، وتستخدم هذه الطَّريقة صيغة  بسيطة لتقييم مساهمات المؤلفين، فيمكن تقييمها على أساس المسافة الدّلالية من المنشورات التي استشهد بها إلى المنشورات المستشهَد بِها.  **المصطلحات ذات الصِّلة:** القياسات الدّلاليَّة",
                "Related_terms": "** Semantometrics"
            },
            {
                "Title": "Research Cycle (دورة البحث) *",
                "Definition": "** Describes the circular process of conducting scientific research, with “researchers working at various stages of inquiry, from more tentative and exploratory investigations to the testing of more definitive and well-supported claims” (Lieberman, 2020, p. 42). The cycle includes literature research and hypothesis generation, data collection and analysis, as well as dissemination of results (e.g. through publication in peer-reviewed journals), which again informs theory and new hypotheses/research.  **",
                "Reference(s)": "** Bramoullé and Saint Paul (2010); Lieberman (2020)",
                "Originally drafted by": "** Helena Hartmann",
                "Reviewed (or Edited) by": "** Jamie P. Cockcroft; Aleksandra Lazić; Graham Reid; Beatrice Valentini",
                "Translated by": "** Ahmed Hakami",
                "Translation reviewed by": "** Ali H. Al-Hoorie, Ahlam Ahmed, Asma Alzahrani, Mohammed Mohsen",
                "Translation": "التَّعريف:** هي العمليَّة الدَّائريَّة لإجراء البحث العلمي حيث يعمل \"الباحثون في مراحل مختلفة في البحث، ابتداءً بالبحوث الأوليَّة، والاستكشافيَّة إلى اختبار الادِّعاءات الأكثر تحديدًا والمدعومة جيدًا\" (Lieberman, 2020, p. 42). وتشمل الدَّورة: البحث في الأدبيَّات، وتوليد الفرضيَّات، وجمع البيانات وتحليلها، بالإضافة إلى نشر النَّتائج (على سبيل المثال من خلال النَّشر في المجلَّات المحكّمة)، والتي بدورها تغذي النَّظريَّة والفرضيَّات والبحوث الجديدة.  **المصطلحات ذات الصِّلة:** عمليَّة البحث",
                "Related_terms": "** Research process"
            },
            {
                "Title": "Research Data Management (إدارة بيانات البحث) *",
                "Definition": "** Research Data Management (RDM) is a broad concept that includes processes undertaken to create organized, documented, accessible, and reusable quality research data. Adequate research data management provides many benefits including, but not limited to, reduced likelihood of data loss, greater visibility and collaborations due to data sharing, demonstration of research integrity and accountability. **",
                "Reference(s)": "** CESSDA; Corti et al. (2019)",
                "Drafted by": "** Micah Vandegrift",
                "Reviewed (or Edited) by": "** Helena Hartmann; Tina B. Lonsdorf; Catia M. Oliveira; Julia Wolska",
                "Translated by": "** Ruwayshid",
                "Translation reviewed by": "** Ali H. Al-Hoorie, Ahlam Ahmed, Asma Alzahrani, Mohammed Mohsen",
                "Translation": "التَّعريف:**  إدارة بيانات البحث هو مفهوم واسع يتضمَّن العمليَّات التي يتم إجراؤها لإنشاء بيانات بحثيَّة عالية الجودة، ومنظَّمة، وموثّقة ويمكن الوصول إليها، وقابلة لإعادة الاستخدام. توفِّر الإدارة الملائمة لبيانات البحث عدَّة مزايا بما في ذلك، على سبيل المثال لا الحصر: تقليل احتماليَّة فقدان البيانات، زيادة الشَّفافيَّة، والتَّعاون بسبب مشاركة البيانات، وإظهار النَّزاهة والمسؤوليَّة. **المصطلحات ذات الصِّلة:** تنظيم  البيانات، توثيق البيانات، خطة إدارة البيانات، مشاركة البيانات، البيانات الوصفيَّة، إدارة بيانات البحث",
                "Related_terms": "** Data curation; Data documentation; Data management plan (DMP); Data sharing; Metadata; Research data management"
            },
            {
                "Title": "Research integrity (نزاهة البحث) *",
                "Definition": "** Research integrity is defined by a set of good research practices based on fundamental principles: honesty, reliability, respect and accountability (ALLEA, 2017). Good research practices —which are based on fundamental principles of research integrity and should guide researchers in their work as well as in their engagement with the practical, ethical and intellectual challenges inherent in research— refer to areas such as: research environment (e.g., research institutions and organisations promote awareness and ensure a prevailing culture of research integrity), training, supervision and mentoring (e.g., Research institutions and organisations develop appropriate and adequate training in ethics and research integrity to ensure that all concerned are made aware of the relevant codes and regulations), research procedures (e.g., researchers report their results in a way that is compatible with the standards of the discipline and, where applicable, can be verified and reproduced), safeguards (e.g., researchers have due regard for the health, safety and welfare of the community, of collaborators and others connected with their research), data practices and management (e.g., researchers, research institutions and organisations provide transparency about how to access or make use of their data and research materials), collaborative working, publication and dissemination (e.g., authors and publishers consider negative results to be as valid as positive findings for publication and dissemination), reviewing, evaluating and editing (e.g., researchers review and evaluate submissions for publication, funding, appointment, promotion or reward in a transparent and justifiable manner).  **",
                "Reference(s)": "** ALLEA (2017); Medin (2012); Moher et al. (2020)",
                "Drafted by": "** Ana Barbosa Mendes; Flávio Azevedo",
                "Reviewed (or Edited) by": "** Valeria Agostini; Bradley Baker; Gilad Feldman; Tamara Kalandadze; Charlotte R. Pennington",
                "Translated by": "** Mai Helmy.",
                "Translation reviewed by": "** Ali H. Al-Hoorie, Ahlam Ahmed, Asma Alzahrani, Mohammed Mohsen",
                "Translation": "التَّعريف:** يتم تعريف نزاهة البحث من خلال مجموعة من الممارسات البحثيَّة الجيِّدة القائمة على المبادئ الأساسيَّة: الصِّدق، والموثوقيَّة، والاحترام، ، والمساءلة (ALLEA, 2017).  تشير ممارسات البحث الجيدة  \\_التي تعتمد على المبادئ الأساسيَّة لنزاهة البحث، والتي يجب أن توجه الباحثين في عملهم، وكذلك في تفاعلهم مع التَّحديّات العمليَّة، والأخلاقيَّة، والفكريَّة الكامنة  في البحث\\_  إلى مجالات مثل: بيئة البحث، على سبيل المثال: تعمل المؤسَّسات والمنظمات البحثيَّة على تعزيز الوعي، وضمان وجود ثقافة سائدة لنزاهة البحث، والتَّدريب والإشراف والتَّوجيه، مثلًا عندما تقوم المؤسَّسات، والمنظَّمات البحثيَّة بتطوير التَّدريب المناسب، والكافي في مجال الأخلاقيَّات، ونزاهة البحث؛ لضمان توعية جميع المعنيين بالقوانين والقواعد واللّوائح ذات الصِّلة، وإجراءات البحث، ومثال ذلك عندما يقوم الباحثون بالإبلاغ عن نتائجهم بطريقة متوافقة مع معايير التَّخصُّص، وعند الحاجة يمكن التَّحقُّق منها وإعادة إنتاجها، والضَّمانات (كأن يولي الباحثون الاعتبار الواجب للصحة والسلامة والبيئة) ورفاهية المجتمع والمتعاونين وغيرهم ممن لهم صلة ببحوثهم )، وممارسات البيانات وإدارتها  (كأن يوفر الباحثون والمؤسسات البحثيَّة والمنظمات الشَّفافيَّة حول كيفيَّة الوصول إلى البيانات، والمواد البحثيَّة الخاصّة بهم، أو الاستفادة منها)، والعمل التَّعاوني والنَّشر (على سبيل المثال: يعد المؤلفون والناشرون النَّتائج السَّلبيَّة صالحة للنَّشر، والتَّوزيع مثل النَّتائج الإيجابيَّة) والمراجعة والتَّقييم والتَّحرير (كأن يقوم الباحثون بمراجعة وتقييم الطَّلبات المقدَّمة للنَّشر، أو التَّمويل، أو التَّعيين، أو التَّرقية، أو المكافأة بطريقة شفافّة ومبررة).  **المصطلحات ذات الصِّلة:** مصداقيَّة الادِّعاءات العلميَّة، اكتشاف الأخطاء، أخلاق مهنيَّة، البحث المفتوح، ممارسات البحث المشكوك فيها، أو ممارسات إعداد التَّقارير المشكوك فيها (QRPs) ، ممارسات البحث المسؤولة ، الدِّقة، الشَّفافيَّة ، بحث جدير بالثِّقة.",
                "Related_terms": "** Credibility of scientific claims; Error detection; Ethics; Open research; Questionable Research Practices or Questionable Reporting Practices (QRPs); Responsible Research Practices; Rigour; Transparency; Trustworthy research"
            },
            {
                "Title": "Research Protocol (مراسم البحث) *",
                "Definition": "** A detailed document prepared before conducting a study, often written as part of ethics and funding applications. The protocol should include information relating to the background, rationale and aims of the study, as well as hypotheses which reflect the researchers’ expectations. The protocol should also provide a “recipe” for conducting the study, including methodological details and clear analysis plans. Best practice guidelines for creating a study protocol should be used for specific methodologies and fields. It is possible to publically share research protocols to attract new collaborators or facilitate efficient collaboration across labs (e.g. [https://www.protocols.io/](https://www.protocols.io/)). In medical and educational fields, protocols are often a separate article type suitable for publication in journals. Where protocol sharing or publication is not common practice, researchers can choose preregistration.  **",
                "Reference": "** BMJ (2015); Nosek et al. (2018)",
                "Originally drafted by": "** Marta Topor",
                "Reviewed (or Edited) by": "** Helena Hartmann; Bethan Iley; Annalise A. LaPlume; Charlotte Pennington",
                "Translated by": "** Zainab Alsuhaibani",
                "Translation reviewed by": "Ali H. Al-Hoorie, Ahlam Ahmed, Asma Alzahrani, Mohammed Mohsen",
                "Translation": "التَّعريف**: وثيقة مفصَّلة يتم إعدادها قبل إجراء الدِّراسة، وغالبًا ما تتم كتابتها كجزء من التَّقديم على الأخلاقيَّات وطلبات التَّمويل البحثي.  يجب أن تتضمَّن المراسم معلومات تتعلق بخلفية الدِّراسة، ومبرِّراتها، وأهدافها، بالإضافة إلى الفرضيَّات التي تعكس توقُّعات الباحثين.  ويجب أن تقدِّم المراسم أيضًا \"وصفة\" لإجراء الدِّراسة، بما في ذلك التَّفاصيل المنهجيَّة، وخطط واضحة للتَّحليل، ويجب  استخدام أدلة أفضل الممارسات لإنشاء مراسم الدِّراسة لمنهجيَّات، ومجالات محدَّدة. ومن الممكن مشاركة مراسم البحث علنًا لجذب متعاونين جدد أو تسهيل التَّعاون الفعّال عبر المختبرات (مثل https://www.protocols.io/). وفي المجالات الطبيَّة والتَّعليميَّة، غالبًا ما تكون المراسم نوعًا منفصلًا من المقالات المناسبة للنَّشر في المجلّات.  ويمكن للباحثين اختيار التَّسجيل المسبق عندما لا تكون مشاركة المراسم، أو النَّشر ممارسة شائعة.  **المصطلحات ذات الصِّلة**: المعامل المتعدِّدة ، التَّسجيل المسبق.",
                "Related_terms": "** Many Labs; Preregistration"
            },
            {
                "Title": "Research workflow (مسار البحث العلميّ ) *",
                "Definition": "** The process of conducting research from conceptualisation to dissemination. A typical workflow may look like the following: Starting with conceptualisation to identify a research question and design a study. After study design, researchers need to gain ethical approval (if necessary) and may decide to preregister the final version. Researchers then collect and analyse their data. Finally, the process ends with dissemination; moving between pre-print and post-print stages as the manuscript is submitted to a journal.  **",
                "Reference(s)": "** Kathawalla et al. (2021); Stodden (2011)",
                "Drafted by": "** James E Bartlett",
                "Reviewed (or Edited) by": "** Gilad Feldman; Helena Hartmann; Aleksandra Lazić; Joanne McCuaig; Timo Roettger; Sam Parsons; Steven Verheyen",
                "Translated by": "** Sarah Almutairi",
                "Translation reviewed by": "** Ali H. Al-Hoorie, Ahlam Ahmed, Asma Alzahrani, Mohammed Mohsen",
                "Translation": "التَّعريف:** يشير هذا المصطلح إلى العمليَّة التي يبنى عليها البحث العلمي بدءًا من صياغة فكرة البحث إلى نشره، وعادة ما يكون سير العمل النّموذجي على النَّحو التَّالي: البدء بتصُّور المفاهيم لتحديد سؤال البحث، وتصميم الدِّراسة، وعند الانتهاء من تصميم البحث فإنَّه يتوجَّب على الباحث الحصول على الموافقة الأخلاقيَّة (إذا لزم الأمر) ويمكن أن يتم تسجيل نسخة نهائيَّة من الخطة البحثيَّة قبل البدء بالبحث. يقوم الباحث بعد ذلك بجمع البيانات وتحليلها وصولًا لآخر مرحلة وهي مرحلة نشر البحث، والتي ينتقل فيها الباحث بين مرحلتي ما قبل وما بعد الطِّباعة عند تسليمه البحث لمجلة علميَّة.  **المصطلحات ذات الصِّلة:** مسار البحث العلمي المفتوح، دورة البحث، خط الإنتاج البحثي.",
                "Related_terms": "** Open Research Workflow; Research cycle; Research pipeline"
            },
            {
                "Title": "Researcher degrees of freedom (درجات حرِّيَّة الباحث) *",
                "Definition": "** refers to the flexibility often inherent in the scientific process, from hypothesis generation, designing and conducting a research study to processing the data and analyzing as well as interpreting and reporting results. Due to a lack of precisely defined theories and/or empirical evidence, multiple decisions are often equally justifiable. The term is sometimes used to refer to the opportunistic (ab-)use of this flexibility aiming to achieve desired results —e.g., when in- or excluding certain data— albeit the fact that technically the term is not inherently value-laden.  **",
                "Reference": "** Gelman and Loken (2013); Simmons et al. (2011); Wicherts et al. (2016)",
                "Originally drafted by": "** Tina Lonsdorf",
                "Reviewed (or Edited) by": "** Gilad Feldman; Helena Hartmann; Timo Roettger; Robbie C.M. van Aert; Flávio Azevedo",
                "Translated by": "** Mai Helmy.",
                "Translation reviewed by": "** Ahlam Ahmed, Ali H. Al-Hoorie, Mohammed Mohsen",
                "Translation": "التَّعريف**: يشير إلى المرونة الكامنة في كثير من الأحيان في العمليَّة العلميَّة، بدءً من إنشاء الفرضيَّات، وتصميم وإجراء دراسة بحثيَّة إلى معالجة البيانات وتحليلها وكذلك تفسير النَّتائج والإبلاغ عنها. ونظرًا لعدم وجود نظريَّات محدَّدة بدقّة، أو أدلَّة تجريبيَّة، غالبًا ما تكون القرارات المتعدِّدة مبرَّرة بنفس القدر، وبشكل متساوٍ. ويستخدم المصطلح أحيانًا للإشارة إلى الاستخدام الانتهازي، أو السّيء لهذه المرونة بهدف تحقيق النَّتائج المرجوّة \\-على سبيل المثال عند إدراج بيانات معيّنة أو استبعادها \\- على الرُّغم من حقيقة أنَّ المصطلح من النَّاحية الفنيَّة ليس محملًا بهذا المعنى بطبيعته.  **المصطلحات ذات الصِّلة:** المرونة التَّحليليَّة، حديقة المسارات المتشعبِّة، نموذج عدم اليقين، تحليل الأكاون المتعدِّدة، قرصنة القيمة الاحتماليَّة،  المتانة (في التحليلات)، تحليل منحنى المواصفات",
                "Related_terms": "** Analytic Flexibility; Garden of forking paths; Model uncertainty; Multiverse analysis; *P*\\-hacking; Robustness (analyses); Specification curve analysis"
            },
            {
                "Title": "RepliCATs project (مشروع التِّكرار) *",
                "Definition": "** Collaborative Assessment for Trustworthy Science. The repliCATS project’s aim is to crowdsource predictions about the reliability and replicability of published research in eight social science fields: business research, criminology, economics, education, political science, psychology, public administration, and sociology.  **",
                "Reference": "** Fraser et al.(2021); [https://replicats.research.unimelb.edu.au/](https://replicats.research.unimelb.edu.au/)",
                "Originally drafted by": "** Tamara Kalandadze",
                "Reviewed (or Edited) by": "** Sarah Ashcroft-Jones; Mahmoud Elsherif; Gilad Feldman; Helena Hartmann; Charlotte R. Pennington",
                "Translated by": "** Mai Helmy.",
                "Translation reviewed by": "** Ahlam Ahmed, Ali H. Al-Hoorie, Mohammed Mohsen",
                "Translation": "التَّعريف**: يشير إلى مشروع \"التَّقييم التَّعاوني للعلوم الجديرة بالثقة\". ويهدف المشروع إلى جمع التنبؤات حول موثوقيَّة البحوث المنشورة، وإمكانيَّة تكرارها في ثمانية مجالات من مجالات العلوم الاجتماعيَّة وهي: بحوث الأعمال، وعلم الجريمة، والاقتصاد، والتَّعليم، والعلوم السِّياسيَّة، وعلم النَّفس، والإدارة العامّة، وعلم الاجتماع.  **المصطلحات ذات الصِّلة:** قابليَّة التِّكرار ، الجدارة بالثِّقة.",
                "Related_terms": "** Replicability; Trustworthiness"
            },
            {
                "Title": "Responsible Research and Innovation (البحث والابتكار المسؤول) *",
                "Definition": "** An approach that considers societal implications and expectations, relating to research and innovation, with the aim to foster inclusivity and sustainability. It accounts for the fact that scientific endeavours are not isolated from their wider effects and that research is motivated by factors beyond the pursuit of knowledge. As such, many parties are important in fostering responsible research, including funding bodies, research teams, stakeholders, activists, and members of the public.  **",
                "Reference(s)": "** European Commission (2021)",
                "Drafted by": "** Ana Barbosa Mendes",
                "Reviewed (or Edited) by": "** Helena Hartmann; Joanne McCuaig; Sam Parsons; Graham Reid",
                "Translated by": "** Ahmed Hakami",
                "Translation reviewed by": "** Ahlam Ahmed, Ali H. Al-Hoorie, Mohammed Mohsen",
                "Translation": "التَّعريف**: هو نهج يأخذ في الاعتبار الآثار، والتَّوقّعات المجتمعيَّة المتعلِّقة بالبحث والابتكار، بهدف تعزيز الشُّموليَّة والاستدامة.  وهو يفسر حقيقة أنَّ المساعي العلميَّة ليست معزولة عن آثارها الأوسع نطاقًا، وأنَّ البحث يحفّزه عوامل تتجاوز السَّعي وراء المعرفة، وعليه فإنَّ العديد من الأطراف مهمَّة في تعزيز البحوث المسؤولة، بما في ذلك هيئات التَّمويل، وفرق البحث، وأصحاب المصلحة، والنَّاشطين، والجمهور.  **المصطلحات ذات الصِّلة:**  علم المواطن، المشاركة العامة،  البحوث متعدِّدة التَّخصُّصات.",
                "Related_terms": "** Citizen Science; Public Engagement; Transdisciplinary Research"
            },
            {
                "Title": "Reverse p-hacking (القرصنة العكسيَّة للقيمة الاحتماليَّة)",
                "Definition": "** Exploiting researcher degrees of freedom during statistical analysis in order to increase the likelihood of accepting the null hypothesis (for instance, *p* \\> .05).  **",
                "Reference": "** Chuard et al. (2019)",
                "Originally drafted by": "** Robert M. Ross",
                "Reviewed (or Edited) by": "** Mahmoud Elsherif; Alexander Hart; Sam Parsons; Timo Roettger",
                "Translated by": "** Mai Helmy.",
                "Translation reviewed by": "** Ahlam Ahmed, Ali H. Al-Hoorie, Mohammed Mohsen",
                "Translation": "التَّعريف:** استغلال درجات حريَّة الباحث أثناء التَّحليل الإحصائي من أجل زيادة احتماليَّة قبول الفرضيَّة الصِّفريَّة (ككون القيمة الاحتماليَّة أكبر من 0.05).  **المصطلحات ذات الصِّلة**: المرونة التَّحليليَّة، الافتراض بعد معرفة النَّتائج، ممارسات البحث المشكوك فيهاأو ممارسات إعداد التَّقارير المشكوك فيها، درجات حريَّة الباحث، التَّقارير الانتقائيَّة.",
                "Related_terms": "** Analytic flexibility; HARKing; P-hacking; Questionable Research Practices or Questionable Reporting Practices (QRPs); Researcher degrees of freedom; Selective reporting"
            },
            {
                "Title": "RIOT Science Club (منتدى العلوم المتعدِّد القابلة للتِّكرار، والقابلة للتَّفسير، والمفتوحة، والشَّفافة نادي رايوت للعلوم) *",
                "Definition": "** The RIOT Science Club is a multi-site seminar series that raises awareness and provides training in Reproducible, Interpretable, Open & Transparent science practices. It provides regular talks, workshops and conferences, all of which are openly available and rewatchable on the respective location’s websites and Youtube.  **",
                "Reference": "** [http://riotscience.co.uk/](http://riotscience.co.uk/)",
                "Originally drafted by": "** Tamara Kalandadze",
                "Reviewed (or Edited) by": "** Helena Hartmann; Emma Henderson; Joanne McCuaig; Flávio Azevedo",
                "Translated by": "** Mai Helmy.",
                "Translation reviewed by": "** Ahlam Ahmed, Ali H. Al-Hoorie, Mohammed Mohsen",
                "Translation": "التَّعريف:** هو عبارة عن سلسلة ندوات متعدِّدة المواقع تعمل على زيادة الوعي، وتوفير التَّدريب على الممارسات العلميَّة القابلة للتِّكرار، والتَّفسير، والمفتوحة، والشَّفافة، وهو يوفِّر محاضرات وورش عمل ومؤتمرات منتظمة، وكلّها متاحة بشكل مفتوح، ويمكن إعادة مشاهدتها على مواقع الويب الخاصّة بالموقع المعني وعلى اليوتيوب.  **المصطلحات ذات الصِّلة:** الباحثون المبتدئون، القابليَّة للتَّفسير، الانفتاح، قابلية إعادة الإنتاج، الشفافية.",
                "Related_terms": "** Early career researchers (ECRs); Interpretability; Openness; Reproducibility; Transparency"
            },
            {
                "Title": "Robustness (analyses) ((المتانة (في التحليلات) *",
                "Definition": "** The persistence of support for a hypothesis under perturbations of the methodological/analytical pipeline. In other words, applying different methods/analysis pipelines to examine if the same conclusion is supported under analytical different conditions.  **",
                "Reference(s)": "** Goodman et al. (2016) (alternative); Nosek and Errington (2020)",
                "Drafted by": "** Tina Lonsdorf; Flávio Azevedo",
                "Reviewed (or Edited) by": "** Gilad Feldman; Adrien Fillon; Helena Hartmann; Timo Roettger",
                "Translated by": "** Mai Helmy.",
                "Translation reviewed by": "** Ahlam Ahmed, Ali H. Al-Hoorie, Mohammed Mohsen  ### **Looking for something else?**  We have separated the glossary project across several documents, see links below:  Landing page:\t\t\t[Glossary | Phase 2 | Landing page](https://docs.google.com/document/d/1BKzztg7srUeC_2Yn0b7cMbxp_vYMDlOnEYpxg_S2hWs/edit?usp=sharing) Letters A \\- G:\t\t\t[Glossary | Phase 2 | A-F](https://docs.google.com/document/d/1ob__Alxsnx9yqeTpurEY_N_kv56c1iiZyrg0xxf0ftg/edit?usp=sharing) Letters H \\- L:\t\t\t[Glossary | Phase 2 | G-L](https://docs.google.com/document/d/1LP0cEletpNumDmYHFv3seV3gL6OLQHJ7ZDLQ7T3fzNE/edit?usp=sharing) Letters M \\- R: \t\t\t[Glossary | Phase 2 | M - R](https://docs.google.com/document/d/1FVIgUx717G3pBwI17LGS7y22nKMde-zg0I6AKgz4SeQ/edit?usp=sharing) Letters S \\- Z:\t\t\t[Glossary | Phase 2 | S - Z](https://docs.google.com/document/d/1-5CKFciwhB-WfC1DK8Sajyh8a8CgNOYvOoeZpODux8Y/edit?usp=sharing) References: \t\t\t[Glossary | Phase 2 | References](https://docs.google.com/document/d/12_F8kbnl2GP66iBkIejJeX2KnkZ13iC_jj7VVYZMxpA/edit?usp=sharing) Terms not yet defined: \t\t[Glossary | Phase 2 | Terms not yet defined](https://docs.google.com/document/d/16FGodUkNoNrBYyq2JqHJMNzYuZf-QbsigooMpB_ns_E/edit?usp=sharing) Contributors spreadsheet: \t[Glossary Phase 2 tenzing contributions \\[FORRT\\]](https://docs.google.com/spreadsheets/d/1pkvQ2-h_Hr_ZnlfmKYkuJTuYrxlqS2oSqKqoEDYlKlk/edit?usp=sharing)  [FORRT slack](https://join.slack.com/t/forrt/shared_invite/zt-alobr3z7-NOR0mTBfD1vKXn9qlOKqaQ) \\- join us\\! [FORRT email](mailto:FORRTproject@gmail.com) \\- contact us\\!  ###",
                "Translation": "التَّعريف:** استمرار دعم فرضيّة ما في ظل تغيُّر المسار المنهجي، أو التَّحليلي. وبعبارة أخرى، تطبيق طرق ومسارات تحليل مختلفة لفحص ما إذا كان نفس الاستنتاج مدعومًا في ظل ظروف تحليليَّة مختلفة.  **المصطلحات ذات الصِّلة**: المعامل المتعدِّدة، تحليل الأكوان المتعدِّدة، تحليلات الحساسيَّة، تحليل منحنى المواصفات **التَّعريف البديل:** \"تشير المتانة إلى استقرار الاستنتاجات التَّجريبيَّة للتَّغيرات في الافتراضات الأساسيَّة، أو الإجراءات التَّجريبيَّة وهذا يرتبط إلى حدٍّ ما بمفهوم التَّعميم (المعروف أيضًا باسم قابليّة النَّقل)، والذي يشير إلى استمرار التَّأثير في الإعدادات المختلفة عن إطار العمل التَّجريبي وخارجه \\[...\\] ما إذا كان تصميم الدِّراسة مشابهًا بدرجة كافية للتَّصميم الأصلي أم لا؛ ليتم اعتباره تكرارًا، أو \"اختبار المتانة\"، أو بعض الاختلافات العديدة التي تم تحديدها في التِّكرار الخالص، لا سيما في العلوم الاجتماعيَّة (كالتِّكرار المفاهيمي، والتِّكرار الكاذب)، وهو سؤال غير محسوم\" (Goodman et al., 2016).",
                "Related_terms": "** Many Labs; Multiverse analysis; Sensitivity analyses; Specification Curve Analysis **Alternative definition:** “Robustness refers to the stability of experimental conclusions to variations in either baseline assumptions or experimental procedures. It is somewhat related to the concept of generalizability (also known as transportability), which refers to the persistence of an effect in settings different from and outside of an experimental framework \\[...\\] Whether a study design is similar enough to the original to be considered a replication, a “robustness test,” or some of many variations of pure replication that have been identified, particularly in the social sciences (for example, conceptual replication, pseudoreplication), is an unsettled question” (Goodman et al., 2016)."
            }
        ]
    },
    {
        "english": [
            {
                "Title": "Abstract Bias *",
                "Definition": "** The tendency to report only significant results in the abstract, while reporting non-significant results within the main body of the manuscript (not reporting non-significant results altogether would constitute selective reporting). The consequence of abstract bias is that studies reporting non-significant results may not be captured with standard meta-analytic search procedures (which rely on information in the title, abstract and keywords) and thus biasing the results of meta-analyses.",
                "Reference": "** Duyx et al. (2019)",
                "Originally drafted by": "** Ali H. Al-Hoorie",
                "Reviewed (or Edited) by": "Mahmoud Elsherif; Bethan Iley; Sam Parsons; Gerald Vineyard; Eliza Woodward; Flávio Azevedo",
                "Translation": "** The tendency to report only significant results in the abstract, while reporting non-significant results within the main body of the manuscript (not reporting non-significant results altogether would constitute selective reporting). The consequence of abstract bias is that studies reporting non-significant results may not be captured with standard meta-analytic search procedures (which rely on information in the title, abstract and keywords) and thus biasing the results of meta-analyses.",
                "Related_terms": "** Cherry-picking; Publication bias (File Drawer Problem); Selective reporting"
            },
            {
                "Title": "Academic Impact *",
                "Definition": "** The contribution that a research output (e.g., published manuscript) makes in shifting understanding and advancing scientific theory, method, and application, across and within disciplines. Impact can also refer to the degree to which an output or research programme influences change outside of academia, e.g. societal and economic impact (cf. ESRC: https://esrc.ukri.org/research/impact-toolkit/what-is-impact/).",
                "Reference(s)": "** Anon (2021)",
                "Drafted by": "** Connor Keating",
                "Reviewed (or Edited) by": "** Myriam A. Baum; Adam Parker; Charlotte R. Pennington; Suzanne L. K. Stewart; Flávio Azevedo",
                "Translation": "** The contribution that a research output (e.g., published manuscript) makes in shifting understanding and advancing scientific theory, method, and application, across and within disciplines. Impact can also refer to the degree to which an output or research programme influences change outside of academia, e.g. societal and economic impact (cf. ESRC: https://esrc.ukri.org/research/impact-toolkit/what-is-impact/).",
                "Related_terms": "** Beneficiaries; DORA; Reach; REF"
            },
            {
                "Title": "Accessibility *",
                "Definition": "** Accessibility refers to the ease of access and re-use of materials (e.g., data, code, outputs, publications) for academic purposes, particularly the ease of access is afforded to people with a chronic illness, disability and/or neurodivergence. These groups face numerous financial, legal and/or technical barriers within research, including (but not limited to) the acquisition of appropriately formatted materials and physical access to spaces. Accessibility also encompasses structural concerns about diversity, equity, inclusion, and representation (Pownall et al., 2021). Interfaces, events and spaces should be designed with accessibility in mind to ensure full participation, such as by ensuring that web-based images are colorblind friendly and have alternative text, or by using live captions at events (Brown et al., 2018; Pollet & Bond, 2021; World Wide Web Consortium, 2021).",
                "Reference(s)": "** Brown et al. (2018); Pollet and Bond (2021); Pownall et al. (2021); Suber (2004); World Wide Web Consortium (2021)",
                "Drafted by": "** Kai Krautter",
                "Reviewed (or Edited) by": "** Valeria Agostini; Myriam A. Baum; Mahmoud Elsherif; Bethan Iley; Tamara Kalandadze; Ryan Millager; Sara Middleton; Charlotte R. Pennington; Madeleine Pownall; Robert M. Ross; Flávio Azevedo",
                "Translation": "** Accessibility refers to the ease of access and re-use of materials (e.g., data, code, outputs, publications) for academic purposes, particularly the ease of access is afforded to people with a chronic illness, disability and/or neurodivergence. These groups face numerous financial, legal and/or technical barriers within research, including (but not limited to) the acquisition of appropriately formatted materials and physical access to spaces. Accessibility also encompasses structural concerns about diversity, equity, inclusion, and representation (Pownall et al., 2021). Interfaces, events and spaces should be designed with accessibility in mind to ensure full participation, such as by ensuring that web-based images are colorblind friendly and have alternative text, or by using live captions at events (Brown et al., 2018; Pollet & Bond, 2021; World Wide Web Consortium, 2021).",
                "Related_terms": "** Availability; Data availability statements; Inclusion; Open Access; Under-representation; Universal design for learning (UDL)"
            },
            {
                "Title": "Ad hominem bias *",
                "Definition": "** From Latin meaning “to the person”; Judgment of an argument or piece of work influenced by the characteristics of the person who forwarded it, not the characteristics of the argument itself. Ad hominem bias can be negative, as when work from a competitor or target of personal animosity is viewed more critically than the quality of the work merits, or positive, as when work from a friend benefits from overly favorable evaluation.",
                "Reference(s)": "** Barnes et al. (2018); Tvina et al. (2019)",
                "Drafted by": "** Mahmoud Elsherif",
                "Reviewed (or Edited) by": "** Bradley Baker; Filip Dechterenko; Bethan Iley; Madeleine Ingham; Graham Reid",
                "Translation": "** From Latin meaning “to the person”; Judgment of an argument or piece of work influenced by the characteristics of the person who forwarded it, not the characteristics of the argument itself. Ad hominem bias can be negative, as when work from a competitor or target of personal animosity is viewed more critically than the quality of the work merits, or positive, as when work from a friend benefits from overly favorable evaluation.",
                "Related_terms": "** Peer review"
            },
            {
                "Title": "Adversarial collaboration *",
                "Definition": "** A collaboration where two or more researchers with opposing or contradictory theoretical views —and likely diverging predictions about study results— work together on one project. The aim is to minimise biases and methodological weaknesses as well as to establish a shared base of facts for which competing theories must account.",
                "Reference(s)": "** Bateman et al. (2005); Cowan et al. (2020); Kerr et al. (2018); Mellers et al. (2001); Rakow et al. (2014)",
                "Drafted by": "** Siu Kit Yeung",
                "Reviewed (or Edited) by": "** Matt Jaquiery; Aoife O’Mahony; Charlotte R. Pennington; Flávio Azevedo; Madeleine Pownall**;** Martin Vasilev",
                "Translation": "** A collaboration where two or more researchers with opposing or contradictory theoretical views —and likely diverging predictions about study results— work together on one project. The aim is to minimise biases and methodological weaknesses as well as to establish a shared base of facts for which competing theories must account.",
                "Related_terms": "** Collaboration; Many Analysts; Many Labs; Preregistration; Publication bias (File Drawer Problem)"
            },
            {
                "Title": "Adversarial (collaborative) commentary *",
                "Definition": "** A commentary in which the original authors of a work and critics of said work collaborate to draft a consensus statement. The aim is to draft a commentary that is free of ad hominem attacks and communicates a common understanding or at least identifies where both parties agree and disagree. In doing so, it provides a clear take-home message and path forward, rather than leaving the reader to decide between opposing views conveyed in separate commentaries.",
                "Reference(s)": "** Heyman et al. (2020); Rabagliati et al. (2019); Silberzahn et al. (2014)",
                "Drafted by": "** Steven Verheyen",
                "Reviewed (or Edited) by": "** Mahmoud Elsherif; Sam Parsons; Emma Henderson; Michele C. Lim; Flávio Azevedo",
                "Translation": "** A commentary in which the original authors of a work and critics of said work collaborate to draft a consensus statement. The aim is to draft a commentary that is free of ad hominem attacks and communicates a common understanding or at least identifies where both parties agree and disagree. In doing so, it provides a clear take-home message and path forward, rather than leaving the reader to decide between opposing views conveyed in separate commentaries.",
                "Related_terms": "** Adversarial collaboration; Collaborative commentary"
            },
            {
                "Title": "Affiliation bias *",
                "Definition": "** This bias occurs when one’s opinions or judgements about the quality of research are influenced by the affiliation of the author(s). When publishing manuscripts, a potential example of an affiliation bias could be when editors prefer to publish work from prestigious institutions (Tvina et al., 2019).",
                "Reference(s)": "** Tvina et al. (2019)",
                "Drafted by": "** Mahmoud Elsherif",
                "Reviewed (or Edited) by": "** Christopher Graham; Madeleine Ingham; Adam Parker; Graham Reid",
                "Translation": "** This bias occurs when one’s opinions or judgements about the quality of research are influenced by the affiliation of the author(s). When publishing manuscripts, a potential example of an affiliation bias could be when editors prefer to publish work from prestigious institutions (Tvina et al., 2019).",
                "Related_terms": "** Peer review"
            },
            {
                "Title": "Aleatoric uncertainty *",
                "Definition": "** Variability in outcomes due to unknowable or inherently random factors. The stochastic component of outcome uncertainty that cannot be reduced through additional sources of information. For example, when flipping a coin, uncertainty about whether it will land on heads or tails.",
                "Reference(s)": "** Der Kiureghian and Ditlevsen (2009)",
                "Drafted by": "** Bradley Baker",
                "Reviewed (or Edited) by": "** Nihan Albayrak-Aydemir**;** Brett Gall; Magdalena Grose-Hodge; Bethan Iley; Charlotte R. Pennington",
                "Translation": "** Variability in outcomes due to unknowable or inherently random factors. The stochastic component of outcome uncertainty that cannot be reduced through additional sources of information. For example, when flipping a coin, uncertainty about whether it will land on heads or tails.",
                "Related_terms": "** Epistemic uncertainty; Knightian uncertainty"
            },
            {
                "Title": "Altmetrics *",
                "Definition": "** Departing from traditional citation measures, altmetrics (short for “alternative metrics”) provide an assessment of the attention and broader impact of research work based on diverse sources such as social media (e.g. Twitter), digital news media, number of preprint downloads, etc. Altmetrics have been criticized in that sensational claims usually receive more attention than serious research (Ali, 2021).",
                "Reference(s)": "** Ali (2021); Galligan and Dyas-Correia (2013)",
                "Originally drafted by": "** Mirela Zaneva",
                "Reviewed (or Edited) by": "** Ali H. Al-Hoorie; Charlotte R. Pennington; Birgit Schmidt; Flávio Azevedo  ### ---",
                "Translation": "** Departing from traditional citation measures, altmetrics (short for “alternative metrics”) provide an assessment of the attention and broader impact of research work based on diverse sources such as social media (e.g. Twitter), digital news media, number of preprint downloads, etc. Altmetrics have been criticized in that sensational claims usually receive more attention than serious research (Ali, 2021).",
                "Related_terms": "** Academic impact; Alternative metrics; Bibliometrics; H-index; Impact assessment; Journal impact factor"
            },
            {
                "Title": "AMNESIA *",
                "Definition": "** AMNESIA is a free anonymization tool to remove identifying information from data. After uploading a dataset that contains personal data, the original dataset is transformed by the tool, resulting in a dataset that is anonymized regarding personal and sensitive data.",
                "Reference(s)": "** [https://amnesia.openaire.eu/](https://amnesia.openaire.eu/)",
                "Originally drafted by": "** Norbert Vanek",
                "Reviewed (or Edited) by": "** Ali H. Al-Hoorie; Myriam A. Baum; Charlotte R. Pennington",
                "Translation": "** AMNESIA is a free anonymization tool to remove identifying information from data. After uploading a dataset that contains personal data, the original dataset is transformed by the tool, resulting in a dataset that is anonymized regarding personal and sensitive data.",
                "Related_terms": "** Anonymity; Confidentiality; Research ethics"
            },
            {
                "Title": "Analytic Flexibility *",
                "Definition": "** Analytic flexibility is a type of researcher degrees of freedom (Simmons, Nelson, & Simonsohn, 2011\\) that refers specifically to the large number of choices made during data preprocessing and statistical analysis. “\\[T\\]he range of analysis outcomes across different acceptable analysis methods” (Carp, 2012, p. 1). Analytic flexibility can be problematic, as this variability in analytic strategies can translate into variability in research outcomes, particularly when several strategies are applied, but not transparently reported (Masur, 2021).",
                "Reference(s)": "** Breznau et al. (2021); Carp (2012); Jones et al. (2020); Masur (2021); Simmons et al. (2011)",
                "Originally drafted by": "** Mariella Paul",
                "Reviewed (or Edited) by": "** Adrien Fillon; Bettina M. J . Kern; Adam Parker; Charlotte R. Pennington; Flávio Azevedo",
                "Translation": "** Analytic flexibility is a type of researcher degrees of freedom (Simmons, Nelson, & Simonsohn, 2011\\) that refers specifically to the large number of choices made during data preprocessing and statistical analysis. “\\[T\\]he range of analysis outcomes across different acceptable analysis methods” (Carp, 2012, p. 1). Analytic flexibility can be problematic, as this variability in analytic strategies can translate into variability in research outcomes, particularly when several strategies are applied, but not transparently reported (Masur, 2021).",
                "Related_terms": "** Garden of forking paths; Multiverse analysis; Researcher degrees of freedom"
            },
            {
                "Title": "Anonymity *",
                "Definition": "** Anonymising data refers to removing, generalising, aggregating or distorting any information which may potentially identify participants, peer-reviewers, and authors, among others. Data should be anonymised so that participants are not personally identifiable. The most basic level of anonymisation is to replace participants’ names with pseudonyms (fake names) and remove references to specific places. Anonymity is particularly important for open data and data may not be made open for anonymity concerns. Anonymity and open data has been discussed within qualitative research which often focuses on personal experiences and opinions, and in quantitative research that includes participants from clinical populations.",
                "Reference": "** Braun and Clarke (2013)",
                "Originally drafted by": "** Claire Melia",
                "Reviewed (or Edited) by": "** Tsvetomira Dumbalska; Bethan Iley; Tamara Kalandadze; Bettina M.J. Kern; Sam Parsons; Charlotte R. Pennington; Flávio Azevedo; Madeleine Pownall; Birgit Schmidt",
                "Translation": "** Anonymising data refers to removing, generalising, aggregating or distorting any information which may potentially identify participants, peer-reviewers, and authors, among others. Data should be anonymised so that participants are not personally identifiable. The most basic level of anonymisation is to replace participants’ names with pseudonyms (fake names) and remove references to specific places. Anonymity is particularly important for open data and data may not be made open for anonymity concerns. Anonymity and open data has been discussed within qualitative research which often focuses on personal experiences and opinions, and in quantitative research that includes participants from clinical populations.",
                "Related_terms": "** Anonymising; Clinical populations; Confidentiality; Research ethics; Research participants; Vulnerable population"
            },
            {
                "Title": "ARRIVE Guidelines *",
                "Definition": "** The ARRIVE guidelines (Animal Research: Reporting of In Vivo Experiments) are a checklist-based set of reporting guidelines developed to improve reporting standards, and enhance replicability, within living (i.e. *in vivo*) animal research. The second generation ARRIVE guidelines, ARRIVE 2.0, were released in 2020\\. In these new guidelines, the clarity has been improved, items have been prioritised and new information has been added with an accompanying “Explanation” and “Elaboration” document to provide a rationale for each item and a recommended set to add context to the study being described.",
                "Reference(s)": "** Percie du Sert et al. (2020)",
                "Drafted by": "** Ben Farrar",
                "Reviewed (or Edited) by": "** Jamie P. Cockcroft; Mahmoud Elsherif; Elias Garcia-Pelegrin; Helena Hartmann; Wanyin Li; Charlotte R. Pennington",
                "Translation": "** The ARRIVE guidelines (Animal Research: Reporting of In Vivo Experiments) are a checklist-based set of reporting guidelines developed to improve reporting standards, and enhance replicability, within living (i.e. *in vivo*) animal research. The second generation ARRIVE guidelines, ARRIVE 2.0, were released in 2020\\. In these new guidelines, the clarity has been improved, items have been prioritised and new information has been added with an accompanying “Explanation” and “Elaboration” document to provide a rationale for each item and a recommended set to add context to the study being described.",
                "Related_terms": "** PREPARE Guidelines; Reporting Guideline; STRANGE"
            },
            {
                "Title": "Article Processing Charge (APC) *",
                "Definition": "** An article (sometimes author) processing charge (APC) is a fee charged to authors by a publisher in exchange for publishing and hosting an open access article. APCs are often intended to compensate for a potential loss of revenue the journal may experience when moving from traditional publication models, such as subscription services or pay-per-view, to open access. While some journals charge only about US$300, APCs vary widely, from US$1000 (Advances in Methods and Practice in Psychological Science) or less to over US$10,000 (Nature). While some publishers offer waivers for researchers from certain regions of the world or who lack funds, some APCs have been criticized for being disproportionate compared to actual processing and hosting costs (Grossmann & Brembs, 2021\\) and for creating possible inequities with regard to which scientists can afford to make their works freely available (Smith et al. 2020).",
                "Reference": "** Grossmann and Brembs (2021); Smith et al. (2020)",
                "Originally drafted by": "** Nick Ballou",
                "Reviewed (or Edited) by": "** Ali H. Al-Hoorie; Bethan Iley; Flávio Azevedo; Robert Ross; Tobias Wingen \\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_",
                "Translation": "** An article (sometimes author) processing charge (APC) is a fee charged to authors by a publisher in exchange for publishing and hosting an open access article. APCs are often intended to compensate for a potential loss of revenue the journal may experience when moving from traditional publication models, such as subscription services or pay-per-view, to open access. While some journals charge only about US$300, APCs vary widely, from US$1000 (Advances in Methods and Practice in Psychological Science) or less to over US$10,000 (Nature). While some publishers offer waivers for researchers from certain regions of the world or who lack funds, some APCs have been criticized for being disproportionate compared to actual processing and hosting costs (Grossmann & Brembs, 2021\\) and for creating possible inequities with regard to which scientists can afford to make their works freely available (Smith et al. 2020).",
                "Related_terms": "** Open Access; Under-representation"
            },
            {
                "Title": "Authorship *",
                "Definition": "** Authorship assigns credit for research outputs (e.g. manuscripts, data, and software) and accountability for content (McNutt et al. 2018; Patience et al. 2019). Conventions differ across disciplines, cultures, and even research groups, in their expectations of what efforts earn authorship, what the order of authorship signifies (if anything), how much accountability for the research the corresponding author assumes, and the extent to which authors are accountable for aspects of the work that they did not personally conduct.",
                "Reference(s)": "** ALLEA (2017); German Research Foundation (2019); McNutt et al. (2018); Patience et al. (2019)",
                "Drafted by": "** Jacob Miranda",
                "Reviewed (or Edited) by": "** Bradley Baker; Brett J. Gall; Matt Jaquiery; Charlotte R. Pennington; Flávio Azevedo; Birgit Schmidt; Yuki Yamada",
                "Translation": "** Authorship assigns credit for research outputs (e.g. manuscripts, data, and software) and accountability for content (McNutt et al. 2018; Patience et al. 2019). Conventions differ across disciplines, cultures, and even research groups, in their expectations of what efforts earn authorship, what the order of authorship signifies (if anything), how much accountability for the research the corresponding author assumes, and the extent to which authors are accountable for aspects of the work that they did not personally conduct.",
                "Related_terms": "** Co-authorship; Consortium authorship; Contributorship; CRediT; First-last-author-emphasis norm (FLAE); Gift (or Guest) Authorship; Sequence-determines-credit approach (SDC)"
            },
            {
                "Title": "Auxiliary Hypothesis *",
                "Definition": "** All theories contain assumptions about the nature of constructs and how they can be measured. However, not all predictions are derived from theories and assumptions can sometimes be drawn from other premises. Additional assumptions that are made to deduce a prediction and tested by making links to observable data. These auxiliary hypotheses are sometimes invoked to explain why a replication attempt has failed.",
                "Reference(s)": "** Dienes (2008); Lakatos (1978)",
                "Drafted by": "** Alaa Aldoh",
                "Reviewed (or Edited) by": "** Ali H. Al-Hoorie; Nihan Albayrak-Aydemir; Mahmoud Elsherif; Bethan Iley; Sam Parsons; Flávio Azevedo  ### **B** {#b}",
                "Translation": "** All theories contain assumptions about the nature of constructs and how they can be measured. However, not all predictions are derived from theories and assumptions can sometimes be drawn from other premises. Additional assumptions that are made to deduce a prediction and tested by making links to observable data. These auxiliary hypotheses are sometimes invoked to explain why a replication attempt has failed.",
                "Related_terms": "** Epistemic uncertainty; Hypothesis; Statistical assumptions; Hidden moderators"
            },
            {
                "Title": "Badges (Open Science) *",
                "Definition": "** Badges are symbols that editorial teams add to published manuscripts to acknowledge open science practices and act as incentives for researchers to share data, materials, or to embed study preregistration. As clearly-visible symbols, they are intended to signal to the reader that content has met the standard of open research required to receive the badge (typically from that journal). Different badges may be assigned for different practices, such as research having been made available and accessible in a persistent location (“open material badge” and “open data badge”), or study preregistration (“preregistration badge”).",
                "Reference(s)": "** Hardwicke et al. (2020); Kidwell et al. (2016); Rowhani-Farid et al. (2020); Science (n.d.)",
                "Drafted by": "** Jacob Miranda",
                "Reviewed (or Edited) by": "** Brett Gall; Helena Hartmann; Mariella Paul; Charlotte R. Pennington; Lisa Spitzer; Suzanne L. K. Stewart",
                "Translation": "** Badges are symbols that editorial teams add to published manuscripts to acknowledge open science practices and act as incentives for researchers to share data, materials, or to embed study preregistration. As clearly-visible symbols, they are intended to signal to the reader that content has met the standard of open research required to receive the badge (typically from that journal). Different badges may be assigned for different practices, such as research having been made available and accessible in a persistent location (“open material badge” and “open data badge”), or study preregistration (“preregistration badge”).",
                "Related_terms": "** Incentives; Open Data badge; Preregistration; Triple badge"
            },
            {
                "Title": "Bayes Factor *",
                "Definition": "** A continuous statistical measure for model selection used in Bayesian inference, describing the *relative* evidence for one model over another, regardless of whether the models are correct. Bayes factors (BF) range from 0 to infinity, indicating the relative strength of the evidence, and where 1 is a neutral point of no evidence. In contrast to *p*\\-values, Bayes factors allow for 3 types of conclusions: a) evidence for the alternative hypothesis, b) evidence for the null hypothesis, and c) no sufficient evidence for either. Thus, BF are typically expressed as BF10 for evidence regarding the alternative compared to the null hypothesis, and as BF01 for evidence regarding the null compared to the alternative hypothesis.",
                "Reference": "** Hoijtink et al. (2019) Makowski et al. (2019)",
                "Originally drafted by": "** Meng Liu",
                "Reviewed (or Edited) by": "** Alaa AlDoh; Helena Hartmann; Connor Keating; Kai Krautter; Michele C. Lim; Suzanne L. K. Stewart; Ana Todorovic",
                "Translation": "** A continuous statistical measure for model selection used in Bayesian inference, describing the *relative* evidence for one model over another, regardless of whether the models are correct. Bayes factors (BF) range from 0 to infinity, indicating the relative strength of the evidence, and where 1 is a neutral point of no evidence. In contrast to *p*\\-values, Bayes factors allow for 3 types of conclusions: a) evidence for the alternative hypothesis, b) evidence for the null hypothesis, and c) no sufficient evidence for either. Thus, BF are typically expressed as BF10 for evidence regarding the alternative compared to the null hypothesis, and as BF01 for evidence regarding the null compared to the alternative hypothesis.",
                "Related_terms": "** Bayesian inference; Bayesian statistics; Likelihood function; Null Hypothesis Significance Testing (NHST); *p*\\-value"
            },
            {
                "Title": "Bayesian Inference *",
                "Definition": "** A method of statistical inference based upon Bayes’ theorem, which makes use of epistemological (un)certainty using the mathematical language of probability. Bayesian inference is based on allocating (and reallocating, based on newly-observed data or evidence) credibility across possibilities. Two existing approaches to Bayesian inference include “Bayes factors” (BF) and Bayesian parameter estimation.",
                "Reference": "** Dienes (2011; 2014; 2016); Etz et al. (2018); Kruschke (2015); McElreath (2020); Wagenmakers et al. (2018)",
                "Originally drafted by": "** Charlotte R. Pennington",
                "Reviewed (or Edited) by": "** Alaa AlDoh; Bradley Baker; Robert Ross; Markus Weinmann; Tobias Wingen; Steven Verheyen",
                "Translation": "** A method of statistical inference based upon Bayes’ theorem, which makes use of epistemological (un)certainty using the mathematical language of probability. Bayesian inference is based on allocating (and reallocating, based on newly-observed data or evidence) credibility across possibilities. Two existing approaches to Bayesian inference include “Bayes factors” (BF) and Bayesian parameter estimation.",
                "Related_terms": "** Bayes Factor; Bayesian statistics; Bayesian Parameter Estimation"
            },
            {
                "Title": "Bayesian Parameter Estimation **",
                "Definition": "** A Bayesian approach to estimating parameter values by updating a prior belief about model parameters (i.e., prior distribution) with new evidence (i.e., observed data) via a likelihood function, resulting in a posterior distribution. The posterior distribution may be summarised in a number of ways including: point estimates (mean/mode/median of a posterior probability distribution), intervals of defined boundaries, and intervals of defined mass (typically referred to as a credible interval). In turn, a posterior distribution may become a prior distribution in a subsequent estimation. A posterior distribution can also be sampled using Monte-Carlo Markov Chain methods which can be used to determine complex model uncertainties (e.g. Foreman-Mackey et al., 2013).",
                "Reference": "** Foreman-Mackey et al. (2013); McElreath (2020); Press (2007); [https://blog.stata.com/2016/11/15/introduction-to-bayesian-statistics-part-2-mcmc-and-the-metropolis-hastings-algorithm/](https://blog.stata.com/2016/11/15/introduction-to-bayesian-statistics-part-2-mcmc-and-the-metropolis-hastings-algorithm/)",
                "Originally drafted by": "** Alaa AlDoh",
                "Reviewed (or Edited) by": "** Mahmoud Elsherif; Helena Hartmann; Dominik Kiersz; Meng Liu; Ana Todorovic; Markus Weinmann",
                "Translation": "** A Bayesian approach to estimating parameter values by updating a prior belief about model parameters (i.e., prior distribution) with new evidence (i.e., observed data) via a likelihood function, resulting in a posterior distribution. The posterior distribution may be summarised in a number of ways including: point estimates (mean/mode/median of a posterior probability distribution), intervals of defined boundaries, and intervals of defined mass (typically referred to as a credible interval). In turn, a posterior distribution may become a prior distribution in a subsequent estimation. A posterior distribution can also be sampled using Monte-Carlo Markov Chain methods which can be used to determine complex model uncertainties (e.g. Foreman-Mackey et al., 2013).",
                "Related_terms": "** Bayes Factor; Bayesian inference; Bayesian statistics; Null Hypothesis Significance Testing (NHST)"
            },
            {
                "Title": "BIDS data structure *",
                "Definition": "** The Brain Imaging Data Structure (BIDS) describes a simple and easy-to-adopt way of organizing neuroimaging, electrophysiological, and behavioral data (i.e., file formats, folder structures). BIDS is a community effort developed *by* the community *for* the community and was inspired by the format used internally by the OpenfMRI repository known as [OpenNeuro](https://openneuro.org). Having initially been developed for fMRI data, the BIDS data structure has been extended for many other measures, such as EEG (Pernet et al., 2019).",
                "Reference(s)": "** Gorgolewski et al. (2016); [https://bids.neuroimaging.io/](https://bids.neuroimaging.io/)",
                "Drafted by": "** Tina Lonsdorf",
                "Reviewed (or Edited) by": "** Ali H. Al-Hoorie; David Moreau; Mariella Paul; Charlotte R. Pennington",
                "Translation": "** The Brain Imaging Data Structure (BIDS) describes a simple and easy-to-adopt way of organizing neuroimaging, electrophysiological, and behavioral data (i.e., file formats, folder structures). BIDS is a community effort developed *by* the community *for* the community and was inspired by the format used internally by the OpenfMRI repository known as [OpenNeuro](https://openneuro.org). Having initially been developed for fMRI data, the BIDS data structure has been extended for many other measures, such as EEG (Pernet et al., 2019).",
                "Related_terms": "** Open Data"
            },
            {
                "Title": "BIZARRE *",
                "Definition": "** This acronym refers to Barren, Institutional, Zoo, and other Rare Rearing Environments (BIZARRE). Most research for chimpanzees is conducted on this specific sample. This limits the generalizability of a large number of research findings in the chimpanzee population. The BIZARRE has been argued to reflect the universal concept of what is a chimpanzee (see also WEIRD, which has been argued to be a universal concept for what is a human).",
                "Reference(s)": "** Clark et al. (2019); Leavens et al. (2010)",
                "Originally drafted by": "** Mahmoud Elsherif",
                "Reviewed (or Edited) by": "** Zoe Flack; Charlotte R. Pennington  ### ---",
                "Translation": "** This acronym refers to Barren, Institutional, Zoo, and other Rare Rearing Environments (BIZARRE). Most research for chimpanzees is conducted on this specific sample. This limits the generalizability of a large number of research findings in the chimpanzee population. The BIZARRE has been argued to reflect the universal concept of what is a chimpanzee (see also WEIRD, which has been argued to be a universal concept for what is a human).",
                "Related_terms": "** Populations; STRANGE; WEIRD"
            },
            {
                "Title": "Bottom-up approach (to Open Scholarship) **",
                "Definition": "** Within academic culture, an approach focusing on the intrinsic interest of academics to improve the quality of research and research culture, for instance by making it supportive, collaborative, creative and inclusive. Usually indicates leadership from early-career researchers acting as the changemakers driving shifts and change in scientific methodology through enthusiasm and innovation, compared to a “top-down” approach initiated by more senior researchers \"Bottom-up approaches take into account the specific local circumstances of the case itself, often using empirical data, lived experience, personal accounts, and circumstances as the starting point for developing policy solutions.\"",
                "Reference(s)": "** Button et al. (2016); Button et al. (2020); Hart and Silka (2020); Meslin (2010); Moran et al. (2020); [https://www.cos.io/blog/strategy-for-culture-change](https://www.cos.io/blog/strategy-for-culture-change)",
                "Drafted by": "** Catherine Laverty",
                "Reviewed (or Edited) by": "** Helena Hartmann; Michele C. Lim; Adam Parker; Charlotte R. Pennington; Birgit Schmidt; Marta Topor; Flávio Azevedo",
                "Translation": "** Within academic culture, an approach focusing on the intrinsic interest of academics to improve the quality of research and research culture, for instance by making it supportive, collaborative, creative and inclusive. Usually indicates leadership from early-career researchers acting as the changemakers driving shifts and change in scientific methodology through enthusiasm and innovation, compared to a “top-down” approach initiated by more senior researchers \"Bottom-up approaches take into account the specific local circumstances of the case itself, often using empirical data, lived experience, personal accounts, and circumstances as the starting point for developing policy solutions.\"",
                "Related_terms": "** Early Career Researchers (ECRs); Grassroot initiatives"
            },
            {
                "Title": "Bracketing Interviews *",
                "Definition": "Bracketing interviews are commonly used within qualitative approaches. During these interviews researchers explore their personal subjectivities and assumptions surrounding their ongoing research. This allows researchers to be aware of their own interests and helps them to become both more reflective and critical about their research, considering how their own experiences may impact the research process. Bracketing interviews can also be subject to qualitative analysis.",
                "Originally drafted by": "Claire Melia",
                "Reviewed (or Edited) by": "Tamara Kalandadze; Charlotte R. Pennington; Graham Reid; Marta Topor",
                "Translation": "Bracketing interviews are commonly used within qualitative approaches. During these interviews researchers explore their personal subjectivities and assumptions surrounding their ongoing research. This allows researchers to be aware of their own interests and helps them to become both more reflective and critical about their research, considering how their own experiences may impact the research process. Bracketing interviews can also be subject to qualitative analysis.",
                "Related_terms": "Qualitative research; Reflexivity; Researcher bias **Reference (s)**: Rolls and Relf (2006); Sorsa et al. (2015)"
            },
            {
                "Title": "Bropenscience *",
                "Definition": "A tongue-in-cheek expression intended to raise awareness of the lack of diverse voices in open science (Bahlai, Bartlett, Burgio et al. 2019; Onie, 2020), in addition to the presence of behavior and communication styles that can be toxic or exclusionary. Importantly, not all bros are men; rather, they are individuals who demonstrate rigid thinking, lack self-awareness, and tend towards hostility, unkindness, and exclusion (Pownall et al., 2021; Whitaker & Guest, 2020). They generally belong to dominant groups who benefit from structural privileges. To address \\#bropenscience, researchers should examine and address structural inequalities within academic systems and institutions.",
                "Originally drafted by": "Zoe Flack",
                "Reviewed (or Edited) by": "Magdalena Grose-Hodge; Helena Hartmann; Bethan Iley; Tamara Kalandadze; Adam Parker; Charlotte R. Pennington; Flávio Azevedo; Bradley Baker; Mahmoud Elsherif   ### **C** {#c}",
                "Translation": "A tongue-in-cheek expression intended to raise awareness of the lack of diverse voices in open science (Bahlai, Bartlett, Burgio et al. 2019; Onie, 2020), in addition to the presence of behavior and communication styles that can be toxic or exclusionary. Importantly, not all bros are men; rather, they are individuals who demonstrate rigid thinking, lack self-awareness, and tend towards hostility, unkindness, and exclusion (Pownall et al., 2021; Whitaker & Guest, 2020). They generally belong to dominant groups who benefit from structural privileges. To address \\#bropenscience, researchers should examine and address structural inequalities within academic systems and institutions.",
                "Related_terms": "Diversity; Inclusion; Intersectionality; Open Science **Reference (s)**: Guest (2017); Whitaker and Guest (2020); Pownall et al. (2021)"
            },
            {
                "Title": "CARKing *",
                "Definition": "** Critiquing After the Results are Known (CARKing) refers to presenting a criticism of a design as one that you would have made in advance of the results being known. It usually forms a reaction or criticism to unwelcome or unfavourable results, results whether the critic is conscious of this fact or not.",
                "Reference(s)": "** Bardsley (2018); Nosek and Lakens (2014)",
                "Originally drafted by": "** Mahmoud Elsherif",
                "Reviewed (or Edited) by": "** Ali H. Al-Hoorie; Ashley Blake; Adrien Fillon; Charlotte R. Pennington",
                "Translation": "** Critiquing After the Results are Known (CARKing) refers to presenting a criticism of a design as one that you would have made in advance of the results being known. It usually forms a reaction or criticism to unwelcome or unfavourable results, results whether the critic is conscious of this fact or not.",
                "Related_terms": "** HARKing; Preregistration; Registered Report"
            },
            {
                "Title": "Center for Open Science (COS) *",
                "Definition": "** A non-profit technology organization based in Charlottesville, Virginia with the mission “to increase openness, integrity, and reproducibility of research.” Among other resources, the COS hosts the Open Science Framework (OSF) and the Open Scholarship Knowledge Base.",
                "Reference(s)": "** cos.io",
                "Drafted by": "** Beatrix Arendt",
                "Reviewed (or Edited) by": "** Ali H. Al-Hoorie; Mariella Paul; Charlotte R. Pennington; Lisa Spitzer",
                "Translation": "** A non-profit technology organization based in Charlottesville, Virginia with the mission “to increase openness, integrity, and reproducibility of research.” Among other resources, the COS hosts the Open Science Framework (OSF) and the Open Scholarship Knowledge Base.",
                "Related_terms": "** Open Science badges; Open Science Framework; OSF collections; OSF institutions; OSF meetings; OSF preprints; OSF registries; Registrations (Preregistrations & Registered Reports); Transparency and Openness Promotion Guidelines (TOP)"
            },
            {
                "Title": "Citation bias *",
                "Definition": "** A biased selection of papers or authors cited and included in the references section. When citation bias is present, it is often in a way which would benefit the author(s) or reviewers, over-represents statistically significant studies, or reflects pervasive gender or racial biases (Brooks, 1985; Jannot et al., 2013; Zurn et al., 2020). One proposed solution is the use of Citation Diversity Statements, in which authors reflect on their citation practices and identify biases which may have emerged (Zurn et al., 2020).",
                "Reference(s)": "** Brooks (1985); Jannot et al. (2013); Thombs et al. (2015); Zurn et al. (2020)",
                "Originally drafted by": "** Bettina M. J. Kern",
                "Reviewed (or Edited) by": "** Mahmoud Elsherif; Annalise A. LaPlume; Helena Hartmann; Bethan Iley; Charlotte R. Pennington; Timo Roettger; Tobias Wingen",
                "Translation": "** A biased selection of papers or authors cited and included in the references section. When citation bias is present, it is often in a way which would benefit the author(s) or reviewers, over-represents statistically significant studies, or reflects pervasive gender or racial biases (Brooks, 1985; Jannot et al., 2013; Zurn et al., 2020). One proposed solution is the use of Citation Diversity Statements, in which authors reflect on their citation practices and identify biases which may have emerged (Zurn et al., 2020).",
                "Related_terms": "** Citation diversity statement; Reporting bias"
            },
            {
                "Title": "Citation Diversity Statement *",
                "Definition": "** A current effort trying to increase awareness and mitigate the citation bias in relation to gender and race is the Citation Diversity Statement, a short paragraph where “the authors consider their own bias and quantify the equitability of their reference lists. It states: (i) the importance of citation diversity, (ii) the percentage breakdown (or other diversity indicators) of citations in the paper, (iii) the method by which percentages were assessed and its limitations, and (iv) a commitment to improving equitable practices in science” (Zurn et al., 2020, p. 669).",
                "Reference": "** Zurn et al. (2020)",
                "Originally drafted by": "** Helena Hartmann",
                "Reviewed (or Edited) by": "** Mahmoud Elsherif; Magdalena Grose-Hodge; Sam Parsons; Timo Roettger",
                "Translation": "** A current effort trying to increase awareness and mitigate the citation bias in relation to gender and race is the Citation Diversity Statement, a short paragraph where “the authors consider their own bias and quantify the equitability of their reference lists. It states: (i) the importance of citation diversity, (ii) the percentage breakdown (or other diversity indicators) of citations in the paper, (iii) the method by which percentages were assessed and its limitations, and (iv) a commitment to improving equitable practices in science” (Zurn et al., 2020, p. 669).",
                "Related_terms": "** Citation bias; Diversity; Under-representation"
            },
            {
                "Title": "Citizen Science *",
                "Definition": "** Citizen science refers to projects that actively involve the general public in the scientific endeavour, with the goal of democratizing science. Citizen scientists can be involved in all stages of research, acting as collaborators, contributors or project leaders. An example of a major citizen science project involved individuals identifying astronomical bodies (Lintott, 2008).",
                "Reference(s)": "** Cohn (2008); European Citizen Science Association (2015); Lintott (2008)",
                "Drafted by": "** Mahmoud Elsherif; Ana Barbosa Mendes",
                "Reviewed (or Edited) by": "** Gisela H. Govaart; Tamara Kalandadze; Dominik Kiersz; Charlotte R. Pennington; Robert M. Ross",
                "Translation": "** Citizen science refers to projects that actively involve the general public in the scientific endeavour, with the goal of democratizing science. Citizen scientists can be involved in all stages of research, acting as collaborators, contributors or project leaders. An example of a major citizen science project involved individuals identifying astronomical bodies (Lintott, 2008).",
                "Related_terms": "** Crowd science; Crowdsourcing **Alternative definition:** (if applicable) In the past, citizen science mostly referred to volunteers who participate as field assistants in scientific studies (Cohn, 2008, p. 193)."
            },
            {
                "Title": "CKAN",
                "Definition": "** The Comprehensive Knowledge Archive Network (CKAN) is an open-source data platform and free software that aims to provide tools to streamline publishing and data sharing. CKAN supports governments, research institutions and other organizations in managing and publishing large amounts of data.",
                "Reference": "** https://ckan.org/",
                "Originally drafted by": "** Tsvetomira Dumbalska",
                "Reviewed (or Edited) by": "** Ali H. Al-Hoorie; Kai Krautter; Charlotte R. Pennington",
                "Translation": "** The Comprehensive Knowledge Archive Network (CKAN) is an open-source data platform and free software that aims to provide tools to streamline publishing and data sharing. CKAN supports governments, research institutions and other organizations in managing and publishing large amounts of data.",
                "Related_terms": "** Data platforms; Data sharing"
            },
            {
                "Title": "COAR Community Framework for Good Practices in Repositories *",
                "Definition": "** A framework which identifies best practices for scientific repositories and evaluation criteria for these practices. Its flexible and multidimensional approach means that it can be applied to different types of repositories, including those which host publications or data, across geographical and thematic contexts.",
                "Reference": "** Confederation of Open Access Repositories (2020, October 8\\)",
                "Originally drafted by": "** Aleksandra Lazić",
                "Reviewed (or Edited) by": "** Ashley Blake; Jamie P. Cockcroft; Bethan Iley; Sam Parsons",
                "Translation": "** A framework which identifies best practices for scientific repositories and evaluation criteria for these practices. Its flexible and multidimensional approach means that it can be applied to different types of repositories, including those which host publications or data, across geographical and thematic contexts.",
                "Related_terms": "** Metadata; Open Access; Open Data; Open Material; Repository; TRUST principles"
            },
            {
                "Title": "Codebook *",
                "Definition": "** A codebook is a high-level summary that describes the contents, structure, nature and layout of a data set. A well-documented codebook contains information intended to be complete and self-explanatory for each variable in a data file, such as the wording and coding of the item, and the underlying construct. It provides transparency to researchers who may be unfamiliar with the data but wish to reproduce analyses or reuse the data.",
                "Reference": "** Arslan et al. (2019);[https://www.icpsr.umich.edu/icpsrweb/content/shared/ICPSR/faqs/what-is-a-codebook.html](https://www.icpsr.umich.edu/icpsrweb/content/shared/ICPSR/faqs/what-is-a-codebook.html)",
                "Originally drafted by": "** Tina Lonsdorf",
                "Reviewed (or Edited) by": "** Ali H. Al-Hoorie; Ashley Blake, Kai Krautter; Charlotte R. Pennington; Flávio Azevedo",
                "Translation": "** A codebook is a high-level summary that describes the contents, structure, nature and layout of a data set. A well-documented codebook contains information intended to be complete and self-explanatory for each variable in a data file, such as the wording and coding of the item, and the underlying construct. It provides transparency to researchers who may be unfamiliar with the data but wish to reproduce analyses or reuse the data.",
                "Related_terms": "** Data dictionary; Metadata"
            },
            {
                "Title": "Code review *",
                "Definition": "** The process of checking another researcher's programming (specifically, computer source code) including but not limited to statistical code and data modelling. This process is designed to detect and resolve mistakes, thereby improving code quality. In practice, a modern peer review process may take place via a hosted online repository such as GitHub, GitLab or SourceForge.**Related terms:** Reproducibility; Version control",
                "Reference(s)": "** Petre and Wilson (2014); Scopatz and Huff (2015)",
                "Originally drafted by": "** Filip Dechterenko",
                "Reviewed (or Edited) by": "** Ali H. Al-Hoorie; Dominik Kiersz; Sam Parsons; Charlotte R. Pennington",
                "Translation": "** The process of checking another researcher's programming (specifically, computer source code) including but not limited to statistical code and data modelling. This process is designed to detect and resolve mistakes, thereby improving code quality. In practice, a modern peer review process may take place via a hosted online repository such as GitHub, GitLab or SourceForge.**Related terms:** Reproducibility; Version control"
            },
            {
                "Title": "Collaborative Replication and Education Project (CREP) *",
                "Definition": "** The Collaborative Replication and Education Project (CREP) is an initiative designed to organize and structure replication efforts of highly-cited empirical studies in psychology to satisfy the dual needs for more high-quality direct replications and more training in empirical research techniques for psychology students. CREP aims to address the need for replications of highly cited studies, and to provide training, support and professional growth opportunities for academics completing replication projects.",
                "Reference(s)": "** Wagge et al. (2019)",
                "Drafted by": "** Connor Keating",
                "Reviewed (or Edited) by": "** Bradley Baker; Mahmoud Elsherif; Zoe Flack; Charlotte R. Pennington",
                "Translation": "** The Collaborative Replication and Education Project (CREP) is an initiative designed to organize and structure replication efforts of highly-cited empirical studies in psychology to satisfy the dual needs for more high-quality direct replications and more training in empirical research techniques for psychology students. CREP aims to address the need for replications of highly cited studies, and to provide training, support and professional growth opportunities for academics completing replication projects.",
                "Related_terms": "** Direct replication; Exact replication"
            },
            {
                "Title": "Committee on Best Practices in Data Analysis and Sharing (COBIDAS) *",
                "Definition": "** The Organization for Human Brain Mapping (OHBM) neuroimaging community has developed a guideline for best practices in neuroimaging data acquisition, analysis, reporting, and sharing of both data and analysis code. It contains eight elements that should be included when writing up or submitting a manuscript in order to improve reporting methods and the resulting neuroimages in order to optimize transparency and reproducibility. **Alternative definition:** (if applicable) Checklist for data analysis and sharing",
                "Reference(s)": "** Nichols et al. (2017); Pernet et al. (2020)",
                "Originally drafted by": "** Yu-Fang Yang",
                "Reviewed (or Edited) by": "** Jamie P. Cockcroft; Helena Hartmann; Adam Parker; Sam Parsons",
                "Translation": "** The Organization for Human Brain Mapping (OHBM) neuroimaging community has developed a guideline for best practices in neuroimaging data acquisition, analysis, reporting, and sharing of both data and analysis code. It contains eight elements that should be included when writing up or submitting a manuscript in order to improve reporting methods and the resulting neuroimages in order to optimize transparency and reproducibility. **Alternative definition:** (if applicable) Checklist for data analysis and sharing"
            },
            {
                "Title": "Communality *",
                "Definition": "** The common ownership of scientific results and methods and the consequent imperative to share both freely. Communality is based on the fact that every scientific finding is seen as a product of the effort of a number of agents. This norm is followed when scientists openly share their new findings with colleagues.",
                "Reference(s)": "** Anderson et al. (2010); Hardwicke (2014); Merton (1938, 1942\\)",
                "Drafted by": "** David Moreau",
                "Reviewed (or Edited) by": "** Ashley Blake; Mahmoud Elsherif; Charlotte R. Pennington; Beatrice Valentini",
                "Translation": "** The common ownership of scientific results and methods and the consequent imperative to share both freely. Communality is based on the fact that every scientific finding is seen as a product of the effort of a number of agents. This norm is followed when scientists openly share their new findings with colleagues.",
                "Related_terms": "** Mertonian norms; Objectivity **Alternative definition:** Communism (in Merton, 1942\\) **Related terms to alternative definition** (if applicable)"
            },
            {
                "Title": "Community Projects *",
                "Definition": "** Collaborative projects that involve researchers from different career levels, disciplines, institutions or countries. Projects may have different goals including peer support and learning, conducting research, teaching and education. They can be short-term (e.g., conference events or hackathons) or long-term (e.g., journal clubs or consortium-led research projects). Collaborative culture and community building are key to achieving project goals.",
                "Reference(s)": "** Ellemers (2021); Orben (2019); Shepard (2015)",
                "Drafted by": "** Marta Topor",
                "Reviewed (or Edited) by": "** Ali H. Al-Hoorie; Jamie P. Cockcroft; Mahmoud Elsherif; Kai Krautter; Gerald Vineyard",
                "Translation": "** Collaborative projects that involve researchers from different career levels, disciplines, institutions or countries. Projects may have different goals including peer support and learning, conducting research, teaching and education. They can be short-term (e.g., conference events or hackathons) or long-term (e.g., journal clubs or consortium-led research projects). Collaborative culture and community building are key to achieving project goals.",
                "Related_terms": "** Bottom-up approach (to Open Scholarship); Crowdsourced research; Hackathon; Many Labs; ReproducibiliTea"
            },
            {
                "Title": "Compendium *",
                "Definition": "** A collection of files prepared by a researcher to support a report or publication that include the data, metadata, programming code, software dependencies, licenses, and other instructions necessary for another researcher to independently reproduce the findings presented in the report or publication.",
                "Originally drafted by": "** Ben Marwick",
                "Reviewed (or Edited) by": "** Ali H. Al-Hoorie; Charlotte R. Pennington",
                "Translation": "** A collection of files prepared by a researcher to support a report or publication that include the data, metadata, programming code, software dependencies, licenses, and other instructions necessary for another researcher to independently reproduce the findings presented in the report or publication.",
                "Related_terms": "** Compendia; Replication; Reproducibility; Research compendium; **References:** Claerbout and Karrenfach (1992); Gentleman (2005); Marwick et al. (2018); Nüst et al. (2018)"
            },
            {
                "Title": "Computational reproducibility *",
                "Definition": "** Ability to recreate the same results as the original study (including tables, figures, and quantitative findings), using the same input data, computational methods, and conditions of analysis. The availability of code and data facilitates computational reproducibility, as does preparation of these materials (annotating data, delineating software versions used, sharing computational environments, etc). Ideally, computational reproducibility should be achievable by another second researcher (or the original researcher, at a future time), using only a set of files and written instructions. Also referred to as analytic reproducibility (LeBel et al., 2018).",
                "Reference(s)": "** Committee on Reproducibility and Replicability in Science et al. (2019); Kitzes et al (2017, p. xxii); LeBel et al. (2018); Nosek and Errington (2020); Obels et al. (2020)",
                "Drafted by": "** Tina Lonsdorf",
                "Reviewed (or Edited) by": "** Sarah Ashcroft-Jones; Helena Hartmann; Annalise A. LaPlume; Adam Parker; Charlotte R. Pennington; Eike Mark Rinke",
                "Translation": "** Ability to recreate the same results as the original study (including tables, figures, and quantitative findings), using the same input data, computational methods, and conditions of analysis. The availability of code and data facilitates computational reproducibility, as does preparation of these materials (annotating data, delineating software versions used, sharing computational environments, etc). Ideally, computational reproducibility should be achievable by another second researcher (or the original researcher, at a future time), using only a set of files and written instructions. Also referred to as analytic reproducibility (LeBel et al., 2018).",
                "Related_terms": "** FAIR principles; Replicability; Reproducibility"
            },
            {
                "Title": "Conceptual replication *",
                "Definition": "** A replication attempt whereby the primary effect of interest is the same but tested in a different sample and captured in a different way to that originally reported (i.e., using different operationalisations, data processing and statistical approaches and/or different constructs; LeBel et al., 2018). The purpose of a conceptual replication is often to explore what conditions limit the extent to which an effect can be observed and generalised (e.g., only within certain contexts, with certain samples, using certain measurement approaches) towards evaluating and advancing theory (Hüffmeier et al., 2016).",
                "Reference(s)": "** Crüwell et al. (2019); Hüffmeier et al. (2016); LeBel et al. (2018)",
                "Drafted by": "** Mahmoud Elsherif; Thomas Rhys Evans",
                "Reviewed (or Edited) by": "** Adrien Fillon; Helena Hartmann; Matt Jaquiery; Tina B. Lonsdorf; Catia M. Oliveira; Charlotte R. Pennington; Graham Reid; Timo Roettger; Lisa Spitzer; Suzanne L. K. Stewart; Flávio Azevedo",
                "Translation": "** A replication attempt whereby the primary effect of interest is the same but tested in a different sample and captured in a different way to that originally reported (i.e., using different operationalisations, data processing and statistical approaches and/or different constructs; LeBel et al., 2018). The purpose of a conceptual replication is often to explore what conditions limit the extent to which an effect can be observed and generalised (e.g., only within certain contexts, with certain samples, using certain measurement approaches) towards evaluating and advancing theory (Hüffmeier et al., 2016).",
                "Related_terms": "** Direct replication; Generalizability"
            },
            {
                "Title": "Confirmation bias *",
                "Definition": "** The tendency to seek out, interpret, favor and recall information in a way that supports one’s prior values, beliefs, expectations, or hypothesis.",
                "Reference(s)": "** Bishop (2020); Nickerson (1998); Spencer and Heneghan (2018); Wason (1960)",
                "Drafted by": "** Barnabas Szaszi; Jenny Terry",
                "Reviewed (or Edited) by": "** Mahmoud Elsherif; Tamara Kalandadze; Sam Parsons; Charlotte R. Pennington",
                "Translation": "** The tendency to seek out, interpret, favor and recall information in a way that supports one’s prior values, beliefs, expectations, or hypothesis.",
                "Related_terms": "** Confirmatory bias; Congeniality bias; Myside bias"
            },
            {
                "Title": "Confirmatory analyses *",
                "Definition": "** Part of the confirmatory-exploratory distinction (Wagenmakers et al., 2012), where confirmatory analyses refer to analyses that were set *a priori* and test existent hypotheses. The lack of this distinction within published research findings has been suggested to explain replicability issues and is suggested to be overcome through study preregistration which clearly distinguishes confirmatory from exploratory analyses. Other researchers have questioned these terms and recommended a replacement with ‘discovery-oriented’ and ‘theory-testing research’ (Oberauer & Lewandowsky, 2019; see also Szollosi & Donkin, 2019).",
                "Reference(s)": "** Box (1976); Oberauer and Lewandowsky (2019); Szollosi and Donkin (2019); Tukey (1977); Wagenmakers et al. (2012)",
                "Drafted by": "** Jenny Terry",
                "Reviewed (or Edited) by": "** Mahmoud Elsherif; Eduardo Garcia-Garzon; Helena Hartmann; Mariella Paul; Charlotte R. Pennington; Timo Roettger; Lisa Spitzer",
                "Translation": "** Part of the confirmatory-exploratory distinction (Wagenmakers et al., 2012), where confirmatory analyses refer to analyses that were set *a priori* and test existent hypotheses. The lack of this distinction within published research findings has been suggested to explain replicability issues and is suggested to be overcome through study preregistration which clearly distinguishes confirmatory from exploratory analyses. Other researchers have questioned these terms and recommended a replacement with ‘discovery-oriented’ and ‘theory-testing research’ (Oberauer & Lewandowsky, 2019; see also Szollosi & Donkin, 2019).",
                "Related_terms": "** Exploratory data analysis; Preregistration"
            },
            {
                "Title": "Conflict of interest **",
                "Definition": "** A conflict of interest (COI, also ‘competing interest’) is a financial or non-financial relationship, activity or other interest that might compromise objectivity or professional judgement on the part of an author, reviewer, editor, or editorial staff. The *Principles of Transparency and Best Practice in Scholarly Publishing* by the Committee on Publication Ethics (COPE), the Directory of Open Access Journals (DOAJ), the Open Access Scholarly Publishers Association (OASPA), and the World Association of Medical Editors (WAME) states that journals should have policies on publication ethics, including policies on COI (DOAJ, 2018). COIs should be made transparent so that readers can properly evaluate research and assess for potential or actual bias(es). Outside publishing, academic presenters, panel members and educators should also declare COIs. Purposeful failure to disclose a COI may be considered a form of misconduct.",
                "Reference(s)": "** [http://www.icmje.org/recommendations/browse/roles-and-responsibilities/author-responsibilities--conflicts-of-interest.html](http://www.icmje.org/recommendations/browse/roles-and-responsibilities/author-responsibilities--conflicts-of-interest.html); DOAJ, 2018: [https://doaj.org/apply/transparency/](https://doaj.org/apply/transparency/)",
                "Drafted by": "** Christopher Graham",
                "Reviewed (or Edited) by": "** Flávio Azevedo",
                "Translation": "** A conflict of interest (COI, also ‘competing interest’) is a financial or non-financial relationship, activity or other interest that might compromise objectivity or professional judgement on the part of an author, reviewer, editor, or editorial staff. The *Principles of Transparency and Best Practice in Scholarly Publishing* by the Committee on Publication Ethics (COPE), the Directory of Open Access Journals (DOAJ), the Open Access Scholarly Publishers Association (OASPA), and the World Association of Medical Editors (WAME) states that journals should have policies on publication ethics, including policies on COI (DOAJ, 2018). COIs should be made transparent so that readers can properly evaluate research and assess for potential or actual bias(es). Outside publishing, academic presenters, panel members and educators should also declare COIs. Purposeful failure to disclose a COI may be considered a form of misconduct.",
                "Related_terms": "** Objectivity; Peer review; Public Trust in Science; Publication ethics; Transparency"
            },
            {
                "Title": "Consortium authorship",
                "Definition": "** Only the name of the consortium or organization appears in the author column, and the individuals' names do not appear in the literature: For example, ‘FORRT’ as an author. This can be seen in the products of collaborative projects with a very large number of collaborators and/or contributors. Depending on the journal policy, individual researchers may be recorded as one of the authors of the product in literature databases such as ORCID and Scopus. Consortium authorship can also be termed group, corporate, organisation/organization or collective authorship (e.g. [https://www.bmj.com/about-bmj/resources-authors/article-submission/authorship-contributorship](https://www.bmj.com/about-bmj/resources-authors/article-submission/authorship-contributorship)), or collaborative authorship (e.g. [https://support.jmir.org/hc/en-us/articles/115001449591-What-is-a-group-author-collaborative-author-and-does-it-need-an-ORCID](https://support.jmir.org/hc/en-us/articles/115001449591-What-is-a-group-author-collaborative-author-and-does-it-need-an-ORCID))",
                "Reference(s)": "** Open Science Collaboration (2015); Tierney et al. (2020, 2021\\)",
                "Drafted by": "** Yuki Yamada",
                "Reviewed (or Edited) by": "** Adam Parker; Charlotte R. Pennington; Beatrice Valentini; Qinyu Xiao; Flávio Azevedo",
                "Translation": "** Only the name of the consortium or organization appears in the author column, and the individuals' names do not appear in the literature: For example, ‘FORRT’ as an author. This can be seen in the products of collaborative projects with a very large number of collaborators and/or contributors. Depending on the journal policy, individual researchers may be recorded as one of the authors of the product in literature databases such as ORCID and Scopus. Consortium authorship can also be termed group, corporate, organisation/organization or collective authorship (e.g. [https://www.bmj.com/about-bmj/resources-authors/article-submission/authorship-contributorship](https://www.bmj.com/about-bmj/resources-authors/article-submission/authorship-contributorship)), or collaborative authorship (e.g. [https://support.jmir.org/hc/en-us/articles/115001449591-What-is-a-group-author-collaborative-author-and-does-it-need-an-ORCID](https://support.jmir.org/hc/en-us/articles/115001449591-What-is-a-group-author-collaborative-author-and-does-it-need-an-ORCID))",
                "Related_terms": "** Authorship; CRediT"
            },
            {
                "Title": "Constraints on Generality (COG)",
                "Definition": "** A statement that explicitly identifies and justifies the target population, and conditions, for the reported findings. Researchers should be explicit about potential boundary conditions for their generalisations (Simons et al., 2017). Researchers should provide detailed descriptions of the sampled population and/or contextual factors that might have affected the results such that future replication attempts can take these factors into account (Brandt et al., 2014). Conditions not explicitly listed are assumed not to have theoretical relevance to the replicability of the effect.",
                "Reference(s)": "** Busse et al. (2017); Brandt et al. (2014); Simons et al. (2017); Yarkoni (2020)",
                "Drafted by": "** Mahmoud Elsherif",
                "Reviewed (or Edited) by": "** Ali H. Al-Hoorie; Jamie P. Cockcroft; Sam Parsons; Charlotte R. Pennington; Timo Roettger  ### ---",
                "Translation": "** A statement that explicitly identifies and justifies the target population, and conditions, for the reported findings. Researchers should be explicit about potential boundary conditions for their generalisations (Simons et al., 2017). Researchers should provide detailed descriptions of the sampled population and/or contextual factors that might have affected the results such that future replication attempts can take these factors into account (Brandt et al., 2014). Conditions not explicitly listed are assumed not to have theoretical relevance to the replicability of the effect.",
                "Related_terms": "** BIZARRE; Diversity; Equity; Generalizability; Inclusion; Reproducibility; Replication; STRANGE; WEIRD"
            },
            {
                "Title": "Construct validity *",
                "Definition": "** When used in the context of measurement and testing, construct validity refers to the degree to which a test measures what it claims to be measuring. In fields that study hypothetical unobservable entities, construct validation is essentially theory testing, because it involves determining whether an objective measure (a questionnaire, lab task, etc.) is a valid representation of a hypothetical construct (i.e., conforms to a theory). When used in a broader sense about a research study, or a claim, conclusion, or observed effect in a research study, construct validity concerns the extent to which the sampling particulars used in the study (participants, settings, treatments, and dependent variables) map onto the higher order constructs the study, the claim, the conclusion is about. According to Shadish et al. (2002), construct validity can be defined as “the degree to which inferences are warranted from the observed persons, settings, and cause and effect operations included in a study to the constructs that these instances might represent” (p. 38).",
                "Reference": "** Cronbach and Meehl (1955); Shadish et al. (2002); Smith (2005)",
                "Originally drafted by": "** Annalise A. LaPlume",
                "Reviewed (or Edited) by": "** Ali H. Al-Hoorie; Mahmoud Elsherif; Zoltan Kekecs; Charlotte R. Pennington",
                "Translation": "** When used in the context of measurement and testing, construct validity refers to the degree to which a test measures what it claims to be measuring. In fields that study hypothetical unobservable entities, construct validation is essentially theory testing, because it involves determining whether an objective measure (a questionnaire, lab task, etc.) is a valid representation of a hypothetical construct (i.e., conforms to a theory). When used in a broader sense about a research study, or a claim, conclusion, or observed effect in a research study, construct validity concerns the extent to which the sampling particulars used in the study (participants, settings, treatments, and dependent variables) map onto the higher order constructs the study, the claim, the conclusion is about. According to Shadish et al. (2002), construct validity can be defined as “the degree to which inferences are warranted from the observed persons, settings, and cause and effect operations included in a study to the constructs that these instances might represent” (p. 38).",
                "Related_terms": "** Measurement crisis; Measurement validity; Questionable Measurement Practices (QMP); Theory; Validity; Validation"
            },
            {
                "Title": "Content validity *",
                "Definition": "** The degree to which a measurement includes all aspects of the concept that the researcher claims to measure; “A qualitative type of validity where the domain of the concept is made clear and the analyst judges whether the measures fully represent the domain” (Bollen, 1989, p.185). It is a component of *construct validity* and can be established using both quantitative and qualitative methods, often involving expert assessment.",
                "Reference": "** Bollen (1989); Brod et al. (2009); Drost (2011); Haynes et al. (1995)",
                "Originally drafted by": "** Annalise A. LaPlume",
                "Reviewed (or Edited) by": "** Mahmoud Elsherif; Wanyin Li; Aoife O’Mahony; Eike Mark Rinke; Sam Parsons; Graham Reid",
                "Translation": "** The degree to which a measurement includes all aspects of the concept that the researcher claims to measure; “A qualitative type of validity where the domain of the concept is made clear and the analyst judges whether the measures fully represent the domain” (Bollen, 1989, p.185). It is a component of *construct validity* and can be established using both quantitative and qualitative methods, often involving expert assessment.",
                "Related_terms": "** Construct validity; Validity"
            },
            {
                "Title": "Contribution",
                "Reference": "** Knoth and Herrmannova (2014); Larivière et al. (2016); Holcombe (2019)",
                "Originally drafted by": "** Micah Vandegrift",
                "Reviewed (or Edited) by": "** Jamie P. Cockcroft; Dominik Kiersz; Michele C. Lim; Leticia Micheli; Sam Parsons; Gerald Vineyard",
                "Related_terms": "** authorship; CRediT; Semantometrics"
            },
            {
                "Title": "Corrigendum *",
                "Definition": "** A corrigendum (pl. corrigenda, Latin: 'to correct') documents one or multiple errors within a published work that do not alter the central claim or conclusions and thus does not rise to the standard of requiring a retraction of the work. Corrigenda are typically available alongside the original work to aid transparency. Some publishers refer to this document as an erratum (pl. errata, Latin: 'error'), while others draw a distinction between the two (corrigenda as author-errors and errata as publisher-errors).",
                "Reference": "** Correction or retraction? (2006)",
                "Originally drafted by": "** Charlotte R. Pennington",
                "Reviewed (or Edited) by": "** Bradley Baker; Nick Ballou; Wanyin Li; Adam Parker; Emily A. Williams",
                "Translation": "** A corrigendum (pl. corrigenda, Latin: 'to correct') documents one or multiple errors within a published work that do not alter the central claim or conclusions and thus does not rise to the standard of requiring a retraction of the work. Corrigenda are typically available alongside the original work to aid transparency. Some publishers refer to this document as an erratum (pl. errata, Latin: 'error'), while others draw a distinction between the two (corrigenda as author-errors and errata as publisher-errors).",
                "Related_terms": "** Correction; Errata; Retraction"
            },
            {
                "Title": "Co-production *",
                "Definition": "** An approach to research where stakeholders who are not traditionally involved in the research process are empowered to collaborate, either at the start of the project or throughout the research lifecycle. For example, co-produced health research may involve health professionals and patients, while co-produced education research may involve teaching staff and pupils/students. This is motivated by principles such as respecting and valuing the experiences of non-researchers, addressing power dynamics, and building mutually beneficial relationships.",
                "Reference": "** Filipe et al. (2017); Graham et al. (2019); NIHR (2021); Co-production Collective (2021)",
                "Originally drafted by": "** Emma Norris",
                "Reviewed (or Edited) by": "** Gisela H. Govaart; Magdalena Grose-Hodge; Helena Hartmann;Charlotte R. Pennington; Sonia Rishi; Emily A. Williams  ### ---",
                "Translation": "** An approach to research where stakeholders who are not traditionally involved in the research process are empowered to collaborate, either at the start of the project or throughout the research lifecycle. For example, co-produced health research may involve health professionals and patients, while co-produced education research may involve teaching staff and pupils/students. This is motivated by principles such as respecting and valuing the experiences of non-researchers, addressing power dynamics, and building mutually beneficial relationships.",
                "Related_terms": "** Citizen science; Collaboration; Collaborative research; Crowd science; Engaged scholarship; Integrated Knowledge Translation (IKT); Mode 2 of knowledge production; Participatory research; Patient and Public Involvement (PPI)"
            },
            {
                "Title": "Creative Commons (CC) license",
                "Definition": "** A set of free and easy-to-use copyright licences that define the rights of the authors and users of open data and materials in a standardized way. CC licenses enable authors or creators to share copyright-law-protected work with the public and come in different varieties with more or less clauses. For example, the Attribution-NonCommercial-ShareAlike 4.0 International (CC BY-NC-SA 4.0) allows you to share and adapt the material, under the condition that you; give credit to the original creators, indicate if changes were made, and share under the same license as the original, and you cannot use the material for commercial purposes.",
                "Reference(s)": "** https://creativecommons.org/about/cclicenses/",
                "Originally drafted by": "** Tina Lonsdorf",
                "Reviewed (or Edited) by": "** Adrien Fillon; Gisela H. Govaart; Annalise A. LaPlume; Sam Parsons; Charlotte R. Pennington",
                "Translation": "** A set of free and easy-to-use copyright licences that define the rights of the authors and users of open data and materials in a standardized way. CC licenses enable authors or creators to share copyright-law-protected work with the public and come in different varieties with more or less clauses. For example, the Attribution-NonCommercial-ShareAlike 4.0 International (CC BY-NC-SA 4.0) allows you to share and adapt the material, under the condition that you; give credit to the original creators, indicate if changes were made, and share under the same license as the original, and you cannot use the material for commercial purposes.",
                "Related_terms": "** Copyright; Licence **Alternative definition:** (if applicable) Creative Commons is an international nonprofit organization that provides Creative Commons licences, with the goal to minimize legal obstacles to the sharing of knowledge and creativity."
            },
            {
                "Title": "Credibility revolution *",
                "Definition": "** The problems and the solutions resulting from a growing distrust in scientific findings, following concerns about the credibility of scientific claims (e.g., low replicability). The term has been proposed as a more positive alternative to the term replicability crisis, and includes the many solutions to improve the credibility of research, such as preregistration, transparency, and replication.",
                "Reference": "** Angrist and Pischke (2010); Vazire (2018); Vazire et al. (2020)",
                "Originally drafted by": "** Tamara Kalandadze",
                "Reviewed (or Edited) by": "** Bradley Baker; Mahmoud Elsherif; Helena Hartmann; Kai Krautter; Annalise A. LaPlume; Oscar Lecuona; Charlotte R. Pennington; Robert Ross; Tobias Wingen; Flávio Azevedo",
                "Translation": "** The problems and the solutions resulting from a growing distrust in scientific findings, following concerns about the credibility of scientific claims (e.g., low replicability). The term has been proposed as a more positive alternative to the term replicability crisis, and includes the many solutions to improve the credibility of research, such as preregistration, transparency, and replication.",
                "Related_terms": "** Credibility of scientific claims; High standards of evidence; Openness; Open Science;Reproducibility crisis (aka Replicability or replication crisis); Transparency"
            },
            {
                "Title": "Creative destruction approach to replication *",
                "Definition": "** Replication efforts should seek not just to support or question the original findings, but also to replace them with revised, stronger theories with greater explanatory power. This approach therefore involves ‘pruning’ existing theories, comparing all the alternative theories, and making replication efforts more generative and engaged in theory-building (Tierney et al. 2020, 2021).",
                "Reference(s)": "** Tierney et al. (2020, 2021\\)",
                "Drafted by": "** Mahmoud Elsherif",
                "Reviewed (or Edited) by": "** Magdalena Grose-Hodge; Aoife O’Mahony; Adam Parker; Charlotte R. Pennington; Sonia Rishi; Beatrice Valentini",
                "Translation": "** Replication efforts should seek not just to support or question the original findings, but also to replace them with revised, stronger theories with greater explanatory power. This approach therefore involves ‘pruning’ existing theories, comparing all the alternative theories, and making replication efforts more generative and engaged in theory-building (Tierney et al. 2020, 2021).",
                "Related_terms": "** Crowdsourced research; Falsification; Replication; Theory"
            },
            {
                "Title": "CRediT",
                "Definition": "** The Contributor Roles Taxonomy (CRediT; [https://casrai.org/credit/](https://casrai.org/credit/)) is a high-level taxonomy used to indicate the roles typically adopted by contributors to scientific scholarly output. There are currently 14 roles that describe each contributor’s specific contribution to the scholarly output. They can be assigned multiple times to different authors and one author can also be assigned multiple roles. CRediT includes the following roles: Conceptualization, Data curation, Formal Analysis, Funding acquisition, Investigation, Methodology, Project administration, Resources, Software, Supervision, Validation, Visualization, Writing – original draft, Writing – review & editing. A description of the different roles can be found in the work of Brand et al., (2015).",
                "Reference(s)": "** Brand et al. (2015); Holcombe (2019); [https://casrai.org/credit/](https://casrai.org/credit/)",
                "Drafted by": "** Sam Parsons",
                "Reviewed (or Edited) by": "** Myriam A. Baum; Matt Jaquiery; Tamara Kalandadze; Connor Keating; Charlotte R. Pennington; Yuki Yamada; Flávio Azevedo",
                "Translation": "** The Contributor Roles Taxonomy (CRediT; [https://casrai.org/credit/](https://casrai.org/credit/)) is a high-level taxonomy used to indicate the roles typically adopted by contributors to scientific scholarly output. There are currently 14 roles that describe each contributor’s specific contribution to the scholarly output. They can be assigned multiple times to different authors and one author can also be assigned multiple roles. CRediT includes the following roles: Conceptualization, Data curation, Formal Analysis, Funding acquisition, Investigation, Methodology, Project administration, Resources, Software, Supervision, Validation, Visualization, Writing – original draft, Writing – review & editing. A description of the different roles can be found in the work of Brand et al., (2015).",
                "Related_terms": "** Authorship; Contributions"
            },
            {
                "Title": "Criterion validity *",
                "Definition": "** The degree to which a measure corresponds to other valid measures of the same concept. Criterion validity is usually established by calculating regression coefficients or bivariate correlations estimating the direction and strength of relation between test measure and criterion measure. It is often confused with *construct validity* although it differs from it in intent (merely predictive rather than theoretical) and interest (predicting an observable outcome rather than a latent construct). Unreliability in either test or criterion scores usually diminishes criterion validity. Also called criterion-related or concrete validity.",
                "Reference": "** DeVellis (2017); Drost (2011)",
                "Originally drafted by": "** Annalise A. LaPlume",
                "Reviewed (or Edited) by": "** Helena Hartmann; Kai Krautter; Sam Parsons; Eike Mark Rinke",
                "Translation": "** The degree to which a measure corresponds to other valid measures of the same concept. Criterion validity is usually established by calculating regression coefficients or bivariate correlations estimating the direction and strength of relation between test measure and criterion measure. It is often confused with *construct validity* although it differs from it in intent (merely predictive rather than theoretical) and interest (predicting an observable outcome rather than a latent construct). Unreliability in either test or criterion scores usually diminishes criterion validity. Also called criterion-related or concrete validity.",
                "Related_terms": "** Construct validity; Validity"
            },
            {
                "Title": "Crowdsourced Research *",
                "Definition": "** Crowdsourced research is a model of the social organisation of research as a large-scale collaboration in which one or more research projects are conducted by multiple teams in an independent yet coordinated manner. Crowdsourced research aims at achieving efficiency and scalability gains by pooling resources, promoting transparency and social inclusion, as well as increasing the rigor, reliability, and trustworthiness by enhancing statistical power and mutual social vetting. It stands in contrast to the traditional model of academic research production, which is dominated by the independent work of individual or small groups of researchers (‘small science’). Examples of crowdsourced research include so-called ‘many labs replication’ studies (Klein et al., 2018), ‘many analysts, one dataset’ studies (Silberzahn et al., 2018), distributive collaborative networks (Moshontz et al., 2018\\) and open collaborative writing projects such as Massively Open Online Papers (MOOPs) (Himmelstein et al., 2019; Tennant et al., 2019). Alternatively, crowdsourced research can refer to the use of a large number of research “crowdworkers” in data collection hired through online labor markets like Amazon Mechanical Turk or Prolific, for example in content analysis (Benoit et al., 2016; Lind et al., 2017\\) or experimental research (Peer et al., 2017). Crowdsourced research that is both open for participation and open through shared intermediate outputs has been referred to as *crowd science* (Franzoni & Sauermann, 2014).",
                "Reference": "** Benoit et al. (2016); Breznau (2021); Franzoni and Sauermann (2014); Himmelstein et al. (2019); Klein et al. (2018); Lind et al. (2017); Moshontz et al. (2018); Peer et al. (2017); Silberzahn et al. (2018); Stewart et al. (2017); Tennant et al. (2019); Uhlmann et al. (2019); [https://psysciacc.org/](https://psysciacc.org/); [https://crowdsourcingweek.com/what-is-crowdsourcing/](https://crowdsourcingweek.com/what-is-crowdsourcing/#:~:text=Crowdsourcing%20is%20the%20practice%20of,levels%20and%20across%20various%20industries)",
                "Originally drafted by": "** Eike Mark Rinke",
                "Reviewed (or Edited) by": "** Ali H. Al-Hoorie; Sam Parsons; Charlotte R. Pennington; Suzanne L. K. Stewart; Flávio Azevedo",
                "Translation": "** Crowdsourced research is a model of the social organisation of research as a large-scale collaboration in which one or more research projects are conducted by multiple teams in an independent yet coordinated manner. Crowdsourced research aims at achieving efficiency and scalability gains by pooling resources, promoting transparency and social inclusion, as well as increasing the rigor, reliability, and trustworthiness by enhancing statistical power and mutual social vetting. It stands in contrast to the traditional model of academic research production, which is dominated by the independent work of individual or small groups of researchers (‘small science’). Examples of crowdsourced research include so-called ‘many labs replication’ studies (Klein et al., 2018), ‘many analysts, one dataset’ studies (Silberzahn et al., 2018), distributive collaborative networks (Moshontz et al., 2018\\) and open collaborative writing projects such as Massively Open Online Papers (MOOPs) (Himmelstein et al., 2019; Tennant et al., 2019). Alternatively, crowdsourced research can refer to the use of a large number of research “crowdworkers” in data collection hired through online labor markets like Amazon Mechanical Turk or Prolific, for example in content analysis (Benoit et al., 2016; Lind et al., 2017\\) or experimental research (Peer et al., 2017). Crowdsourced research that is both open for participation and open through shared intermediate outputs has been referred to as *crowd science* (Franzoni & Sauermann, 2014).",
                "Related_terms": "** Citizen science; Collaboration; Crowdsourcing; Team science"
            },
            {
                "Title": "Cultural taxation *",
                "Definition": "** The additional labor expected or demanded of members of underrepresented or marginalized minority groups, particularly scholars of color. This labor often comes from service roles providing ethnic, cultural, or gender representation and diversity. These roles can be formal or informal, and are generally unrewarded or uncompensated. Such labor includes providing expertise on matters of diversity, educating members of majority groups, acting as a liaison to minority communities, and formal and informal roles as mentor and support system for minority students.",
                "Reference(s)": "** Joseph and Hirschfeld (2011); Ledgerwood et al. (2021); Padilla (1994)",
                "Originally drafted by": "** Bradley Baker",
                "Reviewed (or Edited) by": "** Helena Hartmann; Bethan Iley; Aoife O’Mahony; Charlotte R. Pennington; Flávio Azevedo",
                "Translation": "** The additional labor expected or demanded of members of underrepresented or marginalized minority groups, particularly scholars of color. This labor often comes from service roles providing ethnic, cultural, or gender representation and diversity. These roles can be formal or informal, and are generally unrewarded or uncompensated. Such labor includes providing expertise on matters of diversity, educating members of majority groups, acting as a liaison to minority communities, and formal and informal roles as mentor and support system for minority students.",
                "Related_terms": "** Invisible labor; Power imbalances; Power relations"
            },
            {
                "Title": "Cumulative science *",
                "Definition": "** Goal of any empirical science, it is the pursuit of “the construction of a cumulative base of knowledge upon which the future of the science may be built” (Curran, 2009, p. 1). The idea that science will create more complete and accurate theories as a function of the amount of evidence and data that has been collected. Cumulative science develops in gradual and incremental steps, as opposed to one abrupt discovery. While revolutionary science occurs scarcely, cumulative science is the most common form of science.",
                "Reference": "** Curran (2009); d’Espagnat (2008); Kuhn (1962); Mischel (2008)",
                "Originally drafted by": "** Beatrice Valentini",
                "Reviewed (or Edited) by": "** Sarah Ashcroft-Jones; Mahmoud Elsherif; Helena Hartmann; Oscar Lecuona; Wanyin Li; Sonia Rishi; Flávio Azevedo  ###  ### **D** {#d}",
                "Translation": "** Goal of any empirical science, it is the pursuit of “the construction of a cumulative base of knowledge upon which the future of the science may be built” (Curran, 2009, p. 1). The idea that science will create more complete and accurate theories as a function of the amount of evidence and data that has been collected. Cumulative science develops in gradual and incremental steps, as opposed to one abrupt discovery. While revolutionary science occurs scarcely, cumulative science is the most common form of science.",
                "Related_terms": "** Slow Science"
            },
            {
                "Title": "Data Access and Research Transparency (DA-RT) *",
                "Definition": "** Data Access and Research Transparency ([DA-RT](https://www.dartstatement.org/)) is an initiative aimed at increasing data access and research transparency in the social sciences. It is a multi-epistemic and multi-method initiative, created in 2014 by the Council of the American Political Science Association (APSA), to bolster the rigor of empirical social inquiry. In addition to other activities, DA-RT developed the Journal Editors' Transparency Statement (JETS), which requires subscribing journals to (a) making relevant data publicly available if the study is published, (b) following a strict data citation policy, (c) transparently describing the analytical procedures and, if possible, providing public access to analytical code, and (d) updating their journal style guides, codes of ethics to include improved data access and research transparency requirements.",
                "Reference": "** Carsey (2014); Monroe (2018)",
                "Originally drafted by": "** Eike Mark Rinke",
                "Reviewed (or Edited) by": "** Filip Dechterenko; Kai Krautter; Flávio Azevedo",
                "Translation": "** Data Access and Research Transparency ([DA-RT](https://www.dartstatement.org/)) is an initiative aimed at increasing data access and research transparency in the social sciences. It is a multi-epistemic and multi-method initiative, created in 2014 by the Council of the American Political Science Association (APSA), to bolster the rigor of empirical social inquiry. In addition to other activities, DA-RT developed the Journal Editors' Transparency Statement (JETS), which requires subscribing journals to (a) making relevant data publicly available if the study is published, (b) following a strict data citation policy, (c) transparently describing the analytical procedures and, if possible, providing public access to analytical code, and (d) updating their journal style guides, codes of ethics to include improved data access and research transparency requirements.",
                "Related_terms": "** Accessibility; Data sharing; Replicability; Reproducibility"
            },
            {
                "Title": "Data management plan (DMP) *",
                "Definition": "** A structured document that describes the process of data acquisition, analysis, management and storage during a research project. It also describes data ownership and how the data will be preserved and shared during and upon completion of a project. Data management templates also provide guidance on how to make research data FAIR and where possible, openly available.",
                "Reference(s)": "** Burnette et al. (2016); Michener (2015); Research Data Alliance (2020); https://library.stanford.edu/research/data-management-services/data-management-plans\\#:\\~:text=A%20data%20management%20plan%20(DMP,share%20and%20preserve%20your%20data.",
                "Drafted by": "** Dominique Roche",
                "Reviewed (or Edited) by": "** Charlotte R. Pennington; Sam Parsons; Birgit Schmidt; Flávio Azevedo",
                "Translation": "** A structured document that describes the process of data acquisition, analysis, management and storage during a research project. It also describes data ownership and how the data will be preserved and shared during and upon completion of a project. Data management templates also provide guidance on how to make research data FAIR and where possible, openly available.",
                "Related_terms": "** Data archiving; Data sharing; Data storage; FAIR principles; Open data"
            },
            {
                "Title": "Data sharing *",
                "Definition": "** collection of practices, technologies, cultural elements and legal frameworks that are relevant to the practice of making data used for scholarly research available to other investigators. Gollwitzer et al. (2020) describe two types of data sharing: Type 1: Data that is necessary to reproduce the findings of a published research article. Type 2: data that have been collected in a research project but have not (or only partly) been analysed or reported after the completion of the project and are hence typically shared under a specified embargo period.",
                "Reference(s)": "** Abele-Brehm et al. (2019); Gollwitzer et al. (2020); https://eudatasharing.eu/what-data-sharing",
                "Drafted by": "** Mahmoud Elsherif",
                "Reviewed (or Edited) by": "** Helena Hartmann; Tina Lonsdorf; Charlotte R. Pennington; Timo Roettger; Flávio Azevedo",
                "Translation": "** collection of practices, technologies, cultural elements and legal frameworks that are relevant to the practice of making data used for scholarly research available to other investigators. Gollwitzer et al. (2020) describe two types of data sharing: Type 1: Data that is necessary to reproduce the findings of a published research article. Type 2: data that have been collected in a research project but have not (or only partly) been analysed or reported after the completion of the project and are hence typically shared under a specified embargo period.",
                "Related_terms": "** FAIR principles; Open data"
            },
            {
                "Title": "Data visualisation *",
                "Definition": "** Graphical representation of data or information. Data visualisation takes advantage of humans’ well-developed visual processing capacity to convey insight and communicate key information. Data visualisations often display the raw data, descriptive statistics, and/or inferential statistics.",
                "Reference(s)": "** Healy (2018); Tufte (1983)",
                "Drafted by": "** Bradley Baker",
                "Reviewed (or Edited) by": "** Mahmoud Elsherif; Charlotte R. Pennington; Suzanne L. K. Stewart;",
                "Translation": "** Graphical representation of data or information. Data visualisation takes advantage of humans’ well-developed visual processing capacity to convey insight and communicate key information. Data visualisations often display the raw data, descriptive statistics, and/or inferential statistics.",
                "Related_terms": "** Figure; Graph; Plot"
            },
            {
                "Title": "Decolonisation *",
                "Definition": "** Coloniality can be described as the naturalisation of concepts such as imperialism, capitalism, and nationalism. Together these concepts can be thought of as a matrix of power (and power relations) that can be traced to the colonial period. Decoloniality seeks to break down and decentralize those power relations, with the aim to understand their persistence and to reconstruct the norms and values of a given domain. In an academic setting, decolonisation refers to the rethinking of the lens through which we teach, research, and co-exist, so that the lens generalises beyond Western-centred and colonial perspectives. Decolonising academia involves reconstructing the historical and cultural frameworks being used, redistributing a sense of belonging in universities, and empowering and including voices and knowledge types that have historically been excluded from academia. This is done when people engage with their past, present, and future whilst holding a perspective that is separate from the socially dominant perspective. Also, by including, not rejecting, an individuals’ internalised norms and taboos from the specific colony.",
                "Reference(s)": "** Albayrak (2018)",
                "Drafted by": "** Nihan Albayrak-Aydemir",
                "Reviewed (or Edited) by": "** Sarah Ashcroft-Jones; Mahmoud Elsherif; Michele C. Lim; Emma Norris; Flávio Azevedo",
                "Translation": "** Coloniality can be described as the naturalisation of concepts such as imperialism, capitalism, and nationalism. Together these concepts can be thought of as a matrix of power (and power relations) that can be traced to the colonial period. Decoloniality seeks to break down and decentralize those power relations, with the aim to understand their persistence and to reconstruct the norms and values of a given domain. In an academic setting, decolonisation refers to the rethinking of the lens through which we teach, research, and co-exist, so that the lens generalises beyond Western-centred and colonial perspectives. Decolonising academia involves reconstructing the historical and cultural frameworks being used, redistributing a sense of belonging in universities, and empowering and including voices and knowledge types that have historically been excluded from academia. This is done when people engage with their past, present, and future whilst holding a perspective that is separate from the socially dominant perspective. Also, by including, not rejecting, an individuals’ internalised norms and taboos from the specific colony.",
                "Related_terms": "** Diversity; Equity; Inclusion"
            },
            {
                "Title": "Demarcation criterion **",
                "Definition": "** A criterion for distinguishing science from non-science which aims to indicate an optimal way for knowledge of the world to grow. In a Popperian approach, the demarcation criterion was falsifiability and the application of a falsificationist attitude. Alternative approaches include that of Kuhn, who believed that the criterion was puzzle solving with the aim of understanding nature, and Lakatos, who argued that science is marked by working within a progressive research programme.",
                "Reference(s)": "** Dienes (2008)",
                "Drafted by": "** Alaa AlDoh",
                "Reviewed (or Edited) by": "** Bethan Iley; Sara Middleton",
                "Translation": "** A criterion for distinguishing science from non-science which aims to indicate an optimal way for knowledge of the world to grow. In a Popperian approach, the demarcation criterion was falsifiability and the application of a falsificationist attitude. Alternative approaches include that of Kuhn, who believed that the criterion was puzzle solving with the aim of understanding nature, and Lakatos, who argued that science is marked by working within a progressive research programme.",
                "Related_terms": "** Hypothesis; Falsification"
            },
            {
                "Title": "DOI (digital object identifier) *",
                "Definition": "** Digital Object Identifiers (DOI) are alpha-numeric strings that can be assigned to any entity, including: publications (including preprints), materials, datasets, and feature films \\- the use of DOIs is not restricted to just scholarly or academic material. DOIs “provides a system for persistent and actionable identification and interoperable exchange of managed information on digital networks.” ([https://doi.org/hb.html](https://doi.org/hb.html)). There are many different DOI registration agencies that operate DOIs, but the two that researchers would most likely encounter are [Crossref](https://www.crossref.org/) and [Datacite](https://datacite.org/).",
                "Reference": "** Bilder (2013); Morgan (1998); [https://www.doi.org/hb.html](https://www.doi.org/hb.html)",
                "Originally drafted by": "** Tina Lonsdorf",
                "Reviewed (or Edited) by": "** Ashley Blake; Helena Hartmann; Sam Parsons; Charlotte R. Pennington; Flávio Azevedo",
                "Translation": "** Digital Object Identifiers (DOI) are alpha-numeric strings that can be assigned to any entity, including: publications (including preprints), materials, datasets, and feature films \\- the use of DOIs is not restricted to just scholarly or academic material. DOIs “provides a system for persistent and actionable identification and interoperable exchange of managed information on digital networks.” ([https://doi.org/hb.html](https://doi.org/hb.html)). There are many different DOI registration agencies that operate DOIs, but the two that researchers would most likely encounter are [Crossref](https://www.crossref.org/) and [Datacite](https://datacite.org/).",
                "Related_terms": "** arXiv and BibTex; Crossref, Datacite, ISBN, ISO, ORCID; Permalink"
            },
            {
                "Title": "Double-blind peer review *",
                "Definition": "** Evaluation of research products by qualified experts where both the author(s) and reviewer(s) are anonymous to each other. “This approach conceals the identity of the authors and their affiliations from reviewers and would, in theory, remove biases of professional reputation, gender, race, and institutional affiliation, allowing the reviewer to avoid bias and to focus on the manuscript’s merit alone.” (Tvina et al., 2019, 1082). Like all types of peer-review, double-blind peer review is not without flaws. Anonymity can be difficult, if not impossible, to achieve for certain researchers working in a niche area.",
                "Reference(s)": "** Largent and Snodgrass (2016); Tvina et al. (2019)",
                "Drafted by": "** Mahmoud Elsherif",
                "Reviewed (or Edited) by": "** Bradley Baker; Helena Hartmann; Meng Liu; Emma Norris",
                "Translation": "** Evaluation of research products by qualified experts where both the author(s) and reviewer(s) are anonymous to each other. “This approach conceals the identity of the authors and their affiliations from reviewers and would, in theory, remove biases of professional reputation, gender, race, and institutional affiliation, allowing the reviewer to avoid bias and to focus on the manuscript’s merit alone.” (Tvina et al., 2019, 1082). Like all types of peer-review, double-blind peer review is not without flaws. Anonymity can be difficult, if not impossible, to achieve for certain researchers working in a niche area.",
                "Related_terms": "** Ad hominem bias; Affiliation bias; Anonymous review; Masked review; Open peer review; Peer review; Single-blind peer review; Traditional peer review; Triple-Blind peer review"
            },
            {
                "Title": "Double consciousness *",
                "Definition": "** An identity confusion, as the individual feels like they have two distinct identities. One is to assimilate to the dominant culture at university when the individual is with colleagues and professors, while the other is when the individual is with their families. This continuous shift may cause a lack of certainty about the individual’s identity and a belief that the individual does not fully belong anywhere. This lack of belonging can lead to poor social integration within the academic culture that can manifest in less opportunities and more mental health issues in the individual (Rubin, 2021; Rubin et al., 2019).",
                "Reference(s)": "** Albayrak and Okoroji (2019); Du Bois (1968); Gilroy (1993)",
                "Drafted by": "** Nihan Albayrak-Aydemir",
                "Reviewed (or Edited) by": "** Mahmoud Elsherif; Wanyin Li; Michele C. Lim; Adam Parker",
                "Translation": "** An identity confusion, as the individual feels like they have two distinct identities. One is to assimilate to the dominant culture at university when the individual is with colleagues and professors, while the other is when the individual is with their families. This continuous shift may cause a lack of certainty about the individual’s identity and a belief that the individual does not fully belong anywhere. This lack of belonging can lead to poor social integration within the academic culture that can manifest in less opportunities and more mental health issues in the individual (Rubin, 2021; Rubin et al., 2019).",
                "Related_terms": "** Social class; Social integration"
            },
            {
                "Title": "DORA *",
                "Definition": "** The San Francisco Declaration on Research Assessment (DORA) is a global initiative aiming to reduce dependence on journal-based metrics (e.g. journal impact factor and citation counts) and, instead, promote a culture which emphasises the intrinsic value of research. The DORA declaration targets research funders, publishers, research institutes and researchers and signing it represents a commitment to aligning research practices and procedures with the declaration’s principles.",
                "Reference(s)": "** Health Research Board (n.d.); [https://sfdora.org/](https://sfdora.org/)",
                "Drafted by": "** Aoife O’Mahony",
                "Reviewed (or Edited) by": "** Connor Keating; Charlotte R. Pennington",
                "Translation": "** The San Francisco Declaration on Research Assessment (DORA) is a global initiative aiming to reduce dependence on journal-based metrics (e.g. journal impact factor and citation counts) and, instead, promote a culture which emphasises the intrinsic value of research. The DORA declaration targets research funders, publishers, research institutes and researchers and signing it represents a commitment to aligning research practices and procedures with the declaration’s principles.",
                "Related_terms": "** Generalizability; Journal Impact Factor; Open Science"
            },
            {
                "Title": "Direct replication *",
                "Definition": "** As ‘direct replication’ does not have a widely-agreed technical meaning nor there is no clear cut distinction between a direct and conceptual replication, below we list several contributions towards a consensus. Rather than debating the ‘exactness’ of a replication, it is more helpful to discuss the relevant differences between a replication and its target, and their implications for the reliability and generality of the target’s results. Generally, direct replication refers to a new data collection that attempts to replicate original studies’ methods as closely as possible. A replication attempt that “seek(s) to duplicate the necessary elements that produced the original finding.” (Cruwell et al., 2019; p.243). The purpose of a direct replication can be to identify type 1 errors and/or experimenter effects, determine the replicability of an effect using the same or improved practices, or to create more specific estimates of effect size (Hűffmeier et al., 2016). Directness of replication is a continuum between repeating specific observations (data) and observing generalised effects (phenomena). How closely a replication replicates an original study is often a matter for debate, often with differences being cited as hidden moderators of effects. Furthermore, there can be debate over the relevant importance of technical equivalence (i.e., using identical materials) versus psychological equivalence (i.e., realizing the identical psychological conditions) to the original study (Schwarz and Strack, 2014). For example, consider a study on Trust in the US- President conducted in 2018\\. A technical equivalent replication would use Trump as stimulus (he was president in 2018\\) a psychological equivalent study would use Biden (he is the current president).",
                "Reference(s)": "** Crüwell et al. (2019); Hüffmeier et al. (2016); LeBel et al. (2019); Schwarz and Strack (2014)",
                "Drafted by": "**Mahmoud Elsherif (original); Thomas Rhys Evans (alternative); Tina Lonsdorf (alternative)",
                "Reviewed (or Edited) by": "** Beatrix Arendt; Adrien Fillon; Matt Jaquiery; Charlotte R. Pennington; Graham Reid; Lisa Spitzer; Tobias Wingen; Flávio Azevedo",
                "Translation": "** As ‘direct replication’ does not have a widely-agreed technical meaning nor there is no clear cut distinction between a direct and conceptual replication, below we list several contributions towards a consensus. Rather than debating the ‘exactness’ of a replication, it is more helpful to discuss the relevant differences between a replication and its target, and their implications for the reliability and generality of the target’s results. Generally, direct replication refers to a new data collection that attempts to replicate original studies’ methods as closely as possible. A replication attempt that “seek(s) to duplicate the necessary elements that produced the original finding.” (Cruwell et al., 2019; p.243). The purpose of a direct replication can be to identify type 1 errors and/or experimenter effects, determine the replicability of an effect using the same or improved practices, or to create more specific estimates of effect size (Hűffmeier et al., 2016). Directness of replication is a continuum between repeating specific observations (data) and observing generalised effects (phenomena). How closely a replication replicates an original study is often a matter for debate, often with differences being cited as hidden moderators of effects. Furthermore, there can be debate over the relevant importance of technical equivalence (i.e., using identical materials) versus psychological equivalence (i.e., realizing the identical psychological conditions) to the original study (Schwarz and Strack, 2014). For example, consider a study on Trust in the US- President conducted in 2018\\. A technical equivalent replication would use Trump as stimulus (he was president in 2018\\) a psychological equivalent study would use Biden (he is the current president).",
                "Related_terms": "** close replication; Conceptual replication; exact replication; hidden moderators"
            },
            {
                "Title": "Diversity *",
                "Definition": "** Diversity refers to between-person (i.e., interindividual) variation in humans, e.g. ability, age, beliefs, cognition, country, disability, ethnicity, gender, language, race, religion or sexual orientation. Diversity can refer to diversity of researchers (who do the research), the diversity of participant samples (who is included in the study), and diversity of perspectives (the views and beliefs researchers bring into their work; Syed & Kathawalla, 2020).",
                "Reference(s)": "** Syed and Kathawalla (2020)",
                "Drafted by": "** Ryan Millager; Mariella Paul",
                "Reviewed (or Edited) by": "** Nihan Albayrak-Aydemir; Mahmoud Elsherif; Helena Hartmann; Madeleine Ingham; Annalise A. LaPlume; Wanyin Li; Charlotte R. Pennington; Olly Robertson; Flávio Azevedo  ### **E** {#e}",
                "Translation": "** Diversity refers to between-person (i.e., interindividual) variation in humans, e.g. ability, age, beliefs, cognition, country, disability, ethnicity, gender, language, race, religion or sexual orientation. Diversity can refer to diversity of researchers (who do the research), the diversity of participant samples (who is included in the study), and diversity of perspectives (the views and beliefs researchers bring into their work; Syed & Kathawalla, 2020).",
                "Related_terms": "** Bropenscience; BIZARRE; Decolonisation; Double Consciousness; Equity; Inclusion; STRANGE; WEIRD"
            },
            {
                "Title": "Early career researchers (ECRs) *",
                "Definition": "** A label given to researchers who “range from senior doctoral students to postdoctoral workers who may have up to 10 years postdoctoral education; the latter group may therefore include early career or junior academics” (Eley et al., 2012, p. 3). What specifically (e.g. age, time since PhD inclusive or exclusive of career breaks and leave, title, funding awarded) constitutes an ECR can vary across funding bodies, academic organisations, and countries.",
                "Reference(s)": "** Bazeley (2003); Eley et al. (2012); Pownall et al (2021)",
                "Drafted by": "** Micah Vandegrift",
                "Reviewed (or Edited) by": "** Thomas Rhys Evans; Sam Parsons; Olly Robertson; Suzanne L. K. Stewart; Flávio Azevedo",
                "Translation": "** A label given to researchers who “range from senior doctoral students to postdoctoral workers who may have up to 10 years postdoctoral education; the latter group may therefore include early career or junior academics” (Eley et al., 2012, p. 3). What specifically (e.g. age, time since PhD inclusive or exclusive of career breaks and leave, title, funding awarded) constitutes an ECR can vary across funding bodies, academic organisations, and countries.",
                "Related_terms": "** Early Career Investigator"
            },
            {
                "Title": "Economic and societal impact *",
                "Definition": "** The contribution a research item makes to the broader economy and society. It also captures the benefits of research to individuals, organisations, and/or nations.",
                "Reference": "** https://esrc.ukri.org/research/impact-toolkit/what-is-impact/",
                "Originally drafted by": "** Adam Parker",
                "Reviewed (or Edited) by": "** Helena Hartmann; Aoife O’Mahony; Charlotte R. Pennington",
                "Translation": "** The contribution a research item makes to the broader economy and society. It also captures the benefits of research to individuals, organisations, and/or nations.",
                "Related_terms": "** Academic Impact"
            },
            {
                "Title": "Embargo Period *",
                "Definition": "** Applied to Open Scholarship, in academic publishing, the period of time after an article has been published and before it can be made available as Open Access. If an author decides to self-archive their article (e.g., in an Open Access repository) they need to observe any embargo period a publisher might have in place. Embargo periods vary from instantaneous up to 48 months, with 6 and 12 months being common (Laakso & Björk, 2013). Embargo periods may also apply to pre-registrations, materials, and data, when authors decide to only make these available to the public after a certain period of time, for instance upon publication or even later when they have additional publication plans and want to avoid being scooped (Klein et al., 2018).",
                "Reference": "** Klein et al. (2018), Laakso and Björk (2013); [https://en.wikipedia.org/wiki/Embargo\\_(academic\\_publishing)](https://en.wikipedia.org/wiki/Embargo_\\(academic_publishing\\))",
                "Originally drafted by": "** Aleksandra Lazić",
                "Reviewed (or Edited) by": "** Bradley Baker; Adam Parker; Sam Parsons; Steven Verheyen; Flávio Azevedo",
                "Translation": "** Applied to Open Scholarship, in academic publishing, the period of time after an article has been published and before it can be made available as Open Access. If an author decides to self-archive their article (e.g., in an Open Access repository) they need to observe any embargo period a publisher might have in place. Embargo periods vary from instantaneous up to 48 months, with 6 and 12 months being common (Laakso & Björk, 2013). Embargo periods may also apply to pre-registrations, materials, and data, when authors decide to only make these available to the public after a certain period of time, for instance upon publication or even later when they have additional publication plans and want to avoid being scooped (Klein et al., 2018).",
                "Related_terms": "** Open access; Paywall; Preprint"
            },
            {
                "Title": "Epistemic uncertainty",
                "Definition": "** Systematic uncertainty due to limited data, measurement precision, model or process specification, or lack of knowledge. That is, uncertainty due to lack of knowledge that could, in theory, be reduced through conducting additional research to increase understanding. Such uncertainty is said to be personal, since knowledge differs across scientists, and temporary since it can change as new data become available.",
                "Reference(s)": "** Der Kiureghian and Ditlevsen (2009); Ferson et al., (2004) **Alternative terms:** Epistemic uncertainty is also known as knowledge uncertainty, subjective uncertainty, or type B uncertainty.",
                "Drafted by": "** Bradley Baker",
                "Reviewed (or Edited) by": "** Jamie P. Cockcroft; Elizabeth Collins; Charlotte R. Pennington; Graham Reid",
                "Translation": "** Systematic uncertainty due to limited data, measurement precision, model or process specification, or lack of knowledge. That is, uncertainty due to lack of knowledge that could, in theory, be reduced through conducting additional research to increase understanding. Such uncertainty is said to be personal, since knowledge differs across scientists, and temporary since it can change as new data become available.",
                "Related_terms": "** Aleatoric uncertainty; Knightian uncertainty"
            },
            {
                "Title": "Epistemology",
                "Definition": "** Alongside ethics, logic, and metaphysics, epistemology is one of the four main branches of philosophy. Epistemology is largely concerned with nature, origin, and scope of knowledge, as well as the rationality of beliefs.",
                "Reference": "** Steup et al. (2020)",
                "Originally drafted by": "** Amélie Beffara Bret",
                "Reviewed (or Edited) by": "** Emma Norris; Adam Parker; Robert M Ross; Steven Verheyen",
                "Translation": "** Alongside ethics, logic, and metaphysics, epistemology is one of the four main branches of philosophy. Epistemology is largely concerned with nature, origin, and scope of knowledge, as well as the rationality of beliefs.",
                "Related_terms": "** Meta-science or Meta-research ; Ontology (Artificial Intelligence)"
            },
            {
                "Title": "Equity *",
                "Definition": "** Different individuals have different starting positions (cf. “opportunity gaps”) and needs. Whereas equal treatment focuses on treating all individuals *equally*, equitable treatment aims to level the playing field by actively increasing opportunities for under-represented minorities. Equitable treatment aims to attain equality through “fairness”: taking into account different needs for support for different individuals, instead of focusing merely on the needs of the majority.",
                "Reference(s)": "** Albayrak-Aydemir (2020); Posselt (2020)",
                "Drafted by": "** Gisela H. Govaart",
                "Reviewed (or Edited) by": "** Nihan Albayrak-Aydemir; Mahmoud Elsherif; Ryan Millager; Charlotte R. Pennington; Beatrice Valentini; Flávio Azevedo",
                "Translation": "** Different individuals have different starting positions (cf. “opportunity gaps”) and needs. Whereas equal treatment focuses on treating all individuals *equally*, equitable treatment aims to level the playing field by actively increasing opportunities for under-represented minorities. Equitable treatment aims to attain equality through “fairness”: taking into account different needs for support for different individuals, instead of focusing merely on the needs of the majority.",
                "Related_terms": "** Diversity; Equality; Fairness; Inclusion; Social justice"
            },
            {
                "Title": "Equivalence Testing *",
                "Definition": "** Equivalence tests statistically assess the null hypothesis that a given effect exceeds a minimum criterion to be considered meaningful. Thus, rejection of the null hypothesis provides evidence of a lack of (meaningful) effect. Based upon frequentist statistics, equivalence tests work by specifying equivalence bounds: a lower and upper value that reflect the smallest effect size of interest. Two one-sided *t*\\-tests are then conducted against each of these equivalence bounds to assess whether effects that are deemed meaningful can be *rejected* (see Schuirmann, 1972; Lakens et al., 2018; 2020).",
                "Reference": "** Lakens et al. (2018); Lakens et al. (2020); Schuirmann (1987)",
                "Originally drafted by": "** Charlotte R. Pennington",
                "Reviewed (or Edited) by": "** Bradley Baker; James E. Bartlett; Jamie P. Cockcroft; Tobias Wingen; Flávio Azevedo",
                "Translation": "** Equivalence tests statistically assess the null hypothesis that a given effect exceeds a minimum criterion to be considered meaningful. Thus, rejection of the null hypothesis provides evidence of a lack of (meaningful) effect. Based upon frequentist statistics, equivalence tests work by specifying equivalence bounds: a lower and upper value that reflect the smallest effect size of interest. Two one-sided *t*\\-tests are then conducted against each of these equivalence bounds to assess whether effects that are deemed meaningful can be *rejected* (see Schuirmann, 1972; Lakens et al., 2018; 2020).",
                "Related_terms": "** Equivalence bounds; Falsification; Frequentist analyses; Inference by confidence intervals; Null Hypothesis Significance Testing (NHST); Smallest effect size of interest (SESOI); TOSTER; TOST procedure."
            },
            {
                "Title": "Error detection",
                "Definition": "** Broadly refers to examining research data and manuscripts for mistakes or inconsistencies in reporting. Commonly discussed approaches include: checking inconsistencies in descriptive statistics (e.g. summary statistics that are not possible given the sample size and measure characteristics; Brown & Heathers, 2017; Heathers et al. 2018), inconsistencies in reported statistics (e.g. *p*\\-values that do not match the reported *F* statistics and accompanying degrees of freedom; Epskamp, & Nuijten, 2016; Nuijten et al. 2016), and image manipulation (Bik et al., 2016). Error detection is one motivation for data and analysis code to be openly available, so that peer review can confirm a manuscript’s findings, or if already published, the record can be corrected. Detected errors can result in corrections or retractions of published articles, though these actions are often delayed, long after erroneous findings have influenced and impacted further research.",
                "Reference(s)": "** Bik et al. (2016); Brown and Heathers (2017); Epskamp and Nuijten (2016); Heathers et al. (2018); Nuijten et al. (2016); [https://retractionwatch.com/](https://retractionwatch.com/)",
                "Drafted by": "** William Ngiam",
                "Reviewed (or Edited) by": "** Ali H. Al-Hoorie; Jamie P. Cockcroft; Dominik Kiersz; Sam Parsons; Suzanne L. K. Stewart; Marta Topor",
                "Translation": "** Broadly refers to examining research data and manuscripts for mistakes or inconsistencies in reporting. Commonly discussed approaches include: checking inconsistencies in descriptive statistics (e.g. summary statistics that are not possible given the sample size and measure characteristics; Brown & Heathers, 2017; Heathers et al. 2018), inconsistencies in reported statistics (e.g. *p*\\-values that do not match the reported *F* statistics and accompanying degrees of freedom; Epskamp, & Nuijten, 2016; Nuijten et al. 2016), and image manipulation (Bik et al., 2016). Error detection is one motivation for data and analysis code to be openly available, so that peer review can confirm a manuscript’s findings, or if already published, the record can be corrected. Detected errors can result in corrections or retractions of published articles, though these actions are often delayed, long after erroneous findings have influenced and impacted further research.",
                "Related_terms": "** Research integrity; correction; retraction"
            },
            {
                "Title": "Evidence Synthesis *",
                "Definition": "** This is a type of research method which aims to draw general conclusions to address a research question on a certain topic, phenomenon or effect by reviewing research outcomes and information from a range of different sources. Information which is subject to synthesis can be extracted from both qualitative and quantitative studies. The method used to synthesise the gathered information can be qualitative (narrative synthesis), quantitative (meta-analysis) or mixed (meta-synthesis, systematic mapping). Evidence synthesis has many applications and is often used in the context of healthcare, public policy as well as understanding and advancement of specific research fields.",
                "Reference": "** Centre for Evaluation (n.d.); James et al., (2016); Siddaway et al. (2019)",
                "Originally drafted by": "** Marta Topor",
                "Reviewed (or Edited) by": "** Aoife O’Mahony; Tamara Kalandadze; Adam Parker; Charlotte R. Pennington",
                "Translation": "** This is a type of research method which aims to draw general conclusions to address a research question on a certain topic, phenomenon or effect by reviewing research outcomes and information from a range of different sources. Information which is subject to synthesis can be extracted from both qualitative and quantitative studies. The method used to synthesise the gathered information can be qualitative (narrative synthesis), quantitative (meta-analysis) or mixed (meta-synthesis, systematic mapping). Evidence synthesis has many applications and is often used in the context of healthcare, public policy as well as understanding and advancement of specific research fields.",
                "Related_terms": "** Literature Review; Meta-analysis; Meta-synthesis; Meta-science or Meta-research; Narrative review; Scoping review; Systematic map; Systematic review"
            },
            {
                "Title": "Exploratory data analysis",
                "Definition": "** Exploratory Data Analysis (EDA) is a well-established statistical tradition that provides conceptual and computational tools for discovering patterns in data to foster hypothesis development and refinement. These tools and attitudes complement the use of hypothesis tests used in confirmatory data analysis (CDA). Even when well-specified theories are held, EDA helps one interpret the results of CDA and may reveal unexpected or misleading patterns in the data.",
                "Reference(s)": "** Behrens (1997); Box (1976); Tukey (1977); Wagenmakers (2012)",
                "Drafted by": "** Jenny Terry",
                "Reviewed (or Edited) by": "** Helena Hartmann; Timo Roettger; Charlotte R. Pennington; Flávio Azevedo",
                "Translation": "** Exploratory Data Analysis (EDA) is a well-established statistical tradition that provides conceptual and computational tools for discovering patterns in data to foster hypothesis development and refinement. These tools and attitudes complement the use of hypothesis tests used in confirmatory data analysis (CDA). Even when well-specified theories are held, EDA helps one interpret the results of CDA and may reveal unexpected or misleading patterns in the data.",
                "Related_terms": "** Confirmatory analyses; Data-driven research; Exploratory research"
            },
            {
                "Title": "External Validity *",
                "Definition": "** Whether the findings of a scientific study can be generalized to other contexts outside the study context (different measures, settings, people, places, and times). Statistically, threats to external validity may reflect interactions whereby the effect of one factor (the independent variable) depends on another factor (a confounding variable). External validity may also be limited by the study design (e.g., an artificial laboratory setting or a non-representative sample).",
                "Reference": "** Cook and Campbell (1979); Lynch (1982); Steckler and McLeroy (2008) **Alternative definition:** (if applicable) In Psychometrics, the degree of evidence that confirms the relations of a tested psychological construct with external variables **Related terms to alternative definition:** Criterion validity; Convergent validity; Divergent validity",
                "Reference(s)": "** Messick (1995)",
                "Originally drafted by": "** Annalise A. LaPlume",
                "Reviewed (or Edited) by": "** Mahmoud Elsherif; Helena Hartmann; Kai Krautter; Oscar Lecuona; Flávio Azevedo  ###  ### **F** {#f}",
                "Translation": "** Whether the findings of a scientific study can be generalized to other contexts outside the study context (different measures, settings, people, places, and times). Statistically, threats to external validity may reflect interactions whereby the effect of one factor (the independent variable) depends on another factor (a confounding variable). External validity may also be limited by the study design (e.g., an artificial laboratory setting or a non-representative sample).",
                "Related_terms": "** Constraints on Generality (COG); Internal validity; Generalizability; Representativity; Validity"
            },
            {
                "Title": "Face validity *",
                "Definition": "** A subjective judgement of how suitable a measure appears to be on the surface, that is, how well a measure is operationalized. For example, judging whether questionnaire items should relate to a construct of interest at face value. Face validity is related to construct validity, but since it is subjective/informal, it is considered an easy but weak form of validity.",
                "Reference": "** Holden (2010)",
                "Originally drafted by": "** Annalise A. LaPlume",
                "Reviewed (or Edited) by": "** Helena Hartmann; Kai Krautter; Adam Parker; Sam Parsons",
                "Translation": "** A subjective judgement of how suitable a measure appears to be on the surface, that is, how well a measure is operationalized. For example, judging whether questionnaire items should relate to a construct of interest at face value. Face validity is related to construct validity, but since it is subjective/informal, it is considered an easy but weak form of validity.",
                "Related_terms": "** Construct Validity; Content Validity; Logical Validity; Operationalization; Validity"
            },
            {
                "Title": "FAIR principles *",
                "Definition": "** Describes making scholarly materials Findable, Accessible, Interoperable and Reusable (FAIR). ‘Findable’ and ‘Accessible’ are concerned with where materials are stored (e.g. in data repositories), while ‘Interoperable’ and ‘Reusable’ focus on the importance of data formats and how such formats might change in the future.",
                "Reference(s)": "** Crüwell et al. (2019); Wilkinson et al. (2016); [https://www.go-fair.org/fair-principles/](https://www.go-fair.org/fair-principles/)",
                "Drafted by": "** Sonia Rishi",
                "Reviewed (or Edited) by": "** Mahmoud Elsherif; Sam Parsons; Charlotte R. Pennington",
                "Translation": "** Describes making scholarly materials Findable, Accessible, Interoperable and Reusable (FAIR). ‘Findable’ and ‘Accessible’ are concerned with where materials are stored (e.g. in data repositories), while ‘Interoperable’ and ‘Reusable’ focus on the importance of data formats and how such formats might change in the future.",
                "Related_terms": "** Metadata; Open Access; Open Code; Open Data; Open Material; Repository"
            },
            {
                "Title": "Feminist psychology *",
                "Definition": "** With a particular focus on gender and sexuality, feminist psychology is inherently concerned with representation, diversity, inclusion, accessibility, and equality. Feminist psychology initially grew out out of a concern for representing the lived experiences of girls and women, but has since evolved into a more nuanced, intersectional and comprehensive concern for all aspects of equality (e.g., Eagly & Riger, 2014). Feminist psychologists have advocated for more rigorous consideration of equality, diversity, and inclusion within Open Science spaces (Pownall et al., 2021).",
                "Reference": "** Eagly and Riger (2014); Grzanka (2020); Pownall et al (2021)",
                "Originally drafted by": "** Madeleine Pownall",
                "Reviewed (or Edited) by": "** Mahmoud Elsherif; Helena Hartmann; Kai Krautter; Charlotte R. Pennington",
                "Translation": "** With a particular focus on gender and sexuality, feminist psychology is inherently concerned with representation, diversity, inclusion, accessibility, and equality. Feminist psychology initially grew out out of a concern for representing the lived experiences of girls and women, but has since evolved into a more nuanced, intersectional and comprehensive concern for all aspects of equality (e.g., Eagly & Riger, 2014). Feminist psychologists have advocated for more rigorous consideration of equality, diversity, and inclusion within Open Science spaces (Pownall et al., 2021).",
                "Related_terms": "** Inclusion; Positionality; Reflexivity; Under-representation; Equity"
            },
            {
                "Title": "First-last-author-emphasis norm (FLAE) *",
                "Definition": "** An authorship system that assigns the order of authorship depending on the contributions of a given author while simultaneously valuing the first and last position of the authorship order most. According to this system, the two main authors are indicated as the first and last author \\- the order of the authors between the first and last position is determined by contribution in a descending order.",
                "Reference": "** Tscharntke et al. (2007)",
                "Originally drafted by": "** Myriam A. Baum",
                "Reviewed (or Edited) by": "** Charlotte R. Pennington",
                "Translation": "** An authorship system that assigns the order of authorship depending on the contributions of a given author while simultaneously valuing the first and last position of the authorship order most. According to this system, the two main authors are indicated as the first and last author \\- the order of the authors between the first and last position is determined by contribution in a descending order.",
                "Related_terms": "** Authorship; Author contributions; CreDit taxonomy"
            },
            {
                "Title": "FORRT *",
                "Definition": "** Framework of Open Reproducible Research and Teaching. It aims to provide a pedagogical infrastructure designed to recognize and support the teaching and mentoring of open and reproducible research in tandem with prototypical subject matters in higher education. FORRT strives to be an effective, evolving, and community-driven organization raising awareness of the pedagogical implications of open and reproducible science and its associated challenges (i.e., curricular reform, epistemological uncertainty, methods of education). FORRT also advocates for the opening of teaching and mentoring materials as a means to facilitate access, discovery, and learning to those who otherwise would be educationally disenfranchised.",
                "Reference(s)": "** [FORRT \\- Framework for Open and Reproducible Research Training](https://forrt.org/); F (2019)",
                "Drafted by": "** Tamara Kalandadze",
                "Reviewed (or Edited) by": "** Mahmoud Elsherif; Charlotte R. Pennington; Flávio Azevedo",
                "Translation": "** Framework of Open Reproducible Research and Teaching. It aims to provide a pedagogical infrastructure designed to recognize and support the teaching and mentoring of open and reproducible research in tandem with prototypical subject matters in higher education. FORRT strives to be an effective, evolving, and community-driven organization raising awareness of the pedagogical implications of open and reproducible science and its associated challenges (i.e., curricular reform, epistemological uncertainty, methods of education). FORRT also advocates for the opening of teaching and mentoring materials as a means to facilitate access, discovery, and learning to those who otherwise would be educationally disenfranchised.",
                "Related_terms": "** Integrating open and reproducible science tenets into higher education"
            },
            {
                "Title": "Free Our Knowledge Platform *",
                "Definition": "** A collective action platform aiming to support the open science movement by obtaining pledges from researchers that they will implement certain research practices (e.g., pre-registration, pre-print). Initially pledges will be anonymous until a sufficient number of people pledge, upon which names of pledges will be released. The initiative is a grassroots movement instigated by early career researchers.",
                "Reference(s)": "** [https://freeourknowledge.org/about/](https://freeourknowledge.org/about/)",
                "Drafted by": "** Jamie P. Cockcroft",
                "Reviewed (or Edited) by": "** Ashley Blake; Elizabeth Collins; Mahmoud Elsherif; Sam Parsons; Flávio Azevedo   ###  ### **G** {#g}",
                "Translation": "** A collective action platform aiming to support the open science movement by obtaining pledges from researchers that they will implement certain research practices (e.g., pre-registration, pre-print). Initially pledges will be anonymous until a sufficient number of people pledge, upon which names of pledges will be released. The initiative is a grassroots movement instigated by early career researchers.",
                "Related_terms": "** Open Science; Preregistration Pledge"
            },
            {
                "Title": "GPower *",
                "Definition": "** Free to use statistical software for performing power analyses. The user specifies the desired statistical test (e.g. t-test, regression, ANOVA), and three of the following: the number of groups/observations, effect size, significance level, or power, in order to calculate the unspecified aspect.",
                "Reference": "** Faul et al. (2007); Faul et al. (2009)",
                "Originally drafted by": "** Filip Dechterenko",
                "Reviewed (or Edited) by": "** Thomas Rhys Evans; Kai Krautter; Charlotte R. Pennington",
                "Translation": "** Free to use statistical software for performing power analyses. The user specifies the desired statistical test (e.g. t-test, regression, ANOVA), and three of the following: the number of groups/observations, effect size, significance level, or power, in order to calculate the unspecified aspect.",
                "Related_terms": "** Power analysis; Sample size justification; Sample size planning; Statistical power"
            },
            {
                "Title": "Gaming (the system) *",
                "Definition": "** Adopting questionable research practices (QRPs, e.g., salami slicing of an academic paper) that would align with academic incentive structures that benefit the academic (e.g. in prestige, hiring, or promotion) regardless of whether they support the process of scholarship. If systems rely on metrics to determine an outcome (e.g. academic credit) those metrics can be subject to intentional manipulation (Naudet et al., 2018\\) or “gamed”. Where promotions, hiring, and tenure are based on flawed metrics they may disfavor openness, rigor, and transparent work (Naudet et al., 2018\\) \\- for example favoring “quantity over quality” \\- and exacerbate existing inequalities.",
                "Reference(s)": "** Moher et al. (2018); Naudet et al. (2018)",
                "Drafted by": "** Adrien Fillon",
                "Reviewed (or Edited) by": "** Sarah Ashcroft-Jones; Helena Hartmann; Sam Parsons; Charlotte R. Pennington",
                "Translation": "** Adopting questionable research practices (QRPs, e.g., salami slicing of an academic paper) that would align with academic incentive structures that benefit the academic (e.g. in prestige, hiring, or promotion) regardless of whether they support the process of scholarship. If systems rely on metrics to determine an outcome (e.g. academic credit) those metrics can be subject to intentional manipulation (Naudet et al., 2018\\) or “gamed”. Where promotions, hiring, and tenure are based on flawed metrics they may disfavor openness, rigor, and transparent work (Naudet et al., 2018\\) \\- for example favoring “quantity over quality” \\- and exacerbate existing inequalities.",
                "Related_terms": "** Incentive structure; Journal Impact Factor; *P*\\-hacking"
            },
            {
                "Title": "Garden of forking paths *",
                "Definition": "** The typically-invisible decision tree traversed during operationalization and statistical analysis given that ‘there is a one-to-many mapping from scientific to statistical hypotheses' (Gelman and Loken, 2013, p. 6). In other words, even in absence of p-hacking or fishing expeditions and when the research hypothesis was posited ahead of time, there can be a plethora of statistical results that can appear to be supported by theory given data. “The problem is there can be a large number of potential comparisons when the details of data analysis are highly contingent on data, without the researcher having to perform any conscious procedure of fishing or examining multiple p-values” (Gelman and Loken, 2013, p. 1). The term aims to highlight the uncertainty ensuing from idiosyncratic analytical and statistical choices in mapping theory-to-test, and contrasting intentional (and unethical) questionable research practices (e.g. p-hacking and fishing expeditions) versus non-intentional research practices that can, potentially, have the same effect despite not having intent to corrupt their results. The garden of forking paths refers to the decisions during the scientific process that inflate the false-positive rate as a consequence of the potential paths which could have been taken (had other decisions been made).",
                "Reference(s)": "** Gelman and Loken (2013)",
                "Drafted by": "** Flávio Azevedo; Mahmoud Elsherif",
                "Reviewed (or Edited) by": "** Gisela H. Govaart; Matt Jaquiery; Tamara Kalandadze; Charlotte R. Pennington",
                "Translation": "** The typically-invisible decision tree traversed during operationalization and statistical analysis given that ‘there is a one-to-many mapping from scientific to statistical hypotheses' (Gelman and Loken, 2013, p. 6). In other words, even in absence of p-hacking or fishing expeditions and when the research hypothesis was posited ahead of time, there can be a plethora of statistical results that can appear to be supported by theory given data. “The problem is there can be a large number of potential comparisons when the details of data analysis are highly contingent on data, without the researcher having to perform any conscious procedure of fishing or examining multiple p-values” (Gelman and Loken, 2013, p. 1). The term aims to highlight the uncertainty ensuing from idiosyncratic analytical and statistical choices in mapping theory-to-test, and contrasting intentional (and unethical) questionable research practices (e.g. p-hacking and fishing expeditions) versus non-intentional research practices that can, potentially, have the same effect despite not having intent to corrupt their results. The garden of forking paths refers to the decisions during the scientific process that inflate the false-positive rate as a consequence of the potential paths which could have been taken (had other decisions been made).",
                "Related_terms": "** False-positive; Familywise error; Multiverse Analysis; Preregistration; Researcher degrees of freedom; Specification Curve Analysis"
            },
            {
                "Title": "General Data Protection Regulation (GDPR) *",
                "Definition": "** A legal framework of seven principles implemented across the European Union (EU) that aims to safeguard individuals’ information. The framework seeks to commission citizens with control over their personal data, whilst regulating the parties involved in storing and processing these data. This set of legislation dictates the free movement of individuals’ personal information both within and outside the EU and must be considered by researchers when designing and running studies.",
                "Reference": "** Crutzen et al. (2019); [https://ico.org.uk/for-organisations/guide-to-data-protection/guide-to-the-general-data-protection-regulation-gdpr/](https://ico.org.uk/for-organisations/guide-to-data-protection/guide-to-the-general-data-protection-regulation-gdpr/); [https://ec.europa.eu/info/law/law-topic/data-protection\\_en](https://ec.europa.eu/info/law/law-topic/data-protection_en)",
                "Originally drafted by": "** Graham Reid",
                "Reviewed (or Edited) by": "** Elizabeth Collins; Mahmoud Elsherif; Christopher Graham; Sam Parsons",
                "Translation": "** A legal framework of seven principles implemented across the European Union (EU) that aims to safeguard individuals’ information. The framework seeks to commission citizens with control over their personal data, whilst regulating the parties involved in storing and processing these data. This set of legislation dictates the free movement of individuals’ personal information both within and outside the EU and must be considered by researchers when designing and running studies.",
                "Related_terms": "** Anonymity; Data Management Plan (DMP); Data sharing; Repeatability; Replicability; Reproducibility"
            },
            {
                "Title": "Generalizability *",
                "Definition": "** Generalizability refers to how applicable a study’s results are to broader groups of people, settings, or situations they study and how the findings relate to this wider context (Frey, 2018; Kukull & Ganguli, 2012).",
                "Reference(s)": "** Esterling et al. (2021); Frey (2018); Kukull and Ganguli (2012); LeBel et al. (2017); Nosek and Errington (2020); Yarkoni (2020)",
                "Drafted by": "** Aoife O’Mahony",
                "Reviewed (or Edited) by": "** Adrien Fillon; Matt Jaquiery; Tina Lonsdorf; Sam Parsons; Julia Wolska",
                "Translation": "** Generalizability refers to how applicable a study’s results are to broader groups of people, settings, or situations they study and how the findings relate to this wider context (Frey, 2018; Kukull & Ganguli, 2012).",
                "Related_terms": "** Conceptual replication; External Validity; Opportunistic sampling; Sampling bias; WEIRD **Alternative definition:** Applying modified materials and/or analysis pipelines to new data or samples to answer the same hypothesis (different materials, different data) to test how generalizable the effect under study is (The Turing Way Community & Scriberia, 2021). **Related terms to alternative definition:** (if applicable): Conceptual Replication"
            },
            {
                "Title": "Gift (or Guest) Authorship *",
                "Definition": "** The inclusion in an article’s author list of individuals who do not meet the criteria for authorship. As authorship is associated with benefits including peer recognition and financial rewards, there are incentives for inclusion as an author on published research. Gifting authorship, or extending authorship credit to an individual who does not merit such recognition, can be intended to help the gift recipient, repay favors (including reciprocal gift authorship), maintain personal and professional relationships, and enhance chances of publication. Gift authorship is widely considered an unethical practice.",
                "Reference": "** Bhopal et al. (1997); ICMJE (2019)",
                "Originally drafted by": "** Bradley Baker",
                "Reviewed (or Edited) by": "** Helena Hartmann; Aoife O’Mahony; Sam Parsons; Charlotte R. Pennington; Flávio Azevedo",
                "Translation": "** The inclusion in an article’s author list of individuals who do not meet the criteria for authorship. As authorship is associated with benefits including peer recognition and financial rewards, there are incentives for inclusion as an author on published research. Gifting authorship, or extending authorship credit to an individual who does not merit such recognition, can be intended to help the gift recipient, repay favors (including reciprocal gift authorship), maintain personal and professional relationships, and enhance chances of publication. Gift authorship is widely considered an unethical practice.",
                "Related_terms": "** Authorship; CRediT"
            },
            {
                "Title": "Git *",
                "Definition": "** A software package for tracking changes in a local set of files (local version control), initially developed by Linus Torvalds. In general, it is used by programmers to track and develop computer source code within a set directory, folder or a file system. Git can access remote repository hosting services (e.g. GitHub) for remote version control that enables collaborative software development by uploading contributions from a local system. This process found its way into the scientific process to enable open data, open code and reproducible analyses.",
                "Reference": "** Kalliamvakov et al. (2014); Scopatz and Huff (2015); Vuorre and Curley (2018); [https://github.com/git/git/commit/e83c5163316f89bfbde7d9ab23ca2e25604af290](https://github.com/git/git/commit/e83c5163316f89bfbde7d9ab23ca2e25604af290)",
                "Originally drafted by": "** Emma Norris",
                "Reviewed (or Edited) by": "** Adrien Fillon; Bettina M.J. Kern; Dominik Kiersz; Robert M. Ross",
                "Translation": "** A software package for tracking changes in a local set of files (local version control), initially developed by Linus Torvalds. In general, it is used by programmers to track and develop computer source code within a set directory, folder or a file system. Git can access remote repository hosting services (e.g. GitHub) for remote version control that enables collaborative software development by uploading contributions from a local system. This process found its way into the scientific process to enable open data, open code and reproducible analyses.",
                "Related_terms": "** GitHub; Repository; Version control"
            },
            {
                "Title": "Goodhart’s Law *",
                "Definition": "** A term coined by economist Charles Goodhart to refer to the observation that measuring something inherently changes user behaviour. In relation to examination performance, Strathern (1997) stated that “when a measure becomes a target, it ceases to be a good measure” (p. 308). Applied to open scholarship, and the structure of incentives in academia, Goodhart’s Law would predict that metrics of scientific evaluation will likely be abused and exploited, as evidenced by Muller (2019)",
                "Originally drafted by": "** Adam Parker",
                "Reviewed (or Edited) by": "Sam Parsons; Flávio Azevedo  ### ---  ### **H** {#h}",
                "Translation": "** A term coined by economist Charles Goodhart to refer to the observation that measuring something inherently changes user behaviour. In relation to examination performance, Strathern (1997) stated that “when a measure becomes a target, it ceases to be a good measure” (p. 308). Applied to open scholarship, and the structure of incentives in academia, Goodhart’s Law would predict that metrics of scientific evaluation will likely be abused and exploited, as evidenced by Muller (2019)",
                "Related_terms": "Campbell's law; DORA; Reification (fallacy) **Reference (s):** Muller (2019); Strathern (1997)"
            },
            {
                "Title": "H-index *",
                "Definition": "** Hirsch’s index, abbreviated as H-index, intends to measure both productivity and research impact by combining the number of publications and the number of citations to these publications. Hirsch (2005) defined the index as “the number of papers with citation number ≥ *h*” (p. 16569). That is, the greatest number such that an author (or journal) has published at least that many papers that have been cited at least that many times. The index is perceived as a superior measure to measures that only assess, for instance, the number of citations and number of publications but this index has been criticised for the purpose of researcher assessment (e.g. Wendl, 2007).",
                "Reference(s)": "** Hirsch (2005); Wendl (2007)",
                "Drafted by": "** Jacob Miranda",
                "Reviewed (or Edited) by": "** Bradley J. Baker; Mahmoud M. Elsherif; Brett J. Gall; Charlotte R. Pennington",
                "Translation": "** Hirsch’s index, abbreviated as H-index, intends to measure both productivity and research impact by combining the number of publications and the number of citations to these publications. Hirsch (2005) defined the index as “the number of papers with citation number ≥ *h*” (p. 16569). That is, the greatest number such that an author (or journal) has published at least that many papers that have been cited at least that many times. The index is perceived as a superior measure to measures that only assess, for instance, the number of citations and number of publications but this index has been criticised for the purpose of researcher assessment (e.g. Wendl, 2007).",
                "Related_terms": "** Citation; DORA; I10-index; Impact"
            },
            {
                "Title": "Hackathon *",
                "Definition": "** An organized event where experts, designers, or researchers collaborate for a relatively short amount of time to work intensively on a project or problem. The term is originally borrowed from computer programmer and software development events whose goal is to create a fully fledged product (resources, research, software, hardware) by the end of the event, which can last several hours to several days.",
                "Reference": "** Kienzler and Fontanesi (2017)",
                "Originally drafted by": "** Flávio Azevedo",
                "Reviewed (or Edited) by": "** Tsvetomira Dumbalska; Brett J. Gall; Emma Norris",
                "Translation": "** An organized event where experts, designers, or researchers collaborate for a relatively short amount of time to work intensively on a project or problem. The term is originally borrowed from computer programmer and software development events whose goal is to create a fully fledged product (resources, research, software, hardware) by the end of the event, which can last several hours to several days.",
                "Related_terms": "** Collaboration; Edithaton"
            },
            {
                "Title": "HARKing *",
                "Definition": "** A questionable research practice termed ‘Hypothesizing After the Results are Known’ (HARKing). “HARKing is defined as presenting a post hoc hypothesis (i.e., one based on or informed by one's results) in a research report as if it was, in fact, *a priori*” (Kerr, 1998, p. 196). For example, performing subgroup analyses, finding an effect in one subgroup, and writing the introduction with a ‘hypothesis’ that matches these results.",
                "Reference(s)": "** Kerr (1998); Nosek and Lakens (2014)",
                "Drafted by": "** Beatrix Arendt",
                "Reviewed (or Edited) by": "** Matt Jaquiery; Charlotte R. Pennington; Martin Vasilev; Flávio Azevedo",
                "Translation": "** A questionable research practice termed ‘Hypothesizing After the Results are Known’ (HARKing). “HARKing is defined as presenting a post hoc hypothesis (i.e., one based on or informed by one's results) in a research report as if it was, in fact, *a priori*” (Kerr, 1998, p. 196). For example, performing subgroup analyses, finding an effect in one subgroup, and writing the introduction with a ‘hypothesis’ that matches these results.",
                "Related_terms": "** Analytic Flexibility; Confirmatory analyses; Exploratory data analysis; Fudging; Garden of forking paths; P-hacking; Questionable Research Practices or Questionable Reporting Practices (QRPs)"
            },
            {
                "Title": "Hidden Moderators **",
                "Definition": "** Contextual conditions that can, unbeknownst to researchers, make the results of a replication attempt deviate from those of the original study. Hidden moderators are sometimes invoked to explain (away) failed replications. Also called hidden assumptions.",
                "Reference": "** Zwaan et al. (2018)",
                "Originally drafted by": "** Ali H. Al-Hoorie",
                "Reviewed (or Edited) by": "** Mahmoud Elsherif; Sam Parsons",
                "Translation": "** Contextual conditions that can, unbeknownst to researchers, make the results of a replication attempt deviate from those of the original study. Hidden moderators are sometimes invoked to explain (away) failed replications. Also called hidden assumptions.",
                "Related_terms": "** Auxiliary Hypothesis"
            },
            {
                "Title": "Hypothesis *",
                "Definition": "** A hypothesis is an unproven statement relating the connection between variables (Glass & Hall, 2008\\) and can be based on prior experiences, scientific knowledge, preliminary observations, theory and/or logic. In scientific testing, a hypothesis can be usually formulated with (e.g. a positive correlation) or without a direction (e.g. there will be a correlation). Popper (1959) posits that hypotheses must be falsifiable, that is, it must be conceivably possible to prove the hypothesis false. However, hypothesis testing based on falsification has been argued to be vague, as it is contingent on many other untested assumptions in the hypothesis (i.e., auxiliary hypotheses). Longino (1990, 1992\\) argued that ontological heterogeneity should be valued more than ontological simplicity for the biological sciences, which considers we should investigate differences between and within biological organisms.",
                "Reference(s)": "** Beller and Bender (2017); Glass and Hall (2008); Longino (1990, 1992); Popper (1959)",
                "Drafted by": "** Ana Barbosa Mendes",
                "Reviewed (or Edited) by": "** Ali H. Al-Hoorie; Mahmoud Elsherif; Helena Hartmann; Charlotte R. Pennington**;** Graham Reid; Olly Robertson   ### **I** {#i}",
                "Translation": "** A hypothesis is an unproven statement relating the connection between variables (Glass & Hall, 2008\\) and can be based on prior experiences, scientific knowledge, preliminary observations, theory and/or logic. In scientific testing, a hypothesis can be usually formulated with (e.g. a positive correlation) or without a direction (e.g. there will be a correlation). Popper (1959) posits that hypotheses must be falsifiable, that is, it must be conceivably possible to prove the hypothesis false. However, hypothesis testing based on falsification has been argued to be vague, as it is contingent on many other untested assumptions in the hypothesis (i.e., auxiliary hypotheses). Longino (1990, 1992\\) argued that ontological heterogeneity should be valued more than ontological simplicity for the biological sciences, which considers we should investigate differences between and within biological organisms.",
                "Related_terms": "** Auxiliary Hypothesis; Confirmatory analyses; False negative result; False positive result; Modelling; Predictions; Quantitative research; Theory; Theory building; Type I error; Type II error"
            },
            {
                "Title": "i10-index *",
                "Definition": "** A research metric created by Google Scholar that represents the number of publications a researcher has with at least 10 citations.",
                "Reference(s)": "** [https://guides.library.cornell.edu/impact/author-impact-10](https://guides.library.cornell.edu/impact/author-impact-10)",
                "Drafted by": "** Emma Norris",
                "Reviewed (or Edited) by": "** Flávio Azevedo; Sam Parsons",
                "Translation": "** A research metric created by Google Scholar that represents the number of publications a researcher has with at least 10 citations.",
                "Related_terms": "** Citation; DORA; H-index; Impact"
            },
            {
                "Title": "Ideological bias *",
                "Definition": "** The idea that pre-existing opinions about the quality of research can depend on the ideological views of the author(s). One of the many biases in the peer review process, it expects that favourable opinions towards the research would be more likely if friends, collaborators, or scientists agree with an editor or reviewer’s political viewpoints (Tvina et al. 2019). This could potentially lead to a variety of conflicts of interest that undermine diverse perspectives, for example: speeding or delaying peer-review, or influencing the chances of an individual being invited to present their research, thus promoting their work.",
                "Reference(s)": "** Tvina et al. (2019)",
                "Drafted by": "** Mahmoud Elsherif",
                "Reviewed (or Edited) by": "** Elizabeth Collins; Flávio Azevedo; Madeleine Ingham; Sam Parsons; Graham Reid",
                "Translation": "** The idea that pre-existing opinions about the quality of research can depend on the ideological views of the author(s). One of the many biases in the peer review process, it expects that favourable opinions towards the research would be more likely if friends, collaborators, or scientists agree with an editor or reviewer’s political viewpoints (Tvina et al. 2019). This could potentially lead to a variety of conflicts of interest that undermine diverse perspectives, for example: speeding or delaying peer-review, or influencing the chances of an individual being invited to present their research, thus promoting their work.",
                "Related_terms": "** Ad hominem bias; Peer review"
            },
            {
                "Title": "Inclusion *",
                "Definition": "** Inclusion, or inclusivity, refers to a sense of welcome and respect within a given collaborative project or environment (such as academia) where *diversity* simply indicates a wide range of backgrounds, perspectives, and experiences, efforts to increase *inclusion* go further to promote engagement and equal valuation among diverse individuals, who might otherwise be marginalized. Increasing inclusivity often involves minimising the impact of, or even removing, systemic barriers to accessibility and engagement.",
                "Reference": "Calvert (2019); [Martinez-Acosta and Favero (2018)](https://www-ncbi-nlm-nih-gov.proxy.library.vanderbilt.edu/pmc/articles/PMC6153014/)",
                "Drafted by": "** Ryan Millager",
                "Reviewed (or Edited) by": "** Mahmoud Elsherif; Graham Reid; Kai Krautter; Suzanne L. K. Stewart; Flávio Azevedo",
                "Translation": "** Inclusion, or inclusivity, refers to a sense of welcome and respect within a given collaborative project or environment (such as academia) where *diversity* simply indicates a wide range of backgrounds, perspectives, and experiences, efforts to increase *inclusion* go further to promote engagement and equal valuation among diverse individuals, who might otherwise be marginalized. Increasing inclusivity often involves minimising the impact of, or even removing, systemic barriers to accessibility and engagement.",
                "Related_terms": "** Diversity; Equity; Social Justice"
            },
            {
                "Title": "Incentive structure *",
                "Definition": "** The set of evaluation and reward mechanisms (explicit and implicit) for scientists and their work. Incentivised areas within the broader structure include hiring and promotion practices, track record for awarding funding, and prestige indicators such as publication in journals with high impact factors, invited presentations, editorships, and awards. It is commonly believed that these criteria are often misaligned with the telos of science, and therefore do not promote rigorous scientific output. Initiatives like DORA aim to reduce the field’s dependency on evaluation criteria such as journal impact factors in favor of assessments based on the intrinsic quality of research outputs.",
                "Reference": "** Koole and Lakens (2012); Nosek et al. (2012); Schonbrodt (2019); Smaldino and McElreath (2016)",
                "Originally drafted by": "** Charlotte R. Pennington; Olmo van den Akker",
                "Reviewed (or Edited) by": "** Helena Hartmann; Flávio Azevedo; Robert M. Ross; Graham Reid; Suzanne L. K. Stewart",
                "Translation": "** The set of evaluation and reward mechanisms (explicit and implicit) for scientists and their work. Incentivised areas within the broader structure include hiring and promotion practices, track record for awarding funding, and prestige indicators such as publication in journals with high impact factors, invited presentations, editorships, and awards. It is commonly believed that these criteria are often misaligned with the telos of science, and therefore do not promote rigorous scientific output. Initiatives like DORA aim to reduce the field’s dependency on evaluation criteria such as journal impact factors in favor of assessments based on the intrinsic quality of research outputs.",
                "Related_terms": "** DORA; Metrics; Pressure; Publish or perish; Quantity; Reward structure; Scientific publications; Slow science; Structural factors"
            },
            {
                "Title": "Induction **",
                "Definition": "** “Reasoning by drawing a conclusion not guaranteed by the premises; for example, by inferring a general rule from a limited number of observations. Popper believed that there was no such logical process; we may guess general rules but such guesses are not rendered even more probable by any number of observations. By contrast, Bayesians inductively work out the increase in probability of a hypothesis that follows from the observations.” Dienes (p. 164, 2008\\)",
                "Reference": "Dienes (2008)",
                "Drafted by": "** Alaa Aldoh",
                "Reviewed (or Edited) by": "**",
                "Translation": "** “Reasoning by drawing a conclusion not guaranteed by the premises; for example, by inferring a general rule from a limited number of observations. Popper believed that there was no such logical process; we may guess general rules but such guesses are not rendered even more probable by any number of observations. By contrast, Bayesians inductively work out the increase in probability of a hypothesis that follows from the observations.” Dienes (p. 164, 2008\\)",
                "Related_terms": "** Hypothesis"
            },
            {
                "Title": "Interaction Fallacy *",
                "Definition": "** A statistical error in which a formal test is not conducted to assess the difference between a significant and non-significant correlation (or other measures, such as Odds Ratio). This fallacy occurs when a significant and non-significant correlation coefficient are assumed to represent a statistically significant difference but the comparison itself is not explicitly tested.",
                "Reference": "** Gelman and Stern (2006); Morabia et al. (1997); Nieuwenhuis et al. (2011)",
                "Originally drafted by": "** Graham Reid",
                "Reviewed (or Edited) by": "** Ali H. Al-Hoorie; Mahmoud Elsherif; Kai Krautter; Sam Parsons",
                "Translation": "** A statistical error in which a formal test is not conducted to assess the difference between a significant and non-significant correlation (or other measures, such as Odds Ratio). This fallacy occurs when a significant and non-significant correlation coefficient are assumed to represent a statistically significant difference but the comparison itself is not explicitly tested.",
                "Related_terms": "** Comparison of Correlations; Null Hypothesis Significance Testing (NHST); Statistical Validity; Type I error; Type II error"
            },
            {
                "Title": "Interlocking *",
                "Definition": "** An analysis at the core of intersectionality to analyse power, inequality and exclusion, as efforts to reform academic culture cannot be completed by investigating only one avenue in isolation (e.g. race, gender or ability) but by considering all the systems of exclusion. In contrast to intersectionality (which refers to the individual having multiple social identities), interlocking is usually used to describe the systems that combine to serve as oppressive measures toward the individual based on these identities.",
                "Reference(s)": "** Ledgerwood et al. (2021)",
                "Drafted by": "** Christina Pomareda",
                "Reviewed (or Edited) by": "** Ali H. Al-Hoorie; Flávio Azevedo; Mahmoud Elsherif; Eliza Woodward; Gerald Vineyard;",
                "Translation": "** An analysis at the core of intersectionality to analyse power, inequality and exclusion, as efforts to reform academic culture cannot be completed by investigating only one avenue in isolation (e.g. race, gender or ability) but by considering all the systems of exclusion. In contrast to intersectionality (which refers to the individual having multiple social identities), interlocking is usually used to describe the systems that combine to serve as oppressive measures toward the individual based on these identities.",
                "Related_terms": "** Bropenscience; Equity; Diversity; Inclusion; Intersectionality; Open Science; Social Justice"
            },
            {
                "Title": "Internal Validity *",
                "Definition": "** An indicator of the extent to which a study’s findings are representative of the true effect in the population of interest and not due to research confounds, such as methodological shortcomings. In other words, whether the observed evidence or covariation between the independent (predictor) and dependent (criterion) variables can be taken as a bona fide relationship and not a spurious effect owing to uncontrolled aspects of the study’s set up. Since it involves the quality of the study itself, internal validity is a priority for scientific research.",
                "Reference": "** Campbell and Stanley (1966) **Alternative definition:** In Psychometrics, the degree of evidence that confirms the internal structure of a psychometric test as compatible with the structure of a psychological construct. **Related terms to alternative definition:** Construct validity",
                "Reference(s)": "** Messick (1995)",
                "Originally drafted by": "** Annalise A. LaPlume",
                "Reviewed (or Edited) by": "** Helena Hartmann; Oscar Lecuona; Meng Liu; Sam Parsons; Graham Reid; Flávio Azevedo",
                "Translation": "** An indicator of the extent to which a study’s findings are representative of the true effect in the population of interest and not due to research confounds, such as methodological shortcomings. In other words, whether the observed evidence or covariation between the independent (predictor) and dependent (criterion) variables can be taken as a bona fide relationship and not a spurious effect owing to uncontrolled aspects of the study’s set up. Since it involves the quality of the study itself, internal validity is a priority for scientific research.",
                "Related_terms": "** External validity; Validity"
            },
            {
                "Title": "Intersectionality *",
                "Definition": "** A term which derives from Black feminist thought and broadly describes how social identities exist within ‘interlocking systems of oppression’ and structures of (in)equalities (Crenshaw, 1989\\)**.** Intersectionality offers a perspective on the way multiple forms of inequality operate together to compound or exacerbate each other. Multiple concurrent forms of identity can have a multiplicative effect and are not merely the sum of the component elements. One implication is that identity cannot be adequately understood through examining a single axis (e.g., race, gender, sexual orientation, class) at a time in isolation, but requires simultaneous consideration of overlapping forms of identity.",
                "Reference(s)": "** Crenshaw (1989); Grzanka (2020); Ledgerwood et al. (2021)",
                "Drafted by": "** Madeleine Pownall",
                "Reviewed (or Edited) by": "** Ali H. Al-Hoorie; Bradley Baker; Mahmoud Elsherif; Wanyin Li; Ryan Millager; Charlotte R. Pennington; Flávio Azevedo  ### ---  ### **J** {#j}",
                "Translation": "** A term which derives from Black feminist thought and broadly describes how social identities exist within ‘interlocking systems of oppression’ and structures of (in)equalities (Crenshaw, 1989\\)**.** Intersectionality offers a perspective on the way multiple forms of inequality operate together to compound or exacerbate each other. Multiple concurrent forms of identity can have a multiplicative effect and are not merely the sum of the component elements. One implication is that identity cannot be adequately understood through examining a single axis (e.g., race, gender, sexual orientation, class) at a time in isolation, but requires simultaneous consideration of overlapping forms of identity.",
                "Related_terms": "** Bropenscience; Diversity; Inclusion; Interlocking; Open Science"
            },
            {
                "Title": "JabRef *",
                "Definition": "** An open-sourced, cross-platform citation and reference management tool that is available free of charge. It allows editing BibTeX files, importing data from online scientific databases, and managing and searching BibTeX files.",
                "Reference": "** JabRef Development Team (2021)",
                "Originally drafted by": "** Aleksandra Lazić",
                "Reviewed (or Edited) by": "** Christopher Graham; Michele C. Lim; Sam Parsons; Steven Verheyen",
                "Translation": "** An open-sourced, cross-platform citation and reference management tool that is available free of charge. It allows editing BibTeX files, importing data from online scientific databases, and managing and searching BibTeX files.",
                "Related_terms": "** Open source software"
            },
            {
                "Title": "Jamovi *",
                "Definition": "** Free and open source software for data analysis based on the R language. The software has a graphical user interface and provides the R code to the analyses. Jamovi supports computational reproducibility by saving the data, code, analyses, and results in a single file.",
                "Reference(s)": "** The jamovi project (2020)",
                "Drafted by": "** Amélie Beffara Bret",
                "Reviewed (or Edited) by": "** Adrien Fillon; Alexander Hart; Charlotte R. Pennington",
                "Translation": "** Free and open source software for data analysis based on the R language. The software has a graphical user interface and provides the R code to the analyses. Jamovi supports computational reproducibility by saving the data, code, analyses, and results in a single file.",
                "Related_terms": "** JASP; Open source; R; Reproducibility"
            },
            {
                "Title": "JASP *",
                "Definition": "** Named after Sir Harold Jeffreys, JASP stands for Jeffrey’s Amazing Statistics Program. It is a free and open source software for data analysis. JASP relies on a user interface and offers both null hypothesis tests and their Bayesian counterparts. JASP supports computational reproducibility by saving the data, code, analyses, and results in a single file.",
                "Reference(s)": "** JASP Team (2020)",
                "Drafted by": "** Amélie Beffara Bret",
                "Reviewed (or Edited) by": "** Adrien Fillon, Adam Parker; Sam Parsons",
                "Translation": "** Named after Sir Harold Jeffreys, JASP stands for Jeffrey’s Amazing Statistics Program. It is a free and open source software for data analysis. JASP relies on a user interface and offers both null hypothesis tests and their Bayesian counterparts. JASP supports computational reproducibility by saving the data, code, analyses, and results in a single file.",
                "Related_terms": "** Jamovi; Open source"
            },
            {
                "Title": "Journal Impact Factor™ *",
                "Definition": "** The mean number of citations to research articles in that journal over the preceding two years. It is a proprietary and opaque calculation marketed by Clarivate**™**. Journal Impact Factors are not associated with the content quality or the peer review process.",
                "Reference(s)": "** Brembs et al (2013); Curry (2012); Naudet et al. (2018); Rossner et al. (2008); Sharma et al. (2014)",
                "Drafted by": "** Jacob Miranda",
                "Reviewed (or Edited) by": "** Tsvetomira Dumbalska; Adam Parker",
                "Translation": "** The mean number of citations to research articles in that journal over the preceding two years. It is a proprietary and opaque calculation marketed by Clarivate**™**. Journal Impact Factors are not associated with the content quality or the peer review process.",
                "Related_terms": "** DORA; H-index"
            },
            {
                "Title": "JSON file *",
                "Definition": "** JavaScript Object Notation (JSON) is a data format for structured data that can be used to represent attribute-value pairs. Values thereby can contain further JSON notation (i.e., nested information). JSON files can be formally encoded as strings of text and thus are human-readable. Beyond storing information this feature makes them suitable for annotating other content. For example, JSON files are used in Brain Imaging Data Structure (BIDS) for describing the metadata dataset by following a standardized format (dataset\\_description.json).",
                "Reference(s)": "** https://bids-specification.readthedocs.io/en/stable/03-modality-agnostic-files.html",
                "Drafted by": "** Tina Lonsdorf",
                "Reviewed (or Edited) by": "** Alexander Hart; Matt Jaquiery; Emma Norris; Charlotte R. Pennington  ###  ### **K** {#k}",
                "Translation": "** JavaScript Object Notation (JSON) is a data format for structured data that can be used to represent attribute-value pairs. Values thereby can contain further JSON notation (i.e., nested information). JSON files can be formally encoded as strings of text and thus are human-readable. Beyond storing information this feature makes them suitable for annotating other content. For example, JSON files are used in Brain Imaging Data Structure (BIDS) for describing the metadata dataset by following a standardized format (dataset\\_description.json).",
                "Related_terms": "** BIDS data structure; Metadata"
            },
            {
                "Title": "Knowledge acquisition *",
                "Definition": "** The process by which the mind decodes or extracts, stores, and relates new information to existing information in long term memory. Given the complex structure and nature of knowledge, this process is studied in the philosophical field of epistemology, as well as the psychological field of learning and memory.",
                "Reference(s)": "** Brule and Blount (1989)",
                "Drafted by": "** Oscar Lecuona",
                "Reviewed (or Edited) by": "** Bradley Baker; Helena Hartmann; Kai Krautter; Graham Reid  ###  ### **L** {#l}",
                "Translation": "** The process by which the mind decodes or extracts, stores, and relates new information to existing information in long term memory. Given the complex structure and nature of knowledge, this process is studied in the philosophical field of epistemology, as well as the psychological field of learning and memory.",
                "Related_terms": "** Epistemology; Information; Learning"
            },
            {
                "Title": "Likelihood function *",
                "Definition": "** A statistical model of the data used in frequentist and Bayesian analyses, defined up to a constant of proportionality. A likelihood function represents the likeliness of different parameters for your distribution given the data. Given that probability distributions have unknown population parameters, the likelihood function indicates how well the sample data summarise these parameters. As such, the likelihood function gives an idea of the goodness of fit of a model to the sample data for a given set of values of the unknown population parameters.",
                "Reference(s)": "** Dienes (2008); Hogg et al. (2010); van de Schoot et al. (2021); Geyer (2003); Geyer (2007); https://blog.stata.com/2016/11/01/introduction-to-bayesian-statistics-part-1-the-basic-concepts/",
                "Drafted by": "** Alaa AlDoh",
                "Reviewed (or Edited) by": "** Dominik Kiersz; Graham Reid; Sam Parsons; Flávio Azevedo",
                "Translation": "** A statistical model of the data used in frequentist and Bayesian analyses, defined up to a constant of proportionality. A likelihood function represents the likeliness of different parameters for your distribution given the data. Given that probability distributions have unknown population parameters, the likelihood function indicates how well the sample data summarise these parameters. As such, the likelihood function gives an idea of the goodness of fit of a model to the sample data for a given set of values of the unknown population parameters.",
                "Related_terms": "** Bayes factor; Bayesian inference; Bayesian parameter estimation; Posterior distribution; Prior distribution **Alternative definition:** For a more statistically-informed definition, given a parametric model specified by a probability (densidity) function f(x|theta), a likelihood *for* a statistical model is defined by the same formula as the density except that the roles of the data *x* and the parameter *theta* are interchanged, and thus the likelihood can be considered a function of *theta* for fixed data *x*. Here, then, the likelihood function would describe a curve or hypersurface whose peak, if it exists, represents the combination of model parameter values that maximize the probability of drawing the sample obtained."
            },
            {
                "Title": "Likelihood Principle **",
                "Definition": "** The notion that all information relevant to inference contained in data is provided by the likelihood. The principle suggests that the likelihood function can be used to compare the plausibility of various parameter values. While Bayesians and likelihood theorists subscribe to the likelihood principle, Neyman-Pearson theorists do not, as significance tests violate the likelihood principle because they take into account information not in the likelihood.",
                "Reference(s)": "** Dienes (2008); Geyer (2003; 2007);",
                "Drafted by": "** Alaa Aldoh",
                "Reviewed (or Edited) by": "** Sam Parsons; Flávio Azevedo",
                "Translation": "** The notion that all information relevant to inference contained in data is provided by the likelihood. The principle suggests that the likelihood function can be used to compare the plausibility of various parameter values. While Bayesians and likelihood theorists subscribe to the likelihood principle, Neyman-Pearson theorists do not, as significance tests violate the likelihood principle because they take into account information not in the likelihood.",
                "Related_terms": "** Bayesian inference; Likelihood Function"
            },
            {
                "Title": "Literature Review *",
                "Definition": "** Researchers often review research records on a given topic to better understand effects and phenomena of interest before embarking on a new research project, to understand how theory links to evidence or to investigate common themes and directions of existing study results and claims. Different types of reviews can be conducted depending on the research question and literature scope. To determine the scope and key concepts in a given field, researchers may want to conduct a scoping literature review. Systematic reviews aim to access and review all available records for the most accurate and unbiased representation of existing literature. Non-systematic or focused literature reviews synthesise information from a selection of studies relevant to the research question although they are uncommon due to susceptibility to biases (e.g. researcher bias; Siddaway et al., 2019).",
                "Reference(s)": "** Huelin et al., (2015); Munn et al., (2018); Pautasso (2013); Siddaway et al. (2019)",
                "Drafted by": "** Marta Topor",
                "Reviewed (or Edited) by": "** Jamie P. Cockcroft; Mahmoud Elsherif; Helena Hartmann; Flávio Azevedo; Meng Liu; Charlotte R. Pennington  **Are you looking for terms beginning M \\- Z ???** **If so, see this document [Glossary Organizing document M-Z - instructions for contributors](https://docs.google.com/document/d/1OV1WKyLMmCvcrHaO9iVCdxOGVxoEza4yjvdT6Q5ZBKE/edit?usp=sharing)**  # **References** {#references}  Abele-Brehm, A. E., Gollwitzer, M., Steinberg, U., & Schönbrodt, F. D. (2019). Attitudes toward open science and public data sharing. *Social Psychology, 50*, 252-260. [https://doi.org/10.1027/1864-9335/a000384](https://doi.org/10.1027/1864-9335/a000384) Aczel, B., Szaszi, B., Nilsonne, G., Van den Akker, O., Albers, C. J., van Assen, M. A. L. M., … Wagenmakers, E. (2021, April 21). Guidance for Multi-Analyst Studies. [https://doi.org/10.31222/osf.io/5ecnh](https://doi.org/10.31222/osf.io/5ecnh) Aczel, B., Szaszi, B., Sarafoglou, A., Kekecs, Z., Kucharský, Š., Benjamin, D., ... & Wagenmakers, E. J. (2020). A consensus-based transparency checklist. *Nature Human Behaviour, 4*(1), 4-6. [https://doi.org/10.1038/s41562-019-0772-6](https://doi.org/10.1038/s41562-019-0772-6) Albayrak, N. (2018a). Diversity helps but decolonisation is the key to equality in higher education. [https://lsepgcertcitl.wordpress.com/2018/04/16/diversity-helps-but-decolonisation-is-the-key-to-equality-in-higher-education/](https://lsepgcertcitl.wordpress.com/2018/04/16/diversity-helps-but-decolonisation-is-the-key-to-equality-in-higher-education/) Albayrak, N. (2018b). Academics’ role on the future of higher education: Important but unrecognised. [https://lsepgcertcitl.wordpress.com/2018/11/29/academics-role-on-the-future-of-higher-education-important-but-unrecognised/](https://lsepgcertcitl.wordpress.com/2018/11/29/academics-role-on-the-future-of-higher-education-important-but-unrecognised/) Albayrak, N., & Okoroji, C. (2019). Facing the challenges of postgraduate study as a minority student. *A Guide for Psychology Postgraduates*, 63\\. Albayrak-Aydemir, N. (2020). The hidden costs of being a scholar from the global south. *Higher Education Across Borders (LSE Blog).* [https://blogs.lse.ac.uk/highereducation/2020/02/20/the-hidden-costs-of-being-a-scholar-from-the-global-south/](https://blogs.lse.ac.uk/highereducation/2020/02/20/the-hidden-costs-of-being-a-scholar-from-the-global-south/) ALLEA \\- All European Academies (2017). The European Code of Conduct for Research Integrity. Revised Edition. Available at: [https://allea.org/code-of-conduct/](https://allea.org/code-of-conduct/) Allen, L., & McGonagle-O’Connell, A. (n.d.). *CRediT – Contributor Roles Taxonomy.* CASRAI. [https://casrai.org/credit/](https://casrai.org/credit/) Ali, M. J. (2021) Understanding the Altmetrics. *Seminars in Ophthalmology.* https://doi.org/10.1080/08820538.2021.1930806 Anderson, M.S., Ronning, E.A., Devries, R., & Martinson, B.C. (2010). Extending the Mertonian norms: Scientists’ subscription to norms of research. *Journal of Higher Education, 81*(3), 366–393. https://doi.org/10.1353/jhe.0.0095. Angrist, J. D., & Pischke, J. S. (2010). The credibility revolution in empirical economics: How better research design is taking the con out of econometrics. *Journal of Economic Perspectives, 24*, 3-30. https://doi.org/10.1257/jep.24.2.3. Anon (n.d.). *About CC Licenses.* Creative Commons. [https://creativecommons.org/about/cclicenses/](https://creativecommons.org/about/cclicenses/) Anon. (n.d.). *Ckan*. [https://ckan.org/](https://ckan.org/) Anon. (2006). Correction or retraction?. *Nature, 444*, 123–124. [https://doi.org/10.1038/444123b](https://doi.org/10.1038/444123b) Anon (n.d.). *Datacite Metadata Schema.* Datacite Schema. https://schema.datacite.org/ Anon. (n.d.). Domov | SKRN (Slovak Reproducibility network). SKRN. https://slovakrn.wixsite.com/skrn Anon (n.d.). *Home | re3data.org*. Registry of Research Data Repositories. Retrieved 6 June 2021, from [https://www.re3data.org/](https://www.re3data.org/) Anon (n.d.). *INVOLVE – INVOLVE Supporting public involvement in NHS, public health and social care research*. INVOLVE. [https://www.invo.org.uk/](https://www.invo.org.uk/) Anon. (n.d.). *Licenses & Standards | Open Source Initiative*. OpenSource.Com. https://opensource.org/licenses Anon (n.d.). *Open Source in Open Science | FOSTER*. Foster. [https://www.fosteropenscience.eu/foster-taxonomy/open-source-open-science](https://www.fosteropenscience.eu/foster-taxonomy/open-source-open-science) Anon. (2019). *The DOI Handbook.* DOI. [https://www.doi.org/hb.html](https://www.doi.org/hb.html) Anon (n.d.). *Welcome to Sherpa Romeo \\- v2.sherpa*. Sherpa Romeo. [https://v2.sherpa.ac.uk/romeo/](https://v2.sherpa.ac.uk/romeo/) Anon (n.d.). *What is a codebook?* ICPSR. [https://www.icpsr.umich.edu/icpsrweb/content/shared/ICPSR/faqs/what-is-a-codebook.html](https://www.icpsr.umich.edu/icpsrweb/content/shared/ICPSR/faqs/what-is-a-codebook.html) Anon.(2009-2020). *What is a digital object identifier, or DOI?* American Psychological Association. h[ttps://apastyle.apa.org/learn/faqs/what-is-doi](https://apastyle.apa.org/learn/faqs/what-is-doi) Anon. (n.d.). *What is a reporting guideline?* Equator Network. https://www.equator-network.org/about-us/what-is-a-reporting-guideline/ Anon (2021). *What is impact?* The Economic and Social Research Council. [https://esrc.ukri.org/research/impact-toolkit/what-is-impact/](https://esrc.ukri.org/research/impact-toolkit/what-is-impact/) Anon. (n.d.). *What is open education?* Opensource.Com. [https://opensource.com/resources/what-open-education](https://opensource.com/resources/what-open-education) Arslan, R. C. (2019). How to Automatically Document Data With the codebook Package to Facilitate Data Reuse. *Advances in Methods and Practices in Psychological Science, 2*(2), 169–187. https://doi.org/10.1177/2515245919838783 Arts and Humanities Research Council. (n.d.). *Definition of eligibility for funding*. Arts and Humanities Research Council. Available at: https://ahrc.ukri.org/skills/earlycareerresearchers/definitionofeligibility/ Aspers, P., & Corte, U. (2019). What is qualitative in qualitative research. *Qualitative Sociology, 42*(2), 139-160. https://doi.org/10.1007/s11133-019-9413-7 AusRN. (n.d.). *Australian Reproducibility Network.* Retrieved 5 June 2021, from [https://www.aus-rn.org/](https://www.aus-rn.org/) Bahlai, C., Bartlett, L.J., Burgio, K.R. et al. (2019). Open science isn’t always open to all scientists. *American Scientist, 107*(2), 78\\. Banks, G. C., Rogelberg, S. G., Woznyj, H. M., Landis, R. S., & Rupp, D. E. (2016). Editorial: Evidence on questionable research practices: The good, the bad, and the ugly. *Journal of Business and Psychology, 31*(3), 323–338. https://doi.org/10.1007/s10869-016-9456-7 Barba, L. A. (2018). Terminologies for reproducible research. arXiv preprint arXiv:1802.03311. Bardsley, N. (2018) What lessons does the “replication crisis” in psychology hold for experimental economics? In: *Handbook of Psychology and Economic Behaviour. 2nd edition. Cambridge Handbooks in Psychology.* Cambridge University Press. ISBN 9781107161399 Available at http://centaur.reading.ac.uk/69874/ Barnes, R. M., Johnston, H. M., MacKenzie, N., Tobin, S. J., & Taglang, C. M. (2018). The effect of ad hominem attacks on the evaluation of claims promoted by scientists. *PloS one, 13*(1), e0192025. https://doi.org/10.1371/journal.pone.0192025 Bartoš, F., & Schimmack, U. (2020). Z-Curve 2.0: Estimating replication rates and discovery rates. [https://doi.org/10.31234/osf.io/urgtn](https://doi.org/10.31234/osf.io/urgtn) Bateman, I., Kahneman, D., Munro, A., Starmer, C., & Sugden, R. (2005). Testing competing models of loss aversion: An adversarial collaboration. *Journal of Public Economics, 89*(8), 1561-1580. https://doi.org/10.1016/j.jpubeco.2004.06.013 Baturay, M. H. (2015). An overview of the world of MOOCs. *Procedia-Social and Behavioral Sciences, 174,* 427-433. https://doi.org/10.1016/j.sbspro.2015.01.685 Bazeley, P. (2003). Defining 'Early Career' in Research. *Higher Education 45*, 257–279 [https://doi.org/10.1023/A:1022698529612](https://doi.org/10.1023/A:1022698529612) Beffara Bret, B., Beffara Bret, A., & Nalborczyk, L. (2021). A fully automated, transparent, reproducible, and blind protocol for sequential analyses. Meta-Psychology, 5\\. https://doi.org/10.15626/MP.2018.869 Behrens, J. T. (1997). Principles and procedures of exploratory data analysis. *Psychological Methods, 2*(2), 131-160. https://doi.org/10.1037/1082-989X.2.2.131 Beller, S., & Bender, A. (2017). Theory, the final frontier? A corpus-based analysis of the role of theory in psychological articles. *Frontiers in Psychology, 8*, 951\\. [https://doi.org/10.3389/fpsyg.2017.00951](https://doi.org/10.3389/fpsyg.2017.00951) Benoit, K., Conway, D., Lauderdale, B. E., Laver, M., & Mikhaylov, S. (2016). Crowd-sourced text analysis: Reproducible and agile production of political data. A*merican Political Science Review, 110*(2), 278–295. https://doi.org/10.1017/S0003055416000058 Bhopal, R., Rankin, J., McColl, E., Thomas, L., Kaner, E., Stacy, R., Pearson, P., Vernon, B., & Rodgers, H. (1997). The vexed question of authorship: views of researchers in a British medical faculty. *BMJ, 314,* 1009-1012. https://doi.org/10.1136/bmj.314.7086.1009 BIDS (n.d.). *Modality agnostic files.* Brain Imaging Data Structure. [https://bids-specification.readthedocs.io/en/stable/03-modality-agnostic-files.html](https://bids-specification.readthedocs.io/en/stable/03-modality-agnostic-files.html) BIDS. (2020). *About BIDS*. Brain Imaging Data Structure. [https://bids.neuroimaging.io](https://bids.neuroimaging.io/) Bik, E. M., Casadevall, A., & Fang, F. C. (2016). The prevalence of inappropriate image duplication in biomedical research publications. *MBio, 7*(3), e00809-16. Bilder, G. (2013). *DOIs unambiguously and persistently identify published, trustworthy, citable online scholarly literature. Right?* Crossref. [https://www.crossref.org/blog/dois-unambiguously-and-persistently-identify-published-trustworthy-citable-online-scholarly-literature-right/](https://www.crossref.org/blog/dois-unambiguously-and-persistently-identify-published-trustworthy-citable-online-scholarly-literature-right/) Bishop, D. V. (2020). The psychology of experimental psychologists: Overcoming cognitive constraints to improve research: The 47th Sir Frederic Bartlett Lecture. *Quarterly Journal of Experimental Psychology, 73*(1), 1-19. [https://doi.org/10.1177/1747021819886519](https://doi.org/10.1177/1747021819886519) Björneborn, L., & Ingwersen, P. (2004). Toward a basic framework for webometrics. *Journal of the American society for information science and technology, 55*(14), 1216-1227.https://doi.org/10.1002/asi.20077 Blohowiak, B. B., Cohoon, J., de-Wit, L., Eich, E., Farach, F. J., Hasselman, F., … Riss, C. (2020, July 4). Badges to Acknowledge Open Practices. Retrieved from osf.io/tvyxz BMJ. (2015). *Introducing ‘How to write and publish a Study Protocol’ using BMJ’s new eLearning programme: Research to Publication.* Retrieved, March 2021, from: https://blogs.bmj.com/bmjopen/2015/09/22/introducing-how-to-write-and-publish-a-study-protocol-using-bmjs-new-elearning-programme-research-to-publication/ Boivin, A., Richards, T., Forsythe, L., Gregoire, A., L’Esperance, A., Abelson, J., & Carman, K.L. (2018). Evaluating the patient and public involvement in research. *British Medical Journal, 363*, k5147. https://doi.org/10.1136/bmj.k5147 Bol, T., de Vaan, M., & van de Rijt, A. (2018). The Matthew effect in science funding. *Proceedings of the National Academy of Sciences, 115*(19), 4887-4890. https://doi.org/10.1073/pnas.1719557115 Bollen, K. A. (1989). *Structural Equations with Latent Variables* (pp. 179-225). John Wiley & Sons. Borenstein, M., Hedges, L. V., Higgins, J. P., & Rothstein, H. R. (2011). *Introduction to meta-analysis.* John Wiley & Sons. Bornmann, L., Ganser, C., Tekles, A., & Leydesdorff, L. (2019). Does the $ h\\_\\\\alpha $ index reinforce the Matthew effect in science? Agent-based simulations using Stata and R. *arXiv preprint arXiv:1905.11052.* Borsboom, D., Mellenbergh, G. J., & Van Heerden, J. (2004). The concept of validity. *Psychological review, 111*(4), 1061\\. [https://doi.org/10.1037/0033-295X.111.4.1061](https://doi.org/10.1037/0033-295X.111.4.1061) Borsboom, D., van der Maas, H., Dalege, J., Kievit, R., & Haig, B. (2020, February 29). Theory Construction Methodology: A practical framework for theory formation in psychology. https://doi.org/10.31234/osf.io/w5tp8 Bourne, P. E., Polka, J. K., Vale, R. D., & Kiley, R. (2017). Ten simple rules to consider regarding preprint submission.*PLoS Computational Biology, 13*(5), e1005473. [https://doi.org/10.1371/journal.pcbi.1005473](https://doi.org/10.1371/journal.pcbi.1005473) Box, G.E. P. (1976). Science and statistics. J*ournal of the American Statistical Association 71*(356), 791–799. Bramoullé, Y., & Saint-Paul, G. (2010). Research cycles. *Journal of economic theory, 145*(5), 1890-1920. [https://doi.org/10.2139/ssrn.965816](https://doi.org/10.2139/ssrn.965816) Brand, A., Allen, L., Altman, M., Hlava, M., & Scott, J. (2015). Beyond authorship: attribution, contribution, collaboration, and credit. *Learned Publishing, 28*(2), 151-155. [https://doi.org/10.1087/20150211](https://doi.org/10.1087/20150211) Brandt, M. J., IJzerman, H., Dijksterhuis, A., Farach, F. J., Geller, J., Giner-Sorolla, R., ... & Van't Veer, A. (2014). The replication recipe: What makes for a convincing replication?. *Journal of Experimental Social Psychology, 50*, 217-224. https://doi.org/10.1016/j.jesp.2013.10.005 Braun, V., & Clarke, V. (2013) *Successful Qualitative Research.* SAGE Publications. Brembs, B., Button, K., & Munafò, M. (2013). Deep impact: unintended consequences of journal rank. *Frontiers in Human Neuroscience, 7*, 291\\.[https://doi.org/10.3389/fnhum.2013.00291](https://doi.org/10.3389/fnhum.2013.00291) Breznau, N. (2021). I saw you in the crowd: Credibility, reproducibility, and meta-utility. *PS: Political Science & Politics, 54*(2), 309-313. [https://doi.org/10.1017/S1049096520000980](https://doi.org/10.1017/S1049096520000980) Brod, M., Tesler, L., & Christensen, T. (2009). Qualitative research and content validity: Developing best practices based on science and experience. *Quality of Life Research, 18*(9), 1263–1278. [https://doi.org/10.1007/s11136-009-9540-9](https://doi.org/10.1007/s11136-009-9540-9) Brooks, T. A. (1985). Private acts and public objects: An investigation of citer motivations. *Journal of the American Society for Information Science, 36*(4), 223-229. [https://doi.org/10.1002/asi.4630360402](https://doi.org/10.1002/asi.4630360402) Brown, N. J. and Heathers, J. A. (2017). The grim test: A simple technique detects numerous anomalies in the reporting of results in psychology. *Social Psychological and Personality Science, 8*(4), 363–369. Brown, N., Thompson, P., & Leigh, J. S. (2018). Making academia more accessible. *Journal of Perspectives in Applied Academic Practice*, *6*(2), 82–90. [https://doi.org/10.14297/jpaap.v6i2.348](https://doi.org/10.14297/jpaap.v6i2.348) Brule, J., & Blount, A. (1989). *Knowledge acquisition.* New York : McGraw-Hill Brunner, J., & Schimmack, U. (2020). Estimating population mean power under conditions of heterogeneity and selection for significance. *Meta-Psychology, 4*, MP.2018.874. https://doi.org/1[0.15626/MP.2018.874](https://doi.org/10.15626/MP.2018.874) Bruns, S. B., & Ioannidis, J. P. (2016). P-curve and p-hacking in observational research. *PLoS ONE, 11*(2), e0149144. https://doi.org/10.1371/journal.pone.0149144 Budapest Open Access Initiative (2002) *Read the Budapest open access initiative.* Budapest, Hungary. Available from: [https://www.budapestopenaccessinitiative.org/read](https://www.budapestopenaccessinitiative.org/read) Burnette, M., Williams, S., & Imker, H. (2016). From Plan to Action: Successful Data Management Plan Implementation in a Multidisciplinary Project. *Journal of eScience librarianship, 5*(1), e1101. https://doi.org/10.7191/jeslib.2016.1101 Button, K. S., Chambers, C. D., Lawrence, N., & Munafò, M. R. (2020). Grassroots training for reproducible science: a consortium-based approach to the empirical dissertation. *Psychology Learning & Teaching, 19*(1), 77-90. https://doi.org/10.1177/1475725719857659 Button, K. S., Lawrence, N. S., Chambers, C. D., & Munafò, M. R. (2016). Instilling scientific rigour at the grassroots. *Psychologist, 29*(3), 158-159. Busse, C., Kach, A. P., & Wagner, S. M. (2017). Boundary conditions: What they are, how to explore them, why we need them, and when to consider them. Organizational Research Methods, 20(4), 574–609. [https://doi.org/10.1177/1094428116641191](https://doi.org/10.1177/1094428116641191) Byrne J. A. & Christopher J. (2020). Digital magic, or the dark arts of the 21st century—how can journals and peer reviewers detect manuscripts and publications from paper mills? *FEBS Lett, 594*(4), 583-589. [https://doi.org/10.1002/1873-3468.13747](https://doi.org/10.1002/1873-3468.13747) Calvert, D. (2019, June 5). *How to Make Inclusivity More Than Just an Office Buzzword.* Kellogg Insight. https://insight.kellogg.northwestern.edu/article/how-to-make-inclusivity-more-than-just-an-office-buzzword Campbell, D. T., & Stanley, J.C. (1966) *Experimental and Quasi Experimental Designs.* Rand McNally. Carp, J. (2012). On the plurality of (methodological) worlds: estimating the analytic flexibility of FMRI experiments. *Frontiers in Neuroscience, 6*, 149\\. https://doi.org/10.3389/fnins.2012.00149 Carsey, T. M. (2014). Making DA-RT a reality. *PS: Political Science & Politics, 47*(1), 72–77. [https://doi.org/10.1017/S1049096513001753](https://doi.org/10.1017/S1049096513001753) Carter, A., Tilling, K., & Munafo, M. R. (2021, January 26). Considerations of sample size and power calculations given a range of analytical scenarios. https://doi.org/10.31234/osf.io/tcqrn Case, C. M. (1928). Scholarship in Sociology. *Sociology and Social Research, 12*, 323–340. http://www.sudoc.fr/036493414 Cassidy, S. A., Dimova, R., Giguère, B., Spence, J. R., & Stanley, D. J. (2019). Failing grade: 89% of introduction-to-psychology textbooks that define or explain statistical significance do so incorrectly. *Advances in Methods and Practices in Psychological Science, 2*(3), 233-239. https://doi.org/10.1177/2515245919858072 Centre for Evaluation. (n.d.). *Evidence Synthesis.* https://www.lshtm.ac.uk/research/centres/centre-evaluation/evidence-synthesis Centre for Open Science. (2011-2021) *Open Science Framework*. Centre for Open Science. [https://osf.io/](https://osf.io/) Centre for Open Science. (n.d.). S*how Your Work. Share Your Work.* Advance Science. That's Open Science. The Centre for Open Science. https://www.cos.io/ CESSDA Training Team (2017 \\- 2020). C*ESSDA Data Management Expert Guide. B*ergen, Norway: CESSDA ERIC. Retrieved from [https://www.cessda.eu/DMGuide](https://www.cessda.eu/DMGuide) Chambers, C. D. (2013). Registered reports: a new publishing initiative at Cortex. C*ortex, 49*(3), 609-610. https://doi.org/10.1016/j.cortex.2012.12.016. Chambers, C. D., Dienes, Z., McIntosh, R. D., Rotshtein, P., & Willmes, K. (2015). Registered reports: realigning incentives in scientific publishing. *Cortex, 66*, A1-A2. https://doi.org/10.1016/j.cortex.2015.03.022. Chambers, C. D., & Tzavella, L. (2020, February 10). Registered Reports: Past, Present and Future. [https://doi.org/10.31222/osf.io/43298](https://doi.org/10.31222/osf.io/43298) Chartier, C. R., Riegelman, A., & McCarthy, R. J. (2018). StudySwap: A platform for interlab replication, collaboration, and resource exchange. *Advances in Methods and Practices in Psychological Science, 1*(4), 574-579. [https://doi.org/10.1177/2515245918808767](https://doi.org/10.1177/2515245918808767) Chuard, P. J. C., Vrtilek, M., Head, M. L., & Jennions, M. D. (2019). Evidence that non-significant results are sometimes preferred: Reverse P-hacking or selective reporting? *PLoS Biol 17*(1), e3000127. https://doi.org/10.1371/journal.pbio.3000127 Citizen Science Association (2015). *Who We Are.* Citizen Science. [https://www.citizenscience.org/about-3/](https://www.citizenscience.org/about-3/) Claerbout, J. F., & Karrenbach, M. (1992). Electronic documents give reproducible research a new meaning. In *SEG Technical Program Expanded Abstracts 1992* (pp. 601-604). Society of Exploration Geophysicists. Available at [http://sepwww.stanford.edu/doku.php?id=sep:research:reproducible:seg92](http://sepwww.stanford.edu/doku.php?id=sep:research:reproducible:seg92) Clark, H., Elsherif, M. M., & Leavens, D. A. (2019). Ontogeny vs. phylogeny in primate/canid comparisons: a meta-analysis of the object choice task. *Neuroscience & Biobehavioral Reviews, 105,* 178-189. [https://doi.org/10.1016/j.neubiorev.2019.06.001](https://doi.org/10.1016/j.neubiorev.2019.06.001) Cohen, J. (1962). The statistical power of abnormal-social psychological research: A review. *The Journal of Abnormal and Social Psychology, 65*(3), 145–153. https://doi.org/10.1037/h0045186 Cohen, J. (1969). *Statistical power analysis for the behavioral sciences.* Academic Press. Cohn, J. P. (2008). Citizen science: Can volunteers do real research?. *BioScience, 58*(3), 192-197. [https://doi.org/10.1641/B580303](https://doi.org/10.1641/B580303) Coles, N. A.; Tiokhin, L.; Arslan, R.; Forscher, P.; Scheel, A.; & Lakens, D. (2020, May 11). *Red Team Challenge.* [http://daniellakens.blogspot.com/2020/05/red-team-challenge.html](http://daniellakens.blogspot.com/2020/05/red-team-challenge.html) Committee on Reproducibility and Replicability in Science, Board on Behavioral, Cognitive, and Sensory Sciences, Committee on National Statistics, Division of Behavioral and Social Sciences and Education, Nuclear and Radiation Studies Board, Division on Earth and Life Studies, … National Academies of Sciences, Engineering, and Medicine. (2019). Reproducibility and Replicability in Science (p. 25303). National Academies Press. https://doi.org/10.17226/25303 Confederation of Open Access Repositories. (2020, October 8). COAR Community Framework for Best Practices in Repositories. (Version 1). Zenodo. [http://doi.org/10.5281/zenodo.4110829](http://doi.org/10.5281/zenodo.4110829) Cook, T. D., & Campbell, D. T. (1979). *Quasi-Experimentation.* Rand McNally. Coproduction Collective (2021). *Our approach.* [https://www.coproductioncollective.co.uk/what-is-co-production/our-approach](https://www.coproductioncollective.co.uk/what-is-co-production/our-approach) Corley, K. G., & Gioia, D. A. (2011). Building theory about theory building: what constitutes a theoretical contribution?. *Academy of management review, 36*(1), 12-32. [https://doi.org/10.5465/amr.2009.0486](https://doi.org/10.5465/amr.2009.0486) Cornell University (2020). *Measuring your research impact: i10 index.* Cornell University Library. https://guides.library.cornell.edu/impact/author-impact-10 Corti, L., Van den Eynden, V., Bishop, L., & Woollard, M. (2019). *Managing and sharing research data: a guide to good practice.* Sage. Cowan, N., Belletier, C., Doherty, J. M., Jaroslawska, A. J., Rhodes, S., Forsberg, A., ... & Logie, R. H. (2020). How do scientific views change? Notes from an extended adversarial collaboration. *Perspectives on Psychological Science, 15*(4), 1011-1025. [https://doi.org/10.1177/1745691620906415](https://doi.org/10.1177/1745691620906415) Crenshaw, K. W. (1989). Demarginalizing the Intersection of Race and Sex: A Black Feminist Critique of Antidiscrimination Doctrine. *University of Chicago Legal Forum, 1989* (8), 139–168. Cronbach, L. J., & Meehl, P. E. (1955). Construct validity in psychological tests. P*sychological Bulletin, 52*(4), 281–302. [https://doi.org/10.1037/h0040957](https://psycnet.apa.org/doi/10.1037/h0040957) Crowdsourcing Week. (2021, April 29). *What is Crowdsourcing?* https://crowdsourcingweek.com/what-is-crowdsourcing/ Crutzen, R., Ygram Peters, G. J., & Mondschein, C. (2019). Why and how we should care about the General Data Protection Regulation. *Psychology & health*, *34*(11), 1347-1357. https://doi.org/10.1080/08870446.2019.1606222 Crüwell, S., van Doorn, J., Etz, A., Makel, M. C., Moshontz, H., Niebaum, J. C., Orben, A., Parsons, S., & Schulte-Mecklenbeck, M. (2019). Seven Easy Steps to Open Science: An Annotated Reading List. Zeitschrift Für Psychologie, 227(4), 237–248. [https://doi.org/10.1027/2151-2604/a000387](https://doi.org/10.1027/2151-2604/a000387) Curry, S. (2012) *Sick of impact factors.* \\[blogpost\\] [http://occamstypewriter.org/scurry/2012/08/13/sick-of-impact-factors/](http://occamstypewriter.org/scurry/2012/08/13/sick-of-impact-factors/) d’Espagnat, B. (2008). Is science cumulative? A physicist viewpoint. In *Rethinking Scientific Change and Theory Comparison* (pp. 145-151). Springer, Dordrecht. [https://doi.org/10.1007/978-1-4020-6279-7\\_10](https://doi.org/10.1007/978-1-4020-6279-7_10) Davies, G. M., & Gray, A. (2015). Don’t let spurious accusations of pseudoreplication limit our ability to learn from natural experiments (and other messy kinds of ecological monitoring). *Ecology and Evolution, 5*(22), 5295–5304. [https://doi.org/10.1002/ece3.1782](https://doi.org/10.1002/ece3.1782) Del Giudice, M., & Gangestad, S. W. (2021). A traveler’s guide to the multiverse: Promises, pitfalls, and a framework for the evaluation of analytic decisions. *Advances in Methods and Practices in Psychological Science, 4*(1), 2515245920954925\\. [https://doi.org/10.1177/2515245920954925](https://doi.org/10.1177/2515245920954925) DeVellis, R. F. (2017). *Scale development: Theory and applications* (4th ed.). Sage. Der Kiureghian, A., & Ditlevsen, O. (2009). Aleatory or epistemic? Does it matter?. Structural Safety, 31(2), 105-112. [https://doi.org/10.1016/j.strusafe.2008.06.020](https://doi.org/10.1016/j.strusafe.2008.06.020) Dienes, Z. (2008). Understanding psychology as a science: An introduction to scientific and statistical inference. Macmillan International Higher Education. Dienes, Z. (2011). Bayesian versus orthodox statistics: Which side are you on?. *Perspectives on Psychological Science, 6*(3), 274-290.https://doi.org/10.1177/1745691611406920 Dienes, Z. (2014). Using Bayes to get the most out of non-significant results. *Frontiers in psychology, 5*, 781\\. https://doi.org/10.3389/fpsyg.2014.00781 Dienes, Z. (2016). How Bayes factors change scientific practice. *Journal of Mathematical Psychology, 72*, 78-89. https://doi.org/10.1016/j.jmp.2015.10.003 Doll, R., & Hill, A. B. (1954). The mortality of doctors in relation to their smoking habits; a preliminary report. *British Medical Journal, 1* (4877), 1451–1455. doi:10.1136/bmj.1.4877.1451 Drost, E. A. (2011). Validity and reliability in social science research. E*ducation Research and Perspectives, 38*(1), 105-123. Du Bois, W.E.B. (1968). *The souls of black folk; essays and sketches.* Chicago, A.G. McClurg, 1903\\. New York: Johnson Reprint Corp. Duyx, B., Swaen, G. M., Urlings, M. J., Bouter, L. M., & Zeegers, M. P. (2019). The strong focus on positive results in abstracts may cause bias in systematic reviews: a case study on abstract reporting bias. *Systematic reviews, 8*(1), 1-8. Duval, S., & Tweedie, R. (2000a). A nonparametric “trim and fill” method of accounting for publication bias in meta-analysis. *Journal of the American Statistical Association, 95*, 89–98. [https://doi.org/10.2307/2669529](https://doi.org/10.2307/2669529) Duval, S., & Tweedie, R. (2000b). Trim and fill: A simple funnel-plot–based method of testing and adjusting for publication bias in meta-analysis. *Biometrics, 56*, 455–463. https://doi.org/10.1111/j.0006-341x.2000.00455.x. Eagly, A. H., & Riger, S. (2014). Feminism and psychology: Critiques of methods and epistemology. *American Psychologist, 69*(7), 685–702. https://doi.org/10.1037/a0037372 Ebersole, C. R., Atherton, O. E., Belanger, A. L., Skulborstad, H. M., Allen, J. M., Banks, J. B., … Nosek, B. A. (2016). Many Labs 3: Evaluating participant pool quality across the academic semester via replication. *Journal of Experimental Social Psychology, 67,* 68–82. [https://doi.org/10.1016/j.jesp.2015.10.012](https://doi.org/10.1016/j.jesp.2015.10.012) Edyburn, D. L. (2010). Would you recognize universal design for learning if you saw it? Ten propositions for new directions for the second decade of UDL. L*earning Disability Quarterly, 33*(1), 33-41. https://doi.org/10.1177/073194871003300103 Ellemers, N. (2021). Science as collaborative knowledge generation. *British Journal of Social Psychology, 60* (1), 1-28.https://doi.org/10.1111/bjso.12430 Eley, A. R. (2012). *Becoming a successful early career researcher.* Routledge. [http://www.worldcat.org/oclc/934369360](http://www.worldcat.org/oclc/934369360) Elliott, K. C., & Resnik, D. B. (2019). Making open science work for science and society. *Environmental Health Perspectives, 127*(7). [https://doi.org/10.1289/EHP4808](https://doi.org/10.1289/EHP4808) Epskamp, S. & Nuijten, M. B. (2016). statcheck: Extract statistics from articles and recompute p values. Retrieved from [http://CRAN.R-project.org/package=statcheck](http://CRAN.R-project.org/package=statcheck) . (R package version 1.2.2) Esterling, K., Brady, D., & Schwitzgebel, E. (2021, January 27). *The Necessity of Construct and External Validity for Generalized Causal Claims.* [https://doi.org/10.31219/osf.io/2s8w5](https://doi.org/10.31219/osf.io/2s8w5) Etz, A., Gronau, Q.F., Dablander, F. et al. (2018). How to become a Bayesian in eight easy steps: An annotated reading list. *Psychon Bull Rev 25*, 219–234. [https://doi.org/10.3758/s13423-017-1317-5](https://doi.org/10.3758/s13423-017-1317-5) European Commission (2021, January 17th). *Responsible research & innovation.* Horizon 2020\\. [https://ec.europa.eu/programmes/horizon2020/en/h2020-section/responsible-research-innovation](https://ec.europa.eu/programmes/horizon2020/en/h2020-section/responsible-research-innovation) F. (2019, December 13). *Introducing a Framework for Open and Reproducible Research Training (FORRT).* https://doi.org/10.31219/osf.io/bnh7p Fanelli, D. (2010). Do Pressures to Publish Increase Scientists' Bias? An Empirical Support from US States Data. *PLOS ONE. 5* (4), e10271. https://doi.org/10.1371/journal.pone.0010271 Fanelli, D. (2018). Opinion: Is science really facing a reproducibility crisis, and do we need it to?. *Proceedings of the National Academy of Sciences, 115*(11), 2628-2631. [https://doi.org/10.1073/pnas.1708272114](https://doi.org/10.1073/pnas.1708272114) Faul, F., Erdfelder, E., Lang, A.-G., & Buchner, A. (2007). G\\*Power 3: A flexible statistical power analysis program for the social, behavioral, and biomedical sciences. B*ehavior Research Methods, 39*, 175-191. https://doi.org/10.3758/BF03193146 Faul, F., Erdfelder, E., Buchner, A., & Lang, A.-G. (2009). Statistical power analyses using G\\*Power 3.1: Tests for correlation and regression analyses. *Behavior Research Methods, 41*, 1149-1160. [https://doi.org/10.3758/BRM.41.4.1149](https://doi.org/10.3758/BRM.41.4.1149) Ferson, S., Joslyn, C. A., Helton, J. C., Oberkampf, W. L., & Sentz, K. (2004). Summary from the epistemic uncertainty workshop: consensus amid diversity. *Reliability Engineering & System Safety, 85*(1-3), 355-369. [https://doi.org/10.1016/j.ress.2004.03.023](https://doi.org/10.1016/j.ress.2004.03.023) Fiedler K., Kutzner F., Krueger J. I.. (2012). The long way from α-error control to validity proper: Problems with a short-sighted false-positive debate. Perspectives on *Psychological Science, 7*(6), 661-669. https://doi.org/ 10.1177/1745691612462587. Fiedler, K., & Schwarz, N. (2016). Questionable research practices revisited. Social *Psychological and Personality Science, 7*(1), 45–52. https://doi.org/10.1177/1948550615612150 Filipe, A., Renedo, A., & Marston, C. (2017). The co-production of what? Knowledge, values, and social relations in health care. *PLoS biology, 15*(5), e2001403. https://doi.org/10.1371/journal.pbio.2001403 Fillon, A.A., Feldman, G., Yeung, S. K., Protzko, J., Elsherif, M. M., Xiao, Q., Nanakdewa, K. & Brick, C. (2021). *Correlational Meta-Analysis Registered Report Template.* \\[Manuscript in preparation\\]. Findley, M. G., Jensen, N. M., Malesky, E. J., & Pepinsky, T. B. (2016). Can results-free review reduce publication bias? The results and implications of a pilot study. *Comparative Political Studies, 49*(13), 1667–1703. [https://doi.org/10.1177/0010414016655539](https://doi.org/10.1177/0010414016655539) Finlay, L., & Gough, B. (Eds.). (2008). *Reflexivity: A practical guide for researchers in health and social sciences.* John Wiley & Sons. Flake, J. K., & Fried, E. I. (2020). Measurement schmeasurement: Questionable measurement practices and how to avoid them. *Advances in Methods and Practices in Psychological Science, 3*(4), 456-465. [https://doi.org/10.1177%2F2515245920952393](https://doi.org/10.1177%2F2515245920952393) FORRT. (2021). *Welcome to FORRT.* Framework for Open and Reproducible Research Training. https://forrt.org Foster, E. D., & Deardorff, A. (2017). Open science framework (OSF). *Journal of the Medical Library Association: JMLA, 105*(2), 203\\. https://doi.org/ 10.5195/jmla.2017.88 Franco, A., Malhotra, N., & Simonovits, G. (2014). Publication bias in the social sciences: \tUnlocking the file drawer. *Science, 345*(6203), 1502-1505. [https://doi.org/10.1126/science.1255484](https://doi.org/10.1126/science.1255484) Franzoni, C., & Sauermann, H. (2014). Crowd science: The organization of scientific research in open collaborative projects. *Research Policy, 43*(1), 1–20. https://doi.org/10.1016/j.respol.2013.07.005 Fraser, H., Bush, M., Wintle, B., Mody, F., Smith, E., Hanea, A., ... & Fidler, F. (2021). *Predicting reliability through structured expert elicitation with repliCATS* (Collaborative Assessments for Trustworthy Science). Free Our Knowledge. (n.d.). *About*. Free Our Knowledge. https://freeourknowledge.org/about/. Frith, U. (2020). Fast lane to slow science. *Trends in Cognitive Sciences, 24*(1), 1-2.https://doi.org/10.1016/j.tics.2019.10.007 Galligan, F., & Dyas-Correia, S. (2013). Altmetrics: rethinking the way we measure. *Serials Review, 39*(1), 56-61. https://doi.org/10.1016/j.serrev.2013.01.003 Gelman, A., & Loken, E. (2013). The garden of forking paths: Why multiple comparisons can be a problem, even when there is no “fishing expedition” or “p-hacking” and the research hypothesis was posited ahead of time. *Department of Statistics, Columbia University, 348\\.* http://www.stat.columbia.edu/\\~gelman/research/unpublished/p\\_hacking.pdf Gelman, A., & Carlin, J. (2014). Beyond Power Calculations: Assessing Type S (Sign) and Type M (Magnitude) Errors. *Perspectives on Psychological Science, 9*(6), 641-651. https://doi.org/ 10.1177/1745691614551642 Gelman, A., & Stern. H. (2006) The difference between “significant” and “not significant” is not itself statistically significant. The American Statistician, 60(4), 328-331. https://doi.org/10.1198/000313006X152649 Gentleman, R. (2005). Reproducible Research: A Bioinformatics Case Study. *Statistical Applications in Genetics and Molecular Biology, 4*, 1034\\. https://doi.org/10.2202/1544-6115.1034 German Research Foundation (2019). Guidelines for Safeguarding Good Research Practice. Code of Conduct. [http://doi.org/10.5281/zenodo.3923602](http://doi.org/10.5281/zenodo.3923602) Gilroy, P. (1993). *The black Atlantic: Modernity and double consciousness*. New York: Harvard University Press. Giner-Sorolla, R., Aberson, C. L., Bostyn, D. H., Carpenter, T., Conrique, B. G., Lewis, N. A., & Soderberg, C. (2019). Power to detect what? Considerations for planning and evaluating sample size \\[Preprint\\]. https://osf.io/jnmya/ Ginsparg, P. (1997). Winners and losers in the global research village, *The Serials Librarian, 30*(3-4), 83-95. https://doi.org/10.1300/J123v30n03\\_13 Ginsparg, P. (2001). Creating a global knowledge network. In *Second Joint ICSU Press-UNESCO Expert Conference on Electronic Publishing in Scienc*e (pp. 19-23). Gioia, D. A., & Pitre, E. (1990). Multiparadigm perspectives on theory building. Academy of management review, 15(4), 584-602. https://doi.org/10.5465/amr.1990.4310758 Glass, D. J., & Hall, N. (2008). A brief history of the hypothesis. *Cell, 134*(3), 378-381. https://doi.org/10.1016/j.cell.2008.07.033 Goertzen, M.J. (2017). *Introduction to Quantitative Research and Data.* Library Technology Reports. 53(4), 12–18. Gollwitzer, M., Abele-Brehm, A., Fiebach, C., Ramthun, R., Scheel, A. M., Schönbrodt, F. D., & Steinberg, U. (2020, September 10). Data Management and Data Sharing in Psychological Science: Revision of the DGPs Recommendations. https://doi.org/10.31234/osf.io/24ncs Goodman, S. N., Fanelli, D., & Ioannidis, J. P. A. (2016). What does research reproducibility mean? *Science Translational Medicine, 8*(341), 341ps12-341ps12. https://doi.org/10.1126/scitranslmed.aaf5027 Gorgolewski, K., Auer, T., Calhoun, V. et al. (2016). The brain imaging data structure, a format for organizing and describing outputs of neuroimaging experiments. S*cientific Data, 3,* 160044\\. [https://doi.org/10.1038/sdata.2016.44](https://doi.org/10.1038/sdata.2016.44) Graham, I. D., McCutcheon, C., & Kothari, A. (2019). Exploring the frontiers of research co-production: the Integrated Knowledge Translation Research Network concept papers. *Health Research Policy and Systems, 17*, 88\\. [https://doi.org/10.1186/s12961-019-0501-7](https://doi.org/10.1186/s12961-019-0501-7) GRN Â· German Reproducibility Network. (n.d.). *A German Reproducibility Network.* Retrieved 5 June 2021, from https://reproducibilitynetwork.de/ Grossmann, A., & Brembs, B. (2021). Current market rates for scholarly publishing services. *F1000Research, 10*(20), 20\\. https://doi.org/10.12688/f1000research.27468.1 Grzanka, P. R. (2020). From buzzword to critical psychology: An invitation to take intersectionality seriously. *Women & Therapy, 43*(3-4), 244-261. Guest, O. \\[@o\\_guest\\]. (2017, June 5). *Thanks\\! Hopefully this thread & many other similar discussions & blogs will help make it less Bropen Science and more Open Science. \\*hides\\** \\[Tweet\\]. Twitter. https://twitter.com/o\\_guest/status/871675631062458368 Guest, O., & Martin, A. E. (2020). How computational modeling can force theory building in psychological science. *Perspectives on Psychological Science.* https://doi.org/10.1177/1745691620970585 Haak, L. L., Fenner, M., Paglione, L., Pentz, E., & Ratner, H. (2012). ORCID: A system to uniquely identify researchers. *Learned Publishing, 25*(4), 259-264. doi:10.1087/20120404 Hackett, R., & Kelly, S. (2020). Publishing ethics in the era of paper mills. *Biology Open, 9*(10), bio056556. [https://doi.org/10.1242/bio.056556](https://doi.org/10.1242/bio.056556) Hardwicke, T. E., Jameel, L., Jones, M., Walczak, E. J., & Weinberg, L. M. (2014). Only human: Scientists, systems, and suspect statistics. *Opticon1826, 16,* 25\\. DOI:10.5334/OPT.CH Hardwicke, T. E., Bohn, M., MacDonald, K., Hembacher, E., Nuijten, M. B., Peloquin, B. N., ... & Frank, M. C. (2020). Analytic reproducibility in articles receiving open data badges at the journal Psychological Science: an observational study. *Royal Society Open Science, 8*(1), 201494\\. https://doi.org/10.1098/rsos.201494 Hart, D. D., & Silka, L. (2020). Rebuilding the Ivory Tower: A Bottom-Up Experiment in Aligning Research with Societal Needs. *Issues in Science and Technology,* 79-85. https://issues.org/aligning-research-with-societal-needs/ Hartgerink, C. H., Wicherts, J. M., & Van Assen, M. A. L. M. (2017). Too good to be false: Nonsignificant results revisited. *Collabra: Psychology, 3*(1). https://doi.org/10.1525/collabra.71 Haven, T. L., & van Grootel, L. (2019). Preregistering qualitative research. *Accountability in Research, 26*(3), 229–244. [https://doi.org/10.1080/08989621.2019.1580147](https://doi.org/10.1080/08989621.2019.1580147) Haynes, S. N., Richard, D. C. S., & Kubany, E. S. (1995). Content validity in psychological assessment: A functional approach to concepts and methods. *Psychological Assessment, 7*(3), 238–247. https://doi.org/10.1037/1040-3590.7.3.238 Health Research Board (n.d.) *Declaration on Research Assessment.* Available from: [https://www.hrb.ie/funding/funding-schemes/before-you-apply/how-we-assess-applications/declaration-on-research-assessment/](https://www.hrb.ie/funding/funding-schemes/before-you-apply/how-we-assess-applications/declaration-on-research-assessment/) Healy, K. (2018). *Data visualization: A practical introduction.* Princeton University Press. Heathers JA, Anaya J, van der Zee T, Brown NJ. (2018). Recovering data from summary statistics: Sample Parameter Reconstruction via Iterative TEchniques (SPRITE). *PeerJ Preprints 6*:e26968v1. [https://doi.org/10.7287/peerj.preprints.26968v1](https://doi.org/10.7287/peerj.preprints.26968v1) Hendriks, F., Kienhues, D., & Bromme, R. (2016). Trust in science and the science of trust. In *Trust and communication in a digitized world* (S. 143–159). Springer. Henrich, J. (2020). T*he weirdest people in the world: How the west became psychologically peculiar and particularly prosperous.* Farrar, Straus and Giroux. Henrich, J., Heine, S. J., & Norenzayan, A. (2010). The weirdest people in the world?. *Behavioral and brain sciences, 33*(2-3), 61-83.[https://doi.org/10.1017/S0140525X0999152X](https://doi.org/10.1017/S0140525X0999152X) Herrmannova, D., & Knoth, P. (2016). *Semantometrics Towards Full text-based Research Evaluation.* [https://arxiv.org/pdf/1605.04180.pdf](https://arxiv.org/pdf/1605.04180.pdf) Heyman, T., Moors, P., & Rabagliati, H. (2020). The benefits of adversarial collaboration for commentaries. *Nature Human Behavior, 4*, 1217\\. [https://doi.org/10.1038/s41562-020-00978-6](https://doi.org/10.1038/s41562-020-00978-6) Higgins, J.P.T., Thomas, J., Chandler, J., Cumpston, M., Li, T., Page, M.J., Welch, V.A. (Eds). (2019). *Cochrane Handbook for Systematic Reviews of Interventions.* 2nd Edition. Chichester, UK: John Wiley & Sons. Himmelstein, D. S., Rubinetti, V., Slochower, D. R., Hu, D., Malladi, V. S., Greene, C. S., & Gitter, A. (2019). Open collaborative writing with Manubot. *PLOS Computational Biology, 15*(6), e1007128. [https://doi.org/10.1371/journal.pcbi.1007128](https://doi.org/10.1371/journal.pcbi.1007128) Hirsch, J. E. (2005). An index to quantify an individual's scientific research output. *Proceedings of the National Academy of Sciences, 102*(46), 16569-16572. [https://doi.org/10.1073/pnas.0507655102](https://doi.org/10.1073/pnas.0507655102) Hitchcock, C., Meyer, A., Rose, D., & Jackson, R. (2002). Providing new access to the general curriculum: Universal design for learning. *Teaching exceptional children, 35*(2), 8-17. [https://www.proquest.com/scholarly-journals/providing-new-access-general-curriculum/docview/201139970/se-2?accountid=8630](https://www.proquest.com/scholarly-journals/providing-new-access-general-curriculum/docview/201139970/se-2?accountid=8630) Hoekstra, R., Kiers, H., & Johnson, A. (2012). Are assumptions of well-known statistical techniques checked, and why (not)?. *Frontiers in Psychology, 3*(137), 1-9. [https://doi.org/10.3389/fpsyg.2012.00137](https://doi.org/10.3389/fpsyg.2012.00137) Hogg, D., Bovy, J., & Lang, D. (2010). Data analysis recipes: Fitting a model to data. arXiv arXiv:1008.4686 \\[astro-ph.IM\\] Hoijtink, H., Mulder, J., van Lissa, C., & Gu, X. (2019). A tutorial on testing hypotheses using the Bayes factor. *Psychological Methods, 24*(5), 539–556. https://doi.org/10.1037/met0000201 Holcombe, A. O. (2019). Contributorship, not authorship: Use CRediT to indicate who did what. *Publications, 7*(3), 48\\. [https://doi.org/10.3390/publications7030048](https://doi.org/10.3390/publications7030048) Holcombe, A. O., Kovacs, M., Aust, F., & Aczel, B. (2020). Documenting contributions to scholarly articles using CRediT and tenzing. *Plos one, 15*(12), e0244611. Holden, R. B. (2010). Face Validity. In I. B. Weiner, & W. E. Craighead (Eds.), The Corsini Encyclopedia of Psychology (4th ed.). Hoboken, NJ: Wiley. [http://dx.doi.org/10.1002/9780470479216.corpsy0341](http://dx.doi.org/10.1002/9780470479216.corpsy0341) Homepage. (n.d.). *Open Science MOOC.* Retrieved 5 June 2021, from https://opensciencemooc.eu/ Houtkoop, B. L., Chambers, C., Macleod, M. Bishop, D. V. M. Nichols, T. E., & Wagenmekers, E.-J. (2018). Data sharing in psychology: A survey on barriers and preconditions. *Advances in Methods and Practices in Psychological Science, 1*(1), 70.85. https://doi.org/10.1177/2515245917751886 Huber, B., Barnidge, M., Gil de Zúñiga, H., & Liu, J. (2019). Fostering public trust in science: The role of social media. *Public understanding of science, 28*(7), 759-777. https://doi.org/10.1177/0963662519869097 Huelin, R., Iheanacho, I., Payne, K., & Sandman, K. (2015). What’s in a name? Systematic and non-systematic literature reviews, and why the distinction matters. *The evidence Forum*, 34-37. Retrieved from: [https://www.evidera.com/wp-content/uploads/2015/06/Whats-in-a-Name-Systematic-and-Non-Systematic-Literature-Reviews-and-Why-the-Distinction-Matters.pdf](https://www.evidera.com/wp-content/uploads/2015/06/Whats-in-a-Name-Systematic-and-Non-Systematic-Literature-Reviews-and-Why-the-Distinction-Matters.pdf) Hüffmeier, J., Mazei, J., & Schultze, T. (2016). Reconceptualizing replication as a sequence of different studies: A replication typology. *Journal of Experimental Social Psychology, 66*, 81-92. https://doi.org/10.1016/j.jesp.2015.09.009 Hultsch, D. F., MacDonald, S. W., & Dixon, R. A. (2002). Variability in reaction time performance of younger and older adults. *The Journals of Gerontology Series B: Psychological Sciences and Social Sciences, 57*(2), P101-P115. [https://doi.org/10.1093/geronb/57.2.P101](https://doi.org/10.1093/geronb/57.2.P101) Hurlbert, S. H. (1984). Pseudoreplication and the Design of Ecological Field Experiments. *Ecological Monographs, 54*(2), 187–211. [https://doi.org/10.2307/1942661](https://doi.org/10.2307/1942661) Ikeda, A., Xu, H., Fuji, N., Zhu, S., & Yamada, Y. (2019). Questionable research practices following pre-registration. *Japanese Psychological Review, 62*, 281–295. International Committee of Medical Journal Editors \\[ICMJE\\]. (2019)*. Recommendations for the conduct, reporting, eduting, and publication of scholarly work in medical journals.* http://www.icmje.org/icmje-recommendations.pdf ISO. (1993). *Guide to the Expression of Uncertainty in Measuremen*t. 1st ed. Geneva: International Organization for Standardization. Ioannidis, J. P. (2005). Why most published research findings are false. *PLoS medicine, 2*(8), e124.https://doi.org/10.1371/journal.pmed.0020124 Ioannidis, J. P., Fanelli, D., Dunne, D. D., & Goodman, S. N. (2015). Meta-research: evaluation and improvement of research methods and practices. *PLoS Biology, 13*(10), e1002264. https://doi.org/10.1371/journal.pbio.1002264 JabRef Development Team (2021). *JabRef \\- An open-source, cross-platform citation and reference management software*. https://www.jabref.org Jacobson, D., & Mustafa, N. (2019). Social Identity Map: A Reflexivity Tool for Practicing Explicit Positionality in Critical Qualitative Research. I*nternational Journal of Qualitative Methods, 18*, 1609406919870075\\. [https://doi.org/10.1177/1609406919870075](https://doi.org/10.1177/1609406919870075) Jafar, A. J. N. (2018). What is positionality and should it be expressed in quantitative studies? *Emergency Medicine Journal, 35*(5), 323–324. https://doi.org/10.1136/emermed-2017-207158 James, K. L., Randall, N. P., & Haddaway, N. R. (2016). A methodology for systematic mapping in environmental sciences. *Environmental evidence, 5*(1), 1-13. [https://doi.org/](https://doi.org/10.1242/bio.056556)[10.1186/s13750-016-0059-6](https://doi.org/10.1186/s13750-016-0059-6) Jannot, A. S., Agoritsas, T., Gayet-Ageron, A., & Perneger, T. V. (2013). Citation bias favoring statistically significant studies was present in medical research. J*ournal of clinical epidemiology, 66*(3), 296-301. https://doi.org/10.1016/j.jclinepi.2012.09.015. JASP Team (2020). *JASP* (Version 0.14.1)\\[Computer software\\] John, L. K., Loewenstein, G., & Prelec, D. (2012). Measuring the prevalence of questionable research practices with incentives for truth telling. *Psychological Science, 23*(5), 524–532. [https://doi.org/10.1177/0956797611430953](https://doi.org/10.1177/0956797611430953) Jones, A., Dr, Duckworth, J., & Christiansen, P. (2020, June 29). May I have your attention, please? Methodological and Analytical Flexibility in the Addiction Stroop. https://doi.org/10.31234/osf.io/ws8xp Joseph, T. D., & Hirshfield, L. E. (2011). ‘Why don't you get somebody new to do it?’Race and cultural taxation in the academy. *Ethnic and Racial Studies, 34*(1), 121-141. https://doi.org/10.1080/01419870.2010.496489 Kalliamvakou, E., Gousios, G., Blincoe, K., Singer, L., German, D. M., & Damian, D. (2014). The promises and perils of mining github. In *Proceedings of the 11th working conference on mining software repositories* (pp. 92-101). Kathawalla, U., Silverstein, P., & Syed, M. (2020). Easing into Open Science: A Guide for Graduate Students and Their Advisors*. Collabra: Psychology.* [https://psyarxiv.com/vzjdp](https://psyarxiv.com/vzjdp) Kelley, T. L. (1927). *Interpretation of educational measurements*. New York: Macmillan. Kerr, N. L. (1998). HARKing: Hypothesizing after the results are known. *Personality and social psychology review, 2*(3), 196-217. [https://doi.org/10.1207/s15327957pspr0203\\_4](https://doi.org/10.1207/s15327957pspr0203_4) Kerr, N. L., Ao, X., Hogg, M. A., & Zhang, J. (2018). Addressing replicability concerns via adversarial collaboration: Discovering hidden moderators of the minimal intergroup discrimination effect. *Journal of Experimental Social Psychology, 78*, 66-76. https://doi.org/10.1016/j.jesp.2018.05.001 Kidwell, M. C., Lazarević, L. B., Baranski, E., Hardwicke, T. E., Piechowski, S., Falkenberg, L. S., ... & Nosek, B. A. (2016). Badges to acknowledge open practices: A simple, low-cost, effective method for increasing transparency. *PLoS biology, 14*(5), e1002456.[https://doi.org/10.1371/journal.pbio.1002456](https://doi.org/10.1371/journal.pbio.1002456) Kienzler, H., & Fontanesi, C. (2017). Learning through inquiry: A global health hackathon. *Teaching in Higher Education, 22*(2), 129-142. https://doi.org/10.1080/13562517.2016.1221805 King, G. (1995). Replication, replication. *PS: Political Science & Politics, 28*(3), 444–452. [https://doi.org/10.2307/420301](https://doi.org/10.2307/420301) Kitzes, J., Turek, D., Deniz, F. (2017). *The practice of reproducible research: Case studies and lessons from the data-intensive sciences.* University of California Press. Klein, O., Hardwicke, T. E., Aust, F., Breuer, J., Danielsson, H., Mohr, A. H., IJzerman, H., Nilsonne, G., Vanpaemel, W., & Frank, M. C. (2018). A practical guide for transparency in psychological science. *Collabra: Psychology, 4*(1): 20\\. [https://doi.org/10.1525/collabra.158](https://doi.org/10.1525/collabra.158) Klein, R. A., Ratliff, K. A., Vianello, M., Adams, R. B., Bahník, Š., Bernstein, M. J., et al. (2014). Investigating variation in replicability: A “many labs” replication project. S*ocial Psychology, 45*, 142–152. https://doi.org/10.1027/1864-9335/a000178 Klein, R. A., Vianello, M., Hasselman, F., Adams, B. G., Adams, R. B., Alper, S., … Nosek, B. A. (2018). Many Labs 2: Investigating Variation in Replicability Across Samples and Settings. Advances in Methods and Practices in Psychological *Science, 1*(4), 443–490. [https://doi.org/10.1177/2515245918810225](https://doi.org/10.1177/2515245918810225) Kleinberg, B., Mozes, M., van der Toolen, Y., & verschuere, B. (2017, June 3). NETANOS \\- Named entity-based Text Anonymization for Open Science. Retrieved from [https://osf.io/w9nhb/](https://osf.io/w9nhb/) Knoth, P., & Herrmannova, D. (2014). Towards semantometrics: A new semantic similarity based measure for assessing a research publication’s contribution. *D-Lib Magazine, 20*(11), 8\\. [https://doi.org/10.1045/november14-knoth](https://doi.org/10.1045/november14-knoth) Knowledge, F. O. (2020, December 3). Preregistration Pledge. Free Our Knowledge. https://freeourknowledge.org/2020-12-03-preregistration-pledge/ Koole, S. L., & Lakens, D. (2012). Rewarding replications: A sure and simple way to improve psychological science. *Perspectives on Psychological Science, 7*(6), 608-614. https://doi.org/10.1177/1745691612462586 Kreuter, F. (Ed.). (2013). *Improving Surveys with Paradata.* doi:10.1002/9781118596869 Kruschke, J. K. (2015). *Doing Bayesian data analysis: A tutorial with R, JAGS, and Stan* (2nd ed.). Academic Press. Kuhn, T. (1962). *The Structure of Scientific Revolutions.* University of Chicago Press. ISBN 978-0226458083. Kukull, W.A. & Ganguli, M. (2012). Generalizability: The trees, the forest, and the low-hanging fruit. *Neurology, 78*(23), 1886-1891. [https://doi.org/10.1212/WNL.0b013e318258f812](https://doi.org/10.1212/WNL.0b013e318258f812) Lakens, D. (2014). Performing high-powered studies efficiently with sequential analyses. *European Journal of Social Psychology, 44*(7), 701–710. [https://doi.org/10.1002/ejsp.2023](https://doi.org/10.1002/ejsp.2023) Lakens, D. (2020). Pandemic researchers — recruit your own best critics. *Nature, 581*, 121\\. Lakens, D. (2021, January 4). Sample Size Justification. [https://doi.org/10.31234/osf.io/9d3yf](https://doi.org/10.31234/osf.io/9d3yf) Lakens, D., Scheel, A. M., & Isager, P. M. (2018). Equivalence testing for psychological research: A tutorial. *Advances in Methods and Practices in Psychological Science, 1*(2), 259-269. [https://doi.org/10.1177/2515245918770963](https://doi.org/10.1177/2515245918770963) Lakens, D., McLatchie, N., Isager, P. M., Scheel, A. M., & Dienes, Z. (2020). Improving inferences about null effects with Bayes factors and equivalence tests. *The Journals of Gerontology: Series B, 75*(1), 45-57. [https://doi.org/10.1093/geronb/gby065](https://doi.org/10.1093/geronb/gby065) Laakso, M., & Björk, B. C. (2013). Delayed open access: An overlooked high‐impact category of openly available scientific literature. *Journal of the American Society for Information Science and Technology, 64*(7), 1323-1329. Laine, H. (2017) Afraid of scooping – Case study on researcher strategies against fear of scooping in the context of open science. *Data Science Journal, 16*(29), 1–14. [https://doi.org/10.5334/dsj-2017-029](https://doi.org/10.5334/dsj-2017-029) Larivière, V., Desrochers, N., Macaluso, B., Mongeon, P., Paul-Hus, A., & Sugimoto, C. R. (2016). Contributorship and division of labor in knowledge production. Social Studies of Science, 46(3), 417–435. [https://doi.org/10.1177/0306312716650046](https://doi.org/10.1177/0306312716650046) Largent, E. A., & Snodgrass, R. T. (2016). Blind peer review by academic journals. In C. T. Robertson and A. S. Kesselheim (Eds.) *Blinding as a solution to bias: Strengthening biomedical science, forensic science, and law*, (pp. 75-95). Academic Press. [https://doi.org/10.1016/B978-0-12-802460-7.00005-X](https://doi.org/10.1016/B978-0-12-802460-7.00005-X) Lazic, S. E. (2019). *Genuine replication and pseudoreplication: What’s the difference?* BMJ Open Science. [https://blogs.bmj.com/openscience/2019/09/16/genuine-replication-and-pseudoreplication-whats-the-difference/](https://blogs.bmj.com/openscience/2019/09/16/genuine-replication-and-pseudoreplication-whats-the-difference/) Leavens, D. A., Bard, K. A., & Hopkins, W. D. (2010). BIZARRE chimpanzees do not represent “the chimpanzee”. *Behavioral and Brain Sciences, 33(*2-3), 100-101. https://doi.org/10.1017/S0140525X10000166 LeBel, E. P., Vanpaemel, W., Cheung, I., & Campbell, L. (2017). A brief guide to evaluate replications. *Meta-Psychology, 3*. https://doi.org/10.15626/MP.2018.843 LeBel, E. P., McCarthy, R. J., Earp, B. D., Elson, M., & Vanpaemel, W. (2018). A unified framework to quantify the credibility of scientific findings. A*dvances in Methods and Practices in Psychological Science, 1*(3), 389-402. [https://doi.org/10.1177/2515245918787489](https://doi.org/10.1177/2515245918787489) Ledgerwood, A., Hudson, S. T. J., Lewis, N. A., Jr., Maddox, K. B., Pickett, C., Remedios, J. D., … Wilkins, C. L. (2021, January 11). The Pandemic as a Portal: Reimagining Psychological Science as Truly Open and Inclusive. [https://doi.org/10.31234/osf.io/gdzue](https://doi.org/10.31234/osf.io/gdzue) Lee, R.M. (1993). *Doing research on sensitive topics.* London: Sage. Levitt, H. M., Motulsky, S. L., Wertz, F. J., Morrow, S. L., & Ponterotto, J. G. (2017). Recommendations for designing and reviewing qualitative research in psychology: Promoting methodological integrity. *Qualitative psychology, 4*(1), 2\\. https://doi.org/10.1037/qup0000082 Lewandowsky, S., & Bishop, D. (2016). Research integrity: Don't let transparency damage science. *Nature News, 529*(7587), 459\\. [https://doi.org/10.1038/529459a](https://doi.org/10.1038/529459a) LibGuides. (n.d.). *Measuring your research impact: i10-Index*. https://guides.library.cornell.edu/impact/author-impact-10. Lieberman, E. (2020). Research Cycles. In C. Elman, J. Gerring, & J. Mahoney (Eds.), *The Production of Knowledge: Enhancing Progress in Social Science* (Strategies for Social Inquiry, pp. 42-70). Cambridge: Cambridge University Press. [https://doi.org10.1017/9781108762519.003](https://doi.org10.1017/9781108762519.003) Lind, F., Gruber, M., & Boomgaarden, H. G. (2017). Content analysis by the crowd: Assessing the usability of crowdsourcing for coding latent constructs. *Communication Methods and Measures, 11*(3), 191–209. [https://doi.org/10.1080/19312458.2017.1317338](https://doi.org/10.1080/19312458.2017.1317338) Lindsay, D. S. (2015). Replication in Psychological Science \\[Editorial\\]. *Psychological Science, 26*(12), 1827-1832. https://doi.org/10.1177/0956797615616374 Lindsay, D. S. (2020). Seven steps toward transparency and replicability in psychological science. *Canadian Psychology/Psychologie canadienne., 61*(4), 310–317. [https://doi.org/10.1037/cap0000222](https://doi.org/10.1037/cap0000222) Lintott, C. J., Schawinski, K., Slosar, A., Land, K., Bamford, S., Thomas, D., ... & Vandenberg, J. (2008). Galaxy Zoo: morphologies derived from visual inspection of galaxies from the Sloan Digital Sky Survey. *Monthly Notices of the Royal Astronomical Society, 389*(3), 1179-1189. https://doi.org/10.1111/j.1365-2966.2008.13689.x Longino, H. E. (1990). *Science as Social Knowledge: Values and Objectivity in Scientific Inquiry.* Princeton University Press. Longino, H. E. (1992). Taking gender seriously in philosophy of science. *PSA, 2,* 333–340. Lu, J., Qiu, Y., & Deng, A. (2018). A note on Type S/M errors in hypothesis testing. *British Journal of Mathematical and Statistical Psychology, 72*(1), 1-17. [https://doi.org/10.1111/bmsp.12132](https://doi.org/10.1111/bmsp.12132) Lüdtke, O., Ulitzsch, E., & Robitzsch, A. (2020). *A Comparison of Penalized Maximum Likelihood Estimation and Markov Chain Monte Carlo Techniques for Estimating Confirmatory Factor Analysis Models with Small Sample Sizes* \\[Preprint\\]. PsyArXiv. [https://doi.org/10.31234/osf.io/u3qag](https://doi.org/10.31234/osf.io/u3qag) Lutz, M. (2001). *Programming python.* O'Reilly Media, Inc. Lyon, L. (2016) Transparency: The Emerging Third Dimension of Open Science and Open Data. *LIBER Quarterly, 25*(4), 153-171. [http://doi.org/10.18352/lq.10113](http://doi.org/10.18352/lq.10113) Makowski, D., Ben-Shachar, M. S., Chen, S. H. A., & Lüdecke, D. (2019). Indices of Effect Existence and Significance in the Bayesian Framework. Retrieved from [https://doi.org/10.3389/fpsyg.2019.02767](https://doi.org/10.3389/fpsyg.2019.02767) Martinez-Acosta, V. G., & Favero, C. B. (2018). A discussion of diversity and inclusivity at the institutional level: The need for a strategic plan. J*ournal of Undergraduate Neuroscience Education, 16*(3), A252. Marwick, B., Boettiger, C., & Mullen, L. (2018). Packaging data analytical work reproducibly using R (and friends). *The American Statistician, 72*(1), 80-88. [https://doi.org/10.1080/00031305.2017.1375986](https://doi.org/10.1080/00031305.2017.1375986) McElreath, R. (2020). *Statistical rethinking: A Bayesian course with examples in R and Stan* (2nd ed.). Taylor and Francis, CRC Press. McNutt, M. K., Bradford, M., Drazen, J. M., Hanson, B., Howard, B., Jamieson, K. H., Kiermer, V., Marcus, E., Pope, B. K., Schekman, R., Swaminathan, S., Stang, P. J., and Verma, I. M. (2018). Transparency in authors’ contributions and responsibilities to promote integrity in scientific publication. *Proceedings of the National Academy of Sciences of the United States of America, 115(*11), 2557-2560. [https://doi.org/10.1073/pnas.1715374115](https://doi.org/10.1073/pnas.1715374115) Medin, D. L. (2012). Rigor without rigor mortis: The APS Board discusses research integrity. *APS Observer, 25* (5-9), 27-28. https://www.psychologicalscience.org/observer/scientific-rigor Mellers, B., Hertwig, R., & Kahneman, D. (2001). Do frequency representations eliminate conjunction effects? An exercise in adversarial collaboration. P*sychological Science, 12*(4), 269-275. [https://doi.org/10.1111/1467-9280.00350](https://doi.org/10.1111/1467-9280.00350) Mertens, G., & Krypotos, A. M. (2019). Preregistration of analyses of preexisting data. *Psychologica Belgica, 59*(1), 338\\. Merton, R.K. (1938). Science and the social order. *Philosophy of Science, 5*(3), 321–337 https://doi.org/10.1086/286513 Merton, R. K. (1942). A note on science and democracy. *Journal of Legal and Political Sociology, 1*, 115–126. [https://doi.org/10.1515/9783110375008-013](https://doi.org/10.1515/9783110375008-013) Merton, R.K. (1968). The Matthew Effect in Science. *Science, 159* (3810), 56–63. https:/doi.org/10.1126/science.159.3810.56 Meslin EM. Achieving global justice in health through global research ethics: supplementing Macklin's ‘top-down' approach with one from the ‘ground up’ In: Green RM, Donovan A, Jauss SA, editors. Global Bioethics: Issues of Conscience for the Twenty-First Century. New York: University Press; 2009\\. pp. 163–177. Messick, S. (1995). Standards of validity and the validity of standards in performance assessment. *Educational measurement: Issues and practice, 14*(4), 5-8. [https://doi.org/10.1111/j.1745-3992.1995.tb00881.x](https://doi.org/10.1111/j.1745-3992.1995.tb00881.x) Michener W.K. (2015). Ten simple rules for creating a good data management plan. *PLoS Computational Biology, 11*(10), e1004525. https:/doi.org/10.1371/journal.pcbi.1004525 Moher, D., Bouter, L., Kleinert, S., Glasziou, P., Sham, M. H., Barbour, V., ... & Dirnagl, U. (2020). The Hong Kong Principles for assessing researchers: Fostering research integrity. *PLoS Biology, 18*(7), e3000737. [https://doi.org/10.1371/journal.pbio.3000737](https://doi.org/10.1371/journal.pbio.3000737) Moher, D., Liberati, A., Tetzlaff, J., & Altman, D. (2009). Preferred Reporting Items for Systematic Reviews and Meta-Analyses: The PRISMA Statement. *PLoS Medicine, 6*(7), e1000097. https://doi.org/10.1371/journal.pmed.1000097 Moher, D., Naudet, F., Cristea, I. A., Miedema, F., Ioannidis, J. P. A., & Goodman, S. N. (2018). Assessing scientists for hiring, promotion, and tenure. *PLOS Biology, 16*(3), e2004089. https://doi.org/10.1371/journal.pbio.2004089 Moshontz, H., Campbell, L., Ebersole, C. R., IJzerman, H., Urry, H. L., Forscher, P. S., ... & Chartier, C. R. (2018). The Psychological Science Accelerator: Advancing psychology through a distributed collaborative network. *Advances in Methods and Practices in Psychological Science, 1*(4), 501-515. https://doi.org/10.1177/2515245918797607 Monroe, K. R. (2018). The rush to transparency: DA-RT and the potential dangers for qualitative research. *Perspectives on Politics, 16*(1), 141–148. [https://doi.org/10.1017/S153759271700336X](https://doi.org/10.1017/S153759271700336X) Moran, H., Karlin, L., Lauchlan, E., Rappaport, S. J., Bleasdale, B., Wild, L., & Dorr, J. (2020). Understanding Research Culture: What researchers think about the culture they work in. *Wellcome Open Research*, 5, 201\\. [https://doi.org/10.12688/wellcomeopenres.15832.1](https://doi.org/10.12688/wellcomeopenres.15832.1) Moretti, M. (2020, August 25). *Beyond Open-washing: Are Narratives the Future of Open Data Portals?* Medium. https://medium.com/nightingale/beyond-open-washing-are-stories-and-narratives-the-future-of-open-data-portals-93228d8882f3 Morgan, C. (1998). The DOI (Digital Object Identifier). *Serials, 11*(1), pp.47–51. [http://doi.org/10.1629/1147](http://doi.org/10.1629/1147) Munn, Z., Peters, M. D., Stern, C., Tufanaru, C., McArthur, A., & Aromataris, E. (2018). Systematic review or scoping review? Guidance for authors when choosing between a systematic or scoping review approach. *BMC medical research methodology, 18*(1), 1-7. https://doi.org/10.1186/s12874-018-0611-x Muthukrishna, M., Bell, A. V., Henrich, J., Curtin, C. M., Gedranovich, A., McInerney, J., & Thue, B. (2020). Beyond Western, Educated, Industrial, Rich, and Democratic (WEIRD) psychology: Measuring and mapping scales of cultural and psychological distance. *Psychological Science, 31*, 678-701. https://doi.org/10.1177/0956797620916782 National Academies of Sciences, Engineering, and Medicine; Policy and Global Affairs; Committee on Science, Engineering, Medicine, and Public Policy; Board on Research Data and Information; Division on Engineering and Physical Sciences; Committee on Applied and Theoretical Statistics; Board on Mathematical Sciences and Analytics; Division on Earth and Life Studies; Nuclear and Radiation Studies Board; Division of Behavioral and Social Sciences and Education; Committee on National Statistics; Board on Behavioral, Cognitive, and Sensory Sciences; Committee on Reproducibility and Replicability in Science. (2019). Reproducibility and Replicability in Science. In *Understanding Reproducibility and Replicability*. Washington (DC): National Academies Press (US), Available from: [https://www.ncbi.nlm.nih.gov/books/NBK547546/](https://www.ncbi.nlm.nih.gov/books/NBK547546/) Nature (n.d.). *Recommended Data Repositories.* Scientific Data. [https://www.nature.com/sdata/policies/repositories](https://www.nature.com/sdata/policies/repositories) Naudet, F., Ioannidis, J., Miedema, F., Cristea, I. A., Goodman, S. N., & Moher, D. (2018). *Six principles for assessing scientists for hiring, promotion, and tenure.* Impact of Social Sciences Blog. Navarro, D. (2020). *Paths in strange spaces: A comment on preregistration.* Nelson, L.D., Simmons, J.P. & Simonsohn, U. (2012) Let's Publish Fewer Papers, *Psychological Inquiry, 23* (3), 291-293, https://doi.org/10.1080/1047840X.2012.705245 Neuroskeptic. (2012). The nine circles of scientific hell. (2012). *Perspectives on Psychological Science, 7*(6), 643–644. [https://doi.org/10.1177/1745691612459519](https://doi.org/10.1177/1745691612459519) Nichols, T. E., Das, S., Eickhoff, S. B., Evans, A. C., Glatard, T., Hanke, M., ... & Yeo, B. T. (2017). Best practices in data analysis and sharing in neuroimaging using MRI. N*ature neuroscience, 20*(3), 299-303. https://doi.org/10.1038/nn.4500 Nickerson, R. S. (1998). Confirmation bias: A ubiquitous phenomenon in many guises. *Review of general psychology, 2*(2), 175-220. [https://doi.org/10.1037/1089-2680.2.2.175](https://doi.org/10.1037/1089-2680.2.2.175) Nieuwenhuis, S., Forstmann, B. U., & Wagenmakers, E. J. (2011). Erroneous analyses of interactions in neuroscience: a problem of significance. *Nature Neuroscience*, *14*(9), 1105\\-1107. [https://doi.org/10.1038/nn.2886](https://doi.org/10.1038/nn.2886) Nittrouer, C., Hebl, M., Ashburn-Nardo, L., Trump-Steele, R., Lane, D., Valian, V. (2018). Gender disparities in colloquium speakers. *Proceedings of the National Academy of Sciences Jan, 115* (1) 104-108; DOI: 10.1073/pnas.1708414115 NIHR (2021) https://www.learningforinvolvement.org.uk/?opportunity=nihr-guidance-on-co-producing-a-research-project Nosek, B. A., & Bar-Anan, Y. (2012). Scientific utopia: I. Opening scientific communication. *Psychological Inquiry, 23*(3), 217-243. [https://doi.org/10.1080/1047840X.2012.692215](https://doi.org/10.1080/1047840X.2012.692215) Nosek, B. A., Ebersole, C. R., DeHaven, A. C., & Mellor, D. T. (2018). The preregistration revolution. *Proceedings of the National Academy of Sciences, 115*(11), 2600-2606. https://doi.org/10.1073/pnas.1708274114 Nosek, B.A. & Errington, T.M. (2020) What is replication? *PlosBiology, 18*(3), e3000691. [https://doi.org/10.1371/journal.pbio.3000691](https://doi.org/10.1371/journal.pbio.3000691) Nosek, B. A., & Lakens, D. (2014). Registered reports, *Social Psychology, 45,* 137-141. [https://doi.org/10.1027/1864-9335/a000192](https://doi.org/10.1027/1864-9335/a000192). Nosek, B. A., Spies, J. R., & Motyl, M. (2012). Scientific utopia: II. Restructuring incentives and practices to promote truth over publishability. *Perspectives on Psychological Science, 7*(6), 615-631.[https://doi.org/10.1177%2F1745691612459058](https://doi.org/10.1177%2F1745691612459058) Noy, N. F., & McGuinness, D. L. (2001). *Ontology development 101: A guide to creating your first ontology* .[https://corais.org/sites/default/files/ontology\\_development\\_101\\_aguide\\_to\\_creating\\_your\\_first\\_ontology.pdf](https://corais.org/sites/default/files/ontology_development_101_aguide_to_creating_your_first_ontology.pdf) Nuijten, M. B., Hartgerink, C. H., van Assen, M. A., Epskamp, S., and Wicherts, J. M. (2016). The prevalence of statistical reporting errors in psychology (1985–2013). *Behavior research methods, 48*(4), 1205–1226. Nüst, D. C., Boettiger, C., & Marwick, B. (2018) How to Read a Research Compendium. arXiv preprint arXiv:1806.09525 O’Dea, R. E., Parker, T. H., Chee, Y. E., Culina, A., Drobniak, S. M., Duncan, D. H., Fidler, F., Gould, E., Ihle, M., Kelly, C. D., Lagisz, M., Roche, D. G., Sánchez-Tójar, A., Wilkinson, D. P., Wintle, B. C., & Nakagawa, S. (2021). Towards open, reliable, and transparent ecology and evolutionary biology. *BMC Biology, 19*(1). https://doi.org/10.1186/s12915-021-01006-3 O’Grady (2020) *Psychology’s replication crisis inspires ecologists to push for more reliable research.* Science. [https://doi.org/10.1126/science.abg0894](https://doi.org/10.1126/science.abg0894) Obels, P., Lakens, D., Coles, N. A., Gottfried, J., & Green, S. A. (2020). Analysis of open data and computational reproducibility in registered reports in psychology. *Advances in Methods and Practices in Psychological Science, 3*(2), 229-237. Oberauer K., & Lewandowsky S. (2019). Addressing the theory crisis in psychology. *Psychonomic bulletin & review. 26*(5),1596–1618. [https://doi.org/10.3758/s13423-019-01645-2](https://doi.org/10.3758/s13423-019-01645-2) Onie, S.(2020). Redesign open science for Asia, Africa and Latin America. *Nature, 587*, 5-37. [https://doi.org/10.1038/d41586-020-03052-3](https://doi.org/10.1038/d41586-020-03052-3) Open Science Collaboration. (2015). Estimating the reproducibility of psychological science. *Science, 349*(6251), aac4716. https://doi.org/10.1126/science.aac4716 Open Aire. (2020). *High accuracy Data anonymisation*. Amnesia. [https://amnesia.openaire.eu/](https://amnesia.openaire.eu/) Open Source Initiative (n.d.). *The Open Source Definition.* Open Source Initiative. [https://opensource.org/osd](https://opensource.org/osd) Orben, A. (2019). A journal club to fix science. *Nature, 573*(7775), 465-466. https://doi.org/10.1038/d41586-019-02842-8 Oxford Dictionaries. (2017). *Bias—definition of bias in English.* [https://en.oxforddictionaries.com/definition/bias](https://en.oxforddictionaries.com/definition/bias) Oxford Reference. (2017). *Reflexivity*. http://www.oxfordreference.com/view/10.1093/ acref/9780199599868.001.0001/acref-9780199599868-e-1530 Padilla, A. M. (1994). Research news and comment: Ethnic minority scholars; research, and mentoring: Current and future issues. *Educational Researcher, 23*(4), 24-27. [https://doi.org/10.3102/0013189X023004024](https://doi.org/10.3102/0013189X023004024) Page, M. J., Moher, D., Bossuyt, P. M., Boutron, I., Hoffmann, T. C., Mulrow, C. D., ... & McKenzie, J. E. (2021). PRISMA 2020 explanation and elaboration: updated guidance and exemplars for reporting systematic reviews. *British Medical Journal, 372*. https://doi.org/ 10.1136/bmj.n160 Patience, G. S., Galli, F., Patience, P. A., & Boffito, D. C. (2019). Intellectual contributions meriting authorship: Survey results from the top cited authors across all science categories. *PLoS One, 14*(1), e0198117. https://doi.org/10.1371/journal.pone.0198117 PCI (n.d.). *PCI IN A FEW LINES.* Peer community in. [https://peercommunityin.org/](https://peercommunityin.org/) Peer, E., Brandimarte, L., Samat, S., & Acquisti, A. (2017). Beyond the Turk: Alternative platforms for crowdsourcing behavioral research. *Journal of Experimental Social Psychology, 70*, 153–163. https://doi.org/10.1016/j.jesp.2017.01.006 Peng, R. D. (2011). Reproducible Research in Computational Science. *Science, 334*(6060), 1226–1227. [https://doi.org/10.1126/science.1213847](https://doi.org/10.1126/science.1213847) Percie du Sert, N., Hurst, V., Ahluwalia, A., Alam, S., Avey, M. T., Baker, M., ... & Würbel, H. (2020). The ARRIVE guidelines 2.0: Updated guidelines for reporting animal research. *Journal of Cerebral Blood Flow & Metabolism, 40*(9), 1769-1777. [https://doi.org/10.1371/journal.pbio.3000410](https://doi.org/10.1371/journal.pbio.3000410) Pernet, C. R. (2015). Null hypothesis significance testing: a short tutorial. *F1000Research, 4,* 621\\. https:/doi.org/10.12688/f1000research.6963.3 Pernet, C. R., Appelhoff, S., Gorgolewski, K. J., Flandin, G., Phillips, C., Delorme, A., & Oostenveld, R. (2019). EEG-BIDS, an extension to the brain imaging data structure for electroencephalography. *Scientific Data, 6*(1), 103\\. https://doi.org/10.1038/s41597-019-0104-8 Pernet, C., Garrido, M. I., Gramfort, A., Maurits, N., Michel, C. M., Pang, E., ... & Puce, A. (2020). Issues and recommendations from the OHBM COBIDAS MEEG committee for reproducible EEG and MEG research. *Nature Neuroscience, 23*(12), 1473-1483. https://doi.org/10.1038/s41593-020-00709-0 Peterson, D., & Panofsky, A. (2020, August 4). Metascience as a scientific social movement. [https://doi.org/10.31235/osf.io/4dsqa](https://doi.org/10.31235/osf.io/4dsqa) Petre, M., & Wilson, G. (2014). Code review for and by scientists. arXiv preprint arXiv:1407.5648. Pollet, I. L., & Bond, A. L. (2021). Evaluation and recommendations for greater accessibility of colour figures in ornithology. *Ibis*, *163*, 292–295. [https://doi.org/10.1111/ibi.12887](https://doi.org/10.1111/ibi.12887) Popper, K. (1959). *The logic of scientific discovery.* London, United Kingdom: Routledge. Posselt, J. R. (2020). *Equity in Science: Representation, Culture, and the Dynamics of Change in Graduate Education.* Stanford University Press. [https://books.google.de/books?id=2CjwDwAAQBAJ](https://books.google.de/books?id=2CjwDwAAQBAJ) Pownall, M., Talbot, C. V., Henschel, A., Lautarescu, A., Lloyd, K., Hartmann, H., … Siegel, J. A. (2020, October 13). Navigating Open Science as Early Career Feminist Researchers. [https://doi.org/10.31234/osf.io/f9m47](https://doi.org/10.31234/osf.io/f9m47) Press, W. (2007). *Numerical recipes: the art of scientific computing, 3rd edition*. 978-0-521-88068-8 Psychological Science Accelerator. (n.d.). https://psysciacc.org/. R Core Team (2020). *R: A language and environment for statistical computing*. R Foundation for Statistical Computing, Vienna, Austria. URL [https://www.R-project.org/](https://www.R-project.org/) Rabagliati, H., Moors, P., & Heyman, T. (2019). Can item effects explain away the evidence for unconscious sound symbolism? An adversarial commentary on Heyman, Maerten, Vankrunkelsven, Voorspoels, and Moors (2019). *Psychological Science, 31*(9), 1200-1204. [https://doi.org/10.1177/0956797620949461](https://doi.org/10.1177%2F0956797620949461) Rakow, T., Thompson, V., Ball, L., & Markovits, H. (2014). Rationale and guidelines for empirical adversarial collaboration: A Thinking & Reasoning initiative. *Thinking & Reasoning, 21*(2), 167–175. doi:10.1080/13546783.2015.975405 RepliCATS project. (2020). *Collaborative Assessment for Trustworthy Science.* The University of Melbourne. [https://replicats.research.unimelb.edu.au/](https://replicats.research.unimelb.edu.au/) ReproducibiliTea. (n.d.). *Welcome to ReproducibiliTea.* ReproducibiliTea. [https://reproducibilitea.org/](https://reproducibilitea.org/) Research Data Alliance (2020). *Data management plan (DMP) common standard.* Available from: [https://github.com/RDA-DMP-Common/RDA-DMP-Common-Standard](https://github.com/RDA-DMP-Common/RDA-DMP-Common-Standard) RIOT Science Club. (2021, May 28). *RIOT Science Club.* http://riotscience.co.uk/ Rolls, L., & Relf, M. (2006). Bracketing interviews: addressing methodological challenges in qualitative interviewing in bereavement and palliative care, *Mortality, 11* (3), 286-305, https://doi.org/10.1080/13576270600774893 Rose, D. (2000). Universal design for learning. J*ournal of Special Education Technology, 15*(3), 45-49. https://doi.org/10.1177/016264340001500307 Rose, D. H., & Meyer, A. (2002). *Teaching every student in the digital age: Universal design for learning.* Association for Supervision and Curriculum Development, 1703 N. Beauregard St., Alexandria, VA 22311-1714 (Product no. 101042: $22.95 ASCD members; $26.95 nonmembers). Ross-Hellauer, T. (2017). What is open peer review? A systematic review \\[version 2; peer review: 4 approved\\]. *F1000Research, 6,* 588 ([https://doi.org/10.12688/f1000research.11369.2](https://doi.org/10.12688/f1000research.11369.2) Rossner, M., Van Epps, H., & Hill, E. (2008). *Show me the data.* https://doi.org/10.1083/jcb.200711140 Rothstein, H. R., Sutton, A. J., & Borenstein, M. (2005). Publication bias in meta-analysis. In Rothstein, A. J. Sutton, & M. Borenstein (Eds.).P*ublication bias in meta-analysis: Prevention, assessment and adjustments* (pp. 1-7). John Wiley & Sons, Ltd. [https://doi.org/10.1002/0470870168.ch1](https://doi.org/10.1002/0470870168.ch1) Rowhani-Farid, A., Aldcroft, A., & Barnett, A. G. (2020). Did awarding badges increase data sharing in BMJ Open? A randomized controlled trial. *Royal Society open science, 7*(3), 191818\\. [https://doi.org/10.1098/rsos.191818](https://doi.org/10.1098/rsos.191818) Rubin, M. (2021). Explaining the association between subjective social status and mental health among university students using an impact ratings approach. S*N Social Sciences, 1*(1), 1-21. https://doi.org/10.1007/s43545-020-00031-3 Rubin, M., Evans, O., & McGuffog, R. (2019). Social class differences in social integration at university: Implications for academic outcomes and mental health. In J. Jetten, & K. Peters (Eds.), *The social psychology of inequality* (pp. 87-102). Springer. [https://doi.org/10.1007/978-3-030-28856-3\\_6](https://doi.org/10.1007/978-3-030-28856-3_6) S. (2021, June 5). OSF | StudySwap: A platform for interlab replication, collaboration, and research resource exchange. OSF. [https://osf.io/meetings/StudySwap/](https://osf.io/meetings/StudySwap/) Sagarin, B. J., Ambler, J. K., & Lee, E. M. (2014). An ethical approach to peeking at data. *Perspectives on Psychological Science, 9*(3), 293-304. https://doi.org/10.1177/1745691614528214 San Francisco Declaration on Research Assessment (DORA). [https://sfdora.org/](https://sfdora.org/) Retrieved February 18th 2021\\. Sato, T. (1996). Type I and Type II error in multiple comparisons. *The Journal of Psychology, 130*(3), 293-302. https://doi.org/10.1080/00223980.1996.9915010 Schafersman, S.D. (1997). *An Introduction to Science.* Available from: [https://www.geo.sunysb.edu/esp/files/scientific-method.html](https://www.geo.sunysb.edu/esp/files/scientific-method.html) Schmidt, F. L., & Hunter, J. E. (2014). *Methods of meta-analysis: Correcting error and bias in research findings* (3rd ed.). Thousand Oaks, CA: Sage. Schmidt, R.H. (1987) A worksheet for authorship of scientific articles. *The Bulletin of the Ecological Society of America, 68*, 8–10. Retrieved March 4, 2021, from http://www.jstor.org/stable/20166549 Schneider, J., Merk, S., & Rosman, T. (2019). (Re)Building Trust? Investigating the effects of open science badges on perceived trustworthiness in journal articles. [https://doi.org/10.17605/OSF.IO/VGBRS](https://doi.org/10.17605/OSF.IO/VGBRS) Schönbrodt, F. D., Wagenmakers, E.-J., Zehetleitner, M., & Perugini, M. (2017). Sequential hypothesis testing with Bayes factors: Efficiently testing mean differences. *Psychological Methods, 22*(2), 322–339. [https://doi.org/10.1037/met0000061](https://doi.org/10.1037/met0000061) Schönbrodt, F. (2019). Training students for the Open Science future. *Nature human behaviour, 3*(10), 1031-1031.[https://doi.org/10.1038/s41562-019-0726-z](https://doi.org/10.1038/s41562-019-0726-z) Schuirmann, D. J. (1987). A comparison of the two one-sided tests procedure and the power approach for assessing the equivalence of average bioavailability. *Journal of Pharmacokinetics and Biopharmaceutics, 15*, 657–680. https://doi.org/10.1007/BF01068419 Schulz, K. F., & Grimes, D. A. (2005). Multiplicity in randomised trials I: endpoints and treatments. *The Lancet, 365*(9470), 1591-1595. [https://doi.org/10.1016/S0140-6736(05)66461-6](https://doi.org/10.1016/S0140-6736\\(05\\)66461-6) Schulz, K. F., Altman, D. G., & Moher, D. (2010). CONSORT 2010 statement: updated guidelines for reporting parallel group randomised trials. *Trials, 11*(1), 1-8. [https://doi.org/10.1186/1745-6215-11-32](https://doi.org/10.1186/1745-6215-11-32) Schwarz, N., & Strack, F. (2014). Does Merely Going Through the Same Moves Make for a “Direct” Replication?: Concepts, Contexts, and Operationalizations. S*ocial Psychology, 45*(4), 305-306. Science, C. (n.d.). Open science badges. Retrieved February 08, 2021, from [https://www.cos.io/initiatives/badges](https://www.cos.io/initiatives/badges) Scopatz, Anthony M., and Kathryn D. Huff. 2015\\. Effective Computation in Physics: Field Guide to Research with Python. 1st ed. Sebastopol, CA: O’Reilly Media. [http://shop.oreilly.com/product/0636920033424.do](http://shop.oreilly.com/product/0636920033424.do) ISBN: 978-1-4919-0153-3 Sert, N. P. du, Hurst, V., Ahluwalia, A., Alam, S., Avey, M. T., Baker, M., Browne, W. J., Clark, A., Cuthill, I. C., Dirnagl, U., Emerson, M., Garner, P., Holgate, S. T., Howells, D. W., Karp, N. A., Lazic, S. E., Lidster, K., MacCallum, C. J., Macleod, M., … Würbel, H. (2020). The ARRIVE guidelines 2.0: Updated guidelines for reporting animal research. *PLOS Biology, 18*(7), e3000410. https://doi.org/10.1371/journal.pbio.3000410 Shadish, W. R., Cook, T. D., & Campbell, D. T. (2002). *Experimental and quasi-experimental designs for generalized causal inference.* Houghton, Mifflin and Company. Sharma, M., Sarin, A., Gupta, P., Sachdeva, S., & Desai, A. (2014). Journal impact factor: its use, significance and limitations. *World journal of nuclear medicine, 13*(2), 146\\. [https://doi.org/10.4103/1450-1147.139151](https://doi.org/10.4103/1450-1147.139151) Shepard, B. (2015). Community projects as social activism. SAGE Siddaway, A. P., Wood, A. M., & Hedges, L. V. (2019). How to do a systematic review: a best practice guide for conducting and reporting narrative reviews, meta-analyses, and meta-syntheses. *Annual review of psychology, 70*, 747-770. https://doi.org/[10.1146/annurev-psych-010418-102803](https://doi-org.surrey.idm.oclc.org/10.1146/annurev-psych-010418-102803) Sijtsma, K. (2016). Playing with data—Or how to discourage questionable research practices and stimulate researchers to do things right. *Psychometrika, 81*(1), 1–15. https://doi.org/10.1007/s11336-015-9446-0 Silberzahn, R., Simonsohn, U., & Ulhmann, E. L. (2014). Matched-names analysis reveals no evidence of name-meaning effects: A collaborative commentary on Silberzahn and Uhlmann (2013). *Psychological Science, 25*(7), 1504-1505. [https://doi.org/10.1177/0956797614533802](https://doi.org/10.1177%2F0956797614533802) Silberzahn, R., Uhlmann, E. L., Martin, D. P., Anselmi, P., Aust, F., Awtrey, E., Bahník, Š., Bai, F., Bannard, C., Bonnier, E., Carlsson, R., Cheung, F., Christensen, G., Clay, R., Craig, M. A., Dalla Rosa, A., Dam, L., Evans, M. H., Flores Cervantes, I., … Nosek, B. A. (2018). Many Analysts, One Data Set: Making Transparent How Variations in Analytic Choices Affect Results. *Advances in Methods and Practices in Psychological Science,* 337–356. [https://doi.org/10.1177/2515245917747646](https://doi.org/10.1177/2515245917747646) Simons, D. J., Shoda, Y., & Lindsay, D. S. (2017). Constraints on generality (COG): A proposed addition to all empirical papers. *Perspectives on Psychological Science, 12*(6), 1123-1128. https://doi.org/10.1177/1745691617708630 Simmons, J. P., Nelson, L. D., & Simonsohn, U. (2011). False-positive psychology: Undisclosed flexibility in data collection and analysis allows presenting anything as significant. *Psychological Science, 22*(11), 1359-1366. [https://doi.org/10.1177/0956797611417632](https://doi.org/10.1177/0956797611417632) Simmons, J., Nelson, L., & Simonsohn, U. (2021). Pre‐registration: Why and how. *Journal of Consumer Psychology, 31*(1), 151–162. https://doi.org/10.1002/jcpy.1208 Simonsohn, U., Nelson, L. D., & Simmons, J. P. (2014). P-curve: a key to the file-drawer. *Journal of experimental psychology: General, 143*(2), 534\\. Simonsohn, U., Simmons, J. P., & Nelson, L. D. (2015). Specification curve: Descriptive and inferential statistics on all reasonable specifications. Retrieved from http://sticerd.lse.ac.uk/seminarpapers/psyc16022016.pdf. Simonsohn, U., Nelson, L. D., & Simmons, J. P. (2014). p-curve and effect size: Correcting for publication bias using only significant results. *Perspectives on Psychological Science, 9*(6), 666–681. [https://doi.org/10.1177/1745691614553988](https://doi.org/10.1177/1745691614553988) Simonsohn, U., Nelson, L. D., & Simmons, J. P. (2019). P-curve won’t do your laundry, but it will distinguish replicable from non-replicable findings in observational research: Comment on Bruns & Ioannidis (2016). *PLoS ONE, 14*(3), e0213454. https://doi.org/10.1371/journal.pone.0213454 Simonsohn, U., Simmons, J. P., & Nelson, L. D. (2020). Specification curve analysis. *Nature Human Behaviour, 4*(11), 1208-1214. https://doi.org/10.1038/s41562-020-0912-z Slow Science Academy. (2010). *The Slow Science Manifesto*. Slow Science. [http://slow-science.org/](http://slow-science.org/). SIPS. (2021). *The Society for the Improvement of Psychological Science.* SIPS. https://improvingpsych.org/ Smaldino, P. E., & McElreath, R. (2016). The natural selection of bad science. *Royal Society Open Science, 3*(9), 160384\\. https://doi.org/10.1098/rsos.160384 Smith, G. T. (2005). On Construct Validity: Issues of Method and Measurement. *Psychological Assessment, 17*(4), 396–408. [https://doi.org/10.1037/1040-3590.17.4.396](https://psycnet.apa.org/doi/10.1037/1040-3590.17.4.396) Smith, A. C., Merz, L., Borden, J. B., Gulick, C., Kshirsagar, A. R., & Bruna, E. M. (2020, September 2). Assessing the effect of article processing charges on the geographic diversity of authors using Elsevier’s ‘Mirror Journal’ system. [https://doi.org/10.31222/osf.io/s7cx4](https://doi.org/10.31222/osf.io/s7cx4) Smith, A.J., Clutton, R.E., Lilley, E., Hansen K.E.A., Brattelid, T. (2018): PREPARE: Guidelines for planning animal research and testing. *Laboratory Animals, 52*(2), 135-141. https:/doi.org/10.1177/0023677217724823 Sorsa, M.A., Kiikkala, I., & Åstedt-Kurki, P. (2015). Bracketing as a skill in conducting unstructured qualitative interviews. *Nursing Research, 22*(4), 8-12. https:/doi.org/10.7748/nr.22.4.8.e1317. Society for Open, Reliable and Transparent Ecology and Evolutionary biology (n.d.). SORTEE. Retrieved 5 June 2021, from [https://www.sortee.org/](https://www.sortee.org/) Spence, J. R., & Stanley, D. J. (2018). Concise, simple, and not wrong: In search of a short-hand interpretation of statistical significance. *Frontiers in psychology, 9,* 2185\\. https:/doi.org/10.3389/fpsyg.2018.02185 Spencer, E. A., & Heneghan, C. (2018). Confirmation bias. Catalogue Of Bias [https://catalogofbias.org/biases/confirmation-bias/](https://catalogofbias.org/biases/confirmation-bias/) Stanford Libraries. (n.d.). *Data management plans.* [https://library.stanford.edu/research/data-management-services/data-management-plans\\#:\\~:text=A%20data%20management%20plan%20](https://library.stanford.edu/research/data-management-services/data-management-plans#:~:text=A%20data%20management%20plan%20)(DMP,share%20and%20preserve%20your%20data. Steup, M., & Neta, R. (2020, April 11). *Epistemology.* Stanford Encyclopedia of Philosophy. [https://plato.stanford.edu/entries/epistemology/](https://plato.stanford.edu/entries/epistemology/). Steegen, S., Tuerlinckx, F., , Gelman, A. & Vanpaemel, W. (2016). Increasing Transparency through a Multiverse Analysis. *Perspectives on Psychological Science, 11,* 702-712. [https://doi.org/10.1177/1745691616658637](https://doi.org/10.1177/1745691616658637) Stewart, N., Chandler, J., & Paolacci, G. (2017). Crowdsourcing samples in cognitive science. *Trends in cognitive sciences, 21*(10), 736-748. [https://doi.org/10.1016/j.tics.2017.06.007](https://doi.org/10.1016/j.tics.2017.06.007) Stodden, V. C. (2011). Trust your science? Open your data and code. Strathern, M. (1997). ‘Improving ratings’: audit in the British University system. *European review, 5*(3), 305-321. https://doi.org/10.1002/(SICI)1234-981X(199707)5:3\\<305::AID-EURO184\\>3.0.CO;2-4 Suber, P. (2004). The primacy of authors in achieving Open Access. *Nature.* June 10, 2004\\. (previous, unabridged version: [http://dash.harvard.edu/handle/1/4391161](http://dash.harvard.edu/handle/1/4391161)) Suber, P. (2015). *Open Access Overview.* Available from: [http://legacy.earlham.edu/\\~peters/fos/overview.htm](http://legacy.earlham.edu/~peters/fos/overview.htm) SwissRN. (n.d.). Swiss Reproducibility Network. Retrieved 5 June 2021, from https://www.swissrn.org/ [Syed, M. (2019). The Open Science Movement is for all of us. PsyArXiv.](https://psyarxiv.com/cteyb/) Syed, M., & Kathawalla, U. (2020, February 25). Cultural Psychology, Diversity, and Representation in Open Science. https://doi.org/10.31234/osf.io/t7hp2 Szollosi, A., & Donkin, C., (2019). Arrested theory development: The misguided distinction between exploratory and confirmatory research. PsyArXiv. Tenney, S., & Abdelgawad, I. (2019). Statistical significance. In *StatsPearls*. Treasure Island (FL), StatPearls Publishing. Tscharntke, T., Hochberg, M. E., Rand, T. A., Resh, V. H., & Krauss, J. (2007). Author sequence and credit for contributions in multiauthored publications. *PLoS Biology, 5*(1), e18. https://doi.org/10.1371/journal.pbio.0050018 Tennant, J., Bielczyk, N. Z., Cheplygina, V., Greshake Tzovaras, B., Hartgerink, C. H. J., Havemann, J., Masuzzo, P., & Steiner, T. (2019). Ten simple rules for researchers collaborating on Massively Open Online Papers (MOOPs) \\[Preprint\\]. MetaArXiv. [https://doi.org/10.31222/osf.io/et8ak](https://doi.org/10.31222/osf.io/et8ak) The jamovi project (2020). *jamovi* (Version 1.2) \\[Computer Software\\]. Retrieved from [https://www.jamovi.org](https://www.jamovi.org) The Nine Circles of Scientific Hell. (2012). *Perspectives on Psychological Science, 7*(6), 643–644. [https://doi.org/10.1177/1745691612459519](https://doi.org/10.1177/1745691612459519) The R Foundation (n.d.). *The R Project for Statistical Computing.* The R Foundation. [https://www.r-project.org](https://www.r-project.org/)/ Thombs, B. D., Levis, A. W., Razykov, I., Syamchandra, A., Leentjens, A. F., Levenson, J. L., & Lumley, M. A. (2015). Potentially coercive self-citation by peer reviewers: a cross-sectional study. *Journal of Psychosomatic Research, 78*(1), 1-6. https://doi.org/10.1016/j.jpsychores.2014.09.015 Tierney, W., Hardy III, J. H., Ebersole, C. R., Leavitt, K., Viganola, D., Clemente, E. G., ... & Hiring Decisions Forecasting Collaboration. (2020). Creative destruction in science. *Organizational Behavior and Human Decision Processes, 161,* 291-309. https://doi.org/10.1016/j.obhdp.2020.07.002 Tierney, W., Hardy III, J., Ebersole, C. R., Viganola, D., Clemente, E. G., Gordon, M., ... & Culture & Work Morality Forecasting Collaboration. (2021). A creative destruction approach to replication: Implicit work and sex morality across cultures. *Journal of Experimental Social Psychology, 93*, 104060\\. [https://doi.org/10.1016/j.jesp.2020.104060](https://doi.org/10.1016/j.jesp.2020.104060) Tiokhin, L., Yan, M., & Horgan, T. J. H. (2021). Competition for priority harms the reliability of science, but reforms can help. *Nature Human Behaviour.* [https://doi.org/10.1038/s41562-020-01040-1](https://doi.org/10.1038/s41562-020-01040-1) Topor, M., Pickering, J. S., Barbosa Mendes, A., Bishop, D. V. M., Büttner, F. C., Elsherif, M. M., … Westwood, S. J. (2021, March 15). An integrative framework for planning and conducting Non-Intervention, Reproducible, and Open Systematic Reviews (NIRO-SR). [https://doi.org/10.31222/osf.io/8gu5z](https://doi.org/10.31222/osf.io/8gu5z) Tufte, E. R. (1983). *The visual display of quantitative information.* Graphics Press. Tukey, J.W. (1977). *Exploratory data analysis.* Reading, MA: Addison-Wesley. Tvina, A., Spellecy, R., & Palatnik, A. (2019). Bias in the peer review process: can we do better?. *Obstetrics & Gynecology, 133*(6), 1081-1083. https://doi.org/10.1097/AOG.0000000000003260 Uhlmann, E. L., Ebersole, C. R., Chartier, C. R., Errington, T. M., Kidwell, M. C., Lai, C. K., McCarthy, R. J., Riegelman, A., Silberzahn, R., & Nosek, B. A. (2019). Scientific utopia III: Crowdsourcing science. *Perspectives on Psychological Science, 14*(5), 711–733. [https://doi.org/10.1177/1745691619850561](https://doi.org/10.1177/1745691619850561) van de Schoot, R., Depaoli, S., King, R., Kramer, B., Märtens, K., Tadesse, M. G., Vannucci, M., Gelman, A., Veen, D., Willemsen, J., & Yau, C. (2021). Bayesian statistics and modelling. *Nature Reviews Methods Primers, 1*(1), 1–26. [https://doi.org/10.1038/s43586-020-00001-2](https://doi.org/10.1038/s43586-020-00001-2) Vazire, S. (2018). Implications of the Credibility Revolution for Productivity, Creativity, and Progress. *Perspectives on Psychological Science, 13*(4), 411–417. [https://doi.org/10.1177/1745691617751884](https://doi.org/10.1177/1745691617751884) Vazire, S., Schiavone, S. R., & Bottesini, J. G. (2020). Credibility Beyond Replicability: Improving the Four Validities in Psychological Science. [https://doi.org/10.31234/osf.io/bu4d3](https://doi.org/10.31234/osf.io/bu4d3) Villum, C. (2016, July 2). *“Open-washing” – The difference between opening your data and simply making them available*. Open Knowledge Foundation Blog. https://blog.okfn.org/2014/03/10/open-washing-the-difference-between-opening-your-data-and-simply-making-them-available/ Von Elm, E., Altman, D. G., Egger, M., Pocock, S. J., Gøtzsche, P. C., & Vandenbroucke, J. P. (2007). The Strengthening the Reporting of Observational Studies in Epidemiology (STROBE) statement: guidelines for reporting observational studies. *Annals of internal medicine, 147*(8), 573-577. [https://doi.org/10.1136/bmj.39335.541782.AD](https://doi.org/10.1136/bmj.39335.541782.AD) Voracek, M., Kossmeier, M., & Tran, U. S. (2019). Which Data to Meta-Analyze, and How?. *Zeitschrift für Psychologie.* [https://doi.org/10.1027/2151-2604/a000357](https://doi.org/10.1027/2151-2604/a000357) Vuorre, M., & Curley, J. P. (2018). Curating research assets: A tutorial on the Git version control system. *Advances in methods and Practices in Psychological Science, 1*(2), 219-236. https://doi.org/10.1177/2515245918754826 Wacker, J. (1998). A definition of theory: research guidelines for different theory-building research methods in operations management. *Journal of Operations Management, 16*(4), 361–385. doi:10.1016/s0272-6963(98)00019-9 Wagenmakers, E. J., Wetzels, R., Borsboom, D., van der Maas, H. L., & Kievit, R. A. (2012). An agenda for purely confirmatory research. *Perspectives on Psychological Science, 7*(6), 632-638. [https://doi.org/10.1177/1745691612463078](https://doi.org/10.1177/1745691612463078) Wagenmakers, E.-J., Marsman, M., Jamil, T., Ly, A., Verhagen, J., Love, J., Selker, R., Gronau, Q. F., Šmíra, M., Epskamp, S., Matzke, D., Rouder, J. N., & Morey, R. D. (2018). Bayesian inference for psychology. Part I: Theoretical advantages and practical ramifications. *Psychonomic Bulletin & Review, 25*(1), 35–57. https://doi.org/10.3758/s13423-017-1343-3 Wagge, J. R., Baciu, C., Banas, K., Nadler, J. T., Schwarz, S., Weisberg, Y., ... & McCarthy, R. (2019). A demonstration of the collaborative replication and education project: Replication attempts of the red-romance effect. *Collabra: Psychology, 5*(1). https://doi.org/10.1525/collabra.177 Wason, P. C. (1960). On the failure to eliminate hypotheses in a conceptual task. *Quarterly Journal of Experimental Psychology, 12*(3), 129-140. https://doi.org/10.1080/17470216008416717 Webster, M.M., & Rutz, C. (2020). How STRANGE are your study animals? *Nature, 582,* 337–40. [https://doi.org/10.1038/d41586-020-01751-5](https://doi.org/10.1038/d41586-020-01751-5) Wendl, M. C. (2007). H-index: however ranked, citations need context. *Nature, 449(*7161), 403-403. https://doi.org/10.1038/449403b Whitaker, K., & Guest, O. (2020). \\#bropenscience is broken science. *The Psychologist, 33,* 34-37. Wicherts, J. M., Veldkamp, C. L., Augusteijn, H. E., Bakker, M., Van Aert, R., & Van Assen, M. A. (2016). Degrees of freedom in planning, running, analyzing, and reporting psychological studies: A checklist to avoid p-hacking. *Frontiers in psychology, 7,* 1832\\. [https://doi.org/10.3389/fpsyg.2016.01832](https://doi.org/10.3389/fpsyg.2016.01832) Wilkinson, M. D., Dumontier, M., Aalbersberg, I. J., Appleton, G., Axton, M., Baak, A., ... & Mons, B. (2016). The FAIR Guiding Principles for scientific data management and stewardship. *Scientific data, 3*(1), 1-9. https://doi.org/10.1038/sdata.2016.18 Wilson, B. & Fenner, M. (2012) Open Researcher & Contributor ID (ORCID): Solving the Name Ambiguity Problem. *Educause Review \\- E-Content 47*(3), 54-55. [https://er.educause.edu/articles/2012/5/open-researcher--contributor-id-orcid-solving-the-name-ambiguity-problem](https://er.educause.edu/articles/2012/5/open-researcher--contributor-id-orcid-solving-the-name-ambiguity-problem) Wilson, R. C., & Collins, A. G. (2019). Ten simple rules for the computational modeling of behavioral data. *Elife, 8,* e49547. https://doi.org/10.7554/eLife.49547 Wingen, T., Berkessel, J. B., & Englich, B. (2020). No Replication, No Trust? How Low Replicability Influences Trust in Psychology. *Social Psychological and Personality Science, 11*(4). https://doi.org/10.1177/1948550619877412 Woelfle, M., Olliaro, P., & Todd, M. H. (2011). Open science is a research accelerator. *Nature chemistry, 3*(10), 745-748. [https://doi.org/10.1038/nchem.1149](https://doi.org/10.1038/nchem.1149) World Wide Web Consortium (2021). *Web Accessibility Initiative*. [https://www.w3.org/WAI/](https://www.w3.org/WAI/) Wren, J. D., Valencia, A., & Kelso, J. (2019). Reviewer-coerced citation: case report, update on journal policy and suggestions for future prevention. *Bioinformatics, 35* (18), 3217-3218. [https://doi.org/10.1093/bioinformatics/btz071](https://doi.org/10.1093/bioinformatics/btz071) Wuchty, S., Jones, B. F., & Uzzi, B. (2007). The increasing dominance of teams in production of knowledge. *Science, 316*(5827), 1036-1039. https://doi.org/10.1126/science.1136099 Xia, J., Harmon, J. L., Connolly, K. G., Donnelly, R. M., Anderson, M. R., & Howard, H. A. (2015). Who publishes in “predatory” journals?. *Journal of the Association for Information Science and Technology, 66*(7), 1406-1417. [https://doi.org/10.1002/asi.23265](https://doi.org/10.1002/asi.23265) Yamada, Y. (2018). How to crack pre-registration: Toward transparent and open science. *Frontiers in Psychology, 9*, 1831\\. [https://doi.org/10.3389/fpsyg.2018.01831](https://doi.org/10.3389/fpsyg.2018.01831) Yarkoni, T. (2020). The generalizability crisis. *Behavioral and Brain Sciences,* 1-37. https://doi.org/10.1017/S0140525X20001685 Yeung, S. K., Feldman, G., Fillon, A., Protzko, J., Elsherif, M. M., Xiao, Q., & Pickering, J. (2020a). Experimental Studies Meta-Analysis Registered Report Templates. \\[Manuscript in preparation\\]. Zurn, P., Bassett, D. S., & Rust, N. C. (2020). The Citation Diversity Statement: A Practice of Transparency, A Way of Life. *Trends in Cognitive Sciences, 24*(9), 669-672. [https://doi.org/10.1016/j.tics.2020.06.009](https://doi.org/10.1016/j.tics.2020.06.009) Zwaan, R., Etz, A., Lucas, R., & Donnellan, M. (2018). Making replication mainstream. *Behavioral and Brain Sciences, 41*, E120. https://doi.org/10.1017/S0140525X17001972",
                "Translation": "** Researchers often review research records on a given topic to better understand effects and phenomena of interest before embarking on a new research project, to understand how theory links to evidence or to investigate common themes and directions of existing study results and claims. Different types of reviews can be conducted depending on the research question and literature scope. To determine the scope and key concepts in a given field, researchers may want to conduct a scoping literature review. Systematic reviews aim to access and review all available records for the most accurate and unbiased representation of existing literature. Non-systematic or focused literature reviews synthesise information from a selection of studies relevant to the research question although they are uncommon due to susceptibility to biases (e.g. researcher bias; Siddaway et al., 2019).",
                "Related_terms": "** Evidence synthesis; Meta-research; Narrative reviews; Systematic reviews"
            },
            {
                "Title": "Manel *",
                "Definition": "** Portmanteau for ‘male panel’, usually to refer to speaker panels at conferences entirely composed of (usually caucasian) males. Typically discussed in the context of gender disparities in academia (e.g., women being less likely to be recognised as experts by their peers and, subsequently, having fewer opportunities for career development).",
                "Reference(s)": "** Bouvy and Mujoomdar (2019); Goodman and Pepinsky (2019); Nittrouer et al. (2018); Rodriguez and Günther (2020)",
                "Drafted by": "** Sam Parsons",
                "Reviewed (or Edited) by": "** Mahmoud Elsherif; Thomas Rhys Evans; Beatrice Valentini; Christopher Graham; Flávio Azevedo",
                "Translation": "** Portmanteau for ‘male panel’, usually to refer to speaker panels at conferences entirely composed of (usually caucasian) males. Typically discussed in the context of gender disparities in academia (e.g., women being less likely to be recognised as experts by their peers and, subsequently, having fewer opportunities for career development).",
                "Related_terms": "** Bropenscience; Diversity; Equity; Feminist psychology; Inclusion; Under-representation"
            },
            {
                "Title": "Many authors *",
                "Definition": "** Large-scale collaborative projects involving tens or hundreds of authors from different institutions. This kind of approach has become increasingly common in psychology and other sciences in recent years as opposed to research carried out by small teams of authors, following earlier trends which have been observed e.g. for high-energy physics or biomedical research in the 1990s. These large international scientific consortia work on a research project to bring together a broader range of expertise and work collaboratively to produce manuscripts.",
                "Reference(s)": "** Cronin (2001); Moshontz et al. (2021); Wuchty et al. (2007)",
                "Drafted by": "** Yu-Fang Yang",
                "Reviewed (or Edited) by": "** Christopher Graham; Adam Parker; Charlotte R. Pennington; Birgit Schmidt; Beatrice Valentini",
                "Translation": "** Large-scale collaborative projects involving tens or hundreds of authors from different institutions. This kind of approach has become increasingly common in psychology and other sciences in recent years as opposed to research carried out by small teams of authors, following earlier trends which have been observed e.g. for high-energy physics or biomedical research in the 1990s. These large international scientific consortia work on a research project to bring together a broader range of expertise and work collaboratively to produce manuscripts.",
                "Related_terms": "** Collaboration; Consortia; Consortium authorship; Crowdsourcing; Hyperauthorship; Multiple-authors; Team science"
            },
            {
                "Title": "Many Labs *",
                "Definition": "** A crowdsourcing initiative led by the Open Science Collaboration (2015) whereby several hundred separate research groups from various universities run replication studies of published effects. This initiative is also known as “Many Labs I” and was subsequently followed by a “Many Labs II” project that assessed variation in replication results across samples and settings. Similar projects include ManyBabies, EEGManyLabs, and the Psychological Science Accelerator.",
                "Reference(s)": "** Ebersole et al. (2016); Frank et al. (2017); Klein et al. (2014); Klein et al. (2018); Moshontz et al. (2018); Open Science Collaboration (2015); Pavlov et al. (2020)",
                "Drafted by": "** Sam Parsons",
                "Reviewed (or Edited) by": "** Helena Hartmann; Charlotte R. Pennington; Mirela Zaneva",
                "Translation": "** A crowdsourcing initiative led by the Open Science Collaboration (2015) whereby several hundred separate research groups from various universities run replication studies of published effects. This initiative is also known as “Many Labs I” and was subsequently followed by a “Many Labs II” project that assessed variation in replication results across samples and settings. Similar projects include ManyBabies, EEGManyLabs, and the Psychological Science Accelerator.",
                "Related_terms": "** Collaboration; Many analysts; Many Labs I; Many Labs II; Open Science Collaboration; Replication"
            },
            {
                "Title": "Massive Open Online Courses (MOOCs) *",
                "Definition": "** Exclusively online courses which are accessible to any learner at any time, are typically free to access (while not necessarily openly licensed), and provide video-based instructions and downloadable data sets and exercises. The “massive” aspect describes the high volume of students that can access the course at any one time due to their flexibility, low or no cost, and online nature of the materials.",
                "Reference(s)": "** Baturay (2015); [https://opensciencemooc.eu/](https://opensciencemooc.eu/)",
                "Drafted by": "** Elizabeth Collins",
                "Reviewed (or Edited) by": "** Tsvetomira Dumbalska; Mahmoud Elsherif; Helena Hartmann; Sam Parsons; Charlotte R. Pennington",
                "Translation": "** Exclusively online courses which are accessible to any learner at any time, are typically free to access (while not necessarily openly licensed), and provide video-based instructions and downloadable data sets and exercises. The “massive” aspect describes the high volume of students that can access the course at any one time due to their flexibility, low or no cost, and online nature of the materials.",
                "Related_terms": "** Accessibility; Distance education; Inclusion; Open learning"
            },
            {
                "Title": "Massively Open Online Papers (MOOPs) *",
                "Definition": "** Unlike the traditional collaborative article, a MOOP follows an open participatory and dynamic model that is not restricted by a predetermined list of contributors.",
                "Reference(s)": "** Himmelstein et al. (2019); Tennant et al. (2019)",
                "Drafted by": "** Ali H. Al-Hoorie",
                "Reviewed (or Edited) by": "**",
                "Translation": "** Unlike the traditional collaborative article, a MOOP follows an open participatory and dynamic model that is not restricted by a predetermined list of contributors.",
                "Related_terms": "** Citizen science; Collaboration; Crowdsourced Research; Many authors; Team science"
            },
            {
                "Title": "Matthew effect (in science) *",
                "Definition": "** Named for the ‘rich get richer; poor get poorer’ paraphrase of the Gospel of Matthew. Eminent scientists and early-career researchers with a prestigious fellowship are disproportionately attributed greater levels of credit and funding for their contributions to science while relatively unknown or early-career researchers without a prestigious fellowship tend to get disproportionately little credit for comparable contributions. The impact is a substantial cumulative advantage that results from modest initial comparative advantages (and vice versa).",
                "Reference(s)": "** Bol et al. (2018); Bornmann et al. (2019); Merton (1968)",
                "Drafted by": "** Tamara Kalandadze",
                "Reviewed (or Edited) by": "** Bradley Baker; Tsvetomira Dumbalska; Mahmoud Elsherif; Matt Jaquiery; Charlotte R. Pennington",
                "Translation": "** Named for the ‘rich get richer; poor get poorer’ paraphrase of the Gospel of Matthew. Eminent scientists and early-career researchers with a prestigious fellowship are disproportionately attributed greater levels of credit and funding for their contributions to science while relatively unknown or early-career researchers without a prestigious fellowship tend to get disproportionately little credit for comparable contributions. The impact is a substantial cumulative advantage that results from modest initial comparative advantages (and vice versa).",
                "Related_terms": "** Matthew effect in education; Stigler’s law of eponymy"
            },
            {
                "Title": "Meta-analysis *",
                "Definition": "** A meta-analysis is a statistical synthesis of results from a series of studies examining the same phenomenon. A variety of meta-analytic approaches exist, including random or fixed effects models or meta-regressions, which allow for an examination of moderator effects. By aggregating data from multiple studies, a meta-analysis could provide a more precise estimate for a phenomenon (e.g. type of treatment) than individual studies. Results are usually visualized in a forest plot. Meta-analyses can also help examine heterogeneity across study results. Meta-analyses are often carried out in conjunction with systematic reviews and similarly require a systematic search and screening of studies. Publication bias is also commonly examined in the context of a meta-analysis and is typically visually presented via a funnel plot.",
                "Reference(s)": "** Borenstein et al. (2011); [Yeung et al. (2021)](https://mgto.org/exp-ma-rr-template-folder)",
                "Drafted by": "** Martin Vasilev; Siu Kit Yeung",
                "Reviewed (or Edited) by": "** Thomas Rhys Evans; Tamara Kalandadze; Charlotte R. Pennington; Mirela Zaneva",
                "Translation": "** A meta-analysis is a statistical synthesis of results from a series of studies examining the same phenomenon. A variety of meta-analytic approaches exist, including random or fixed effects models or meta-regressions, which allow for an examination of moderator effects. By aggregating data from multiple studies, a meta-analysis could provide a more precise estimate for a phenomenon (e.g. type of treatment) than individual studies. Results are usually visualized in a forest plot. Meta-analyses can also help examine heterogeneity across study results. Meta-analyses are often carried out in conjunction with systematic reviews and similarly require a systematic search and screening of studies. Publication bias is also commonly examined in the context of a meta-analysis and is typically visually presented via a funnel plot.",
                "Related_terms": "** CONSORT; Correlational Meta-Analysis; Effect size; Evidence synthesis; Non-Intervention, Reproducible, and Open Systematic Reviews (NIRO-SR); PRISMA; Publication bias (File Drawer Problem); STROBE; Systematic Review"
            },
            {
                "Title": "Metadata *",
                "Definition": "** Structured data that describes and synthesises other data. Metadata can help find, organize, and understand data. Examples of metadata include creator, title, contributors, keywords, tags, as well as any kind of information necessary to verify and understand the results and conclusions of a study such as codebook on data labels, descriptions, the sample and data collection process.",
                "Reference(s)": "** Gollwitzer et al. (2020); [https://schema.datacite.org/](https://schema.datacite.org/)",
                "Drafted by": "** Matt Jaquiery",
                "Reviewed (or Edited) by": "** Helena Hartmann; Tina Lonsdorf; Charlotte R. Pennington; Mirela Zaneva",
                "Translation": "** Structured data that describes and synthesises other data. Metadata can help find, organize, and understand data. Examples of metadata include creator, title, contributors, keywords, tags, as well as any kind of information necessary to verify and understand the results and conclusions of a study such as codebook on data labels, descriptions, the sample and data collection process.",
                "Related_terms": "** Data; Open Data **Alternative definition:** (if applicable) Data about data"
            },
            {
                "Title": "Meta-science or Meta-research *",
                "Definition": "** The scientific study of science itself with the aim to describe, explain, evaluate and/or improve scientific practices. Meta-science typically investigates scientific methods, analyses, the reporting and evaluation of data, the reproducibility and replicability of research results, and research incentives.",
                "Reference(s)": "** Ioannidis et al. (2015); Peterson and Panofsky (2020)",
                "Drafted by": "** Elizabeth Collins",
                "Reviewed (or Edited) by": "** Tamara Kalandadze; Lisa Spitzer; Olmo van den Akker",
                "Translation": "** The scientific study of science itself with the aim to describe, explain, evaluate and/or improve scientific practices. Meta-science typically investigates scientific methods, analyses, the reporting and evaluation of data, the reproducibility and replicability of research results, and research incentives.",
                "Related_terms": "**"
            },
            {
                "Title": "Model (computational) *",
                "Definition": "** Computational models aim to mathematically translate the phenomena under study to better understand, communicate and predict complex behaviours.",
                "Reference(s)": "** Guest and Martin (2020); Wilson and Collins (2019)",
                "Drafted by": "** Charlotte R. Pennington",
                "Reviewed (or Edited) by": "** Jamie P. Cockcroft; Mahmoud Elsherif; Meng Liu; Yu-Fang Yang; Michele C. Lim",
                "Translation": "** Computational models aim to mathematically translate the phenomena under study to better understand, communicate and predict complex behaviours.",
                "Related_terms": "** algorithms; data simulation; hypothesis; theory; theory building"
            },
            {
                "Title": "Model (statistical) *",
                "Definition": "** A mathematical representation of observed data that aims to reflect the population under study, allowing for the better understanding of the phenomenon of interest, identification of relationships among variables and predictions about future instances. A classic example would be the application of Chi square to understand the relationship between smoking and cancer (Doll & Hill, 1954\\)**.**",
                "Reference(s)": "** Doll and Hill (1954)",
                "Drafted by": "** Jamie P. Cockcroft",
                "Reviewed (or Edited) by": "** Alaa AlDoh; Mahmoud Elsherif; Meng Liu; Catia M. Oliveira; Charlotte R. Pennington",
                "Translation": "** A mathematical representation of observed data that aims to reflect the population under study, allowing for the better understanding of the phenomenon of interest, identification of relationships among variables and predictions about future instances. A classic example would be the application of Chi square to understand the relationship between smoking and cancer (Doll & Hill, 1954\\)**.**",
                "Related_terms": "** Bayesian Inference; Model (computational); Model (philosophy); Null Hypothesis Significance Testing (NHST) **Alternative definition:** A mathematical model that embodies a set of statistical assumptions concerning the generation of sample data and is used to apply statistical analysis."
            },
            {
                "Title": "Model (philosophy) **",
                "Definition": "** The process by which a verbal description is formalised to remove ambiguity, while also constraining the dimensions a theory can span. The model is thus data derived. “Many scientific models are representational models: they represent a selected part or aspect of the world, which is the model’s target system” (Frigg & Hartman, 2020).",
                "Reference(s)": "** Frigg and Hartman, (2020); Glass and Martin (2008); Guest and Martin (2020)",
                "Drafted by": "** Mahmoud Elsherif",
                "Reviewed (or Edited) by": "Charlotte R. Pennington; Michele C. Lim",
                "Translation": "** The process by which a verbal description is formalised to remove ambiguity, while also constraining the dimensions a theory can span. The model is thus data derived. “Many scientific models are representational models: they represent a selected part or aspect of the world, which is the model’s target system” (Frigg & Hartman, 2020).",
                "Related_terms": "** Hypothesis; Theory; Theory building"
            },
            {
                "Title": "Multi-Analyst Studies *",
                "Definition": "** In typical empirical studies, a single researcher or research team conducts the analysis, which creates uncertainty about the extent to which the choice of analysis influences the results. In multi-analyst studies, two or more researchers independently analyse the same research question or hypothesis on the same dataset. According to Aczel and colleagues (2021), a multi-analyst approach may be beneficial in increasing our confidence in a particular finding; uncovering the impact of analytical preferences across research teams; and highlighting the variability in such analytical approaches.",
                "Reference(s)": "** Aczel et. al. (2021); Silberzahn et al. (2018)",
                "Drafted by": "** Sam Parsons",
                "Reviewed (or Edited) by": "** Tsvetomira Dumbalska; Mahmoud Elsherif; William Ngiam; Charlotte R. Pennington; Graham Reid; Barnabas Szaszi; Flávio Azevedo",
                "Translation": "** In typical empirical studies, a single researcher or research team conducts the analysis, which creates uncertainty about the extent to which the choice of analysis influences the results. In multi-analyst studies, two or more researchers independently analyse the same research question or hypothesis on the same dataset. According to Aczel and colleagues (2021), a multi-analyst approach may be beneficial in increasing our confidence in a particular finding; uncovering the impact of analytical preferences across research teams; and highlighting the variability in such analytical approaches.",
                "Related_terms": "** Analytic flexibility; Crowdsourcing science; Data Analysis; Garden of Forking Paths; Multiverse Analysis; Researcher Degrees of Freedom; Scientific Transparency"
            },
            {
                "Title": "Multiplicity *",
                "Definition": "** Potential inflation of Type I error rates (incorrectly rejecting the null hypothesis) because of multiple statistical testing, for example, multiple outcomes, multiple follow-up time points, or multiple subgroup analyses. To overcome issues with multiplicity, researchers will often apply controlling procedures (e.g., Bonferroni, Holm-Bonferroni; Tukey) that correct the alpha value to control for inflated Type I errors. However, by controlling for Type I errors, one can increase the possibility of Type II errors (i.e., incorrectly accepting the null hypothesis).",
                "Reference(s)": "** Sato (1996); Schultz and Grimes (2005)",
                "Drafted by": "** Aidan Cashin",
                "Reviewed (or Edited) by": "** Jamie P. Cockcroft; Mahmoud Elsherif; Meng Liu; Charlotte R. Pennington",
                "Translation": "** Potential inflation of Type I error rates (incorrectly rejecting the null hypothesis) because of multiple statistical testing, for example, multiple outcomes, multiple follow-up time points, or multiple subgroup analyses. To overcome issues with multiplicity, researchers will often apply controlling procedures (e.g., Bonferroni, Holm-Bonferroni; Tukey) that correct the alpha value to control for inflated Type I errors. However, by controlling for Type I errors, one can increase the possibility of Type II errors (i.e., incorrectly accepting the null hypothesis).",
                "Related_terms": "** Alpha; False Discovery Rate; Multiple comparisons problem; Multiple testing; Null Hypothesis Significance Testing (NHST)"
            },
            {
                "Title": "Multiverse analysis *",
                "Definition": "** Multiverse analyses are based on all potentially equally justifiable data processing and statistical analysis pipelines that can be employed to test a single hypothesis. In a data multiverse analysis, a single set of raw data is processed into a multiverse of data sets by applying all possible combinations of justifiable preprocessing choices. Model multiverse analyses apply equally justifiable statistical models to the same data to answer the same hypothesis. The statistical analysis is then conducted on all data sets in the multiverse and all results are reported which enhances promoting transparency and illustrates the robustness of results against different data processing (data multiverse) or statistical (model multiverse) pipelines). Multiverse analysis differs from Specification curve analysis with regards to the graphical displays (a histogram and tile plota rather than a specification curve plot).",
                "Reference(s)": "** Del Giudice and Gangestad (2021); Steegen et al. (2016)",
                "Drafted by": "** Tina Lonsdorf; Flávio Azevedo",
                "Reviewed (or Edited) by": "** Mahmoud Elsherif; Adrien Fillon; William Ngiam; Sam Parsons  ###  ### **N** {#n}",
                "Translation": "** Multiverse analyses are based on all potentially equally justifiable data processing and statistical analysis pipelines that can be employed to test a single hypothesis. In a data multiverse analysis, a single set of raw data is processed into a multiverse of data sets by applying all possible combinations of justifiable preprocessing choices. Model multiverse analyses apply equally justifiable statistical models to the same data to answer the same hypothesis. The statistical analysis is then conducted on all data sets in the multiverse and all results are reported which enhances promoting transparency and illustrates the robustness of results against different data processing (data multiverse) or statistical (model multiverse) pipelines). Multiverse analysis differs from Specification curve analysis with regards to the graphical displays (a histogram and tile plota rather than a specification curve plot).",
                "Related_terms": "** Garden of forking paths; Robustness (analyses); Specification curve analysis; Vibration of effects"
            },
            {
                "Title": "Name Ambiguity Problem *",
                "Definition": "** An attribution issue arising from two related problems: authors may use multiple names or monikers to publish work, and multiple authors in a single field may share full names. This makes accurate identification of authors on names and specialisms alone a difficult task. This can be addressed through the creation and use of unique digital identifiers that act akin to digital fingerprints such as ORCID.",
                "Reference(s)": "** Wilson and Fenner (2012)",
                "Drafted by": "** Shannon Francis",
                "Reviewed (or Edited) by": "** Tsvetomira Dumbalska; Mahmoud Elsherif; Helena Hartmann; Wanyin Li; Charlotte R. Pennington",
                "Translation": "** An attribution issue arising from two related problems: authors may use multiple names or monikers to publish work, and multiple authors in a single field may share full names. This makes accurate identification of authors on names and specialisms alone a difficult task. This can be addressed through the creation and use of unique digital identifiers that act akin to digital fingerprints such as ORCID.",
                "Related_terms": "** Authorship; DOI (digital object identifier); ORCID (Open Researcher and Contributor ID)"
            },
            {
                "Title": "Named entity-based Text Anonymization for Open Science (NETANOS) *",
                "Definition": "** A free, open-source anonymisation software that identifies and modifies named entities (e.g. persons, locations, times, dates). Its key feature is that it preserves critical context needed for secondary analyses. The aim is to assist researchers in sharing their raw text data, while adhering to research ethics.",
                "Reference(s)": "** Kleinberg et al. (2017)",
                "Originally drafted by": "** Norbert Vanek",
                "Reviewed (or Edited) by": "** Jamie P. Cockcroft; Aleksandra Lazić; Charlotte R. Pennington; Sam Parsons",
                "Translation": "** A free, open-source anonymisation software that identifies and modifies named entities (e.g. persons, locations, times, dates). Its key feature is that it preserves critical context needed for secondary analyses. The aim is to assist researchers in sharing their raw text data, while adhering to research ethics.",
                "Related_terms": "** Anonymity; Confidentiality; Data sharing; Research ethics"
            },
            {
                "Title": "Non-Intervention, Reproducible, and Open Systematic Reviews (NIRO-SR) *",
                "Definition": "** A comprehensive set of tools to facilitate the development, preregistration and dissemination of systematic literature reviews for non-intervention research. Part A represents detailed guidelines for creating and preregistering a systematic review protocol in the context of non-intervention research whilst preparing for transparency. Part B represents guidelines for writing up the completed systematic review, with a focus on enhancing reproducibility.",
                "Reference(s)": "** Topor et al. (2021)",
                "Drafted by": "** Asma Assaneea",
                "Reviewed (or Edited) by": "** Tsvetomira Dumbalska; Thomas Rhys Evans; Tamara Kalandadze; Jade Pickering; Mirela Zaneva",
                "Translation": "** A comprehensive set of tools to facilitate the development, preregistration and dissemination of systematic literature reviews for non-intervention research. Part A represents detailed guidelines for creating and preregistering a systematic review protocol in the context of non-intervention research whilst preparing for transparency. Part B represents guidelines for writing up the completed systematic review, with a focus on enhancing reproducibility.",
                "Related_terms": "** Knowledge accumulation; Systematic review; Systematic Review Protocol"
            },
            {
                "Title": "Null Hypothesis Significance Testing (NHST) *",
                "Definition": "** A frequentist approach to inference used to test the probability of an observed effect against the null hypothesis of no effect/relationship (Pernet, 2015). Such a conclusion is arrived at through use of an index called the *p*\\-value. Specifically, researchers will conclude an effect is present when an a priori alpha threshold, set by the researchers, is satisfied; this determines the acceptable level of uncertainty and is closely related to Type I error.",
                "Reference": "** Lakens et al. (2018); Pernet (2015); Spence and Stanley (2018)",
                "Originally drafted by": "** Alaa AlDoh",
                "Reviewed (or Edited) by": "** Jamie P. Cockcroft; Annalise A. LaPlume; Charlotte R. Pennington; Sonia Rishi  ### **O** {#o}",
                "Translation": "** A frequentist approach to inference used to test the probability of an observed effect against the null hypothesis of no effect/relationship (Pernet, 2015). Such a conclusion is arrived at through use of an index called the *p*\\-value. Specifically, researchers will conclude an effect is present when an a priori alpha threshold, set by the researchers, is satisfied; this determines the acceptable level of uncertainty and is closely related to Type I error.",
                "Related_terms": "** Inference; P-value; Statistical significance; Type I error"
            },
            {
                "Title": "Objectivity *",
                "Definition": "** The idea that scientific claims, methods, results and scientists themselves should remain value-free and unbiased, and thus not be affected by cultural, political, racial or religious bias as well as any personal interests (Merton, 1942).",
                "Reference(s)": "** Macfarlane and Cheng (2008); Merton (1942)",
                "Originally drafted by": "** Ryan Millager",
                "Reviewed (or Edited) by": "** Mahmoud Elsherif; Madeleine Ingham; Kai Krautter; Sam Parsons; Charlotte R. Pennington",
                "Translation": "** The idea that scientific claims, methods, results and scientists themselves should remain value-free and unbiased, and thus not be affected by cultural, political, racial or religious bias as well as any personal interests (Merton, 1942).",
                "Related_terms": "** Communality; Mertonian norms; Neutrality"
            },
            {
                "Title": "Ontology (Artificial Intelligence) *",
                "Definition": "** A set of axioms in a subject area that help classify and explain the nature of the entities under study and the relationships between them.",
                "Reference": "** Noy and McGuinness (2001)",
                "Originally drafted by": "** Emma Norris",
                "Reviewed (or Edited) by": "** Charlotte R. Pennington; Graham Reid",
                "Translation": "** A set of axioms in a subject area that help classify and explain the nature of the entities under study and the relationships between them.",
                "Related_terms": "** Axiology; Epistemology; Taxonomy"
            },
            {
                "Title": "Open access *",
                "Definition": "** “Free availability of scholarship on the public internet, permitting any users to read, download, copy, distribute, print, search, or link to the full texts of these research articles, crawl them for indexing, pass them as data to software, or use them for any other lawful purpose, without financial, legal, or technical barriers other than those inseparable from gaining access to the internet itself” (Boai, 2002). Different methods of achieving open access (OA) are often referred to by color, including Green Open Access (when the work is openly accessible from a public repository), Gold Open Access (when the work is immediately openly accessible upon publication via a journal website), and Platinum (or Diamond) Open Access (a subset of Gold OA in which all works in the journal are immediately accessible after publication from the journal website without the authors needing to pay an article processing fee \\[APC\\]).",
                "Reference(s)": "** [Budapest Open Access Initiative (2002)](https://www.budapestopenaccessinitiative.org/read); Suber (2015)",
                "Drafted by": "** Mahmoud Elsherif",
                "Reviewed (or Edited) by": "** Nick Ballou; Helena Hartmann; Aoife O’Mahony; Ross Mounce; Mariella Paul; Charlotte R. Pennington",
                "Translation": "** “Free availability of scholarship on the public internet, permitting any users to read, download, copy, distribute, print, search, or link to the full texts of these research articles, crawl them for indexing, pass them as data to software, or use them for any other lawful purpose, without financial, legal, or technical barriers other than those inseparable from gaining access to the internet itself” (Boai, 2002). Different methods of achieving open access (OA) are often referred to by color, including Green Open Access (when the work is openly accessible from a public repository), Gold Open Access (when the work is immediately openly accessible upon publication via a journal website), and Platinum (or Diamond) Open Access (a subset of Gold OA in which all works in the journal are immediately accessible after publication from the journal website without the authors needing to pay an article processing fee \\[APC\\]).",
                "Related_terms": "** Article Processing Charge; FAIR principles; Paywall; Preprint; Repository"
            },
            {
                "Title": "Open Code *",
                "Definition": "** Making computer code (e.g., programming, analysis code, stimuli generation) freely and publicly available in order to make research methodology and analysis transparent and allow for reproducibility and collaboration. Code can be made available via open code websites, such as GitHub, the Open Science Framework, and Codeshare (to name a few), enabling others to evaluate and correct errors and re-use and modify the code for subsequent research.",
                "Reference": "** Easterbrook (2014)",
                "Originally drafted by": "** Charlotte R. Pennington",
                "Reviewed (or Edited) by": "** Elizabeth Collins; Mahmoud Elsherif; Christopher Graham; Emma Henderson",
                "Translation": "** Making computer code (e.g., programming, analysis code, stimuli generation) freely and publicly available in order to make research methodology and analysis transparent and allow for reproducibility and collaboration. Code can be made available via open code websites, such as GitHub, the Open Science Framework, and Codeshare (to name a few), enabling others to evaluate and correct errors and re-use and modify the code for subsequent research.",
                "Related_terms": "** Computational Reproducibility; Open Access; Open Licensing; Open Material; Open Source; Open Source Software; Reproducibility; Syntax"
            },
            {
                "Title": "Open Data *",
                "Definition": "** Open data refers to data that is freely available and readily accessible for use by others without restriction, “Open data and content can be freely used, modified, and shared by anyone for any purpose” ([https://opendefinition.org/](https://opendefinition.org/)). Open data are subject to the requirement to attribute and share alike, thus it is important to consider appropriate Open Licenses. Sensitive or time-sensitive datasets can be embargoed or shared with more selective access options to ensure data integrity is upheld.",
                "Reference": "** [https://opendefinition.org/](https://opendefinition.org/) (version 2.1); [https://opendatahandbook.org/guide/en/what-is-open-data/](https://opendatahandbook.org/guide/en/what-is-open-data/)",
                "Originally drafted by": "** Lisa Spitzer",
                "Reviewed (or Edited) by": "** Sarah Ashcroft-Jones; Mahmoud Elsherif; Helena Hartmann; Matt Jaquiery; Flávio Azevedo; Ross Mounce; Charlotte R. Pennington; Steven Verheyen",
                "Translation": "** Open data refers to data that is freely available and readily accessible for use by others without restriction, “Open data and content can be freely used, modified, and shared by anyone for any purpose” ([https://opendefinition.org/](https://opendefinition.org/)). Open data are subject to the requirement to attribute and share alike, thus it is important to consider appropriate Open Licenses. Sensitive or time-sensitive datasets can be embargoed or shared with more selective access options to ensure data integrity is upheld.",
                "Related_terms": "** Badges (Open Science); Data availability; FAIR principles; Metadata; Open Licenses; Open Material; Reproducibility; Secondary data analysis"
            },
            {
                "Title": "Open Educational Resources (OERs) *",
                "Definition": "** Learning materials that can be modified and enhanced because their creators have given others permission to do so. The individuals or organizations that create OERs—which can include materials such as presentation slides, podcasts, syllabi, images, lesson plans, lecture videos, maps, worksheets, and even entire textbooks—waive some (if not all) of the copyright associated with their works, typically via legal tools like Creative Commons licenses, so others can freely access, reuse, translate, and modify them.",
                "Reference": "** [https://opensource.com/resources/what-open-education](https://opensource.com/resources/what-open-education); [https://en.unesco.org/themes/building-knowledge-societies/oer](https://en.unesco.org/themes/building-knowledge-societies/oer)",
                "Originally drafted by": "** Aleksandra Lazić",
                "Reviewed (or Edited) by": "** Sam Parsons; Charlotte R. Pennington; Steven Verheyen; Elizabeth Collins",
                "Translation": "** Learning materials that can be modified and enhanced because their creators have given others permission to do so. The individuals or organizations that create OERs—which can include materials such as presentation slides, podcasts, syllabi, images, lesson plans, lecture videos, maps, worksheets, and even entire textbooks—waive some (if not all) of the copyright associated with their works, typically via legal tools like Creative Commons licenses, so others can freely access, reuse, translate, and modify them.",
                "Related_terms": "** Accessibility; FORRT; Open access; Open Licenses; Open Material"
            },
            {
                "Title": "Open Educational Resources (OER) Commons **",
                "Definition": "** OER Commons (with OER standing for open educational resources) is a freely accessible online library allowing teachers to create, share and remix educational resources. The goal of the OER movement is to stimulate “collaborative teaching and learning” ([https://www.oercommons.org/about](https://www.oercommons.org/about)) and provide high-quality educational resources that are accessible for everyone.",
                "Reference(s)": "** www.oercommons.org",
                "Drafted by": "** Ali H. Al-Hoorie",
                "Reviewed (or Edited) by": "** Mahmoud Elsherif, Gisela H. Govaart",
                "Translation": "** OER Commons (with OER standing for open educational resources) is a freely accessible online library allowing teachers to create, share and remix educational resources. The goal of the OER movement is to stimulate “collaborative teaching and learning” ([https://www.oercommons.org/about](https://www.oercommons.org/about)) and provide high-quality educational resources that are accessible for everyone.",
                "Related_terms": "** Equity; FORRT; Inclusion; Open Scholarship Knowledge Base; Open Science Framework"
            },
            {
                "Title": "Open Licenses *",
                "Definition": "** Open licenses are provided with open data and open software (e.g., analysis code) to define how others can (re)use the licensed material. In setting out the permissions and restrictions, open licenses often permit the unrestricted access, reuse and retribution of an author’s original work. Datasets are typically licensed under a type of open licence known as a Creative Commons license (e.g., MIT, Apache, and GPL). These can differ in relatively subtle ways with GPL licenses (and their variants) being Copyleft licenses that require that any derivative work is licensed under the same terms as the original.",
                "Reference(s)": "** [https://opensource.org/licenses](https://opensource.org/licenses)",
                "Originally drafted by": "** Andrew J. Stewart",
                "Reviewed (or Edited) by": "** Elizabeth Collins; Sam Parsons; Graham Reid; Steven Verheyen",
                "Translation": "** Open licenses are provided with open data and open software (e.g., analysis code) to define how others can (re)use the licensed material. In setting out the permissions and restrictions, open licenses often permit the unrestricted access, reuse and retribution of an author’s original work. Datasets are typically licensed under a type of open licence known as a Creative Commons license (e.g., MIT, Apache, and GPL). These can differ in relatively subtle ways with GPL licenses (and their variants) being Copyleft licenses that require that any derivative work is licensed under the same terms as the original.",
                "Related_terms": "** Creative Commons (CC) License; Copyleft; Copyright; Licence; Open Data; Open Source"
            },
            {
                "Title": "Open Material *",
                "Definition": "** Author’s public sharing of materials that were used in a study, “such as survey items, stimulus materials, and experiment programs” (Kidwell et al., 2016, p. 3). Digitally-shareable materials are posted on open access repositories, which makes them publicly available and accessible. Depending on licensing, the material can be reused by other authors for their own studies. Components that are not digitally-shareable (e.g. biological materials, equipment) must be described in sufficient detail to allow reproducibility.",
                "Reference": "** Blohowiak et al. (2020); Kidwell et al. (2016)",
                "Originally drafted by": "** Lisa Spitzer",
                "Reviewed (or Edited) by": "** Sam Parsons; Charlotte R. Pennington; Olly Robertson; Emily A. Williams; Flávio Azevedo",
                "Translation": "** Author’s public sharing of materials that were used in a study, “such as survey items, stimulus materials, and experiment programs” (Kidwell et al., 2016, p. 3). Digitally-shareable materials are posted on open access repositories, which makes them publicly available and accessible. Depending on licensing, the material can be reused by other authors for their own studies. Components that are not digitally-shareable (e.g. biological materials, equipment) must be described in sufficient detail to allow reproducibility.",
                "Related_terms": "** Badges (Open Science); Credibility of scientific claims; FAIR principles; Open Access; Open Code; Open Data; Reproducibility; Transparency"
            },
            {
                "Title": "OpenNeuro *",
                "Definition": "** A free platform where researchers can freely and openly share, browse, download and re-use brain imaging data (e.g., MRI, MEG, EEG, iEEG, ECoG, ASL, and PET data).",
                "Reference(s)": "** Poldrack et al. (2013); Poldrack and Gorgolewski (2014) https://openneuro.org/",
                "Drafted by": "** Ali H. Al-Hoorie",
                "Reviewed (or Edited) by": "** Leticia Micheli, Gisela H. Govaart",
                "Translation": "** A free platform where researchers can freely and openly share, browse, download and re-use brain imaging data (e.g., MRI, MEG, EEG, iEEG, ECoG, ASL, and PET data).",
                "Related_terms": "** BIDS data structure; Open data; OpenfMRI"
            },
            {
                "Title": "Open Peer Review *",
                "Definition": "** A scholarly review mechanism providing disclosure of any combination of author and referee identities, as well as peer-review reports and editorial decision letters, to one another or publicly at any point during or after the peer review or publication process. It may also refer to the removal of restrictions on who can participate in peer review and the platforms for doing so. Note that ‘open peer review’ has been used interchangeably to refer to any, or all, of the above practices.",
                "Reference(s)": "Ross-Hellauer (2017)",
                "Originally drafted by": "** Sonia Rishi",
                "Reviewed (or Edited) by": "** Mahmoud Elsherif; Sam Parsons; Charlotte R. Pennington; Yuki Yamada; Flávio Azevedo",
                "Translation": "** A scholarly review mechanism providing disclosure of any combination of author and referee identities, as well as peer-review reports and editorial decision letters, to one another or publicly at any point during or after the peer review or publication process. It may also refer to the removal of restrictions on who can participate in peer review and the platforms for doing so. Note that ‘open peer review’ has been used interchangeably to refer to any, or all, of the above practices.",
                "Related_terms": "** Non-anonymised peer review; Open science; PRO (peer review openness) initiative; Transparent peer review"
            },
            {
                "Title": "Open Scholarship *",
                "Definition": "‘**Open scholarship’ is often used synonymously with ‘open science’, but extends to all disciplines, drawing in those which might not traditionally identify as science-based. It reflects the idea that knowledge of all kinds should be openly shared, transparent, rigorous, reproducible, replicable, accumulative, and inclusive (allowing for all knowledge systems). Open scholarship includes all scholarly activities that are not solely limited to research such as teaching and pedagogy.",
                "Reference(s)": "** Tennant et al. (2019) Foundations for Open Scholarship Strategy Development https://www.researchgate.net/publication/330742805\\_Foundations\\_for\\_Open\\_Scholarship\\_Strategy\\_Development",
                "Drafted by": "** Gerald Vineyard",
                "Reviewed (or Edited) by": "** Mahmoud Elsherif; Zoe Flack; Sam Parsons; Charlotte R. Pennington",
                "Translation": "‘**Open scholarship’ is often used synonymously with ‘open science’, but extends to all disciplines, drawing in those which might not traditionally identify as science-based. It reflects the idea that knowledge of all kinds should be openly shared, transparent, rigorous, reproducible, replicable, accumulative, and inclusive (allowing for all knowledge systems). Open scholarship includes all scholarly activities that are not solely limited to research such as teaching and pedagogy.",
                "Related_terms": "** Bropenscience; Decolonisation; Knowledge; Open Research; Open Science"
            },
            {
                "Title": "Open Scholarship Knowledge Base **",
                "Definition": "** The Open Scholarship Knowledge Base (OSKB) is a collaborative initiative to share knowledge on the what, why and how of open scholarship to make this knowledge easy to find and apply. Information is curated and created by the community. The OSKB is a community under the Center for Open Science (COS).",
                "Reference(s)": "** www.oercommons.org/hubs/OSKB",
                "Drafted by": "** Ali H. Al-Hoorie",
                "Reviewed (or Edited) by": "** Mahmoud Elsherif; Samuel Guay; Tamara Kalandadze",
                "Translation": "** The Open Scholarship Knowledge Base (OSKB) is a collaborative initiative to share knowledge on the what, why and how of open scholarship to make this knowledge easy to find and apply. Information is curated and created by the community. The OSKB is a community under the Center for Open Science (COS).",
                "Related_terms": "** Center for Open Science (COS), Open Educational Resources (OERs); Open scholarship; Open Science"
            },
            {
                "Title": "Open Science *",
                "Definition": "** An umbrella term reflecting the idea that scientific knowledge of all kinds, where appropriate, should be openly accessible, transparent, rigorous, reproducible, replicable, accumulative, and inclusive, all which are considered fundamental features of the scientific endeavour. Open science consists of principles and behaviors that promote transparent, credible, reproducible, and accessible science. Open science has six major aspects: open data, open methodology, open source, open access, open peer review, and open educational resources.",
                "Reference(s)": "** Abele-Brehm et al. (2019); Crüwell et al. (2019); Kathawalla et al. (2020); Syed (2019); Woelfe et al. (2011)",
                "Drafted by": "** Mahmoud Elsherif",
                "Reviewed (or Edited) by": "** Zoe Flack; Tamara Kalandadze; Charlotte R. Pennington; Qinyu Xiao",
                "Translation": "** An umbrella term reflecting the idea that scientific knowledge of all kinds, where appropriate, should be openly accessible, transparent, rigorous, reproducible, replicable, accumulative, and inclusive, all which are considered fundamental features of the scientific endeavour. Open science consists of principles and behaviors that promote transparent, credible, reproducible, and accessible science. Open science has six major aspects: open data, open methodology, open source, open access, open peer review, and open educational resources.",
                "Related_terms": "** Accessibility; Credibility; Open Data; Open Material; Open Peer Review; Open Research; Open Science Practices; Open Scholarship; Reproducibility crisis (aka Replicability or replication crisis); Reproducibility; Transparency"
            },
            {
                "Title": "Open Science Framework *",
                "Definition": "** A free and open source platform for researchers to organize and share their research project and to encourage collaboration. Often used as an open repository for research code, data and materials, preprints and preregistrations, while managing a more efficient workflow. Created and maintained by the Center for Open Science.",
                "Reference(s)": "** Foster and Deardorff (2017); https://osf.io/",
                "Drafted by": "** William Ngiam",
                "Reviewed (or Edited) by": "** Mahmoud Elsherif; Charlotte R. Pennington; Lisa Spitzer  ### ---",
                "Translation": "** A free and open source platform for researchers to organize and share their research project and to encourage collaboration. Often used as an open repository for research code, data and materials, preprints and preregistrations, while managing a more efficient workflow. Created and maintained by the Center for Open Science.",
                "Related_terms": "** Archive; Center for Open Science (COS); Open Code; Open Data; Preprint; Preregistration"
            },
            {
                "Title": "Open Source software *",
                "Definition": "** A type of computer software in which source code is released under a license that permits others to use, change, and distribute the software to anyone and for any purpose. Open source is more than openly accessible: the distribution terms of open-source software must comply with 10 specific criteria (see: [https://opensource.org/osd](https://opensource.org/osd)).",
                "Reference": "** [https://opensource.org/osd](https://opensource.org/osd); [https://www.fosteropenscience.eu/foster-taxonomy/open-source-open-science](https://www.fosteropenscience.eu/foster-taxonomy/open-source-open-science)",
                "Originally drafted by": "** Connor Keating",
                "Reviewed (or Edited) by": "** Jamie P. Cockcroft; Helena Hartmann; Charlotte R. Pennington; Andrew J. Stewart",
                "Translation": "** A type of computer software in which source code is released under a license that permits others to use, change, and distribute the software to anyone and for any purpose. Open source is more than openly accessible: the distribution terms of open-source software must comply with 10 specific criteria (see: [https://opensource.org/osd](https://opensource.org/osd)).",
                "Related_terms": "** Github; Open Access; Open Code; Open Data; Open Licenses; Python; R; Repository"
            },
            {
                "Title": "Open washing *",
                "Definition": "** Open washing, termed after “greenwashing”, refers to the act of claiming openness to secure perceptions of rigor or prestige associated with open practices. It has been used to characterise the marketing strategy of software companies that have the appearance of open-source and open-licensing, while engaging in proprietary practices. Open washing is a growing concern for those adopting open science practices as their actions are undermined by misleading uses of the practices, and actions designed to facilitate progressive developments are reduced to ‘ticking the box’ without clear quality control.",
                "Reference": "** Farrow (2017); Moretti (2020); Villum (2016); Vlaeminck and Podkrajac (2017)",
                "Originally drafted by": "** Meng Liu",
                "Reviewed (or Edited) by": "** Thomas Rhys Evans; Sam Guay; Sam Parsons; Charlotte R. Pennington; Beatrice Valentini",
                "Translation": "** Open washing, termed after “greenwashing”, refers to the act of claiming openness to secure perceptions of rigor or prestige associated with open practices. It has been used to characterise the marketing strategy of software companies that have the appearance of open-source and open-licensing, while engaging in proprietary practices. Open washing is a growing concern for those adopting open science practices as their actions are undermined by misleading uses of the practices, and actions designed to facilitate progressive developments are reduced to ‘ticking the box’ without clear quality control.",
                "Related_terms": "Open Access; Open Data; Open Source"
            },
            {
                "Title": "Optional Stopping *",
                "Definition": "** The practice of (repeatedly) analyzing data during the data collection process and deciding to stop data collection if a statistical criterion (e.g. *p*\\-value, or bayes factor) reaches a specified threshold. If appropriate methodological precautions are taken to control the type 1 error rate, this can be an efficient analysis procedure (e.g. Lakens, 2014). However, without transparent reporting or appropriate error control the type 1 error can increase greatly and optional stopping could be considered a Questionable Research Practice (QRP) or a form of p-hacking.",
                "Reference(s)": "** Beffara Bret et al. (2021); Lakens (2014); Sagarin et al. (2014); Schönbrodt et al. (2017)",
                "Originally Drafted by": "** Brice Beffara Bret; Bettina M. J. Kern",
                "Reviewed (or Edited) by": "** Ali H. Al-Hoorie; Helena Hartmann; Catia M. Oliveira; Sam Parsons",
                "Translation": "** The practice of (repeatedly) analyzing data during the data collection process and deciding to stop data collection if a statistical criterion (e.g. *p*\\-value, or bayes factor) reaches a specified threshold. If appropriate methodological precautions are taken to control the type 1 error rate, this can be an efficient analysis procedure (e.g. Lakens, 2014). However, without transparent reporting or appropriate error control the type 1 error can increase greatly and optional stopping could be considered a Questionable Research Practice (QRP) or a form of p-hacking.",
                "Related_terms": "** *P*\\-hacking; Questionable Research Practices or Questionable Reporting Practices (QRPs); Sequential testing"
            },
            {
                "Title": "ORCID (Open Researcher and Contributor ID) *",
                "Definition": "** A organisation that provides a registry of persistent unique identifiers (ORCID iDs) for researchers and scholars, allowing these users to link their digital research documents and other contributions to their ORCID record. This avoids the name ambiguity problem in scholarly communication. ORCID iDs provide unique, persistent identifiers connecting researchers and their scholarly work. It is free to register for an ORCID iD at https://orcid.org/register.",
                "Reference(s)": "** Haak et al. (2012); [https://orcid.org/](https://orcid.org/)",
                "Drafted by": "** Martin Vasilev",
                "Reviewed (or Edited) by": "** Bradley Baker; Mahmoud Elsherif; Shannon Francis; Charlotte R. Pennington; Emily A. Williams; Flávio Azevedo  ### ---",
                "Translation": "** A organisation that provides a registry of persistent unique identifiers (ORCID iDs) for researchers and scholars, allowing these users to link their digital research documents and other contributions to their ORCID record. This avoids the name ambiguity problem in scholarly communication. ORCID iDs provide unique, persistent identifiers connecting researchers and their scholarly work. It is free to register for an ORCID iD at https://orcid.org/register.",
                "Related_terms": "** Authorship; DOI (digital object identifier); Name Ambiguity Problem"
            },
            {
                "Title": "Overlay Journal *",
                "Definition": "** Open access electronic journals that collect and curate articles available from other sources (typically preprint servers, such as arXiv). Article curation may include (post-publication) peer review or editorial selection. Overlay journals do not publish novel material; rather, they organize and collate articles available in existing repositories.",
                "Reference": "** Ginsparg (1997, 2001); [https://discovery.ucl.ac.uk/id/eprint/19081/](https://discovery.ucl.ac.uk/id/eprint/19081/)",
                "Originally drafted by": "** Bradley Baker",
                "Reviewed (or Edited) by": "** Christopher Graham; Helena Hartmann; Sam Parsons; Charlotte R. Pennington   ###  ### **P** {#p}",
                "Translation": "** Open access electronic journals that collect and curate articles available from other sources (typically preprint servers, such as arXiv). Article curation may include (post-publication) peer review or editorial selection. Overlay journals do not publish novel material; rather, they organize and collate articles available in existing repositories.",
                "Related_terms": "** Open access; Preprint"
            },
            {
                "Title": "P-curve *",
                "Definition": "** P-curve is a tool for identifying potential publication bias and makes use of the distribution of significant *p*\\-values in a series of independent findings. The deviation from the expected right-skewed distribution can be used to assess the existence and degree of publication bias: if the curve is right-skewed, there are more low, highly significant *p*\\-values, reflecting an underlying true effect. If the curve is left-skewed, there are many barely significant results just under the 0.05-threshold. This suggests that the studies lack evidential value and may be underpinned by questionable research practices (QRPs; e.g., p-hacking). In the case of no true effect present (true null hypothesis) and unbiased *p*\\-value reporting, the *p*\\-curve should be a flat, horizontal line, representing the typical distribution of *p*\\-values.",
                "Reference": "** Bruns and Ioannidis (2016); Simonsohn et al. (2014a); Simonsohn et al.(2014b); Simonsohn et al. (2019)",
                "Originally drafted by": "** Bettina M. J. Kern",
                "Reviewed (or Edited) by": "** Sam Guay; Kamil Izydorczak; Charlotte R. Pennington; Robert M. Ross; Olmo van den Akker",
                "Translation": "** P-curve is a tool for identifying potential publication bias and makes use of the distribution of significant *p*\\-values in a series of independent findings. The deviation from the expected right-skewed distribution can be used to assess the existence and degree of publication bias: if the curve is right-skewed, there are more low, highly significant *p*\\-values, reflecting an underlying true effect. If the curve is left-skewed, there are many barely significant results just under the 0.05-threshold. This suggests that the studies lack evidential value and may be underpinned by questionable research practices (QRPs; e.g., p-hacking). In the case of no true effect present (true null hypothesis) and unbiased *p*\\-value reporting, the *p*\\-curve should be a flat, horizontal line, representing the typical distribution of *p*\\-values.",
                "Related_terms": "** File-drawer; Hypothesis; *P*\\-hacking; *p*\\-value; Publication bias (File Drawer Problem); Questionable Research Practices or Questionable Reporting Practices (QRPs); Selective reporting; Z-curve"
            },
            {
                "Title": "*p*****-hacking *",
                "Definition": "** Exploiting techniques that may artificially increase the likelihood of obtaining a statistically significant result by meeting the standard statistical significance criterion (typically α \\= .05). For example, performing multiple analyses and reporting only those at *p* \\< .05, selectively removing data until *p* \\< .05, selecting variables for use in analyses based on whether those parameters are statistically significant.",
                "Reference(s)": "** Hardwicke et al. (2014); Neuroskeptic (2012)",
                "Drafted by": "** Mahmoud Elsherif",
                "Reviewed (or Edited) by": "** Tamara Kalandadze; William Ngiam; Sam Parsons; Martin Vasilev",
                "Translation": "** Exploiting techniques that may artificially increase the likelihood of obtaining a statistically significant result by meeting the standard statistical significance criterion (typically α \\= .05). For example, performing multiple analyses and reporting only those at *p* \\< .05, selectively removing data until *p* \\< .05, selecting variables for use in analyses based on whether those parameters are statistically significant.",
                "Related_terms": "** Analytic flexibility; Fishing; Garden of forking paths; HARKing; Questionable Research Practices or Questionable Reporting Practices (QRPs); Selective reporting"
            },
            {
                "Title": "*p*****-value *",
                "Definition": "** A statistic used to evaluate the outcome of a hypothesis test in Null Hypothesis Significance Testing (NHST). It refers to the probability of observing an effect, or more extreme effect, assuming the null hypothesis is true (Lakens, 2021b). The American Statistical Association’s statement on p-values (Wasserstein & Lazar, 2016\\) notes that p-values are not an indicator of the truth of the null hypothesis and instead defines p-values in this way: “Informally, a p-value is the probability under a specified statistical model that a statistical summary of the data (e.g., the sample mean difference between two compared groups) would be equal to or more extreme than its observed value” (p. 131).",
                "Reference(s)": "** [https://psyteachr.github.io/glossary/p.html](https://psyteachr.github.io/glossary/p.html); Lakens (2021b); Wasserstein and Lazar (2016)",
                "Drafted by": "** Alaa AlDoh; Flávio Azevedo",
                "Reviewed (or Edited) by": "** Jamie P. Cockcroft; Charlotte R. Pennington; Suzanne L. K. Stewart; Robbie C.M. van Aert; Marcel A.L.M. van Assen; Martin Vasilev",
                "Translation": "** A statistic used to evaluate the outcome of a hypothesis test in Null Hypothesis Significance Testing (NHST). It refers to the probability of observing an effect, or more extreme effect, assuming the null hypothesis is true (Lakens, 2021b). The American Statistical Association’s statement on p-values (Wasserstein & Lazar, 2016\\) notes that p-values are not an indicator of the truth of the null hypothesis and instead defines p-values in this way: “Informally, a p-value is the probability under a specified statistical model that a statistical summary of the data (e.g., the sample mean difference between two compared groups) would be equal to or more extreme than its observed value” (p. 131).",
                "Related_terms": "** Null Hypothesis Statistical Testing (NHST); statistical significance"
            },
            {
                "Title": "Papermill *",
                "Definition": "** An organization that is engaged in scientific misconduct wherein multiple papers are produced by falsifying or fabricating data, e.g. by editing figures or numerical data or plagiarizing written text. Papermills are “alleged to offer products ranging from research data through to ghostwritten fraudulent or fabricated manuscripts and submission services” (Byrne & Christopher, 2020, p. 583). A papermill relates to the fast production and dissemination of multiple allegedly new papers. These are often not detected in the scientific publishing process and therefore either never found or retracted if discovered (e.g. through plagiarism software).",
                "Reference(s)": "** Byrne and Christopher (2020); Hackett and Kelly (2020)",
                "Originally drafted by": "** Helena Hartmann",
                "Reviewed (or Edited) by": "** Sarah Ashcroft-Jones; Elizabeth Collins; Mahmoud Elsherif; Charlotte R. Pennington",
                "Translation": "** An organization that is engaged in scientific misconduct wherein multiple papers are produced by falsifying or fabricating data, e.g. by editing figures or numerical data or plagiarizing written text. Papermills are “alleged to offer products ranging from research data through to ghostwritten fraudulent or fabricated manuscripts and submission services” (Byrne & Christopher, 2020, p. 583). A papermill relates to the fast production and dissemination of multiple allegedly new papers. These are often not detected in the scientific publishing process and therefore either never found or retracted if discovered (e.g. through plagiarism software).",
                "Related_terms": "** Data fabrication; Data falsification; Fraud; Plagiarism; Questionable Research Practices or Questionable Reporting Practices (QRPs); Scientific misconduct; Scientific publishing"
            },
            {
                "Title": "Paradata *",
                "Definition": "** Data that are captured about the characteristics and context of primary data collected from an individual \\- distinct from metadata. Paradata can be used to investigate a respondent’s interaction with a survey or an experiment on a micro-level. They can be most easily collected during computer mediated surveys but are not limited to them. Examples include response times to survey questions, repeated patterns of responses such as choosing the same answer for all questions, contextual characteristics of the participant such as injuries that prevent good performance on tasks, the number of premature responses to stimuli in an experiment. Paradata have been used for the investigation and adjustment of measurement and sampling errors.",
                "Reference": "** Kreuter (2013)",
                "Originally drafted by": "** Alexander Hart; Graham Reid",
                "Reviewed (or Edited) by": "** Helena Hartmann; Charlotte R. Pennington; Marta Topor; Flávio Azevedo",
                "Translation": "** Data that are captured about the characteristics and context of primary data collected from an individual \\- distinct from metadata. Paradata can be used to investigate a respondent’s interaction with a survey or an experiment on a micro-level. They can be most easily collected during computer mediated surveys but are not limited to them. Examples include response times to survey questions, repeated patterns of responses such as choosing the same answer for all questions, contextual characteristics of the participant such as injuries that prevent good performance on tasks, the number of premature responses to stimuli in an experiment. Paradata have been used for the investigation and adjustment of measurement and sampling errors.",
                "Related_terms": "** Auxiliary data; Data collection; Data quality; Metadata; Process information"
            },
            {
                "Title": "PARKing *",
                "Definition": "** PARKing (preregistering after results are known) is defined as the practice where researchers complete an experiment (possibly with infinite re-experimentation) before preregistering. This practice invalidates the purpose of preregistration, and is one of the QRPs (or, even scientific misconduct) that try to gain only \"credibility that it has been preregistered.\"",
                "Reference": "** [Ikeda et al. (2019)](https://www.jstage.jst.go.jp/article/sjpr/62/3/62_281/_pdf/-char/ja); [Yamada (2018)](https://www.frontiersin.org/articles/10.3389/fpsyg.2018.01831/full)",
                "Originally drafted by": "** Qinyu Xiao",
                "Reviewed (or Edited) by": "Helena Hartmann; Sam Parsons; Yuki Yamada",
                "Translation": "** PARKing (preregistering after results are known) is defined as the practice where researchers complete an experiment (possibly with infinite re-experimentation) before preregistering. This practice invalidates the purpose of preregistration, and is one of the QRPs (or, even scientific misconduct) that try to gain only \"credibility that it has been preregistered.\"",
                "Related_terms": "** HARKing; Preregistration; Questionable Research Practices or Questionable Reporting Practices (QRPs)"
            },
            {
                "Title": "Participatory Research *",
                "Definition": "** Participatory research refers to incorporating the views of people from relevant communities in the entire research process to achieve shared goals between researchers and the communities. This approach takes a collaborative stance that seeks to reduce the power imbalance between the researcher and those researched through a “systematic cocreation of new knowledge” (Andersson, 2018).",
                "Reference": "** Cornwall and Jewkes (1995); Fletcher-Watson et al. (2019); Kiernan (1999); Leavy (2017); Ottmann et al. (2011); Rose (2018)",
                "Originally drafted by": "** Tamara Kalandadze",
                "Reviewed (or Edited) by": "** Jamie P. Cockcroft; Bethan Iley; Halil E. Kocalar; Michele C. Lim",
                "Translation": "** Participatory research refers to incorporating the views of people from relevant communities in the entire research process to achieve shared goals between researchers and the communities. This approach takes a collaborative stance that seeks to reduce the power imbalance between the researcher and those researched through a “systematic cocreation of new knowledge” (Andersson, 2018).",
                "Related_terms": "** Collaborative research; Inclusion; Neurodiversity; Patient and Public Involvement (PPI); Transformative paradigm"
            },
            {
                "Title": "Patient and Public Involvement (PPI) *",
                "Definition": "** Active research collaboration with the population of interest, as opposed to conducting research “about” them. Researchers can incorporate the lived experience and expertise of patients and the public at all stages of the research process. For example, patients can help to develop a set of research questions, review the suitability of a study design, approve plain English summaries for grant/ethics applications and dissemination, collect and analyse data, and assist with writing up a project for publication. This is becoming highly recommended and even required by funders (Boivin et al., 2018).",
                "Reference": "** Boivin et al. (2018); [https://www.invo.org.uk/](https://www.invo.org.uk/)",
                "Originally drafted by": "** Jade Pickering",
                "Reviewed (or Edited) by": "** Mahmoud Elsherif; Sam Parsons; Catia M. Oliveira",
                "Translation": "** Active research collaboration with the population of interest, as opposed to conducting research “about” them. Researchers can incorporate the lived experience and expertise of patients and the public at all stages of the research process. For example, patients can help to develop a set of research questions, review the suitability of a study design, approve plain English summaries for grant/ethics applications and dissemination, collect and analyse data, and assist with writing up a project for publication. This is becoming highly recommended and even required by funders (Boivin et al., 2018).",
                "Related_terms": "** Co-production; Participatory research"
            },
            {
                "Title": "Paywall *",
                "Definition": "** A technological barrier that permits access to information only to individuals who have paid \\- either personally, or via an organisation \\- a designated fee or subscription.",
                "Reference": "** Day et al. (2020); [https://casrai.org/term/closed-access/](https://casrai.org/term/closed-access/);",
                "Originally drafted by": "** Bradley Baker",
                "Reviewed (or Edited) by": "** Mahmoud Elsherif; Sam Parsons; Charlotte R. Pennington; Julia Wolska",
                "Translation": "** A technological barrier that permits access to information only to individuals who have paid \\- either personally, or via an organisation \\- a designated fee or subscription.",
                "Related_terms": "** Accessibility; Open Access"
            },
            {
                "Title": "PCI (Peer Community In) *",
                "Definition": "** PCI is a non-profit organisation that creates communities of researchers who review and recommend unpublished preprints based upon high-quality peer review from at least two researchers in their field. These preprints are then assigned a DOI, similarly to a journal article. PCI was developed to establish a free, transparent and public scientific publication system based on the review and recommendation of preprints.",
                "Reference(s)": "** https://peercommunityin.org/",
                "Originally drafted by": "** Emma Henderson",
                "Reviewed (or Edited) by": "** Jamie P. Cockcroft; Christopher Graham; Bethan Iley; Aleksandra Lazić; Charlotte R. Pennington",
                "Translation": "** PCI is a non-profit organisation that creates communities of researchers who review and recommend unpublished preprints based upon high-quality peer review from at least two researchers in their field. These preprints are then assigned a DOI, similarly to a journal article. PCI was developed to establish a free, transparent and public scientific publication system based on the review and recommendation of preprints.",
                "Related_terms": "** Open Access; Open Archives; Open Peer Review; PCI Registered Reports; Peer review; Preprints"
            },
            {
                "Title": "PCI Registered Reports *",
                "Definition": "** An initiative launched in 2021 dedicated to receiving, reviewing, and recommending Registered Reports (RRs) across the full spectrum of Science, technology, engineering, and mathematics (STEM), medicine, social sciences and humanities. Peer Community In (PCI) RRs are overseen by a ‘Recommender’ (equivalent to an Action Editor) and reviewed by at least two experts in the relevant field. It provides free and transparent pre- (Stage 1\\) and post-study (Stage 2\\) reviews across research fields. A network of PCI RR-friendly journals endorse the PCI RR review criteria and commit to accepting, without further peer review, RRs that receive a positive final recommendation from PCI RR.",
                "Reference(s)": "** [https://rr.peercommunityin.org/about/about](https://rr.peercommunityin.org/about/about)",
                "Originally drafted by": "** Charlotte R. Pennington",
                "Reviewed (or Edited) by": "** Sarah Ashcroft-Jones; Jamie P. Cockcroft; Mahmoud Elsherif; Helena Hartmann",
                "Translation": "** An initiative launched in 2021 dedicated to receiving, reviewing, and recommending Registered Reports (RRs) across the full spectrum of Science, technology, engineering, and mathematics (STEM), medicine, social sciences and humanities. Peer Community In (PCI) RRs are overseen by a ‘Recommender’ (equivalent to an Action Editor) and reviewed by at least two experts in the relevant field. It provides free and transparent pre- (Stage 1\\) and post-study (Stage 2\\) reviews across research fields. A network of PCI RR-friendly journals endorse the PCI RR review criteria and commit to accepting, without further peer review, RRs that receive a positive final recommendation from PCI RR.",
                "Related_terms": "** In Principle Acceptance (IPA); Open Access; PCI (Peer Community In); Publication bias (File Drawer Problem); Registered Report; Results blind; Stage 1 study review; Stage 2 study review; Transparency"
            },
            {
                "Title": "Plan S *",
                "Definition": "** Plan S is an initiative, launched in September 2018 by cOAlition S, a consortium of research funding organisations, which aims to accelerate the transition to full and immediate Open Access. Participating funders require recipients of research grants to publish their research in compliant Open Access journals or platforms, or make their work openly and immediately available in an Open Access repository, from 2021 onwards. cOAlition S funders have commited to not financially support ‘hybrid’ Open Access publication fees in subscription venues. However, authors can comply with plan S through publishing Open Access in a subscription journal under a “transformative arrangement” as further described in the implementation guidance. The “S” in Plan S stands for shock.",
                "Reference": "** [https://www.coalition-s.org](https://www.coalition-s.org/)",
                "Originally drafted by": "** Olmo van den Akker",
                "Reviewed (or Edited) by": "** Jamie P. Cockcroft; Helena Hartmann; Halil E. Kocalar; Birgit Schmidt",
                "Translation": "** Plan S is an initiative, launched in September 2018 by cOAlition S, a consortium of research funding organisations, which aims to accelerate the transition to full and immediate Open Access. Participating funders require recipients of research grants to publish their research in compliant Open Access journals or platforms, or make their work openly and immediately available in an Open Access repository, from 2021 onwards. cOAlition S funders have commited to not financially support ‘hybrid’ Open Access publication fees in subscription venues. However, authors can comply with plan S through publishing Open Access in a subscription journal under a “transformative arrangement” as further described in the implementation guidance. The “S” in Plan S stands for shock.",
                "Related_terms": "** Open Access; DORA; Repository"
            },
            {
                "Title": "Positionality *",
                "Definition": "** The contextualization of both the research environment and the researcher, to define the boundaries within the research was produced (Jaraf, 2018). Positionality is typically centred and celebrated in qualitative research, but there have been recent calls for it to also be used in quantitative research as well. Positionality statements, whereby a researcher outlines their background and ‘position’ within and towards the research, have been suggested as one method of recognising and centring researcher bias.",
                "Reference(s)": "** Jafar (2018); Oxford Dictionaries (2017)",
                "Originally drafted by": "** Joanne McCuaig",
                "Reviewed (or Edited) by": "** Helena Hartmann; Aoife O’Mahony; Madeleine Pownall; Graham Reid",
                "Translation": "** The contextualization of both the research environment and the researcher, to define the boundaries within the research was produced (Jaraf, 2018). Positionality is typically centred and celebrated in qualitative research, but there have been recent calls for it to also be used in quantitative research as well. Positionality statements, whereby a researcher outlines their background and ‘position’ within and towards the research, have been suggested as one method of recognising and centring researcher bias.",
                "Related_terms": "** Bias; Reflexivity; Perspective"
            },
            {
                "Title": "Positionality Map *",
                "Definition": "** A reflexive tool for practicing explicit positionality in critical qualitative research. The map is to be used “as a flexible starting point to guide researchers to reflect and be reflexive about their social location. The map involves three tiers: the identification of social identities (Tier 1), how these positions impact our life (Tier 2), and details that may be tied to the particularities of our social identity (Tier 3).” (Jacobson and Mustafa 2019, p. 1). The aim of the map is “for researchers to be able to better identify and understand their social locations and how they may pose challenges and aspects of ease within the qualitative research process.”",
                "Reference": "** Jacobson and Mustafa (2019)",
                "Originally drafted by": "** Joanne McCuaig",
                "Reviewed (or Edited) by": "** Helena Hartmann; Michele C. Lim; Charlotte R. Pennington; Graham Reid",
                "Translation": "** A reflexive tool for practicing explicit positionality in critical qualitative research. The map is to be used “as a flexible starting point to guide researchers to reflect and be reflexive about their social location. The map involves three tiers: the identification of social identities (Tier 1), how these positions impact our life (Tier 2), and details that may be tied to the particularities of our social identity (Tier 3).” (Jacobson and Mustafa 2019, p. 1). The aim of the map is “for researchers to be able to better identify and understand their social locations and how they may pose challenges and aspects of ease within the qualitative research process.”",
                "Related_terms": "** Positionality; Qualitative research; Social identity map; Transparency"
            },
            {
                "Title": "Post Hoc *",
                "Definition": "** Post hoc is borrowed from Latin, meaning “after this”. In statistics, post hoc (or post hoc analysis) refers to the testing of hypotheses not specified prior to data analysis. In frequentist statistics, the procedure differs based on whether the analysis was planned or post-hoc, for example by applying more stringent error control. In contrast, Bayesian and likelihood approaches do not differ as a function of when the hypothesis was specified.",
                "Reference(s)": "** Dienes (p.166, 2008\\)",
                "Drafted by": "** Alaa Aldoh",
                "Reviewed (or Edited) by": "** Sam Parsons; Jamie P. Cockcroft; Bethan Iley; Halil E. Kocalar; Graham Reid; Flávio Azevedo",
                "Translation": "** Post hoc is borrowed from Latin, meaning “after this”. In statistics, post hoc (or post hoc analysis) refers to the testing of hypotheses not specified prior to data analysis. In frequentist statistics, the procedure differs based on whether the analysis was planned or post-hoc, for example by applying more stringent error control. In contrast, Bayesian and likelihood approaches do not differ as a function of when the hypothesis was specified.",
                "Related_terms": "** A priori, Ad hoc; HARKing; P-hacking"
            },
            {
                "Title": "Post Publication Peer Review **",
                "Definition": "** Peer review that takes place after research has been published. It is typically posted on a dedicated platform (e.g., PubPeer). It is distinct from the traditional commentary which is published in the same journal and which is itself usually peer reviewed.",
                "Drafted by": "** Ali H. Al-Hoorie",
                "Reviewed (or Edited) by": "** Mahmoud Elsherif; Sam Parsons",
                "Translation": "** Peer review that takes place after research has been published. It is typically posted on a dedicated platform (e.g., PubPeer). It is distinct from the traditional commentary which is published in the same journal and which is itself usually peer reviewed.",
                "Related_terms": "** Open Peer Review; PeerPub; Peer review"
            },
            {
                "Title": "Posterior distribution *",
                "Definition": "** A way to summarize one’s updated knowledge in Bayesian inference, balancing prior knowledge with observed data. In statistical terms, posterior distributions are proportional to the product of the likelihood function and the prior. A posterior probability distribution captures (un)certainty about a given parameter value.",
                "Reference(s)": "** Dienes (2014); Lüdtke et al. (2020); van de Schoot et al. (2021)",
                "Drafted by": "** Alaa AlDoh",
                "Reviewed (or Edited) by": "** Adam Parker; Jamie P. Cockcroft; Julia Wolska; Yu-Fang Yang; Charlotte R. Pennington",
                "Translation": "** A way to summarize one’s updated knowledge in Bayesian inference, balancing prior knowledge with observed data. In statistical terms, posterior distributions are proportional to the product of the likelihood function and the prior. A posterior probability distribution captures (un)certainty about a given parameter value.",
                "Related_terms": "** Bayes Factor; Bayesian inference; Bayesian parameter estimation; Likelihood function; Prior distribution"
            },
            {
                "Title": "Predatory Publishing *",
                "Definition": "** Predatory (sometimes “vanity”) publishing describes a range of business practices in which publishers seek to profit, primarily by collecting article processing charges (APCs), from publishing scientific works without necessarily providing legitimate quality checks (e.g., peer review) or editorial services. In its most extreme form, predatory publishers will publish any work, so long as charges are paid. Other less extreme strategies, such as sending out high numbers of unsolicited requests for editing or publishing in fee-driven special issues, have also been accused as predatory (Crosetto, 2021).",
                "Reference": "** Crosetto (2021); Xia et al. (2015)",
                "Originally drafted by": "** Nick Ballou",
                "Reviewed (or Edited) by": "** Olmo van den Akker; Helena Hartmann; Aleksandra Lazić; Graham Reid; Flávio Azevedo",
                "Translation": "** Predatory (sometimes “vanity”) publishing describes a range of business practices in which publishers seek to profit, primarily by collecting article processing charges (APCs), from publishing scientific works without necessarily providing legitimate quality checks (e.g., peer review) or editorial services. In its most extreme form, predatory publishers will publish any work, so long as charges are paid. Other less extreme strategies, such as sending out high numbers of unsolicited requests for editing or publishing in fee-driven special issues, have also been accused as predatory (Crosetto, 2021).",
                "Related_terms": "** Article Processing Charge (APC); Gaming (the system)"
            },
            {
                "Title": "PREPARE Guidelines *",
                "Definition": "** The PREPARE guidelines and checklist (Planning Research and Experimental Procedures on Animals: Recommendations for Excellence) aim to help the planning of animal research, and support adherence to the 3Rs (Replacement, Reduction or Refinement) and facilitate the reproducibility of animal research.",
                "Reference(s)": "** Smith et al. (2018)",
                "Drafted by": "** Ben Farrar",
                "Reviewed (or Edited) by": "** Mahmoud Elsherif; Gilad Feldman; Elias Garcia-Pelegrin",
                "Translation": "** The PREPARE guidelines and checklist (Planning Research and Experimental Procedures on Animals: Recommendations for Excellence) aim to help the planning of animal research, and support adherence to the 3Rs (Replacement, Reduction or Refinement) and facilitate the reproducibility of animal research.",
                "Related_terms": "** ARRIVE Guidelines; Reporting Guideline; STRANGE"
            },
            {
                "Title": "Preprint *",
                "Definition": "** A publicly available version of any type of scientific manuscript/research output preceding formal publication, considered a form of Green Open Access. Preprints are usually hosted on a repository (e.g. arXiv) that facilitates dissemination by sharing research results more quickly than through traditional publication. Preprint repositories typically provide persistent identifiers (e.g. DOIs) to preprints. Preprints can be published at any point during the research cycle, but are most commonly published upon submission (i.e., before peer-review). Accepted and peer-reviewed versions of articles are also often uploaded to preprint servers, and are called postprints.",
                "Reference(s)": "** Bourne et al. (2017); Elmore (2018)",
                "Drafted by": "** Mariella Paul",
                "Reviewed (or Edited) by": "** Gisela H. Govaart; Helena Hartmann; Sam Parsons; Tobias Wingen; Flávio Azevedo",
                "Translation": "** A publicly available version of any type of scientific manuscript/research output preceding formal publication, considered a form of Green Open Access. Preprints are usually hosted on a repository (e.g. arXiv) that facilitates dissemination by sharing research results more quickly than through traditional publication. Preprint repositories typically provide persistent identifiers (e.g. DOIs) to preprints. Preprints can be published at any point during the research cycle, but are most commonly published upon submission (i.e., before peer-review). Accepted and peer-reviewed versions of articles are also often uploaded to preprint servers, and are called postprints.",
                "Related_terms": "** Open Access; DOI (digital object identifier); Postprint; Working Paper"
            },
            {
                "Title": "Preregistration *",
                "Definition": "** The practice of publishing the plan for a study, including research questions/hypotheses, research design, data analysis before the data has been collected or examined. It is also possible to preregister secondary data analyses (Merten & Krypotos, 2019). A preregistration document is time-stamped and typically registered with an independent party (e.g., a repository) so that it can be publicly shared with others (possibly after an embargo period). Preregistration provides a transparent documentation of what was planned at a certain time point, and allows third parties to assess what changes may have occurred afterwards. The more detailed a preregistration is, the better third parties can assess these changes and with that the validity of the performed analyses. Preregistration aims to clearly distinguish confirmatory from exploratory research.",
                "Reference(s)": "** Haven and van Grootel (2019); Lewandowsky and Bishop (2016); Merten and Krypotos (2019); Navarro (2020); Nosek et al. (2018); Simmons et al. (2021)",
                "Drafted by": "** Mahmoud Elsherif",
                "Reviewed (or Edited) by": "** Gisela H. Govaart; Helena Hartmann; Tina Lonsdorf; William Ngiam; Eike Mark Rinke; Lisa Spitzer; Olmo van den Akker; Flávio Azevedo",
                "Translation": "** The practice of publishing the plan for a study, including research questions/hypotheses, research design, data analysis before the data has been collected or examined. It is also possible to preregister secondary data analyses (Merten & Krypotos, 2019). A preregistration document is time-stamped and typically registered with an independent party (e.g., a repository) so that it can be publicly shared with others (possibly after an embargo period). Preregistration provides a transparent documentation of what was planned at a certain time point, and allows third parties to assess what changes may have occurred afterwards. The more detailed a preregistration is, the better third parties can assess these changes and with that the validity of the performed analyses. Preregistration aims to clearly distinguish confirmatory from exploratory research.",
                "Related_terms": "** Confirmation bias; Confirmatory analyses; Exploratory Data Analysis; HARKing; Pre-analysis plan; Questionable Research Practices or Questionable Reporting Practices (QRPs); Registered Report; Research Protocol; Transparency"
            },
            {
                "Title": "Preregistration Pledge *",
                "Definition": "** In a “collective action in support of open and reproducible research practices'', the preregistration pledge is a campaign from the Project Free Our Knowledge that asks a researcher to commit to preregistering at least one study in the next two years ([https://freeourknowledge.org/about/](https://freeourknowledge.org/about/)). The project is a grassroots movement initiated by early career researchers (ECRs).",
                "Reference(s)": "** https://freeourknowledge.org/2020-12-03-preregistration-pledge/",
                "Originally drafted by": "** Helena Hartmann",
                "Reviewed (or Edited) by": "** Jamie P. Cockcroft; Mahmoud Elsherif; Aleksandra Lazić, Steven Verheyen",
                "Translation": "** In a “collective action in support of open and reproducible research practices'', the preregistration pledge is a campaign from the Project Free Our Knowledge that asks a researcher to commit to preregistering at least one study in the next two years ([https://freeourknowledge.org/about/](https://freeourknowledge.org/about/)). The project is a grassroots movement initiated by early career researchers (ECRs).",
                "Related_terms": "** Preregistration"
            },
            {
                "Title": "PRO (peer review openness) initiative *",
                "Definition": "** The agreement made by several academics that they will not provide a peer review of a manuscript unless certain conditions are met. Specifically, the manuscript authors should ensure the data and materials will be made publically available (or give a justification as to why they are not freely available or shared), provide documentation detailing how to interpret and run any files or code and detail where these files can be located via the manuscript itself.",
                "Reference": "** Morey et al. (2016)",
                "Originally drafted by": "** Jamie P. Cockcroft",
                "Reviewed (or Edited) by": "** Sarah Ashcroft-Jones; Mahmoud Elsherif; Helena Hartmann; Steven Verheyen",
                "Translation": "** The agreement made by several academics that they will not provide a peer review of a manuscript unless certain conditions are met. Specifically, the manuscript authors should ensure the data and materials will be made publically available (or give a justification as to why they are not freely available or shared), provide documentation detailing how to interpret and run any files or code and detail where these files can be located via the manuscript itself.",
                "Related_terms": "** Non-anonymised peer review; Open Science; Open Peer Review; Transparent peer review"
            },
            {
                "Title": "Prior distribution **",
                "Definition": "** Beliefs held by researchers about the parameters in a statistical model before further evidence is taken into account. A ‘prior’ is expressed as a probability distribution and can be determined in a number of ways (e.g., previous research, subjective assessment, principles such as maximising entropy given constraints), and is typically combined with the likelihood function using Bayes’ theorem to obtain a posterior distribution.",
                "Reference(s)": "** van de Schoot et al. (2021)",
                "Drafted by": "** Alaa AlDoh",
                "Reviewed (or Edited) by": "** Charlotte R. Pennington; Martin Vasilev",
                "Translation": "** Beliefs held by researchers about the parameters in a statistical model before further evidence is taken into account. A ‘prior’ is expressed as a probability distribution and can be determined in a number of ways (e.g., previous research, subjective assessment, principles such as maximising entropy given constraints), and is typically combined with the likelihood function using Bayes’ theorem to obtain a posterior distribution.",
                "Related_terms": "** Bayes Factor; Bayesian inference; Bayesian Parameter Estimation; Likelihood function; Posterior distribution"
            },
            {
                "Title": "Pseudonymisation *",
                "Definition": "** Pseudonymisation refers to a technique that involves replacing or removing any information that could lead to identification of research subjects’ identity whilst still being able to make them identifiable through the use of the combination of code number and identifiers. This process comprises the following steps: removal of all identifiers from the research dataset; attribution of a specific identifier (pseudonym) for each participant and using it to label each research record; and maintenance of a cipher that links the code number to the participant in a document physically separate from the dataset. Pseudonymisation is typically a minimum requirement from ethical committees when conducting research, especially on human participants or involving confidential information, in order to ensure upholding of data privacy.",
                "Reference": "** Mourby et al. (2018); UKRI ([https://mrc.ukri.org/documents/pdf/gdpr-guidance-note-5-identifiability-anonymisation-and-pseudonymisation/](https://mrc.ukri.org/documents/pdf/gdpr-guidance-note-5-identifiability-anonymisation-and-pseudonymisation/))",
                "Originally drafted by": "** Catia M. Oliveira",
                "Reviewed (or Edited) by": "** Helena Hartmann; Sam Parsons; Charlotte R. Pennington; Birgit Schmidt",
                "Translation": "** Pseudonymisation refers to a technique that involves replacing or removing any information that could lead to identification of research subjects’ identity whilst still being able to make them identifiable through the use of the combination of code number and identifiers. This process comprises the following steps: removal of all identifiers from the research dataset; attribution of a specific identifier (pseudonym) for each participant and using it to label each research record; and maintenance of a cipher that links the code number to the participant in a document physically separate from the dataset. Pseudonymisation is typically a minimum requirement from ethical committees when conducting research, especially on human participants or involving confidential information, in order to ensure upholding of data privacy.",
                "Related_terms": "** Anonymity; Confidentiality; Data privacy; De-identification; Pseudonymisation; Research ethics"
            },
            {
                "Title": "Pseudoreplication *",
                "Definition": "** When there is a lack of statistical independence presented in the data and thus artificially inflating the number of samples (i.e. replicates). For instance, collecting more than one data point from the same experimental unit (e.g. participant or crops). Numerous methods can overcome this, such as averaging across replicates (e.g., taking the mean RT for a participant) or implementing mixed effects models with the random effects structure accounting for the pseudoreplication (e.g., specifying each individual RT as belonging to the same subject). Note, the former option would be associated with a loss of information and statistical power.",
                "Reference(s)": "** Davies and Gray (2015); Hurlbert (1984); Lazic (2019)",
                "Drafted by": "** Ben Farrar",
                "Reviewed (or Edited) by": "** Jamie P. Cockcroft; Mahmoud Elsherif; Elias Garcia-Pelegrin; Annalise A. LaPlume",
                "Translation": "** When there is a lack of statistical independence presented in the data and thus artificially inflating the number of samples (i.e. replicates). For instance, collecting more than one data point from the same experimental unit (e.g. participant or crops). Numerous methods can overcome this, such as averaging across replicates (e.g., taking the mean RT for a participant) or implementing mixed effects models with the random effects structure accounting for the pseudoreplication (e.g., specifying each individual RT as belonging to the same subject). Note, the former option would be associated with a loss of information and statistical power.",
                "Related_terms": "** Confounding; Generalizability; Replication; Validity"
            },
            {
                "Title": "Psychometric meta-analysis *",
                "Definition": "** Psychometric meta-analyses aim to correct for attenuation of the effect sizes of interest due to measurement error and other artifacts by using procedures based on psychometric principles, e.g. reliability of the measures. These procedures should be implemented before using the synthesised effect sizes in correlational or experimental meta-analysis, as making these corrections tends to lead to larger and less variable effect sizes.",
                "Reference(s)": "** Borenstein et al. (2009); Schmidt and Hunter (2014)",
                "Drafted by": "** Adrien Fillon",
                "Reviewed (or Edited) by": "** Mahmoud Elsherif; Eduardo Garcia-Garzon; Helena Hartmann; Catia M. Oliveira; Flávio Azevedo",
                "Translation": "** Psychometric meta-analyses aim to correct for attenuation of the effect sizes of interest due to measurement error and other artifacts by using procedures based on psychometric principles, e.g. reliability of the measures. These procedures should be implemented before using the synthesised effect sizes in correlational or experimental meta-analysis, as making these corrections tends to lead to larger and less variable effect sizes.",
                "Related_terms": "** Correlational meta-analysis; Hunter-Schmidt meta-analysis; Meta-analysis; Non-Intervention, Reproducible, and Open Systematic Reviews (NIRO-SR); Publication bias (File Drawer Problem); Validity generalization"
            },
            {
                "Title": "Publication bias (File Drawer Problem) *",
                "Definition": "** The failure to publish results based on the \"direction or strength of the study findings\" (Dickersin & Min, 1993, p. 135). The bias arises when the evaluation of a study’s publishability disproportionately hinges on the outcome of the study, often with the inclination that novel and significant results are worth publishing more than replications and null results. This bias typically materializes through a disproportionate number of significant findings and inflated effect sizes. This process leads to the published scientific literature not being representative of the full extent of all research, and specifically underrepresents null finding. Such findings, in turn, land in the so called “file drawer”, where they are never published and have no findable documentation.",
                "Reference(s)": "** Dickersin and Min (1993); Devito and Goldacre (2019); Duval and Tweedie (2000a, 2000b); Franco et al. (2014); Lindsay (2020); Rothstein et al. (2005)",
                "Drafted by": "** Mahmoud Elsherif",
                "Reviewed (or Edited) by": "** Jamie P. Cockcroft; Gilad Feldman; Adrien Fillon; Helena Hartmann; Tamara Kalandadze; William Ngiam; Martin Vasilev; Olmo van den Akker; Flávio Azevedo",
                "Translation": "** The failure to publish results based on the \"direction or strength of the study findings\" (Dickersin & Min, 1993, p. 135). The bias arises when the evaluation of a study’s publishability disproportionately hinges on the outcome of the study, often with the inclination that novel and significant results are worth publishing more than replications and null results. This bias typically materializes through a disproportionate number of significant findings and inflated effect sizes. This process leads to the published scientific literature not being representative of the full extent of all research, and specifically underrepresents null finding. Such findings, in turn, land in the so called “file drawer”, where they are never published and have no findable documentation.",
                "Related_terms": "** Dissemination bias; P-curve; P-hacking; Selective reporting; Statistical significance; Trim and fill **Alternative definition:** In the context of meta-analysis, publication bias “...occurs whenever the research that appears in the published literature is systematically unrepresentative of the population of completed studies. Simply put, when the research that is readily available differs in its results from the results of all the research that has been done in an area, readers and reviewers of that research are in danger of drawing the wrong conclusion about what that body of research shows.” (Rothstein et al., 2005, p. 1\\) **Related terms to alternative definition:** meta-analysis"
            },
            {
                "Title": "Public Trust in Science",
                "Definition": "** Trust in the knowledge, guidelines and recommendations that has been produced or provided by scientists to the benefit of civil society (Hendriks et al., 2016). These may also refer to trust in scientific-based recommendations on public health (e.g., universal health-care, stem cell research, federal funds for women’s reproductive rights, preventive measures of contagious diseases, and vaccination), climate change, economic policies (e.g., welfare, inequality- and poverty-control) and their intersections. The trust a member of the public has in science has been shown to be influenced by a vast number of factors such as age (Anderson et al., 2012), gender (Von Roten, 2004), rejection of scientific norms (Lewandowsky & Oberauer, 2021), political ideology (Azevedo & Jost, 2021; Brewer & Ley, 2012; Leiserowitz et al., 2010), \tright-wing authoritarianism and social dominance (Kerr & Wilson, 2021), education (Bak, 2001; Hayes & Tariq, 2000), income (Anderson et al., 2012), science knowledge (Evans & Durant, 1995; Nisbet et al., 2002), social media use (Huber et al., 2019), and religiosity (Azevedo, 2021; Brewer & Ley, 2013; Liu & Priest, 2009).",
                "Reference(s)": "** Anderson et al. (2012); Azevedo (2021); Azevedo and Jost (2021); Bak (2001); Brewer and Ley (2013); Evans and Durant (1995); Hayes and Tariq (2000); Hendriks et al. (2016); Huber et al. (2019); Kerr and Wilson (2021); Lewandowsky and Oberauer (2021); Liu and Priest (2009); Nisbet et al. (2002); Schneider et al., (2019); Wingen et al. (2020)",
                "Originally drafted by": "** Tobias Wingen; Flávio Azevedo",
                "Reviewed (or Edited) by": "** Elias Garcia-Pelegrin; Helena Hartmann; Catia M. Oliveira; Olmo van den Akker",
                "Translation": "** Trust in the knowledge, guidelines and recommendations that has been produced or provided by scientists to the benefit of civil society (Hendriks et al., 2016). These may also refer to trust in scientific-based recommendations on public health (e.g., universal health-care, stem cell research, federal funds for women’s reproductive rights, preventive measures of contagious diseases, and vaccination), climate change, economic policies (e.g., welfare, inequality- and poverty-control) and their intersections. The trust a member of the public has in science has been shown to be influenced by a vast number of factors such as age (Anderson et al., 2012), gender (Von Roten, 2004), rejection of scientific norms (Lewandowsky & Oberauer, 2021), political ideology (Azevedo & Jost, 2021; Brewer & Ley, 2012; Leiserowitz et al., 2010), \tright-wing authoritarianism and social dominance (Kerr & Wilson, 2021), education (Bak, 2001; Hayes & Tariq, 2000), income (Anderson et al., 2012), science knowledge (Evans & Durant, 1995; Nisbet et al., 2002), social media use (Huber et al., 2019), and religiosity (Azevedo, 2021; Brewer & Ley, 2013; Liu & Priest, 2009).",
                "Related_terms": "** Credibility of scientific claims; Epistemic Trust"
            },
            {
                "Title": "Publish or Perish *",
                "Definition": "** An aphorism describing the pressure researchers feel to publish academic manuscripts, often in high prestige academic journals, in order to have a successful academic career. This pressure to publish a high quantity of manuscripts can go at the expense of the quality of the manuscripts. This institutional pressure is exacerbated by hiring procedures and funding decisions strongly focusing on the number and impact of publications.",
                "Reference(s)": "** Case (1928); Fanelli (2010)",
                "Drafted by": "** Eliza Woodward",
                "Reviewed (or Edited) by": "** Nick Ballou; Mahmoud Elsherif; Helena Hartmann; Annalise A. LaPlume; Sam Parsons; Timo Roettger; Olmo van den Akker",
                "Translation": "** An aphorism describing the pressure researchers feel to publish academic manuscripts, often in high prestige academic journals, in order to have a successful academic career. This pressure to publish a high quantity of manuscripts can go at the expense of the quality of the manuscripts. This institutional pressure is exacerbated by hiring procedures and funding decisions strongly focusing on the number and impact of publications.",
                "Related_terms": "** Incentive structure; Journal Impact Factor; Reproducibility crisis (aka Replicability or replication crisis); Salami slicing; Slow Science"
            },
            {
                "Title": "PubPeer **",
                "Definition": "** A website that allows users to post anonymous peer reviews of research that has been published (i.e. post-publication peer review).",
                "Reference(s)": "** www.pubpeer.com",
                "Drafted by": "** Ali H. Al-Hoorie",
                "Reviewed (or Edited) by": "** Mahmoud ELsherif",
                "Translation": "** A website that allows users to post anonymous peer reviews of research that has been published (i.e. post-publication peer review).",
                "Related_terms": "** Open Peer Review"
            },
            {
                "Title": "Python *",
                "Definition": "** An interpreted general-purpose programming language, intended to be user-friendly and easily readable, originally created by Guido van Rossum in 1991\\. Python has an extensive library of additional features with accessible documentation for tasks ranging from data analysis to experiment creation. It is a popular programming language in data science, machine learning and web development. Similar to R Markdown, Python can be presented in an interactive online format called a Jupyter notebook, combining code, data, and text.",
                "Reference": "** Lutz (2001)",
                "Originally drafted by": "** Shannon Francis",
                "Reviewed (or Edited) by": "** James E. Bartlett; Alexander Hart; Helena Hartmann; Dominik Kiersz; Graham Reid; Andrew J. Stewart  ### **Q** {#q}",
                "Translation": "** An interpreted general-purpose programming language, intended to be user-friendly and easily readable, originally created by Guido van Rossum in 1991\\. Python has an extensive library of additional features with accessible documentation for tasks ranging from data analysis to experiment creation. It is a popular programming language in data science, machine learning and web development. Similar to R Markdown, Python can be presented in an interactive online format called a Jupyter notebook, combining code, data, and text.",
                "Related_terms": "** Jupyter; Matplotlib; NumPy; OpenSesame; PsychoPy; R"
            },
            {
                "Title": "Qualitative research *",
                "Definition": "** Research which uses non-numerical data, such as textual responses, images, videos or other artefacts, to explore in-depth concepts, theories, or experiences. There are a wide range of qualitative approaches, from micro-detailed exploration of language or focusing on personal subjective experiences, to those which explore macro-level social experiences and opinions.",
                "Reference(s)": "** Aspers and Corte (2019); Levitt et al. (2017)",
                "Drafted by": "** Madeleine Pownall",
                "Reviewed (or Edited) by": "** Mahmoud Elsherif; Helena Hartmann; Oscar Lecuona; Claire Melia; Flávio Azevedo",
                "Translation": "** Research which uses non-numerical data, such as textual responses, images, videos or other artefacts, to explore in-depth concepts, theories, or experiences. There are a wide range of qualitative approaches, from micro-detailed exploration of language or focusing on personal subjective experiences, to those which explore macro-level social experiences and opinions.",
                "Related_terms": "** Bracketing Interviews; Positionality; Quantitative research; Reflexivity **Alternative definition:** (if applicable) In Psychology, the **epistemology** of qualitative research is typically concerned with understanding people’s perspectives. Such epistemology proposes assuming the equity of researchers and participants as human beings, and in consequence, the need of sympathetic human understanding instead of data-driven conclusions"
            },
            {
                "Title": "Quantitative research **",
                "Definition": "** Quantitative research encompasses a diverse range of methods to systematically investigate a range of phenomena via the use of numerical data which can be analysed with statistics.",
                "Reference(s)": "** Goertzen (2017)",
                "Drafted by": "** Aoife O’Mahony",
                "Reviewed (or Edited) by": "** Valeria Agostini; Tamara Kalandadze; Adam Parker",
                "Translation": "** Quantitative research encompasses a diverse range of methods to systematically investigate a range of phenomena via the use of numerical data which can be analysed with statistics.",
                "Related_terms": "** Measuring; Qualitative research; Sample size; Statistical power; Statistics"
            },
            {
                "Title": "Questionable Research Practices or Questionable Reporting Practices (QRPs) *",
                "Definition": "** A range of activities that intentionally or unintentionally distort data in favour of a researcher’s own hypotheses \\- or omissions in reporting such practices \\- including; selective inclusion of data, hypothesising after the results are known (HARKing), and *p*\\-hacking. Popularized by John et al. (2012).",
                "Reference(s)": "** Banks et al. (2016); Fiedler and Schwartz (2016); Hardwicke et al. (2014); John et al. (2012); Neuroskeptic (2012); Sijtsma (2016); Simonsohn et al. (2011)",
                "Drafted by": "** Mahmoud Elsherif",
                "Reviewed (or Edited) by": "** Tamara Kalandadze; William Ngiam; Sam Parsons; Mariella Paul; Eike Mark Rinke; Timo Roettger; Flávio Azevedo",
                "Translation": "** A range of activities that intentionally or unintentionally distort data in favour of a researcher’s own hypotheses \\- or omissions in reporting such practices \\- including; selective inclusion of data, hypothesising after the results are known (HARKing), and *p*\\-hacking. Popularized by John et al. (2012).",
                "Related_terms": "** Creative use of outliers; Fabrication; File-drawer; Garden of forking paths; HARKing; Nonpublication of data; *P*\\-hacking; *P*\\-value fishing; Partial publication of data; Post-hoc storytelling; Preregistration; Questionable Measurement Practices (QMP); Researcher degrees of freedom; Reverse *p*\\-hacking; Salami slicing"
            },
            {
                "Title": "Questionable Measurement Practices (QMP) *",
                "Definition": "** Decisions researchers make that raise doubts about the validity of measures used in a study, and ultimately the study’s final conclusions (Flake & Fried, 2020). Issues arise from a lack of transparency in reporting measurement practices, a failure to address construct validity, negligence, ignorance, or deliberate misrepresentation of information.",
                "Reference": "** Flake and Fried (2020)",
                "Originally drafted by": "** Halil Emre Kocalar",
                "Reviewed (or Edited) by": "** Jamie P. Cockcroft; Annalise A. LaPlume; Sam Parsons; Mirela Zaneva; Flávio Azevedo   ###  ### **R** {#r}",
                "Translation": "** Decisions researchers make that raise doubts about the validity of measures used in a study, and ultimately the study’s final conclusions (Flake & Fried, 2020). Issues arise from a lack of transparency in reporting measurement practices, a failure to address construct validity, negligence, ignorance, or deliberate misrepresentation of information.",
                "Related_terms": "** Construct validity; Measurement schmeasurement; *P*\\-hacking; Psychometrics; Questionable Research Practices or Questionable Reporting Practices (QRPs); Validity"
            },
            {
                "Title": "R *",
                "Definition": "** R is a free, open-source programming language and software environment that can be used to conduct statistical analyses and plot data. R was created by Ross Ihaka and Robert Gentleman at the University of Auckland. R enables authors to share reproducible analysis scripts, which increases the transparency of a study. Often, R is used in conjunction with an integrated development environment (IDE) which simplifies working with the language, for example RStudio or Visual Studio Code, or Tinn-R .",
                "Reference": "** [https://www.r-project.org/](https://www.r-project.org/); R Core Team (2020)",
                "Originally drafted by": "** Lisa Spitzer",
                "Reviewed (or Edited) by": "** Bradley Baker; Alexander Hart; Joanne McCuaig; Andrew J. Stewart",
                "Translation": "** R is a free, open-source programming language and software environment that can be used to conduct statistical analyses and plot data. R was created by Ross Ihaka and Robert Gentleman at the University of Auckland. R enables authors to share reproducible analysis scripts, which increases the transparency of a study. Often, R is used in conjunction with an integrated development environment (IDE) which simplifies working with the language, for example RStudio or Visual Studio Code, or Tinn-R .",
                "Related_terms": "** Open-source; Statistical analysis"
            },
            {
                "Title": "Red Teams *",
                "Definition": "** An approach that integrates external criticism by colleagues and peers into the research process. Red teams are based on the idea that research that is more critically and widely evaluated is more reliable. The term originates from a military practice: One group (the red team) attacks something, and another group (the blue team) defends it. The practice has been applied to open science, by giving a red team (designated critical individuals) financial incentives to find errors in or identify improvements to the materials or content of a research project (in the materials, code, writing, etc.; Coles et al., 2020).",
                "Reference": "** Coles et al. (2020); Lakens (2020)",
                "Originally drafted by": "** Annalise A. LaPlume",
                "Reviewed (or Edited) by": "** Nick Ballou; Mahmoud Elsherif**;** Thomas Rhys Evans; Helena Hartmann; Timo Roettger",
                "Translation": "** An approach that integrates external criticism by colleagues and peers into the research process. Red teams are based on the idea that research that is more critically and widely evaluated is more reliable. The term originates from a military practice: One group (the red team) attacks something, and another group (the blue team) defends it. The practice has been applied to open science, by giving a red team (designated critical individuals) financial incentives to find errors in or identify improvements to the materials or content of a research project (in the materials, code, writing, etc.; Coles et al., 2020).",
                "Related_terms": "** Adversarial collaboration"
            },
            {
                "Title": "Reflexivity *",
                "Definition": "** The process of reflexivity refers to critically considering the knowledge that we produce through research, how it is produced, and our own role as researchers in producing this knowledge. There are different forms of reflexivity; personal reflexivity whereby researchers consider the impact of their own personal experiences, and functional whereby researchers consider the way in which our research tools and methods may have impacted knowledge production. Reflexivity aims to bring attention to underlying factors which may impact the research process, including development of research questions, data collection, and the analysis.",
                "Reference(s)": "** Braun and Clarke (2013); Finlay and Gough (2008)",
                "Drafted by": "** Claire Melia",
                "Reviewed (or Edited) by": "** Gilad Feldman; Annalise A. LaPlume",
                "Translation": "** The process of reflexivity refers to critically considering the knowledge that we produce through research, how it is produced, and our own role as researchers in producing this knowledge. There are different forms of reflexivity; personal reflexivity whereby researchers consider the impact of their own personal experiences, and functional whereby researchers consider the way in which our research tools and methods may have impacted knowledge production. Reflexivity aims to bring attention to underlying factors which may impact the research process, including development of research questions, data collection, and the analysis.",
                "Related_terms": "** Bracketing Interviews; Qualitative Research"
            },
            {
                "Title": "Registered Report *",
                "Definition": "** A scientific publishing format that includes an initial round of peer review of the background and methods (study design, measurement, and analysis plan); sufficiently high quality manuscripts are accepted for in-principle acceptance (IPA) at this stage. Typically, this stage 1 review occurs before data collection, however secondary data analyses are possible in this publishing format. Following data analyses and write up of results and discussion sections, the stage 2 review assesses whether authors sufficiently followed their study plan and reported deviations from it (and remains indifferent to the results). This shifts the focus of the review to the study’s proposed research question and methodology and away from the perceived interest in the study’s results.",
                "Reference(s)": "** Chambers (2013); Chambers et al. (2015); Chambers and Tzavella (2020); Findley et al. (2016); [https://www.cos.io/initiatives/registered-reports](https://www.cos.io/initiatives/registered-reports)",
                "Drafted by": "** Madeleine Pownall",
                "Reviewed (or Edited) by": "** Gilad Feldman; Emma Henderson; Aoife O’Mahony; Sam Parsons; Mariella Paul; Charlotte R. Pennington; Eike Mark Rinke; Timo Roettger; Olmo van den Akker; Yuki Yamada; Flávio Azevedo",
                "Translation": "** A scientific publishing format that includes an initial round of peer review of the background and methods (study design, measurement, and analysis plan); sufficiently high quality manuscripts are accepted for in-principle acceptance (IPA) at this stage. Typically, this stage 1 review occurs before data collection, however secondary data analyses are possible in this publishing format. Following data analyses and write up of results and discussion sections, the stage 2 review assesses whether authors sufficiently followed their study plan and reported deviations from it (and remains indifferent to the results). This shifts the focus of the review to the study’s proposed research question and methodology and away from the perceived interest in the study’s results.",
                "Related_terms": "** Preregistration; Publication bias (File Drawer Problem); Results-free review; PCI (Peer Community In); Research Protocol"
            },
            {
                "Title": "Registry of Research Data Repositories",
                "Definition": "** A global registry of research data repositories from different academic disciplines. It includes repositories that enable permanent storage of, description via metadata and access to, data sets by researchers, funding bodies, publishers, and scholarly institutions.",
                "Reference": "** [https://www.re3data.org/](https://www.re3data.org/) \\- Registry of Research Data Repositories.",
                "Originally drafted by": "** Aleksandra Lazić",
                "Reviewed (or Edited) by": "** Mahmoud Elsherif; Sam Parsons; Charlotte R. Pennington; Helena Hartmann",
                "Translation": "** A global registry of research data repositories from different academic disciplines. It includes repositories that enable permanent storage of, description via metadata and access to, data sets by researchers, funding bodies, publishers, and scholarly institutions.",
                "Related_terms": "** Metadata; Open Access; Open Data; Open Material; Repository"
            },
            {
                "Title": "Reliability *",
                "Definition": "** The extent to which repeated measurements lead to the same results. In psychometrics, reliability refers to the extent to which respondents have similar scores when they take a questionnaire on multiple occasions. Noteworthy, reliability does not imply validity. Furthermore, additional types of reliability besides internal consistency exist, including: test-retest reliability, parallel forms reliability and interrater reliability.",
                "Reference": "** Bollen (1989); Drost (2011)",
                "Originally drafted by": "** Annalise A. LaPlume",
                "Reviewed (or Edited) by": "** Mahmoud Elsherif**;** Eduardo Garcia-Garzon; Kai Krautter; Olmo van den Akker",
                "Translation": "** The extent to which repeated measurements lead to the same results. In psychometrics, reliability refers to the extent to which respondents have similar scores when they take a questionnaire on multiple occasions. Noteworthy, reliability does not imply validity. Furthermore, additional types of reliability besides internal consistency exist, including: test-retest reliability, parallel forms reliability and interrater reliability.",
                "Related_terms": "** Consistency; Internal consistency; Quality Criteria; Replicability; Reproducibility; Validity"
            },
            {
                "Title": "Repeatability *",
                "Definition": "** Synonymous with *test-retest* *reliability*. It refers to the agreement between the results of successive measurements of the same measure. Repeatability requires the same experimental tools, the same observer, the same measuring instrument administered under the same conditions, the same location, repetition over a short period of time, and the same objectives (Joint Committee for Guidelines in Metrology, 2008\\)",
                "Reference(s)": "** ISO (1993); Stodden (2011)",
                "Drafted by": "** Mahmoud Elsherif, Adam Parker",
                "Reviewed (or Edited) by": "** Gilad Feldman; Helena Hartmann; Joanne McCuaig; Sam Parsons",
                "Translation": "** Synonymous with *test-retest* *reliability*. It refers to the agreement between the results of successive measurements of the same measure. Repeatability requires the same experimental tools, the same observer, the same measuring instrument administered under the same conditions, the same location, repetition over a short period of time, and the same objectives (Joint Committee for Guidelines in Metrology, 2008\\)",
                "Related_terms": "** Reliability"
            },
            {
                "Title": "Replicability *",
                "Definition": "** An umbrella term, used differently across fields, covering concepts of: direct and conceptual replication, computational reproducibility/replicability, generalizability analysis and robustness analyses. Some of the definitions used previously include: a different team arriving at the same results using the original author's artifacts (Barba 2018); a study arriving at the same conclusion after collecting new data (Claerbout and Karrenbach, 1992); as well as studies for which any outcome would be considered diagnostic evidence about a claim from prior research (Nosek & Errington, 2020).",
                "Reference(s)": "** Barba (2018); Crüwell et al. (2019); King (1996); National Academies of Sciences et al. (2011); Nosek and Errington (2020)",
                "Drafted by": "** Mahmoud Elsherif",
                "Reviewed (or Edited) by": "** Jamie P. Cockcroft; Adrien Fillon; Gilad Feldman; Annalise A. LaPlume; Tina B. Lonsdorf; Sam Parsons; Eike Mark Rinke; Tobias Wingen",
                "Translation": "** An umbrella term, used differently across fields, covering concepts of: direct and conceptual replication, computational reproducibility/replicability, generalizability analysis and robustness analyses. Some of the definitions used previously include: a different team arriving at the same results using the original author's artifacts (Barba 2018); a study arriving at the same conclusion after collecting new data (Claerbout and Karrenbach, 1992); as well as studies for which any outcome would be considered diagnostic evidence about a claim from prior research (Nosek & Errington, 2020).",
                "Related_terms": "** Conceptual replication; Direct Replication; Generalizability; Reproducibility; Reliability; Robustness (analyses)"
            },
            {
                "Title": "Replication Markets **",
                "Definition": "** A replication market is an environment where users bet on the replicability of certain effects. Forecasters are incentivized to make accurate predictions and the top successful forecasters receive monetary compensation or contributorship for their bets. The rationale behind a replication market is that it leverages the collective wisdom of the scientific community to predict which effect will most likely replicate, thus encouraging researchers to channel their limited resources to replicating these effects.",
                "Reference": "** Liu et al. (2020); Tierney et al. (2020); Tierney et al. (2021); www.replicationmarkets.com",
                "Originally drafted by": "** Ali H. Al-Hoorie",
                "Reviewed (or Edited) by": "** Mahmoud Elsherif; Leticia Micheli; Sam Parsons",
                "Translation": "** A replication market is an environment where users bet on the replicability of certain effects. Forecasters are incentivized to make accurate predictions and the top successful forecasters receive monetary compensation or contributorship for their bets. The rationale behind a replication market is that it leverages the collective wisdom of the scientific community to predict which effect will most likely replicate, thus encouraging researchers to channel their limited resources to replicating these effects.",
                "Related_terms": "** Citizen science; Crowdsourcing; Replicability; Reproducibility"
            },
            {
                "Title": "Reporting Guideline *",
                "Definition": "** A reporting guideline is a “checklist, flow diagram, or structured text to guide authors in reporting a specific type of research, developed using explicit methodology.” (EQUATOR Network, n.d.). Reporting guidelines provide the minimum guidance required to ensure that research findings can be appropriately interpreted, appraised, synthesized and replicated. Their use often differs per scientific journal or publisher.",
                "Reference": "** Moher et al. (2009) Schulz et al. (2010); Torpor et al. (2021); Von Elm et al. (2007); [https://www.equator-network.org/about-us/what-is-a-reporting-guideline/](https://www.equator-network.org/about-us/what-is-a-reporting-guideline/)",
                "Originally drafted by": "** Aidan Cashin",
                "Reviewed (or Edited) by": "** Gilad Feldman; Helena Hartmann; Joanne McCuaig",
                "Translation": "** A reporting guideline is a “checklist, flow diagram, or structured text to guide authors in reporting a specific type of research, developed using explicit methodology.” (EQUATOR Network, n.d.). Reporting guidelines provide the minimum guidance required to ensure that research findings can be appropriately interpreted, appraised, synthesized and replicated. Their use often differs per scientific journal or publisher.",
                "Related_terms": "** CONSORT; Non-Intervention, Reproducible, and Open Systematic Reviews (NIRO-SR); PRISMA; STROBE"
            },
            {
                "Title": "Repository *",
                "Definition": "** An online archive for the storage of digital objects including research outputs, manuscripts, analysis code and/or data. Examples include preprint servers such as bioRxiv, MetaArXiv, PsyArXiv, institutional research repositories, as well as data repositories that collect and store datasets including zenodo.org, PsychData, and code repositories such as Github, or more general repositories for all kinds of research data, such as the Open Science Framework (OSF). Digital objects stored in repositories are typically described through metadata which enables discovery across different storage locations.",
                "Reference(s)": "** [https://www.nature.com/sdata/policies/repositories](https://www.nature.com/sdata/policies/repositories)",
                "Originally drafted by": "** Tina Lonsdorf",
                "Reviewed (or Edited) by": "** Gilad Feldman; Connor Keating; Mariella Paul; Charlotte R. Pennington; Flávio Azevedo",
                "Translation": "** An online archive for the storage of digital objects including research outputs, manuscripts, analysis code and/or data. Examples include preprint servers such as bioRxiv, MetaArXiv, PsyArXiv, institutional research repositories, as well as data repositories that collect and store datasets including zenodo.org, PsychData, and code repositories such as Github, or more general repositories for all kinds of research data, such as the Open Science Framework (OSF). Digital objects stored in repositories are typically described through metadata which enables discovery across different storage locations.",
                "Related_terms": "** Data sharing; Github; Metadata; Open Access; Open data; Open Material; Open Science Framework; Open Source; Preprint"
            },
            {
                "Title": "ReproducibiliTea *",
                "Definition": "A grassroots initiative that helps researchers create local journal clubs at their universities to discuss a range of topics relating to open research and scholarship. Each meeting usually centres around a specific paper that discusses, for example, reproducibility, research practice, research quality, social justice and inclusion, and ideas for improving science.",
                "Reference": "** [https://reproducibilitea.org/](https://reproducibilitea.org/); Orben (2019)",
                "Originally drafted by": "** Emma Norris",
                "Reviewed (or Edited) by": "** Mahmoud Elsherif; Gilad Feldman; Connor Keating; Charlotte R. Pennington; Sam Parsons; Flávio Azevedo",
                "Translation": "A grassroots initiative that helps researchers create local journal clubs at their universities to discuss a range of topics relating to open research and scholarship. Each meeting usually centres around a specific paper that discusses, for example, reproducibility, research practice, research quality, social justice and inclusion, and ideas for improving science.",
                "Related_terms": "** Grassroots initiative; Journal club; Open science; Reproducibility"
            },
            {
                "Title": "Reproducibility *",
                "Definition": "** A minimum standard on a spectrum of activities (\"reproducibility spectrum\") for assessing the value or accuracy of scientific claims based on the original methods, data, and code. For instance, where the original researcher's data and computer codes are used to regenerate the results (Barba, 2018), often referred to as computational reproducibility. Reproducibility does not guarantee the quality, correctness, or validity of the published results (Peng, 2011). In some fields, this meaning is, instead, associated with the term “replicability” or ‘repeatability’.",
                "Reference(s)": "** Barba (2018); Cruwell et al. (2019); Peng (2011), Stodden (2011); Syed (2019); National Academies of Sciences, Engineering, and Medicine (2019)",
                "Drafted by": "** Mahmoud Elsherif",
                "Reviewed (or Edited) by": "** Helena Hartmann; Annalise A. LaPlume; Tina B. Lonsdorf; Sam Parsons; Charlotte R. Pennington; Suzanne L. K. Stewart",
                "Translation": "** A minimum standard on a spectrum of activities (\"reproducibility spectrum\") for assessing the value or accuracy of scientific claims based on the original methods, data, and code. For instance, where the original researcher's data and computer codes are used to regenerate the results (Barba, 2018), often referred to as computational reproducibility. Reproducibility does not guarantee the quality, correctness, or validity of the published results (Peng, 2011). In some fields, this meaning is, instead, associated with the term “replicability” or ‘repeatability’.",
                "Related_terms": "** Computational reproducibility; Replicability; repeatability"
            },
            {
                "Title": "Reproducibility crisis (aka Replicability or replication crisis) *",
                "Definition": "** The finding, and related shift in academic culture and thinking, that a large proportion of scientific studies published across disciplines do not replicate (e.g. Open Science Collaboration, 2015). This is considered to be due to a lack of quality and integrity of research and publication practices, such as publication bias, QRPs and a lack of transparency, leading to an inflated rate of false positive results. Others have described this process as a ‘Credibility revolution’ towards improving these practices.",
                "Reference(s)": "** Fanelli (2018); Open Science Collaboration (2015)",
                "Drafted by": "** Mahmoud Elsherif",
                "Reviewed (or Edited) by": "** Helena Hartmann; Annalise A. LaPlume; Mariella Paul; Sonia Rishi; Lisa Spitzer",
                "Translation": "** The finding, and related shift in academic culture and thinking, that a large proportion of scientific studies published across disciplines do not replicate (e.g. Open Science Collaboration, 2015). This is considered to be due to a lack of quality and integrity of research and publication practices, such as publication bias, QRPs and a lack of transparency, leading to an inflated rate of false positive results. Others have described this process as a ‘Credibility revolution’ towards improving these practices.",
                "Related_terms": "** Credibility crisis; Publication bias (File Drawer Problem); Questionable Research Practices or Questionable Reporting Practices (QRPs); Replicability; Reproducibility"
            },
            {
                "Title": "Reproducibility Network *",
                "Definition": "** A reproducibility network is a consortium of open research working groups, often peer-led. The groups operate on a wheel-and-spoke model across a particular country, in which the network connects local cross-disciplinary researchers, groups, and institutions with a central steering group, who also connect with external stakeholders in the research ecosystem. The goals of reproducibility networks include; advocating for greater awareness, promoting training activities, and disseminating best-practices at grassroots, institutional, and research ecosystem levels. Such networks exist in the UK, Germany, Switzerland, Slovakia, and Australia (as of March 2021).",
                "Reference": "** [https://www.ukrn.org/](https://www.ukrn.org/) ; [https://reproducibilitynetwork.de/](https://reproducibilitynetwork.de/); [https://www.swissrn.org/](https://www.swissrn.org/); [https://slovakrn.wixsite.com/skrn](https://slovakrn.wixsite.com/skrn); [https://www.aus-rn.org/](https://www.aus-rn.org/)",
                "Originally drafted by": "** Suzanne L. K. Stewart",
                "Reviewed (or Edited) by": "** Annalise A. LaPlume; Sam Parsons; Flávio Azevedo",
                "Translation": "** A reproducibility network is a consortium of open research working groups, often peer-led. The groups operate on a wheel-and-spoke model across a particular country, in which the network connects local cross-disciplinary researchers, groups, and institutions with a central steering group, who also connect with external stakeholders in the research ecosystem. The goals of reproducibility networks include; advocating for greater awareness, promoting training activities, and disseminating best-practices at grassroots, institutional, and research ecosystem levels. Such networks exist in the UK, Germany, Switzerland, Slovakia, and Australia (as of March 2021)."
            },
            {
                "Title": "Research Contribution Metric (*p*)",
                "Definition": "** Type of semantometric measure assessing similarity of publications connected in a citation network. This method uses a simple formula to assess authors’ contributions. Publication *p* can be estimated based on the semantic distance from the publications cited by *p* to publications citing *p*.",
                "Reference": "** Knoth and Herrmannova (2014); Holcombe (2019); Larivière et al. (2016)",
                "Originally drafted by": "** Alaa AlDoh",
                "Reviewed (or Edited) by": "** Michele C. Lim; Jamie P. Cockcroft; Micah Vandegrift; Dominik Kiersz  ### ---  ####",
                "Translation": "** Type of semantometric measure assessing similarity of publications connected in a citation network. This method uses a simple formula to assess authors’ contributions. Publication *p* can be estimated based on the semantic distance from the publications cited by *p* to publications citing *p*.",
                "Related_terms": "** Semantometrics"
            },
            {
                "Title": "Research Cycle",
                "Definition": "** Describes the circular process of conducting scientific research, with “researchers working at various stages of inquiry, from more tentative and exploratory investigations to the testing of more definitive and well-supported claims” (Lieberman, 2020, p. 42). The cycle includes literature research and hypothesis generation, data collection and analysis, as well as dissemination of results (e.g. through publication in peer-reviewed journals), which again informs theory and new hypotheses/research.",
                "Reference(s)": "** Bramoullé and Saint Paul (2010); Lieberman (2020)",
                "Originally drafted by": "** Helena Hartmann",
                "Reviewed (or Edited) by": "** Jamie P. Cockcroft; Aleksandra Lazić; Graham Reid; Beatrice Valentini",
                "Translation": "** Describes the circular process of conducting scientific research, with “researchers working at various stages of inquiry, from more tentative and exploratory investigations to the testing of more definitive and well-supported claims” (Lieberman, 2020, p. 42). The cycle includes literature research and hypothesis generation, data collection and analysis, as well as dissemination of results (e.g. through publication in peer-reviewed journals), which again informs theory and new hypotheses/research.",
                "Related_terms": "** Research process"
            },
            {
                "Title": "Research Data Management *",
                "Definition": "** Research Data Management (RDM) is a broad concept that includes processes undertaken to create organized, documented, accessible, and reusable quality research data. Adequate research data management provides many benefits including, but not limited to, reduced likelihood of data loss, greater visibility and collaborations due to data sharing, demonstration of research integrity and accountability.",
                "Reference(s)": "** CESSDA; Corti et al. (2019)",
                "Drafted by": "** Micah Vandegrift",
                "Reviewed (or Edited) by": "** Helena Hartmann; Tina B. Lonsdorf; Catia M. Oliveira; Julia Wolska",
                "Translation": "** Research Data Management (RDM) is a broad concept that includes processes undertaken to create organized, documented, accessible, and reusable quality research data. Adequate research data management provides many benefits including, but not limited to, reduced likelihood of data loss, greater visibility and collaborations due to data sharing, demonstration of research integrity and accountability.",
                "Related_terms": "** Data curation; Data documentation; Data management plan (DMP); Data sharing; Metadata; Research data management"
            },
            {
                "Title": "Research integrity *",
                "Definition": "** Research integrity is defined by a set of good research practices based on fundamental principles: honesty, reliability, respect and accountability (ALLEA, 2017). Good research practices —which are based on fundamental principles of research integrity and should guide researchers in their work as well as in their engagement with the practical, ethical and intellectual challenges inherent in research— refer to areas such as: research environment (e.g., research institutions and organisations promote awareness and ensure a prevailing culture of research integrity), training, supervision and mentoring (e.g., Research institutions and organisations develop appropriate and adequate training in ethics and research integrity to ensure that all concerned are made aware of the relevant codes and regulations), research procedures (e.g., researchers report their results in a way that is compatible with the standards of the discipline and, where applicable, can be verified and reproduced), safeguards (e.g., researchers have due regard for the health, safety and welfare of the community, of collaborators and others connected with their research), data practices and management (e.g., researchers, research institutions and organisations provide transparency about how to access or make use of their data and research materials), collaborative working, publication and dissemination (e.g., authors and publishers consider negative results to be as valid as positive findings for publication and dissemination), reviewing, evaluating and editing (e.g., researchers review and evaluate submissions for publication, funding, appointment, promotion or reward in a transparent and justifiable manner).",
                "Reference(s)": "** ALLEA (2017); Medin (2012); Moher et al. (2020)",
                "Drafted by": "** Ana Barbosa Mendes; Flávio Azevedo",
                "Reviewed (or Edited) by": "** Valeria Agostini; Bradley Baker; Gilad Feldman; Tamara Kalandadze; Charlotte R. Pennington",
                "Translation": "** Research integrity is defined by a set of good research practices based on fundamental principles: honesty, reliability, respect and accountability (ALLEA, 2017). Good research practices —which are based on fundamental principles of research integrity and should guide researchers in their work as well as in their engagement with the practical, ethical and intellectual challenges inherent in research— refer to areas such as: research environment (e.g., research institutions and organisations promote awareness and ensure a prevailing culture of research integrity), training, supervision and mentoring (e.g., Research institutions and organisations develop appropriate and adequate training in ethics and research integrity to ensure that all concerned are made aware of the relevant codes and regulations), research procedures (e.g., researchers report their results in a way that is compatible with the standards of the discipline and, where applicable, can be verified and reproduced), safeguards (e.g., researchers have due regard for the health, safety and welfare of the community, of collaborators and others connected with their research), data practices and management (e.g., researchers, research institutions and organisations provide transparency about how to access or make use of their data and research materials), collaborative working, publication and dissemination (e.g., authors and publishers consider negative results to be as valid as positive findings for publication and dissemination), reviewing, evaluating and editing (e.g., researchers review and evaluate submissions for publication, funding, appointment, promotion or reward in a transparent and justifiable manner).",
                "Related_terms": "** Credibility of scientific claims; Error detection; Ethics; Open research; Questionable Research Practices or Questionable Reporting Practices (QRPs); Responsible Research Practices; Rigour; Transparency; Trustworthy research"
            },
            {
                "Title": "Research Protocol",
                "Definition": "** A detailed document prepared before conducting a study, often written as part of ethics and funding applications. The protocol should include information relating to the background, rationale and aims of the study, as well as hypotheses which reflect the researchers’ expectations. The protocol should also provide a “recipe” for conducting the study, including methodological details and clear analysis plans. Best practice guidelines for creating a study protocol should be used for specific methodologies and fields. It is possible to publically share research protocols to attract new collaborators or facilitate efficient collaboration across labs (e.g. [https://www.protocols.io/](https://www.protocols.io/)). In medical and educational fields, protocols are often a separate article type suitable for publication in journals. Where protocol sharing or publication is not common practice, researchers can choose preregistration.",
                "Reference": "** BMJ (2015); Nosek et al. (2018)",
                "Originally drafted by": "** Marta Topor",
                "Reviewed (or Edited) by": "** Helena Hartmann; Bethan Iley; Annalise A. LaPlume; Charlotte Pennington",
                "Translation": "** A detailed document prepared before conducting a study, often written as part of ethics and funding applications. The protocol should include information relating to the background, rationale and aims of the study, as well as hypotheses which reflect the researchers’ expectations. The protocol should also provide a “recipe” for conducting the study, including methodological details and clear analysis plans. Best practice guidelines for creating a study protocol should be used for specific methodologies and fields. It is possible to publically share research protocols to attract new collaborators or facilitate efficient collaboration across labs (e.g. [https://www.protocols.io/](https://www.protocols.io/)). In medical and educational fields, protocols are often a separate article type suitable for publication in journals. Where protocol sharing or publication is not common practice, researchers can choose preregistration.",
                "Related_terms": "** Many Labs; Preregistration"
            },
            {
                "Title": "Research workflow *",
                "Definition": "** The process of conducting research from conceptualisation to dissemination. A typical workflow may look like the following: Starting with conceptualisation to identify a research question and design a study. After study design, researchers need to gain ethical approval (if necessary) and may decide to preregister the final version. Researchers then collect and analyse their data. Finally, the process ends with dissemination; moving between pre-print and post-print stages as the manuscript is submitted to a journal.",
                "Reference(s)": "** Kathawalla et al. (2021); Stodden (2011)",
                "Drafted by": "** James E Bartlett",
                "Reviewed (or Edited) by": "** Gilad Feldman; Helena Hartmann; Aleksandra Lazić; Joanne McCuaig; Timo Roettger; Sam Parsons; Steven Verheyen",
                "Translation": "** The process of conducting research from conceptualisation to dissemination. A typical workflow may look like the following: Starting with conceptualisation to identify a research question and design a study. After study design, researchers need to gain ethical approval (if necessary) and may decide to preregister the final version. Researchers then collect and analyse their data. Finally, the process ends with dissemination; moving between pre-print and post-print stages as the manuscript is submitted to a journal.",
                "Related_terms": "** Open Research Workflow; Research cycle; Research pipeline"
            },
            {
                "Title": "Researcher degrees of freedom *",
                "Definition": "** refers to the flexibility often inherent in the scientific process, from hypothesis generation, designing and conducting a research study to processing the data and analyzing as well as interpreting and reporting results. Due to a lack of precisely defined theories and/or empirical evidence, multiple decisions are often equally justifiable. The term is sometimes used to refer to the opportunistic (ab-)use of this flexibility aiming to achieve desired results —e.g., when in- or excluding certain data— albeit the fact that technically the term is not inherently value-laden.",
                "Reference": "** Gelman and Loken (2013); Simmons et al. (2011); Wicherts et al. (2016)",
                "Originally drafted by": "** Tina Lonsdorf",
                "Reviewed (or Edited) by": "** Gilad Feldman; Helena Hartmann; Timo Roettger; Robbie C.M. van Aert; Flávio Azevedo",
                "Translation": "** refers to the flexibility often inherent in the scientific process, from hypothesis generation, designing and conducting a research study to processing the data and analyzing as well as interpreting and reporting results. Due to a lack of precisely defined theories and/or empirical evidence, multiple decisions are often equally justifiable. The term is sometimes used to refer to the opportunistic (ab-)use of this flexibility aiming to achieve desired results —e.g., when in- or excluding certain data— albeit the fact that technically the term is not inherently value-laden.",
                "Related_terms": "** Analytic Flexibility; Garden of forking paths; Model uncertainty; Multiverse analysis; *P*\\-hacking; Robustness (analyses); Specification curve analysis"
            },
            {
                "Title": "RepliCATs project *",
                "Definition": "** Collaborative Assessment for Trustworthy Science. The repliCATS project’s aim is to crowdsource predictions about the reliability and replicability of published research in eight social science fields: business research, criminology, economics, education, political science, psychology, public administration, and sociology.",
                "Reference": "** Fraser et al.(2021); [https://replicats.research.unimelb.edu.au/](https://replicats.research.unimelb.edu.au/)",
                "Originally drafted by": "** Tamara Kalandadze",
                "Reviewed (or Edited) by": "** Sarah Ashcroft-Jones; Mahmoud Elsherif; Gilad Feldman; Helena Hartmann; Charlotte R. Pennington",
                "Translation": "** Collaborative Assessment for Trustworthy Science. The repliCATS project’s aim is to crowdsource predictions about the reliability and replicability of published research in eight social science fields: business research, criminology, economics, education, political science, psychology, public administration, and sociology.",
                "Related_terms": "** Replicability; Trustworthiness"
            },
            {
                "Title": "Responsible Research and Innovation *",
                "Definition": "** An approach that considers societal implications and expectations, relating to research and innovation, with the aim to foster inclusivity and sustainability. It accounts for the fact that scientific endeavours are not isolated from their wider effects and that research is motivated by factors beyond the pursuit of knowledge. As such, many parties are important in fostering responsible research, including funding bodies, research teams, stakeholders, activists, and members of the public.",
                "Reference(s)": "** European Commission (2021)",
                "Drafted by": "** Ana Barbosa Mendes",
                "Reviewed (or Edited) by": "** Helena Hartmann; Joanne McCuaig; Sam Parsons; Graham Reid",
                "Translation": "** An approach that considers societal implications and expectations, relating to research and innovation, with the aim to foster inclusivity and sustainability. It accounts for the fact that scientific endeavours are not isolated from their wider effects and that research is motivated by factors beyond the pursuit of knowledge. As such, many parties are important in fostering responsible research, including funding bodies, research teams, stakeholders, activists, and members of the public.",
                "Related_terms": "** Citizen Science; Public Engagement; Transdisciplinary Research"
            },
            {
                "Title": "Reverse p-hacking *",
                "Definition": "** Exploiting researcher degrees of freedom during statistical analysis in order to increase the likelihood of accepting the null hypothesis (for instance, *p* \\> .05).",
                "Reference": "** Chuard et al. (2019)",
                "Originally drafted by": "** Robert M. Ross",
                "Reviewed (or Edited) by": "** Mahmoud Elsherif; Alexander Hart; Sam Parsons; Timo Roettger",
                "Translation": "** Exploiting researcher degrees of freedom during statistical analysis in order to increase the likelihood of accepting the null hypothesis (for instance, *p* \\> .05).",
                "Related_terms": "** Analytic flexibility; HARKing; P-hacking; Questionable Research Practices or Questionable Reporting Practices (QRPs); Researcher degrees of freedom; Selective reporting"
            },
            {
                "Title": "RIOT Science Club *",
                "Definition": "** The RIOT Science Club is a multi-site seminar series that raises awareness and provides training in Reproducible, Interpretable, Open & Transparent science practices. It provides regular talks, workshops and conferences, all of which are openly available and rewatchable on the respective location’s websites and Youtube.",
                "Reference": "** [http://riotscience.co.uk/](http://riotscience.co.uk/)",
                "Originally drafted by": "** Tamara Kalandadze",
                "Reviewed (or Edited) by": "** Helena Hartmann; Emma Henderson; Joanne McCuaig; Flávio Azevedo",
                "Translation": "** The RIOT Science Club is a multi-site seminar series that raises awareness and provides training in Reproducible, Interpretable, Open & Transparent science practices. It provides regular talks, workshops and conferences, all of which are openly available and rewatchable on the respective location’s websites and Youtube.",
                "Related_terms": "** Early career researchers (ECRs); Interpretability; Openness; Reproducibility; Transparency"
            },
            {
                "Title": "Robustness (analyses) *",
                "Definition": "** The persistence of support for a hypothesis under perturbations of the methodological/analytical pipeline In other words, applying different methods/analysis pipelines to examine if the same conclusion is supported under analytical different conditions.",
                "Reference(s)": "** Goodman et al. (2016) (alternative); Nosek and Errington (2020)",
                "Drafted by": "** Tina Lonsdorf; Flávio Azevedo",
                "Reviewed (or Edited) by": "** Gilad Feldman; Adrien Fillon; Helena Hartmann; Timo Roettger  ###  ### **S** {#s}",
                "Translation": "** The persistence of support for a hypothesis under perturbations of the methodological/analytical pipeline In other words, applying different methods/analysis pipelines to examine if the same conclusion is supported under analytical different conditions.",
                "Related_terms": "** Many Labs; Multiverse analysis; Sensitivity analyses; Specification Curve Analysis **Alternative definition:** “Robustness refers to the stability of experimental conclusions to variations in either baseline assumptions or experimental procedures. It is somewhat related to the concept of generalizability (also known as transportability), which refers to the persistence of an effect in settings different from and outside of an experimental framework \\[...\\] Whether a study design is similar enough to the original to be considered a replication, a “robustness test,” or some of many variations of pure replication that have been identified, particularly in the social sciences (for example, conceptual replication, pseudoreplication), is an unsettled question” (Goodman et al., 2016)."
            },
            {
                "Title": "Salami slicing *",
                "Definition": "** A questionable research/reporting practice strategy, often done *post hoc*, to increase the number of publishable manuscripts by ‘slicing’ up the data from a single study \\- one example of a method of ‘gaming the system’ of academic incentives. For instance, this may involve publishing multiple studies based on a single dataset, or publishing multiple studies from different data collection sites without transparently stating where the data originally derives from. Such practices distort the literature, and particularly meta-analyses, because it is unclear that the findings were obtained from the same dataset, thereby concealing the dependencies across the separately published papers.",
                "Reference(s)": "** Fanelli (2018)",
                "Drafted by": "** Mahmoud Elsherif",
                "Reviewed (or Edited) by": "** Sarah Ashcroft-Jones; Adrien Fillon; Helena Hartmann; Matt Jaquiery; Tamara Kalandadze; Charlotte R. Pennington; Graham Reid; Suzanne L. K. Stewart",
                "Translation": "** A questionable research/reporting practice strategy, often done *post hoc*, to increase the number of publishable manuscripts by ‘slicing’ up the data from a single study \\- one example of a method of ‘gaming the system’ of academic incentives. For instance, this may involve publishing multiple studies based on a single dataset, or publishing multiple studies from different data collection sites without transparently stating where the data originally derives from. Such practices distort the literature, and particularly meta-analyses, because it is unclear that the findings were obtained from the same dataset, thereby concealing the dependencies across the separately published papers.",
                "Related_terms": "** Gaming (the system); Questionable Research Practices or Questionable Reporting Practices (QRPs); Partial publication"
            },
            {
                "Title": "Scooping *",
                "Definition": "** The act of reporting or publishing a novel finding prior to another researcher/team. Survey-based research indicates that fear of being scooped is an important fear-related barrier for data sharing in psychology, and agent-based models suggest that competition for priority harms scientific reliability (Tiokhin et al. 2021).",
                "Reference(s)": "** Houtkoop et al. (2018); Laine (2017); Tiokhin et al. (2021)",
                "Drafted by": "** William Ngiam",
                "Reviewed (or Edited) by": "** Ashley Blake; Thomas Rhys Evans; Connor Keating; Graham Reid; Timo Roettger; Robert M. Ross; Flávio Azevedo",
                "Translation": "** The act of reporting or publishing a novel finding prior to another researcher/team. Survey-based research indicates that fear of being scooped is an important fear-related barrier for data sharing in psychology, and agent-based models suggest that competition for priority harms scientific reliability (Tiokhin et al. 2021).",
                "Related_terms": "** Novelty; Open data; Preregistration"
            },
            {
                "Title": "Semantometrics **",
                "Definition": "** A class of metrics for evaluating research using full publication text to measure semantic similarity of publications and highlighting an article’s contribution to the progress of scholarly discussion. It is an extension of tools such as bibliometrics, webometrics, and altmetrics.",
                "Reference": "** Herrmannova and Knoth (2016); Knoth and Herrmannova (2014)",
                "Originally drafted by": "** Alaa AlDoh",
                "Reviewed (or Edited) by": "** Jamie P. Cockcroft; Mahmoud Elsherif; Christopher Graham; Charlotte R. Pennington",
                "Translation": "** A class of metrics for evaluating research using full publication text to measure semantic similarity of publications and highlighting an article’s contribution to the progress of scholarly discussion. It is an extension of tools such as bibliometrics, webometrics, and altmetrics.",
                "Related_terms": "** Bibliometrics; Contribution(p)"
            },
            {
                "Title": "Sensitive research *",
                "Definition": "** Research that poses a threat to those who are or have been involved in it, including the researchers, the participants, and the wider society. This threat can be physical danger (e.g. suicide) or a negative emotional response (e.g. depression) to those who are involved in the research process. For instance, research conducted on victims of suicide, the researcher might be emotionally traumatised by the descriptions of the suicidal behaviours. Indeed, the communication with the victims might also make them re-experience the traumatic memories, leading to negative psychological responses.",
                "Reference": "** Lee (1993); Albayrak-Aydemir (2019)",
                "Originally drafted by": "** Nihan Albayrak-Aydemir",
                "Reviewed (or Edited) by": "** Valeria Agostini; Mahmoud Elsherif; Helena Hartmann; Graham Reid",
                "Translation": "** Research that poses a threat to those who are or have been involved in it, including the researchers, the participants, and the wider society. This threat can be physical danger (e.g. suicide) or a negative emotional response (e.g. depression) to those who are involved in the research process. For instance, research conducted on victims of suicide, the researcher might be emotionally traumatised by the descriptions of the suicidal behaviours. Indeed, the communication with the victims might also make them re-experience the traumatic memories, leading to negative psychological responses.",
                "Related_terms": "** Anonymity"
            },
            {
                "Title": "Sequence-determines-credit approach (SDC) **",
                "Definition": "** An authorship system that assigns authorship order based on the contribution of each author. The names of the authors are listed according to their contribution in descending order with the most contributing author first and the least contributing author last.",
                "Reference": "** Schmidt (1987); Tscharntke et al. (2007)",
                "Originally drafted by": "** Myriam A. Baum",
                "Reviewed (or Edited) by": "** Sam Parsons; Charlotte R. Pennington",
                "Translation": "** An authorship system that assigns authorship order based on the contribution of each author. The names of the authors are listed according to their contribution in descending order with the most contributing author first and the least contributing author last.",
                "Related_terms": "** Authorship; First-last-author-emphasis norm (FLAE)"
            },
            {
                "Title": "Sherpa Romeo *",
                "Definition": "** An online resource that collects and presents open access policies from publishers, from across the world, providing summaries of individual journal's copyright and open access archiving policies.",
                "Reference": "** [https://v2.sherpa.ac.uk/romeo/](https://v2.sherpa.ac.uk/romeo/)",
                "Originally drafted by": "** Aleksandra Lazić",
                "Reviewed (or Edited) by": "** Mahmoud Elsherif; Christopher Graham; Sam Parsons; Martin Vasilev",
                "Translation": "** An online resource that collects and presents open access policies from publishers, from across the world, providing summaries of individual journal's copyright and open access archiving policies.",
                "Related_terms": "** Embargo period; Open access; Paywall; Preprint; Repository"
            },
            {
                "Title": "Single-blind peer review *",
                "Definition": "** Evaluation of research products by qualified experts where the reviewer(s) knows the identity of the author(s), but the reviewer(s) remains anonymous to the author(s).",
                "Reference": "** Largent and Snodgrass (2016)",
                "Originally drafted by": "** Bradley Baker",
                "Reviewed (or Edited) by": "** Ashley Blake; Christopher Graham; Helena Hartmann; Graham Reid",
                "Translation": "** Evaluation of research products by qualified experts where the reviewer(s) knows the identity of the author(s), but the reviewer(s) remains anonymous to the author(s).",
                "Related_terms": "** Anonymous review; Double-blind peer review; Masked review; Open Peer Review; Peer review; Triple-blind peer review"
            },
            {
                "Title": "Slow science *",
                "Definition": "** Adopting Open Scholarship practices leads to a longer research process overall, with more focus on transparency, reproducibility, replicability and quality, over the quantity of outputs. Slow Science opposes publish-or-perish culture and describes an academic system that allows time and resources to produce fewer higher-quality and transparent outputs, for instance prioritising researcher time towards collecting more data, more time to read the literature, think about how their findings fit the literature and documenting and sharing research materials instead of running additional studies.",
                "Reference(s)": "** http://slow-science.org/; Nelson et al., (2012); Frith (2020)",
                "Drafted by": "** Sonia Rishi",
                "Reviewed (or Edited) by": "** Adrien Fillon; Tamara Kalandadze; Sam Parsons Charlotte R. Pennington; Robert M Ross; Timo Roettger",
                "Translation": "** Adopting Open Scholarship practices leads to a longer research process overall, with more focus on transparency, reproducibility, replicability and quality, over the quantity of outputs. Slow Science opposes publish-or-perish culture and describes an academic system that allows time and resources to produce fewer higher-quality and transparent outputs, for instance prioritising researcher time towards collecting more data, more time to read the literature, think about how their findings fit the literature and documenting and sharing research materials instead of running additional studies.",
                "Related_terms": "** collaboration; Incentive structure; Publish or Perish; research culture; research quality"
            },
            {
                "Title": "Society for Open, Reliable, and Transparent Ecology and Evolutionary biology (SORTEE) *",
                "Definition": "**SORTEE ([https://www.sortee.org/](https://www.sortee.org/)) is an international society with the aim of improving the transparency and reliability of research results in the fields of ecology, evolution, and related disciplines through cultural and institutional changes. SORTEE was launched in December 2020 to anyone interested in improving research in these disciplines, regardless of experience. The society is international in scope, membership, and objectives. As of May 2021, SORTEE comprises of over 600 members.",
                "Reference(s)": "** https://www.sortee.org/",
                "Drafted by": "** Brice Beffara Bret; Dominique Roche",
                "Reviewed (or Edited) by": "** Sarah Ashcroft-Jones; Mahmoud Elsherif; Charlotte R. Pennington; Graham Reid",
                "Translation": "**SORTEE ([https://www.sortee.org/](https://www.sortee.org/)) is an international society with the aim of improving the transparency and reliability of research results in the fields of ecology, evolution, and related disciplines through cultural and institutional changes. SORTEE was launched in December 2020 to anyone interested in improving research in these disciplines, regardless of experience. The society is international in scope, membership, and objectives. As of May 2021, SORTEE comprises of over 600 members.",
                "Related_terms": "** Society for the Improvement of Psychological Science (SIPS)"
            },
            {
                "Title": "Society for the Improvement of Psychological Science (SIPS) *",
                "Definition": "** A membership society founded to further promote improved methods and practices in the psychological research field. The society aims to complete its mission statement by enhancing the training of psychological researchers; by promoting research cultures that are more conducive to better quality research; by quantifying and empirically assessing the impact of such reforms; and by leading outreach events within and outside psychology to better the current state of research norms.",
                "Reference(s)": "** https://improvingpsych.org/",
                "Drafted by": "** Mahmoud Elsherif",
                "Reviewed (or Edited) by": "** Ashley Blake; Jade Pickering; Graham Reid; Flávio Azevedo",
                "Translation": "** A membership society founded to further promote improved methods and practices in the psychological research field. The society aims to complete its mission statement by enhancing the training of psychological researchers; by promoting research cultures that are more conducive to better quality research; by quantifying and empirically assessing the impact of such reforms; and by leading outreach events within and outside psychology to better the current state of research norms.",
                "Related_terms": "** Society for Open, Reliable, and Transparent Ecology and Evolutionary biology (SORTEE)"
            },
            {
                "Title": "Social class *",
                "Definition": "** Social class is usually measured using both objective and subjective measurements, as recommended by the American Psychological Association (American Psychological Association,Task Force on Socioeconomic Status, 2007). Unlike the conventional concept, which only considers one factor, either education or income (e.g., economic variables), an individual's social class is considered to be a combination of their education, income, occupational prestige, subjective social status, and self-identified social class. Social class is partly a cultural variable, as it is a stable variable and likely to change slowly over the years. Social class can have important implications to academic outcomes. An individual may have a high socio-economic status yet identify as a working class individual. Working class students tend to have different life circumstances and often more restrictive commitments than middle-class students, which make their integration with other students more difficult (Rubin, 2021). The lack of time and money is obstructive to their social experience at university. Working class students are more likely to work to support themselves, resulting in less time for academic activities and for socializing with other students as well as less money to purchase items linked to social experiences (e.g. food).",
                "Reference(s)": "** Evans and Rubin (2021); Rubin et al. (2019); Rubin (2021); Saegert et al. (2007)",
                "Drafted by": "** Mahmoud Elsherif",
                "Reviewed (or Edited) by": "** Leticia Micheli; Eliza Woodward; Julika Wolska; Gerald Vineyard**;** Yu-Fang Yang",
                "Translation": "** Social class is usually measured using both objective and subjective measurements, as recommended by the American Psychological Association (American Psychological Association,Task Force on Socioeconomic Status, 2007). Unlike the conventional concept, which only considers one factor, either education or income (e.g., economic variables), an individual's social class is considered to be a combination of their education, income, occupational prestige, subjective social status, and self-identified social class. Social class is partly a cultural variable, as it is a stable variable and likely to change slowly over the years. Social class can have important implications to academic outcomes. An individual may have a high socio-economic status yet identify as a working class individual. Working class students tend to have different life circumstances and often more restrictive commitments than middle-class students, which make their integration with other students more difficult (Rubin, 2021). The lack of time and money is obstructive to their social experience at university. Working class students are more likely to work to support themselves, resulting in less time for academic activities and for socializing with other students as well as less money to purchase items linked to social experiences (e.g. food).",
                "Related_terms": "** Social integration"
            },
            {
                "Title": "Social integration *",
                "Definition": "** Social integration is a multi-dimensional construct. In an academic context, social integration is related to the quantity and quality of the social interactions with staff and students, as well as the sense of connection and belonging to the university and the people within the institute. To be more specific, social support, trust, and connectedness are all variables that contribute to social integration. Social integration has important implications for academic outcomes and mental wellbeing (Evans & Rubin, 2021). Working class students are less likely to integrate with other students, since they have differing social and economic backgrounds and less disposable income. Thus they are not able to experience as many educational and fiscal opportunities than others. In turn, this can lead to poor mental health and feelings of ostracism (Rubin, 2021).",
                "Reference(s)": "** Evans and Rubin (2021); Rubin et al. (2019); Rubin (2021)",
                "Drafted by": "** Mahmoud Elsherif",
                "Reviewed (or Edited) by": "** Leticia Micheli; Eliza Woodward; Julika Wolska**;** Gerald Vineyard; Yu-Fang Yang; Flávio Azevedo",
                "Translation": "** Social integration is a multi-dimensional construct. In an academic context, social integration is related to the quantity and quality of the social interactions with staff and students, as well as the sense of connection and belonging to the university and the people within the institute. To be more specific, social support, trust, and connectedness are all variables that contribute to social integration. Social integration has important implications for academic outcomes and mental wellbeing (Evans & Rubin, 2021). Working class students are less likely to integrate with other students, since they have differing social and economic backgrounds and less disposable income. Thus they are not able to experience as many educational and fiscal opportunities than others. In turn, this can lead to poor mental health and feelings of ostracism (Rubin, 2021).",
                "Related_terms": "** Social class"
            },
            {
                "Title": "Specification Curve Analysis **",
                "Definition": "** An analytic approach that consists of identifying, calculating, visualising and interpreting results (through inferential statistics) for *all* reasonable specifications for a particular research question (see Simonsohn et al. 2015). Specification curve analysis helps make transparent the influence of presumably arbitrary decisions during the scientific progress (e.g., experimental design, construct operationalization, statistical models or several of these) made by a researcher by comprehensively reporting all non-redundant, sensible tests of the research question. Voracek et al. (2019) suggest that SCA differs from multiverse analysis with regards to the graphical displays (a specification curve plot rather than a histogram and tile plot) and the use of inferential statistics to interpret findings.",
                "Reference(s)": "** Simonsohn et al. (2015); Simonsohn (2020); Voracek et al. (2019)",
                "Drafted by": "** Bradley Baker",
                "Reviewed (or Edited) by": "** Tina B. Lonsdorf; Sam Parsons; Charlotte R. Pennington",
                "Translation": "** An analytic approach that consists of identifying, calculating, visualising and interpreting results (through inferential statistics) for *all* reasonable specifications for a particular research question (see Simonsohn et al. 2015). Specification curve analysis helps make transparent the influence of presumably arbitrary decisions during the scientific progress (e.g., experimental design, construct operationalization, statistical models or several of these) made by a researcher by comprehensively reporting all non-redundant, sensible tests of the research question. Voracek et al. (2019) suggest that SCA differs from multiverse analysis with regards to the graphical displays (a specification curve plot rather than a histogram and tile plot) and the use of inferential statistics to interpret findings.",
                "Related_terms": "** Multiverse analysis; Research synthesis; Robustness (analyses); Selective reporting; Vibration of effects"
            },
            {
                "Title": "Statistical Assumptions",
                "Definition": "** Analytical approaches and models assume certain characteristics of one’s data (e.g., statistical independence, random samples, normality, equal variance,...). Before running an analysis, these assumptions should be checked since their violation can change the results and conclusion of a study. Good practice in open and reproducible science is to report assumption testing in terms of the assumptions verified and the results of such checks or corrections applied.",
                "Reference": "** Garson (2012); Hahn and Meeker (1993); Hoekstra et al. (2012); Nimon (2012)",
                "Originally drafted by": "** Graham Reid",
                "Reviewed (or Edited) by": "** Jamie P. Cockcroft; Sam Parsons; Martin Vasilev; Julia Wolska",
                "Translation": "** Analytical approaches and models assume certain characteristics of one’s data (e.g., statistical independence, random samples, normality, equal variance,...). Before running an analysis, these assumptions should be checked since their violation can change the results and conclusion of a study. Good practice in open and reproducible science is to report assumption testing in terms of the assumptions verified and the results of such checks or corrections applied.",
                "Related_terms": "** Null Hypothesis Significance Testing (NHST); Statistical Significance; Statistical Validity; Transparency; Type I error; Type II error; Type M error; Type S error"
            },
            {
                "Title": "Statistical power *",
                "Definition": "** Statistical power is the long-run probability that a statistical test correctly rejects the null hypothesis if the alternative hypothesis is true. It ranges from 0 to 1, but is often expressed as a percentage. Power can be estimated using the significance criterion (alpha), effect size, and sample size used for a specific analysis technique. There are two main applications of statistical power. A priori power where the researcher asks the question “given an effect size, how many participants would I need for X% power?”. Sensitivity power asks the question “given a known sample size, what effect size could I detect with X% power?”.",
                "Reference(s)": "** Carter et al. (2021); Cohen (1962); Cohen (1988); Dienes (2008); Giner-Sorolla et al. (2019); Ioannidis (2005); Lakens (2021a)",
                "Drafted by": "** Thomas Rhys Evans",
                "Reviewed (or Edited) by": "** James E. Bartlett; Jamie P. Cockcroft; Adrien Fillon; Emma Henderson; Tamara Kalandadze; William Ngiam; Catia M. Oliveira; Charlotte R. Pennington; Graham Reid; Martin Vasilev; Qinyu Xiao; Flávio Azevedo",
                "Translation": "** Statistical power is the long-run probability that a statistical test correctly rejects the null hypothesis if the alternative hypothesis is true. It ranges from 0 to 1, but is often expressed as a percentage. Power can be estimated using the significance criterion (alpha), effect size, and sample size used for a specific analysis technique. There are two main applications of statistical power. A priori power where the researcher asks the question “given an effect size, how many participants would I need for X% power?”. Sensitivity power asks the question “given a known sample size, what effect size could I detect with X% power?”.",
                "Related_terms": "** Effect Size; Meta-analysis; Null Hypothesis Significance Testing (NHST); Power Analysis; Positive Predictive Value; Quantitative research; Sample size; Significance criterion (alpha); Type I error; Type II error **Related terms to alternative definition:** Type II Error"
            },
            {
                "Title": "Statistical significance",
                "Definition": "** A property of a result using Null Hypothesis Significance Testing (NHST) that, given a significance level, is deemed unlikely to have occurred given the null hypothesis. Tenny and Abdelgawad (2017) defined it as “a measure of the probability of obtaining your data or more extreme data assuming the null hypothesis is true, compared to a pre-selected acceptable level of uncertainty regarding the true answer” (p. 1). Conventions for determining the threshold vary between applications and disciplines but ultimately depend on the considerations of the researcher about an appropriate error margin. The American Statistical Association’s statement (Wasserstein & Lazar, 2016\\) notes that “Researchers often wish to turn a p-value into a statement about the truth of a null hypothesis, or about the probability that random chance produced the observed data. The p-value is neither. It is a statement about data in relation to a specified hypothetical explanation, and is not a statement about the explanation itself” (p. 131).",
                "Reference(s)": "** Cassidy et al. (2019); Tenny and Abdelgawad (2021); Wasserstein and Lazar (2016)",
                "Drafted by": "** Alaa AlDoh; Flávio Azevedo",
                "Reviewed (or Edited) by": "** James E. Bartlett; Alexander Hart**;** Annalise A. LaPlume; Charlotte R. Pennington; Graham Reid; Timo Roettger; Suzanne L. K. Stewart",
                "Translation": "** A property of a result using Null Hypothesis Significance Testing (NHST) that, given a significance level, is deemed unlikely to have occurred given the null hypothesis. Tenny and Abdelgawad (2017) defined it as “a measure of the probability of obtaining your data or more extreme data assuming the null hypothesis is true, compared to a pre-selected acceptable level of uncertainty regarding the true answer” (p. 1). Conventions for determining the threshold vary between applications and disciplines but ultimately depend on the considerations of the researcher about an appropriate error margin. The American Statistical Association’s statement (Wasserstein & Lazar, 2016\\) notes that “Researchers often wish to turn a p-value into a statement about the truth of a null hypothesis, or about the probability that random chance produced the observed data. The p-value is neither. It is a statement about data in relation to a specified hypothetical explanation, and is not a statement about the explanation itself” (p. 131).",
                "Related_terms": "** Alpha error; Frequentist statistics; Null hypothesis; Null Hypothesis Significance Testing (NHST); *P*\\-value; Type I error **Incorrect definition:** Statistical significance describes the likelihood of the observed result against chance (regardless of the null hypotheses)"
            },
            {
                "Title": "Statistical validity **",
                "Definition": "** The extent to which conclusions from a statistical test are accurate and reflective of the true effect found in nature. In other words, whether or not a relationship exists between two variables and can be accurately detected with the conducted analyses. Threats to statistical validity include low power, violation of assumptions, reliability of measures, etc, affecting the reliability and generality of the conclusions.",
                "Reference(s)": "** Cook and Campbell (1979); Drost (2011)",
                "Drafted by": "** Annalise A. LaPlume",
                "Reviewed (or Edited) by": "** Jamie P. Cockcroft, Zoltan Kekecs; Graham Reid",
                "Translation": "** The extent to which conclusions from a statistical test are accurate and reflective of the true effect found in nature. In other words, whether or not a relationship exists between two variables and can be accurately detected with the conducted analyses. Threats to statistical validity include low power, violation of assumptions, reliability of measures, etc, affecting the reliability and generality of the conclusions.",
                "Related_terms": "** Power; Validity; Statistical assumptions"
            },
            {
                "Title": "STRANGE",
                "Definition": "** The STRANGE “framework” is a proposal and series of questions to help animal behaviour researchers consider sampling biases when planning, performing and interpreting research with animals. STRANGE is an acronym highlighting several possible sources of sampling bias in animal research, such as the animals’ Social background; Trappability and self-selection; Rearing history; Acclimation and habituation; Natural changes in responsiveness; Genetic make-up, and Experience.",
                "Reference(s)": "** Webster and Rutz (2020)",
                "Drafted by": "** Mahmoud Elsherif",
                "Reviewed (or Edited) by": "** Ben Farrar; Zoe Flack; Elias Garcia-Pelegrin; Charlotte R. Pennington; Graham Reid  ### ---",
                "Translation": "** The STRANGE “framework” is a proposal and series of questions to help animal behaviour researchers consider sampling biases when planning, performing and interpreting research with animals. STRANGE is an acronym highlighting several possible sources of sampling bias in animal research, such as the animals’ Social background; Trappability and self-selection; Rearing history; Acclimation and habituation; Natural changes in responsiveness; Genetic make-up, and Experience.",
                "Related_terms": "** Bias; Constraints on Generality (COG); Populations; Sampling bias; WEIRD"
            },
            {
                "Title": "StudySwap *",
                "Definition": "** A free online platform through which researchers post brief descriptions of research projects or resources that are available for use (“haves”) or that they require and another researcher may have (“needs”). StudySwap is a crowdsourcing approach to research which can ensure that fewer research resources go unused and more researchers have access to the resources they need.",
                "Reference": "** Chartier et al. (2018); [https://osf.io/view/StudySwap](https://osf.io/view/StudySwap)",
                "Originally drafted by": "** Charlotte R. Pennington",
                "Reviewed (or Edited) by": "** Mahmoud Elsherif; Helena Hartmann; Emma Henderson; Graham Reid",
                "Translation": "** A free online platform through which researchers post brief descriptions of research projects or resources that are available for use (“haves”) or that they require and another researcher may have (“needs”). StudySwap is a crowdsourcing approach to research which can ensure that fewer research resources go unused and more researchers have access to the resources they need.",
                "Related_terms": "** Collaboration; Crowdsourcing; Team science"
            },
            {
                "Title": "Systematic Review",
                "Definition": "** A form of literature review and evidence synthesis. A systematic review will usually include a thorough, repeatable (reproducible) search strategy including key terms and databases in order to find relevant literature on a given topic or research question. Systematic reviewers follow a process of screening the papers found through their search, until they have filtered down to a set of papers that fit their predefined inclusion criteria. These papers can then be synthesised in a written review which may optionally include statistical synthesis in the form of a meta-analysis as well. A systematic review should follow a standard set of guidelines to ensure that bias is kept to a minimum for example PRISMA (Moher et al., 2009; Page et al., 2021), Cochrane Systematic Reviews (Higgins et al., 2019), or NIRO-SR (Topor et al., 2021).",
                "Reference": "** Higgins et al. (2019); Moher et al. (2009); Page et al. (2021); Topor et al. (2021)",
                "Originally drafted by": "** Jade Pickering",
                "Reviewed (or Edited) by": "** Mahmoud Elsherif; Adam Parker; Charlotte R. Pennington; Timo Roettger; Marta Topor; Emily A. Williams; Flávio Azevedo  ### ---  ###  ###  ### **T** {#t}",
                "Translation": "** A form of literature review and evidence synthesis. A systematic review will usually include a thorough, repeatable (reproducible) search strategy including key terms and databases in order to find relevant literature on a given topic or research question. Systematic reviewers follow a process of screening the papers found through their search, until they have filtered down to a set of papers that fit their predefined inclusion criteria. These papers can then be synthesised in a written review which may optionally include statistical synthesis in the form of a meta-analysis as well. A systematic review should follow a standard set of guidelines to ensure that bias is kept to a minimum for example PRISMA (Moher et al., 2009; Page et al., 2021), Cochrane Systematic Reviews (Higgins et al., 2019), or NIRO-SR (Topor et al., 2021).",
                "Related_terms": "** Meta-analysis; CONSORT; Non-Intervention, Reproducible, and Open Systematic Reviews (NIRO-SR); PRISMA"
            },
            {
                "Title": "Tenzing",
                "Definition": "** *tenzing* is an online webapp and R package that helps researchers to track and report the contributions of each team member using the CRediT taxonomy in an efficient way. Team members of a research project can indicate their contributions to each CRediT role using an online spreadsheet template, and provide any additional authors' information (e.g., name, affiliation, order in publication, email address, and ORCID iD). Upon writing the manuscript, *tenzing* can automatically create a list of contributors belonging to each CRediT role to be included in the contributions section and create the manuscript’s title page.",
                "Reference(s)": "** Holcombe et al. (2020)",
                "Drafted by": "** Marton Kovacs",
                "Reviewed (or Edited) by": "** Balazs Aczel; Mahmoud Elsherif; Helena Hartmann; Charlotte R. Pennington; Graham Reid; Flávio Azevedo",
                "Translation": "** *tenzing* is an online webapp and R package that helps researchers to track and report the contributions of each team member using the CRediT taxonomy in an efficient way. Team members of a research project can indicate their contributions to each CRediT role using an online spreadsheet template, and provide any additional authors' information (e.g., name, affiliation, order in publication, email address, and ORCID iD). Upon writing the manuscript, *tenzing* can automatically create a list of contributors belonging to each CRediT role to be included in the contributions section and create the manuscript’s title page.",
                "Related_terms": "** Authorship; Consortium authorship; Contributions; CRediT"
            },
            {
                "Title": "Theory **",
                "Definition": "** A theory is a unifying explanation or description of a process or phenomenon, which is amenable to repeated testing and verifiable through scientific investigation, using various experiments led by several independent researchers. Theories may be rejected or deemed an unsatisfactory explanation of a phenomenon after rigorous testing of a new hypothesis that explains the phenomena better or seems to contradict them but is more generalisable to a wider array of findings.",
                "Reference(s)": "** Schafersman (1997); Wacker (1998)",
                "Drafted by": "** Aoife O’Mahony",
                "Reviewed (or Edited) by": "** Mahmoud Elsherif; Helena Hartmann; Charlotte R. Pennington; Graham Reid",
                "Translation": "** A theory is a unifying explanation or description of a process or phenomenon, which is amenable to repeated testing and verifiable through scientific investigation, using various experiments led by several independent researchers. Theories may be rejected or deemed an unsatisfactory explanation of a phenomenon after rigorous testing of a new hypothesis that explains the phenomena better or seems to contradict them but is more generalisable to a wider array of findings.",
                "Related_terms": "** Hypothesis; Model (philosophy); Theory building"
            },
            {
                "Title": "Theory building **",
                "Definition": "** The process of creating and developing a statement of concepts and their interrelationships to show how and/or why a phenomenon occurs. Theory building leads to theory testing.",
                "Reference(s)": "** Borsboom et al. (2020); Corley and Gioia (2011); Gioia and Pitrie (1990); Wacker (1998)",
                "Drafted by": "** Filip Dechterenko",
                "Reviewed (or Edited) by": "** Mahmoud Elsherif; Helena Hartmann; Charlotte R. Pennington",
                "Translation": "** The process of creating and developing a statement of concepts and their interrelationships to show how and/or why a phenomenon occurs. Theory building leads to theory testing.",
                "Related_terms": "** Hypothesis; Model (philosophy); Theory; Theoretical contribution; Theoretical model"
            },
            {
                "Title": "The Troubling Trio",
                "Definition": "** Described as a combination of low statistical power, a surprising result, and a *p*\\-value only slightly lower than .05.",
                "Reference(s)": "** Lindsay (2015)",
                "Drafted by": "** Halil Emre Kocalar",
                "Reviewed (or Edited) by": "**; Catia M. Oliveira; Adam Parker; Sam Parsons;Charlotte R. Pennington",
                "Translation": "** Described as a combination of low statistical power, a surprising result, and a *p*\\-value only slightly lower than .05.",
                "Related_terms": "** Replication; Reproducibility; Null Hypothesis Significance Testing (NHST); *P*\\-hacking; Questionable Research Practices or Questionable Reporting Practices (QRPs)"
            },
            {
                "Title": "Transparency *",
                "Definition": "** Having one’s actions open and accessible for external evaluation. Transparency pertains to researchers being honest about theoretical, methodological, and analytical decisions made throughout the research cycle. Transparency can be usefully differentiated into “scientifically relevant transparency” and “socially relevant transparency”. While the former has been the focus of early Open Science discourses, the latter is needed to provide scientific information in ways that are relevant to decision makers and members of the public (Elliott & Resnik, 2019).",
                "Reference(s)": "** Elliott and Resnik (2019); Lyon (2016); [Syed (2019)](https://psyarxiv.com/cteyb/)",
                "Drafted by": "** William Ngiam",
                "Reviewed (or Edited) by": "** Tamara Kalandadze; Aoife O’Mahony; Eike Mark Rinke; Flávio Azevedo",
                "Translation": "** Having one’s actions open and accessible for external evaluation. Transparency pertains to researchers being honest about theoretical, methodological, and analytical decisions made throughout the research cycle. Transparency can be usefully differentiated into “scientifically relevant transparency” and “socially relevant transparency”. While the former has been the focus of early Open Science discourses, the latter is needed to provide scientific information in ways that are relevant to decision makers and members of the public (Elliott & Resnik, 2019).",
                "Related_terms": "** Credibility of scientific claims; Open science; Preregistration; Reproducibility; Trustworthiness"
            },
            {
                "Title": "Transparency Checklist *",
                "Definition": "** The transparency checklist is a consensus-based, comprehensive checklist that contains 36 items that cover the prepregistration, methods, results and discussion and data, code and materials availability. A shortened 12-item version of the checklist is also available. Checklist responses can be submitted alongside a manuscript for review. While the checklist can also work for educational purposes, it mainly aims to support researchers to identify concrete actions that can increase the transparency of their research while a disclosed checklist can help the readers and reviewers gain critical information about different aspects of transparency of the submitted research.",
                "Reference(s)": "** Aczel et. al. (2021)",
                "Drafted by": "** Barnabas Szaszi",
                "Reviewed (or Edited) by": "** Sarah Ashcroft-Jones; Mahmoud Elsherif; Helena Hartmann; Graham Reid; Flávio Azevedo",
                "Translation": "** The transparency checklist is a consensus-based, comprehensive checklist that contains 36 items that cover the prepregistration, methods, results and discussion and data, code and materials availability. A shortened 12-item version of the checklist is also available. Checklist responses can be submitted alongside a manuscript for review. While the checklist can also work for educational purposes, it mainly aims to support researchers to identify concrete actions that can increase the transparency of their research while a disclosed checklist can help the readers and reviewers gain critical information about different aspects of transparency of the submitted research.",
                "Related_terms": "** Credibility of scientific claims; Open science; Preregistration; Reproducibility; Trustworthiness"
            },
            {
                "Title": "Triple-blind peer review *",
                "Definition": "** Evaluation of research products by qualified experts where the author(s) are anonymous to both the reviewer(s) and editor(s). **“**Blinding of the authors and their affiliations to both editors and reviewers. This approach aims to eliminate institutional, personal, and gender biases” (Tvina et al., 2019, p. 1082).",
                "Reference(s)": "** Largent and Snodgrass (2016); Tvina et al. (2019)",
                "Drafted by": "** Mahmoud Elsherif",
                "Reviewed (or Edited) by": "** Bradley Baker; Helena Hartmann; Charlotte R. Pennington; Christopher Graham",
                "Translation": "** Evaluation of research products by qualified experts where the author(s) are anonymous to both the reviewer(s) and editor(s). **“**Blinding of the authors and their affiliations to both editors and reviewers. This approach aims to eliminate institutional, personal, and gender biases” (Tvina et al., 2019, p. 1082).",
                "Related_terms": "** Double-blind peer review; Open Peer Review; Single-blind peer review"
            },
            {
                "Title": "TRUST Principles *",
                "Definition": "** A set of guiding principles that consider Transparency, Responsibility, User focus, Sustainability, and Technology (TRUST) as the essential components for assessing, developing, and sustaining the trustworthiness of digital data repositories (especially those that store research data). They are complementary to the FAIR Data Principles.",
                "Reference": "** Lin et al. (2020)",
                "Originally drafted by": "** Aleksandra Lazić",
                "Reviewed (or Edited) by": "** Jamie P. Cockcroft; Mahmoud Elsherif; Helena Hartmann; Sam Parsons",
                "Translation": "** A set of guiding principles that consider Transparency, Responsibility, User focus, Sustainability, and Technology (TRUST) as the essential components for assessing, developing, and sustaining the trustworthiness of digital data repositories (especially those that store research data). They are complementary to the FAIR Data Principles.",
                "Related_terms": "** FAIR principles; Metadata; Open Access; Open Data; Open Material; Repository"
            },
            {
                "Title": "Type I error *",
                "Definition": "** “Incorrect rejection of a null hypothesis” (Simmons et al., 2011, p. 1359), i.e. finding evidence to reject the null hypothesis that there is no effect when the evidence is actually in favouring of retaining the null that there is no effect (For example, a judge imprisoning an innocent person). Concluding that there is a significant effect and rejecting the null hypothesis when your findings actually occured by chance.",
                "Reference": "** Simmons et al., (2011)",
                "Originally drafted by": "** Lisa Spitzer",
                "Reviewed (or Edited) by": "** Mahmoud Elsherif; Adrien Fillon; Helena Hartmann; Matt Jaquiery; Mariella Paul; Charlotte R. Pennington; Graham Reid; Olly Robertson; Mirela Zaneva",
                "Translation": "** “Incorrect rejection of a null hypothesis” (Simmons et al., 2011, p. 1359), i.e. finding evidence to reject the null hypothesis that there is no effect when the evidence is actually in favouring of retaining the null that there is no effect (For example, a judge imprisoning an innocent person). Concluding that there is a significant effect and rejecting the null hypothesis when your findings actually occured by chance.",
                "Related_terms": "** Frequentist statistics; Null Hypothesis Significance Testing (NHST); Null Result; *P* value; Questionable Research Practices or Questionable Reporting Practices (QRPs); Reproducibility crisis (aka Replicability or replication crisis); Scientific integrity; Statistical power; True positive result; Type II error"
            },
            {
                "Title": "Type II error *",
                "Definition": "** A false negative result occurs when the alternative hypothesis is true in the population but the null hypothesis is accepted as part of the analysis (Hartgerink et al., 2017). That is, finding a non-significant statistical result when the effect is true (For example, a judge passing an innocent verdict on a guilty person). False negatives are less likely to be the subject of replications than positive results (Fiedler et al., 2012), and remain an unresolved issue in scientific research (Hartgerink et al., 2017).",
                "Reference(s)": "** Fiedler et al. (2012); Hartgerink et al. (2017)",
                "Originally drafted by": "** Olly Robertson",
                "Reviewed (or Edited) by": "** Mahmoud Elsherif; Charlotte R. Pennington",
                "Translation": "** A false negative result occurs when the alternative hypothesis is true in the population but the null hypothesis is accepted as part of the analysis (Hartgerink et al., 2017). That is, finding a non-significant statistical result when the effect is true (For example, a judge passing an innocent verdict on a guilty person). False negatives are less likely to be the subject of replications than positive results (Fiedler et al., 2012), and remain an unresolved issue in scientific research (Hartgerink et al., 2017).",
                "Related_terms": "** Effect size; Null Hypothesis Significance Testing (NHST); Questionable Research Practices or Questionable Reporting Practices (QRPs); Reproducibility crisis (aka Replicability or replication crisis); Scientific integrity; Statistical power; True positive result; Type I error"
            },
            {
                "Title": "Type M error *",
                "Definition": "** A Type M error occurs when a researcher concludes that an effect was observed with magnitude lower or higher than the real one. For example, a type M error occurs when a researcher claims that an effect of small magnitude was observed when it is large in truth or vice versa.",
                "Reference(s)": "** Gelman and Carlin (2014); Lu et al.(2018)",
                "Originally drafted by": "** Eduardo Garcia-Garzon",
                "Reviewed (or Edited) by": "** Mahmoud Elsherif; Helena Hartmann; Graham Reid; Mirela Zaneva  ### ---",
                "Translation": "** A Type M error occurs when a researcher concludes that an effect was observed with magnitude lower or higher than the real one. For example, a type M error occurs when a researcher claims that an effect of small magnitude was observed when it is large in truth or vice versa.",
                "Related_terms": "** Statistical power; Type S error; Type I error; Type II error"
            },
            {
                "Title": "Type S error *",
                "Definition": "** A Type S error occurs when a researcher concludes that an effect was observed with an opposite sign than real one. For example, a type S error occurs when a researcher claims that a positive effect was observed when it is negative in reality or vice versa.",
                "Reference(s)": "** Gelman and Carlin (2014); Lu et al. (2018)",
                "Originally drafted by": "** Eduardo Garcia-Garzon",
                "Reviewed (or Edited) by": "** Helena Hartmann; Sam Parsons; Graham Reid; Mirela Zaneva   ### **U** {#u}",
                "Translation": "** A Type S error occurs when a researcher concludes that an effect was observed with an opposite sign than real one. For example, a type S error occurs when a researcher claims that a positive effect was observed when it is negative in reality or vice versa.",
                "Related_terms": "** Statistical power; Type M error; Type I error; Type II error"
            },
            {
                "Title": "Under-representation *",
                "Definition": "** Not all voices, perspectives, and members of the community are adequately represented. Under-representation typically occurs when the voices or perspectives of one group dominate, resulting in the marginalization of another. This often affects groups who are a minority in relation to certain personal characteristics.",
                "Drafted by": "** Madeleine Pownall",
                "Reviewed (or Edited) by": "** Mahmoud Elsherif; Helena Hartmann; Bethan Iley; Adam Parker; Charlotte R. Pennington, Mirela Zaneva",
                "Translation": "** Not all voices, perspectives, and members of the community are adequately represented. Under-representation typically occurs when the voices or perspectives of one group dominate, resulting in the marginalization of another. This often affects groups who are a minority in relation to certain personal characteristics.",
                "Related_terms": "** Equity; Fairness; Inequality; WEIRD"
            },
            {
                "Title": "Universal design for learning (UDL) *",
                "Definition": "** A framework for improving learning and optimising teaching based upon scientific insights of how humans learn. It aims to make learning inclusive and transformative for all people in which the focus is on catering to the differing needs of different students. It is often regarded as an evidence-based and scientifically valid framework to guide educational practice, consisting of three key principles: engagement, representation, and action and expression. In addition, UDL is included in the Higher Education Opportunity Act of 2008 (Edyburn, 2010).",
                "Reference(s)": "** Hitchcock et al. (2002); Rose (2000); Rose and Meyer (2002)",
                "Drafted by": "** Charlotte R. Pennington",
                "Reviewed (or Edited) by": "** Valeria Agostini; Mahmoud Elsherif; Graham Reid; Mirela Zaneva; Flávio Azevedo  ### ---  ### **V** {#v}",
                "Translation": "** A framework for improving learning and optimising teaching based upon scientific insights of how humans learn. It aims to make learning inclusive and transformative for all people in which the focus is on catering to the differing needs of different students. It is often regarded as an evidence-based and scientifically valid framework to guide educational practice, consisting of three key principles: engagement, representation, and action and expression. In addition, UDL is included in the Higher Education Opportunity Act of 2008 (Edyburn, 2010).",
                "Related_terms": "** Equal opportunities; Inclusivity; Pedagogy; Teaching practice"
            },
            {
                "Title": "Validity *",
                "Definition": "** Validity refers to the application of statistical principles to arrive at well-founded —i.e., likely corresponding accurately to the real world— concepts, conclusions or measurement. In psychometrics, validity refers to the extent to which something measures what it intends to or claims to measure. Under this generic term, there are different types of validity (e.g., internal validity, construct validity, face validity, criterion validity, diagnostic validity, discriminant validity, concurrent validity, convergent validity, predictive validity, external validity).",
                "Reference(s)": "** Campbell (1957); Boorsboom et al. (2004); Kelley (1927)",
                "Drafted by": "** Tamara Kalandadze; Madeleine Pownall; Flávio Azevedo",
                "Reviewed (or Edited) by": "** Eduardo Garcia-Garzon; Halil E. Kocalar; Annalise A. LaPlume; Joanne McCuaig; Adam Parker; Charlotte R. Pennington",
                "Translation": "** Validity refers to the application of statistical principles to arrive at well-founded —i.e., likely corresponding accurately to the real world— concepts, conclusions or measurement. In psychometrics, validity refers to the extent to which something measures what it intends to or claims to measure. Under this generic term, there are different types of validity (e.g., internal validity, construct validity, face validity, criterion validity, diagnostic validity, discriminant validity, concurrent validity, convergent validity, predictive validity, external validity).",
                "Related_terms": "** Causality; Construct validity; Content validity; Criterion validity; External validity; Face validity; Internal validity; Measurement; Questionable Measurement Practices (QMP); Psychometry; Reliability; Statistical power; Statistical validity; Test"
            },
            {
                "Title": "Version control *",
                "Definition": "** The practice of managing and recording changes to digital resources (e.g. files, websites, programmes, etc.) over time so that you can recall specific versions later. Version control systems are designed to record the history of changes (who, what and when), and help to avoid human errors (e.g. working on the wrong version). For example, the Git version control system is a widely used software tool that originally helped software developers to version control shared code and is now used across many scientific disciplines to manage and share files.",
                "Reference": "** [https://git-scm.com/book/en/v2/Getting-Started-About-Version-Control](https://git-scm.com/book/en/v2/Getting-Started-About-Version-Control)",
                "Drafted by": "** Mahmoud Elsherif",
                "Reviewed (or Edited) by": "** Sarah Ashcroft-Jones; Thomas Rhys Evans; Helena Hartmann; Matt Jaquiery; Adam Parker; Charlotte R. Pennington; Robert M. Ross; Timo Roettger; Andrew J. Stewart  ###  ### **W** {#w}",
                "Translation": "** The practice of managing and recording changes to digital resources (e.g. files, websites, programmes, etc.) over time so that you can recall specific versions later. Version control systems are designed to record the history of changes (who, what and when), and help to avoid human errors (e.g. working on the wrong version). For example, the Git version control system is a widely used software tool that originally helped software developers to version control shared code and is now used across many scientific disciplines to manage and share files.",
                "Related_terms": "** Git; Reproducibility; Software configuration management; Source code management; Source control"
            },
            {
                "Title": "Webometrics **",
                "Definition": "** Webometrics involves the study of online content. Webometrics focuses on the numbers and types of hyperlinks between different online sites. Such approaches have been considered as a type of altmetrics. “The study of the quantitative aspects of the construction and use of information resources, structures and technologies on the Web drawing on [bibliometric](https://en.wikipedia.org/wiki/Bibliometrics) and [informetric](https://en.wikipedia.org/wiki/Informetrics) approaches” (Björneborn & Ingwersen, 2004).",
                "Reference(s)": "** Björneborn and Ingwersen (2004)",
                "Drafted by": "** Charlotte R. Pennington",
                "Reviewed (or Edited) by": "** Christopher Graham; Mirela Zaneva",
                "Translation": "** Webometrics involves the study of online content. Webometrics focuses on the numbers and types of hyperlinks between different online sites. Such approaches have been considered as a type of altmetrics. “The study of the quantitative aspects of the construction and use of information resources, structures and technologies on the Web drawing on [bibliometric](https://en.wikipedia.org/wiki/Bibliometrics) and [informetric](https://en.wikipedia.org/wiki/Informetrics) approaches” (Björneborn & Ingwersen, 2004).",
                "Related_terms": "** Altmetrics; Bibliometrics"
            },
            {
                "Title": "WEIRD *",
                "Definition": "**",
                "Reference(s)": "**",
                "Drafted by": "**",
                "Reviewed (or Edited) by": "**  ###  ### **Z** {#z}",
                "Translation": "**",
                "Related_terms": "** **Alternative definition:** (if applicable) **Related terms to alternative definition:** (if applicable)"
            },
            {
                "Title": "Z-Curve *",
                "Definition": "** Computing a Z-score is a statistical approach mainly used to obtain the ‘Estimated Replication Rate’ (ERR) and ‘Expected Discovery Rate’ (EDR) for a set of reported studies. Calculating a *z*\\-curve for a set of statistically significant studies involves converting reported *p*\\-values to *z*\\-scores, fitting a finite mixture model to the distribution of *z*\\-scores, and estimating mean power based on the mixture model. The Z-curve analysis can be performed in R through a dedicated package \\- https://cran.r-project.org/web/packages/zcurve/index.html.",
                "Reference(s)": "** Bartoš and Schimmack (2020); Brunner and Schimmack (2020)",
                "Drafted by": "** Bradley J. Baker",
                "Reviewed (or Edited) by": "** Kamil Izydorczak; Sam Parsons; Charlotte R. Pennington; Mirela Zaneva",
                "Translation": "** Computing a Z-score is a statistical approach mainly used to obtain the ‘Estimated Replication Rate’ (ERR) and ‘Expected Discovery Rate’ (EDR) for a set of reported studies. Calculating a *z*\\-curve for a set of statistically significant studies involves converting reported *p*\\-values to *z*\\-scores, fitting a finite mixture model to the distribution of *z*\\-scores, and estimating mean power based on the mixture model. The Z-curve analysis can be performed in R through a dedicated package \\- https://cran.r-project.org/web/packages/zcurve/index.html.",
                "Related_terms": "** Altmetrics; File drawer ratio; P-curve; P-hacking; Replication; Statistical power"
            },
            {
                "Title": "Zenodo **",
                "Definition": "** An open science repository where researchers can deposit research papers, reports, data sets, research software, and any other research-related digital artifacts. Zenodo creates a persistent digital object identifier (DOI) for each submission to make it citable. This platform was developed under the European OpenAIRE program and operated by CERN.",
                "Reference": "** www.zenodo.org",
                "Originally drafted by": "** Ali H. Al-Hoorie",
                "Reviewed (or Edited) by": "** Sara Middleton  ####  #  {#heading}  # **References** {#references}  Abele-Brehm, A. E., Gollwitzer, M., Steinberg, U., & Schönbrodt, F. D. (2019). Attitudes toward open science and public data sharing. *Social Psychology, 50*, 252-260. [https://doi.org/10.1027/1864-9335/a000384](https://doi.org/10.1027/1864-9335/a000384) Aczel, B., Szaszi, B., Nilsonne, G., Van den Akker, O., Albers, C. J., van Assen, M. A. L. M., … Wagenmakers, E. (2021, April 21). Guidance for Multi-Analyst Studies. [https://doi.org/10.31222/osf.io/5ecnh](https://doi.org/10.31222/osf.io/5ecnh) Aczel, B., Szaszi, B., Sarafoglou, A., Kekecs, Z., Kucharský, Š., Benjamin, D., ... & Wagenmakers, E. J. (2020). A consensus-based transparency checklist. *Nature Human Behaviour, 4*(1), 4-6. [https://doi.org/10.1038/s41562-019-0772-6](https://doi.org/10.1038/s41562-019-0772-6) Albayrak, N. (2018a). Diversity helps but decolonisation is the key to equality in higher education. [https://lsepgcertcitl.wordpress.com/2018/04/16/diversity-helps-but-decolonisation-is-the-key-to-equality-in-higher-education/](https://lsepgcertcitl.wordpress.com/2018/04/16/diversity-helps-but-decolonisation-is-the-key-to-equality-in-higher-education/) Albayrak, N. (2018b). Academics’ role on the future of higher education: Important but unrecognised. [https://lsepgcertcitl.wordpress.com/2018/11/29/academics-role-on-the-future-of-higher-education-important-but-unrecognised/](https://lsepgcertcitl.wordpress.com/2018/11/29/academics-role-on-the-future-of-higher-education-important-but-unrecognised/) Albayrak, N., & Okoroji, C. (2019). Facing the challenges of postgraduate study as a minority student. *A Guide for Psychology Postgraduates*, 63\\. Albayrak-Aydemir, N. (2020). The hidden costs of being a scholar from the global south. *Higher Education Across Borders (LSE Blog).* [https://blogs.lse.ac.uk/highereducation/2020/02/20/the-hidden-costs-of-being-a-scholar-from-the-global-south/](https://blogs.lse.ac.uk/highereducation/2020/02/20/the-hidden-costs-of-being-a-scholar-from-the-global-south/) ALLEA \\- All European Academies (2017). The European Code of Conduct for Research Integrity. Revised Edition. Available at: [https://allea.org/code-of-conduct/](https://allea.org/code-of-conduct/) Allen, L., & McGonagle-O’Connell, A. (n.d.). *CRediT – Contributor Roles Taxonomy.* CASRAI. [https://casrai.org/credit/](https://casrai.org/credit/) Anderson, M.S., Ronning, E.A., Devries, R., & Martinson, B.C. (2010). Extending the Mertonian norms: Scientists’ subscription to norms of research. *Journal of Higher Education, 81*(3), 366–393. https://doi.org/10.1353/jhe.0.0095. Andersson, N. (2018). Participatory research—a modernizing science for primary health care. *Journal of General and Family Medicine*, *19*(5): 154–159. https://doi.org/10.1002/jgf2.187 Angrist, J. D., & Pischke, J. S. (2010). The credibility revolution in empirical economics: How better research design is taking the con out of econometrics. *Journal of Economic Perspectives, 24*, 3-30. https://doi.org/10.1257/jep.24.2.3. Anon (n.d.). *About CC Licenses.* Creative Commons. [https://creativecommons.org/about/cclicenses/](https://creativecommons.org/about/cclicenses/) Anon. (n.d.). *Ckan*. [https://ckan.org/](https://ckan.org/) Anon. (2006). Correction or retraction?. *Nature, 444*, 123–124. [https://doi.org/10.1038/444123b](https://doi.org/10.1038/444123b) Anon (n.d.). *Datacite Metadata Schema.* Datacite Schema. https://schema.datacite.org/ Anon. (n.d.). Domov | SKRN (Slovak Reproducibility network). SKRN. https://slovakrn.wixsite.com/skrn Anon (n.d.). *Home | re3data.org*. Registry of Research Data Repositories. Retrieved 6 June 2021, from [https://www.re3data.org/](https://www.re3data.org/) Anon (n.d.). *INVOLVE – INVOLVE Supporting public involvement in NHS, public health and social care research*. INVOLVE. [https://www.invo.org.uk/](https://www.invo.org.uk/) Anon. (n.d.). *Licenses & Standards | Open Source Initiative*. OpenSource.Com. https://opensource.org/licenses Anon (n.d.). *Open Source in Open Science | FOSTER*. Foster. [https://www.fosteropenscience.eu/foster-taxonomy/open-source-open-science](https://www.fosteropenscience.eu/foster-taxonomy/open-source-open-science) Anon. (2019). *The DOI Handbook.* DOI. [https://www.doi.org/hb.html](https://www.doi.org/hb.html) Anon (n.d.). *Welcome to Sherpa Romeo \\- v2.sherpa*. Sherpa Romeo. [https://v2.sherpa.ac.uk/romeo/](https://v2.sherpa.ac.uk/romeo/) Anon (n.d.). *What is a codebook?* ICPSR. [https://www.icpsr.umich.edu/icpsrweb/content/shared/ICPSR/faqs/what-is-a-codebook.html](https://www.icpsr.umich.edu/icpsrweb/content/shared/ICPSR/faqs/what-is-a-codebook.html) Anon.(2009-2020). *What is a digital object identifier, or DOI?* American Psychological Association. h[ttps://apastyle.apa.org/learn/faqs/what-is-doi](https://apastyle.apa.org/learn/faqs/what-is-doi) Anon. (n.d.). *What is a reporting guideline?* Equator Network. https://www.equator-network.org/about-us/what-is-a-reporting-guideline/ Anon (2021). *What is impact?* The Economic and Social Research Council. [https://esrc.ukri.org/research/impact-toolkit/what-is-impact/](https://esrc.ukri.org/research/impact-toolkit/what-is-impact/) Anon. (n.d.). *What is open education?* Opensource.Com. [https://opensource.com/resources/what-open-education](https://opensource.com/resources/what-open-education) Arslan, R. C. (2019). How to Automatically Document Data With the codebook Package to Facilitate Data Reuse. *Advances in Methods and Practices in Psychological Science, 2*(2), 169–187. https://doi.org/10.1177/2515245919838783 Arts and Humanities Research Council. (n.d.). *Definition of eligibility for funding*. Arts and Humanities Research Council. Available at: https://ahrc.ukri.org/skills/earlycareerresearchers/definitionofeligibility/ Aspers, P., & Corte, U. (2019). What is qualitative in qualitative research. *Qualitative Sociology, 42*(2), 139-160. https://doi.org/10.1007/s11133-019-9413-7 AusRN. (n.d.). *Australian Reproducibility Network.* Retrieved 5 June 2021, from [https://www.aus-rn.org/](https://www.aus-rn.org/) Banks, G. C., Rogelberg, S. G., Woznyj, H. M., Landis, R. S., & Rupp, D. E. (2016). Editorial: Evidence on questionable research practices: The good, the bad, and the ugly. *Journal of Business and Psychology, 31*(3), 323–338. https://doi.org/10.1007/s10869-016-9456-7 Barba, L. A. (2018). Terminologies for reproducible research. arXiv preprint arXiv:1802.03311. Bardsley, N. (2018) What lessons does the “replication crisis” in psychology hold for experimental economics? In: *Handbook of Psychology and Economic Behaviour. 2nd edition. Cambridge Handbooks in Psychology.* Cambridge University Press. ISBN 9781107161399 Available at http://centaur.reading.ac.uk/69874/ Bartoš, F., & Schimmack, U. (2020). Z-Curve 2.0: Estimating replication rates and discovery rates. [https://doi.org/10.31234/osf.io/urgtn](https://doi.org/10.31234/osf.io/urgtn) Bateman, I., Kahneman, D., Munro, A., Starmer, C., & Sugden, R. (2005). Testing competing models of loss aversion: An adversarial collaboration. *Journal of Public Economics, 89*(8), 1561-1580. https://doi.org/10.1016/j.jpubeco.2004.06.013 Baturay, M. H. (2015). An overview of the world of MOOCs. *Procedia-Social and Behavioral Sciences, 174,* 427-433. https://doi.org/10.1016/j.sbspro.2015.01.685 Bazeley, P. (2003). Defining 'Early Career' in Research. *Higher Education 45*, 257–279 [https://doi.org/10.1023/A:1022698529612](https://doi.org/10.1023/A:1022698529612) Beffara Bret, B., Beffara Bret, A., & Nalborczyk, L. (2021). A fully automated, transparent, reproducible, and blind protocol for sequential analyses. Meta-Psychology, 5\\. https://doi.org/10.15626/MP.2018.869  Behrens, J. T. (1997). Principles and procedures of exploratory data analysis. *Psychological Methods, 2*(2), 131-160. https://doi.org/10.1037/1082-989X.2.2.131 Beller, S., & Bender, A. (2017). Theory, the final frontier? A corpus-based analysis of the role of theory in psychological articles. *Frontiers in Psychology, 8*, 951\\. [https://doi.org/10.3389/fpsyg.2017.00951](https://doi.org/10.3389/fpsyg.2017.00951) Benoit, K., Conway, D., Lauderdale, B. E., Laver, M., & Mikhaylov, S. (2016). Crowd-sourced text analysis: Reproducible and agile production of political data. A*merican Political Science Review, 110*(2), 278–295. https://doi.org/10.1017/S0003055416000058 Bhopal, R., Rankin, J., McColl, E., Thomas, L., Kaner, E., Stacy, R., Pearson, P., Vernon, B., & Rodgers, H. (1997). The vexed question of authorship: views of researchers in a British medical faculty. *BMJ, 314,* 1009-1012. https://doi.org/10.1136/bmj.314.7086.1009 BIDS (n.d.). *Modality agnostic files.* Brain Imaging Data Structure. [https://bids-specification.readthedocs.io/en/stable/03-modality-agnostic-files.html](https://bids-specification.readthedocs.io/en/stable/03-modality-agnostic-files.html) BIDS. (2020). *About BIDS*. Brain Imaging Data Structure. [https://bids.neuroimaging.io](https://bids.neuroimaging.io/) Bilder, G. (2013). *DOIs unambiguously and persistently identify published, trustworthy, citable online scholarly literature. Right?* Crossref. [https://www.crossref.org/blog/dois-unambiguously-and-persistently-identify-published-trustworthy-citable-online-scholarly-literature-right/](https://www.crossref.org/blog/dois-unambiguously-and-persistently-identify-published-trustworthy-citable-online-scholarly-literature-right/) Bishop, D. V. (2020). The psychology of experimental psychologists: Overcoming cognitive constraints to improve research: The 47th Sir Frederic Bartlett Lecture. *Quarterly Journal of Experimental Psychology, 73*(1), 1-19. [https://doi.org/10.1177/1747021819886519](https://doi.org/10.1177/1747021819886519) Björneborn, L., & Ingwersen, P. (2004). Toward a basic framework for webometrics. *Journal of the American society for information science and technology, 55*(14), 1216-1227.https://doi.org/10.1002/asi.20077 Blohowiak, B. B., Cohoon, J., de-Wit, L., Eich, E., Farach, F. J., Hasselman, F., … Riss, C. (2020, July 4). Badges to Acknowledge Open Practices. Retrieved from osf.io/tvyxz BMJ. (2015). *Introducing ‘How to write and publish a Study Protocol’ using BMJ’s new eLearning programme: Research to Publication.* Retrieved, March 2021, from: https://blogs.bmj.com/bmjopen/2015/09/22/introducing-how-to-write-and-publish-a-study-protocol-using-bmjs-new-elearning-programme-research-to-publication/ Boivin, A., Richards, T., Forsythe, L., Gregoire, A., L’Esperance, A., Abelson, J., & Carman, K.L. (2018). Evaluating the patient and public involvement in research. *British Medical Journal, 363*, k5147. https://doi.org/10.1136/bmj.k5147 Bol, T., de Vaan, M., & van de Rijt, A. (2018). The Matthew effect in science funding. *Proceedings of the National Academy of Sciences, 115*(19), 4887-4890. https://doi.org/10.1073/pnas.1719557115 Bollen, K. A. (1989). *Structural Equations with Latent Variables* (pp. 179-225). John Wiley & Sons. Borenstein, M., Hedges, L. V., Higgins, J. P., & Rothstein, H. R. (2011). *Introduction to meta-analysis.* John Wiley & Sons. Bornmann, L., Ganser, C., Tekles, A., & Leydesdorff, L. (2019). Does the $ h\\_\\\\alpha $ index reinforce the Matthew effect in science? Agent-based simulations using Stata and R. *arXiv preprint arXiv:1905.11052.* Borsboom, D., Mellenbergh, G. J., & Van Heerden, J. (2004). The concept of validity. *Psychological review, 111*(4), 1061\\. [https://doi.org/10.1037/0033-295X.111.4.1061](https://doi.org/10.1037/0033-295X.111.4.1061) Borsboom, D., van der Maas, H., Dalege, J., Kievit, R., & Haig, B. (2020, February 29). Theory Construction Methodology: A practical framework for theory formation in psychology. https://doi.org/10.31234/osf.io/w5tp8 Bourne, P. E., Polka, J. K., Vale, R. D., & Kiley, R. (2017). Ten simple rules to consider regarding preprint submission.*PLoS Computational Biology, 13*(5), e1005473. [https://doi.org/10.1371/journal.pcbi.1005473](https://doi.org/10.1371/journal.pcbi.1005473) Box, G.E. P. (1976). Science and statistics. J*ournal of the American Statistical Association 71*(356), 791–799. Bouvy, J. C., & Mujoomdar, M. (2019). All-Male Panels and Gender Diversity of Issue Panels and Plenary Sessions at ISPOR Europe. *PharmacoEconomics-open*, *3*(3), 419-422. https://doi.org/10.1007/s41669-019-0153-0 Bramoullé, Y., & Saint-Paul, G. (2010). Research cycles. *Journal of economic theory, 145*(5), 1890-1920. [https://doi.org/10.2139/ssrn.965816](https://doi.org/10.2139/ssrn.965816) Brand, A., Allen, L., Altman, M., Hlava, M., & Scott, J. (2015). Beyond authorship: attribution, contribution, collaboration, and credit. *Learned Publishing, 28*(2), 151-155. [https://doi.org/10.1087/20150211](https://doi.org/10.1087/20150211) Brandt, M. J., IJzerman, H., Dijksterhuis, A., Farach, F. J., Geller, J., Giner-Sorolla, R., ... & Van't Veer, A. (2014). The replication recipe: What makes for a convincing replication?. *Journal of Experimental Social Psychology, 50*, 217-224. https://doi.org/10.1016/j.jesp.2013.10.005 Braun, V., & Clarke, V. (2013) *Successful Qualitative Research.* SAGE Publications. Brembs, B., Button, K., & Munafò, M. (2013). Deep impact: unintended consequences of journal rank. *Frontiers in Human Neuroscience, 7*, 291\\.[https://doi.org/10.3389/fnhum.2013.00291](https://doi.org/10.3389/fnhum.2013.00291) Breznau, N. (2021). I saw you in the crowd: Credibility, reproducibility, and meta-utility. *PS: Political Science & Politics, 54*(2), 309-313. [https://doi.org/10.1017/S1049096520000980](https://doi.org/10.1017/S1049096520000980) Brod, M., Tesler, L., & Christensen, T. (2009). Qualitative research and content validity: Developing best practices based on science and experience. *Quality of Life Research, 18*(9), 1263–1278. [https://doi.org/10.1007/s11136-009-9540-9](https://doi.org/10.1007/s11136-009-9540-9) Brooks, T. A. (1985). Private acts and public objects: An investigation of citer motivations. *Journal of the American Society for Information Science, 36*(4), 223-229. https://doi.org/10.1002/asi.4630360402 Brunner, J., & Schimmack, U. (2020). Estimating population mean power under conditions of heterogeneity and selection for significance. *Meta-Psychology, 4*, MP.2018.874. https://doi.org/1[0.15626/MP.2018.874](https://doi.org/10.15626/MP.2018.874) Bruns, S. B., & Ioannidis, J. P. (2016). P-curve and p-hacking in observational research. *PLoS ONE, 11*(2), e0149144. https://doi.org/10.1371/journal.pone.0149144 Budapest Open Access Initiative (2002) *Read the Budapest open access initiative.* Budapest, Hungary. Available from: [https://www.budapestopenaccessinitiative.org/read](https://www.budapestopenaccessinitiative.org/read) Burnette, M., Williams, S., & Imker, H. (2016). From Plan to Action: Successful Data Management Plan Implementation in a Multidisciplinary Project. *Journal of eScience librarianship, 5*(1), e1101. https://doi.org/10.7191/jeslib.2016.1101 Button, K. S., Chambers, C. D., Lawrence, N., & Munafò, M. R. (2020). Grassroots training for reproducible science: a consortium-based approach to the empirical dissertation. *Psychology Learning & Teaching, 19*(1), 77-90. https://doi.org/10.1177/1475725719857659 Button, K. S., Lawrence, N. S., Chambers, C. D., & Munafò, M. R. (2016). Instilling scientific rigour at the grassroots. *Psychologist, 29*(3), 158-159. Byrne J. A. & Christopher J. (2020). Digital magic, or the dark arts of the 21st century—how can journals and peer reviewers detect manuscripts and publications from paper mills? *FEBS Lett, 594*(4), 583-589. https://doi.org/10.1002/1873-3468.13747 Campbell, D. T., & Stanley, J.C. (1966) *Experimental and Quasi Experimental Designs.* Rand McNally. Carp, J. (2012). On the plurality of (methodological) worlds: estimating the analytic flexibility of FMRI experiments. *Frontiers in Neuroscience, 6*, 149\\. https://doi.org/10.3389/fnins.2012.00149 Carsey, T. M. (2014). Making DA-RT a reality. *PS: Political Science & Politics, 47*(1), 72–77. [https://doi.org/10.1017/S1049096513001753](https://doi.org/10.1017/S1049096513001753) Carter, A., Tilling, K., & Munafo, M. R. (2021, January 26). Considerations of sample size and power calculations given a range of analytical scenarios. https://doi.org/10.31234/osf.io/tcqrn Case, C. M. (1928). Scholarship in Sociology. *Sociology and Social Research, 12*, 323–340. http://www.sudoc.fr/036493414 Cassidy, S. A., Dimova, R., Giguère, B., Spence, J. R., & Stanley, D. J. (2019). Failing grade: 89% of introduction-to-psychology textbooks that define or explain statistical significance do so incorrectly. *Advances in Methods and Practices in Psychological Science, 2*(3), 233-239. https://doi.org/10.1177/2515245919858072 Centre for Evaluation. (n.d.). *Evidence Synthesis.* https://www.lshtm.ac.uk/research/centres/centre-evaluation/evidence-synthesis Centre for Open Science. (2011-2021) *Open Science Framework*. Centre for Open Science. [https://osf.io/](https://osf.io/) Centre for Open Science. (n.d.). S*how Your Work. Share Your Work.* Advance Science. That's Open Science. The Centre for Open Science. https://www.cos.io/ CESSDA Training Team (2017 \\- 2020). C*ESSDA Data Management Expert Guide. B*ergen, Norway: CESSDA ERIC. Retrieved from [https://www.cessda.eu/DMGuide](https://www.cessda.eu/DMGuide) Chambers, C. D. (2013). Registered reports: a new publishing initiative at Cortex. C*ortex, 49*(3), 609-610. https://doi.org/10.1016/j.cortex.2012.12.016. Chambers, C. D., Dienes, Z., McIntosh, R. D., Rotshtein, P., & Willmes, K. (2015). Registered reports: realigning incentives in scientific publishing. *Cortex, 66*, A1-A2. https://doi.org/10.1016/j.cortex.2015.03.022. Chambers, C. D., & Tzavella, L. (2020, February 10). Registered Reports: Past, Present and Future. [https://doi.org/10.31222/osf.io/43298](https://doi.org/10.31222/osf.io/43298) Chartier, C. R., Riegelman, A., & McCarthy, R. J. (2018). StudySwap: A platform for interlab replication, collaboration, and resource exchange. *Advances in Methods and Practices in Psychological Science, 1*(4), 574-579. [https://doi.org/10.1177/2515245918808767](https://doi.org/10.1177/2515245918808767) Chuard, P. J. C., Vrtilek, M., Head, M. L., & Jennions, M. D. (2019). Evidence that non-significant results are sometimes preferred: Reverse P-hacking or selective reporting? *PLoS Biol 17*(1), e3000127. https://doi.org/10.1371/journal.pbio.3000127 Citizen Science Association (2015). *Who We Are.* Citizen Science. [https://www.citizenscience.org/about-3/](https://www.citizenscience.org/about-3/) Claerbout, J. F., & Karrenbach, M. (1992). Electronic documents give reproducible research a new meaning. In *SEG Technical Program Expanded Abstracts 1992* (pp. 601-604). Society of Exploration Geophysicists. Available at [http://sepwww.stanford.edu/doku.php?id=sep:research:reproducible:seg92](http://sepwww.stanford.edu/doku.php?id=sep:research:reproducible:seg92) Clark, H., Elsherif, M. M., & Leavens, D. A. (2019). Ontogeny vs. phylogeny in primate/canid comparisons: a meta-analysis of the object choice task. *Neuroscience & Biobehavioral Reviews, 105,* 178-189. https://doi.org/10.1016/j.neubiorev.2019.06.001 Cohen, J. (1962). The statistical power of abnormal-social psychological research: A review. *The Journal of Abnormal and Social Psychology, 65*(3), 145–153. https://doi.org/10.1037/h0045186 Cohen, J. (1969). *Statistical power analysis for the behavioral sciences.* Academic Press. Cohn, J. P. (2008). Citizen science: Can volunteers do real research?. *BioScience, 58*(3), 192-197. [https://doi.org/10.1641/B580303](https://doi.org/10.1641/B580303) Coles, N. A.; Tiokhin, L.; Arslan, R.; Forscher, P.; Scheel, A.; & Lakens, D. (2020, May 11). *Red Team Challenge.* [http://daniellakens.blogspot.com/2020/05/red-team-challenge.html](http://daniellakens.blogspot.com/2020/05/red-team-challenge.html) Committee on Reproducibility and Replicability in Science, Board on Behavioral, Cognitive, and Sensory Sciences, Committee on National Statistics, Division of Behavioral and Social Sciences and Education, Nuclear and Radiation Studies Board, Division on Earth and Life Studies, … National Academies of Sciences, Engineering, and Medicine. (2019). Reproducibility and Replicability in Science (p. 25303). National Academies Press. https://doi.org/10.17226/25303 Cook, T. D., & Campbell, D. T. (1979). *Quasi-Experimentation.* Rand McNally. Coproduction Collective (2021). *Our approach.* [https://www.coproductioncollective.co.uk/what-is-co-production/our-approach](https://www.coproductioncollective.co.uk/what-is-co-production/our-approach) Corley, K. G., & Gioia, D. A. (2011). Building theory about theory building: what constitutes a theoretical contribution?. *Academy of management review, 36*(1), 12-32. [https://doi.org/10.5465/amr.2009.0486](https://doi.org/10.5465/amr.2009.0486) Cornell University (2020). *Measuring your research impact: i10 index.* Cornell University Library. https://guides.library.cornell.edu/impact/author-impact-10 Corti, L., Van den Eynden, V., Bishop, L., & Woollard, M. (2019). *Managing and sharing research data: a guide to good practice.* Sage. Cowan, N., Belletier, C., Doherty, J. M., Jaroslawska, A. J., Rhodes, S., Forsberg, A., ... & Logie, R. H. (2020). How do scientific views change? Notes from an extended adversarial collaboration. *Perspectives on Psychological Science, 15*(4), 1011-1025. [https://doi.org/10.1177/1745691620906415](https://doi.org/10.1177/1745691620906415) Crenshaw, K. W. (1989). Demarginalizing the Intersection of Race and Sex: A Black Feminist Critique of Antidiscrimination Doctrine. *University of Chicago Legal Forum, 1989* (8), 139–168. Cronbach, L. J., & Meehl, P. E. (1955). Construct validity in psychological tests. P*sychological Bulletin, 52*(4), 281–302. [https://doi.org/10.1037/h0040957](https://psycnet.apa.org/doi/10.1037/h0040957) Cronin, B. (2001). Hyperauthorship: A postmodern perversion or evidence of a structural shift in scholarly communication practices? *Journal of the American Society for Information Science and Technology*, 52(7), 558–569. [https://doi.org/10.1002/asi.1097](https://doi.org/10.1002/asi.1097) Crowdsourcing Week. (2021, April 29). *What is Crowdsourcing?* [https://crowdsourcingweek.com/what-is-crowdsourcing/](https://crowdsourcingweek.com/what-is-crowdsourcing/) Crutzen, R., Ygram Peters, G. J., & Mondschein, C. (2019). Why and how we should care about the General Data Protection Regulation. *Psychology & health*, *34*(11), 1347-1357. https://doi.org/10.1080/08870446.2019.1606222 Crüwell, S., van Doorn, J., Etz, A., Makel, M. C., Moshontz, H., Niebaum, J. C., Orben, A., Parsons, S., & Schulte-Mecklenbeck, M. (2019). Seven Easy Steps to Open Science: An Annotated Reading List. Zeitschrift Für Psychologie, 227(4), 237–248. [https://doi.org/10.1027/2151-2604/a000387](https://doi.org/10.1027/2151-2604/a000387) Curry, S. (2012) *Sick of impact factors.* \\[blogpost\\] [http://occamstypewriter.org/scurry/2012/08/13/sick-of-impact-factors/](http://occamstypewriter.org/scurry/2012/08/13/sick-of-impact-factors/) d’Espagnat, B. (2008). Is science cumulative? A physicist viewpoint. In *Rethinking Scientific Change and Theory Comparison* (pp. 145-151). Springer, Dordrecht. [https://doi.org/10.1007/978-1-4020-6279-7\\_10](https://doi.org/10.1007/978-1-4020-6279-7_10) Davies, G. M., & Gray, A. (2015). Don’t let spurious accusations of pseudoreplication limit our ability to learn from natural experiments (and other messy kinds of ecological monitoring). *Ecology and Evolution, 5*(22), 5295–5304. [https://doi.org/10.1002/ece3.1782](https://doi.org/10.1002/ece3.1782) Del Giudice, M., & Gangestad, S. W. (2021). A traveler’s guide to the multiverse: Promises, pitfalls, and a framework for the evaluation of analytic decisions. *Advances in Methods and Practices in Psychological Science, 4*(1), 2515245920954925\\. [https://doi.org/10.1177/2515245920954925](https://doi.org/10.1177/2515245920954925) Der Kiureghian, A., & Ditlevsen, O. (2009). Aleatory or epistemic? Does it matter?. *Structural Safety, 31*(2), 105-112. [https://doi.org/10.1016/j.strusafe.2008.06.020](https://doi.org/10.1016/j.strusafe.2008.06.020) DeVellis, R. F. (2017). *Scale development: Theory and applications* (4th ed.). Sage. Devito, N., & Goldacre, B. (2019). Publication bias. Catalogue Of Bias [https://catalogofbias.org/biases/publication-bias/](https://catalogofbias.org/biases/publication-bias/) Dickersin, K., & Min, Y. (1993). Publication Bias: The problem that wont go away. *Annals New York Academy of Sciences, 703*(1), 135-148. [https://doi.org/10.1111/j.1749-6632.1993.tb26343.x](https://doi.org/10.1111/j.1749-6632.1993.tb26343.x) Dienes, Z. (2011). Bayesian versus orthodox statistics: Which side are you on?. *Perspectives on Psychological Science, 6*(3), 274-290.https://doi.org/10.1177/1745691611406920 Dienes, Z. (2014). Using Bayes to get the most out of non-significant results. *Frontiers in psychology, 5*, 781\\. https://doi.org/10.3389/fpsyg.2014.00781 Dienes, Z. (2016). How Bayes factors change scientific practice. *Journal of Mathematical Psychology, 72*, 78-89. https://doi.org/10.1016/j.jmp.2015.10.003 Doll, R., & Hill, A. B. (1954). The mortality of doctors in relation to their smoking habits; a preliminary report. *British Medical Journal, 1* (4877), 1451–1455. doi:10.1136/bmj.1.4877.1451 Drost, E. A. (2011). Validity and reliability in social science research. E*ducation Research and Perspectives, 38*(1), 105-123. Du Bois, W.E.B. (1968). *The souls of black folk; essays and sketches.* Chicago, A.G. McClurg, 1903\\. New York: Johnson Reprint Corp. Duval, S., & Tweedie, R. (2000a). A nonparametric “trim and fill” method of accounting for publication bias in meta-analysis. *Journal of the American Statistical Association, 95*, 89–98. [https://doi.org/10.2307/2669529](https://doi.org/10.2307/2669529) Duval, S., & Tweedie, R. (2000b). Trim and fill: A simple funnel-plot–based method of testing and adjusting for publication bias in meta-analysis. *Biometrics, 56*, 455–463. https://doi.org/10.1111/j.0006-341x.2000.00455.x. Eagly, A. H., & Riger, S. (2014). Feminism and psychology: Critiques of methods and epistemology. *American Psychologist, 69*(7), 685–702. [https://doi.org/10.1037/a0037372](https://doi.org/10.1037/a0037372) Easterbrook, S. M. (2014). Open code for open science? *Nature Geoscience, 7,* 779-781. [https://doi.org/10.1038/ngeo2283](https://doi.org/10.1038/ngeo2283) Ebersole, C. R., Atherton, O. E., Belanger, A. L., Skulborstad, H. M., Allen, J. M., Banks, J. B., … Nosek, B. A. (2016). Many Labs 3: Evaluating participant pool quality across the academic semester via replication. *Journal of Experimental Social Psychology, 67,* 68–82. [https://doi.org/10.1016/j.jesp.2015.10.012](https://doi.org/10.1016/j.jesp.2015.10.012) Edyburn, D. L. (2010). Would you recognize universal design for learning if you saw it? Ten propositions for new directions for the second decade of UDL. L*earning Disability Quarterly, 33*(1), 33-41. https://doi.org/10.1177/073194871003300103 Ellemers, N. (2021). Science as collaborative knowledge generation. *British Journal of Social Psychology, 60* (1), 1-28.https://doi.org/10.1111/bjso.12430 Eley, A. R. (2012). *Becoming a successful early career researcher.* Routledge. [http://www.worldcat.org/oclc/934369360](http://www.worldcat.org/oclc/934369360) f Elliott, K. C., & Resnik, D. B. (2019). Making open science work for science and society. *Environmental Health Perspectives, 127*(7). [https://doi.org/10.1289/EHP4808](https://doi.org/10.1289/EHP4808) Esterling, K., Brady, D., & Schwitzgebel, E. (2021, January 27). *The Necessity of Construct and External Validity for Generalized Causal Claims.* [https://doi.org/10.31219/osf.io/2s8w5](https://doi.org/10.31219/osf.io/2s8w5) European Commission (2021, January 17th). *Responsible research & innovation.* Horizon 2020\\. [https://ec.europa.eu/programmes/horizon2020/en/h2020-section/responsible-research-innovation](https://ec.europa.eu/programmes/horizon2020/en/h2020-section/responsible-research-innovation) F. (2019, December 13). *Introducing a Framework for Open and Reproducible Research Training (FORRT).* https://doi.org/10.31219/osf.io/bnh7p Fanelli, D. (2010). Do Pressures to Publish Increase Scientists' Bias? An Empirical Support from US States Data. *PLOS ONE. 5* (4), e10271. https://doi.org/10.1371/journal.pone.0010271 Fanelli, D. (2018). Opinion: Is science really facing a reproducibility crisis, and do we need it to?. *Proceedings of the National Academy of Sciences, 115*(11), 2628-2631. [https://doi.org/10.1073/pnas.1708272114](https://doi.org/10.1073/pnas.1708272114) Farrow, R. (2017). Open education and critical pedagogy. *Learning, Media and Technology*, *42*(2), 130-146. [https://doi.org/10.1080/17439884.2016.1113991](https://doi.org/10.1080/17439884.2016.1113991) Faul, F., Erdfelder, E., Lang, A.-G., & Buchner, A. (2007). G\\*Power 3: A flexible statistical power analysis program for the social, behavioral, and biomedical sciences. B*ehavior Research Methods, 39*, 175-191. https://doi.org/10.3758/BF03193146 Faul, F., Erdfelder, E., Buchner, A., & Lang, A.-G. (2009). Statistical power analyses using G\\*Power 3.1: Tests for correlation and regression analyses. *Behavior Research Methods, 41*, 1149-1160. [https://doi.org/10.3758/BRM.41.4.1149](https://doi.org/10.3758/BRM.41.4.1149) Ferson, S., Joslyn, C. A., Helton, J. C., Oberkampf, W. L., & Sentz, K. (2004). Summary from the epistemic uncertainty workshop: consensus amid diversity. *Reliability Engineering & System Safety, 85*(1-3), 355-369. [https://doi.org/10.1016/j.ress.2004.03.023](https://doi.org/10.1016/j.ress.2004.03.023) Fiedler K., Kutzner F., Krueger J. I.. (2012). The long way from α-error control to validity proper: Problems with a short-sighted false-positive debate. Perspectives on *Psychological Science, 7*(6), 661-669. https://doi.org/ 10.1177/1745691612462587. Fiedler, K., & Schwarz, N. (2016). Questionable research practices revisited. Social *Psychological and Personality Science, 7*(1), 45–52. https://doi.org/10.1177/1948550615612150 Filipe, A., Renedo, A., & Marston, C. (2017). The co-production of what? Knowledge, values, and social relations in health care. *PLoS biology, 15*(5), e2001403. https://doi.org/10.1371/journal.pbio.2001403 Fillon, A.A., Feldman, G., Yeung, S. K., Protzko, J., Elsherif, M. M., Xiao, Q., Nanakdewa, K. & Brick, C. (2021). *Correlational Meta-Analysis Registered Report Template.* \\[Manuscript in preparation\\]. Findley, M. G., Jensen, N. M., Malesky, E. J., & Pepinsky, T. B. (2016). Can results-free review reduce publication bias? The results and implications of a pilot study. *Comparative Political Studies, 49*(13), 1667–1703. [https://doi.org/10.1177/0010414016655539](https://doi.org/10.1177/0010414016655539) Finlay, L., & Gough, B. (Eds.). (2008). *Reflexivity: A practical guide for researchers in health and social sciences.* John Wiley & Sons. Flake, J. K., & Fried, E. I. (2020). Measurement schmeasurement: Questionable measurement practices and how to avoid them. *Advances in Methods and Practices in Psychological Science, 3*(4), 456-465. [https://doi.org/10.1177%2F2515245920952393](https://doi.org/10.1177%2F2515245920952393) Fletcher-Watson, S., Adams, J., Brook, K., Charman, T., Crane, L., Cusack, J., Leekam, S., Milton, D., Parr, J. R., & Pellicano, E. (2019). Making the future together: Shaping autism research through meaningful participation. *Autism*, *23*(4), 943–953 FORRT. (2021). *Welcome to FORRT.* Framework for Open and Reproducible Research Training. https://forrt.org Foster, E. D., & Deardorff, A. (2017). Open science framework (OSF). *Journal of the Medical Library Association: JMLA, 105*(2), 203\\. https://doi.org/ 10.5195/jmla.2017.88 Franco, A., Malhotra, N., & Simonovits, G. (2014). Publication bias in the social sciences: \tUnlocking the file drawer. *Science, 345*(6203), 1502-1505. [https://doi.org/10.1126/science.1255484](https://doi.org/10.1126/science.1255484) Frank, M.C., Bergelson, E., Bergmann, C., Cristia, A., Floccia, C., Gervain, J., Hamlin, J.K., Hannon, E.E., Kline, M., Levelt, C., Lew-Williams, C., Nazzi, T., Panneton, R., Rabagliati, H., Soderstrom, M., Sullivan, J., Waxman, S. and Yurovsky, D. (2017). A Collaborative Approach to Infant Research: Promoting Reproducibility, Best Practices, and Theory-Building. *Infancy, 22,* 421-435. [https://doi.org/10.1111/infa.12182](https://doi.org/10.1111/infa.12182) Franzoni, C., & Sauermann, H. (2014). Crowd science: The organization of scientific research in open collaborative projects. *Research Policy, 43*(1), 1–20. https://doi.org/10.1016/j.respol.2013.07.005 Fraser, H., Bush, M., Wintle, B., Mody, F., Smith, E., Hanea, A., ... & Fidler, F. (2021). *Predicting reliability through structured expert elicitation with repliCATS* (Collaborative Assessments for Trustworthy Science). Free Our Knowledge. (n.d.). *About*. Free Our Knowledge. https://freeourknowledge.org/about/. Frith, U. (2020). Fast lane to slow science. *Trends in Cognitive Sciences, 24*(1), 1-2.https://doi.org/10.1016/j.tics.2019.10.007 Galligan, F., & Dyas-Correia, S. (2013). Altmetrics: rethinking the way we measure. *Serials Review, 39*(1), 56-61. https://doi.org/10.1016/j.serrev.2013.01.003 Gelman, A., & Loken, E. (2013). The garden of forking paths: Why multiple comparisons can be a problem, even when there is no “fishing expedition” or “p-hacking” and the research hypothesis was posited ahead of time. *Department of Statistics, Columbia University, 348\\.* http://www.stat.columbia.edu/\\~gelman/research/unpublished/p\\_hacking.pdf Gelman, A., & Carlin, J. (2014). Beyond Power Calculations: Assessing Type S (Sign) and Type M (Magnitude) Errors. *Perspectives on Psychological Science, 9*(6), 641-651. https://doi.org/ 10.1177/1745691614551642 Gentleman, R. (2005). Reproducible Research: A Bioinformatics Case Study. *Statistical Applications in Genetics and Molecular Biology, 4*, 1034\\. https://doi.org/10.2202/1544-6115.1034 German Research Foundation (2019). Guidelines for Safeguarding Good Research Practice. Code of Conduct. [http://doi.org/10.5281/zenodo.3923602](http://doi.org/10.5281/zenodo.3923602) Gilroy, P. (1993). *The black Atlantic: Modernity and double consciousness*. New York: Harvard University Press. Giner-Sorolla, R., Aberson, C. L., Bostyn, D. H., Carpenter, T., Conrique, B. G., Lewis, N. A., & Soderberg, C. (2019). Power to detect what? Considerations for planning and evaluating sample size \\[Preprint\\]. https://osf.io/jnmya/ Ginsparg, P. (1997). Winners and losers in the global research village, *The Serials Librarian, 30*(3-4), 83-95. https://doi.org/10.1300/J123v30n03\\_13 Ginsparg, P. (2001). Creating a global knowledge network. In *Second Joint ICSU Press-UNESCO Expert Conference on Electronic Publishing in Scienc*e (pp. 19-23). Gioia, D. A., & Pitre, E. (1990). Multiparadigm perspectives on theory building. Academy of management review, 15(4), 584-602. https://doi.org/10.5465/amr.1990.4310758 Glass, D. J., & Hall, N. (2008). A brief history of the hypothesis. *Cell, 134*(3), 378-381. https://doi.org/10.1016/j.cell.2008.07.033 Goertzen, M.J. (2017). *Introduction to Quantitative Research and Data.* Library Technology Reports. 53(4), 12–18. Gollwitzer, M., Abele-Brehm, A., Fiebach, C., Ramthun, R., Scheel, A. M., Schönbrodt, F. D., & Steinberg, U. (2020, September 10). Data Management and Data Sharing in Psychological Science: Revision of the DGPs Recommendations. https://doi.org/10.31234/osf.io/24ncs Goodman, S. N., Fanelli, D., & Ioannidis, J. P. A. (2016). What does research reproducibility mean? *Science Translational Medicine, 8*(341), 341ps12-341ps12. [https://doi.org/10.1126/scitranslmed.aaf5027](https://doi.org/10.1126/scitranslmed.aaf5027) Goodman, S. W., & Pepinsky, T. B. (2019). Gender Representation and Strategies for Panel Diversity: Lessons from the APSA Annual Meeting. *PS: Political Science & Politics*, *52*(4), 669-676. https://doi.org/10.1017/S1049096519000908 Gorgolewski, K., Auer, T., Calhoun, V. et al. (2016). The brain imaging data structure, a format for organizing and describing outputs of neuroimaging experiments. S*cientific Data, 3,* 160044\\. [https://doi.org/10.1038/sdata.2016.44](https://doi.org/10.1038/sdata.2016.44) Graham, I. D., McCutcheon, C., & Kothari, A. (2019). Exploring the frontiers of research co-production: the Integrated Knowledge Translation Research Network concept papers. *Health Research Policy and Systems, 17*, 88\\. [https://doi.org/10.1186/s12961-019-0501-7](https://doi.org/10.1186/s12961-019-0501-7) GRN Â· German Reproducibility Network. (n.d.). *A German Reproducibility Network.* Retrieved 5 June 2021, from https://reproducibilitynetwork.de/ Grossmann, A., & Brembs, B. (2021). Current market rates for scholarly publishing services. *F1000Research, 10*(20), 20\\. https://doi.org/10.12688/f1000research.27468.1 Grzanka, P. R. (2020). From buzzword to critical psychology: An invitation to take intersectionality seriously. *Women & Therapy, 43*(3-4), 244-261. Guest, O. \\[@o\\_guest\\]. (2017, June 5). *Thanks\\! Hopefully this thread & many other similar discussions & blogs will help make it less Bropen Science and more Open Science. \\*hides\\** \\[Tweet\\]. Twitter. https://twitter.com/o\\_guest/status/871675631062458368 Guest, O., & Martin, A. E. (2020). How computational modeling can force theory building in psychological science. *Perspectives on Psychological Science.* https://doi.org/10.1177/1745691620970585 Haak, L. L., Fenner, M., Paglione, L., Pentz, E., & Ratner, H. (2012). ORCID: A system to uniquely identify researchers. *Learned Publishing, 25*(4), 259-264. doi:10.1087/20120404 Hackett, R., & Kelly, S. (2020). Publishing ethics in the era of paper mills. *Biology Open, 9*(10), bio056556. [https://doi.org/10.1242/bio.056556](https://doi.org/10.1242/bio.056556) Hardwicke, T. E., Jameel, L., Jones, M., Walczak, E. J., & Weinberg, L. M. (2014). Only human: Scientists, systems, and suspect statistics. *Opticon1826, 16,* 25\\. DOI:10.5334/OPT.CH Hardwicke, T. E., Bohn, M., MacDonald, K., Hembacher, E., Nuijten, M. B., Peloquin, B. N., ... & Frank, M. C. (2020). Analytic reproducibility in articles receiving open data badges at the journal Psychological Science: an observational study. *Royal Society Open Science, 8*(1), 201494\\. https://doi.org/10.1098/rsos.201494 Hart, D. D., & Silka, L. (2020). Rebuilding the Ivory Tower: A Bottom-Up Experiment in Aligning Research with Societal Needs. *Issues in Science and Technology,* 79-85. https://issues.org/aligning-research-with-societal-needs/ Hartgerink, C. H., Wicherts, J. M., & Van Assen, M. A. L. M. (2017). Too good to be false: Nonsignificant results revisited. *Collabra: Psychology, 3*(1). https://doi.org/10.1525/collabra.71 Haven, T. L., & van Grootel, L. (2019). Preregistering qualitative research. *Accountability in Research, 26*(3), 229–244. [https://doi.org/10.1080/08989621.2019.1580147](https://doi.org/10.1080/08989621.2019.1580147) Haynes, S. N., Richard, D. C. S., & Kubany, E. S. (1995). Content validity in psychological assessment: A functional approach to concepts and methods. *Psychological Assessment, 7*(3), 238–247. https://doi.org/10.1037/1040-3590.7.3.238 Health Research Board (n.d.) *Declaration on Research Assessment.* Available from: [https://www.hrb.ie/funding/funding-schemes/before-you-apply/how-we-assess-applications/declaration-on-research-assessment/](https://www.hrb.ie/funding/funding-schemes/before-you-apply/how-we-assess-applications/declaration-on-research-assessment/) Healy, K. (2018). *Data visualization: A practical introduction.* Princeton University Press. Hendriks, F., Kienhues, D., & Bromme, R. (2016). Trust in science and the science of trust. In *Trust and communication in a digitized world* (S. 143–159). Springer. Henrich, J. (2020). T*he weirdest people in the world: How the west became psychologically peculiar and particularly prosperous.* Farrar, Straus and Giroux. Henrich, J., Heine, S. J., & Norenzayan, A. (2010). The weirdest people in the world?. *Behavioral and brain sciences, 33*(2-3), 61-83.[https://doi.org/10.1017/S0140525X0999152X](https://doi.org/10.1017/S0140525X0999152X) Herrmannova, D., & Knoth, P. (2016). *Semantometrics Towards Full text-based Research Evaluation.* [https://arxiv.org/pdf/1605.04180.pdf](https://arxiv.org/pdf/1605.04180.pdf) Higgins, J.P.T., Thomas, J., Chandler, J., Cumpston, M., Li, T., Page, M.J., Welch, V.A. (Eds). (2019). *Cochrane Handbook for Systematic Reviews of Interventions.* 2nd Edition. Chichester, UK: John Wiley & Sons. Himmelstein, D. S., Rubinetti, V., Slochower, D. R., Hu, D., Malladi, V. S., Greene, C. S., & Gitter, A. (2019). Open collaborative writing with Manubot. *PLOS Computational Biology, 15*(6), e1007128. [https://doi.org/10.1371/journal.pcbi.1007128](https://doi.org/10.1371/journal.pcbi.1007128) Hirsch, J. E. (2005). An index to quantify an individual's scientific research output. *Proceedings of the National Academy of Sciences, 102*(46), 16569-16572. [https://doi.org/10.1073/pnas.0507655102](https://doi.org/10.1073/pnas.0507655102) Hitchcock, C., Meyer, A., Rose, D., & Jackson, R. (2002). Providing new access to the general curriculum: Universal design for learning. *Teaching exceptional children, 35*(2), 8-17. [https://www.proquest.com/scholarly-journals/providing-new-access-general-curriculum/docview/201139970/se-2?accountid=8630](https://www.proquest.com/scholarly-journals/providing-new-access-general-curriculum/docview/201139970/se-2?accountid=8630) Hoekstra, R., Kiers, H., & Johnson, A. (2012). Are assumptions of well-known statistical techniques checked, and why (not)?. *Frontiers in Psychology, 3*(137), 1-9. https://doi.org/10.3389/fpsyg.2012.00137 Hoijtink, H., Mulder, J., van Lissa, C., & Gu, X. (2019). A tutorial on testing hypotheses using the Bayes factor. *Psychological Methods, 24*(5), 539–556. https://doi.org/10.1037/met0000201 Holcombe, A. O. (2019). Contributorship, not authorship: Use CRediT to indicate who did what. *Publications, 7*(3), 48\\. [https://doi.org/10.3390/publications7030048](https://doi.org/10.3390/publications7030048) Holcombe, A. O., Kovacs, M., Aust, F., & Aczel, B. (2020). Documenting contributions to scholarly articles using CRediT and tenzing. *Plos one, 15*(12), e0244611. Homepage. (n.d.). *Open Science MOOC.* Retrieved 5 June 2021, from https://opensciencemooc.eu/ Houtkoop, B. L., Chambers, C., Macleod, M. Bishop, D. V. M. Nichols, T. E., & Wagenmekers, E.-J. (2018). Data sharing in psychology: A survey on barriers and preconditions. *Advances in Methods and Practices in Psychological Science, 1*(1), 70.85. https://doi.org/10.1177/2515245917751886 Huber, B., Barnidge, M., Gil de Zúñiga, H., & Liu, J. (2019). Fostering public trust in science: The role of social media. *Public understanding of science, 28*(7), 759-777. https://doi.org/10.1177/0963662519869097 Huelin, R., Iheanacho, I., Payne, K., & Sandman, K. (2015). What’s in a name? Systematic and non-systematic literature reviews, and why the distinction matters. *The evidence Forum*, 34-37. Retrieved from: [https://www.evidera.com/wp-content/uploads/2015/06/Whats-in-a-Name-Systematic-and-Non-Systematic-Literature-Reviews-and-Why-the-Distinction-Matters.pdf](https://www.evidera.com/wp-content/uploads/2015/06/Whats-in-a-Name-Systematic-and-Non-Systematic-Literature-Reviews-and-Why-the-Distinction-Matters.pdf) Hüffmeier, J., Mazei, J., & Schultze, T. (2016). Reconceptualizing replication as a sequence of different studies: A replication typology. *Journal of Experimental Social Psychology, 66*, 81-92. https://doi.org/10.1016/j.jesp.2015.09.009 Hultsch, D. F., MacDonald, S. W., & Dixon, R. A. (2002). Variability in reaction time performance of younger and older adults. *The Journals of Gerontology Series B: Psychological Sciences and Social Sciences, 57*(2), P101-P115. [https://doi.org/10.1093/geronb/57.2.P101](https://doi.org/10.1093/geronb/57.2.P101) Hurlbert, S. H. (1984). Pseudoreplication and the Design of Ecological Field Experiments. *Ecological Monographs, 54*(2), 187–211. [https://doi.org/10.2307/1942661](https://doi.org/10.2307/1942661) Ikeda, A., Xu, H., Fuji, N., Zhu, S., & Yamada, Y. (2019). Questionable research practices following pre-registration. *Japanese Psychological Review, 62*, 281–295. International Committee of Medical Journal Editors \\[ICMJE\\]. (2019)*. Recommendations for the conduct, reporting, eduting, and publication of scholarly work in medical journals.* http://www.icmje.org/icmje-recommendations.pdf ISO. (1993). *Guide to the Expression of Uncertainty in Measuremen*t. 1st ed. Geneva: International Organization for Standardization. Ioannidis, J. P. (2005). Why most published research findings are false. *PLoS medicine, 2*(8), e124.https://doi.org/10.1371/journal.pmed.0020124 Ioannidis, J. P., Fanelli, D., Dunne, D. D., & Goodman, S. N. (2015). Meta-research: evaluation and improvement of research methods and practices. *PLoS Biology, 13*(10), e1002264. https://doi.org/10.1371/journal.pbio.1002264 JabRef Development Team (2021). *JabRef \\- An open-source, cross-platform citation and reference management software*. https://www.jabref.org Jacobson, D., & Mustafa, N. (2019). Social Identity Map: A Reflexivity Tool for Practicing Explicit Positionality in Critical Qualitative Research. I*nternational Journal of Qualitative Methods, 18*, 1609406919870075\\. [https://doi.org/10.1177/1609406919870075](https://doi.org/10.1177/1609406919870075) Jafar, A. J. N. (2018). What is positionality and should it be expressed in quantitative studies? *Emergency Medicine Journal, 35*(5), 323–324. https://doi.org/10.1136/emermed-2017-207158 James, K. L., Randall, N. P., & Haddaway, N. R. (2016). A methodology for systematic mapping in environmental sciences. *Environmental evidence, 5*(1), 1-13. [https://doi.org/](https://doi.org/10.1242/bio.056556)[10.1186/s13750-016-0059-6](https://doi.org/10.1186/s13750-016-0059-6) Jannot, A. S., Agoritsas, T., Gayet-Ageron, A., & Perneger, T. V. (2013). Citation bias favoring statistically significant studies was present in medical research. J*ournal of clinical epidemiology, 66*(3), 296-301. https://doi.org/10.1016/j.jclinepi.2012.09.015. JASP Team (2020). *JASP* (Version 0.14.1)\\[Computer software\\] John, L. K., Loewenstein, G., & Prelec, D. (2012). Measuring the prevalence of questionable research practices with incentives for truth telling. *Psychological Science, 23*(5), 524–532. [https://doi.org/10.1177/0956797611430953](https://doi.org/10.1177/0956797611430953) Jones, A., Dr, Duckworth, J., & Christiansen, P. (2020, June 29). May I have your attention, please? Methodological and Analytical Flexibility in the Addiction Stroop. https://doi.org/10.31234/osf.io/ws8xp Joseph, T. D., & Hirshfield, L. E. (2011). ‘Why don't you get somebody new to do it?’Race and cultural taxation in the academy. *Ethnic and Racial Studies, 34*(1), 121-141. https://doi.org/10.1080/01419870.2010.496489 Kalliamvakou, E., Gousios, G., Blincoe, K., Singer, L., German, D. M., & Damian, D. (2014). The promises and perils of mining github. In *Proceedings of the 11th working conference on mining software repositories* (pp. 92-101). Kathawalla, U., Silverstein, P., & Syed, M. (2020). Easing into Open Science: A Guide for Graduate Students and Their Advisors*. Collabra: Psychology.* [https://psyarxiv.com/vzjdp](https://psyarxiv.com/vzjdp) Kelley, T. L. (1927). *Interpretation of educational measurements*. New York: Macmillan. Kerr, N. L. (1998). HARKing: Hypothesizing after the results are known. *Personality and social psychology review, 2*(3), 196-217. [https://doi.org/10.1207/s15327957pspr0203\\_4](https://doi.org/10.1207/s15327957pspr0203_4) Kerr, N. L., Ao, X., Hogg, M. A., & Zhang, J. (2018). Addressing replicability concerns via adversarial collaboration: Discovering hidden moderators of the minimal intergroup discrimination effect. *Journal of Experimental Social Psychology, 78*, 66-76. https://doi.org/10.1016/j.jesp.2018.05.001 Kidwell, M. C., Lazarević, L. B., Baranski, E., Hardwicke, T. E., Piechowski, S., Falkenberg, L. S., ... & Nosek, B. A. (2016). Badges to acknowledge open practices: A simple, low-cost, effective method for increasing transparency. *PLoS biology, 14*(5), e1002456.[https://doi.org/10.1371/journal.pbio.1002456](https://doi.org/10.1371/journal.pbio.1002456) Kienzler, H., & Fontanesi, C. (2017). Learning through inquiry: A global health hackathon. *Teaching in Higher Education, 22*(2), 129-142. [https://doi.org/10.1080/13562517.2016.1221805](https://doi.org/10.1080/13562517.2016.1221805) Kiernan, C. (1999). Participation in research by people with learning disability: Origins and issues. British Journal of Learning Disabilities, 27(2), 43–47. https://doi.org/10.1111/j.1468-3156.1999.tb00084.x King, G. (1995). Replication, replication. *PS: Political Science & Politics, 28*(3), 444–452. [https://doi.org/10.2307/420301](https://doi.org/10.2307/420301) Kitzes, J., Turek, D., Deniz, F. (2017). *The practice of reproducible research: Case studies and lessons from the data-intensive sciences.* University of California Press. Klein, R. A., Ratliff, K. A., Vianello, M., Adams, R. B., Bahník, Š., Bernstein, M. J., et al. (2014). Investigating variation in replicability: A “many labs” replication project. S*ocial Psychology, 45*, 142–152. https://doi.org/10.1027/1864-9335/a000178 Klein, R. A., Vianello, M., Hasselman, F., Adams, B. G., Adams, R. B., Alper, S., … Nosek, B. A. (2018). Many Labs 2: Investigating Variation in Replicability Across Samples and Settings. Advances in Methods and Practices in Psychological *Science, 1*(4), 443–490. [https://doi.org/10.1177/2515245918810225](https://doi.org/10.1177/2515245918810225) Kleinberg, B., Mozes, M., van der Toolen, Y., & verschuere, B. (2017, June 3). NETANOS \\- Named entity-based Text Anonymization for Open Science. Retrieved from [https://osf.io/w9nhb/](https://osf.io/w9nhb/) Knoth, P., & Herrmannova, D. (2014). Towards semantometrics: A new semantic similarity based measure for assessing a research publication’s contribution. *D-Lib Magazine, 20*(11), 8\\. [https://doi.org/10.1045/november14-knoth](https://doi.org/10.1045/november14-knoth) Knowledge, F. O. (2020, December 3). Preregistration Pledge. Free Our Knowledge. https://freeourknowledge.org/2020-12-03-preregistration-pledge/ Koole, S. L., & Lakens, D. (2012). Rewarding replications: A sure and simple way to improve psychological science. *Perspectives on Psychological Science, 7*(6), 608-614 .https://doi.org/10.1177/1745691612462586 Kreuter, F. (Ed.). (2013). *Improving Surveys with Paradata.* doi:10.1002/9781118596869 Kruschke, J. K. (2015). *Doing Bayesian data analysis: A tutorial with R, JAGS, and Stan* (2nd ed.). Academic Press. Kuhn, T. (1962). *The Structure of Scientific Revolutions.* University of Chicago Press. ISBN 978-0226458083. Kukull, W.A. & Ganguli, M. (2012). Generalizability: The trees, the forest, and the low-hanging fruit. *Neurology, 78*(23), 1886-1891. [https://doi.org/10.1212/WNL.0b013e318258f812](https://doi.org/10.1212/WNL.0b013e318258f812) Lakens, D. (2014). Performing high-powered studies efficiently with sequential analyses. *European Journal of Social Psychology, 44*(7), 701–710. [https://doi.org/10.1002/ejsp.2023](https://doi.org/10.1002/ejsp.2023) Lakens, D. (2020). Pandemic researchers — recruit your own best critics. *Nature, 581*, 121\\. Lakens, D. (2021a, January 4). Sample Size Justification. [https://doi.org/10.31234/osf.io/9d3yf](https://doi.org/10.31234/osf.io/9d3yf) Lakens, D. (2021b). The practical alternative to the p-value is the correctly used p-value. Lakens, D., Scheel, A. M., & Isager, P. M. (2018). Equivalence testing for psychological research: A tutorial. *Advances in Methods and Practices in Psychological Science, 1*(2), 259-269. https://doi.org/10.1177/2515245918770963 Lakens, D., McLatchie, N., Isager, P. M., Scheel, A. M., & Dienes, Z. (2020). Improving inferences about null effects with Bayes factors and equivalence tests. *The Journals of Gerontology: Series B, 75*(1), 45-57. https://doi.org/10.1093/geronb/gby065 Laine, H. (2017) Afraid of scooping – Case study on researcher strategies against fear of scooping in the context of open science. *Data Science Journal, 16*(29), 1–14. [https://doi.org/10.5334/dsj-2017-029](https://doi.org/10.5334/dsj-2017-029) Largent, E. A., & Snodgrass, R. T. (2016). Blind peer review by academic journals. In C. T. Robertson and A. S. Kesselheim (Eds.) *Blinding as a solution to bias: Strengthening biomedical science, forensic science, and law*, (pp. 75-95). Academic Press. https://doi.org/10.1016/B978-0-12-802460-7.00005-X Lazic, S. E. (2019). *Genuine replication and pseudoreplication: What’s the difference?* BMJ Open Science. [https://blogs.bmj.com/openscience/2019/09/16/genuine-replication-and-pseudoreplication-whats-the-difference/](https://blogs.bmj.com/openscience/2019/09/16/genuine-replication-and-pseudoreplication-whats-the-difference/) Leavy, P. (2017). *Research design: Quantitative, qualitative, mixed methods, arts-based, and community-based participatory research approaches.* The Guilford Press. Leavens, D. A., Bard, K. A., & Hopkins, W. D. (2010). BIZARRE chimpanzees do not represent “the chimpanzee”. *Behavioral and Brain Sciences, 33(*2-3), 100-101. https://doi.org/10.1017/S0140525X10000166 LeBel, E. P., Vanpaemel, W., Cheung, I., & Campbell, L. (2017). A brief guide to evaluate replications. *Meta-Psychology, 3*. https://doi.org/10.15626/MP.2018.843 LeBel, E. P., McCarthy, R. J., Earp, B. D., Elson, M., & Vanpaemel, W. (2018). A unified framework to quantify the credibility of scientific findings. A*dvances in Methods and Practices in Psychological Science, 1*(3), 389-402. [https://doi.org/10.1177/2515245918787489](https://doi.org/10.1177/2515245918787489) Ledgerwood, A., Hudson, S. T. J., Lewis, N. A., Jr., Maddox, K. B., Pickett, C., Remedios, J. D., … Wilkins, C. L. (2021, January 11). The Pandemic as a Portal: Reimagining Psychological Science as Truly Open and Inclusive. [https://doi.org/10.31234/osf.io/gdzue](https://doi.org/10.31234/osf.io/gdzue) Lee, R.M. (1993). *Doing research on sensitive topics.* London: Sage. Levitt, H. M., Motulsky, S. L., Wertz, F. J., Morrow, S. L., & Ponterotto, J. G. (2017). Recommendations for designing and reviewing qualitative research in psychology: Promoting methodological integrity. *Qualitative psychology, 4*(1), 2\\. https://doi.org/10.1037/qup0000082 Lewandowsky, S., & Bishop, D. (2016). Research integrity: Don't let transparency damage science. *Nature News, 529*(7587), 459\\. [https://doi.org/10.1038/529459a](https://doi.org/10.1038/529459a) LibGuides. (n.d.). *Measuring your research impact: i10-Index*. https://guides.library.cornell.edu/impact/author-impact-10. Lieberman, E. (2020). Research Cycles. In C. Elman, J. Gerring, & J. Mahoney (Eds.), *The Production of Knowledge: Enhancing Progress in Social Science* (Strategies for Social Inquiry, pp. 42-70). Cambridge: Cambridge University Press. [https://doi.org10.1017/9781108762519.003](https://doi.org10.1017/9781108762519.003) Lind, F., Gruber, M., & Boomgaarden, H. G. (2017). Content analysis by the crowd: Assessing the usability of crowdsourcing for coding latent constructs. *Communication Methods and Measures, 11*(3), 191–209. https://doi.org/10.1080/19312458.2017.1317338 Lindsay, D. S. (2015). Replication in Psychological Science \\[Editorial\\]. *Psychological Science, 26*(12), 1827-1832. https://doi.org/10.1177/0956797615616374 Lindsay, D. S. (2020). Seven steps toward transparency and replicability in psychological science. *Canadian Psychology/Psychologie canadienne., 61*(4), 310–317. [https://doi.org/10.1037/cap0000222](https://doi.org/10.1037/cap0000222) Liu, Y. et al. (2020). Replication markets: Results, lessons, challenges and opportunities in AI replication. arXiv:2005.04543 Lu, J., Qiu, Y., & Deng, A. (2018). A note on Type S/M errors in hypothesis testing. *British Journal of Mathematical and Statistical Psychology, 72*(1), 1-17. [https://doi.org/10.1111/bmsp.12132](https://doi.org/10.1111/bmsp.12132) Lüdtke, O., Ulitzsch, E., & Robitzsch, A. (2020). *A Comparison of Penalized Maximum Likelihood Estimation and Markov Chain Monte Carlo Techniques for Estimating Confirmatory Factor Analysis Models with Small Sample Sizes* \\[Preprint\\]. PsyArXiv. [https://doi.org/10.31234/osf.io/u3qag](https://doi.org/10.31234/osf.io/u3qag) Lutz, M. (2001). *Programming python.* O'Reilly Media, Inc. Lyon, L. (2016) Transparency: The Emerging Third Dimension of Open Science and Open Data. *LIBER Quarterly, 25*(4), 153-171. [http://doi.org/10.18352/lq.10113](http://doi.org/10.18352/lq.10113) Makowski, D., Ben-Shachar, M. S., Chen, S. H. A., & Lüdecke, D. (2019). Indices of Effect Existence and Significance in the Bayesian Framework. Retrieved from 10.3389/fpsyg.2019.02767 Marwick, B., Boettiger, C., & Mullen, L. (2018). Packaging data analytical work reproducibly using R (and friends). *The American Statistician, 72*(1), 80-88. [https://doi.org/10.1080/00031305.2017.1375986](https://doi.org/10.1080/00031305.2017.1375986) McElreath, R. (2020). *Statistical rethinking: A Bayesian course with examples in R and Stan* (2nd ed.). Taylor and Francis, CRC Press. McNutt, M. K., Bradford, M., Drazen, J. M., Hanson, B., Howard, B., Jamieson, K. H., Kiermer, V., Marcus, E., Pope, B. K., Schekman, R., Swaminathan, S., Stang, P. J., and Verma, I. M. (2018). Transparency in authors’ contributions and responsibilities to promote integrity in scientific publication. *Proceedings of the National Academy of Sciences of the United States of America, 115(*11), 2557-2560. [https://doi.org/10.1073/pnas.1715374115](https://doi.org/10.1073/pnas.1715374115) Medin, D. L. (2012). Rigor without rigor mortis: The APS Board discusses research integrity. *APS Observer, 25* (5-9), 27-28. https://www.psychologicalscience.org/observer/scientific-rigor Mellers, B., Hertwig, R., & Kahneman, D. (2001). Do frequency representations eliminate conjunction effects? An exercise in adversarial collaboration. P*sychological Science, 12*(4), 269-275. [https://doi.org/10.1111/1467-9280.00350](https://doi.org/10.1111/1467-9280.00350) Mertens, G., & Krypotos, A. M. (2019). Preregistration of analyses of preexisting data. *Psychologica Belgica, 59*(1), 338\\. Merton, R.K. (1938). Science and the social order. *Philosophy of Science, 5*(3), 321–337 https://doi.org/10.1086/286513 Merton, R. K. (1942). A note on science and democracy. *Journal of Legal and Political Sociology, 1*, 115–126. [https://doi.org/10.1515/9783110375008-013](https://doi.org/10.1515/9783110375008-013) Merton, R.K. (1968). The Matthew Effect in Science. *Science, 159* (3810), 56–63. https:/doi.org/10.1126/science.159.3810.56 Messick, S. (1995). Standards of validity and the validity of standards in performance assessment. *Educational measurement: Issues and practice, 14*(4), 5-8. [https://doi.org/10.1111/j.1745-3992.1995.tb00881.x](https://doi.org/10.1111/j.1745-3992.1995.tb00881.x) Michener W.K. (2015). Ten simple rules for creating a good data management plan. *PLoS Computational Biology, 11*(10), e1004525. https:/doi.org/10.1371/journal.pcbi.1004525 Moher, D., Bouter, L., Kleinert, S., Glasziou, P., Sham, M. H., Barbour, V., ... & Dirnagl, U. (2020). The Hong Kong Principles for assessing researchers: Fostering research integrity. *PLoS Biology, 18*(7), e3000737. [https://doi.org/10.1371/journal.pbio.3000737](https://doi.org/10.1371/journal.pbio.3000737) Moher, D., Liberati, A., Tetzlaff, J., & Altman, D. (2009). Preferred Reporting Items for Systematic Reviews and Meta-Analyses: The PRISMA Statement. *PLoS Medicine, 6*(7), e1000097. https://doi.org/10.1371/journal.pmed.1000097 Moher, D., Naudet, F., Cristea, I. A., Miedema, F., Ioannidis, J. P. A., & Goodman, S. N. (2018). Assessing scientists for hiring, promotion, and tenure. *PLOS Biology, 16*(3), e2004089. https://doi.org/10.1371/journal.pbio.2004089 Moshontz, H., Campbell, L., Ebersole, C. R., IJzerman, H., Urry, H. L., Forscher, P. S., ... & Chartier, C. R. (2018). The Psychological Science Accelerator: Advancing psychology through a distributed collaborative network. *Advances in Methods and Practices in Psychological Science, 1*(4), 501-515. https://doi.org/10.1177/2515245918797607 Monroe, K. R. (2018). The rush to transparency: DA-RT and the potential dangers for qualitative research. *Perspectives on Politics, 16*(1), 141–148. [https://doi.org/10.1017/S153759271700336X](https://doi.org/10.1017/S153759271700336X) Moretti, M. (2020, August 25). *Beyond Open-washing: Are Narratives the Future of Open Data Portals?* Medium. https://medium.com/nightingale/beyond-open-washing-are-stories-and-narratives-the-future-of-open-data-portals-93228d8882f3 Morgan, C. (1998). The DOI (Digital Object Identifier). *Serials, 11*(1), pp.47–51. [http://doi.org/10.1629/1147](http://doi.org/10.1629/1147) Munn, Z., Peters, M. D., Stern, C., Tufanaru, C., McArthur, A., & Aromataris, E. (2018). Systematic review or scoping review? Guidance for authors when choosing between a systematic or scoping review approach. *BMC medical research methodology, 18*(1), 1-7. https://doi.org/10.1186/s12874-018-0611-x Muthukrishna, M., Bell, A. V., Henrich, J., Curtin, C. M., Gedranovich, A., McInerney, J., & Thue, B. (2020). Beyond Western, Educated, Industrial, Rich, and Democratic (WEIRD) psychology: Measuring and mapping scales of cultural and psychological distance. *Psychological Science, 31*, 678-701. https://doi.org/10.1177/0956797620916782 National Academies of Sciences, Engineering, and Medicine; Policy and Global Affairs; Committee on Science, Engineering, Medicine, and Public Policy; Board on Research Data and Information; Division on Engineering and Physical Sciences; Committee on Applied and Theoretical Statistics; Board on Mathematical Sciences and Analytics; Division on Earth and Life Studies; Nuclear and Radiation Studies Board; Division of Behavioral and Social Sciences and Education; Committee on National Statistics; Board on Behavioral, Cognitive, and Sensory Sciences; Committee on Reproducibility and Replicability in Science. (2019). Reproducibility and Replicability in Science. In *Understanding Reproducibility and Replicability*. Washington (DC): National Academies Press (US), Available from: [https://www.ncbi.nlm.nih.gov/books/NBK547546/](https://www.ncbi.nlm.nih.gov/books/NBK547546/) Nature (n.d.). *Recommended Data Repositories.* Scientific Data. [https://www.nature.com/sdata/policies/repositories](https://www.nature.com/sdata/policies/repositories) Naudet, F., Ioannidis, J., Miedema, F., Cristea, I. A., Goodman, S. N., & Moher, D. (2018). *Six principles for assessing scientists for hiring, promotion, and tenure.* Impact of Social Sciences Blog. Navarro, D. (2020). *Paths in strange spaces: A comment on preregistration.* Nelson, L.D., Simmons, J.P. & Simonsohn, U. (2012) Let's Publish Fewer Papers, *Psychological Inquiry, 23* (3), 291-293, https://doi.org/10.1080/1047840X.2012.705245 Neuroskeptic. (2012). The nine circles of scientific hell. (2012). *Perspectives on Psychological Science, 7*(6), 643–644. [https://doi.org/10.1177/1745691612459519](https://doi.org/10.1177/1745691612459519) Nichols, T. E., Das, S., Eickhoff, S. B., Evans, A. C., Glatard, T., Hanke, M., ... & Yeo, B. T. (2017). Best practices in data analysis and sharing in neuroimaging using MRI. N*ature neuroscience, 20*(3), 299-303. https://doi.org/10.1038/nn.4500 Nickerson, R. S. (1998). Confirmation bias: A ubiquitous phenomenon in many guises. *Review of general psychology, 2*(2), 175-220. [https://doi.org/10.1037/1089-2680.2.2.175](https://doi.org/10.1037/1089-2680.2.2.175) Nieuwenhuis, S., Forstmann, B. U., & Wagenmakers, E. J. (2011). Erroneous analyses of interactions in neuroscience: a problem of significance. *Nature Neuroscience*, *14*(9), 1105\\-1107. https://doi.org/10.1038/nn.2886 NIHR (2021) [https://www.learningforinvolvement.org.uk/?opportunity=nihr-guidance-on-co-producing-a-research-project](https://www.learningforinvolvement.org.uk/?opportunity=nihr-guidance-on-co-producing-a-research-project) Nittrouer, C., Hebl, M., Ashburn-Nardo, L., Trump-Steele, R., Lane, D., Valian, V. (2018). Gender disparities in colloquium speakers. *Proceedings of the National Academy of Sciences Jan, 115* (1) 104-108; DOI: 10.1073/pnas.1708414115 Nosek, B. A., & Bar-Anan, Y. (2012). Scientific utopia: I. Opening scientific communication. *Psychological Inquiry, 23*(3), 217-243. [https://doi.org/10.1080/1047840X.2012.692215](https://doi.org/10.1080/1047840X.2012.692215) Nosek, B. A., Ebersole, C. R., DeHaven, A. C., & Mellor, D. T. (2018). The preregistration revolution. *Proceedings of the National Academy of Sciences, 115*(11), 2600-2606. https://doi.org/10.1073/pnas.1708274114 Nosek, B.A. & Errington, T.M. (2020) What is replication? *PlosBiology, 18*(3), e3000691. [https://doi.org/10.1371/journal.pbio.3000691](https://doi.org/10.1371/journal.pbio.3000691) Nosek, B. A., & Lakens, D. (2014). Registered Reports. *Social Psychology, 45,* 137-141. [https://doi.org/10.1027/1864-9335/a000192](https://doi.org/10.1027/1864-9335/a000192). Nosek, B. A., Spies, J. R., & Motyl, M. (2012). Scientific utopia: II. Restructuring incentives and practices to promote truth over publishability. *Perspectives on Psychological Science, 7*(6), 615-631.[https://doi.org/10.1177%2F1745691612459058](https://doi.org/10.1177%2F1745691612459058) Noy, N. F., & McGuinness, D. L. (2001). *Ontology development 101: A guide to creating your first ontology* .[https://corais.org/sites/default/files/ontology\\_development\\_101\\_aguide\\_to\\_creating\\_your\\_first\\_ontology.pdf](https://corais.org/sites/default/files/ontology_development_101_aguide_to_creating_your_first_ontology.pdf) Nüst, D. C., Boettiger, C., & Marwick, B. (2018) How to Read a Research Compendium. arXiv preprint arXiv:1806.09525 O’Dea, R. E., Parker, T. H., Chee, Y. E., Culina, A., Drobniak, S. M., Duncan, D. H., Fidler, F., Gould, E., Ihle, M., Kelly, C. D., Lagisz, M., Roche, D. G., Sánchez-Tójar, A., Wilkinson, D. P., Wintle, B. C., & Nakagawa, S. (2021). Towards open, reliable, and transparent ecology and evolutionary biology. *BMC Biology, 19*(1). https://doi.org/10.1186/s12915-021-01006-3 O’Grady (2020) *Psychology’s replication crisis inspires ecologists to push for more reliable research.* Science. [https://doi.org/10.1126/science.abg0894](https://doi.org/10.1126/science.abg0894) Obels, P., Lakens, D., Coles, N. A., Gottfried, J., & Green, S. A. (2020). Analysis of open data and computational reproducibility in registered reports in psychology. *Advances in Methods and Practices in Psychological Science, 3*(2), 229-237. Oberauer K., & Lewandowsky S. (2019). Addressing the theory crisis in psychology. *Psychonomic bulletin & review. 26*(5),1596–1618. https://doi.org/10.3758/s13423-019-01645-2 Open Science Collaboration. (2015). Estimating the reproducibility of psychological science. *Science, 349*(6251), aac4716. https://doi.org/10.1126/science.aac4716 Open Aire. (2020). *High accuracy Data anonymisation*. Amnesia. [https://amnesia.openaire.eu/](https://amnesia.openaire.eu/) Open Source Initiative (n.d.). *The Open Source Definition.* Open Source Initiative. [https://opensource.org/osd](https://opensource.org/osd) Orben, A. (2019). A journal club to fix science. *Nature, 573*(7775), 465-466. [https://doi.org/10.1038/d41586-019-02842-8](https://doi.org/10.1038/d41586-019-02842-8) Ottmann, G., Laragy, C., Allen, J., Feldman, P. (2011). Coproduction in practice: Participatory action research to develop a model of community aged care. Systemic Practice and Action Research, 24, 413–427. https://doi.org/10.1007/s11213-011-9192-x Oxford Dictionaries. (2017). *Bias—definition of bias in English.* [https://en.oxforddictionaries.com/definition/bias](https://en.oxforddictionaries.com/definition/bias) Oxford Reference. (2017). *Reflexivity*. [https://www.oxfordreference.com/view/10.1093/acref/9780199599868.001.0001/acref-9780199599868-e-1530](https://www.oxfordreference.com/view/10.1093/acref/9780199599868.001.0001/acref-9780199599868-e-1530) Padilla, A. M. (1994). Research news and comment: Ethnic minority scholars; research, and mentoring: Current and future issues. *Educational Researcher, 23*(4), 24-27. [https://doi.org/10.3102/0013189X023004024](https://doi.org/10.3102/0013189X023004024) Page, M. J., Moher, D., Bossuyt, P. M., Boutron, I., Hoffmann, T. C., Mulrow, C. D., ... & McKenzie, J. E. (2021). PRISMA 2020 explanation and elaboration: updated guidance and exemplars for reporting systematic reviews. *British Medical Journal, 372*. https://doi.org/ 10.1136/bmj.n160 Patience, G. S., Galli, F., Patience, P. A., & Boffito, D. C. (2019). Intellectual contributions meriting authorship: Survey results from the top cited authors across all science categories. *PLoS One, 14*(1), e0198117. [https://doi.org/10.1371/journal.pone.0198117](https://doi.org/10.1371/journal.pone.0198117) Pavlov, Y. G., Adamian, N., Appelhoff, S., Arvaneh, M., Benwell, C., Ph.D., Beste, C., … Mushtaq, F. (2020, November 27). \\#EEGManyLabs: Investigating the Replicability of Influential EEG Experiments. [https://doi.org/10.31234/osf.io/528nr](https://doi.org/10.31234/osf.io/528nr) PCI (n.d.). *PCI IN A FEW LINES.* Peer community in. [https://peercommunityin.org/](https://peercommunityin.org/) Peer, E., Brandimarte, L., Samat, S., & Acquisti, A. (2017). Beyond the Turk: Alternative platforms for crowdsourcing behavioral research. *Journal of Experimental Social Psychology, 70*, 153–163. https://doi.org/10.1016/j.jesp.2017.01.006 Peng, R. D. (2011). Reproducible Research in Computational Science. *Science, 334*(6060), 1226–1227. [https://doi.org/10.1126/science.1213847](https://doi.org/10.1126/science.1213847) Percie du Sert, N., Hurst, V., Ahluwalia, A., Alam, S., Avey, M. T., Baker, M., ... & Würbel, H. (2020). The ARRIVE guidelines 2.0: Updated guidelines for reporting animal research. *Journal of Cerebral Blood Flow & Metabolism, 40*(9), 1769-1777. [https://doi.org/10.1371/journal.pbio.3000410](https://doi.org/10.1371/journal.pbio.3000410) Pernet, C. R. (2015). Null hypothesis significance testing: a short tutorial. *F1000Research, 4,* 621\\. https:/doi.org/10.12688/f1000research.6963.3 Pernet, C. R., Appelhoff, S., Gorgolewski, K. J., Flandin, G., Phillips, C., Delorme, A., & Oostenveld, R. (2019). EEG-BIDS, an extension to the brain imaging data structure for electroencephalography. *Scientific Data, 6*(1), 103\\. https://doi.org/10.1038/s41597-019-0104-8 Pernet, C., Garrido, M. I., Gramfort, A., Maurits, N., Michel, C. M., Pang, E., ... & Puce, A. (2020). Issues and recommendations from the OHBM COBIDAS MEEG committee for reproducible EEG and MEG research. *Nature Neuroscience, 23*(12), 1473-1483. https://doi.org/10.1038/s41593-020-00709-0 Peterson, D., & Panofsky, A. (2020, August 4). Metascience as a scientific social movement. [https://doi.org/10.31235/osf.io/4dsqa](https://doi.org/10.31235/osf.io/4dsqa) Petre, M., & Wilson, G. (2014). Code review for and by scientists. arXiv preprint arXiv:1407.5648. Popper, K. (1959). *The logic of scientific discovery.* London, United Kingdom: Routledge. Posselt, J. R. (2020). *Equity in Science: Representation, Culture, and the Dynamics of Change in Graduate Education.* Stanford University Press. [https://books.google.de/books?id=2CjwDwAAQBAJ](https://books.google.de/books?id=2CjwDwAAQBAJ) Pownall, M., Talbot, C. V., Henschel, A., Lautarescu, A., Lloyd, K., Hartmann, H., … Siegel, J. A. (2020, October 13). Navigating Open Science as Early Career Feminist Researchers. [https://doi.org/10.31234/osf.io/f9m47](https://doi.org/10.31234/osf.io/f9m47) Psychological Science Accelerator. (n.d.). https://psysciacc.org/. R Core Team (2020). *R: A language and environment for statistical computing*. R Foundation for Statistical Computing, Vienna, Austria. URL [https://www.R-project.org/](https://www.R-project.org/) Rakow, T., Thompson, V., Ball, L., & Markovits, H. (2014). Rationale and guidelines for empirical adversarial collaboration: A Thinking & Reasoning initiative. *Thinking & Reasoning, 21*(2), 167–175. doi:10.1080/13546783.2015.975405 RepliCATS project. (2020). *Collaborative Assessment for Trustworthy Science.* The University of Melbourne. [https://replicats.research.unimelb.edu.au/](https://replicats.research.unimelb.edu.au/) ReproducibiliTea. (n.d.). *Welcome to ReproducibiliTea.* ReproducibiliTea. [https://reproducibilitea.org/](https://reproducibilitea.org/) Research Data Alliance (2020). *Data management plan (DMP) common standard.* Available from: [https://github.com/RDA-DMP-Common/RDA-DMP-Common-Standard](https://github.com/RDA-DMP-Common/RDA-DMP-Common-Standard) RIOT Science Club. (2021, May 28). *RIOT Science Club.* [http://riotscience.co.uk/](http://riotscience.co.uk/) Rodriguez, J. K., & Günther, E. A. (2020, Oct 14). *What’s wrong with manels and what can we do about it.* The Conversation. [https://theconversation.com/whats-wrong-with-manels-and-what-can-we-do-about-them-148068](https://theconversation.com/whats-wrong-with-manels-and-what-can-we-do-about-them-148068) Rolls, L., & Relf, M. (2006). Bracketing interviews: addressing methodological challenges in qualitative interviewing in bereavement and palliative care, *Mortality, 11* (3), 286-305, https://doi.org/10.1080/13576270600774893 Rose, D. (2000). Universal design for learning. J*ournal of Special Education Technology, 15*(3), 45-49. https://doi.org/10.1177/016264340001500307 Rose, D. (2018). Participatory research: real or imagined. *Social Psychiatry and Psychiatric Epidemiology*. *53*, 765–771. https://doi.org/10.1007/s00127-018-1549-3 Rose, D. H., & Meyer, A. (2002). *Teaching every student in the digital age: Universal design for learning.* Association for Supervision and Curriculum Development, 1703 N. Beauregard St., Alexandria, VA 22311-1714 (Product no. 101042: $22.95 ASCD members; $26.95 nonmembers). Ross-Hellauer, T. (2017). What is open peer review? A systematic review \\[version 2; peer review: 4 approved\\]. *F1000Research, 6,* 588 ([https://doi.org/10.12688/f1000research.11369.2](https://doi.org/10.12688/f1000research.11369.2) Rossner, M., Van Epps, H., & Hill, E. (2008). *Show me the data.* https://doi.org/10.1083/jcb.200711140 Rothstein, H. R., Sutton, A. J., & Borenstein, M. (2005). Publication bias in meta-analysis. In Rothstein, A. J. Sutton, & M. Borenstein (Eds.).P*ublication bias in meta-analysis: Prevention, assessment and adjustments* (pp. 1-7). John Wiley & Sons, Ltd. [https://doi.org/10.1002/0470870168.ch1](https://doi.org/10.1002/0470870168.ch1) Rowhani-Farid, A., Aldcroft, A., & Barnett, A. G. (2020). Did awarding badges increase data sharing in BMJ Open? A randomized controlled trial. *Royal Society open science, 7*(3), 191818\\. [https://doi.org/10.1098/rsos.191818](https://doi.org/10.1098/rsos.191818) Rubin, M. (2021). Explaining the association between subjective social status and mental health among university students using an impact ratings approach. S*N Social Sciences, 1*(1), 1-21. https://doi.org/10.1007/s43545-020-00031-3 Rubin, M., Evans, O., & McGuffog, R. (2019). Social class differences in social integration at university: Implications for academic outcomes and mental health. In J. Jetten, & K. Peters (Eds.), *The social psychology of inequality* (pp. 87-102). Springer. [https://doi.org/10.1007/978-3-030-28856-3\\_6](https://doi.org/10.1007/978-3-030-28856-3_6) S. (2021, June 5). OSF | StudySwap: A platform for interlab replication, collaboration, and research resource exchange. OSF. [https://osf.io/meetings/StudySwap/](https://osf.io/meetings/StudySwap/) Sagarin, B. J., Ambler, J. K., & Lee, E. M. (2014). An ethical approach to peeking at data. *Perspectives on Psychological Science, 9*(3), 293-304. https://doi.org/10.1177/1745691614528214 San Francisco Declaration on Research Assessment (DORA). [https://sfdora.org/](https://sfdora.org/) Retrieved February 18th 2021\\. Sato, T. (1996). Type I and Type II error in multiple comparisons. *The Journal of Psychology, 130*(3), 293-302. https://doi.org/10.1080/00223980.1996.9915010 Schafersman, S.D. (1997). *An Introduction to Science.* Available from: [https://www.geo.sunysb.edu/esp/files/scientific-method.html](https://www.geo.sunysb.edu/esp/files/scientific-method.html) Schmidt, F. L., & Hunter, J. E. (2014). *Methods of meta-analysis: Correcting error and bias in research findings* (3rd ed.). Thousand Oaks, CA: Sage. Schmidt, R.H. (1987) A worksheet for authorship of scientific articles. *The Bulletin of the Ecological Society of America, 68*, 8–10. Retrieved March 4, 2021, from http://www.jstor.org/stable/20166549 Schneider, J., Merk, S., & Rosman, T. (2019). (Re)Building Trust? Investigating the effects of open science badges on perceived trustworthiness in journal articles. [https://doi.org/10.17605/OSF.IO/VGBRS](https://doi.org/10.17605/OSF.IO/VGBRS) Schönbrodt, F. D., Wagenmakers, E.-J., Zehetleitner, M., & Perugini, M. (2017). Sequential hypothesis testing with Bayes factors: Efficiently testing mean differences. *Psychological Methods, 22*(2), 322–339. [https://doi.org/10.1037/met0000061](https://doi.org/10.1037/met0000061) Schönbrodt, F. (2019). Training students for the Open Science future. *Nature human behaviour, 3*(10), 1031-1031.[https://doi.org/10.1038/s41562-019-0726-z](https://doi.org/10.1038/s41562-019-0726-z) Schuirmann, D. J. (1987). A comparison of the two one-sided tests procedure and the power approach for assessing the equivalence of average bioavailability. *Journal of Pharmacokinetics and Biopharmaceutics, 15*, 657–680. https://doi.org/10.1007/BF01068419 Schulz, K. F., & Grimes, D. A. (2005). Multiplicity in randomised trials I: endpoints and treatments. *The Lancet, 365*(9470), 1591-1595. [https://doi.org/10.1016/S0140-6736(05)66461-6](https://doi.org/10.1016/S0140-6736\\(05\\)66461-6) Schulz, K. F., Altman, D. G., & Moher, D. (2010). CONSORT 2010 statement: updated guidelines for reporting parallel group randomised trials. *Trials, 11*(1), 1-8. [https://doi.org/10.1186/1745-6215-11-32](https://doi.org/10.1186/1745-6215-11-32) Schwarz, N., & Strack, F. (2014). Does Merely Going Through the Same Moves Make for a “Direct” Replication?: Concepts, Contexts, and Operationalizations. S*ocial Psychology, 45*(4), 305-306. Science, C. (n.d.). Open science badges. Retrieved February 08, 2021, from [https://www.cos.io/initiatives/badges](https://www.cos.io/initiatives/badges) Sert, N. P. du, Hurst, V., Ahluwalia, A., Alam, S., Avey, M. T., Baker, M., Browne, W. J., Clark, A., Cuthill, I. C., Dirnagl, U., Emerson, M., Garner, P., Holgate, S. T., Howells, D. W., Karp, N. A., Lazic, S. E., Lidster, K., MacCallum, C. J., Macleod, M., … Würbel, H. (2020). The ARRIVE guidelines 2.0: Updated guidelines for reporting animal research. *PLOS Biology, 18*(7), e3000410. https://doi.org/10.1371/journal.pbio.3000410 Shadish, W. R., Cook, T. D., & Campbell, D. T. (2002). *Experimental and quasi-experimental designs for generalized causal inference.* Houghton, Mifflin and Company. Sharma, M., Sarin, A., Gupta, P., Sachdeva, S., & Desai, A. (2014). Journal impact factor: its use, significance and limitations. *World journal of nuclear medicine, 13*(2), 146\\. https://doi.org/10.4103/1450-1147.139151 Siddaway, A. P., Wood, A. M., & Hedges, L. V. (2019). How to do a systematic review: a best practice guide for conducting and reporting narrative reviews, meta-analyses, and meta-syntheses. *Annual review of psychology, 70*, 747-770. https://doi.org/[10.1146/annurev-psych-010418-102803](https://doi-org.surrey.idm.oclc.org/10.1146/annurev-psych-010418-102803) Sijtsma, K. (2016). Playing with data—Or how to discourage questionable research practices and stimulate researchers to do things right. *Psychometrika, 81*(1), 1–15. https://doi.org/10.1007/s11336-015-9446-0 Silberzahn, R., Uhlmann, E. L., Martin, D. P., Anselmi, P., Aust, F., Awtrey, E., Bahník, Š., Bai, F., Bannard, C., Bonnier, E., Carlsson, R., Cheung, F., Christensen, G., Clay, R., Craig, M. A., Dalla Rosa, A., Dam, L., Evans, M. H., Flores Cervantes, I., … Nosek, B. A. (2018). Many Analysts, One Data Set: Making Transparent How Variations in Analytic Choices Affect Results. *Advances in Methods and Practices in Psychological Science,* 337–356. [https://doi.org/10.1177/2515245917747646](https://doi.org/10.1177/2515245917747646) Simons, D. J., Shoda, Y., & Lindsay, D. S. (2017). Constraints on generality (COG): A proposed addition to all empirical papers. *Perspectives on Psychological Science, 12*(6), 1123-1128. https://doi.org/10.1177/1745691617708630 Simmons, J. P., Nelson, L. D., & Simonsohn, U. (2011). False-positive psychology: Undisclosed flexibility in data collection and analysis allows presenting anything as significant. *Psychological Science, 22*(11), 1359-1366. [https://doi.org/10.1177/0956797611417632](https://doi.org/10.1177/0956797611417632) Simmons, J., Nelson, L., & Simonsohn, U. (2021). Pre‐registration: Why and how. *Journal of Consumer Psychology, 31*(1), 151–162. https://doi.org/10.1002/jcpy.1208 Simonsohn, U., Nelson, L. D., & Simmons, J. P. (2014). P-curve: a key to the file-drawer. *Journal of experimental psychology: General, 143*(2), 534\\. Simonsohn, U., Simmons, J. P., & Nelson, L. D. (2015). Specification curve: Descriptive and inferential statistics on all reasonable specifications. Retrieved from http://sticerd.lse.ac.uk/seminarpapers/psyc16022016.pdf. Simonsohn, U., Nelson, L. D., & Simmons, J. P. (2014). p-curve and effect size: Correcting for publication bias using only significant results. *Perspectives on Psychological Science, 9*(6), 666–681. [https://doi.org/10.1177/1745691614553988](https://doi.org/10.1177/1745691614553988) Simonsohn, U., Nelson, L. D., & Simmons, J. P. (2019). P-curve won’t do your laundry, but it will distinguish replicable from non-replicable findings in observational research: Comment on Bruns & Ioannidis (2016). *PLoS ONE, 14*(3), e0213454. https://doi.org/10.1371/journal.pone.0213454 Simonsohn, U., Simmons, J. P., & Nelson, L. D. (2020). Specification curve analysis. *Nature Human Behaviour, 4*(11), 1208-1214. https://doi.org/10.1038/s41562-020-0912-z Slow Science Academy. (2010). *The Slow Science Manifesto*. Slow Science. [http://slow-science.org/](http://slow-science.org/). SIPS. (2021). *The Society for the Improvement of Psychological Science.* SIPS. https://improvingpsych.org/ Smaldino, P. E., & McElreath, R. (2016). The natural selection of bad science. *Royal Society Open Science, 3*(9), 160384\\. https://doi.org/10.1098/rsos.160384 Smith, G. T. (2005). On Construct Validity: Issues of Method and Measurement. *Psychological Assessment, 17*(4), 396–408. [https://doi.org/10.1037/1040-3590.17.4.396](https://psycnet.apa.org/doi/10.1037/1040-3590.17.4.396) Smith, A. C., Merz, L., Borden, J. B., Gulick, C., Kshirsagar, A. R., & Bruna, E. M. (2020, September 2). Assessing the effect of article processing charges on the geographic diversity of authors using Elsevier’s ‘Mirror Journal’ system. [https://doi.org/10.31222/osf.io/s7cx4](https://doi.org/10.31222/osf.io/s7cx4) Smith, A.J., Clutton, R.E., Lilley, E., Hansen K.E.A., Brattelid, T. (2018): PREPARE: Guidelines for planning animal research and testing. *Laboratory Animals, 52*(2), 135-141. https:/doi.org/10.1177/0023677217724823 Sorsa, M.A., Kiikkala, I., & Åstedt-Kurki, P. (2015). Bracketing as a skill in conducting unstructured qualitative interviews. *Nursing Research, 22*(4), 8-12. https:/doi.org/10.7748/nr.22.4.8.e1317. Society for Open, Reliable and Transparent Ecology and Evolutionary biology (n.d.). SORTEE. Retrieved 5 June 2021, from [https://www.sortee.org/](https://www.sortee.org/) Spence, J. R., & Stanley, D. J. (2018). Concise, simple, and not wrong: In search of a short-hand interpretation of statistical significance. *Frontiers in psychology, 9,* 2185\\. https:/doi.org/10.3389/fpsyg.2018.02185 Stanford Libraries. (n.d.). *Data management plans.* [https://library.stanford.edu/research/data-management-services/data-management-plans\\#:\\~:text=A%20data%20management%20plan%20](https://library.stanford.edu/research/data-management-services/data-management-plans#:~:text=A%20data%20management%20plan%20)(DMP,share%20and%20preserve%20your%20data. Steup, M., & Neta, R. (2020, April 11). *Epistemology.* Stanford Encyclopedia of Philosophy. [https://plato.stanford.edu/entries/epistemology/](https://plato.stanford.edu/entries/epistemology/). Steegen, S., Tuerlinckx, F., , Gelman, A. & Vanpaemel, W. (2016). Increasing Transparency through a Multiverse Analysis. *Perspectives on Psychological Science, 11,* 702-712. [https://doi.org/10.1177/1745691616658637](https://doi.org/10.1177/1745691616658637) Stewart, N., Chandler, J., & Paolacci, G. (2017). Crowdsourcing samples in cognitive science. *Trends in cognitive sciences, 21*(10), 736-748. [https://doi.org/10.1016/j.tics.2017.06.007](https://doi.org/10.1016/j.tics.2017.06.007) Stodden, V. C. (2011). Trust your science? Open your data and code. Strathern, M. (1997). ‘Improving ratings’: audit in the British University system. *European review, 5*(3), 305-321. https://doi.org/10.1002/(SICI)1234-981X(199707)5:3\\<305::AID-EURO184\\>3.0.CO;2-4https://doi.org/10.1002/(SICI)1234-981X(199707)5:3\\<305::AID-EURO184\\>3.0.CO;2-4 Suber, P. (2004). The primacy of authors in achieving Open Access. *Nature.* June 10, 2004\\. (previous, unabridged version: [http://dash.harvard.edu/handle/1/4391161](http://dash.harvard.edu/handle/1/4391161)) Suber, P. (2015). *Open Access Overview.* Available from: [http://legacy.earlham.edu/\\~peters/fos/overview.htm](http://legacy.earlham.edu/~peters/fos/overview.htm) SwissRN. (n.d.). Swiss Reproducibility Network. Retrieved 5 June 2021, from https://www.swissrn.org/ [Syed, M. (2019). The Open Science Movement is for all of us. PsyArXiv.](https://psyarxiv.com/cteyb/) Syed, M., & Kathawalla, U. (2020, February 25). Cultural Psychology, Diversity, and Representation in Open Science. https://doi.org/10.31234/osf.io/t7hp2 Szollosi, A., & Donkin, C., (2019). Arrested theory development: The misguided distinction between exploratory and confirmatory research. PsyArXiv. Tenney, S., & Abdelgawad, I. (2019). Statistical significance. In *StatsPearls*. Treasure Island (FL), StatPearls Publishing. Tscharntke, T., Hochberg, M. E., Rand, T. A., Resh, V. H., & Krauss, J. (2007). Author sequence and credit for contributions in multiauthored publications. *PLoS Biology, 5*(1), e18. https://doi.org/10.1371/journal.pbio.0050018 Tennant, J., Bielczyk, N. Z., Cheplygina, V., Greshake Tzovaras, B., Hartgerink, C. H. J., Havemann, J., Masuzzo, P., & Steiner, T. (2019). Ten simple rules for researchers collaborating on Massively Open Online Papers (MOOPs) \\[Preprint\\]. MetaArXiv. [https://doi.org/10.31222/osf.io/et8ak](https://doi.org/10.31222/osf.io/et8ak) The jamovi project (2020). *jamovi* (Version 1.2) \\[Computer Software\\]. Retrieved from [https://www.jamovi.org](https://www.jamovi.org) The Nine Circles of Scientific Hell. (2012). *Perspectives on Psychological Science, 7*(6), 643–644. [https://doi.org/10.1177/1745691612459519](https://doi.org/10.1177/1745691612459519) The R Foundation (n.d.). *The R Project for Statistical Computing.* The R Foundation. [https://www.r-project.org](https://www.r-project.org/)/ Thombs, B. D., Levis, A. W., Razykov, I., Syamchandra, A., Leentjens, A. F., Levenson, J. L., & Lumley, M. A. (2015). Potentially coercive self-citation by peer reviewers: a cross-sectional study. *Journal of Psychosomatic Research, 78*(1), 1-6. https://doi.org/10.1016/j.jpsychores.2014.09.015 Tierney, W., Hardy III, J. H., Ebersole, C. R., Leavitt, K., Viganola, D., Clemente, E. G., ... & Hiring Decisions Forecasting Collaboration. (2020). Creative destruction in science. *Organizational Behavior and Human Decision Processes, 161,* 291-309. https://doi.org/10.1016/j.obhdp.2020.07.002 Tierney, W., Hardy III, J., Ebersole, C. R., Viganola, D., Clemente, E. G., Gordon, M., ... & Culture & Work Morality Forecasting Collaboration. (2021). A creative destruction approach to replication: Implicit work and sex morality across cultures. *Journal of Experimental Social Psychology, 93*, 104060\\. [https://doi.org/10.1016/j.jesp.2020.104060](https://doi.org/10.1016/j.jesp.2020.104060) Tiokhin, L., Yan, M., & Horgan, T. J. H. (2021). Competition for priority harms the reliability of science, but reforms can help. *Nature Human Behaviour.* https://doi.org/10.1038/s41562-020-01040-1 Topor, M., Pickering, J. S., Barbosa Mendes, A., Bishop, D. V. M., Büttner, F. C., Elsherif, M. M., … Westwood, S. J. (2021, March 15). An integrative framework for planning and conducting Non-Intervention, Reproducible, and Open Systematic Reviews (NIRO-SR). [https://doi.org/10.31222/osf.io/8gu5z](https://doi.org/10.31222/osf.io/8gu5z) Tufte, E. R. (1983). *The visual display of quantitative information.* Graphics Press. Tukey, J.W. (1977). *Exploratory data analysis.* Reading, MA: Addison-Wesley. Tvina, A., Spellecy, R., & Palatnik, A. (2019). Bias in the peer review process: can we do better?. *Obstetrics & Gynecology, 133*(6), 1081-1083. https://doi.org/10.1097/AOG.0000000000003260 Uhlmann, E. L., Ebersole, C. R., Chartier, C. R., Errington, T. M., Kidwell, M. C., Lai, C. K., McCarthy, R. J., Riegelman, A., Silberzahn, R., & Nosek, B. A. (2019). Scientific utopia III: Crowdsourcing science. *Perspectives on Psychological Science, 14*(5), 711–733. [https://doi.org/10.1177/1745691619850561](https://doi.org/10.1177/1745691619850561) van de Schoot, R., Depaoli, S., King, R., Kramer, B., Märtens, K., Tadesse, M. G., Vannucci, M., Gelman, A., Veen, D., Willemsen, J., & Yau, C. (2021). Bayesian statistics and modelling. *Nature Reviews Methods Primers, 1*(1), 1–26. [https://doi.org/10.1038/s43586-020-00001-2](https://doi.org/10.1038/s43586-020-00001-2) Vazire, S. (2018). Implications of the Credibility Revolution for Productivity, Creativity, and Progress. *Perspectives on Psychological Science, 13*(4), 411–417. [https://doi.org/10.1177/1745691617751884](https://doi.org/10.1177/1745691617751884) Vazire, S., Schiavone, S. R., & Bottesini, J. G. (2020). Credibility Beyond Replicability: Improving the Four Validities in Psychological Science. [https://doi.org/10.31234/osf.io/bu4d3](https://doi.org/10.31234/osf.io/bu4d3) Villum, C. (2016, July 2). *“Open-washing” – The difference between opening your data and simply making them available*. Open Knowledge Foundation Blog. [https://blog.okfn.org/2014/03/10/open-washing-the-difference-between-opening-your-data-and-simply-making-them-available/](https://blog.okfn.org/2014/03/10/open-washing-the-difference-between-opening-your-data-and-simply-making-them-available/) Vlaeminck, S., & Podkrajac, F. (2017). Journals in Economic Sciences: Paying Lip Service to Reproducible Research? Paying Lip Service to Reproducible Research?. *IASSIST Quarterly, 41*(1-4), 16\\. [https://doi.org/10.29173/iq6](https://doi.org/10.29173/iq6) Von Elm, E., Altman, D. G., Egger, M., Pocock, S. J., Gøtzsche, P. C., & Vandenbroucke, J. P. (2007). The Strengthening the Reporting of Observational Studies in Epidemiology (STROBE) statement: guidelines for reporting observational studies. *Annals of internal medicine, 147*(8), 573-577. [https://doi.org/10.1136/bmj.39335.541782.AD](https://doi.org/10.1136/bmj.39335.541782.AD) Voracek, M., Kossmeier, M., & Tran, U. S. (2019). Which Data to Meta-Analyze, and How?. *Zeitschrift für Psychologie.* [https://doi.org/10.1027/2151-2604/a000357](https://doi.org/10.1027/2151-2604/a000357) Vuorre, M., & Curley, J. P. (2018). Curating research assets: A tutorial on the Git version control system. *Advances in methods and Practices in Psychological Science, 1*(2), 219-236. https://doi.org/10.1177/2515245918754826 Wacker, J. (1998). A definition of theory: research guidelines for different theory-building research methods in operations management. *Journal of Operations Management, 16*(4), 361–385. doi:10.1016/s0272-6963(98)00019-9 Wagenmakers, E. J., Wetzels, R., Borsboom, D., van der Maas, H. L., & Kievit, R. A. (2012). An agenda for purely confirmatory research. *Perspectives on Psychological Science, 7*(6), 632-638. [https://doi.org/10.1177/1745691612463078](https://doi.org/10.1177/1745691612463078) Wagenmakers, E.-J., Marsman, M., Jamil, T., Ly, A., Verhagen, J., Love, J., Selker, R., Gronau, Q. F., Šmíra, M., Epskamp, S., Matzke, D., Rouder, J. N., & Morey, R. D. (2018). Bayesian inference for psychology. Part I: Theoretical advantages and practical ramifications. *Psychonomic Bulletin & Review, 25*(1), 35–57. https://doi.org/10.3758/s13423-017-1343-3 Wagge, J. R., Baciu, C., Banas, K., Nadler, J. T., Schwarz, S., Weisberg, Y., ... & McCarthy, R. (2019). A demonstration of the collaborative replication and education project: Replication attempts of the red-romance effect. *Collabra: Psychology, 5*(1). https://doi.org/10.1525/collabra.177 Wason, P. C. (1960). On the failure to eliminate hypotheses in a conceptual task. *Quarterly Journal of Experimental Psychology, 12*(3), 129-140. https://doi.org/10.1080/17470216008416717 Wasserstein, R. L., & Lazar, N. A. (2016). The ASA statement on *p*\\-values: Context, process, and purpose. *The American Statistician, 70,* 129-133. [https://doi.org/10.1080/00031305.2016.1154108](https://doi.org/10.1080/00031305.2016.1154108) Webster, M.M., & Rutz, C. (2020). How STRANGE are your study animals? *Nature, 582,* 337–40. [https://doi.org/10.1038/d41586-020-01751-5](https://doi.org/10.1038/d41586-020-01751-5) Wendl, M. C. (2007). H-index: however ranked, citations need context. *Nature, 449(*7161), 403-403. https://doi.org/10.1038/449403b Whitaker, K., & Guest, O. (2020). \\#bropenscience is broken science. *The Psychologist, 33,* 34-37. Wicherts, J. M., Veldkamp, C. L., Augusteijn, H. E., Bakker, M., Van Aert, R., & Van Assen, M. A. (2016). Degrees of freedom in planning, running, analyzing, and reporting psychological studies: A checklist to avoid p-hacking. *Frontiers in psychology, 7,* 1832\\. [https://doi.org/10.3389/fpsyg.2016.01832](https://doi.org/10.3389/fpsyg.2016.01832) Wilkinson, M. D., Dumontier, M., Aalbersberg, I. J., Appleton, G., Axton, M., Baak, A., ... & Mons, B. (2016). The FAIR Guiding Principles for scientific data management and stewardship. *Scientific data, 3*(1), 1-9. https://doi.org/10.1038/sdata.2016.18 Wilson, B. & Fenner, M. (2012) Open Researcher & Contributor ID (ORCID): Solving the Name Ambiguity Problem. *Educause Review \\- E-Content 47*(3), 54-55. [https://er.educause.edu/articles/2012/5/open-researcher--contributor-id-orcid-solving-the-name-ambiguity-problem](https://er.educause.edu/articles/2012/5/open-researcher--contributor-id-orcid-solving-the-name-ambiguity-problem) Wilson, R. C., & Collins, A. G. (2019). Ten simple rules for the computational modeling of behavioral data. *Elife, 8,* e49547. https://doi.org/10.7554/eLife.49547 Wingen, T., Berkessel, J. B., & Englich, B. (2020). No Replication, No Trust? How Low Replicability Influences Trust in Psychology. *Social Psychological and Personality Science, 11*(4). https://doi.org/10.1177/1948550619877412 Woelfe, M., Olliaro, P., & Todd, M. H. (2011). Open science is a research accelerator. *Nature chemistry, 3*(10), 745-748. [https://doi.org/10.1038/nchem.1149](https://doi.org/10.1038/nchem.1149) Wren, J. D., Valencia, A., & Kelso, J. (2019). Reviewer-coerced citation: case report, update on journal policy and suggestions for future prevention. *Bioinformatics, 35* (18), 3217-3218. [https://doi.org/10.1093/bioinformatics/btz071](https://doi.org/10.1093/bioinformatics/btz071) Wuchty, S., Jones, B. F., & Uzzi, B. (2007). The increasing dominance of teams in production of knowledge. *Science, 316*(5827), 1036-1039. https://doi.org/10.1126/science.1136099 Xia, J., Harmon, J. L., Connolly, K. G., Donnelly, R. M., Anderson, M. R., & Howard, H. A. (2015). Who publishes in “predatory” journals?. *Journal of the Association for Information Science and Technology, 66*(7), 1406-1417. [https://doi.org/10.1002/asi.23265](https://doi.org/10.1002/asi.23265) Yamada, Y. (2018). How to crack pre-registration: Toward transparent and open science. *Frontiers in Psychology, 9*, 1831\\. [https://doi.org/10.3389/fpsyg.2018.01831](https://doi.org/10.3389/fpsyg.2018.01831) Yarkoni, T. (2020). The generalizability crisis. *Behavioral and Brain Sciences,* 1-37. https://doi.org/10.1017/S0140525X20001685 Yeung, S. K., Feldman, G., Fillon, A., Protzko, J., Elsherif, M. M., Xiao, Q., & Pickering, J. (2020a). Experimental Studies Meta-Analysis Registered Report Templates. \\[Manuscript in preparation\\]. Zurn, P., Bassett, D. S., & Rust, N. C. (2020). The Citation Diversity Statement: A Practice of Transparency, A Way of Life. *Trends in Cognitive Sciences, 24*(9), 669-672. https://doi.org/10.1016/j.tics.2020.06.009",
                "Translation": "** An open science repository where researchers can deposit research papers, reports, data sets, research software, and any other research-related digital artifacts. Zenodo creates a persistent digital object identifier (DOI) for each submission to make it citable. This platform was developed under the European OpenAIRE program and operated by CERN.",
                "Related_terms": "** DOI (digital object identifier); figshare; Open data; Open Science Framework; Preprint"
            }
        ]
    },
    {
        "german": [
            {
                "Title": "Abstract Bias (Abstract-Verzerrung) *",
                "Definition": "** The tendency to report only significant results in the abstract, while reporting non-significant results within the main body of the manuscript (not reporting non-significant results altogether would constitute selective reporting). The consequence of abstract bias is that studies reporting non-significant results may not be captured with standard meta-analytic search procedures (which rely on information in the title, abstract and keywords) and thus biasing the results of meta-analyses.",
                "Reference": "** Duyx et al. (2019)",
                "Originally drafted by": "** Ali H. Al-Hoorie",
                "Reviewed (or Edited) by": "Mahmoud Elsherif; Bethan Iley; Sam Parsons; Gerald Vineyard; Eliza Woodward; Flávio Azevedo",
                "Translated by": "** Susanne Vogel",
                "Translation reviewed by": "** Helena Hartmann, Jennifer Mattschey",
                "Translation": "Die Tendenz, nur signifikante Ergebnisse in der Zusammenfassung (Abstract) zu berichten, obwohl nicht-signifikante Ergebnisse im Hauptteil des Manuskripts berichtet werden (die nicht-signifikanten Ergebnisse gar nicht zu berichten wäre selektives Berichten (selective reporting)). Die Folge von Abstract Verzerrung ist, dass Studien mit nicht-signifikanten Ergebnissen möglicherweise durch übliche Suchstrategien für Metaanalysen (die auf Informationen in Titel, Abstract und Keywords beruhen) nicht gefunden werden und damit die Ergebnisse von Metaanalysen verfälscht (biased) werden.",
                "Related_terms": "** Cherry-picking; Publication bias (File Drawer Problem); Selective reporting"
            },
            {
                "Title": "Academic Impact (Akademischer Einfluss) *",
                "Definition": "** The contribution that a research output (e.g., published manuscript) makes in shifting understanding and advancing scientific theory, method, and application, across and within disciplines. Impact can also refer to the degree to which an output or research programme influences change outside of academia, e.g. societal and economic impact (cf. ESRC: [https://esrc.ukri.org/research/impact-toolkit/what-is-impact/](https://esrc.ukri.org/research/impact-toolkit/what-is-impact/)).",
                "Reference(s)": "** Anon (2021)",
                "Drafted by": "** Connor Keating",
                "Reviewed (or Edited) by": "** Myriam A. Baum; Adam Parker; Charlotte R. Pennington; Suzanne L. K. Stewart; Flávio Azevedo",
                "Translated by": "** Susanne Vogel",
                "Translation reviewed by": "** Helena Hartmann, Jennifer Mattschey",
                "Translation": "Den Einfluss, den ein Forschungsergebnis (z. B. ein publizierter Artikel) auf unser Verstehen von wissenschaftliche Theorien, Methoden und Anwendungen hat, und wie weit es dieses Verstehen vorantreibt, sowohl innerhalb von Disziplinen als auch disziplinübergreifend. Einfluss kann auch den Grad meinen, zu dem ein Forschungsergebnis oder ein Forschungsprogramm Veränderungen außerhalb der Wissenschaft beeinflusst, z. B. soziale oder wirtschaftliche Auswirkung (cf. ESRC: [https://esrc.ukri.org/research/impact-toolkit/what-is-impact/](https://esrc.ukri.org/research/impact-toolkit/what-is-impact/)).",
                "Related_terms": "** Beneficiaries; DORA; Reach; REF"
            },
            {
                "Title": "Accessibility (Barrierefreiheit)  *",
                "Definition": "** Accessibility refers to the ease of access and re-use of materials (e.g., data, code, outputs, publications) for academic purposes, particularly the ease of access is afforded to people with a chronic illness, disability and/or neurodivergence. These groups face numerous financial, legal and/or technical barriers within research, including (but not limited to) the acquisition of appropriately formatted materials and physical access to spaces. Accessibility also encompasses structural concerns about diversity, equity, inclusion, and representation (Pownall et al., 2021). Interfaces, events and spaces should be designed with accessibility in mind to ensure full participation, such as by ensuring that web-based images are colorblind friendly and have alternative text, or by using live captions at events (Brown et al., 2018; Pollet & Bond, 2021; World Wide Web Consortium, 2021).",
                "Reference(s)": "** Brown et al. (2018); Pollet and Bond (2021); Pownall et al. (2021); Suber (2004); World Wide Web Consortium (2021)",
                "Drafted by": "** Kai Krautter",
                "Reviewed (or Edited) by": "** Valeria Agostini; Myriam A. Baum; Mahmoud Elsherif; Bethan Iley; Tamara Kalandadze; Ryan Millager; Sara Middleton; Charlotte R. Pennington; Madeleine Pownall; Robert M. Ross; Flávio Azevedo",
                "Translated by": "** Helena Hartmann",
                "Translation reviewed by": "** Susanne Vogel, Jennifer Mattschey  ####",
                "Translation": "Barrierefreiheit (Accessibility) bezieht sich auf den einfachen Zugriff auf und die Wiederverwendung von Materialien (z. B. Daten, Code, Ergebnisse, Veröffentlichungen) für akademische Zwecke, wobei insbesondere Menschen mit einer chronischen Krankheit, einer Behinderung und/oder Neurodivergenz der Zugang erleichtert wird. Diese Gruppen sind mit zahlreichen finanziellen, rechtlichen und/oder technischen Hindernissen in der Forschung konfrontiert, einschließlich (aber nicht beschränkt auf) dem Erwerb von angemessen formatierten Materialien und dem physischen Zugang zu Räumen. Accessibility umfasst auch strukturelle Anliegen wie Diversität, Gleichberechtigung, Inklusion und Repräsentation (Pownall et al., 2021). Bei der Gestaltung von Benutzer:innenoberflächen, Veranstaltungen und Räumen sollte auf Barrierefreiheit geachtet werden, um eine uneingeschränkte Teilnahme zu gewährleisten. Beispielsweise kann sichergestellt werden, dass webbasierte Bilder erkennbar für farbenblinde Menschen sind und über Bildbeschreibungen verfügen, oder indem bei Veranstaltungen Live-Untertitel eingeblendet werden (Brown et al., 2018; Pollet & Bond, 2021; World Wide Web Consortium, 2021).",
                "Related_terms": "** Availability; Data availability statements; Inclusion; Open Access; Under-representation; Universal design for learning (UDL)"
            },
            {
                "Title": "Ad hominem bias (Ad hominem Verzerrung) *",
                "Definition": "** From Latin meaning “to the person”; Judgment of an argument or piece of work influenced by the characteristics of the person who forwarded it, not the characteristics of the argument itself. Ad hominem bias can be negative, as when work from a competitor or target of personal animosity is viewed more critically than the quality of the work merits, or positive, as when work from a friend benefits from overly favorable evaluation.",
                "Reference(s)": "** Barnes et al. (2018); Tvina et al. (2019)",
                "Drafted by": "** Mahmoud Elsherif",
                "Reviewed (or Edited) by": "** Bradley Baker; Filip Dechterenko; Bethan Iley; Madeleine Ingham; Graham Reid",
                "Translated by": "** Susanne Vogel",
                "Translation reviewed by": "** Helena Hartmann, Jennifer Mattschey",
                "Translation": "Aus dem Lateinischen “zum Menschen hin”; Bewertung eines Arguments oder einer Arbeit in Abhängigkeit von Merkmalen der Person, die diese hervorgebracht hat, und nicht von Merkmalen des Arguments selbst. Ad hominem Verzerrung (bias) kann negativ sein, z. B. wenn Arbeit eine:r Konkurrent:in oder einer Zielperson persönlicher Anfeindungen kritischer betrachtet wird als die Qualität der Arbeit dieser Person verdient, aber auch positiv, z. B. wenn Arbeit eine:r Freund:in von übertrieben positiver Bewertung profitiert.",
                "Related_terms": "** Peer review"
            },
            {
                "Title": "Adversarial collaboration (Gegnerische Kollaboration) *",
                "Definition": "** A collaboration where two or more researchers with opposing or contradictory theoretical views —and likely diverging predictions about study results— work together on one project. The aim is to minimise biases and methodological weaknesses as well as to establish a shared base of facts for which competing theories must account.",
                "Reference(s)": "** Bateman et al. (2005); Cowan et al. (2020); Kerr et al. (2018); Mellers et al. (2001); Rakow et al. (2014)",
                "Drafted by": "** Siu Kit Yeung",
                "Reviewed (or Edited) by": "** Matt Jaquiery; Aoife O’Mahony; Charlotte R. Pennington; Flávio Azevedo; Madeleine Pownall**;** Martin Vasilev",
                "Translated by": "** Susanne Vogel",
                "Translation reviewed by": "** Helena Hartmann, Jennifer Mattschey",
                "Translation": "Eine Kollaboration, bei der zwei oder mehr Forschende mit gegensätzlichen oder widersprüchlichen theoretischen Ansichten \\- und wahrscheinlich unterschiedlichen Vorhersagen über die Ergebnisse einer Studie \\- an einem Projekt zusammenarbeiten. Das Ziel ist, Verzerrung (bias) und methodische Schwächen zu minimieren und eine gemeinsame Basis an Fakten zu erarbeiten, die von gegensätzlichen Theorien berücksichtigt werden muss.",
                "Related_terms": "** Collaboration; Many Analysts; Many Labs; Preregistration; Publication bias (File Drawer Problem)"
            },
            {
                "Title": "Adversarial (collaborative) commentary (Gegnerischer (kollaborativer) Kommentar) *",
                "Definition": "** A commentary in which the original authors of a work and critics of said work collaborate to draft a consensus statement. The aim is to draft a commentary that is free of ad hominem attacks and communicates a common understanding or at least identifies where both parties agree and disagree. In doing so, it provides a clear take-home message and path forward, rather than leaving the reader to decide between opposing views conveyed in separate commentaries.",
                "Reference(s)": "** Heyman et al. (2020); Rabagliati et al. (2019); Silberzahn et al. (2014)",
                "Drafted by": "** Steven Verheyen",
                "Reviewed (or Edited) by": "** Mahmoud Elsherif; Sam Parsons; Emma Henderson; Michele C. Lim; Flávio Azevedo",
                "Translated by": "** Susanne Vogel",
                "Translation reviewed by": "** Helena Hartmann, Jennifer Mattschey",
                "Translation": "Ein Kommentar, bei dem die ursprünglichen Autor:innen eines Werks und die Kritiker:innen dieses Werks zusammenarbeiten, um eine Konsenserklärung zu verfassen. Ziel ist es, einen Kommentar zu schreiben, der frei von persönlichen (ad hominem) Angriffen ist und ein gemeinsames Verständnis beschreibt oder zumindest aufzeigt, wo beide Parteien übereinstimmen und wo nicht. Auf diese Weise wird ein klares Fazit vermittelt und ein zukünftiger Weg aufgezeigt, anstatt die Lesenden zwischen gegensätzlichen Ansichten entscheiden zu lassen, die in separaten Kommentaren dargelegt werden.",
                "Related_terms": "** Adversarial collaboration; Collaborative commentary"
            },
            {
                "Title": "Affiliation bias (Affiliations-Verzerrung) *",
                "Definition": "** This bias occurs when one’s opinions or judgements about the quality of research are influenced by the affiliation of the author(s). When publishing manuscripts, a potential example of an affiliation bias could be when editors prefer to publish work from prestigious institutions (Tvina et al., 2019).",
                "Reference(s)": "** Tvina et al. (2019)",
                "Drafted by": "** Mahmoud Elsherif",
                "Reviewed (or Edited) by": "** Christopher Graham; Madeleine Ingham; Adam Parker; Graham Reid",
                "Translated by": "** Susanne Vogel",
                "Translation reviewed by": "** Helena Hartmann, Jennifer Mattschey",
                "Translation": "Diese Art Verzerrung (bias) tritt auf, wenn Meinungen über die Qualität einer Arbeit oder deren Bewertung durch die Affiliation der Autor:innen beeinflusst werden. Ein mögliches Beispiel für Affiliations-Verzerrung bei der Publikation von Manuskripten wäre, wenn Herausgeber:innen beim Publizieren die Arbeiten von Autor:innen besonders angesehener Institutionen bevorzugen würden.",
                "Related_terms": "** Peer review"
            },
            {
                "Title": "Aleatoric uncertainty (Aleatorische Unsicherheit) *",
                "Definition": "** Variability in outcomes due to unknowable or inherently random factors. The stochastic component of outcome uncertainty that cannot be reduced through additional sources of information. For example, when flipping a coin, uncertainty about whether it will land on heads or tails.",
                "Reference(s)": "** Der Kiureghian and Ditlevsen (2009)",
                "Drafted by": "** Bradley Baker",
                "Reviewed (or Edited) by": "** Nihan Albayrak-Aydemir**;** Brett Gall; Magdalena Grose-Hodge; Bethan Iley; Charlotte R. Pennington",
                "Translated by": "** Helena Hartmann",
                "Translation reviewed by": "** Susanne Vogel, Jennifer Mattschey",
                "Translation": "Die Variabilität von Ergebnissen aufgrund von unbekannten oder inhärent zufälligen Faktoren. Die stochastische Komponente der Ergebnisunsicherheit, die nicht durch zusätzliche Informationsquellen reduziert werden kann. Ein Beispiel: Beim Werfen einer Münze ist ungewiss, ob sie auf Kopf oder Zahl fällt.",
                "Related_terms": "** Epistemic uncertainty; Knightian uncertainty"
            },
            {
                "Title": "Altmetrics *",
                "Definition": "** Departing from traditional citation measures, altmetrics (short for “alternative metrics”) provide an assessment of the attention and broader impact of research work based on diverse sources such as social media (e.g. Twitter), digital news media, number of preprint downloads, etc. Altmetrics have been criticized in that sensational claims usually receive more attention than serious research (Ali, 2021).",
                "Reference(s)": "** Ali (2021); Galligan and Dyas-Correia (2013)",
                "Originally drafted by": "** Mirela Zaneva",
                "Reviewed (or Edited) by": "** Ali H. Al-Hoorie; Charlotte R. Pennington; Birgit Schmidt; Flávio Azevedo",
                "Translated by": "** Susanne Vogel",
                "Translation reviewed by": "** Helena Hartmann, Jennifer Mattschey  ### ---",
                "Translation": "Ausgehend von traditionellen Zitationsmetriken bieten Altmetrics (kurz für den englischen Ausdruck \"alternative metrics\") eine Bewertung der Aufmerksamkeit und der breiteren Bedeutung von Forschungsarbeiten auf der Grundlage verschiedener Quellen. Diese Quellen können soziale Medien umfassen (z. B. Twitter/X), digitale Nachrichtenkanäle, die Anzahl der Downloads von Preprints usw. Altmetrics sind dafür kritisiert worden, dass sensationelle Behauptungen in der Regel mehr Aufmerksamkeit erhalten als seriöse Forschung (Ali, 2021).",
                "Related_terms": "** Academic impact; Alternative metrics; Bibliometrics; H-index; Impact assessment; Journal impact factor"
            },
            {
                "Title": "AMNESIA *",
                "Definition": "** AMNESIA is a free anonymization tool to remove identifying information from data. After uploading a dataset that contains personal data, the original dataset is transformed by the tool, resulting in a dataset that is anonymized regarding personal and sensitive data.",
                "Reference(s)": "** [https://amnesia.openaire.eu/](https://amnesia.openaire.eu/)",
                "Originally drafted by": "** Norbert Vanek",
                "Reviewed (or Edited) by": "** Ali H. Al-Hoorie; Myriam A. Baum; Charlotte R. Pennington",
                "Translated by": "** Helena Hartmann",
                "Translation reviewed by": "** Susanne Vogel, Jennifer Mattschey",
                "Translation": "AMNESIA ist ein kostenloses Anonymisierungswerkzeug zum Entfernen von persönlich identifizierenden Informationen aus Daten. Nach dem Hochladen eines Datensatzes, der personenbezogene Daten enthält, wird der ursprüngliche Datensatz mithilfe von AMNESIA umgewandelt. Dies führt zu einem Datensatz, der hinsichtlich personenbezogener und sensibler Daten anonymisiert ist.",
                "Related_terms": "** Anonymity; Confidentiality; Research ethics"
            },
            {
                "Title": "Analytic Flexibility (Analytische Flexibilität) *",
                "Definition": "** Analytic flexibility is a type of researcher degrees of freedom (Simmons, Nelson, & Simonsohn, 2011\\) that refers specifically to the large number of choices made during data preprocessing and statistical analysis. “\\[T\\]he range of analysis outcomes across different acceptable analysis methods” (Carp, 2012, p. 1). Analytic flexibility can be problematic, as this variability in analytic strategies can translate into variability in research outcomes, particularly when several strategies are applied, but not transparently reported (Masur, 2021).",
                "Reference(s)": "** Breznau et al. (2021); Carp (2012); Jones et al. (2020); Masur (2021); Simmons et al. (2011)",
                "Originally drafted by": "** Mariella Paul",
                "Reviewed (or Edited) by": "** Adrien Fillon; Bettina M. J . Kern; Adam Parker; Charlotte R. Pennington; Flávio Azevedo",
                "Translated by": "** Susanne Vogel",
                "Translation reviewed by": "** Helena Hartmann, Jennifer Mattschey",
                "Translation": "Analytische Flexibilität beschreibt eine Art der Freiheitsgrade von Forschenden (Simmons, Nelson, & Simonsohn, 2011), die sich speziell auf die große Anzahl von Entscheidungen bezieht, die während der Datenvorverarbeitung und Datenauswertung getroffen werden. “\\[T\\]he range of analysis outcomes across different acceptable analysis methods” (dt. \\[D\\]ie Bandbreite an Analyseergebnissen über verschiedene akzeptable Analysemethoden; Carp, 2012, p. 1). Analytische Flexibilität kann problematisch sein, da diese Variabilität bei Datenanalyse-Strategien zu einer Variabilität der Forschungsergebnisse führen kann, insbesondere wenn mehrere Strategien angewandt, aber nicht transparent veröffentlicht werden (Masur, 2021).",
                "Related_terms": "** Garden of forking paths; Multiverse analysis; Researcher degrees of freedom"
            },
            {
                "Title": "Anonymity (Anonymität) *",
                "Definition": "** Anonymising data refers to removing, generalising, aggregating or distorting any information which may potentially identify participants, peer-reviewers, and authors, among others. Data should be anonymised so that participants are not personally identifiable. The most basic level of anonymisation is to replace participants’ names with pseudonyms (fake names) and remove references to specific places. Anonymity is particularly important for open data and data may not be made open for anonymity concerns. Anonymity and open data has been discussed within qualitative research which often focuses on personal experiences and opinions, and in quantitative research that includes participants from clinical populations.",
                "Reference": "** Braun and Clarke (2013)",
                "Originally drafted by": "** Claire Melia",
                "Reviewed (or Edited) by": "** Tsvetomira Dumbalska; Bethan Iley; Tamara Kalandadze; Bettina M.J. Kern; Sam Parsons; Charlotte R. Pennington; Flávio Azevedo; Madeleine Pownall; Birgit Schmidt",
                "Translated by": "** Susanne Vogel",
                "Translation reviewed by": "** Helena Hartmann, Jennifer Mattschey",
                "Translation": "Das Anonymisieren von Daten meint das Entfernen, Generalisieren, Zusammenfassen oder Verzerren von jeglicher Information, die möglicherweise Versuchspersonen, Gutachter:innen, Autor:innen oder andere Personen identifizieren kann. Daten sollten so anonymisiert sein, dass Versuchspersonen nicht identifizierbar sind. Die grundlegendste Art zu anonymisieren ist es, die Namen der Versuchspersonen mit Pseudonymen (künstlichen Namen) zu ersetzen und Bezüge zu konkreten Orten zu entfernen. Anonymität ist besonders wichtig für offene Daten (Datenveröffentlichungen) und Daten können möglicherweise nicht veröffentlicht werden, sollte es Bedenken bezüglich ihrer Anonymität geben. Anonymität und offene Daten wurden sowohl für qualitative Forschung diskutiert, die oft persönliche Erfahrungen und Meinungen untersucht, als auch für quantitative Forschung mit Versuchspersonen aus klinischen Stichproben.",
                "Related_terms": "** Anonymising; Clinical populations; Confidentiality; Research ethics; Research participants; Vulnerable population"
            },
            {
                "Title": "ARRIVE Guidelines  (ARRIVE Leitlinien) *",
                "Definition": "** The ARRIVE guidelines (Animal Research: Reporting of In Vivo Experiments) are a checklist-based set of reporting guidelines developed to improve reporting standards, and enhance replicability, within living (i.e. *in vivo*) animal research. The second generation ARRIVE guidelines, ARRIVE 2.0, were released in 2020\\. In these new guidelines, the clarity has been improved, items have been prioritised and new information has been added with an accompanying “Explanation” and “Elaboration” document to provide a rationale for each item and a recommended set to add context to the study being described.",
                "Reference(s)": "** Percie du Sert et al. (2020)",
                "Drafted by": "** Ben Farrar",
                "Reviewed (or Edited) by": "** Jamie P. Cockcroft; Mahmoud Elsherif; Elias Garcia-Pelegrin; Helena Hartmann; Wanyin Li; Charlotte R. Pennington",
                "Translated by": "** Helena Hartmann",
                "Translation reviewed by": "** Susanne Vogel, Jennifer Mattschey",
                "Translation": "Die ARRIVE-Leitlinien bzw. \\-Guidelines (Animal Research: Reporting of In Vivo Experiments) sind ein auf Checklisten basierender Leitfaden für die Berichterstattung, der entwickelt wurde, um Veröffentlichungsstandards zu verbessern und die Reproduzierbarkeit in der Forschung an lebenden Tieren (d. h. in vivo) zu erhöhen. Die zweite Generation der ARRIVE-Leitlinien, ARRIVE 2.0, wurde im Jahr 2020 veröffentlicht. In diesen neuen Leitlinien wurde die Verständlichkeit verbessert, die Punkte wurden nach Prioritäten sortiert, und es wurden neue Informationen mit einem begleitenden \"Erklärungs-\" (Explanation)  und \"Elaborations-\" (Elaboration) Dokument hinzugefügt, um eine Begründung für jeden einzelnen Punkt zu liefern, sowie ein Vorschlag, um den Kontext der beschriebenen Studie ergänzen.",
                "Related_terms": "** PREPARE Guidelines; Reporting Guideline; STRANGE"
            },
            {
                "Title": "Article Processing Charge (Artikelverarbeitungsgebühr) *",
                "Definition": "** An article (sometimes author) processing charge (APC) is a fee charged to authors by a publisher in exchange for publishing and hosting an open access article. APCs are often intended to compensate for a potential loss of revenue the journal may experience when moving from traditional publication models, such as subscription services or pay-per-view, to open access. While some journals charge only about US$300, APCs vary widely, from US$1000 (Advances in Methods and Practice in Psychological Science) or less to over US$10,000 (Nature). While some publishers offer waivers for researchers from certain regions of the world or who lack funds, some APCs have been criticized for being disproportionate compared to actual processing and hosting costs (Grossmann & Brembs, 2021\\) and for creating possible inequities with regard to which scientists can afford to make their works freely available (Smith et al. 2020).",
                "Reference": "** Grossmann and Brembs (2021); Smith et al. (2020)",
                "Originally drafted by": "** Nick Ballou",
                "Reviewed (or Edited) by": "** Ali H. Al-Hoorie; Bethan Iley; Flávio Azevedo; Robert Ross; Tobias Wingen",
                "Translated by": "** Susanne Vogel",
                "Translation reviewed by": "** Helena Hartmann, Jennifer Mattschey \\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_",
                "Translation": "Eine Artikel- oder Autor:innen-Verarbeitungsgebühr (article processing charge, APC) ist eine Gebühr, die ein Verlag von Autor:innen für die Veröffentlichung und Bereitstellung eines frei verfügbaren (open access) Artikels verlangt. APCs sind oft dazu gedacht, mögliche Einnahmeverluste auszugleichen, die der Zeitschrift entstehen, wenn sie von traditionellen Publikationsmodellen wie Abonnements oder Pay-per-View (das Bezahlen einzelner Artikel durch Lesende) auf Open Access umstellt. Während einige Zeitschriften nur etwa 300 US-Dollar fordern, variieren die APCs stark und reichen von 1000 US-Dollar (Advances in Methods and Practice in Psychological Science; checked Dec 2023\\) oder weniger bis zu über 10.000 US-Dollar (Nature; checked Dec 2023). Während einige Verlage Forschenden aus bestimmten Weltregionen oder mit geringen finanziellen Mitteln eine Gebührenbefreiung gewähren, wurden einige APCs dafür kritisiert, dass sie im Vergleich zu den tatsächlichen Bearbeitungs- und Bereitstellungs-Kosten unverhältnismäßig hoch sind (Grossmann & Brembs, 2021). Damit schaffen sie möglicherweise Ungleichheiten in Bezug darauf, welche Forschenden es sich leisten können, ihre Werke offen zugänglich zu machen (Smith et al. 2020).",
                "Related_terms": "** Open Access; Under-representation"
            },
            {
                "Title": "Authorship (Autor:innenschaft) *",
                "Definition": "** Authorship assigns credit for research outputs (e.g. manuscripts, data, and software) and accountability for content (McNutt et al. 2018; Patience et al. 2019). Conventions differ across disciplines, cultures, and even research groups, in their expectations of what efforts earn authorship, what the order of authorship signifies (if anything), how much accountability for the research the corresponding author assumes, and the extent to which authors are accountable for aspects of the work that they did not personally conduct.",
                "Reference(s)": "** ALLEA (2017); German Research Foundation (2019); McNutt et al. (2018); Patience et al. (2019)",
                "Drafted by": "** Jacob Miranda",
                "Reviewed (or Edited) by": "** Bradley Baker; Brett J. Gall; Matt Jaquiery; Charlotte R. Pennington; Flávio Azevedo; Birgit Schmidt; Yuki Yamada",
                "Translated by": "** Helena Hartmann",
                "Translation reviewed by": "** Susanne Vogel, Jennifer Mattschey",
                "Translation": "Mit der Autor:innenschaft (Authorship) werden Forschungsergebnisse (z. B. Manuskripte, Daten und Software) anerkannt und die Verantwortlichkeit für den Inhalt geregelt (McNutt et al. 2018; Patience et al. 2019). Die Konventionen unterscheiden sich zwischen verschiedenen Disziplinen, Kulturen und sogar Forschungsgruppen. Es gibt unterschiedliche Ansichten welche Beiträge zur Forschung Autor:innenschaft verdienen, was die Reihenfolge von Autor:innen bedeutet (wenn überhaupt etwas), wie viel Verantwortung für die Forschung der/die Korrespondenzautor:in (Corresponding Author) übernimmt und inwieweit die Autor:innen für Aspekte der Arbeit verantwortlich sind, die sie selbst nicht persönlich durchgeführt haben.",
                "Related_terms": "** Co-authorship; Consortium authorship; Contributorship; CRediT; First-last-author-emphasis norm (FLAE); Gift (or Guest) Authorship; Sequence-determines-credit approach (SDC)"
            },
            {
                "Title": "Auxiliary Hypothesis (Hilfshypothese) *",
                "Definition": "** All theories contain assumptions about the nature of constructs and how they can be measured. However, not all predictions are derived from theories and assumptions can sometimes be drawn from other premises. Additional assumptions that are made to deduce a prediction and tested by making links to observable data. These auxiliary hypotheses are sometimes invoked to explain why a replication attempt has failed.",
                "Reference(s)": "** Dienes (2008); Lakatos (1978)",
                "Drafted by": "** Alaa Aldoh",
                "Reviewed (or Edited) by": "** Ali H. Al-Hoorie; Nihan Albayrak-Aydemir; Mahmoud Elsherif; Bethan Iley; Sam Parsons; Flávio Azevedo",
                "Translated by": "** Helena Hartmann",
                "Translation reviewed by": "** Susanne Vogel, Jennifer Mattschey  ### **B** {#b}",
                "Translation": "Alle Theorien enthalten Annahmen über die Art von Konstrukten und wie sie gemessen werden können. Allerdings werden nicht alle Vorhersagen aus Theorien abgeleitet, und Annahmen können manchmal aus anderen Prämissen abgeleitet werden. Zusätzliche Annahmen, die getroffen werden, um eine Vorhersage abzuleiten, werden durch Verknüpfung mit beobachtbaren Daten geprüft. Diese Hilfshypothesen werden manchmal herangezogen, um zu erklären, warum ein Replikationsversuch fehlgeschlagen ist.",
                "Related_terms": "** Epistemic uncertainty; Hypothesis; Statistical assumptions; Hidden moderators"
            },
            {
                "Title": "Badges (Open Science) (Open Science Abzeichen) *",
                "Definition": "** Badges are symbols that editorial teams add to published manuscripts to acknowledge open science practices and act as incentives for researchers to share data, materials, or to embed study preregistration. As clearly-visible symbols, they are intended to signal to the reader that content has met the standard of open research required to receive the badge (typically from that journal). Different badges may be assigned for different practices, such as research having been made available and accessible in a persistent location (“open material badge” and “open data badge”), or study preregistration (“preregistration badge”).",
                "Reference(s)": "** Hardwicke et al. (2020); Kidwell et al. (2016); Rowhani-Farid et al. (2020); Science (n.d.)",
                "Drafted by": "** Jacob Miranda",
                "Reviewed (or Edited) by": "** Brett Gall; Helena Hartmann; Mariella Paul; Charlotte R. Pennington; Lisa Spitzer; Suzanne L. K. Stewart",
                "Translated by": "** Susanne Vogel",
                "Translation reviewed by": "** Helena Hartmann, Jennifer Mattschey",
                "Translation": "Anzeichen (badges) sind Symbole, die Herausgeber:innen veröffentlichten Manuskripten hinzufügen, um Open-Science-Praktiken anzuerkennen und die als Anreiz für Forschende dienen, Daten und Materialien zu teilen oder ihre Studien zu präregistrieren. Als deutlich sichtbare Abzeichen sollen sie den Lesenden signalisieren, dass der Inhalt die Open Science-Anforderungen (in der Regel von dieser Zeitschrift) erfüllt, die für die Verleihung des Abzeichens erforderlich sind. Für unterschiedliche Open Science-Praktiken können verschiedene Abzeichen vergeben werden, z. B. für die Bereitstellung und Zugänglichkeit von Forschungsmaterialien und \\-ergebnissen an einem permanenten Ort (\"Open Material Badge\" und \"Open Data Badge\") oder für die Präregistrierung von Studien (\"Preregistration Badge\").",
                "Related_terms": "** Incentives; Open Data badge; Preregistration; Triple badge"
            },
            {
                "Title": "Bayes Factor (Bayes-Faktor) *",
                "Definition": "** A continuous statistical measure for model selection used in Bayesian inference, describing the *relative* evidence for one model over another, regardless of whether the models are correct. Bayes factors (BF) range from 0 to infinity, indicating the relative strength of the evidence, and where 1 is a neutral point of no evidence. In contrast to *p*\\-values, Bayes factors allow for 3 types of conclusions: a) evidence for the alternative hypothesis, b) evidence for the null hypothesis, and c) no sufficient evidence for either. Thus, BF are typically expressed as BF10 for evidence regarding the alternative compared to the null hypothesis, and as BF01 for evidence regarding the null compared to the alternative hypothesis.",
                "Reference": "** Hoijtink et al. (2019) Makowski et al. (2019)",
                "Originally drafted by": "** Meng Liu",
                "Reviewed (or Edited) by": "** Alaa AlDoh; Helena Hartmann; Connor Keating; Kai Krautter; Michele C. Lim; Suzanne L. K. Stewart; Ana Todorovic",
                "Translated by": "** Helena Hartmann",
                "Translation reviewed by": "** Susanne Vogel, Jennifer Mattschey",
                "Translation": "Ein kontinuierliches statistisches Maß für die Modellauswahl bei der Bayes'schen Inferenz. Es beschreibt die *relative* Evidenz für ein Modell im Vergleich zu einem anderen, unabhängig davon, ob die Modelle korrekt sind. Bayes-Faktoren (BF) reichen von 0 bis unendlich und geben die relative Stärke der Evidenz an, wobei 1 ein neutraler Punkt ohne jegliche Evidenz ist. Im Gegensatz zu p-Werten lassen Bayes-Faktoren drei Arten von Schlussfolgerungen zu: a) Belege für die Alternativhypothese, b) Belege für die Nullhypothese und c) keine ausreichenden Belege für beide Hypothesen. Daher werden BFs in der Regel als BF10 bei Evidenz für die Alternativhypothese im Vergleich zur Nullhypothese und als BF01 bei Evidenz für die Nullhypothese im Vergleich zur Alternativhypothese ausgedrückt.",
                "Related_terms": "** Bayesian inference; Bayesian statistics; Likelihood function; Null Hypothesis Significance Testing (NHST); *p*\\-value"
            },
            {
                "Title": "Bayesian Inference (Bayessche Inferenz) *",
                "Definition": "** A method of statistical inference based upon Bayes’ theorem, which makes use of epistemological (un)certainty using the mathematical language of probability. Bayesian inference is based on allocating (and reallocating, based on newly-observed data or evidence) credibility across possibilities. Two existing approaches to Bayesian inference include “Bayes factors” (BF) and Bayesian parameter estimation.",
                "Reference": "** Dienes (2011; 2014; 2016); Etz et al. (2018); Kruschke (2015); McElreath (2020); Wagenmakers et al. (2018)",
                "Originally drafted by": "** Charlotte R. Pennington",
                "Reviewed (or Edited) by": "** Alaa Aldoh; Bradley Baker; Robert Ross; Markus Weinmann; Tobias Wingen; Steven Verheyen",
                "Translated by": "** Susanne Vogel",
                "Translation reviewed by": "** Helena Hartmann, Jennifer Mattschey",
                "Translation": "Eine Methode der statistischen Inferenz oder Schlussfolgerung auf der Grundlage des Bayes'schen Theorems, welche die wissenschaftstheoretische (Un-)Sicherheit unter Zuhilfenahme der Wahrscheinlichkeit nutzt. Die Bayes'sche Schlussfolgerung basiert auf der Zuweisung (und Neuzuweisung, basierend auf neu beobachteten Daten oder gesammelter Evidenz) der Glaubhaftigkeit zu verschiedenen Möglichkeiten. Zwei bestehende Ansätze für Bayes'sche Schlussfolgerungen sind die \"Bayes-Faktoren\" (BF) und die Bayes'sche Parameterschätzung.",
                "Related_terms": "** Bayes Factor; Bayesian statistics; Bayesian Parameter Estimation"
            },
            {
                "Title": "Bayesian Parameter Estimation (Bayes’sche Parameter-Schätzung) *",
                "Definition": "** A Bayesian approach to estimating parameter values by updating a prior belief about model parameters (i.e., prior distribution) with new evidence (i.e., observed data) via a likelihood function, resulting in a posterior distribution. The posterior distribution may be summarised in a number of ways including: point estimates (mean/mode/median of a posterior probability distribution), intervals of defined boundaries, and intervals of defined mass (typically referred to as a credible interval). In turn, a posterior distribution may become a prior distribution in a subsequent estimation. A posterior distribution can also be sampled using Monte-Carlo Markov Chain methods which can be used to determine complex model uncertainties (e.g. Foreman-Mackey et al., 2013).",
                "Reference": "** Foreman-Mackey et al. (2013); McElreath (2020); Press (2007); [https://blog.stata.com/2016/11/15/introduction-to-bayesian-statistics-part-2-mcmc-and-the-metropolis-hastings-algorithm/](https://blog.stata.com/2016/11/15/introduction-to-bayesian-statistics-part-2-mcmc-and-the-metropolis-hastings-algorithm/)",
                "Originally drafted by": "** Alaa Aldoh",
                "Reviewed (or Edited) by": "** Mahmoud Elsherif; Helena Hartmann; Dominik Kiersz; Meng Liu; Ana Todorovic; Markus Weinmann",
                "Translated by": "** Helena Hartmann",
                "Translation reviewed by": "** Susanne Vogel, Jennifer Mattschey",
                "Translation": "Ein Bayes’scher Ansatz zur Schätzung von Parameterwerten (Bayesian Parameter Estimation), bei dem eine vorherige Annahme über Modellparameter (d. h. die vorherige \"Prior\" Verteilung) mit neuen Belegen (d. h. beobachteten Daten) über eine Wahrscheinlichkeitsfunktion aktualisiert wird. Dies führt zu einer posterioren Verteilung. Die resultierende (\"Posterior\") Verteilung kann auf verschiedene Weise zusammengefasst werden: Punktschätzungen (Mittelwert/Modus/Median einer posterioren Wahrscheinlichkeitsverteilung), Intervalle mit definierten Grenzen und Intervalle mit definierter Masse (typischerweise als credible interval, d. h. glaubwürdiges Intervall bezeichnet). Eine Posterior-Verteilung kann wiederum in einer nachfolgenden Schätzung zu einer neuen Prior-Verteilung zusammengefasst werden. Eine Posterior-Verteilung kann auch mit Hilfe von Monte-Carlo-Markov-Ketten-Methoden untersucht werden, die zur Bestimmung komplexer Modellunsicherheiten verwendet werden können (z. B. Foreman-Mackey et al., 2013).",
                "Related_terms": "** Bayes Factor; Bayesian inference; Bayesian statistics; Null Hypothesis Significance Testing (NHST)"
            },
            {
                "Title": "BIDS data structure (BIDS Datenstruktur) *",
                "Definition": "** The Brain Imaging Data Structure (BIDS) describes a simple and easy-to-adopt way of organizing neuroimaging, electrophysiological, and behavioral data (i.e., file formats, folder structures). BIDS is a community effort developed *by* the community *for* the community and was inspired by the format used internally by the OpenfMRI repository known as [OpenNeuro](https://openneuro.org). Having initially been developed for fMRI data, the BIDS data structure has been extended for many other measures, such as EEG (Pernet et al., 2019).",
                "Reference(s)": "** Gorgolewski et al. (2016); [https://bids.neuroimaging.io/](https://bids.neuroimaging.io/)",
                "Drafted by": "** Tina Lonsdorf",
                "Reviewed (or Edited) by": "** Ali H. Al-Hoorie; David Moreau; Mariella Paul; Charlotte R. Pennington",
                "Translated by": "** Helena Hartmann",
                "Translation reviewed by": "** Susanne Vogel, Jennifer Mattschey",
                "Translation": "Die Brain Imaging Data Structure (BIDS) beschreibt eine einfache und leicht zu implementierende Methode zur Organisation von neuronalen, elektrophysiologischen und Verhaltensdaten (d. h. Dateiformate, Ordnerstrukturen). BIDS ist ein Gemeinschaftsprojekt, das *von* der Gemeinschaft *für* die Gemeinschaft entwickelt wurde. Es wurde von dem Format inspiriert, das intern im OpenfMRI-Repositorium (auch bekannt als [OpenNeuro](https://openneuro.org)) verwendet wird. Obwohl ursprünglich für fMRT-Daten entwickelt, wurde die BIDS-Datenstruktur bereits für viele andere Maße, wie z. B. EEG, erweitert (Pernet et al., 2019).",
                "Related_terms": "** Open Data"
            },
            {
                "Title": "BIZARRE *",
                "Definition": "** This acronym refers to Barren, Institutional, Zoo, and other Rare Rearing Environments (BIZARRE). Most research for chimpanzees is conducted on this specific sample. This limits the generalizability of a large number of research findings in the chimpanzee population. The BIZARRE has been argued to reflect the universal concept of what is a chimpanzee (see also WEIRD, which has been argued to be a universal concept for what is a human).",
                "Reference(s)": "** Clark et al. (2019); Leavens et al. (2010)",
                "Originally drafted by": "** Mahmoud Elsherif",
                "Reviewed (or Edited) by": "** Zoe Flack; Charlotte R. Pennington",
                "Translated by": "** Susanne Vogel",
                "Translation reviewed by": "** Helena Hartmann, Jennifer Mattschey  ### ---",
                "Translation": "Dieses Akronym steht für “Barren, Institutional, Zoo, and other Rare Rearing Environments” (BIZARRE, dt.: karg, institutionell, Zoo und andere ungewöhnliche Umgebungen der Aufzucht). Die meisten Forschungsarbeiten über Schimpansen werden an dieser speziellen Stichprobe durchgeführt. Dies schränkt die Generalisierbarkeit vieler Forschungsergebnisse an der Schimpansenpopulation ein. Es wurde diskutiert, ob BIZARRE das universelle Konzept des Schimpansen widerspiegelt (siehe auch WEIRD, wo ebenfalls diskutiert wurde, ob dieses als universelles Konzept für den Menschen gilt).",
                "Related_terms": "** Populations; STRANGE; WEIRD"
            },
            {
                "Title": "Bottom-up approach (to Open Scholarship) (Bottom-Up Ansatz) *",
                "Definition": "** Within academic culture, an approach focusing on the intrinsic interest of academics to improve the quality of research and research culture, for instance by making it supportive, collaborative, creative and inclusive. Usually indicates leadership from early-career researchers acting as the changemakers driving shifts and change in scientific methodology through enthusiasm and innovation, compared to a “top-down” approach initiated by more senior researchers \"Bottom-up approaches take into account the specific local circumstances of the case itself, often using empirical data, lived experience, personal accounts, and circumstances as the starting point for developing policy solutions.\"",
                "Reference(s)": "** Button et al. (2016); Button et al. (2020); Hart and Silka (2020); Meslin (2010); Moran et al. (2020); [https://www.cos.io/blog/strategy-for-culture-change](https://www.cos.io/blog/strategy-for-culture-change)",
                "Drafted by": "** Catherine Laverty",
                "Reviewed (or Edited) by": "** Helena Hartmann; Michele C. Lim; Adam Parker; Charlotte R. Pennington; Birgit Schmidt; Marta Topor; Flávio Azevedo",
                "Translated by": "** Helena Hartmann",
                "Translation reviewed by": "** Susanne Vogel, Jennifer Mattschey",
                "Translation": "In der akademischen Welt ein Ansatz, der sich auf das intrinsische Interesse von Akademiker:innen konzentriert, die Qualität der Forschung und der Forschungskultur zu verbessern, z. B. indem man sie unterstützend, kooperativ, kreativ und inklusiv gestaltet. In der Regel geht die Führung von Nachwuchswissenschaftler:innen aus, die durch Enthusiasmus und Innovation Veränderungen in wissenschaftlicher Methodik vorantreiben, im Gegensatz zu einem \"Top-down\"-Ansatz, der von erfahrenen Forscher:innen initiiert wird. \"Bottom-up approaches take into account the specific local circumstances of the case itself, often using empirical data, lived experience, personal accounts, and circumstances as the starting point for developing policy solutions.\" (dt. Bottom-up-Ansätze berücksichtigen die spezifischen lokalen Umstände des jeweiligen Falles und verwenden oft empirische Daten, direkte Erfahrungen, persönliche Berichte und Umstände als Ausgangspunkt für die Entwicklung politischer Lösungen; Meslin, 2010, p. 208\\)",
                "Related_terms": "** Early Career Researchers (ECRs); Grassroot initiatives"
            },
            {
                "Title": "Boundary condition (Randbedingungen)! New term - comments needed !**",
                "Definition": "Theories provide answers to what, how and why questions of a specific phenomenon. The ‘What’ refers to the variables involved in a causal model, The ‘how’  describes how the effects relate the variables to one another and ‘Why’ identifies the mechanisms explaining the relationship between these variables. However, boundary condition refers to the who, where and when features of a theory (Whetten, 1989). Put simply, these conditions relate to the values and positionality of the researcher, spatial and temporal boundaries (Bacharach, 1989\\) and the limits of generalisability of a theory (Whetten, 1989).",
                "Originally drafted by": "Mahmoud Elsherif",
                "Reviewed (or Edited) by": "Shijun Yu",
                "Translated by": "** Susanne Vogel",
                "Translation reviewed by": "** Helena Hartmann, Jennifer Mattschey",
                "Translation": "Theorien geben Antworten auf die Fragen nach dem Was, Wie und Warum eines bestimmten Phänomens. Das \"Was\" bezieht sich auf die an einem kausalen Modell beteiligten Variablen, das \"Wie\" beschreibt, wie die Effekte die Variablen zueinander in Beziehung setzen, und das \"Warum\" zeigt die Mechanismen auf, die die Beziehungen zwischen diesen Variablen erklären. Randbedingungen (boundary conditions) hingegen beziehen sich auf das Wer, Wo und Wann einer Theorie (Whetten, 1989). Einfach ausgedrückt beziehen sich diese Bedingungen auf die Werte und die Position der Forschenden, die räumlichen und zeitlichen Grenzen (Bacharach, 1989\\) und die Grenzen der Generalisierbarkeit einer Theorie (Whetten, 1989).",
                "Related_terms": "Generalisability crisis; Positionality; Theory **Reference (s)**: Bacharach, (1989); Busse et.al, (2017); Dubin (1978), Whetten (1989)"
            },
            {
                "Title": "Bracketing Interviews (Klammer-Interviews) *",
                "Definition": "Bracketing interviews are commonly used within qualitative approaches. During these interviews researchers explore their personal subjectivities and assumptions surrounding their ongoing research. This allows researchers to be aware of their own interests and helps them to become both more reflective and critical about their research, considering how their own experiences may impact the research process. Bracketing interviews can also be subject to qualitative analysis.",
                "Originally drafted by": "Claire Melia",
                "Reviewed (or Edited) by": "Tamara Kalandadze; Charlotte R. Pennington; Graham Reid; Marta Topor",
                "Translated by": "** Helena Hartmann",
                "Translation reviewed by": "** Susanne Vogel, Jennifer Mattschey",
                "Translation": "Bracketing Interviews (Klammer-Interviews) werden üblicherweise im Rahmen qualitativer Ansätze verwendet. Während dieser Interviews erkunden Forschende ihre persönlichen Subjektivitäten und Annahmen im Zusammenhang mit ihrer laufenden Forschung. Dies ermöglicht es den Forschenden, sich ihrer eigenen Interessen bewusst zu werden, hilft ihnen, ihre Forschung zu reflektieren und kritisch zu betrachten, und zu überlegen, wie ihre eigenen Erfahrungen den Forschungsprozess beeinflussen können. Klammer-Interviews können auch einer qualitativen Analyse unterzogen werden.",
                "Related_terms": "Qualitative research; Reflexivity; Researcher bias **Reference (s)**: Rolls and Relf (2006); Sorsa et al. (2015)"
            },
            {
                "Title": "Bropenscience *",
                "Definition": "A tongue-in-cheek expression intended to raise awareness of the lack of diverse voices in open science (Bahlai, Bartlett, Burgio et al. 2019; Onie, 2020), in addition to the presence of behavior and communication styles that can be toxic or exclusionary. Importantly, not all bros are men; rather, they are individuals who demonstrate rigid thinking, lack self-awareness, and tend towards hostility, unkindness, and exclusion (Pownall et al., 2021; Whitaker & Guest, 2020). They generally belong to dominant groups who benefit from structural privileges. To address \\#bropenscience, researchers should examine and address structural inequalities within academic systems and institutions.",
                "Originally drafted by": "Zoe Flack",
                "Reviewed (or Edited) by": "Magdalena Grose-Hodge; Helena Hartmann; Bethan Iley; Tamara Kalandadze; Adam Parker; Charlotte R. Pennington; Flávio Azevedo; Bradley Baker; Mahmoud Elsherif",
                "Translated by": "** Susanne Vogel",
                "Translation reviewed by": "** Helena Hartmann, Jennifer Mattschey   ### **C** {#c}  ####",
                "Translation": "Ein ironisch gemeinter Ausdruck, der für den Mangel an diversen Stimmen in der Open Science Bewegung sensibilisieren soll (Bahlai, Bartlett, Burgio et al. 2019; Onie, 2020), sowie für das Vorhandensein von Verhaltensweisen und Kommunikationsarten, die toxisch oder ausgrenzend sein können. Wichtig ist, dass nicht alle “Bros” Männer sind; vielmehr handelt es sich um Personen, die ein starres Denken an den Tag legen, denen es an Selbsterkenntnis fehlt und die zu Feindseligkeit, Unfreundlichkeit und Ausgrenzung neigen (Pownall et al., 2021; Whitaker & Guest, 2020). Sie gehören im Allgemeinen zu dominanten Gruppen, die von strukturellen Privilegien profitieren. Um \\#bropenscience anzugehen, sollten Forschende strukturelle Ungleichheiten innerhalb akademischer Systeme und Institutionen untersuchen und angehen.",
                "Related_terms": "Diversity; Inclusion; Intersectionality; Open Science **Reference (s)**: Guest (2017); Whitaker and Guest (2020); Pownall et al. (2021)"
            },
            {
                "Title": "CARKing *",
                "Definition": "** Critiquing After the Results are Known (CARKing) refers to presenting a criticism of a design as one that you would have made in advance of the results being known. It usually forms a reaction or criticism to unwelcome or unfavourable results, results whether the critic is conscious of this fact or not.",
                "Reference(s)": "** Bardsley (2018); Nosek and Lakens (2014)",
                "Originally drafted by": "** Mahmoud Elsherif",
                "Reviewed (or Edited) by": "** Ali H. Al-Hoorie; Ashley Blake; Adrien Fillon; Charlotte R. Pennington",
                "Translated by": "** Helena Hartmann",
                "Translation reviewed by": "** Susanne Vogel, Jennifer Mattschey",
                "Translation": "Critiquing After the Results are Known (CARKing; Kritik nach Wissen der Ergebnisse) bedeutet, eine Kritik an einem Studiendesign so zu formulieren, als hätte man sie vor dem Vorliegen der Ergebnisse geäußert. Dies ist in der Regel eine Reaktion auf oder Kritik bezüglich unerwünschter oder ungünstiger Ergebnisse, unabhängig davon, ob sich der/die Kritiker:in dieser Tatsache bewusst ist oder nicht.",
                "Related_terms": "** HARKing; PARKing; Preregistration; Registered Report; SPARKing"
            },
            {
                "Title": "Center for Open Science (COS) *",
                "Definition": "** A non-profit technology organization based in Charlottesville, Virginia with the mission “to increase openness, integrity, and reproducibility of research.” Among other resources, the COS hosts the Open Science Framework (OSF) and the Open Scholarship Knowledge Base.",
                "Reference(s)": "** cos.io",
                "Drafted by": "** Beatrix Arendt",
                "Reviewed (or Edited) by": "** Ali H. Al-Hoorie; Mariella Paul; Charlotte R. Pennington; Lisa Spitzer",
                "Translated by": "** Susanne Vogel",
                "Translation reviewed by": "** Helena Hartmann, Jennifer Mattschey",
                "Translation": "Eine gemeinnützige Organisation im Technologiesektor mit Sitz in Charlottesville, Virginia, die sich zum Ziel gesetzt hat, \"to increase openness, integrity, and reproducibility of research” (cos.io; dt.: die Offenheit, Integrität und Reproduzierbarkeit von Forschung zu verbessern). Neben anderen Ressourcen betreibt das COS das Open Science Framework (OSF) und die Open Scholarship Knowledge Base.",
                "Related_terms": "** Open Science badges; Open Science Framework; OSF collections; OSF institutions; OSF meetings; OSF preprints; OSF registries; Registrations (Preregistrations & Registered Reports); Transparency and Openness Promotion Guidelines (TOP)"
            },
            {
                "Title": "Citation bias (Zitations-Verzerrung) *",
                "Definition": "** A biased selection of papers or authors cited and included in the references section. When citation bias is present, it is often in a way which would benefit the author(s) or reviewers, over-represents statistically significant studies, or reflects pervasive gender or racial biases (Brooks, 1985; Jannot et al., 2013; Zurn et al., 2020). One proposed solution is the use of Citation Diversity Statements, in which authors reflect on their citation practices and identify biases which may have emerged (Zurn et al., 2020).",
                "Reference(s)": "** Brooks (1985); Jannot et al. (2013); Thombs et al. (2015); Zurn et al. (2020)",
                "Originally drafted by": "** Bettina M. J. Kern",
                "Reviewed (or Edited) by": "** Mahmoud Elsherif; Annalise A. LaPlume; Helena Hartmann; Bethan Iley; Charlotte R. Pennington; Timo Roettger; Tobias Wingen",
                "Translated by": "** Helena Hartmann",
                "Translation reviewed by": "** Susanne Vogel, Jennifer Mattschey",
                "Translation": "Eine verzerrte (“biased”) Auswahl von Arbeiten oder Autor:innen, die im Text zitiert und in die Referenzen mit aufgenommen werden. Wenn eine Verzerrung von Zitationen vorliegt, geschieht dies häufig in einer Weise, die der/dem/den Autor:innen oder Gutachter:innen zugute kommt, statistisch signifikante Studien überrepräsentiert oder weitreichende geschlechts- oder herkunftsspezifische Vorurteile widerspiegelt (Brooks, 1985; Jannot et al., 2013; Zurn et al., 2020). Eine vorgeschlagene Lösung ist die Verwendung von Citation Diversity Statements (Zitations-Vielfalt-Angaben), in denen Autor:innen ihre Zitierpraxis reflektieren und Verzerrungen aufzeigen, die möglicherweise aufgetreten sind (Zurn et al., 2020).",
                "Related_terms": "** Citation diversity statement; Reporting bias"
            },
            {
                "Title": "Citation Diversity Statement (Zitations-Diversitäts-Erklärung) *",
                "Definition": "** A current effort trying to increase awareness and mitigate the citation bias in relation to gender and race is the Citation Diversity Statement, a short paragraph where “the authors consider their own bias and quantify the equitability of their reference lists. It states: (i) the importance of citation diversity, (ii) the percentage breakdown (or other diversity indicators) of citations in the paper, (iii) the method by which percentages were assessed and its limitations, and (iv) a commitment to improving equitable practices in science” (Zurn et al., 2020, p. 669).",
                "Reference": "** Zurn et al. (2020)",
                "Originally drafted by": "** Helena Hartmann",
                "Reviewed (or Edited) by": "** Mahmoud Elsherif; Magdalena Grose-Hodge; Sam Parsons; Timo Roettger",
                "Translated by": "** Susanne Vogel",
                "Translation reviewed by": "** Helena Hartmann, Jennifer Mattschey",
                "Translation": "Die Zitations-Diversitäts-Erklärung (Citation Diversity Statement) ist eine aktuelle Bemühung, das Bewusstsein für Verzerrungen beim Zitieren in Bezug auf Geschlecht und Herkunft zu schärfen und diese abzuschwächen. Es ist ein kurzer Absatz, in dem \"the authors consider their own bias and quantify the equitability of their reference lists. It states: (i) the importance of citation diversity, (ii) the percentage breakdown (or other diversity indicators) of citations in the paper, (iii) the method by which percentages were assessed and its limitations, and (iv) a commitment to improving equitable practices in science” (Zurn et al., 2020, p. 669, dt. \\[indem\\] die Autor:innen ihre eigene Verzerrung betrachten und die Ausgewogenheit ihrer Quellenangaben quantifizieren. Dabei wird Folgendes angegeben: (i) die Bedeutung der Diversität von zitierten Quellen, (ii) die prozentuale Aufschlüsselung (oder andere Diversitätsindikatoren) der zitierten Quellen in der Arbeit, (iii) die Methode, mit der die Prozentsätze berechnet wurden, und ihre Grenzen, und (iv) der Einsatz zur Verbesserung gerechter Praktiken in der Wissenschaft).",
                "Related_terms": "** Citation bias; Diversity; Under-representation"
            },
            {
                "Title": "Citizen Science (Bürger:innenwissenschaft) *",
                "Definition": "** Citizen science refers to projects that actively involve the general public in the scientific endeavour, with the goal of democratizing science. Citizen scientists can be involved in all stages of research, acting as collaborators, contributors or project leaders. An example of a major citizen science project involved individuals identifying astronomical bodies (Lintott, 2008).",
                "Reference(s)": "** Cohn (2008); European Citizen Science Association (2015); Lintott (2008)",
                "Drafted by": "** Mahmoud Elsherif; Ana Barbosa Mendes",
                "Reviewed (or Edited) by": "** Gisela H. Govaart; Tamara Kalandadze; Dominik Kiersz; Charlotte R. Pennington; Robert M. Ross",
                "Translated by": "** Helena Hartmann",
                "Translation reviewed by": "** Susanne Vogel, Jennifer Mattschey",
                "Translation": "Citizen Science (Bürger:innenwissenschaft) bezieht sich auf Projekte, die die breite Öffentlichkeit aktiv in den wissenschaftlichen Prozess mit einbeziehen, mit dem Ziel, Wissenschaft zu demokratisieren. Bürgerwissenschaftler:innen können in alle Phasen der Forschung eingebunden werden und als Mitarbeiter:innen, Mitwirkende oder Projektleiter:innen fungieren. Ein Beispiel für ein großes Citizen Science Projekt war die Identifizierung von astronomischen Körpern durch Einzelpersonen (Lintott, 2008).",
                "Related_terms": "** Crowd science; Crowdsourcing **Alternative definition:** (if applicable) In the past, citizen science mostly referred to volunteers who participate as field assistants in scientific studies (Cohn, 2008, p. 193)."
            },
            {
                "Title": "CKAN *",
                "Definition": "** The Comprehensive Knowledge Archive Network (CKAN) is an open-source data platform and free software that aims to provide tools to streamline publishing and data sharing. CKAN supports governments, research institutions and other organizations in managing and publishing large amounts of data.",
                "Reference": "** https://ckan.org/",
                "Originally drafted by": "** Tsvetomira Dumbalska",
                "Reviewed (or Edited) by": "** Ali H. Al-Hoorie; Kai Krautter; Charlotte R. Pennington",
                "Translated by": "** Helena Hartmann",
                "Translation reviewed by": "** Susanne Vogel, Jennifer Mattschey",
                "Translation": "Das Comprehensive Knowledge Archive Network (CKAN, dt. ) ist eine Open-Source-Datenplattform und freie Software, die darauf abzielt, Werkzeuge für optimierte Veröffentlichung und Datenaustausch zur Verfügung zu stellen. CKAN unterstützt Regierungen, Forschungseinrichtungen und andere Organisationen bei der Verwaltung und Veröffentlichung großer Datenmengen.",
                "Related_terms": "** Data platforms; Data sharing"
            },
            {
                "Title": "COAR Community Framework for Good Practices in Repositories *",
                "Definition": "** A framework which identifies best practices for scientific repositories and evaluation criteria for these practices. Its flexible and multidimensional approach means that it can be applied to different types of repositories, including those which host publications or data, across geographical and thematic contexts.",
                "Reference": "** Confederation of Open Access Repositories (2020, October 8\\)",
                "Originally drafted by": "** Aleksandra Lazić",
                "Reviewed (or Edited) by": "** Ashley Blake; Jamie P. Cockcroft; Bethan Iley; Sam Parsons",
                "Translated by": "** Helena Hartmann",
                "Translation reviewed by": "** Susanne Vogel, Jennifer Mattschey",
                "Translation": "Ein Leitfaden, der die besten Praktiken für wissenschaftliche Repositorien und Bewertungskriterien für diese Praktiken identifiziert. Sein flexibler und mehrdimensionaler Ansatz bedeutet, dass er auf verschiedene Typen von Repositorien angewendet werden kann, einschließlich solcher, die Publikationen oder Daten bereitstellen, über geografische und thematische Kontexte hinweg.",
                "Related_terms": "** Metadata; Open Access; Open Data; Open Material; Repository; TRUST principles"
            },
            {
                "Title": "Codebook (Codebuch) *",
                "Definition": "** A codebook is a high-level summary that describes the contents, structure, nature and layout of a data set. A well-documented codebook contains information intended to be complete and self-explanatory for each variable in a data file, such as the wording and coding of the item, and the underlying construct. It provides transparency to researchers who may be unfamiliar with the data but wish to reproduce analyses or reuse the data.",
                "Reference": "** Arslan et al. (2019); [https://www.icpsr.umich.edu/icpsrweb/content/shared/ICPSR/faqs/what-is-a-codebook.html](https://www.icpsr.umich.edu/icpsrweb/content/shared/ICPSR/faqs/what-is-a-codebook.html)",
                "Originally drafted by": "** Tina Lonsdorf",
                "Reviewed (or Edited) by": "** Ali H. Al-Hoorie; Ashley Blake, Kai Krautter; Charlotte R. Pennington; Flávio Azevedo",
                "Translated by": "** Bettina MJ Kern",
                "Translation reviewed by": "** Susanne Vogel, Jennifer Mattschey",
                "Translation": "Ein Codebook (Codebuch) ist eine Zusammenfassung, die den Inhalt, die Struktur, die Art und das Layout eines Datensatzes beschreibt. Ein gut dokumentiertes Codebuch enthält Informationen, die für jede Variable in einer Datendatei vollständig selbsterklärend sind, z. B. den Wortlaut und die Kodierung des Items und das zugrunde liegende Konstrukt. Ein Codebook schafft Transparenz für Forschende, die mit den Daten nicht vertraut sind, aber Analysen reproduzieren oder die Daten wiederverwenden möchten.",
                "Related_terms": "** Data dictionary; Metadata"
            },
            {
                "Title": "Code review (Code-Überprüfung) *",
                "Definition": "** The process of checking another researcher's programming (specifically, computer source code) including but not limited to statistical code and data modelling. This process is designed to detect and resolve mistakes, thereby improving code quality. In practice, a modern peer review process may take place via a hosted online repository such as GitHub, GitLab or SourceForge.",
                "Reference(s)": "** Petre and Wilson (2014); Scopatz and Huff (2015)",
                "Originally drafted by": "** Filip Dechterenko",
                "Reviewed (or Edited) by": "** Ali H. Al-Hoorie; Dominik Kiersz; Sam Parsons; Charlotte R. Pennington",
                "Translated by": "** Susanne Vogel",
                "Translation reviewed by": "** Helena Hartmann, Jennifer Mattschey",
                "Translation": "Der Prozess der Überprüfung der Codes eines anderen Forschenden (insbesondere des Computer-Quellcodes), einschließlich (aber nicht beschränkt auf), statistischen Code und Datenmodellierung. Dieser Prozess dient dazu, Fehler zu erkennen und zu beheben und so die Codequalität zu verbessern. In der Praxis kann ein modernes Begutachtungsverfahren (Peer-Review) über ein gehostetes Online-Repository wie GitHub, GitLab oder SourceForge stattfinden.",
                "Related_terms": "** Reproducibility; Version control"
            },
            {
                "Title": "Collaborative Replication and Education Project (CREP) *",
                "Definition": "** The Collaborative Replication and Education Project (CREP) is an initiative designed to organize and structure replication efforts of highly-cited empirical studies in psychology to satisfy the dual needs for more high-quality direct replications and more training in empirical research techniques for psychology students. CREP aims to address the need for replications of highly cited studies, and to provide training, support and professional growth opportunities for academics completing replication projects.",
                "Reference(s)": "** Wagge et al. (2019)",
                "Drafted by": "** Connor Keating",
                "Reviewed (or Edited) by": "** Bradley Baker; Mahmoud Elsherif; Zoe Flack; Charlotte R. Pennington",
                "Translated by": "** Helena Hartmann",
                "Translation reviewed by": "** Susanne Vogel, Jennifer Mattschey",
                "Translation": "Das Collaborative Replication and Education Project (CREP; Kollaboratives Replikations- und Bildungssprojekt) ist eine Initiative, die darauf abzielt, die Replikationen viel zitierter empirischer Studien in der Psychologie zu organisieren und zu strukturieren. So soll der zweifache Bedarf nach mehr qualitativ hochwertigen direkten Replikationen und mehr Ausbildung in empirischen Forschungstechniken für Psychologiestudierende gedeckt werden. CREP zielt darauf ab, den Bedarf an Replikationen viel zitierter Studien zu decken und Wissenschaftler:innen, die Replikationsprojekte durchführen, Schulungen, Unterstützung und berufliche Entwicklungsmöglichkeiten zu bieten.",
                "Related_terms": "** Direct replication; Exact replication"
            },
            {
                "Title": "Committee on Best Practices in Data Analysis and Sharing (COBIDAS) *",
                "Definition": "** The Organization for Human Brain Mapping (OHBM) neuroimaging community has developed a guideline for best practices in neuroimaging data acquisition, analysis, reporting, and sharing of both data and analysis code. It contains eight elements that should be included when writing up or submitting a manuscript in order to improve reporting methods and the resulting neuroimages in order to optimize transparency and reproducibility.",
                "Reference(s)": "** Nichols et al. (2017); Pernet et al. (2020)",
                "Originally drafted by": "** Yu-Fang Yang",
                "Reviewed (or Edited) by": "** Jamie P. Cockcroft; Helena Hartmann; Adam Parker; Sam Parsons",
                "Translated by": "** Susanne Vogel",
                "Translation reviewed by": "** Helena Hartmann, Jennifer Mattschey",
                "Translation": "Die Organization for Human Brain Mapping (OHBM; dt. Organisation für menschliche Hirnbildgebung) hat einen Leitfaden für optimale Verfahren (best practices) bei der Erhebung und Analyse von Bildgebungsdaten, der Veröffentlichung und der gemeinsamen Nutzung von Daten und Analysecodes entwickelt. Er enthält acht Elemente, die beim Verfassen oder Einreichen eines Manuskripts berücksichtigt werden sollten, um die Veröffentlichung der Methodik und die daraus resultierenden Bildgebungsdaten zu verbessern und so die Transparenz und Reproduzierbarkeit zu optimieren. **Alternative definition:** (if applicable) Checklist for data analysis and sharing"
            },
            {
                "Title": "Communality (Kommunalität) *",
                "Definition": "** The common ownership of scientific results and methods and the consequent imperative to share both freely. Communality is based on the fact that every scientific finding is seen as a product of the effort of a number of agents. This norm is followed when scientists openly share their new findings with colleagues.",
                "Reference(s)": "** Anderson et al. (2010); Hardwicke (2014); Merton (1938, 1942\\)",
                "Drafted by": "** David Moreau",
                "Reviewed (or Edited) by": "** Ashley Blake; Mahmoud Elsherif; Charlotte R. Pennington; Beatrice Valentini",
                "Translated by": "** Bettina MJ Kern",
                "Translation reviewed by": "** Helena Hartmann, Jennifer Mattschey",
                "Translation": "Das geteilte öffentliche Eigentum wissenschaftlicher Ergebnisse und Methoden und das daraus folgende Gebot, beides frei zu teilen. Die Kommunalität beruht auf der Tatsache, dass jede wissenschaftliche Erkenntnis das Ergebnis der Bemühungen einer Reihe von Akteuren ist. Um diese Norm zu befolgen, teilen Forschende ihre neuen Erkenntnisse offen mit Kolleg:innen.",
                "Related_terms": "** Mertonian norms; Objectivity **Alternative definition:** Communism (in Merton, 1942\\) **Related terms to alternative definition** (if applicable)"
            },
            {
                "Title": "Community Projects (Gemeinschaftsprojekte) *",
                "Definition": "** Collaborative projects that involve researchers from different career levels, disciplines, institutions or countries. Projects may have different goals including peer support and learning, conducting research, teaching and education. They can be short-term (e.g., conference events or hackathons) or long-term (e.g., journal clubs or consortium-led research projects). Collaborative culture and community building are key to achieving project goals.",
                "Reference(s)": "** Ellemers (2021); Orben (2019); Shepard (2015)",
                "Drafted by": "** Marta Topor",
                "Reviewed (or Edited) by": "** Ali H. Al-Hoorie; Jamie P. Cockcroft; Mahmoud Elsherif; Kai Krautter; Gerald Vineyard",
                "Translated by": "** Helena Hartmann",
                "Translation reviewed by": "** Susanne Vogel, Jennifer Mattschey",
                "Translation": "Kooperationsprojekte, an denen Forschende aus verschiedenen Karrierestufen, Disziplinen, Einrichtungen oder Ländern beteiligt sind. Die Projekte können unterschiedliche Ziele verfolgen, darunter gegenseitige Unterstützung und Lernen voneinander, Durchführen von Forschung, Lehre und Ausbildung. Sie können kurzfristig (z. B. Konferenzveranstaltungen oder Hackathons) oder langfristig (z. B. Journal Clubs oder von einem Konsortium geleitete Forschungsprojekte) sein. Eine Kultur der Zusammenarbeit und der Aufbau einer Gemeinschaft sind der Schlüssel zum Erreichen der Projektziele.",
                "Related_terms": "** Bottom-up approach (to Open Scholarship); Crowdsourced research; Hackathon; Many Labs; ReproducibiliTea"
            },
            {
                "Title": "Compendium (Handbuch) *",
                "Definition": "** A collection of files prepared by a researcher to support a report or publication that include the data, metadata, programming code, software dependencies, licenses, and other instructions necessary for another researcher to independently reproduce the findings presented in the report or publication.",
                "Originally drafted by": "** Ben Marwick",
                "Reviewed (or Edited) by": "** Ali H. Al-Hoorie; Charlotte R. Pennington",
                "Translated by": "** Susanne Vogel",
                "Translation reviewed by": "** Helena Hartmann, Jennifer Mattschey",
                "Translation": "Eine Sammlung von Dateien, die von Forschenden zur Unterstützung eines Berichts oder einer Veröffentlichung erstellt wurden und die Daten, Metadaten, Programmiercodes, Softwareabhängigkeiten, Lizenzen und andere Anweisungen enthalten, die andere Forschende benötigen, um die in dem Bericht oder der Veröffentlichung dargestellten Ergebnisse unabhängig zu reproduzieren.",
                "Related_terms": "** Compendia; Replication; Reproducibility; Research compendium; **References:** Claerbout and Karrenfach (1992); Gentleman (2005); Marwick et al. (2018); Nüst et al. (2018)"
            },
            {
                "Title": "Computational reproducibility (rechnerische Reproduzierbarkeit) *",
                "Definition": "** Ability to recreate the same results as the original study (including tables, figures, and quantitative findings), using the same input data, computational methods, and conditions of analysis. The availability of code and data facilitates computational reproducibility, as does preparation of these materials (annotating data, delineating software versions used, sharing computational environments, etc). Ideally, computational reproducibility should be achievable by another second researcher (or the original researcher, at a future time), using only a set of files and written instructions. Also referred to as analytic reproducibility (LeBel et al., 2018).",
                "Reference(s)": "** Committee on Reproducibility and Replicability in Science et al. (2019); Kitzes et al (2017, p. xxii); LeBel et al. (2018); Nosek and Errington (2020); Obels et al. (2020)",
                "Drafted by": "** Tina Lonsdorf",
                "Reviewed (or Edited) by": "** Sarah Ashcroft-Jones; Helena Hartmann; Annalise A. LaPlume; Adam Parker; Charlotte R. Pennington; Eike Mark Rinke",
                "Translated by": "** Helena Hartmann",
                "Translation reviewed by": "** Susanne Vogel, Jennifer Mattschey",
                "Translation": "Die Fähigkeit, dieselben Ergebnisse wie in der Originalstudie (einschließlich Tabellen, Abbildungen und quantitativer Ergebnisse) unter Verwendung derselben Eingangsdaten, Berechnungsmethoden und Analysebedingungen zu reproduzieren. Die Verfügbarkeit von Code und Daten erleichtert die rechnerische Reproduzierbarkeit, ebenso wie die Aufbereitung dieser Materialien (Datenbeschriftung, Angabe der verwendeten Softwareversionen, gemeinsame Nutzung von Rechenumgebungen usw.). Im Idealfall sollte die rechnerische Reproduzierbarkeit von einer/m zweiten Forschenden (oder der/m ursprünglichen Forschenden zu einem späteren Zeitpunkt) erreicht werden können, wofür nur eine Reihe von Dateien und schriftliche Anweisungen benötigt werden. Dies wird auch als analytische Reproduzierbarkeit bezeichnet (LeBel et al., 2018).",
                "Related_terms": "** FAIR principles; Replicability; Reproducibility"
            },
            {
                "Title": "Conceptual replication (Konzeptionelle Replikation) *",
                "Definition": "** A replication attempt whereby the primary effect of interest is the same but tested in a different sample and captured in a different way to that originally reported (i.e., using different operationalisations, data processing and statistical approaches and/or different constructs; LeBel et al., 2018). The purpose of a conceptual replication is often to explore what conditions limit the extent to which an effect can be observed and generalised (e.g., only within certain contexts, with certain samples, using certain measurement approaches) towards evaluating and advancing theory (Hüffmeier et al., 2016).",
                "Reference(s)": "** Crüwell et al. (2019); Hüffmeier et al. (2016); LeBel et al. (2018)",
                "Drafted by": "** Mahmoud Elsherif; Thomas Rhys Evans",
                "Reviewed (or Edited) by": "** Adrien Fillon; Helena Hartmann; Matt Jaquiery; Tina B. Lonsdorf; Catia M. Oliveira; Charlotte R. Pennington; Graham Reid; Timo Roettger; Lisa Spitzer; Suzanne L. K. Stewart; Flávio Azevedo",
                "Translated by": "** Susanne Vogel",
                "Translation reviewed by": "** Helena Hartmann, Jennifer Mattschey",
                "Translation": "Ein Replikationsversuch, bei dem der primäre Effekt, der von Interesse ist, derselbe ist, aber in einer anderen Stichprobe getestet und auf eine andere Weise erfasst wird als ursprünglich berichtet (d. h. durch Verwendung anderer Operationalisierungen, Datenverarbeitungs- und statistischer Ansätze und/oder anderer Konstrukte; LeBel et al., 2018). Der Zweck einer konzeptionellen Replikation besteht häufig darin, zu erforschen, welche Rahmenbedingungen das Ausmaß begrenzen, in dem ein Effekt beobachtet und verallgemeinert werden kann (z. B. nur in bestimmten Kontexten, mit bestimmten Stichproben, unter Verwendung bestimmter Messansätze), um die Theorie zu evaluieren und weiterzuentwickeln (Hüffmeier et al., 2016).",
                "Related_terms": "** Direct replication; Generalizability"
            },
            {
                "Title": "Confirmation bias (Bestätigungsverzerrung) *",
                "Definition": "** The tendency to seek out, interpret, favour and recall information in a way that supports one’s prior values, beliefs, expectations, or hypothesis.",
                "Reference(s)": "** Bishop (2020); Nickerson (1998); Spencer and Heneghan (2018); Wason (1960)",
                "Drafted by": "** Barnabas Szaszi; Jenny Terry",
                "Reviewed (or Edited) by": "** Mahmoud Elsherif; Tamara Kalandadze; Sam Parsons; Charlotte R. Pennington",
                "Translated by": "** Bettina MJ Kern",
                "Translation reviewed by": "** Helena Hartmann, Jennifer Mattschey",
                "Translation": "Die Tendenz, Informationen in einer Weise zu suchen, zu interpretieren, zu bevorzugen und abzurufen, die die eigenen früheren Werte, Überzeugungen, Erwartungen oder Hypothesen beschäftigt und unterstützt.",
                "Related_terms": "** Confirmatory bias; Congeniality bias; Myside bias"
            },
            {
                "Title": "Confirmatory analyses (konfirmatorische Analysen) *",
                "Definition": "** Part of the confirmatory-exploratory distinction (Wagenmakers et al., 2012), where confirmatory analyses refer to analyses that were set *a priori* and test existent hypotheses. The lack of this distinction within published research findings has been suggested to explain replicability issues and is suggested to be overcome through study preregistration which clearly distinguishes confirmatory from exploratory analyses. Other researchers have questioned these terms and recommended a replacement with ‘discovery-oriented’ and ‘theory-testing research’ (Oberauer & Lewandowsky, 2019; see also Szollosi & Donkin, 2019).",
                "Reference(s)": "** Box (1976); Oberauer and Lewandowsky (2019); Szollosi and Donkin (2019); Tukey (1977); Wagenmakers et al. (2012)",
                "Drafted by": "** Jenny Terry",
                "Reviewed (or Edited) by": "** Mahmoud Elsherif; Eduardo Garcia-Garzon; Helena Hartmann; Mariella Paul; Charlotte R. Pennington; Timo Roettger; Lisa Spitzer",
                "Translated by": "** Helena Hartmann",
                "Translation reviewed by": "** Susanne Vogel, Jennifer Mattschey",
                "Translation": "Teil der Unterscheidung zwischen konfirmatorisch und explorativ (Wagenmakers et al., 2012), wobei sich konfirmatorische Analysen auf Analysen beziehen, die im Vorfeld festgelegt wurden und bestehende Hypothesen testen. Die fehlende Abgrenzung dieser beiden Analyse-Arten in veröffentlichten Forschungsergebnissen wird als Erklärung für Replikationsprobleme vermutet. Es wurde vorgeschlagen, dieses Problem durch Präregistrierungen (Preregistrations) zu überwinden, da diese klar konfirmatorische und explorative Analysen trennen. Andere Forschende haben diese Begriffe in Frage gestellt und eine Umbenennung in \"entdeckungsorientierte\" und \"theorieprüfende Forschung\" empfohlen (Oberauer & Lewandowsky, 2019; siehe auch Szollosi & Donkin, 2019).",
                "Related_terms": "** Exploratory data analysis; Preregistration"
            },
            {
                "Title": "Conflict of interest (Interessenkonflikt) ]**",
                "Definition": "** A conflict of interest (COI, also ‘competing interest’) is a financial or non-financial relationship, activity or other interest that might compromise objectivity or professional judgement on the part of an author, reviewer, editor, or editorial staff. The *Principles of Transparency and Best Practice in Scholarly Publishing* by the Committee on Publication Ethics (COPE), the Directory of Open Access Journals (DOAJ), the Open Access Scholarly Publishers Association (OASPA), and the World Association of Medical Editors (WAME) states that journals should have policies on publication ethics, including policies on COI (DOAJ, 2018). COIs should be made transparent so that readers can properly evaluate research and assess for potential or actual bias(es). Outside publishing, academic presenters, panel members and educators should also declare COIs. Purposeful failure to disclose a COI may be considered a form of misconduct.",
                "Reference(s)": "** [http://www.icmje.org/recommendations/browse/roles-and-responsibilities/author-responsibilities--conflicts-of-interest.html](http://www.icmje.org/recommendations/browse/roles-and-responsibilities/author-responsibilities--conflicts-of-interest.html); DOAJ, 2018: [https://doaj.org/apply/transparency/](https://doaj.org/apply/transparency/)",
                "Drafted by": "** Christopher Graham",
                "Reviewed (or Edited) by": "** Flávio Azevedo",
                "Translated by": "** Susanne Vogel",
                "Translation reviewed by": "** Helena Hartmann, Jennifer Mattschey",
                "Translation": "Ein Interessenkonflikt (COI, Conflict of Interest) ist eine finanzielle oder nicht-finanzielle Beziehung, Aktivität oder ein anderes Interesse, das die Objektivität oder das professionelle Urteilsvermögen von Autor:innen, Gutachter:innen, Redakteur:innen oder Redaktionsmitgliedern beeinträchtigen könnte. In den “Principles of Transparency and Best Practice in Scholarly Publishing” des Committee on Publication Ethics (COPE), des Directory of Open Access Journals (DOAJ), der Open Access Scholarly Publishers Association (OASPA) und der World Association of Medical Editors (WAME) heißt es, dass Zeitschriften über Richtlinien zur Publikationsethik verfügen sollten, einschließlich Richtlinien zu COI (DOAJ, 2018). COIs sollten transparent offengelegt werden, damit die Lesende die Forschung richtig bewerten und auf potenzielle oder tatsächliche Voreingenommenheit(en) prüfen können. Jenseits von Veröffentlichungen sollten akademische Vortragende, Gremiumsmitglieder und Lehrende ebenfalls COIs angeben. Die absichtliche Nichtoffenlegung eines Interessenkonflikts kann als eine Form des Fehlverhaltens angesehen werden.",
                "Related_terms": "** Objectivity; Peer review; Public Trust in Science; Publication ethics; Transparency"
            },
            {
                "Title": "Consortium authorship (Konsortium Autor:innenschaft) *",
                "Definition": "** Only the name of the consortium or organization appears in the author column, and the individuals' names do not appear in the literature: For example, ‘FORRT’ as an author. This can be seen in the products of collaborative projects with a very large number of collaborators and/or contributors. Depending on the journal policy, individual researchers may be recorded as one of the authors of the product in literature databases such as ORCID and Scopus. Consortium authorship can also be termed group, corporate, organisation/organization or collective authorship (e.g. [https://www.bmj.com/about-bmj/resources-authors/article-submission/authorship-contributorship](https://www.bmj.com/about-bmj/resources-authors/article-submission/authorship-contributorship)), or collaborative authorship (e.g. [https://support.jmir.org/hc/en-us/articles/115001449591-What-is-a-group-author-collaborative-author-and-does-it-need-an-ORCID](https://support.jmir.org/hc/en-us/articles/115001449591-What-is-a-group-author-collaborative-author-and-does-it-need-an-ORCID))",
                "Reference(s)": "** Open Science Collaboration (2015); Tierney et al. (2020, 2021\\)",
                "Drafted by": "** Yuki Yamada",
                "Reviewed (or Edited) by": "** Adam Parker; Charlotte R. Pennington; Beatrice Valentini; Qinyu Xiao; Flávio Azevedo",
                "Translated by": "Bettina MJ Kern",
                "Translation reviewed by": "Helena Hartmann, Jennifer Mattschey",
                "Translation": "In der Autor:innenspalte erscheint nur der Name des Konsortiums oder der Organisation; die Namen der einzelnen Personen werden nicht genannt: Zum Beispiel \"FORRT\" als Autor. Dies ist bei den Publikationen der Ergebnisse von Gemeinschaftsprojekten mit einer sehr großen Anzahl von Mitwirkenden zu beobachten. Je nach den Richtlinien der Zeitschrift können einzelne Forschende in Literaturdatenbanken wie ORCID und Scopus als einer der Autoren aufgeführt werden. Die Autor:innenschaft eines Konsortiums kann auch als Gruppen-, Unternehmens-, Organisations- oder kollektive Autorenschaft (z. B. [https://www.bmj.com/about-bmj/resources-authors/article-submission/authorship-contributorship](https://www.bmj.com/about-bmj/resources-authors/article-submission/authorship-contributorship)) oder als kollaborative Autorenschaft (z. B. [https://support.jmir.org/hc/en-us/articles/115001449591-What-is-a-group-author-collaborative-author-and-does-it-need-an-ORCID](https://support.jmir.org/hc/en-us/articles/115001449591-What-is-a-group-author-collaborative-author-and-does-it-need-an-ORCID)) bezeichnet werden.",
                "Related_terms": "** Authorship; CRediT"
            },
            {
                "Title": "Constraints on Generality (COG, Beschränkungen der Generalisierbarkeit) *",
                "Definition": "** A statement that explicitly identifies and justifies the target population, and conditions, for the reported findings. Researchers should be explicit about potential boundary conditions for their generalisations (Simons et al., 2017). Researchers should provide detailed descriptions of the sampled population and/or contextual factors that might have affected the results such that future replication attempts can take these factors into account (Brandt et al., 2014). Conditions not explicitly listed are assumed not to have theoretical relevance to the replicability of the effect.",
                "Reference(s)": "** Busse et al. (2017); Brandt et al. (2014); Simons et al. (2017); Yarkoni (2020)",
                "Drafted by": "** Mahmoud Elsherif",
                "Reviewed (or Edited) by": "** Ali H. Al-Hoorie; Jamie P. Cockcroft; Sam Parsons; Charlotte R. Pennington; Timo Roettger",
                "Translated by": "** Helena Hartmann",
                "Translation reviewed by": "** Susanne Vogel, Jennifer Mattschey  ### ---",
                "Translation": "Eine Erklärung, in der die Zielpopulation und die Bedingungen für die berichteten Ergebnisse ausdrücklich genannt und begründet werden. Forschende sollten explizit auf mögliche Randbedingungen für ihre Verallgemeinerungen eingehen (Simons et al., 2017). Forschende sollten detaillierte Beschreibungen der Population aus der die Stichprobe stammte und/oder kontextbezogene Faktoren liefern, die die Ergebnisse beeinflusst haben könnten, damit künftige Replikationsversuche diese Faktoren berücksichtigen können (Brandt et al., 2014). Bei nicht explizit aufgeführten Bedingungen wird davon ausgegangen, dass sie keine theoretische Relevanz für die Replizierbarkeit des Effekts haben.",
                "Related_terms": "** BIZARRE; Diversity; Equity; Generalizability; Inclusion; Reproducibility; Replication; STRANGE; WEIRD"
            },
            {
                "Title": "Construct validity (Konstruktvalidität) *",
                "Definition": "** When used in the context of measurement and testing, construct validity refers to the degree to which a test measures what it claims to be measuring. In fields that study hypothetical unobservable entities, construct validation is essentially theory testing, because it involves determining whether an objective measure (a questionnaire, lab task, etc.) is a valid representation of a hypothetical construct (i.e., conforms to a theory). When used in a broader sense about a research study, or a claim, conclusion, or observed effect in a research study, construct validity concerns the extent to which the sampling particulars used in the study (participants, settings, treatments, and dependent variables) map onto the higher order constructs the study, the claim, the conclusion is about. According to Shadish et al. (2002), construct validity can be defined as “the degree to which inferences are warranted from the observed persons, settings, and cause and effect operations included in a study to the constructs that these instances might represent” (p. 38).",
                "Reference": "** Cronbach and Meehl (1955); Shadish et al. (2002); Smith (2005)",
                "Originally drafted by": "** Annalise A. LaPlume",
                "Reviewed (or Edited) by": "** Ali H. Al-Hoorie; Mahmoud Elsherif; Zoltan Kekecs; Charlotte R. Pennington",
                "Translated by": "** Susanne Vogel",
                "Translation reviewed by": "** Helena Hartmann, Jennifer Mattschey",
                "Translation": "Im Zusammenhang mit Messungen und Tests bezieht sich die Konstruktvalidität auf den Grad, in dem ein Test das misst, was er zu messen vorgibt. In Bereichen, in denen hypothetische, nicht beobachtbare Entitäten untersucht werden, ist die Konstruktvalidierung im Wesentlichen ein Theorietest, da es darum geht, festzustellen, ob eine Messung (ein Fragebogen, eine Laboraufgabe usw.) eine gültige Repräsentation eines hypothetischen Konstrukts ist (d. h. einer Theorie entspricht). Wird die Konstruktvalidität im weiteren Sinne auf eine Studie oder eine Behauptung, Schlussfolgerung oder beobachtete Wirkung in einer Forschungsarbeit angewandt, so bezieht sie sich auf das Ausmaß, in dem die in der Studie verwendeten Stichprobenmerkmale und Charakteristika (Teilnehmende, Setting, Interventionen und abhängige Variablen) den Konstrukten höherer Ordnung entsprechen, um die es in der Studie, der Behauptung oder der Schlussfolgerung geht. Nach Shadish et al. (2002) kann Konstruktvalidität definiert werden als \"the degree to which inferences are warranted from the observed persons, settings, and cause and effect operations included in a study to the constructs that these instances might represent\" (S. 38,dt. das Ausmaß, in dem Rückschlüsse von den beobachteten Personen, Settings und Ursache-Wirkungs-Beziehungen in einer Studie auf die Konstrukte, die diese Instanzen repräsentieren könnten, gerechtfertigt sind).",
                "Related_terms": "** Measurement crisis; Measurement validity; Questionable Measurement Practices (QMP); Theory; Validity; Validation"
            },
            {
                "Title": "Content validity (Inhaltsvalidität) *",
                "Definition": "** The degree to which a measurement includes all aspects of the concept that the researcher claims to measure; “A qualitative type of validity where the domain of the concept is made clear and the analyst judges whether the measures fully represent the domain” (Bollen, 1989, p.185). It is a component of *construct validity* and can be established using both quantitative and qualitative methods, often involving expert assessment.",
                "Reference": "** Bollen (1989); Brod et al. (2009); Drost (2011); Haynes et al. (1995)",
                "Originally drafted by": "** Annalise A. LaPlume",
                "Reviewed (or Edited) by": "** Mahmoud Elsherif; Wanyin Li; Aoife O’Mahony; Eike Mark Rinke; Sam Parsons; Graham Reid",
                "Translated by": "** Helena Hartmann",
                "Translation reviewed by": "** Susanne Vogel, Jennifer Mattschey",
                "Translation": "Das Ausmaß, zu dem eine Messung alle Aspekte des Konzepts umfasst, das die/der Forschende zu messen vorgibt; “A qualitative type of validity where the domain of the concept is made clear and the analyst judges whether the measures fully represent the domain” (dt. eine qualitative Art der Validität, bei der der Bereich des Konzepts deutlich gemacht wird und die/der Analytiker:in beurteilt, ob die Maßnahmen den Bereich vollständig repräsentieren; Bollen, 1989, S. 185). Sie ist eine Komponente der *Konstruktvalidität* und kann sowohl mit quantitativen als auch mit qualitativen Methoden ermittelt werden, die häufig eine Expert:innenbewertung beinhalten.",
                "Related_terms": "** Construct validity; Validity"
            },
            {
                "Title": "Contribution (Beitrag) *",
                "Definition": "** A formal addition or activity in a research context. Contribution and contributor statements, including acknowledgments sections in journal articles, are attached to research products to better classify and recognize the variety of labor beyond “authorship” that any intellectual pursuit requires. Contribution is an evolving “source of data for understanding the relationship between authorship and knowledge production.” (Lariviere et al., p.430). In open source software development, a contribution may count as changes committed onto a project's software repository following a peer-review (known technically as a pull request). An example of an open-source project accepting contributions is NumPy (Harris et al., 2020).",
                "Reference": "** Knoth and Herrmannova (2014); Larivière et al. (2016); Holcombe (2019)",
                "Originally drafted by": "** Micah Vandegrift",
                "Reviewed (or Edited) by": "** Jamie P. Cockcroft; Dominik Kiersz; Michele C. Lim; Leticia Micheli; Sam Parsons; Gerald Vineyard",
                "Translated by": "** Helena Hartmann",
                "Translation reviewed by": "** Susanne Vogel, Jennifer Mattschey",
                "Translation": "Eine formale Ergänzung oder Aktivität in einem Forschungskontext. Erklärungen zu Beiträgen und Mitwirkenden, einschließlich der Danksagungen in Zeitschriftenartikeln, werden an Forschungsprodukte angehängt, um die Vielfalt der Arbeit, die über \"Autor:innenschaft\" hinausgeht und die jede intellektuelle Tätigkeit erfordert, besser zu klassifizieren und anzuerkennen. Contributions sind eine sich entwickelnde “source of data for understanding the relationship between authorship and knowledge production.” (dt. Datenquelle für das Verständnis der Beziehung zwischen Autor:innenschaft und Wissensproduktion; Lariviere et al., S.430). Bei der Entwicklung von Open-Source-Software kann ein solcher Beitrag eine Änderung sein, die nach Begutachtung in das Software-Repositorium eines Projekts aufgenommen wird (technisch als Pull-Request bezeichnet). Ein Beispiel für ein Open-Source-Projekt, das Beiträge akzeptiert, ist NumPy (Harris et al., 2020).",
                "Related_terms": "** authorship; CRediT; Semantometrics"
            },
            {
                "Title": "Corrigendum (Korrigendum) *",
                "Definition": "** A corrigendum (pl. corrigenda, Latin: 'to correct') documents one or multiple errors within a published work that do not alter the central claim or conclusions and thus does not rise to the standard of requiring a retraction of the work. Corrigenda are typically available alongside the original work to aid transparency. Some publishers refer to this document as an erratum (pl. errata, Latin: 'error'), while others draw a distinction between the two (corrigenda as author-errors and errata as publisher-errors).",
                "Reference": "** Correction or retraction? (2006)",
                "Originally drafted by": "** Charlotte R. Pennington",
                "Reviewed (or Edited) by": "** Bradley Baker; Nick Ballou; Wanyin Li; Adam Parker; Emily A. Williams",
                "Translated by": "** Susanne Vogel",
                "Translation reviewed by": "** Helena Hartmann, Jennifer Mattschey",
                "Translation": "Ein Korrigendum (pl. Korrigenda, lat.: \"berichtigen\") dokumentiert einen oder mehrere Fehler in einer veröffentlichten Arbeit, die die zentrale Aussage oder die Schlussfolgerungen nicht verändern und somit nicht den Standard erreichen, der ein Zurückziehen der Arbeit erfordert. Korrigenda werden in der Regel neben dem Originalwerk veröffentlicht, um die Transparenz zu erhöhen. Einige Verlage bezeichnen dieses Dokument als Erratum (pl. errata, lateinisch: \"Fehler\"), während andere eine Unterscheidung zwischen beiden treffen (Corrigenda als Fehler der Autor:innen und Errata als Verlagsfehler).",
                "Related_terms": "** Correction; Errata; Retraction"
            },
            {
                "Title": "Co-production (Ko-Produktion) *",
                "Definition": "** An approach to research where stakeholders who are not traditionally involved in the research process are empowered to collaborate, either at the start of the project or throughout the research lifecycle. For example, co-produced health research may involve health professionals and patients, while co-produced education research may involve teaching staff and pupils/students. This is motivated by principles such as respecting and valuing the experiences of non-researchers, addressing power dynamics, and building mutually beneficial relationships.",
                "Reference": "** Filipe et al. (2017); Graham et al. (2019); NIHR (2021); Co-production Collective (2021)",
                "Originally drafted by": "** Emma Norris",
                "Reviewed (or Edited) by": "** Gisela H. Govaart; Magdalena Grose-Hodge; Helena Hartmann;Charlotte R. Pennington; Sonia Rishi; Emily A. Williams",
                "Translated by": "** Bettina MJ Kern",
                "Translation reviewed by": "** Helena Hartmann, Jennifer Mattschey  ### ---",
                "Translation": "Bei diesem Forschungsansatz werden Beteiligte, die traditionell nicht in den Forschungsprozess eingebunden sind, zu Beginn des Projekts oder während des gesamten Forschungsprozesses in die Zusammenarbeit eingebunden. So können beispielsweise an der ko-produzierten Gesundheitsforschung Angehörige von Gesundheitsberufen und Patient:innen beteiligt sein. An der ko-produzierten Bildungsforschung beispielsweise können Lehrkräfte und Schüler:innen/Studierende beteiligt sein. Dahinter stehen Grundsätze wie die Achtung und Wertschätzung der Erfahrungen von Nicht-Forschenden, der bewusste Umgang mit Machtdynamiken und der Aufbau von Beziehungen..",
                "Related_terms": "** Citizen science; Collaboration; Collaborative research; Crowd science; Engaged scholarship; Integrated Knowledge Translation (IKT); Mode 2 of knowledge production; Participatory research; Patient and Public Involvement (PPI)"
            },
            {
                "Title": "Creative Commons (CC) license *",
                "Definition": "** A set of free and easy-to-use copyright licences that define the rights of the authors and users of open data and materials in a standardized way. CC licenses enable authors or creators to share copyright-law-protected work with the public and come in different varieties with more or less clauses. For example, the Attribution-NonCommercial-ShareAlike 4.0 International (CC BY-NC-SA 4.0) allows you to share and adapt the material, under the condition that you; give credit to the original creators, indicate if changes were made, and share under the same license as the original, and you cannot use the material for commercial purposes.",
                "Reference(s)": "** https://creativecommons.org/about/cclicenses/",
                "Originally drafted by": "** Tina Lonsdorf",
                "Reviewed (or Edited) by": "** Adrien Fillon; Gisela H. Govaart; Annalise A. LaPlume; Sam Parsons; Charlotte R. Pennington",
                "Translated by": "** Helena Hartmann",
                "Translation reviewed by": "** Susanne Vogel, Jennifer Mattschey",
                "Translation": "Eine Reihe kostenloser und benutzerfreundlicher Urheberrechtslizenzen, die die Rechte von Autor:innen und Nutzer:innen frei verfügbarer Daten und Materialien auf standardisierte Weise definieren. CC-Lizenzen ermöglichen es Autor:innen oder Schöpfer:innen, urheberrechtlich geschützte Werke mit der Öffentlichkeit zu teilen. Es gibt sie in verschiedenen Varianten mit mehr oder weniger Bestimmungen. Die Attribution-NonCommercial-ShareAlike 4.0 International (CC BY-NC-SA 4.0) erlaubt beispielsweise, das Material zu teilen und anzupassen, unter den Bedingungen, dass die ursprünglichen Urheber:innen genannt werden, angegeben wird, ob Änderungen vorgenommen wurden, es unter derselben Lizenz wie das Original geteilt wird und das Material nicht für kommerzielle Zwecke verwendet wird.",
                "Related_terms": "** Copyright; Licence **Alternative definition:** (if applicable) Creative Commons is an international nonprofit organization that provides Creative Commons licences, with the goal to minimize legal obstacles to the sharing of knowledge and creativity."
            },
            {
                "Title": "Credibility revolution (Glaubhaftigkeitsrevolution) *",
                "Definition": "** The problems and the solutions resulting from a growing distrust in scientific findings, following concerns about the credibility of scientific claims (e.g., low replicability). The term has been proposed as a more positive alternative to the term replicability crisis, and includes the many solutions to improve the credibility of research, such as preregistration, transparency, and replication.",
                "Reference": "** Angrist and Pischke (2010); Vazire (2018); Vazire et al. (2020)",
                "Originally drafted by": "** Tamara Kalandadze",
                "Reviewed (or Edited) by": "** Bradley Baker; Mahmoud Elsherif; Helena Hartmann; Kai Krautter; Annalise A. LaPlume; Oscar Lecuona; Charlotte R. Pennington; Robert Ross; Tobias Wingen; Flávio Azevedo",
                "Translated by": "** Bettina MJ Kern",
                "Translation reviewed by": "** Helena Hartmann, Jennifer Mattschey",
                "Translation": "Der Begriff wurde als Alternative zum Begriff “Replikationskrise” vorgeschlagen und umfasst die vielen Lösungen zur Verbesserung der Glaubwürdigkeit der Forschung, wie Präregistrierung, Transparenz und Replikation.",
                "Related_terms": "** Credibility of scientific claims; High standards of evidence; Openness; Open Science;Reproducibility crisis (aka Replicability or replication crisis); Transparency"
            },
            {
                "Title": "Creative destruction approach to replication (Schöpferische Zerstörung als Herangehensweise an Replikationen) *",
                "Definition": "** Replication efforts should seek not just to support or question the original findings, but also to replace them with revised, stronger theories with greater explanatory power. This approach therefore involves ‘pruning’ existing theories, comparing all the alternative theories, and making replication efforts more generative and engaged in theory-building (Tierney et al. 2020, 2021).",
                "Reference(s)": "** Tierney et al. (2020, 2021\\)",
                "Drafted by": "** Mahmoud Elsherif",
                "Reviewed (or Edited) by": "** Magdalena Grose-Hodge; Aoife O’Mahony; Adam Parker; Charlotte R. Pennington; Sonia Rishi; Beatrice Valentini",
                "Translated by": "** Helena Hartmann",
                "Translation reviewed by": "** Susanne Vogel, Jennifer Mattschey",
                "Translation": "Replikationsbemühungen sollten nicht nur darauf abzielen, die ursprünglichen Ergebnisse zu stützen oder in Frage zu stellen, sondern sie auch durch überarbeitete, stärkere Theorien mit größerer Erklärungskraft zu ersetzen. Dieser Ansatz beinhaltet daher das \"Stutzen\" bestehender Theorien, den Vergleich aller alternativen Theorien und eine generativere und theoriebildende Ausrichtung von Replikationsbemühungen (Tierney et al. 2020, 2021).",
                "Related_terms": "** Crowdsourced research; Falsification; Replication; Theory"
            },
            {
                "Title": "CRediT *",
                "Definition": "** The Contributor Roles Taxonomy (CRediT; [https://casrai.org/credit/](https://casrai.org/credit/)) is a high-level taxonomy used to indicate the roles typically adopted by contributors to scientific scholarly output. There are currently 14 roles that describe each contributor’s specific contribution to the scholarly output. They can be assigned multiple times to different authors and one author can also be assigned multiple roles. CRediT includes the following roles: Conceptualization, Data curation, Formal Analysis, Funding acquisition, Investigation, Methodology, Project administration, Resources, Software, Supervision, Validation, Visualization, Writing – original draft, Writing – review & editing. A description of the different roles can be found in the work of Brand et al., (2015).",
                "Reference(s)": "** Brand et al. (2015); Holcombe (2019); [https://casrai.org/credit/](https://casrai.org/credit/)",
                "Drafted by": "** Sam Parsons",
                "Reviewed (or Edited) by": "** Myriam A. Baum; Matt Jaquiery; Tamara Kalandadze; Connor Keating; Charlotte R. Pennington; Yuki Yamada; Flávio Azevedo",
                "Translated by": "** Susanne Vogel",
                "Translation reviewed by": "** Helena Hartmann, Jennifer Mattschey",
                "Translation": "Die Contributor Roles Taxonomy (CRediT; dt. Klassifikation der Rollen von Mitwirkenden, [https://casrai.org/credit/](https://casrai.org/credit/)) ist eine übergeordnete Taxonomie, die dazu dient, die Rollen zu beschreiben, die typischerweise von Mitwirkenden an wissenschaftlichen Arbeiten eingenommen werden. Gegenwärtig gibt es 14 Rollen, die den spezifischen Beitrag eines jeden Mitwirkenden zur Veröffentlichung beschreiben. Sie können verschiedenen Autor:innen mehrfach zugewiesen werden, und einer:m Autor:in können auch mehrere Rollen zugewiesen werden. CRediT umfasst die folgenden Rollen: Konzeptualisierung, Datenkuratierung, formale Analyse, Akquise von Drittmitteln, Untersuchung, Methodik, Projektverwaltung, Ressourcen, Software, Betreuung, Validierung, Visualisierung, Schreiben \\- ursprünglicher Entwurf, Schreiben \\- Überprüfung und Bearbeitung. Eine Beschreibung der verschiedenen Rollen findet sich in der Arbeit von Brand et al. (2015).",
                "Related_terms": "** Authorship; Contributions"
            },
            {
                "Title": "Criterion validity (Kriteriumsvalidität) *",
                "Definition": "** The degree to which a measure corresponds to other valid measures of the same concept. Criterion validity is usually established by calculating regression coefficients or bivariate correlations estimating the direction and strength of relation between test measure and criterion measure. It is often confused with *construct validity* although it differs from it in intent (merely predictive rather than theoretical) and interest (predicting an observable outcome rather than a latent construct). Unreliability in either test or criterion scores usually diminishes criterion validity. Also called criterion-related or concrete validity.",
                "Reference": "** DeVellis (2017); Drost (2011)",
                "Originally drafted by": "** Annalise A. LaPlume",
                "Reviewed (or Edited) by": "** Helena Hartmann; Kai Krautter; Sam Parsons; Eike Mark Rinke",
                "Translated by": "** Helena Hartmann",
                "Translation reviewed by": "** Susanne Vogel, Jennifer Mattschey",
                "Translation": "Das Ausmaß, in dem eine Messung mit anderen gültigen Messungen desselben Konzepts übereinstimmt. Die Kriteriumsvalidität wird in der Regel durch die Berechnung von Regressionskoeffizienten oder bivariaten Korrelationen ermittelt, mit denen die Richtung und Stärke der Beziehung zwischen dem Testmaß und dem Kriteriumsmaß geschätzt wird. Sie wird oft mit der *Konstruktvalidität* verwechselt, obwohl sie sich von dieser in der Absicht (lediglich prädiktiv und nicht theoretisch) und im Interesse (Vorhersage eines beobachtbaren Ergebnisses und nicht eines latenten Konstrukts) unterscheidet. Fehlende Reliabilität (Zuverlässigkeit) bei Test- oder Kriteriumsergebnissen beeinträchtigt in der Regel die Kriteriumsvalidität. Sie wird auch kriterienbezogene oder konkrete Validität genannt.",
                "Related_terms": "** Construct validity; Validity"
            },
            {
                "Title": "Crowdsourced Research (Crowdsourcing-Forschung) *",
                "Definition": "** Crowdsourced research is a model of the social organisation of research as a large-scale collaboration in which one or more research projects are conducted by multiple teams in an independent yet coordinated manner. Crowdsourced research aims at achieving efficiency and scalability gains by pooling resources, promoting transparency and social inclusion, as well as increasing the rigor, reliability, and trustworthiness by enhancing statistical power and mutual social vetting. It stands in contrast to the traditional model of academic research production, which is dominated by the independent work of individual or small groups of researchers (‘small science’). Examples of crowdsourced research include so-called ‘many labs replication’ studies (Klein et al., 2018), ‘many analysts, one dataset’ studies (Silberzahn et al., 2018), distributive collaborative networks (Moshontz et al., 2018\\) and open collaborative writing projects such as Massively Open Online Papers (MOOPs) (Himmelstein et al., 2019; Tennant et al., 2019). Alternatively, crowdsourced research can refer to the use of a large number of research “crowdworkers” in data collection hired through online labor markets like Amazon Mechanical Turk or Prolific, for example in content analysis (Benoit et al., 2016; Lind et al., 2017\\) or experimental research (Peer et al., 2017). Crowdsourced research that is both open for participation and open through shared intermediate outputs has been referred to as *crowd science* (Franzoni & Sauermann, 2014).",
                "Reference": "** Benoit et al. (2016); Breznau (2021); Franzoni and Sauermann (2014); Himmelstein et al. (2019); Klein et al. (2018); Lind et al. (2017); Moshontz et al. (2018); Peer et al. (2017); Silberzahn et al. (2018); Stewart et al. (2017); Tennant et al. (2019); Uhlmann et al. (2019); [https://psysciacc.org/](https://psysciacc.org/); [https://crowdsourcingweek.com/what-is-crowdsourcing/](https://crowdsourcingweek.com/what-is-crowdsourcing/#:~:text=Crowdsourcing%20is%20the%20practice%20of,levels%20and%20across%20various%20industries)",
                "Originally drafted by": "** Eike Mark Rinke",
                "Reviewed (or Edited) by": "** Ali H. Al-Hoorie; Sam Parsons; Charlotte R. Pennington; Suzanne L. K. Stewart; Flávio Azevedo",
                "Translated by": "** Helena Hartmann",
                "Translation reviewed by": "** Susanne Vogel, Jennifer Mattschey",
                "Translation": "Die Crowdsourcing-Forschung ist ein Modell für die soziale Organisation der Forschung als groß angelegte Zusammenarbeit, bei der ein oder mehrere Forschungsprojekte von mehreren Teams auf unabhängige und koordinierte Weise durchgeführt werden. Crowdsourcing-Forschung zielt darauf ab, durch die Bündelung von Ressourcen, die Förderung von Transparenz und sozialer Inklusion Effizienz und Skalierbarkeit zu erreichen und die Genauigkeit, Zuverlässigkeit (Reliabilität) und Vertrauenswürdigkeit durch die Verbesserung der statistischen Teststärke (Power) und gegenseitiger sozialer Abstimmung zu erhöhen. Sie steht im Gegensatz zum traditionellen Modell der akademischen Forschungsproduktion, das von der unabhängigen Arbeit einzelner Forschenden oder kleiner Forschendengruppen (\"small science\") dominiert wird. Beispiele für Crowdsourcing-Forschung sind sogenannte \"Many Labs\"-Replikations-Studien (Klein et al., 2018), \"Many Analysts, One Dataset\"-Studien (Silberzahn et al., 2018), verteilte kollaborative Netzwerke (Moshontz et al., 2018\\) und offene kollaborative Schreibprojekte wie Massively Open Online Papers (MOOPs) (Himmelstein et al., 2019; Tennant et al., 2019). Alternativ kann sich Crowdsourcing-Forschung auf den Einsatz einer großen Anzahl von \"Crowdforschenden\" bei der Datenerhebung beziehen, die über Online-Arbeitsmärkte wie Amazon Mechanical Turk oder Prolific angeheuert werden, zum Beispiel bei Inhaltsanalyse (Benoit et al., 2016; Lind et al., 2017\\) oder experimenteller Forschung (Peer et al., 2017). Crowdsourcing-Forschung, die sowohl offen für die Teilnahme als auch offen für gemeinsame Zwischenergebnisse ist, wird als Crowd Science bezeichnet (Franzoni & Sauermann, 2014).",
                "Related_terms": "** Citizen science; Collaboration; Crowdsourcing; Team science"
            },
            {
                "Title": "Cultural taxation (kulturelle Taxierung) *",
                "Definition": "** The additional labor expected or demanded of members of underrepresented or marginalized minority groups, particularly scholars of color. This labor often comes from service roles providing ethnic, cultural, or gender representation and diversity. These roles can be formal or informal, and are generally unrewarded or uncompensated. Such labor includes providing expertise on matters of diversity, educating members of majority groups, acting as a liaison to minority communities, and formal and informal roles as mentor and support system for minority students.",
                "Reference(s)": "** Joseph and Hirschfeld (2011); Ledgerwood et al. (2021); Padilla (1994)",
                "Originally drafted by": "** Bradley Baker",
                "Reviewed (or Edited) by": "** Helena Hartmann; Bethan Iley; Aoife O’Mahony; Charlotte R. Pennington; Flávio Azevedo",
                "Translated by": "** Susanne Vogel",
                "Translation reviewed by": "** Helena Hartmann, Jennifer Mattschey",
                "Translation": "Die zusätzliche Arbeit, die von Mitgliedern unterrepräsentierter oder marginalisierter Minderheitsgruppen, insbesondere von Wissenschaftler:innen of color, erwartet oder verlangt wird. Diese Arbeit entsteht oft durch die Übernahme von Rollen für die herkunftsspezifische, kulturelle oder geschlechtsspezifische Repräsentativität und Diversität. Diese Aufgaben können formell oder informell sein und werden in der Regel nicht vergütet oder belohnt. Dazu gehören die Bereitstellung von Fachwissen zu Fragen der Diversität, die Schulung der Mitglieder von Mehrheitsgruppen, die Tätigkeit als Verbindungsperson zu Minderheitsgemeinschaften sowie formelle und informelle Rollen als Mentor:in und Unterstützungssystem für Studierende aus Minderheiten.",
                "Related_terms": "** Invisible labor; Power imbalances; Power relations"
            },
            {
                "Title": "Cumulative science (kumulative Wissenschaft) *",
                "Definition": "** Goal of any empirical science, it is the pursuit of “the construction of a cumulative base of knowledge upon which the future of the science may be built” (Curran, 2009, p. 1). The idea that science will create more complete and accurate theories as a function of the amount of evidence and data that has been collected. Cumulative science develops in gradual and incremental steps, as opposed to one abrupt discovery. While revolutionary science occurs scarcely, cumulative science is the most common form of science.",
                "Reference": "** Curran (2009); d’Espagnat (2008); Kuhn (1962); Mischel (2008)",
                "Originally drafted by": "** Beatrice Valentini",
                "Reviewed (or Edited) by": "** Sarah Ashcroft-Jones; Mahmoud Elsherif; Helena Hartmann; Oscar Lecuona; Wanyin Li; Sonia Rishi; Flávio Azevedo",
                "Translated by": "** Susanne Vogel",
                "Translation reviewed by": "** Helena Hartmann, Jennifer Mattschey  ###  ### **D** {#d}",
                "Translation": "Ziel jeder empirischen Wissenschaft ist \"the construction of a cumulative base of knowledge upon which the future of the science may be built” (dt. das Erschaffen einer kumulativen Wissensbasis, auf der die Zukunft der Wissenschaft aufgebaut werden kann, Curran, 2009, S. 1). Die Idee, dass Wissenschaft in Abhängigkeit von der Menge der gesammelten Evidenz und Daten immer vollständigere und genauere Theorien entwickelt. Die kumulative Wissenschaft entwickelt sich in graduellen und aufeinander aufbauenden Schritten, im Gegensatz zu einer plötzlichen Entdeckung. Während revolutionäre Wissenschaft selten vorkommt, ist die kumulative Wissenschaft die häufigste Form der Wissenschaft.",
                "Related_terms": "** Slow Science"
            },
            {
                "Title": "Data Access and Research Transparency (DA-RT) *",
                "Definition": "** Data Access and Research Transparency ([DA-RT](https://www.dartstatement.org/)) is an initiative aimed at increasing data access and research transparency in the social sciences. It is a multi-epistemic and multi-method initiative, created in 2014 by the Council of the American Political Science Association (APSA), to bolster the rigor of empirical social inquiry. In addition to other activities, DA-RT developed the Journal Editors' Transparency Statement (JETS), which requires subscribing journals to (a) making relevant data publicly available if the study is published, (b) following a strict data citation policy, (c) transparently describing the analytical procedures and, if possible, providing public access to analytical code, and (d) updating their journal style guides, codes of ethics to include improved data access and research transparency requirements.",
                "Reference": "** Carsey (2014); Monroe (2018)",
                "Originally drafted by": "** Eike Mark Rinke",
                "Reviewed (or Edited) by": "** Filip Dechterenko; Kai Krautter; Flávio Azevedo",
                "Translated by": "** Helena Hartmann",
                "Translation reviewed by": "** Susanne Vogel, Jennifer Mattschey",
                "Translation": "Data Access and Research Transparency\" ([DA-RT](https://www.dartstatement.org/); dt. Datenzugang und Forschungstransparenz) ist eine Initiative, die den Datenzugang und die Forschungstransparenz in den Sozialwissenschaften verbessern soll. Es handelt sich um eine multi-erkenntnistheoretische und multimethodische Initiative, die 2014 vom Rat der American Political Science Association (APSA) ins Leben gerufen wurde, um die Genauigkeit (rigor) der empirischen Sozialforschung zu stärken. Neben anderen Aktivitäten hat DA-RT die Transparenzerklärung der Herausgeber:innen von Zeitschriften (Journal Editors' Transparency Statement, JETS) entwickelt. Diese verlangt von den unterzeichnenden Zeitschriften (a) relevante Daten öffentlich zugänglich zu machen, wenn die Studie veröffentlicht wird, (b) strengen Datenzitierregeln zu folgen, (c) die Analyseverfahren transparent zu beschreiben und, wenn möglich, öffentlichen Zugang zum Analysecode zu gewähren und (d) ihre Zeitschriftenleitfäden und Ethikerklärungen zu aktualisieren, um verbesserte Anforderungen an den Datenzugang und die Forschungstransparenz einzubeziehen.",
                "Related_terms": "** Accessibility; Data sharing; Replicability; Reproducibility"
            },
            {
                "Title": "Data management plan (DMP; Datenmanagementplan) *",
                "Definition": "** A structured document that describes the process of data acquisition, analysis, management and storage during a research project. It also describes data ownership and how the data will be preserved and shared during and upon completion of a project. Data management templates also provide guidance on how to make research data FAIR and where possible, openly available.",
                "Reference(s)": "** Burnette et al. (2016); Michener (2015); Research Data Alliance (2020); https://library.stanford.edu/research/data-management-services/data-management-plans\\#:\\~:text=A%20data%20management%20plan%20(DMP,share%20and%20preserve%20your%20data.",
                "Drafted by": "** Dominique Roche",
                "Reviewed (or Edited) by": "** Charlotte R. Pennington; Sam Parsons; Birgit Schmidt; Flávio Azevedo",
                "Translated by": "** Susanne Vogel",
                "Translation reviewed by": "** Helena Hartmann, Jennifer Mattschey",
                "Translation": "Ein strukturiertes Dokument, das den Prozess der Datenerhebung, \\-analyse, \\-verwaltung und \\-speicherung während eines Forschungsprojekts beschreibt. Es beschreibt auch die Eigentumsrechte an den Daten und wie die Daten während und nach Abschluss eines Projekts aufbewahrt und geteilt werden. Vorlagen für das Datenmanagement bieten auch Anhaltspunkte dafür, wie Forschungsdaten FAIR und, wenn möglich, offen zugänglich gemacht werden können.",
                "Related_terms": "** Data archiving; Data sharing; Data storage; FAIR principles; Open data"
            },
            {
                "Title": "Data sharing *",
                "Definition": "** collection of practices, technologies, cultural elements and legal frameworks that are relevant to the practice of making data used for scholarly research available to other investigators. Gollwitzer et al. (2020) describe two types of data sharing: Type 1: Data that is necessary to reproduce the findings of a published research article. Type 2: data that have been collected in a research project but have not (or only partly) been analysed or reported after the completion of the project and are hence typically shared under a specified embargo period.",
                "Reference(s)": "** Abele-Brehm et al. (2019); Gollwitzer et al. (2020); https://eudatasharing.eu/what-data-sharing",
                "Drafted by": "** Mahmoud Elsherif",
                "Reviewed (or Edited) by": "** Helena Hartmann; Tina Lonsdorf; Charlotte R. Pennington; Timo Roettger; Flávio Azevedo",
                "Translated by": "** Susanne Vogel",
                "Translation reviewed by": "** Helena Hartmann, Jennifer Mattschey",
                "Translation": "Sammlung von Praktiken, Technologien, kulturellen Elementen und rechtlichen Rahmenbedingungen, die für die Bereitstellung von Daten, die für wissenschaftliche Forschung verwendet wurden, an andere Forschende relevant sind. Gollwitzer et al. (2020) beschreiben zwei Arten der gemeinsamen Nutzung von Daten: Typ 1: Daten, die notwendig sind, um die Ergebnisse eines veröffentlichten Forschungsartikels zu reproduzieren. Typ 2: Daten, die im Rahmen eines Forschungsprojekts erhoben wurden, aber nach Abschluss des Projekts nicht (oder nur teilweise) analysiert oder veröffentlicht wurden und daher in der Regel unter einer bestimmten Sperrfrist freigegeben werden.",
                "Related_terms": "** FAIR principles; Open data"
            },
            {
                "Title": "Data visualisation (Datenvisualisierung) *",
                "Definition": "** Graphical representation of data or information. Data visualisation takes advantage of humans’ well-developed visual processing capacity to convey insight and communicate key information. Data visualisations often display the raw data, descriptive statistics, and/or inferential statistics.",
                "Reference(s)": "** Healy (2018); Tufte (1983)",
                "Drafted by": "** Bradley Baker",
                "Reviewed (or Edited) by": "** Mahmoud Elsherif; Charlotte R. Pennington; Suzanne L. K. Stewart",
                "Translated by": "** Helena Hartmann",
                "Translation reviewed by": "** Susanne Vogel, Jennifer Mattschey",
                "Translation": "Grafische Darstellung von Daten oder Informationen. Die Datenvisualisierung nutzt die gut entwickelte visuelle Verarbeitungskapazität des Menschen, um Erkenntnisse und wichtige Informationen zu vermitteln. Datenvisualisierungen zeigen oft die Rohdaten, deskriptive Statistiken und/oder Inferenzstatistiken.",
                "Related_terms": "** Figure; Graph; Plot"
            },
            {
                "Title": "Decolonisation (Dekolonialisierung) *",
                "Definition": "** Coloniality can be described as the naturalisation of concepts such as imperialism, capitalism, and nationalism. Together these concepts can be thought of as a matrix of power (and power relations) that can be traced to the colonial period. Decoloniality seeks to break down and decentralize those power relations, with the aim to understand their persistence and to reconstruct the norms and values of a given domain. In an academic setting, decolonisation refers to the rethinking of the lens through which we teach, research, and co-exist, so that the lens generalises beyond Western-centred and colonial perspectives. Decolonising academia involves reconstructing the historical and cultural frameworks being used, redistributing a sense of belonging in universities, and empowering and including voices and knowledge types that have historically been excluded from academia. This is done when people engage with their past, present, and future whilst holding a perspective that is separate from the socially dominant perspective. Also, by including, not rejecting, an individuals’ internalised norms and taboos from the specific colony.",
                "Reference(s)": "** Albayrak (2018)",
                "Drafted by": "** Nihan Albayrak-Aydemir",
                "Reviewed (or Edited) by": "** Sarah Ashcroft-Jones; Mahmoud Elsherif; Michele C. Lim; Emma Norris; Flávio Azevedo",
                "Translated by": "** Helena Hartmann",
                "Translation reviewed by": "** Susanne Vogel, Jennifer Mattschey",
                "Translation": "Kolonialität kann als die Etablierung / Einbürgerung von Konzepten wie Imperialismus, Kapitalismus und Nationalismus beschrieben werden. Zusammen können diese Konzepte als eine Matrix der Macht (und der Machtbeziehungen) betrachtet werden, die sich auf die Kolonialzeit zurückführen lässt. Die Dekolonialisierung zielt darauf ab, diese Machtverhältnisse aufzubrechen und zu dezentralisieren, um ihr Fortbestehen zu verstehen und die Normen und Werte eines bestimmten Bereichs zu rekonstruieren. Im akademischen Umfeld bedeutet Dekolonisierung, dass wir die Sichtweise, durch die wir lehren, forschen und zusammenleben, überdenken, so dass sie über westlich orientierte und koloniale Perspektiven hinausgeht. Die Entkolonialisierung der Wissenschaft beinhaltet die Rekonstruktion der verwendeten historischen und kulturellen Rahmenbedingungen, die Neuverteilung des Zugehörigkeitsgefühls an den Universitäten und die Ermächtigung und Einbeziehung von Stimmen und Wissensarten, die in der Vergangenheit von der Wissenschaft ausgeschlossen waren. Dies geschieht, wenn Menschen sich mit ihrer Vergangenheit, Gegenwart und Zukunft auseinandersetzen und dabei eine Perspektive einnehmen, die sich von der gesellschaftlich dominanten Perspektive unterscheidet. Dies geschieht auch dadurch, dass die verinnerlichten Normen und Tabus der jeweiligen Kolonie berücksichtigt und nicht abgelehnt werden.",
                "Related_terms": "** Diversity; Equity; Inclusion"
            },
            {
                "Title": "Demarcation criterion (Demarkationskriterium) *",
                "Definition": "** A criterion for distinguishing science from non-science which aims to indicate an optimal way for knowledge of the world to grow. In a Popperian approach, the demarcation criterion was falsifiability and the application of a falsificationist attitude. Alternative approaches include that of Kuhn, who believed that the criterion was puzzle solving with the aim of understanding nature, and Lakatos, who argued that science is marked by working within a progressive research programme.",
                "Reference(s)": "** Dienes (2008)",
                "Drafted by": "** Alaa AlDoh",
                "Reviewed (or Edited) by": "** Bethan Iley; Sara Middleton",
                "Translated by": "** Susanne Vogel",
                "Translation reviewed by": "** Helena Hartmann, Jennifer Mattschey",
                "Translation": "Ein Kriterium zur Unterscheidung zwischen Wissenschaft und Nicht-Wissenschaft, das einen optimalen Weg für die Ansammlung des Wissens der Welt aufzeigen soll. In einem Popper’schen Ansatz war das Demarkationskriterium oder Abgrenzungskriterium die Falsifizierbarkeit und die Anwendung einer falsifizierenden Haltung. Zu den alternativen Ansätzen gehören der von Kuhn, der das Kriterium in der Lösung von Rätseln mit dem Ziel des Verständnisses der Natur sah, und der von Lakatos, der argumentierte, dass Wissenschaft durch die Arbeit innerhalb eines fortschreitenden Forschungsprogramms gekennzeichnet ist.",
                "Related_terms": "** Hypothesis; Falsification"
            },
            {
                "Title": "DOI (digital object identifier) *",
                "Definition": "** Digital Object Identifiers (DOI) are alpha-numeric strings that can be assigned to any entity, including: publications (including preprints), materials, datasets, and feature films \\- the use of DOIs is not restricted to just scholarly or academic material. DOIs “provides a system for persistent and actionable identification and interoperable exchange of managed information on digital networks.” ([https://doi.org/hb.html](https://doi.org/hb.html)). There are many different DOI registration agencies that operate DOIs, but the two that researchers would most likely encounter are [Crossref](https://www.crossref.org/) and [Datacite](https://datacite.org/).",
                "Reference": "** Bilder (2013); Morgan (1998); [https://www.doi.org/hb.html](https://www.doi.org/hb.html)",
                "Originally drafted by": "** Tina Lonsdorf",
                "Reviewed (or Edited) by": "** Ashley Blake; Helena Hartmann; Sam Parsons; Charlotte R. Pennington; Flávio Azevedo",
                "Translated by": "** Helena Hartmann",
                "Translation reviewed by": "** Susanne Vogel, Jennifer Mattschey",
                "Translation": "Digital Object Identifiers (DOI) sind alphanumerische Zeichenfolgen, die jeder beliebigen Einheit zugewiesen werden können, z. B. Veröffentlichungen (einschließlich Preprints), Materialien, Datensätze und Spielfilme \\- die Verwendung von DOIs ist nicht nur auf wissenschaftliches oder akademisches Material beschränkt. DOIs “provides a system for persistent and actionable identification and interoperable exchange of managed information on digital networks.” (dt. bieten ein System für die dauerhafte und umsetzbare Identifizierung und den interoperablen Austausch von verwalteten Informationen in digitalen Netzwerken; [https://doi.org/hb.html](https://doi.org/hb.html)). Es gibt viele verschiedene DOI-Registrierungsstellen, die DOIs betreiben, aber die beiden, auf die Forschende am ehesten stoßen, sind [Crossref](https://www.crossref.org/) und [Datacite](https://datacite.org/).",
                "Related_terms": "** arXiv and BibTex; Crossref, Datacite, ISBN, ISO, ORCID; Permalink"
            },
            {
                "Title": "Double-blind peer review (Doppelblinde Begutachtung) *",
                "Definition": "** Evaluation of research products by qualified experts where both the author(s) and reviewer(s) are anonymous to each other. “This approach conceals the identity of the authors and their affiliations from reviewers and would, in theory, remove biases of professional reputation, gender, race, and institutional affiliation, allowing the reviewer to avoid bias and to focus on the manuscript’s merit alone.” (Tvina et al., 2019, 1082). Like all types of peer-review, double-blind peer review is not without flaws. Anonymity can be difficult, if not impossible, to achieve for certain researchers working in a niche area.",
                "Reference(s)": "** Largent and Snodgrass (2016); Tvina et al. (2019)",
                "Drafted by": "** Mahmoud Elsherif",
                "Reviewed (or Edited) by": "** Bradley Baker; Helena Hartmann; Meng Liu; Emma Norris",
                "Translated by": "** Susanne Vogel",
                "Translation reviewed by": "** Helena Hartmann, Jennifer Mattschey",
                "Translation": "Bewertung von Forschungsprodukten durch qualifizierte Expert:innen, wobei sowohl der/die Autor:in(nen) als auch der/die Gutachter:in(nen) anonym bleiben. \"This approach conceals the identity of the authors and their affiliations from reviewers and would, in theory, remove biases of professional reputation, gender, race, and institutional affiliation, allowing the reviewer to avoid bias and to focus on the manuscript’s merit alone” (dt. Dieser Ansatz verbirgt die Identität der Autor:innen und ihre  Affiliation vor den Begutachtenden und würde, theoretisch, Vorurteile aufgrund von beruflicher Reputation, Geschlecht, Herkunft und institutioneller Zugehörigkeit beseitigen, so dass die Begutachtenden Voreingenommenheit vermeiden und sich allein auf die Leistung des Manuskripts konzentrieren können, Tvina et al., 2019, 1082). Wie alle Arten von Peer-Review ist auch die doppelblinde Begutachtung nicht ohne Schwierigkeiten. Anonymität kann für bestimmte Forschende, die in einem Nischenbereich arbeiten, schwierig, wenn nicht gar unmöglich zu erreichen sein.",
                "Related_terms": "** Ad hominem bias; Affiliation bias; Anonymous review; Masked review; Open peer review; Peer review; Single-blind peer review; Traditional peer review; Triple-Blind peer review"
            },
            {
                "Title": "Double consciousness (Doppeltes Bewusstsein) *",
                "Definition": "** An identity confusion, as the individual feels like they have two distinct identities. One is to assimilate to the dominant culture at university when the individual is with colleagues and professors, while the other is when the individual is with their families. This continuous shift may cause a lack of certainty about the individual’s identity and a belief that the individual does not fully belong anywhere. This lack of belonging can lead to poor social integration within the academic culture that can manifest in less opportunities and more mental health issues in the individual (Rubin, 2021; Rubin et al., 2019).",
                "Reference(s)": "** Albayrak and Okoroji (2019); Du Bois (1968); Gilroy (1993)",
                "Drafted by": "** Nihan Albayrak-Aydemir",
                "Reviewed (or Edited) by": "** Mahmoud Elsherif; Wanyin Li; Michele C. Lim; Adam Parker",
                "Translated by": "** Helena Hartmann",
                "Translation reviewed by": "** Susanne Vogel, Jennifer Mattschey",
                "Translation": "Eine Identitätsverwirrung, da der Einzelne das Gefühl hat, zwei verschiedene Identitäten zu haben. Die eine ist die Anpassung an die dominante Kultur an der Universität, wenn die/der Einzelne mit Kolleg:innen und Professor:innen zusammen ist, die andere, wenn sie/er mit der Familie zusammen ist. Dieser ständige Wechsel kann dazu führen, dass man sich seiner Identität nicht sicher ist und glaubt, nirgendwo richtig dazuzugehören. Dieser Mangel an Zugehörigkeit kann zu einer schlechten sozialen Integration innerhalb der akademischen Kultur führen, die sich in geringeren Chancen und mehr psychischen Problemen bei der Person äußern kann (Rubin, 2021; Rubin et al., 2019).",
                "Related_terms": "** Social class; Social integration"
            },
            {
                "Title": "DORA *",
                "Definition": "** The San Francisco Declaration on Research Assessment (DORA) is a global initiative aiming to reduce dependence on journal-based metrics (e.g. journal impact factor and citation counts) and, instead, promote a culture which emphasises the intrinsic value of research. The DORA declaration targets research funders, publishers, research institutes and researchers and signing it represents a commitment to aligning research practices and procedures with the declaration’s principles.",
                "Reference(s)": "** Health Research Board (n.d.); [https://sfdora.org/](https://sfdora.org/)",
                "Drafted by": "** Aoife O’Mahony",
                "Reviewed (or Edited) by": "** Connor Keating; Charlotte R. Pennington",
                "Translated by": "** Helena Hartmann",
                "Translation reviewed by": "** Susanne Vogel, Jennifer Mattschey",
                "Translation": "Die San Francisco Declaration on Research Assessment (DORA) ist eine globale Initiative, die darauf abzielt, die Abhängigkeit von Zeitschriften-basierten Metriken (z. B. Impact Faktor und Zitationszahlen) zu verringern und stattdessen eine Kultur zu fördern, die den intrinsischen Wert der Forschung betont. Die DORA-Erklärung richtet sich an Forschungsfördende, Verlage, Forschungsinstitute und Forschende, die sich mit ihrer Unterschrift verpflichten, ihre Forschungspraktiken und \\-verfahren an die Grundsätze der Erklärung anzupassen.",
                "Related_terms": "** Generalizability; Journal Impact Factor; Open Science"
            },
            {
                "Title": "Direct replication (direkte Replikation) *",
                "Definition": "** As ‘direct replication’ does not have a widely-agreed technical meaning nor there is no clear cut distinction between a direct and conceptual replication, below we list several contributions towards a consensus. Rather than debating the ‘exactness’ of a replication, it is more helpful to discuss the relevant differences between a replication and its target, and their implications for the reliability and generality of the target’s results. Generally, direct replication refers to a new data collection that attempts to replicate original studies’ methods as closely as possible. A replication attempt that “seek(s) to duplicate the necessary elements that produced the original finding.” (Cruwell et al., 2019; p.243). The purpose of a direct replication can be to identify type 1 errors and/or experimenter effects, determine the replicability of an effect using the same or improved practices, or to create more specific estimates of effect size (Hűffmeier et al., 2016). Directness of replication is a continuum between repeating specific observations (data) and observing generalised effects (phenomena). How closely a replication replicates an original study is often a matter for debate, often with differences being cited as hidden moderators of effects. Furthermore, there can be debate over the relevant importance of technical equivalence (i.e., using identical materials) versus psychological equivalence (i.e., realizing the identical psychological conditions) to the original study (Schwarz and Strack, 2014). For example, consider a study on Trust in the US- President conducted in 2018\\. A technical equivalent replication would use Trump as stimulus (he was president in 2018\\) a psychological equivalent study would use Biden (he is the current president).",
                "Reference(s)": "** Crüwell et al. (2019); Hüffmeier et al. (2016); LeBel et al. (2019); Schwarz and Strack (2014)",
                "Drafted by": "**Mahmoud Elsherif (original); Thomas Rhys Evans (alternative); Tina Lonsdorf (alternative)",
                "Reviewed (or Edited) by": "** Beatrix Arendt; Adrien Fillon; Matt Jaquiery; Charlotte R. Pennington; Graham Reid; Lisa Spitzer; Tobias Wingen; Flávio Azevedo",
                "Translated by": "** Susanne Vogel",
                "Translation reviewed by": "** Helena Hartmann, Jennifer Mattschey",
                "Translation": "Da der Begriff \"direkte Replikation\" keine allgemein anerkannte technische Bedeutung hat und es keine eindeutige Unterscheidung zwischen einer direkten und einer konzeptionellen Replikation gibt, werden im Folgenden einige Beiträge zur Erzielung eines Konsenses aufgeführt. Anstatt über die \"Genauigkeit\" einer Replikation zu debattieren, ist es hilfreicher, die relevanten Unterschiede zwischen einer Replikation und der zu replizierenden Arbeit, sowie deren Auswirkungen auf die Zuverlässigkeit und Generalisierbarkeit der Ergebnisse der ursprünglichen Arbeit zu diskutieren. Im Allgemeinen bezieht sich die direkte Replikation auf eine neue Datenerhebung, die versucht, die Methoden der ursprünglichen Studie so genau wie möglich zu replizieren. Ein Replikationsversuch, der \"seek(s) to duplicate the necessary elements that produced the original finding.” (dt. versucht, die notwendigen Elemente zu duplizieren, die zu den ursprünglichen Ergebnissen geführt haben, Cruwell et al., 2019; p. 243). Der Zweck einer direkten Replikation kann darin bestehen, Fehler vom Typ 1 und/oder Versuchsleiter:innen-Effekte zu identifizieren, die Replizierbarkeit eines Effekts unter Verwendung derselben oder verbesserter Verfahren zu bestimmen oder spezifischere Schätzungen der Effektgröße zu erstellen (Hüffmeier et al., 2016). Die Direktheit der Replikation ist ein Kontinuum zwischen der Wiederholung spezifischer Beobachtungen (Daten) und der Beobachtung verallgemeinerter Effekte (Phänomene). Wie genau eine Replikation eine ursprüngliche Studie nachbildet, ist oft umstritten, wobei Unterschiede oft als versteckte Moderatoren von Effekten angeführt werden. Darüber hinaus kann eine Debatte über die Bedeutung der technischen Äquivalenz (d. h. die Verwendung identischer Materialien) gegenüber der psychologischen Äquivalenz (d. h. die Realisierung identischer psychologischer Bedingungen) der Originalstudie geführt werden (Schwarz and Strack, 2014). Nehmen wir zum Beispiel eine Studie zum Vertrauen in den US-Präsidenten aus dem Jahr 2018\\. Eine technisch gleichwertige Replikation würde Trump als Stimulus verwenden (er war 2018 Präsident), eine psychologisch gleichwertige Studie im Jahr 2023 würde Biden als aktuellen Präsidenten verwenden.",
                "Related_terms": "** close replication; Conceptual replication; exact replication; hidden moderators"
            },
            {
                "Title": "Diversity *",
                "Definition": "** Diversity refers to between-person (i.e., interindividual) variation in humans, e.g. ability, age, beliefs, cognition, country, disability, ethnicity, gender, language, race, religion or sexual orientation. Diversity can refer to diversity of researchers (who do the research), the diversity of participant samples (who is included in the study), and diversity of perspectives (the views and beliefs researchers bring into their work; Syed & Kathawalla, 2020).",
                "Reference(s)": "** Syed and Kathawalla (2020)",
                "Drafted by": "** Ryan Millager; Mariella Paul",
                "Reviewed (or Edited) by": "** Nihan Albayrak-Aydemir; Mahmoud Elsherif; Helena Hartmann; Madeleine Ingham; Annalise A. LaPlume; Wanyin Li; Charlotte R. Pennington; Olly Robertson; Flávio Azevedo",
                "Translated by": "** Helena Hartmann",
                "Translation reviewed by": "** Susanne Vogel, Jennifer Mattschey   ### **E** {#e}",
                "Translation": "Diversity (Diversität) bezieht sich auf die zwischenmenschliche (d. h. interindividuelle) Variation bei Menschen, z. B. in Bezug auf Fähigkeiten, Alter, Überzeugungen, Kognition, Land, Behinderung, ethnische Zugehörigkeit, Geschlecht, Sprache, Herkunft, Religion oder sexuelle Orientierung. Diversität kann sich auf die Vielfalt der Forschenden (die die Forschung durchführen), die Vielfalt der Teilnehmendenstichproben (die in die Studie einbezogen werden) und die Vielfalt der Perspektiven (die Ansichten und Überzeugungen der Forschenden) beziehen.",
                "Related_terms": "** Bropenscience; BIZARRE; Decolonisation; Double Consciousness; Equity; Inclusion; STRANGE; WEIRD"
            },
            {
                "Title": "Early career researchers (ECRs, Nachwuchswissenschaftler:innen) *",
                "Definition": "** A label given to researchers who “range from senior doctoral students to postdoctoral workers who may have up to 10 years postdoctoral education; the latter group may therefore include early career or junior academics” (Eley et al., 2012, p. 3). What specifically (e.g. age, time since PhD inclusive or exclusive of career breaks and leave, title, funding awarded) constitutes an ECR can vary across funding bodies, academic organisations, and countries.",
                "Reference(s)": "** Bazeley (2003); Eley et al. (2012); Pownall et al (2021)",
                "Drafted by": "** Micah Vandegrift",
                "Reviewed (or Edited) by": "** Thomas Rhys Evans; Sam Parsons; Olly Robertson; Suzanne L. K. Stewart; Flávio Azevedo",
                "Translated by": "** Helena Hartmann",
                "Translation reviewed by": "** Susanne Vogel, Jennifer Mattschey",
                "Translation": "Eine Bezeichnung für Forschende, die von erfahrenen Doktorand:innen bis hin zu Postdoktorand:innen reichen, die bis zu zehn Jahre nach der Promotion ausgebildet wurden; die letztgenannte Gruppe kann daher auch Juniorprofessor:innen umfassen (Eley et al., 2012, S. 3). Was genau (z. B. Alter, Zeit seit der Promotion einschließlich oder ausschließlich von Karriere-Unterbrechungen und Urlaub / Freistellungen, Titel, bewilligte Fördermittel) eine/n ECR ausmacht, kann je nach Fördereinrichtung, akademischer Organisation und Land variieren.",
                "Related_terms": "** Early Career Investigator"
            },
            {
                "Title": "Economic and societal impact (wirtschaftliche und gesellschaftliche Auswirkungen *",
                "Definition": "** The contribution a research item makes to the broader economy and society. It also captures the benefits of research to individuals, organisations, and/or nations.",
                "Reference": "** https://esrc.ukri.org/research/impact-toolkit/what-is-impact/",
                "Originally drafted by": "** Adam Parker",
                "Reviewed (or Edited) by": "** Helena Hartmann; Aoife O’Mahony; Charlotte R. Pennington",
                "Translated by": "** Helena Hartmann",
                "Translation reviewed by": "** Susanne Vogel, Jennifer Mattschey",
                "Translation": "Der Beitrag, den ein Forschungsgegenstand für die Wirtschaft und Gesellschaft im weiteren Sinne leistet. Erfasst wird auch der Nutzen der Forschung für Einzelpersonen, Organisationen und/oder Nationen.",
                "Related_terms": "** Academic Impact"
            },
            {
                "Title": "Embargo Period (Sperrfrist) *",
                "Definition": "** Applied to Open Scholarship, in academic publishing, the period of time after an article has been published and before it can be made available as Open Access. If an author decides to self-archive their article (e.g., in an Open Access repository) they need to observe any embargo period a publisher might have in place. Embargo periods vary from instantaneous up to 48 months, with 6 and 12 months being common (Laakso & Björk, 2013). Embargo periods may also apply to pre-registrations, materials, and data, when authors decide to only make these available to the public after a certain period of time, for instance upon publication or even later when they have additional publication plans and want to avoid being scooped (Klein et al., 2018).",
                "Reference": "** Klein et al. (2018), Laakso and Björk (2013); [https://en.wikipedia.org/wiki/Embargo\\_(academic\\_publishing)](https://en.wikipedia.org/wiki/Embargo_\\(academic_publishing\\))",
                "Originally drafted by": "** Aleksandra Lazić",
                "Reviewed (or Edited) by": "** Bradley Baker; Adam Parker; Sam Parsons; Steven Verheyen; Flávio Azevedo",
                "Translated by": "** Helena Hartmann",
                "Translation reviewed by": "** Susanne Vogel, Jennifer Mattschey",
                "Translation": "Angewandt auf Open Scholarship (dt. Offene Forschung) beschreibt es im akademischen Verlagswesen die Zeitspanne, nachdem ein Artikel veröffentlicht wurde und bevor er offen (\"open-access\") zugänglich gemacht werden kann. Wenn Autor:innen beschließen, ihren Artikel selbst zu archivieren (z. B. in einem Open-Access-Repositorium), müssen sie alle Sperrfristen beachten. Sperrfristen variieren von sofort bis zu 48 Monaten, wobei 6 und 12 Monate üblich sind (Laakso & Björk, 2013). Sperrfristen können auch für Präregistrierungen, Materialien, und Daten gelten, wenn Autor:innen beschließen, diese erst nach einer bestimmten Zeit der Öffentlichkeit zur Verfügung zu stellen, beispielsweise bei der Veröffentlichung der Publikation oder sogar später, wenn sie zusätzliche Veröffentlichungspläne haben und vermeiden wollen, dass man ihnen zuvorkommt (Klein et al., 2018).",
                "Related_terms": "** Open access; Paywall; Preprint"
            },
            {
                "Title": "Epistemic uncertainty (epistemische Unsicherheit) *",
                "Definition": "** Systematic uncertainty due to limited data, measurement precision, model or process specification, or lack of knowledge. That is, uncertainty due to lack of knowledge that could, in theory, be reduced through conducting additional research to increase understanding. Such uncertainty is said to be personal, since knowledge differs across scientists, and temporary since it can change as new data become available.",
                "Reference(s)": "** Der Kiureghian and Ditlevsen (2009); Ferson et al., (2004) **Alternative terms:** Epistemic uncertainty is also known as knowledge uncertainty, subjective uncertainty, or type B uncertainty.",
                "Drafted by": "** Bradley Baker",
                "Reviewed (or Edited) by": "** Jamie P. Cockcroft; Elizabeth Collins; Charlotte R. Pennington; Graham Reid",
                "Translated by": "** Susanne Vogel",
                "Translation reviewed by": "** Helena Hartmann, Jennifer Mattschey",
                "Translation": "Systematische Unsicherheit aufgrund begrenzter Daten, Messgenauigkeit, Modell- oder Prozessspezifikation oder mangelnden Wissens. Das heißt, Unsicherheit aufgrund mangelnden Kenntnisstands, die theoretisch durch zusätzliche Forschung zur Verbesserung des Verständnisses verringert werden könnte. Eine solche Unsicherheit wird als persönlich bezeichnet, da sich das Wissen der Forschenden unterscheidet, und als vorübergehend, da sie sich ändern kann, wenn neue Daten verfügbar werden.",
                "Related_terms": "** Aleatoric uncertainty; Knightian uncertainty"
            },
            {
                "Title": "Epistemology (Epistemiologie / Erkenntnistheorie)  *",
                "Definition": "** Alongside ethics, logic, and metaphysics, epistemology is one of the four main branches of philosophy. Epistemology is largely concerned with nature, origin, and scope of knowledge, as well as the rationality of beliefs.",
                "Reference": "** Steup et al. (2020)",
                "Originally drafted by": "** Amélie Beffara Bret",
                "Reviewed (or Edited) by": "** Emma Norris; Adam Parker; Robert M Ross; Steven Verheyen",
                "Translated by": "** Helena Hartmann",
                "Translation reviewed by": "** Susanne Vogel, Jennifer Mattschey",
                "Translation": "Neben der Ethik, der Logik und der Metaphysik ist die Erkenntnistheorie einer der vier Hauptzweige der Philosophie. Die Erkenntnistheorie befasst sich weitgehend mit der Natur, dem Ursprung und dem Umfang des Wissens sowie mit der Rationalität von Überzeugungen.",
                "Related_terms": "** Meta-science or Meta-research ; Ontology (Artificial Intelligence)"
            },
            {
                "Title": "Equity  (Gleichstellung) *",
                "Definition": "** Different individuals have different starting positions (cf. “opportunity gaps”) and needs. Whereas equal treatment focuses on treating all individuals *equally*, equitable treatment aims to level the playing field by actively increasing opportunities for under-represented minorities. Equitable treatment aims to attain equality through “fairness”: taking into account different needs for support for different individuals, instead of focusing merely on the needs of the majority.",
                "Reference(s)": "** Albayrak-Aydemir (2020); Posselt (2020)",
                "Drafted by": "** Gisela H. Govaart",
                "Reviewed (or Edited) by": "** Nihan Albayrak-Aydemir; Mahmoud Elsherif; Ryan Millager; Charlotte R. Pennington; Beatrice Valentini; Flávio Azevedo",
                "Translated by": "** Helena Hartmann",
                "Translation reviewed by": "** Susanne Vogel, Jennifer Mattschey",
                "Translation": "Verschiedene Personen haben unterschiedliche Ausgangspositionen (vgl. \"Chancenlücken\") und Bedürfnisse. Während sich die Gleichbehandlung (\"equal treatment\") darauf konzentriert, alle Personen *gleich* zu behandeln, zielt die Gleichstellung darauf ab, die Chancengleichheit zu erhöhen, indem mehr Chancen für unterrepräsentierte Minderheiten hergestellt werden. Die Gleichstellung soll durch Fairness erreicht werden, d. h. durch die Berücksichtigung der unterschiedlichen Unterstützungsbedürfnisse der einzelnen Personen, anstatt sich nur auf die Bedürfnisse der Mehrheit zu konzentrieren.",
                "Related_terms": "** Diversity; Equality; Fairness; Inclusion; Social justice"
            },
            {
                "Title": "Equivalence Testing (Äquivalenztesten) *",
                "Definition": "** Equivalence tests statistically assess the null hypothesis that a given effect exceeds a minimum criterion to be considered meaningful. Thus, rejection of the null hypothesis provides evidence of a lack of (meaningful) effect. Based upon frequentist statistics, equivalence tests work by specifying equivalence bounds: a lower and upper value that reflect the smallest effect size of interest. Two one-sided *t*\\-tests are then conducted against each of these equivalence bounds to assess whether effects that are deemed meaningful can be *rejected* (see Schuirmann, 1972; Lakens et al., 2018; 2020).",
                "Reference": "** Lakens et al. (2018); Lakens et al. (2020); Schuirmann (1987)",
                "Originally drafted by": "** Charlotte R. Pennington",
                "Reviewed (or Edited) by": "** Bradley Baker; James E. Bartlett; Jamie P. Cockcroft; Tobias Wingen; Flávio Azevedo",
                "Translated by": "** Susanne Vogel",
                "Translation reviewed by": "** Helena Hartmann, Jennifer Mattschey",
                "Translation": "Äquivalenztests bewerten statistisch die Nullhypothese, dass ein bestimmter Effekt ein Minimalkriterium überschreitet, um als bedeutsam eingestuft zu werden. Die Ablehnung der Nullhypothese ist somit ein Beweis für das Fehlen eines (bedeutsamen) Effekts. Äquivalenztests basieren auf der frequentistischen Statistik und funktionieren durch die Festlegung von Äquivalenzgrenzen: einer unteren und einer oberen Grenze, die die kleinste interessierende Effektgröße widerspiegeln. Anschließend werden zwei einseitige *t*\\-Tests gegen jede dieser Äquivalenzgrenzen durchgeführt, um festzustellen, ob als bedeutsam erachteten Effekte *abgelehnt* werden können (siehe Schuirmann, 1972; Lakens et al., 2018; 2020).",
                "Related_terms": "** Equivalence bounds; Falsification; Frequentist analyses; Inference by confidence intervals; Null Hypothesis Significance Testing (NHST); Smallest effect size of interest (SESOI); TOSTER; TOST procedure."
            },
            {
                "Title": "Error detection (Fehlererfassung) *",
                "Definition": "** Broadly refers to examining research data and manuscripts for mistakes or inconsistencies in reporting. Commonly discussed approaches include: checking inconsistencies in descriptive statistics (e.g. summary statistics that are not possible given the sample size and measure characteristics; Brown & Heathers, 2017; Heathers et al. 2018), inconsistencies in reported statistics (e.g. *p*\\-values that do not match the reported *F* statistics and accompanying degrees of freedom; Epskamp, & Nuijten, 2016; Nuijten et al. 2016), and image manipulation (Bik et al., 2016). Error detection is one motivation for data and analysis code to be openly available, so that peer review can confirm a manuscript’s findings, or if already published, the record can be corrected. Detected errors can result in corrections or retractions of published articles, though these actions are often delayed, long after erroneous findings have influenced and impacted further research.",
                "Reference(s)": "** Bik et al. (2016); Brown and Heathers (2017); Epskamp and Nuijten (2016); Heathers et al. (2018); Nuijten et al. (2016); [https://retractionwatch.com/](https://retractionwatch.com/)",
                "Drafted by": "** William Ngiam",
                "Reviewed (or Edited) by": "** Ali H. Al-Hoorie; Jamie P. Cockcroft; Dominik Kiersz; Sam Parsons; Suzanne L. K. Stewart; Marta Topor",
                "Translated by": "** Helena Hartmann",
                "Translation reviewed by": "** Susanne Vogel, Jennifer Mattschey",
                "Translation": "Bezieht sich allgemein auf die Prüfung von Forschungsdaten und Manuskripten auf Fehler oder Unstimmigkeiten in der Berichterstattung. Zu den häufig diskutierten Ansätzen gehören: die Überprüfung von Unstimmigkeiten in der deskriptiven Statistik (z. B. zusammenfassende Statistiken, die angesichts des Stichprobenumfangs und der Messmerkmale nicht möglich sind; Brown & Heathers, 2017; Heathers et al. 2018), Unstimmigkeiten in den berichteten Statistiken (z. B. p-Werte, die nicht mit den berichteten F-Statistiken und den dazugehörigen Freiheitsgraden übereinstimmen; Epskamp, & Nuijten, 2016; Nuijten et al. 2016\\) und Bildmanipulationen (Bik et al., 2016). Die Erfassung von  Fehlern ist eine Motivation dafür, dass Daten und Analysecodes offen zugänglich sein sollen, so dass die Ergebnisse eines Manuskripts durch Gutachten bestätigt werden können oder, falls sie bereits veröffentlicht wurden, die Aufzeichnungen korrigiert werden können. Entdeckte Fehler können zu Korrekturen oder zum Rückzug veröffentlichter Artikel führen, auch wenn sich diese Maßnahmen oft verzögern, lange nachdem die fehlerhaften Ergebnisse die weitere Forschung beeinflusst und beeinträchtigt haben.",
                "Related_terms": "** Research integrity; correction; retraction"
            },
            {
                "Title": "Evidence Synthesis (Evidenzsynthese) *",
                "Definition": "** This is a type of research method which aims to draw general conclusions to address a research question on a certain topic, phenomenon or effect by reviewing research outcomes and information from a range of different sources. Information which is subject to synthesis can be extracted from both qualitative and quantitative studies. The method used to synthesise the gathered information can be qualitative (narrative synthesis), quantitative (meta-analysis) or mixed (meta-synthesis, systematic mapping). Evidence synthesis has many applications and is often used in the context of healthcare, public policy as well as understanding and advancement of specific research fields.",
                "Reference": "** Centre for Evaluation (n.d.); James et al., (2016); Siddaway et al. (2019)",
                "Originally drafted by": "** Marta Topor",
                "Reviewed (or Edited) by": "** Aoife O’Mahony; Tamara Kalandadze; Adam Parker; Charlotte R. Pennington",
                "Translated by": "** Susanne Vogel",
                "Translation reviewed by": "** Helena Hartmann, Jennifer Mattschey",
                "Translation": "Hierbei handelt es sich um eine Forschungsmethode, die darauf abzielt, allgemeine Schlussfolgerungen zur Beantwortung einer Forschungsfrage zu einem bestimmten Thema, Phänomen oder Effekt zu ziehen, indem Forschungsergebnisse und Informationen aus vielen unterschiedlichen Quellen betrachtet werden. Informationen, die einer Synthese unterzogen werden, können sowohl aus qualitativen als auch aus quantitativen Studien gewonnen werden. Die zur Synthese der gesammelten Informationen verwendete Methode kann qualitativ (narrative Synthese), quantitativ (Metaanalyse) oder gemischt (Metasynthese, systematisches Mapping) sein. Evidenzsynthese ist vielseitig anwendbar und wird häufig im Kontext der Gesundheitsversorgung, der Öffentlichkeitspolitik sowie des Verständnisses und der Förderung spezifischer Forschungsbereiche eingesetzt.",
                "Related_terms": "** Literature Review; Meta-analysis; Meta-synthesis; Meta-science or Meta-research; Narrative review; Scoping review; Systematic map; Systematic review"
            },
            {
                "Title": "Exploratory data analysis (Explorative Datenanalyse) *",
                "Definition": "** Exploratory Data Analysis (EDA) is a well-established statistical tradition that provides conceptual and computational tools for discovering patterns in data to foster hypothesis development and refinement. These tools and attitudes complement the use of hypothesis tests used in confirmatory data analysis (CDA). Even when well-specified theories are held, EDA helps one interpret the results of CDA and may reveal unexpected or misleading patterns in the data.",
                "Reference(s)": "** Behrens (1997); Box (1976); Tukey (1977); Wagenmakers (2012)",
                "Drafted by": "** Jenny Terry",
                "Reviewed (or Edited) by": "** Helena Hartmann; Timo Roettger; Charlotte R. Pennington; Flávio Azevedo",
                "Translated by": "** Helena Hartmann",
                "Translation reviewed by": "** Susanne Vogel, Jennifer Mattschey",
                "Translation": "Die explorative Datenanalyse (EDA) ist eine etablierte statistische Tradition, die konzeptionelle und computergestützte Instrumente zur Entdeckung von Mustern in Daten bereitstellt, um die Entwicklung und Verfeinerung von Hypothesen zu fördern. Diese Werkzeuge und Einstellungen ergänzen den Einsatz von Hypothesentests, die in der konfirmatorischen Datenanalyse (CDA) verwendet werden. Selbst wenn gut begründete Theorien vorliegen, kann EDA helfen, die Ergebnisse einer CDA zu interpretieren und unerwartete oder irreführende Muster in den Daten aufzudecken.",
                "Related_terms": "** Confirmatory analyses; Data-driven research; Exploratory research"
            },
            {
                "Title": "External Validity (externe Validität) *",
                "Definition": "** Whether the findings of a scientific study can be generalized to other contexts outside the study context (different measures, settings, people, places, and times). Statistically, threats to external validity may reflect interactions whereby the effect of one factor (the independent variable) depends on another factor (a confounding variable). External validity may also be limited by the study design (e.g., an artificial laboratory setting or a non-representative sample).",
                "Reference": "** Cook and Campbell (1979); Lynch (1982); Steckler and McLeroy (2008) **Alternative definition:** (if applicable) In Psychometrics, the degree of evidence that confirms the relations of a tested psychological construct with external variables **Related terms to alternative definition:** Criterion validity; Convergent validity; Divergent validity",
                "Reference(s)": "** Messick (1995)",
                "Originally drafted by": "** Annalise A. LaPlume",
                "Reviewed (or Edited) by": "** Mahmoud Elsherif; Helena Hartmann; Kai Krautter; Oscar Lecuona; Flávio Azevedo",
                "Translated by": "** Helena Hartmann",
                "Translation reviewed by": "** Susanne Vogel, Jennifer Mattschey   ###  ### **F** {#f}",
                "Translation": "Ob die Ergebnisse einer wissenschaftlichen Studie auf andere Kontexte außerhalb des Studienkontextes verallgemeinert werden können (andere Maße, Rahmenbedingungen, Personen, Orte und Zeiten). Statistisch gesehen können Bedrohungen der externen Validität Wechselwirkungen widerspiegeln, bei denen die Wirkung eines Faktors (der unabhängigen Variable) von einem anderen Faktor (einer Störvariable) abhängt. Die externe Validität kann auch durch das Studiendesign eingeschränkt werden (z. B. durch eine künstliche Laborumgebung oder eine nicht repräsentative Stichprobe).",
                "Related_terms": "** Constraints on Generality (COG); Internal validity; Generalizability; Representativity; Validity"
            },
            {
                "Title": "Face validity (Augenscheinvalidität) *",
                "Definition": "** A subjective judgement of how suitable a measure appears to be on the surface, that is, how well a measure is operationalized. For example, judging whether questionnaire items should relate to a construct of interest at face value. Face validity is related to construct validity, but since it is subjective/informal, it is considered an easy but weak form of validity.",
                "Reference": "** Holden (2010)",
                "Originally drafted by": "** Annalise A. LaPlume",
                "Reviewed (or Edited) by": "** Helena Hartmann; Kai Krautter; Adam Parker; Sam Parsons",
                "Translated by": "** Susanne Vogel",
                "Translation reviewed by": "** Helena Hartmann, Jennifer Mattschey",
                "Translation": "Ein subjektives Urteil darüber, wie geeignet ein Maß auf den ersten Blick zu sein scheint, d. h. wie gut ein Maß operationalisiert ist. Zum Beispiel die Beurteilung, ob sich die Items eines Fragebogens auf ein interessierendes Konstrukt beziehen. Die Augenscheinvalidität ist mit der Konstruktvalidität verwandt, aber da sie subjektiv/informell ist, wird sie als eine einfache, aber schwache Form der Validität angesehen.",
                "Related_terms": "** Construct Validity; Content Validity; Logical Validity; Operationalization; Validity"
            },
            {
                "Title": "FAIR principles (FAIR Prinzipien) *",
                "Definition": "** Describes making scholarly materials Findable, Accessible, Interoperable and Reusable (FAIR). ‘Findable’ and ‘Accessible’ are concerned with where materials are stored (e.g. in data repositories), while ‘Interoperable’ and ‘Reusable’ focus on the importance of data formats and how such formats might change in the future.",
                "Reference(s)": "** Crüwell et al. (2019); Wilkinson et al. (2016); [https://www.go-fair.org/fair-principles/](https://www.go-fair.org/fair-principles/)",
                "Drafted by": "** Sonia Rishi",
                "Reviewed (or Edited) by": "** Mahmoud Elsherif; Sam Parsons; Charlotte R. Pennington",
                "Translated by": "** Susanne Vogel",
                "Translation reviewed by": "** Helena Hartmann, Jennifer Mattschey",
                "Translation": "Beschreibt das Prinzip, wissenschaftliche Materialien Findable (auffindbar), Accessible (zugänglich), Interoperable (interoperabel) und Reusable (wiederverwendbar) zu machen (FAIR). Auffindbar und Zugänglich beziehen sich darauf, wo die Materialien gespeichert sind (z. B. in Datenarchiven), während Interoperabel und Wiederverwendbar sich auf die Bedeutung von Datenformaten und deren mögliche künftige Veränderung konzentrieren.",
                "Related_terms": "** Metadata; Open Access; Open Code; Open Data; Open Material; Repository"
            },
            {
                "Title": "Feminist psychology (Feministische Psychologie) *",
                "Definition": "** With a particular focus on gender and sexuality, feminist psychology is inherently concerned with representation, diversity, inclusion, accessibility, and equality. Feminist psychology initially grew out out of a concern for representing the lived experiences of girls and women, but has since evolved into a more nuanced, intersectional and comprehensive concern for all aspects of equality (e.g., Eagly & Riger, 2014). Feminist psychologists have advocated for more rigorous consideration of equality, diversity, and inclusion within Open Science spaces (Pownall et al., 2021).",
                "Reference": "** Eagly and Riger (2014); Grzanka (2020); Pownall et al (2021)",
                "Originally drafted by": "** Madeleine Pownall",
                "Reviewed (or Edited) by": "** Mahmoud Elsherif; Helena Hartmann; Kai Krautter; Charlotte R. Pennington",
                "Translated by": "** Helena Hartmann",
                "Translation reviewed by": "** Susanne Vogel, Jennifer Mattschey",
                "Translation": "Mit einem besonderen Schwerpunkt auf Geschlecht und Sexualität befasst sich die feministische Psychologie von Natur aus mit Repräsentation, Diversität, Inklusion, Zugänglichkeit und Gleichberechtigung. Feministische Psychologie entstand ursprünglich aus dem Anliegen, die gelebten Erfahrungen von Mädchen und Frauen zu repräsentieren, hat sich aber seither zu einem differenzierteren, intersektionalen und umfassenden Anliegen für alle Aspekte der Gleichberechtigung entwickelt (z. B. Eagly & Riger, 2014). Feministische Psycholog:innen haben sich für eine strengere Berücksichtigung von Gleichheit, Vielfalt und Inklusion in Open-Science-Bereichen eingesetzt (Pownall et al., 2021).",
                "Related_terms": "** Inclusion; Positionality; Reflexivity; Under-representation; Equity"
            },
            {
                "Title": "First-last-author-emphasis norm (FLAE, Norm der Betonung der Erst- und Letztautor:innenschaft) *",
                "Definition": "** An authorship system that assigns the order of authorship depending on the contributions of a given author while simultaneously valuing the first and last position of the authorship order most. According to this system, the two main authors are indicated as the first and last author \\- the order of the authors between the first and last position is determined by contribution in a descending order.",
                "Reference": "** Tscharntke et al. (2007)",
                "Originally drafted by": "** Myriam A. Baum",
                "Reviewed (or Edited) by": "** Charlotte R. Pennington",
                "Translated by": "** Helena Hartmann",
                "Translation reviewed by": "** Susanne Vogel, Jennifer Mattschey",
                "Translation": "Ein System der Autor:innenschaft, das die Reihenfolge der Autor:innen in Abhängigkeit von den Leistungen einer/eines bestimmten Autorin/Autors festlegt und gleichzeitig die erste und letzte Position der Autor:innenreihenfolge am höchsten bewertet. Nach diesem System werden die beiden Hauptautoren als erster und letzter Autor angegeben \\- die Reihenfolge der Autor:innen zwischen der ersten und letzten Position wird durch den Leistungsumfang in absteigender Reihenfolge bestimmt.",
                "Related_terms": "** Authorship; Author contributions; CreDit taxonomy"
            },
            {
                "Title": "FORRT *",
                "Definition": "** Framework for Open and Reproducible Research Training. It aims to provide a pedagogical infrastructure designed to recognize and support the teaching and mentoring of open and reproducible research in tandem with prototypical subject matters in higher education. FORRT strives to be an effective, evolving, and community-driven organization raising awareness of the pedagogical implications of open and reproducible science and its associated challenges (i.e., curricular reform, epistemological uncertainty, methods of education). FORRT also advocates for the opening of teaching and mentoring materials as a means to facilitate access, discovery, and learning to those who otherwise would be educationally disenfranchised.",
                "Reference(s)": "** [FORRT \\- Framework for Open and Reproducible Research Training](https://forrt.org/); F (2019)",
                "Drafted by": "** Tamara Kalandadze",
                "Reviewed (or Edited) by": "** Mahmoud Elsherif; Charlotte R. Pennington; Flávio Azevedo",
                "Translated by": "** Susanne Vogel",
                "Translation reviewed by": "** Helena Hartmann, Jennifer Mattschey",
                "Translation": "Framework for Open and Reproducible Research Training (dt. Konzept für offene und reproduzierbare Forschung und Lehre). Es zielt darauf ab, eine pädagogische Infrastruktur bereitzustellen, die die Lehre und Betreuung von offener und reproduzierbarer Forschung in Verbindung mit prototypischen Fächern in der Hochschulbildung anerkennt und unterstützt. FORRT ist bestrebt, eine effektive, sich entwickelnde und von der Gemeinschaft getragene Organisation zu sein, die für die pädagogischen Implikationen offener und reproduzierbarer Wissenschaft und die damit verbundenen Herausforderungen (d. h. Lehrplanreform, epistemologische Unsicherheit, Lehrmethoden) sensibilisiert. FORRT setzt sich auch für die Öffnung von Lehr- und Betreuungsmaterialien ein, um den Zugang, das Entdecken und das Lernen für diejenigen zu erleichtern, die sonst von der Bildung ausgeschlossen wären.",
                "Related_terms": "** Integrating open and reproducible science tenets into higher education"
            },
            {
                "Title": "Free Our Knowledge Platform *",
                "Definition": "** A collective action platform aiming to support the open science movement by obtaining pledges from researchers that they will implement certain research practices (e.g., pre-registration, pre-print). Initially pledges will be anonymous until a sufficient number of people pledge, upon which names of pledges will be released. The initiative is a grassroots movement instigated by early career researchers.",
                "Reference(s)": "** [https://freeourknowledge.org/about/](https://freeourknowledge.org/about/)",
                "Drafted by": "** Jamie P. Cockcroft",
                "Reviewed (or Edited) by": "** Ashley Blake; Elizabeth Collins; Mahmoud Elsherif; Sam Parsons; Flávio Azevedo",
                "Translated by": "** Helena Hartmann",
                "Translation reviewed by": "** Susanne Vogel,  Jennifer Mattschey   ###   ### **Looking for something else?**  We have separated the glossary project across several documents, see links below:  Landing page:\t\t\t[Glossary Translations German template](https://docs.google.com/document/d/1IIZK-F9SX1P4UrPlZeEKgUAOyiEjNbGFSWafr0ADb0M/edit) Letters A \\- F:\t\t\t[German Glossary | Phase 2 | A-F](https://docs.google.com/document/d/1yQH_iYm7FgjVGtJyQWGv7iv1kt4GkUWeRbXHch57Fuo/edit) Letters G \\- L:\t\t\t[German Glossary | Phase 2 | G-L](https://docs.google.com/document/d/1MbfcDK3G6ybzkkq2jVz36q4TcqQMGpNG2cun6GOeuSQ/edit#) Letters M \\- R: \t\t\t[German Glossary | Phase 2 | M - R](https://docs.google.com/document/d/1sv2C1Y-z3WeiYjvhn2B8rhFyPK5YQ8G3sCCh3ZYwV2Q/edit#heading=h.w0bgiwj800db) Letters S \\- Z:\t\t\t[German Glossary | Phase 2 | S - Z](https://docs.google.com/document/d/1pORanWNHkMRkGYs8vNxdetx907gBv-fm8l-tV8XRIiE/edit) References: \t\t\t[Glossary | Phase 2 | References](https://docs.google.com/document/d/12_F8kbnl2GP66iBkIejJeX2KnkZ13iC_jj7VVYZMxpA/edit?usp=sharing) Terms not yet defined: \t\t[Glossary | Phase 2 | Terms not yet defined](https://docs.google.com/document/d/16FGodUkNoNrBYyq2JqHJMNzYuZf-QbsigooMpB_ns_E/edit?usp=sharing) Contributors spreadsheet: [Glossary Translations German tenzing contributions \\[FORRT\\]](https://docs.google.com/spreadsheets/d/1UEM7s27b5pOlrIYX9-fXYdPOmPpZZyoggJOfOIrOhko/edit#gid=0)  [FORRT slack](https://join.slack.com/t/forrt/shared_invite/zt-alobr3z7-NOR0mTBfD1vKXn9qlOKqaQ) \\- join us\\! [FORRT email](mailto:FORRTproject@gmail.com) \\- contact us\\!",
                "Translation": "Eine kollektive Handlungsplattform, die darauf abzielt, die Open-Science-Bewegung zu unterstützen, indem sie Zusagen von Forschenden einholt, dass diese bestimmte Forschungspraktiken (z. B. Präregistrierung, Preprint) anwenden werden. Bei der Initiative handelt es sich um eine Initiative, die von Nachwuchswissenschaftler:innen ins Leben gerufen wurde.",
                "Related_terms": "** Open Science; Preregistration Pledge"
            },
            {
                "Title": "GPower *",
                "Definition": "** Free to use statistical software for performing power analyses. The user specifies the desired statistical test (e.g. t-test, regression, ANOVA), and three of the following: the number of groups/observations, effect size, significance level, or power, in order to calculate the unspecified aspect.",
                "Reference": "** Faul et al. (2007); Faul et al. (2009)",
                "Originally drafted by": "** Filip Dechterenko",
                "Reviewed (or Edited) by": "** Thomas Rhys Evans; Kai Krautter; Charlotte R. Pennington",
                "Translated by": "Bettina MJ Kern",
                "Translation reviewed by": "Susanne Vogel, Helena Hartmann",
                "Translation": "Kostenlose Statistiksoftware zur Durchführung von Power-Analysen. Man gibt den gewünschten statistischen Test (z. B. *t*\\-Test, Regression, ANOVA) und drei der folgenden Punkte an: die Anzahl der Gruppen/Beobachtungen, die Effektgröße, das Signifikanzniveau oder die Power. Aus den drei ausgewählten Parametern wird dann der vierte Parameter, der nicht ausgewählt wurde, bestimmt.",
                "Related_terms": "** Power analysis; Sample size justification; Sample size planning; Statistical power"
            },
            {
                "Title": "Gaming (the system) (das System überlisten) *",
                "Definition": "** Adopting questionable research practices (QRPs, e.g., salami slicing of an academic paper) that would align with academic incentive structures that benefit the academic (e.g. in prestige, hiring, or promotion) regardless of whether they support the process of scholarship. If systems rely on metrics to determine an outcome (e.g. academic credit) those metrics can be subject to intentional manipulation (Naudet et al., 2018\\) or “gamed”. Where promotions, hiring, and tenure are based on flawed metrics they may disfavor openness, rigor, and transparent work (Naudet et al., 2018\\) \\- for example favoring “quantity over quality” \\- and exacerbate existing inequalities.",
                "Reference(s)": "** Moher et al. (2018); Naudet et al. (2018)",
                "Drafted by": "** Adrien Fillon",
                "Reviewed (or Edited) by": "** Sarah Ashcroft-Jones; Helena Hartmann; Sam Parsons; Charlotte R. Pennington",
                "Translated by": "Bettina MJ Kern",
                "Translation reviewed by": "Birgit Schmidt, Susanne Vogel",
                "Translation": "Bezeichnet den Einsatz fragwürdiger Forschungspraktiken (*Questionable Research Practices*; QRPs, wie beispielsweise Salami-Slicing eines akademischen Papers), die aufgrund der akademischen Anreizstrukturen dem Forschenden zugutekommen (z. B. in Bezug auf Prestige, Einstellung oder Beförderung), unabhängig davon, ob diese Praxis den wissenschaftlichen Prozess als solchen unterstützt. Wann immer ein System zur Bestimmung eines Ergebnisses (z. B. akademische Leistungen) auf quantitative Kennzahlen angewiesen ist, können diese Kennzahlen absichtlich und gezielt manipuliert oder “überlistet” werden (Naudet et al., 2018). Wenn Beförderungen, Einstellungen und Entfristungen auf solchen manipulierbaren Kennzahlen beruhen, können sie offene, genaue und transparente Forschungsleistungen benachteiligen (Naudet et al., 2018\\) \\- z. B. durch die Bevorzugung von Quantität gegenüber Qualität \\- und bestehende Ungleichheiten verschärfen.",
                "Related_terms": "** Incentive structure; Journal Impact Factor; *P*\\-hacking"
            },
            {
                "Title": "Garden of forking paths (Garten der Weggabelungen) *",
                "Definition": "** The typically-invisible decision tree traversed during operationalization and statistical analysis given that ‘there is a one-to-many mapping from scientific to statistical hypotheses' (Gelman and Loken, 2013, p. 6). In other words, even in absence of p-hacking or fishing expeditions and when the research hypothesis was posited ahead of time, there can be a plethora of statistical results that can appear to be supported by theory given data. “The problem is there can be a large number of potential comparisons when the details of data analysis are highly contingent on data, without the researcher having to perform any conscious procedure of fishing or examining multiple p-values” (Gelman and Loken, 2013, p. 1). The term aims to highlight the uncertainty ensuing from idiosyncratic analytical and statistical choices in mapping theory-to-test, and contrasting intentional (and unethical) questionable research practices (e.g. p-hacking and fishing expeditions) versus non-intentional research practices that can, potentially, have the same effect despite not having intent to corrupt their results. The garden of forking paths refers to the decisions during the scientific process that inflate the false-positive rate as a consequence of the potential paths which could have been taken (had other decisions been made).",
                "Reference(s)": "** Gelman and Loken (2013)",
                "Drafted by": "** Flávio Azevedo; Mahmoud Elsherif",
                "Reviewed (or Edited) by": "** Gisela H. Govaart; Matt Jaquiery; Tamara Kalandadze; Charlotte R. Pennington",
                "Translated by": "Bettina MJ Kern",
                "Translation reviewed by": "Helena Hartmann, Jennifer Mattschey",
                "Translation": "Der in der Regel unsichtbare Entscheidungsbaum, der während der Operationalisierung und der statistischen Analyse entsteht, da ‘there is a one-to-many mapping from scientific to statistical hypotheses' (dt. es eine Eins-zu-Viele-Zuordnung von wissenschaftlichen zu statistischen Hypothesen gibt; Gelman und Loken, 2013, S. 6). Auch ohne *p*\\-hacking oder fishing expeditions und HARKing, kann es eine Vielzahl von teils sehr unterschiedlichen statistischen Ergebnissen geben, die alle durch die Theorie und die Daten gestützt zu sein scheinen. “The problem is there can be a large number of potential comparisons when the details of data analysis are highly contingent on data, without the researcher having to perform any conscious procedure of fishing or examining multiple p-values” (dt. Das Problem ist, dass es eine große Anzahl potenzieller Vergleiche geben kann, wenn die Details der Datenanalyse in hohem Maße von den Daten abhängen, ohne dass der Forschende ein bewusstes Verfahren des \"fishing\" oder die Berechnung mehrerer *p*\\-Werte durchführen muss; Gelman und Loken, 2013, S. 1). Der Begriff des Gartens der Weggabelungen zielt darauf ab, die Unsicherheit zu unterstreichen, die sich aus analytischen und statistischen Entscheidungen bei der Zuordnung von Theorien zu Tests ergibt. Zugleich soll das Konzept absichtliche (und unethische) fragwürdige Forschungspraktiken (z. B. *p*\\-hacking und fishing expeditions) mit “normalen”, nicht-fragwürdigen Forschungspraktiken kontrastieren, die potenziell denselben Effekt haben können, ohne dass die Absicht besteht, ihre Ergebnisse zu verfälschen. Der Garten der Weggabelungen (garden of forking paths) bezieht sich auf alle Entscheidungen während des wissenschaftlichen Prozesses, die die Falsch-Positiv-Rate als Folge der möglichen Wege, die man hätte einschlagen können (wenn andere Entscheidungen getroffen worden wären), in die Höhe treiben.",
                "Related_terms": "** False-positive; Familywise error; Multiverse Analysis; Preregistration; Researcher degrees of freedom; Specification Curve Analysis"
            },
            {
                "Title": "General Data Protection Regulation (GDPR, dt. Datenschutzgrundverordnung DSGVO) *",
                "Definition": "** A legal framework of seven principles implemented across the European Union (EU) that aims to safeguard individuals’ information. The framework seeks to commission citizens with control over their personal data, whilst regulating the parties involved in storing and processing these data. This set of legislation dictates the free movement of individuals’ personal information both within and outside the EU and must be considered by researchers when designing and running studies.",
                "Reference": "** Crutzen et al. (2019); [https://ico.org.uk/for-organisations/guide-to-data-protection/guide-to-the-general-data-protection-regulation-gdpr/](https://ico.org.uk/for-organisations/guide-to-data-protection/guide-to-the-general-data-protection-regulation-gdpr/); [https://ec.europa.eu/info/law/law-topic/data-protection\\_en](https://ec.europa.eu/info/law/law-topic/data-protection_en)",
                "Originally drafted by": "** Graham Reid",
                "Reviewed (or Edited) by": "** Elizabeth Collins; Mahmoud Elsherif; Christopher Graham; Sam Parsons",
                "Translated by": "Bettina MJ Kern",
                "Translation reviewed by": "Susanne Vogel, Helena Hartmann",
                "Translation": "Ein rechtlicher Rahmen mit sieben Grundsätzen, der in der gesamten Europäischen Union (EU) umgesetzt wurde und den Schutz persönlicher Daten zum Ziel hat. Dieser Rahmen soll den Bürger:innen Kontrolle über ihre personenbezogenen Daten geben und gleichzeitig die an der Speicherung und Verarbeitung dieser Daten beteiligten Parteien regulieren. Dieser Rechtsrahmen bezieht sich auf den freien Verkehr personenbezogener Daten innerhalb und außerhalb der EU und muss von Forschenden bei der Konzeption und Durchführung von Studien berücksichtigt werden.",
                "Related_terms": "** Anonymity; Data Management Plan (DMP); Data sharing; Repeatability; Replicability; Reproducibility"
            },
            {
                "Title": "Generalizability (Generalisierbarkeit) *",
                "Definition": "** Generalizability refers to how applicable a study’s results are to broader groups of people, settings, or situations they study and how the findings relate to this wider context (Frey, 2018; Kukull & Ganguli, 2012).",
                "Reference(s)": "** Esterling et al. (2021); Frey (2018); Kukull and Ganguli (2012); LeBel et al. (2017); Nosek and Errington (2020); Yarkoni (2020)",
                "Drafted by": "** Aoife O’Mahony",
                "Reviewed (or Edited) by": "** Adrien Fillon; Matt Jaquiery; Tina Lonsdorf; Sam Parsons; Julia Wolska",
                "Translated by": "Bettina MJ Kern",
                "Translation reviewed by": "Susanne Vogel, Helena Hartmann",
                "Translation": "Generalisierbarkeit bezieht sich darauf, inwieweit die Ergebnisse einer Studie auf breitere Personengruppen, Rahmenbedingungen oder Situationen übertragen werden können und wie sich die Ergebnisse auf diesen breiteren Kontext beziehen (Frey, 2018; Kukull & Ganguli, 2012).",
                "Related_terms": "** Conceptual replication; External Validity; Opportunistic sampling; Sampling bias; WEIRD **Alternative definition:** Applying modified materials and/or analysis pipelines to new data or samples to answer the same hypothesis (different materials, different data) to test how generalizable the effect under study is (The Turing Way Community & Scriberia, 2021). **Related terms to alternative definition:** (if applicable): Conceptual Replication"
            },
            {
                "Title": "Gift (or Guest) Authorship (geschenkte Autor:innenschaft / Gastautor:innenschaft) *",
                "Definition": "** The inclusion in an article’s author list of individuals who do not meet the criteria for authorship. As authorship is associated with benefits including peer recognition and financial rewards, there are incentives for inclusion as an author on published research. Gifting authorship, or extending authorship credit to an individual who does not merit such recognition, can be intended to help the gift recipient, repay favors (including reciprocal gift authorship), maintain personal and professional relationships, and enhance chances of publication. Gift authorship is widely considered an unethical practice.",
                "Reference": "** Bhopal et al. (1997); ICMJE (2019)",
                "Originally drafted by": "** Bradley Baker",
                "Reviewed (or Edited) by": "** Helena Hartmann; Aoife O’Mahony; Sam Parsons; Charlotte R. Pennington; Flávio Azevedo",
                "Translated by": "Bettina MJ Kern",
                "Translation reviewed by": "Birgit Schmidt, Susanne Vogel",
                "Translation": "Der Begriff bezeichnet die Aufnahme von Personen in die Autor:innenliste eines Forschungsartikels, welche die Kriterien für eine Mitautor:innenschaft nicht erfüllen. Da die Autor:innenschaft mit Vorteilen wie Anerkennung durch Fachkolleg:innen und finanziellen Prämien verbunden ist, gibt es Anreize, auf der Autor:innenliste von publizierten Forschungsarbeiten zu stehen. Das Verschenken von Autor:innenschaft oder das Gewähren von Autor:innenschaft an eine Person, die eine solche Anerkennung nicht verdient, kann somit dazu dienen, einen Gefallen zu erwidern (einschließlich gegenseitiger geschenkter Autor:innenschaft), persönliche und berufliche Beziehungen zu pflegen und die Chancen auf eine Veröffentlichung zu erhöhen. Geschenkte Autorenschaft wird oft als unethische Forschungspraxis angesehen.",
                "Related_terms": "** Authorship; CRediT"
            },
            {
                "Title": "Git *",
                "Definition": "** A software package for tracking changes in a local set of files (local version control), initially developed by Linus Torvalds. In general, it is used by programmers to track and develop computer source code within a set directory, folder or a file system. Git can access remote repository hosting services (e.g. GitHub) for remote version control that enables collaborative software development by uploading contributions from a local system. This process found its way into the scientific process to enable open data, open code and reproducible analyses.",
                "Reference": "** Kalliamvakov et al. (2014); Scopatz and Huff (2015); Vuorre and Curley (2018); [https://github.com/git/git/commit/e83c5163316f89bfbde7d9ab23ca2e25604af290](https://github.com/git/git/commit/e83c5163316f89bfbde7d9ab23ca2e25604af290)",
                "Originally drafted by": "** Emma Norris",
                "Reviewed (or Edited) by": "** Adrien Fillon; Bettina M.J. Kern; Dominik Kiersz; Robert M. Ross",
                "Translated by": "Bettina MJ Kern",
                "Translation reviewed by": "Helena Hartmann, Jennifer Mattschey",
                "Translation": "Ein Softwarepaket zur Nachverfolgung von Änderungen in einer lokalen Gruppe von Dateien (lokale Versionskontrolle), ursprünglich entwickelt von Linus Torvalds. Im Allgemeinen wird es von Programmierenden bei der Entwicklung von Computer-Quellcode innerhalb eines bestimmten Verzeichnisses, Ordners oder Dateisystems verwendet. Git kann auf Remote-Repository-Hosting-Dienste (z. B. GitHub) zugreifen, um eine Remote-Versionskontrolle zu ermöglichen, die eine kollaborative Softwareentwicklung durch Hochladen von Änderungen von einem lokalen System ermöglicht. Dieser Prozess hat seinen Weg in den wissenschaftlichen Prozess gefunden, um offene Daten, offenen Code und reproduzierbare Analysen zu ermöglichen.",
                "Related_terms": "** GitHub; Repository; Version control"
            },
            {
                "Title": "Goodhart’s Law (Goodharts Gesetz) *",
                "Definition": "** A term coined by economist Charles Goodhart to refer to the observation that measuring something inherently changes user behaviour. In relation to examination performance, Strathern (1997) stated that “when a measure becomes a target, it ceases to be a good measure” (p. 308). Applied to open scholarship, and the structure of incentives in academia, Goodhart’s Law would predict that metrics of scientific evaluation will likely be abused and exploited, as evidenced by Muller (2019)",
                "Originally drafted by": "** Adam Parker",
                "Reviewed (or Edited) by": "Sam Parsons; Flávio Azevedo",
                "Translated by": "Bettina MJ Kern",
                "Translation reviewed by": "Birgit Schmidt, Susanne Vogel  ### ---  ### **H** {#h}",
                "Translation": "Der Begriff geht auf den Wirtschaftswissenschaftler Charles Goodhart zurück und bezeichnet die Beobachtung, dass die Messung eines Verhaltens oder einer Leistung das Verhalten der Nutzer:innen verändert. Strathern (1997) stellte in Bezug auf Prüfungsleistungen fest, “when a measure becomes a target, it ceases to be a good measure” (dt. dass eine Kennzahl aufhört nützlich zu sein, wenn das Optimieren der Kennzahl zum Hauptziel wird, S. 308\\) und nicht mehr die Optimierung dessen im Vordergrund steht, was die Kennzahl eigentlich misst oder ursprünglich messen sollte. Übertragen auf Open Scholarship und die Anreizstruktur in der Forschung geht Goodharts Gesetz davon aus, dass Kennzahlen für Bewertung wissenschaftlicher Leistung, wie beispielsweise die Zahl der Zitationen einer forschenden Person, wahrscheinlich manipuliert und gezielt optimiert werden, was verschiedene Studien, wie etwa von Muller (2019) auch so bestätigen.",
                "Related_terms": "Campbell's law; DORA; Reification (fallacy) **Reference (s):** Muller (2019); Strathern (1997)"
            },
            {
                "Title": "H-index (H-Index, Hirsch Index) *",
                "Definition": "** Hirsch’s index, abbreviated as H-index, intends to measure both productivity and research impact by combining the number of publications and the number of citations to these publications. Hirsch (2005) defined the index as “the number of papers with citation number ≥ *h*” (p. 16569). That is, the greatest number such that an author (or journal) has published at least that many papers that have been cited at least that many times. The index is perceived as a superior measure to measures that only assess, for instance, the number of citations and number of publications but this index has been criticised for the purpose of researcher assessment (e.g. Wendl, 2007).",
                "Reference(s)": "** Hirsch (2005); Wendl (2007)",
                "Drafted by": "** Jacob Miranda",
                "Reviewed (or Edited) by": "** Bradley J. Baker; Mahmoud M. Elsherif; Brett J. Gall; Charlotte R. Pennington",
                "Translated by": "Bettina MJ Kern",
                "Translation reviewed by": "Helena Hartmann, Jennifer Mattschey",
                "Translation": "Der Hirsch-Index, abgekürzt als H-Index, soll die Produktivität sowie den Einfluss einer Forschungsarbeit messen, indem er die Anzahl der Veröffentlichungen und die Anzahl der Zitierungen dieser Veröffentlichungen in Relation setzt. Hirsch (2005) definiert den Index als “the number of papers with citation number ≥ *h*” (dt. die Anzahl der Veröffentlichungen mit einer Zitationszahl ≥ h; S. 16569). Eine forschende Person hat einen Hirsch-Index h, wenn h von ihren insgesamt N Publikationen mindestens h-mal und die restlichen (N-h) Publikationen höchstens h-mal zitiert wurden. Der Index gilt zwar als besseres Kriterium als Maßzahlen, die nur die Anzahl der Zitierungen und die Anzahl der Veröffentlichungen bewerten, wird aber dennoch für die Bewertung von Forschenden kritisch gesehen (z. B. Wendl, 2007).",
                "Related_terms": "** Citation; DORA; I10-index; Impact"
            },
            {
                "Title": "Hackathon *",
                "Definition": "** An organized event where experts, designers, or researchers collaborate for a relatively short amount of time to work intensively on a project or problem. The term is originally borrowed from computer programmer and software development events whose goal is to create a fully fledged product (resources, research, software, hardware) by the end of the event, which can last several hours to several days.",
                "Reference": "** Kienzler and Fontanesi (2017)",
                "Originally drafted by": "** Flávio Azevedo",
                "Reviewed (or Edited) by": "** Tsvetomira Dumbalska; Brett J. Gall; Emma Norris",
                "Translated by": "Bettina MJ Kern",
                "Translation reviewed by": "Helena Hartmann, Jennifer Mattschey",
                "Translation": "Eine Veranstaltung, bei der Expert:innen, Designer:innen oder Forschende für einen begrenzten Zeitraum zusammenarbeiten, um intensiv ein Projekt oder Problem zu bearbeiten. Der Begriff kommt ursprünglich von Veranstaltungen für Programmierenden und Softwareentwicklenden mit dem Ziel, am Ende der Veranstaltung, die einige Stunden bis mehrere Tage dauern kann, ein vollständiges Produkt (Ressourcen, Forschung, Software, Hardware) zu schaffen.",
                "Related_terms": "** Collaboration; Edithaton"
            },
            {
                "Title": "HARKing *",
                "Definition": "** A questionable research practice termed ‘Hypothesizing After the Results are Known’ (HARKing). “HARKing is defined as presenting a post hoc hypothesis (i.e., one based on or informed by one's results) in a research report as if it was, in fact, *a priori*” (Kerr, 1998, p. 196). For example, performing subgroup analyses, finding an effect in one subgroup, and writing the introduction with a ‘hypothesis’ that matches these results.",
                "Reference": "Hitchcock and Sober (2014)",
                "Reference(s)": "** Kerr (1998); Nosek and Lakens (2014)",
                "Drafted by": "** Beatrix Arendt",
                "Reviewed (or Edited) by": "** Matt Jaquiery; Charlotte R. Pennington; Martin Vasilev; Flávio Azevedo",
                "Translated by": "Bettina MJ Kern",
                "Translation reviewed by": "Helena Hartmann, Jennifer Mattschey",
                "Translation": "Eine fragwürdige Forschungspraxis, die als \"*Hypothesizing After the Results are Known*\" (HARKing, dt. Hypothesenaufstellung nach Wissen der Ergebnisse) bezeichnet wird. “HARKing is defined as presenting a post hoc hypothesis (i.e., one based on or informed by one's results) in a research report as if it was, in fact, *a priori*” (dt. Bei HARKing wird eine Post-hoc-Hypothese (d. h. einer Hypothese, die erst nach der Datenauswertung entwickelt wurde und auf ihren Ergebnissen beruht) in einer Forschungsarbeit so dargestellt, als wäre sie a priori geplant und hypothesengeleitet gewesen; Kerr, 1998, S. 196). Ein Beispiel ist die Durchführung von Subgruppenanalysen, bei der ein Effekt in einer Untergruppe entdeckt wird und daraufhin im Nachhinein eine Hypothese in die Forschungsarbeit mit aufgenommen wird, die diese Ergebnisse vorhersagt. Dabei wird so getan, als wäre die Subgruppenanalyse hypothesengeleitet und a priori geplant gewesen. **Alternative terms**: accommodational hypothesizing",
                "Related_terms": "** Analytic Flexibility; CARKing; Confirmatory analyses; Exploratory data analysis; Fudging; Garden of forking paths; P-hacking; PARKing; Questionable Research Practices or Questionable Reporting Practices (QRPs); SPARKing"
            },
            {
                "Title": "Hidden Moderators (Versteckte Moderatoren) *",
                "Definition": "** Contextual conditions that can, unbeknownst to researchers, make the results of a replication attempt deviate from those of the original study. Hidden moderators are sometimes invoked to explain (away) failed replications. Also called hidden assumptions.",
                "Reference": "** Zwaan et al. (2018)",
                "Originally drafted by": "** Ali H. Al-Hoorie",
                "Reviewed (or Edited) by": "** Mahmoud Elsherif; Sam Parsons",
                "Translated by": "Helena Hartmann",
                "Translation reviewed by": "Susanne Vogel, Jennifer Mattschey",
                "Translation": "Kontextuelle Bedingungen, die ohne Wissen der Forschenden dazu führen können, dass die Ergebnisse eines Replikationsversuchs von denen der ursprünglichen Studie abweichen. Versteckte (hidden) Moderatoren werden manchmal angeführt, um fehlgeschlagene Replikationen (weg) zu erklären. Auch versteckte Annahmen (hidden assumptions) genannt.",
                "Related_terms": "** Auxiliary Hypothesis"
            },
            {
                "Title": "Hypothesis (Hypothese) *",
                "Definition": "** A hypothesis is an unproven statement relating the connection between variables (Glass & Hall, 2008\\) and can be based on prior experiences, scientific knowledge, preliminary observations, theory and/or logic. In scientific testing, a hypothesis can be usually formulated with (e.g. a positive correlation) or without a direction (e.g. there will be a correlation). Popper (1959) posits that hypotheses must be falsifiable, that is, it must be conceivably possible to prove the hypothesis false. However, hypothesis testing based on falsification has been argued to be vague, as it is contingent on many other untested assumptions in the hypothesis (i.e., auxiliary hypotheses). Longino (1990, 1992\\) argued that ontological heterogeneity should be valued more than ontological simplicity for the biological sciences, which considers we should investigate differences between and within biological organisms.",
                "Reference(s)": "** Beller and Bender (2017); Glass and Hall (2008); Longino (1990, 1992); Popper (1959)",
                "Drafted by": "** Ana Barbosa Mendes",
                "Reviewed (or Edited) by": "** Ali H. Al-Hoorie; Mahmoud Elsherif; Helena Hartmann; Charlotte R. Pennington**;** Graham Reid; Olly Robertson",
                "Translated by": "Bettina MJ Kern",
                "Translation reviewed by": "Susanne Vogel, Jennifer Mattschey   ### **I** {#i}",
                "Translation": "Eine Hypothese ist eine unbewiesene Aussage über den Zusammenhang zwischen Variablen (Glass & Hall, 2008\\) und kann auf früheren Erfahrungen, wissenschaftlichen Erkenntnissen, vorläufigen Beobachtungen, Theorie und/oder Logik beruhen. Bei wissenschaftlichen Tests kann eine Hypothese gerichtet (z. B. eine positive Korrelation) oder ungerichtet (z. B. es wird eine Korrelation geben, aber unklar, ob positiv oder negativ) formuliert werden. Nach Popper (1959) müssen Hypothesen falsifizierbar sein, d. h. es muss möglich sein, die Hypothese zu widerlegen. Es wurde jedoch argumentiert, dass Hypothesentests, die auf Falsifikation beruhen, vage sind, da sie von vielen anderen ungeprüften Annahmen (d. h. Hilfshypothesen, \\[auxiliary hypotheses\\]) abhängig sind. Longino (1990, 1992\\) vertrat die Auffassung, dass in den Biowissenschaften ontologische Heterogenität höher bewertet werden sollte als ontologische Einfachheit, d. h. wir sollten Unterschiede zwischen und innerhalb biologischer Organismen untersuchen.",
                "Related_terms": "** Auxiliary Hypothesis; Confirmatory analyses; False negative result; False positive result; Modelling; Predictions; Quantitative research; Theory; Theory building; Type I error; Type II error"
            },
            {
                "Title": "I10-index (i10-Index) *",
                "Definition": "** A research metric created by Google Scholar that represents the number of publications a researcher has with at least 10 citations.",
                "Reference(s)": "** [https://guides.library.cornell.edu/impact/author-impact-10](https://guides.library.cornell.edu/impact/author-impact-10)",
                "Drafted by": "** Emma Norris",
                "Reviewed (or Edited) by": "** Flávio Azevedo; Sam Parsons",
                "Translated by": "Bettina MJ Kern",
                "Translation reviewed by": "Helena Hartmann, Jennifer Mattschey",
                "Translation": "Eine von Google Scholar erstellte Forschungsmetrik. Sie gibt für eine/n Forschenden die Anzahl der Publikationen mit mindestens 10 Zitationen an.",
                "Related_terms": "** Citation; DORA; H-index; Impact"
            },
            {
                "Title": "Ideological bias (Ideologische Verzerrung) *",
                "Definition": "** The idea that pre-existing opinions about the quality of research can depend on the ideological views of the author(s). One of the many biases in the peer review process, it expects that favourable opinions towards the research would be more likely if friends, collaborators, or scientists agree with an editor or reviewer’s political viewpoints (Tvina et al. 2019). This could potentially lead to a variety of conflicts of interest that undermine diverse perspectives, for example: speeding or delaying peer-review, or influencing the chances of an individual being invited to present their research, thus promoting their work.",
                "Reference(s)": "** Tvina et al. (2019)",
                "Drafted by": "** Mahmoud Elsherif",
                "Reviewed (or Edited) by": "** Elizabeth Collins; Flávio Azevedo; Madeleine Ingham; Sam Parsons; Graham Reid",
                "Translated by": "Bettina MJ Kern",
                "Translation reviewed by": "Helena Hartmann, Jennifer Mattschey",
                "Translation": "Der Gedanke, dass Meinungen über die Qualität von Forschungsarbeiten von den ideologischen Ansichten der Forschenden abhängen können. Ideologische Verzerrung ist eine der vielen möglichen Verzerrungen (Bias), die im Peer-Review-Verfahren auftreten können, und geht davon aus, dass eine positive Bewertung der Forschungsarbeit wahrscheinlicher ist, wenn die politischen Ansichten der begutachtenden Person und/oder ihres Umfeldes mit den Ansichten der Verfasser:innen der Forschungsarbeit übereinstimmen (Tvina et al. 2019). Dies kann zu einer Reihe von Interessenkonflikten führen und unterschiedliche Perspektiven untergraben, so kann es zum Beispiel zu einer Beschleunigung oder Verzögerung des Peer-Review-Verfahrens führen oder die Chance des Forschenden, zu einer Präsentation ihrer Forschung eingeladen zu werden, negativ beeinflussen.",
                "Related_terms": "** Ad hominem bias; Peer review"
            },
            {
                "Title": "Inclusion (Inklusion, Inklusivität) *",
                "Definition": "** Inclusion, or inclusivity, refers to a sense of welcome and respect within a given collaborative project or environment (such as academia) where *diversity* simply indicates a wide range of backgrounds, perspectives, and experiences, efforts to increase *inclusion* go further to promote engagement and equal valuation among diverse individuals, who might otherwise be marginalized. Increasing inclusivity often involves minimising the impact of, or even removing, systemic barriers to accessibility and engagement.",
                "Reference": "Calvert (2019); [Martinez-Acosta and Favero (2018)](https://www-ncbi-nlm-nih-gov.proxy.library.vanderbilt.edu/pmc/articles/PMC6153014/)",
                "Drafted by": "** Ryan Millager",
                "Reviewed (or Edited) by": "** Mahmoud Elsherif; Graham Reid; Kai Krautter; Suzanne L. K. Stewart; Flávio Azevedo",
                "Translated by": "Bettina MJ Kern",
                "Translation reviewed by": "Susanne Vogel, Helena Hartmann",
                "Translation": "Inklusion oder Inklusivität bezieht sich auf eine Kultur des Willkommens und des Respekts in der Zusammenarbeit an einem bestimmten Projekt oder in einem Umfeld (z. B. im akademischen Bereich), wobei Vielfalt (Diversität) hier ein breites Spektrum an unterschiedlichen Hintergründen, Perspektiven und Erfahrungen bedeutet. Bemühungen zur Erhöhung der Inklusivität beinhalten die Verminderung der Folgen oder die komplette Beseitigung systemischer Barrieren für benachteiligte Gruppen und Minderheiten.",
                "Related_terms": "** Diversity; Equity; Social Justice"
            },
            {
                "Title": "Incentive structure (Anreizsystem) *",
                "Definition": "** The set of evaluation and reward mechanisms (explicit and implicit) for scientists and their work. Incentivised areas within the broader structure include hiring and promotion practices, track record for awarding funding, and prestige indicators such as publication in journals with high impact factors, invited presentations, editorships, and awards. It is commonly believed that these criteria are often misaligned with the telos of science, and therefore do not promote rigorous scientific output. Initiatives like DORA aim to reduce the field’s dependency on evaluation criteria such as journal impact factors in favor of assessments based on the intrinsic quality of research outputs.",
                "Reference": "** Koole and Lakens (2012); Nosek et al. (2012); Schönbrodt (2019); Smaldino and McElreath (2016)",
                "Originally drafted by": "** Charlotte R. Pennington; Olmo van den Akker",
                "Reviewed (or Edited) by": "** Helena Hartmann; Flávio Azevedo; Robert M. Ross; Graham Reid; Suzanne L. K. Stewart",
                "Translated by": "Bettina MJ Kern, Helena Hartmann",
                "Translation reviewed by": "Jennifer Mattschey",
                "Translation": "Anreizsystem (incentive structure) bezeichnet die Gesamtheit der (expliziten und impliziten) Bewertungs- und Belohnungsmechanismen für Forschende und ihre Forschung. Zu den Bereichen, in denen Anreize geschaffen werden, gehören Einstellungs- und Beförderungspraktiken, die Erfolgsbilanz bei der Vergabe von Fördermitteln und Prestigeindikatoren wie Veröffentlichungen in Zeitschriften mit hohem Impact-Faktor, eingeladene Vorträge, Herausgeber:innenschaften und Auszeichnungen. Es wird allgemein angenommen, dass diese Kriterien oft nicht mit dem Sinn und Zweck von Wissenschaft übereinstimmen und daher nicht geeignet sind, anspruchsvolle wissenschaftliche Leistungen zu fördern. Initiativen wie DORA zielen darauf ab, die Abhängigkeit des Fachgebiets von rein quantitativen Bewertungskriterien wie dem Impact-Faktor von Fachzeitschriften zugunsten von Bewertungen zu verringern, die auf Qualität der Forschungsarbeiten basieren, und nicht auf ihrer Anzahl.",
                "Related_terms": "** DORA; Metrics; Pressure; Publish or perish; Quantity; Reward structure; Scientific publications; Slow science; Structural factors"
            },
            {
                "Title": "Induction (Induktion) *",
                "Definition": "** “Reasoning by drawing a conclusion not guaranteed by the premises; for example, by inferring a general rule from a limited number of observations. Popper believed that there was no such logical process; we may guess general rules but such guesses are not rendered even more probable by any number of observations. By contrast, Bayesians inductively work out the increase in probability of a hypothesis that follows from the observations.” (Dienes, S. 164, 2008).",
                "Reference": "Dienes (2008)",
                "Drafted by": "** Alaa Aldoh",
                "Reviewed (or Edited) by": "**",
                "Translated by": "Helena Hartmann",
                "Translation reviewed by": "Susanne Vogel, Jennifer Mattschey",
                "Translation": "Reasoning by drawing a conclusion not guaranteed by the premises; for example, by inferring a general rule from a limited number of observations. Popper believed that there was no such logical process; we may guess general rules but such guesses are not rendered even more probable by any number of observations. By contrast, Bayesians inductively work out the increase in probability of a hypothesis that follows from the observations.” (dt. Argumentieren, indem man eine Schlussfolgerung zieht, die nicht durch die Prämissen garantiert ist, z. B. indem man eine allgemeine Regel aus einer begrenzten Anzahl von Beobachtungen ableitet. Popper glaubte, dass es einen solchen logischen Prozess nicht gibt; wir können allgemeine Regeln erraten, aber solche Vermutungen werden nicht durch eine beliebige Anzahl von Beobachtungen noch wahrscheinlicher. Im Gegensatz dazu erarbeiten Bayesianer:innen induktiv die Zunahme der Wahrscheinlichkeit einer Hypothese, die aus den Beobachtungen folgt; Dienes, S. 164, 2008).",
                "Related_terms": "** Hypothesis"
            },
            {
                "Title": "Interaction Fallacy (Interaktionsfehlschluss) *",
                "Definition": "** A statistical error in which a formal test is not conducted to assess the difference between a significant and non-significant correlation (or other measures, such as Odds Ratio). This fallacy occurs when a significant and non-significant correlation coefficient are assumed to represent a statistically significant difference but the comparison itself is not explicitly tested.",
                "Reference": "** Gelman and Stern (2006); Morabia et al. (1997); Nieuwenhuis et al. (2011)",
                "Originally drafted by": "** Graham Reid",
                "Reviewed (or Edited) by": "** Ali H. Al-Hoorie; Mahmoud Elsherif; Kai Krautter; Sam Parsons",
                "Translated by": "Bettina MJ Kern",
                "Translation reviewed by": "Helena Hartmann, Jennifer Mattschey",
                "Translation": "Ein statistischer Fehlschluss, der passiert, wenn kein formaler Test durchgeführt wird, um den Unterschied zwischen einer signifikanten und einer nicht signifikanten Korrelation (oder anderen Maßzahlen wie Odds Ratio) zu bewerten. Wenn angenommen wird, dass ein signifikanter und ein nicht signifikanter Korrelationskoeffizient einen statistisch signifikanten Unterschied darstellen, ohne dass der Unterschied ausdrücklich getestet wird, spricht man vom Interaktionsfehler (interaction fallacy)..",
                "Related_terms": "** Comparison of Correlations; Null Hypothesis Significance Testing (NHST); Statistical Validity; Type I error; Type II error"
            },
            {
                "Title": "Interlocking (Verzahnung) *",
                "Definition": "** An analysis at the core of intersectionality to analyse power, inequality and exclusion, as efforts to reform academic culture cannot be completed by investigating only one avenue in isolation (e.g. race, gender or ability) but by considering all the systems of exclusion. In contrast to intersectionality (which refers to the individual having multiple social identities), interlocking is usually used to describe the systems that combine to serve as oppressive measures toward the individual based on these identities.",
                "Reference(s)": "** Ledgerwood et al. (2021)",
                "Drafted by": "** Christina Pomareda",
                "Reviewed (or Edited) by": "** Ali H. Al-Hoorie; Flávio Azevedo; Mahmoud Elsherif; Eliza Woodward; Gerald Vineyard;",
                "Translated by": "Bettina MJ Kern",
                "Translation reviewed by": "Susanne Vogel, Jennifer Mattschey",
                "Translation": "Im Gegensatz zur Intersektionalität (die sich darauf bezieht, dass das Individuum mehrere soziale Identitäten hat) wird Interlocking verwendet, um die Systeme zu beschreiben, die zusammenwirken, um als Unterdrückungsmaßnahmen gegen ein Individuum auf der Grundlage dieser Identitäten zu dienen. Interlocking ist ein wichtiger Bestandteil von Intersektionalität und hilft dabei, Mechanismen von Macht, Ungleichheit und Ausgrenzung zu analysieren. So können beispielsweise Reformen der akademischen Kultur nicht funktionieren, wenn nur ein Problem isoliert untersucht wird (z. B. Herkunft, Geschlecht oder Fähigkeit), sondern nur, indem alle Systeme der Ausgrenzung und ihr Zusammenwirken berücksichtigt werden.",
                "Related_terms": "** Bropenscience; Equity; Diversity; Inclusion; Intersectionality; Open Science; Social Justice"
            },
            {
                "Title": "Internal Validity (interne Validität) *",
                "Definition": "** An indicator of the extent to which a study’s findings are representative of the true effect in the population of interest and not due to research confounds, such as methodological shortcomings. In other words, whether the observed evidence or covariation between the independent (predictor) and dependent (criterion) variables can be taken as a bona fide relationship and not a spurious effect owing to uncontrolled aspects of the study’s set up. Since it involves the quality of the study itself, internal validity is a priority for scientific research.",
                "Reference": "** Campbell and Stanley (1966) **Alternative definition:** In Psychometrics, the degree of evidence that confirms the internal structure of a psychometric test as compatible with the structure of a psychological construct. **Related terms to alternative definition:** Construct validity",
                "Reference(s)": "** Messick (1995)",
                "Originally drafted by": "** Annalise A. LaPlume",
                "Reviewed (or Edited) by": "** Helena Hartmann; Oscar Lecuona; Meng Liu; Sam Parsons; Graham Reid; Flávio Azevedo",
                "Translated by": "Bettina MJ Kern",
                "Translation reviewed by": "Susanne Vogel, Jennifer Mattschey",
                "Translation": "Ein Indikator dafür, inwieweit die Ergebnisse einer Studie für den tatsächlichen Effekt in der jeweiligen Grundgesamtheit repräsentativ sind und nicht auf Störfaktoren, wie z. B. methodische Mängel zurückzuführen sind. Interne Validität gibt an, ob die beobachtete Evidenz oder Kovariation zwischen der unabhängigen (Prädiktor-) und der abhängigen (Kriteriums-) Variable als eine echte Beziehung und nicht als ein falsch-positiver Effekt aufgrund unkontrollierter Aspekte des Studiendesigns angesehen werden kann. Da es dabei um die Qualität der Studie selbst geht, hat die interne Validität einen hohen Stellenwert für die wissenschaftliche Forschung.",
                "Related_terms": "** External validity; Validity"
            },
            {
                "Title": "Intersectionality (Intersektionalität) *",
                "Definition": "** A term which derives from Black feminist thought and broadly describes how social identities exist within ‘interlocking systems of oppression’ and structures of (in)equalities (Crenshaw, 1989\\)**.** Intersectionality offers a perspective on the way multiple forms of inequality operate together to compound or exacerbate each other. Multiple concurrent forms of identity can have a multiplicative effect and are not merely the sum of the component elements. One implication is that identity cannot be adequately understood through examining a single axis (e.g., race, gender, sexual orientation, class) at a time in isolation, but requires simultaneous consideration of overlapping forms of identity.",
                "Reference(s)": "** Crenshaw (1989); Grzanka (2020); Ledgerwood et al. (2021)",
                "Drafted by": "** Madeleine Pownall",
                "Reviewed (or Edited) by": "** Ali H. Al-Hoorie; Bradley Baker; Mahmoud Elsherif; Wanyin Li; Ryan Millager; Charlotte R. Pennington; Flávio Azevedo",
                "Translated by": "Bettina MJ Kern",
                "Translation reviewed by": "Helena Hartmann, Jennifer Mattschey  ### ---  ### **J** {#j}",
                "Translation": "Der Begriff stammt aus dem schwarzen feministischen Denken und beschreibt, wie soziale Identitäten innerhalb von ineinandergreifenden Unterdrückungssystemen und Strukturen der (Un-)Gleichheit existieren (Crenshaw, 1989). Intersektionalität bietet einen Blickwinkel darauf, wie mehrere Formen der Ungleichheit zusammenwirken und sich gegenseitig verstärken oder verschlimmern können. Mehrere gleichzeitig auftretende soziale Identitäten können einen Multiplikatoreffekt haben und sind mehr als die Summe der einzelnen Elemente. Eine zentrale Annahme von Intersektionalität ist, dass Identität nicht angemessen verstanden werden kann, wenn eine einzelne Achse (z. B. Herkunft, Geschlecht, sexuelle Orientierung, Klasse) isoliert untersucht wird, und dass stattdessen die gleichzeitige Betrachtung sich überschneidender Identitäten erforderlich ist.",
                "Related_terms": "** Bropenscience; Diversity; Inclusion; Interlocking; Open Science"
            },
            {
                "Title": "JabRef *",
                "Definition": "** An open-sourced, cross-platform citation and reference management tool that is available free of charge. It allows editing BibTeX files, importing data from online scientific databases, and managing and searching BibTeX files.",
                "Reference": "** JabRef Development Team (2021)",
                "Originally drafted by": "** Aleksandra Lazić",
                "Reviewed (or Edited) by": "** Christopher Graham; Michele C. Lim; Sam Parsons; Steven Verheyen",
                "Translated by": "Bettina MJ Kern",
                "Translation reviewed by": "Susanne Vogel, Jennifer Mattschey",
                "Translation": "Ein kostenloses, quell-offenes, plattformübergreifendes Tool zur Verwaltung von Zitationen und Referenzen. Es ermöglicht die Bearbeitung von BibTeX-Dateien, den Import von Daten aus wissenschaftlichen Online-Datenbanken sowie die Verwaltung und Suche von BibTeX-Dateien.",
                "Related_terms": "** Open source software"
            },
            {
                "Title": "Jamovi *",
                "Definition": "** Free and open source software for data analysis based on the R language. The software has a graphical user interface and provides the R code to the analyses. Jamovi supports computational reproducibility by saving the data, code, analyses, and results in a single file.",
                "Reference(s)": "** The jamovi project (2020)",
                "Drafted by": "** Amélie Beffara Bret",
                "Reviewed (or Edited) by": "** Adrien Fillon; Alexander Hart; Charlotte R. Pennington",
                "Translated by": "Bettina MJ Kern",
                "Translation reviewed by": "Helena Hartmann, Jennifer Mattschey",
                "Translation": "Freie und quelloffene Software für Datenanalyse, auf der Grundlage der statistischen Programmiersprache R. Jamovi verfügt über eine grafische Benutzeroberfläche und stellt den verwendeten R-Code bereit. Damit unterstützt Jamovi die Reproduzierbarkeit von Forschungsergebnissen, indem Daten, Code, Analysen und Ergebnisse in einer einzigen Datei gespeichert werden.",
                "Related_terms": "** JASP; Open source; R; Reproducibility"
            },
            {
                "Title": "JASP *",
                "Definition": "** Named after Sir Harold Jeffreys, JASP stands for Jeffrey’s Amazing Statistics Program. It is a free and open source software for data analysis. JASP relies on a user interface and offers both null hypothesis tests and their Bayesian counterparts. JASP supports computational reproducibility by saving the data, code, analyses, and results in a single file.",
                "Reference(s)": "** JASP Team (2020)",
                "Drafted by": "** Amélie Beffara Bret",
                "Reviewed (or Edited) by": "** Adrien Fillon, Adam Parker; Sam Parsons",
                "Translated by": "Bettina MJ Kern",
                "Translation reviewed by": "Susanne Vogel, Jennifer Mattschey",
                "Translation": "JASP steht für “Jeffrey's Amazing Statistics Program” und ist nach Sir Harold Jeffreys benannt. Es handelt sich um eine freie und quelloffene Software für Datenanalyse. JASP stützt sich auf eine graphische Benutzeroberfläche und bietet sowohl Nullhypothesentests als auch deren Bayes’sche Gegenstücke. JASP unterstützt die Reproduzierbarkeit von Forschungsergebnissen, indem es die Daten, den Code, die Analysen und die Ergebnisse in einer einzigen Datei speichert.",
                "Related_terms": "** Jamovi; Open source"
            },
            {
                "Title": "Journal Impact Factor™ (Zeitschriften-Impact-Faktor) *",
                "Definition": "** The mean number of citations to research articles in that journal over the preceding two years. It is a proprietary and opaque calculation marketed by Clarivate**™**. Journal Impact Factors are not associated with the content quality or the peer review process.",
                "Reference(s)": "** Brembs et al (2013); Curry (2012); Naudet et al. (2018); Rossner et al. (2008); Sharma et al. (2014)",
                "Drafted by": "** Jacob Miranda",
                "Reviewed (or Edited) by": "** Tsvetomira Dumbalska; Adam Parker",
                "Translated by": "Bettina MJ Kern",
                "Translation reviewed by": "Helena Hartmann, Jennifer Mattschey",
                "Translation": "Die durchschnittliche Anzahl der Zitierungen von Forschungsartikeln in einer bestimmten Zeitschrift in den vorangegangenen zwei Jahren. Es handelt sich um eine urheberrechtlich geschützte und undurchsichtige Berechnung, die von Clarivate™ vermarktet wird. Die Impact-Faktoren von Zeitschriften stehen nicht in Zusammenhang mit der Qualität des Inhalts oder dem Peer-Review-Verfahren.",
                "Related_terms": "** DORA; H-index"
            },
            {
                "Title": "JSON file *",
                "Definition": "** JavaScript Object Notation (JSON) is a data format for structured data that can be used to represent attribute-value pairs. Values thereby can contain further JSON notation (i.e., nested information). JSON files can be formally encoded as strings of text and thus are human-readable. Beyond storing information this feature makes them suitable for annotating other content. For example, JSON files are used in Brain Imaging Data Structure (BIDS) for describing the metadata dataset by following a standardized format (dataset\\_description.json).",
                "Reference(s)": "** https://bids-specification.readthedocs.io/en/stable/03-modality-agnostic-files.html",
                "Drafted by": "** Tina Lonsdorf",
                "Reviewed (or Edited) by": "** Alexander Hart; Matt Jaquiery; Emma Norris; Charlotte R. Pennington",
                "Translated by": "Bettina MJ Kern",
                "Translation reviewed by": "Susanne Vogel, Jennifer Mattschey  ###  ### **K** {#k}",
                "Translation": "JavaScript Object Notation (JSON) ist ein Datenformat für strukturierte Daten, das zur Darstellung von Attribut-Wert-Paaren verwendet werden kann. Die Werte können dabei weitere JSON-Notationen (d. h. verschachtelte Informationen) enthalten. JSON-Dateien können formal als Textketten kodiert werden und sind daher für Menschen lesbar. Neben der Speicherung von Informationen eignet sich diese Eigenschaft auch für die Kommentierung anderer Inhalte. Beispielsweise werden JSON-Dateien in der Brain Imaging Data Structure (BIDS) zur Beschreibung des Metadatensatzes verwendet, wobei ein standardisiertes Format (dataset\\_description.json) verwendet wird.",
                "Related_terms": "** BIDS data structure; Metadata"
            },
            {
                "Title": "Knowledge acquisition *",
                "Definition": "** The process by which the mind decodes or extracts, stores, and relates new information to existing information in long term memory. Given the complex structure and nature of knowledge, this process is studied in the philosophical field of epistemology, as well as the psychological field of learning and memory.",
                "Reference(s)": "** Brule and Blount (1989)",
                "Drafted by": "** Oscar Lecuona",
                "Reviewed (or Edited) by": "** Bradley Baker; Helena Hartmann; Kai Krautter; Graham Reid",
                "Translated by": "Bettina MJ Kern",
                "Translation reviewed by": "Helena Hartmann, Jennifer Mattschey  ###  ### **L** {#l}",
                "Translation": "Der Prozess, durch den der Verstand neue Informationen entschlüsselt oder extrahiert, speichert und mit bereits vorhandenen Informationen im Langzeitgedächtnis verknüpft. Angesichts der komplexen Struktur und Natur von Wissen wird dieser Prozess im philosophischen Bereich der Erkenntnistheorie sowie im psychologischen Bereich des Lernens und des Gedächtnisses untersucht.",
                "Related_terms": "** Epistemology; Information; Learning"
            },
            {
                "Title": "Likelihood function (Likelihood-Funktion) *",
                "Definition": "** A statistical model of the data used in frequentist and Bayesian analyses, defined up to a constant of proportionality. A likelihood function represents the likeliness of different parameters for your distribution given the data. Given that probability distributions have unknown population parameters, the likelihood function indicates how well the sample data summarise these parameters. As such, the likelihood function gives an idea of the goodness of fit of a model to the sample data for a given set of values of the unknown population parameters.",
                "Reference(s)": "** Dienes (2008); Hogg et al. (2010); van de Schoot et al. (2021); Geyer (2003); Geyer (2007); https://blog.stata.com/2016/11/01/introduction-to-bayesian-statistics-part-1-the-basic-concepts/",
                "Drafted by": "** Alaa AlDoh",
                "Reviewed (or Edited) by": "** Dominik Kiersz; Graham Reid; Sam Parsons; Flávio Azevedo",
                "Translated by": "Bettina MJ Kern",
                "Translation reviewed by": "Susanne Vogel, Jennifer Matschey",
                "Translation": "Eine Likelihood-Funktion ist ein statistisches Modell von Daten in frequentistischen und Bayes’schen Analysen und stellt die Wahrscheinlichkeit verschiedener Parameter für ihre Verteilung gegeben den Daten dar. Da Wahrscheinlichkeitsverteilungen unbekannte Populationsparameter haben, gibt die Likelihood-Funktion an, wie gut die Stichprobendaten diese Parameter zusammenfassen. Die Likelihood-Funktion gibt also Aufschluss darüber, wie gut ein Modell an die Stichprobendaten für einen bestimmten Satz von Werten der unbekannten Populationsparameter angepasst ist.",
                "Related_terms": "** Bayes factor; Bayesian inference; Bayesian parameter estimation; Posterior distribution; Prior distribution **Alternative definition:** For a more statistically-informed definition, given a parametric model specified by a probability (density) function f(x|theta), a likelihood *for* a statistical model is defined by the same formula as the density except that the roles of the data *x* and the parameter *theta* are interchanged, and thus the likelihood can be considered a function of *theta* for fixed data *x*. Here, then, the likelihood function would describe a curve or hypersurface whose peak, if it exists, represents the combination of model parameter values that maximize the probability of drawing the sample obtained. \\[GERMAN:\\] Eine statistisch detailliertere Definition: Gegeben sei ein parametrisches Modell spezifiziert mit einer Wahrscheinlichkeitsfunktion (Dichtefunktion) f(x|theta), dann wird Likelihood für ein statistisches Modell durch dieselbe Formel wie die Dichte definiert, mit der Ausnahme, dass die Rollen der Daten *x* und des Parameters *theta* vertauscht werden, so dass die Wahrscheinlichkeit als eine Funktion von *theta* für spezifische Daten x betrachtet werden kann. In diesem Fall würde die Likelihood-Funktion eine Kurve oder Hyperfläche beschreiben, deren Spitze, falls vorhanden, die Kombination von Modellparameterwerten darstellt, die die Wahrscheinlichkeit der Ziehung der erhaltenen Stichprobe maximiert."
            },
            {
                "Title": "Likelihood Principle (Likelihood Prinzip) *",
                "Definition": "** The notion that all information relevant to inference contained in data is provided by the likelihood. The principle suggests that the likelihood function can be used to compare the plausibility of various parameter values. While Bayesians and likelihood theorists subscribe to the likelihood principle, Neyman-Pearson theorists do not, as significance tests violate the likelihood principle because they take into account information not in the likelihood.",
                "Reference(s)": "** Dienes (2008); Geyer (2003; 2007);",
                "Drafted by": "** Alaa Aldoh",
                "Reviewed (or Edited) by": "** Sam Parsons; Flávio Azevedo",
                "Translated by": "Helena Hartmann",
                "Translation reviewed by": "Susanne Vogel, Jennifer Mattschey",
                "Translation": "Die Vorstellung, dass alle in den Daten enthaltenen schlussfolgerelevanten Informationen von der Likelihood geliefert werden. Das Prinzip besagt, dass die Likelihood Funktion verwendet werden kann, um die Plausibilität verschiedener Parameterwerte zu vergleichen. Während Bayesianer:innen und Likelihood-Theoretiker:innen dem Likelihood Prinzip zustimmen, tun dies Neyman-Pearson-Theoretiker:innen nicht, da Signifikanztests das Likelihood Prinzip verletzen, weil sie Informationen berücksichtigen, die nicht in der Likelihood enthalten sind.",
                "Related_terms": "** Bayesian inference; Likelihood Function"
            },
            {
                "Title": "Literature Review (Literaturzusammenfassung) *",
                "Definition": "** Researchers often review research records on a given topic to better understand effects and phenomena of interest before embarking on a new research project, to understand how theory links to evidence or to investigate common themes and directions of existing study results and claims. Different types of reviews can be conducted depending on the research question and literature scope. To determine the scope and key concepts in a given field, researchers may want to conduct a scoping literature review. Systematic reviews aim to access and review all available records for the most accurate and unbiased representation of existing literature. Non-systematic or focused literature reviews synthesise information from a selection of studies relevant to the research question although they are uncommon due to susceptibility to biases (e.g. researcher bias; Siddaway et al., 2019).",
                "Reference(s)": "** Huelin et al., (2015); Munn et al., (2018); Pautasso (2013); Siddaway et al. (2019)",
                "Drafted by": "** Marta Topor",
                "Reviewed (or Edited) by": "** Jamie P. Cockcroft; Mahmoud Elsherif; Helena Hartmann; Flávio Azevedo; Meng Liu; Charlotte R. Pennington",
                "Translated by": "Bettina MJ Kern",
                "Translation reviewed by": "Helena Hartmann, Jennifer Mattschey   We have separated the glossary project across several documents, see links below:  Landing page:\t\t\t[Glossary Translations German template](https://docs.google.com/document/d/1IIZK-F9SX1P4UrPlZeEKgUAOyiEjNbGFSWafr0ADb0M/edit) Letters A \\- F:\t\t\t[German Glossary | Phase 2 | A-F](https://docs.google.com/document/d/1yQH_iYm7FgjVGtJyQWGv7iv1kt4GkUWeRbXHch57Fuo/edit) Letters G \\- L:\t\t\t[German Glossary | Phase 2 | G-L](https://docs.google.com/document/d/1MbfcDK3G6ybzkkq2jVz36q4TcqQMGpNG2cun6GOeuSQ/edit#) Letters M \\- R: \t\t\t[German Glossary | Phase 2 | M - R](https://docs.google.com/document/d/1sv2C1Y-z3WeiYjvhn2B8rhFyPK5YQ8G3sCCh3ZYwV2Q/edit#heading=h.w0bgiwj800db) Letters S \\- Z:\t\t\t[German Glossary | Phase 2 | S - Z](https://docs.google.com/document/d/1pORanWNHkMRkGYs8vNxdetx907gBv-fm8l-tV8XRIiE/edit) References: \t\t\t[Glossary | Phase 2 | References](https://docs.google.com/document/d/12_F8kbnl2GP66iBkIejJeX2KnkZ13iC_jj7VVYZMxpA/edit?usp=sharing) Terms not yet defined: \t\t[Glossary | Phase 2 | Terms not yet defined](https://docs.google.com/document/d/16FGodUkNoNrBYyq2JqHJMNzYuZf-QbsigooMpB_ns_E/edit?usp=sharing) Contributors spreadsheet: [Glossary Translations German tenzing contributions \\[FORRT\\]](https://docs.google.com/spreadsheets/d/1UEM7s27b5pOlrIYX9-fXYdPOmPpZZyoggJOfOIrOhko/edit#gid=0)  [FORRT slack](https://join.slack.com/t/forrt/shared_invite/zt-alobr3z7-NOR0mTBfD1vKXn9qlOKqaQ) \\- join us\\! [FORRT email](mailto:FORRTproject@gmail.com) \\- contact us\\!",
                "Translation": "Wissenschaftler:innen sichten häufig Forschungsarbeiten zu einem bestimmten Thema, um die Auswirkungen und Phänomene, die von Interesse sind, besser zu verstehen, bevor sie ein neues Forschungsprojekt in Angriff nehmen. Dies unterstützt dabei, zu verstehen, wie die Theorie mit den Beweisen zusammenhängt, oder dabei, gemeinsame Themen und Richtungen bestehender Studienergebnisse und Aussagen zu untersuchen. Abhängig von der Forschungsfrage und dem Umfang der Literatur können verschiedene Arten von Literaturzusammenfassungen (Literature Reviews) durchgeführt werden. Um den Umfang und die wichtigsten Konzepte in einem bestimmten Bereich zu bestimmen, können Forscher ein Scoping Literature Review durchführen. Systematische Literaturzusammenfassungen zielen darauf ab, alle verfügbaren Aufzeichnungen aufzurufen und zu überprüfen, um eine möglichst genaue und unvoreingenommene Darstellung der vorhandenen Literatur zu erhalten. Nicht systematische oder fokussierte Literature Reviews fassen Informationen aus einer Auswahl von Studien zusammen, die für die Forschungsfrage relevant sind, obwohl sie aufgrund der Anfälligkeit für Voreingenommenheit (z. B. Voreingenommenheit des Forschenden; Siddaway et al., 2019\\) selten sind.",
                "Related_terms": "** Evidence synthesis; Meta-research; Narrative reviews; Systematic reviews"
            },
            {
                "Title": "Manel *",
                "Definition": "** Portmanteau for ‘male panel’, usually to refer to speaker panels at conferences entirely composed of (usually caucasian) males. Typically discussed in the context of gender disparities in academia (e.g., women being less likely to be recognised as experts by their peers and, subsequently, having fewer opportunities for career development).",
                "Reference(s)": "** Bouvy and Mujoomdar (2019); Goodman and Pepinsky (2019); Nittrouer et al. (2018); Rodriguez and Günther (2020)",
                "Drafted by": "** Sam Parsons",
                "Reviewed (or Edited) by": "** Mahmoud Elsherif; Thomas Rhys Evans; Beatrice Valentini; Christopher Graham; Flávio Azevedo",
                "Translated by": "Susanne Vogel",
                "Translation reviewed by": "Helena Hartmann, Jennifer Mattschey",
                "Translation": "Kofferwort für “male panel” (männliches Gremium), das sich in der Regel auf die Zusammensetzung von Vortragenden auf Konferenzen (panels) bezieht, die ausschließlich aus (meist weißen) Männern bestehen. Wird in der Regel im Zusammenhang mit Geschlechter-Ungleichheiten im akademischen Bereich diskutiert (z. B. werden Frauen von ihren Kolleg:innen seltener als Expertinnen anerkannt und haben infolgedessen weniger Möglichkeiten zur Karriereentwicklung).",
                "Related_terms": "** Bropenscience; Diversity; Equity; Feminist psychology; Inclusion; Under-representation"
            },
            {
                "Title": "Many Authors *",
                "Definition": "** Large-scale collaborative projects involving tens or hundreds of authors from different institutions. This kind of approach has become increasingly common in psychology and other sciences in recent years as opposed to research carried out by small teams of authors, following earlier trends which have been observed e.g. for high-energy physics or biomedical research in the 1990s. These large international scientific consortia work on a research project to bring together a broader range of expertise and work collaboratively to produce manuscripts.",
                "Reference(s)": "** Cronin (2001); Moshontz et al. (2021); Wuchty et al. (2007)",
                "Drafted by": "** Yu-Fang Yang",
                "Reviewed (or Edited) by": "** Christopher Graham; Adam Parker; Charlotte R. Pennington; Birgit Schmidt; Beatrice Valentini",
                "Translated by": "Susanne Vogel",
                "Translation reviewed by": "Helena Hartmann, Jennifer Mattschey",
                "Translation": "Groß angelegte kollaborative Projekte, an denen Dutzende oder Hunderte von Autor:innen aus verschiedenen Einrichtungen beteiligt sind. Dieser Ansatz ist in den letzten Jahren in der Psychologie und anderen Wissenschaften immer häufiger anzutreffen, im Gegensatz zu Forschungsarbeiten, die von kleinen Autor:innenteams durchgeführt werden. Damit folgt er früheren Trends, die z. B. in der Hochenergiephysik oder der biomedizinischen Forschung in den 1990er Jahren zu beobachten waren. Diese großen internationalen wissenschaftlichen Konsortien arbeiten an einem Forschungsprojekt, um ein breiteres Spektrum an Expertise zusammenzubringen und kollaborativ Manuskripte zu erstellen.",
                "Related_terms": "** Collaboration; Consortia; Consortium authorship; Crowdsourcing; Hyperauthorship; Multiple-authors; Team science"
            },
            {
                "Title": "Many Labs *",
                "Definition": "** A crowdsourcing initiative led by the Open Science Collaboration (2015) whereby several hundred separate research groups from various universities run replication studies of published effects. This initiative is also known as “Many Labs I” and was subsequently followed by a “Many Labs II” project that assessed variation in replication results across samples and settings. Similar projects include ManyBabies, EEGManyLabs, and the Psychological Science Accelerator.",
                "Reference(s)": "** Ebersole et al. (2016); Frank et al. (2017); Klein et al. (2014); Klein et al. (2018); Moshontz et al. (2018); Open Science Collaboration (2015); Pavlov et al. (2020)",
                "Drafted by": "** Sam Parsons",
                "Reviewed (or Edited) by": "** Helena Hartmann; Charlotte R. Pennington; Mirela Zaneva",
                "Translated by": "Susanne Vogel",
                "Translation reviewed by": "Helena Hartmann, Jennifer Mattschey",
                "Translation": "Eine von der Open Science Collaboration (2015) geleitete Crowdsourcing-Initiative, bei der mehrere hundert Forschungsgruppen aus verschiedenen Universitäten Replikationsstudien zu veröffentlichten Befunden durchführen. Diese Initiative ist auch unter dem Namen \"Many Labs I\" bekannt und wurde anschließend durch das Projekt \"Many Labs II\" ergänzt, das die Unterschiede zwischen Replikationsergebnissen in verschiedenen Stichproben und Kontexten bewertet. Zu ähnlichen Projekten gehören ManyBabies, EEGManyLabs und der Psychological Science Accelerator.",
                "Related_terms": "** Collaboration; Many analysts; Many Labs I; Many Labs II; Open Science Collaboration; Replication"
            },
            {
                "Title": "Massive Open Online Courses (MOOCs) *",
                "Definition": "** Exclusively online courses which are accessible to any learner at any time, are typically free to access (while not necessarily openly licensed), and provide video-based instructions and downloadable data sets and exercises. The “massive” aspect describes the high volume of students that can access the course at any one time due to their flexibility, low or no cost, and online nature of the materials.",
                "Reference(s)": "** Baturay (2015); [https://opensciencemooc.eu/](https://opensciencemooc.eu/)",
                "Drafted by": "** Elizabeth Collins",
                "Reviewed (or Edited) by": "** Tsvetomira Dumbalska; Mahmoud Elsherif; Helena Hartmann; Sam Parsons; Charlotte R. Pennington",
                "Translated by": "Susanne Vogel",
                "Translation reviewed by": "Helena Hartmann, Jennifer Mattschey",
                "Translation": "Kurse, die ausschließlich online stattfinden, für jeden Lernenden jederzeit zugänglich sind, in der Regel kostenlos sind (wenn auch nicht unbedingt offen lizenziert) und videobasierte Anleitungen sowie herunterladbare Datensätze und Übungen bieten. Der Aspekt \"massive\" beschreibt die große Anzahl von Lernenden, die aufgrund der Flexibilität, der geringen oder gar nicht anfallenden Kosten und der Online-Natur der Materialien gleichzeitig auf den Kurs zugreifen können.",
                "Related_terms": "** Accessibility; Distance education; Inclusion; Open learning"
            },
            {
                "Title": "Massively Open Online Papers (MOOPs) *",
                "Definition": "** Unlike the traditional collaborative article, a MOOP follows an open participatory and dynamic model that is not restricted by a predetermined list of contributors.",
                "Reference(s)": "** Himmelstein et al. (2019); Tennant et al. (2019)",
                "Drafted by": "** Ali H. Al-Hoorie",
                "Reviewed (or Edited) by": "**",
                "Translated by": "Susanne Vogel",
                "Translation reviewed by": "Helena Hartmann, Jennifer Mattschey",
                "Translation": "Im Gegensatz zum traditionellen kollaborativen Artikel folgt ein MOOP einem offenen, partizipativen und dynamischen Modell, das nicht durch eine vorgegebene Liste von Mitwirkenden eingeschränkt ist.",
                "Related_terms": "** Citizen science; Collaboration; Crowdsourced Research; Many authors; Team science"
            },
            {
                "Title": "Matthew effect (in science) (Matthew-Effekt in der Wissenschaft) *",
                "Definition": "** Named for the ‘rich get richer; poor get poorer’ paraphrase of the Gospel of Matthew. Eminent scientists and early-career researchers with a prestigious fellowship are disproportionately attributed greater levels of credit and funding for their contributions to science while relatively unknown or early-career researchers without a prestigious fellowship tend to get disproportionately little credit for comparable contributions. The impact is a substantial cumulative advantage that results from modest initial comparative advantages (and vice versa).",
                "Reference(s)": "** Bol et al. (2018); Bornmann et al. (2019); Merton (1968)",
                "Drafted by": "** Tamara Kalandadze",
                "Reviewed (or Edited) by": "** Bradley Baker; Tsvetomira Dumbalska; Mahmoud Elsherif; Matt Jaquiery; Charlotte R. Pennington",
                "Translated by": "Susanne Vogel",
                "Translation reviewed by": "Helena Hartmann, Jennifer Mattschey",
                "Translation": "Benannt nach dem Sprichwort ‘rich get richer; poor get poorer’ (dt. Die Reichen werden reicher, die Armen werden ärmer) aus dem Matthäus-Evangelium. Herausragende Forschende und Nachwuchsforschende mit einem angesehenen Stipendium erhalten unverhältnismäßig viel Anerkennung und Finanzierung für ihre Beiträge zur Wissenschaft, während relativ unbekannte oder Nachwuchsforschende ohne ein angesehenes Stipendium unverhältnismäßig wenig Anerkennung für vergleichbare Beiträge erhalten. Die Folge ist ein erheblicher kumulativer Vorteil, der sich aus bescheidenen anfänglichen vergleichbaren Vorteilen ergibt (und umgekehrt).",
                "Related_terms": "** Matthew effect in education; Stigler’s law of eponymy"
            },
            {
                "Title": "Meta-analysis (Meta-Analyse) *",
                "Definition": "** A meta-analysis is a statistical synthesis of results from a series of studies examining the same phenomenon. A variety of meta-analytic approaches exist, including random or fixed effects models or meta-regressions, which allow for an examination of moderator effects. By aggregating data from multiple studies, a meta-analysis could provide a more precise estimate for a phenomenon (e.g. type of treatment) than individual studies. Results are usually visualized in a forest plot. Meta-analyses can also help examine heterogeneity across study results. Meta-analyses are often carried out in conjunction with systematic reviews and similarly require a systematic search and screening of studies. Publication bias is also commonly examined in the context of a meta-analysis and is typically visually presented via a funnel plot.",
                "Reference(s)": "** Borenstein et al. (2011); [Yeung et al. (2021)](https://mgto.org/exp-ma-rr-template-folder)",
                "Drafted by": "** Martin Vasilev; Siu Kit Yeung",
                "Reviewed (or Edited) by": "** Thomas Rhys Evans; Tamara Kalandadze; Charlotte R. Pennington; Mirela Zaneva",
                "Translated by": "Susanne Vogel",
                "Translation reviewed by": "Helena Hartmann, Jennifer Mattschey",
                "Translation": "Eine Meta-Analyse ist eine statistische Zusammenfassung von Ergebnissen aus einer Reihe von Studien, die dasselbe Phänomen untersuchen. Es gibt eine Vielzahl von meta-analytischen Ansätzen, darunter Modelle mit zufälligen oder festen Effekten oder Meta-Regressionen, die eine Untersuchung von Moderationseffekten ermöglichen. Durch die Aggregation von Daten aus mehreren Studien könnte eine Meta-Analyse eine genauere Schätzung für ein Phänomen (z. B. die Art der Behandlung) liefern als einzelne Studien. Die Ergebnisse werden in der Regel in einem Forest Plot dargestellt. Meta-Analysen können auch dazu beitragen, die Heterogenität zwischen Studienergebnissen zu untersuchen. Meta-Analysen werden häufig in Verbindung mit systematischen Literaturzusammenfassungen (Reviews) durchgeführt und erfordern ebenfalls eine systematische Suche und ein Screening von Studien. Publikationsverzerrung (Publication bias) wird ebenfalls häufig im Rahmen einer Meta-Analyse untersucht und in der Regel anhand eines Trichterdiagramms (Funnel Plots) visuell dargestellt.",
                "Related_terms": "** CONSORT; Correlational Meta-Analysis; Effect size; Evidence synthesis; Non-Intervention, Reproducible, and Open Systematic Reviews (NIRO-SR); PRISMA; Publication bias (File Drawer Problem); STROBE; Systematic Review"
            },
            {
                "Title": "Metadata (Metadaten) *",
                "Definition": "** Structured data that describes and synthesises other data. Metadata can help find, organize, and understand data. Examples of metadata include creator, title, contributors, keywords, tags, as well as any kind of information necessary to verify and understand the results and conclusions of a study such as codebook on data labels, descriptions, the sample and data collection process.",
                "Reference(s)": "** Gollwitzer et al. (2020); [https://schema.datacite.org/](https://schema.datacite.org/)",
                "Drafted by": "** Matt Jaquiery",
                "Reviewed (or Edited) by": "** Helena Hartmann; Tina Lonsdorf; Charlotte R. Pennington; Mirela Zaneva",
                "Translated by": "Susanne Vogel",
                "Translation reviewed by": "Helena Hartmann, Jennifer Mattschey",
                "Translation": "Strukturierte Daten, die andere Daten beschreiben und zusammenfassen. Metadaten können helfen, Daten zu finden, zu organisieren und zu verstehen. Beispiele für Metadaten sind Urheber:in, Titel, Mitwirkende, Schlüsselwörter (keywords), Tags sowie jede Art von Information, die zur Überprüfung und zum Verständnis der Ergebnisse und Schlussfolgerungen einer Studie erforderlich ist, z. B. ein Codebuch für Datenbeschriftungen, Beschreibungen, die Stichprobe und den Prozess der Datenerhebung.",
                "Related_terms": "** Data; Open Data **Alternative definition:** (if applicable) Data about data"
            },
            {
                "Title": "Meta-science or Meta-research (Meta-Wissenschaft oder Meta-Forschung) *",
                "Definition": "** The scientific study of science itself with the aim to describe, explain, evaluate and/or improve scientific practices. Meta-science typically investigates scientific methods, analyses, the reporting and evaluation of data, the reproducibility and replicability of research results, and research incentives.",
                "Reference(s)": "** Ioannidis et al. (2015); Peterson and Panofsky (2020)",
                "Drafted by": "** Elizabeth Collins",
                "Reviewed (or Edited) by": "** Tamara Kalandadze; Lisa Spitzer; Olmo van den Akker",
                "Translated by": "Susanne Vogel",
                "Translation reviewed by": "Helena Hartmann, Jennifer Mattschey",
                "Translation": "Die wissenschaftliche Untersuchung der Wissenschaft selbst mit dem Ziel, wissenschaftliche Praktiken zu beschreiben, zu erklären, zu bewerten und/oder zu verbessern. Meta-Wissenschaft untersucht typischerweise wissenschaftliche Methoden, Analysen, die Berichterstattung und Auswertung von Daten, die Reproduzierbarkeit und Replizierbarkeit von Forschungsergebnissen sowie Anreize in der Wissenschaft.",
                "Related_terms": "**"
            },
            {
                "Title": "Model (computational) (Rechenmodell) *",
                "Definition": "** Computational models aim to mathematically translate the phenomena under study to better understand, communicate and predict complex behaviours.",
                "Reference(s)": "** Guest and Martin (2020); Wilson and Collins (2019)",
                "Drafted by": "** Charlotte R. Pennington",
                "Reviewed (or Edited) by": "** Jamie P. Cockcroft; Mahmoud Elsherif; Meng Liu; Yu-Fang Yang; Michele C. Lim",
                "Translated by": "Susanne Vogel",
                "Translation reviewed by": "Helena Hartmann, Jennifer Mattschey",
                "Translation": "Rechenmodelle zielen darauf ab, untersuchte Phänomene mathematisch zu übersetzen, um komplexe Verhaltensweisen besser zu verstehen, zu vermitteln und vorherzusagen.",
                "Related_terms": "** algorithms; data simulation; hypothesis; theory; theory building"
            },
            {
                "Title": "Model (statistical) (statistisches Modell) *",
                "Definition": "** A mathematical representation of observed data that aims to reflect the population under study, allowing for the better understanding of the phenomenon of interest, identification of relationships among variables and predictions about future instances. A classic example would be the application of Chi square to understand the relationship between smoking and cancer (Doll & Hill, 1954\\)**.**",
                "Reference(s)": "** Doll and Hill (1954)",
                "Drafted by": "** Jamie P. Cockcroft",
                "Reviewed (or Edited) by": "** Alaa AlDoh; Mahmoud Elsherif; Meng Liu; Catia M. Oliveira; Charlotte R. Pennington",
                "Translated by": "Susanne Vogel",
                "Translation reviewed by": "Helena Hartmann, Jennifer Mattschey",
                "Translation": "Eine mathematische Repräsentation beobachteter Daten, die darauf abzielt, die untersuchte Population widerzuspiegeln, und die ein besseres Verständnis des interessierenden Phänomens, die Identifizierung von Beziehungen zwischen den Variablen und Vorhersagen über zukünftige Fälle ermöglicht. Ein klassisches Beispiel ist die Anwendung von Chi-Quadrat, um den Zusammenhang zwischen Rauchen und Krebs zu verstehen (Doll & Hill, 1954).",
                "Related_terms": "** Bayesian Inference; Model (computational); Model (philosophy); Null Hypothesis Significance Testing (NHST) **Alternative definition:** A mathematical model that embodies a set of statistical assumptions concerning the generation of sample data and is used to apply statistical analysis."
            },
            {
                "Title": "Model (philosophy) (Modell  *",
                "Definition": "** The process by which a verbal description is formalised to remove ambiguity, while also constraining the dimensions a theory can span. The model is thus data derived. “Many scientific models are representational models: they represent a selected part or aspect of the world, which is the model’s target system” (Frigg & Hartman, 2020).",
                "Reference(s)": "** Frigg and Hartman, (2020); Glass and Martin (2008); Guest and Martin (2020)",
                "Drafted by": "** Mahmoud Elsherif",
                "Reviewed (or Edited) by": "Charlotte R. Pennington; Michele C. Lim",
                "Translated by": "Helena Hartmann",
                "Translation reviewed by": "Susanne Vogel, Jennifer Mattschey",
                "Translation": "Der Prozess, bei dem eine verbale Beschreibung formalisiert wird, um Mehrdeutigkeiten zu beseitigen und gleichzeitig die Dimensionen einzuschränken, die eine Theorie umfassen kann. Das Modell wird also aus Daten abgeleitet. “Many scientific models are representational models: they represent a selected part or aspect of the world, which is the model’s target system” (übersetzt \"Viele wissenschaftliche Modelle sind Repräsentationsmodelle: Sie stellen einen ausgewählten Teil oder Aspekt der Welt dar, der das Zielsystem des Modells ist\"; Frigg & Hartman, 2020).",
                "Related_terms": "** Hypothesis; Theory; Theory building"
            },
            {
                "Title": "Multi-Analyst Studies (Multi-Analytiker:innen-Studien) *",
                "Definition": "** In typical empirical studies, a single researcher or research team conducts the analysis, which creates uncertainty about the extent to which the choice of analysis influences the results. In multi-analyst studies, two or more researchers independently analyse the same research question or hypothesis on the same dataset. According to Aczel and colleagues (2021), a multi-analyst approach may be beneficial in increasing our confidence in a particular finding; uncovering the impact of analytical preferences across research teams; and highlighting the variability in such analytical approaches.",
                "Reference(s)": "** Aczel et. al. (2021); Silberzahn et al. (2018)",
                "Drafted by": "** Sam Parsons",
                "Reviewed (or Edited) by": "** Tsvetomira Dumbalska; Mahmoud Elsherif; William Ngiam; Charlotte R. Pennington; Graham Reid; Barnabas Szaszi; Flávio Azevedo",
                "Translated by": "Susanne Vogel",
                "Translation reviewed by": "Helena Hartmann, Jennifer Mattschey",
                "Translation": "In typischen empirischen Studien führt ein:e einzige:r Forschende:r oder ein Forschungsteam die Analyse durch, was zu Unsicherheit darüber führt, inwieweit die Wahl der Analyse die Ergebnisse beeinflusst. Bei Studien mit mehreren Analytiker:innen analysieren zwei oder mehr Forschende unabhängig voneinander dieselbe Forschungsfrage oder Hypothese anhand desselben Datensatzes. Laut Aczel und Kollegen (2021) kann ein Multi-Analytiker:innen-Ansatz vorteilhaft sein, um das Vertrauen in ein bestimmtes Ergebnis zu erhöhen, die Auswirkungen der analytischen Präferenzen verschiedener Forschungsteams aufzudecken und die Variabilität solcher analytischen Ansätze hervorzuheben.",
                "Related_terms": "** Analytic flexibility; Crowdsourcing science; Data Analysis; Garden of Forking Paths; Multiverse Analysis; Researcher Degrees of Freedom; Scientific Transparency"
            },
            {
                "Title": "Multiplicity (Multiplizität) *",
                "Definition": "** Potential inflation of Type I error rates (incorrectly rejecting the null hypothesis) because of multiple statistical testing, for example, multiple outcomes, multiple follow-up time points, or multiple subgroup analyses. To overcome issues with multiplicity, researchers will often apply controlling procedures (e.g., Bonferroni, Holm-Bonferroni; Tukey) that correct the alpha value to control for inflated Type I errors. However, by controlling for Type I errors, one can increase the possibility of Type II errors (i.e., incorrectly accepting the null hypothesis).",
                "Reference(s)": "** Sato (1996); Schultz and Grimes (2005)",
                "Drafted by": "** Aidan Cashin",
                "Reviewed (or Edited) by": "** Jamie P. Cockcroft; Mahmoud Elsherif; Meng Liu; Charlotte R. Pennington",
                "Translated by": "Susanne Vogel",
                "Translation reviewed by": "Helena Hartmann, Jennifer Mattschey",
                "Translation": "Potenzielle Inflation der Typ I \\- Fehlerraten (fälschliche Zurückweisung der Nullhypothese) aufgrund mehrfacher statistischer Tests, z. B. mehrere Ergebnisse, mehrere Follow-up-Zeitpunkte oder mehrere Teilgruppenanalysen. Um Probleme mit der Multiplizität zu überwinden, wenden Forschende häufig Kontrollverfahren an (z. B. Bonferroni, Holm-Bonferroni; Tukey), die den Alpha-Wert korrigieren, um die Inflation von Typ-I-Fehlern zu kontrollieren. Durch die Kontrolle von Fehlern des Typs I kann jedoch die Möglichkeit von Fehlern des Typs II (d. h. das falsche Beibehalten der Nullhypothese) erhöht werden.",
                "Related_terms": "** Alpha; False Discovery Rate; Multiple comparisons problem; Multiple testing; Null Hypothesis Significance Testing (NHST)"
            },
            {
                "Title": "Multiverse analysis (Multiversumsanalyse) *",
                "Definition": "** Multiverse analyses are based on all potentially equally justifiable data processing and statistical analysis pipelines that can be employed to test a single hypothesis. In a data multiverse analysis, a single set of raw data is processed into a multiverse of data sets by applying all possible combinations of justifiable preprocessing choices. Model multiverse analyses apply equally justifiable statistical models to the same data to answer the same hypothesis. The statistical analysis is then conducted on all data sets in the multiverse and all results are reported which enhances promoting transparency and illustrates the robustness of results against different data processing (data multiverse) or statistical (model multiverse) pipelines). Multiverse analysis differs from Specification curve analysis with regards to the graphical displays (a histogram and tile plota rather than a specification curve plot).",
                "Reference(s)": "** Del Giudice and Gangestad (2021); Steegen et al. (2016)",
                "Drafted by": "** Tina Lonsdorf; Flávio Azevedo",
                "Reviewed (or Edited) by": "** Mahmoud Elsherif; Adrien Fillon; William Ngiam; Sam Parsons",
                "Translated by": "Susanne Vogel",
                "Translation reviewed by": "Helena Hartmann, Jennifer Mattschey  ###  ### **N** {#n}",
                "Translation": "Multiversums-Analysen basieren auf allen potenziell gleichermaßen vertretbaren Datenverarbeitungs- und statistischen Analysemöglichkeiten, die zur Prüfung einer einzigen Hypothese eingesetzt werden können. Bei einer Daten-Multiversums-Analyse wird ein einzelner Rohdatensatz zu einem Multiversum von Datensätzen verarbeitet, indem alle möglichen Kombinationen von vertretbaren Vorverarbeitungsentscheidungen angewendet werden. Bei Modell-Multiversums-Analysen werden gleichermaßen vertretbare statistische Modelle auf dieselben Daten angewendet, um dieselbe Hypothese zu beantworten. Die statistische Analyse wird dann für alle Datensätze im Multiversum durchgeführt, und alle Ergebnisse werden berichtet, was die Transparenz fördert und die Robustheit der Ergebnisse gegenüber verschiedenen Datenverarbeitungs- (Daten-Multiversum) oder statistischen (Modell-Multiversum) Pipelines verdeutlicht. Die Multiversum-Analyse unterscheidet sich von der Spezifikationskurven-Analyse durch die grafische Darstellung (Histogramm und Kacheldiagramm anstelle eines Spezifikationskurvendiagramms).",
                "Related_terms": "** Garden of forking paths; Robustness (analyses); Specification curve analysis; Vibration of effects"
            },
            {
                "Title": "Name Ambiguity Problem (Problem mehrdeutiger Namen) *",
                "Definition": "** An attribution issue arising from two related problems: authors may use multiple names or monikers to publish work, and multiple authors in a single field may share full names. This makes accurate identification of authors on names and specialisms alone a difficult task. This can be addressed through the creation and use of unique digital identifiers that act akin to digital fingerprints such as ORCID.",
                "Reference(s)": "** Wilson and Fenner (2012)",
                "Drafted by": "** Shannon Francis",
                "Reviewed (or Edited) by": "** Tsvetomira Dumbalska; Mahmoud Elsherif; Helena Hartmann; Wanyin Li; Charlotte R. Pennington",
                "Translated by": "Helena Hartmann",
                "Translation reviewed by": "Susanne Vogel, Jennifer Mattschey",
                "Translation": "Ein Zuordnungsproblem, das sich aus zwei miteinander zusammenhängenden Problemen ergibt: Autor:innen können mehrere Namen oder Pseudonyme verwenden, um ihre Arbeiten zu veröffentlichen, und mehrere Autor:innen eines Fachgebiets können denselben vollständigen Namen haben. Dies macht die genaue Identifizierung von Autor:innen allein anhand von Namen und Fachgebieten zu einer schwierigen Aufgabe. Dieses Problem kann durch die Schaffung und Verwendung eindeutiger digitaler Identifikatoren gelöst werden, die wie digitale Fingerabdrücke funktionieren, wie z. B. ORCID.",
                "Related_terms": "** Authorship; DOI (digital object identifier); ORCID (Open Researcher and Contributor ID)"
            },
            {
                "Title": "Named entity-based Text Anonymization for Open Science (NETANOS) *",
                "Definition": "** A free, open-source anonymisation software that identifies and modifies named entities (e.g. persons, locations, times, dates). Its key feature is that it preserves critical context needed for secondary analyses. The aim is to assist researchers in sharing their raw text data, while adhering to research ethics.",
                "Reference(s)": "** Kleinberg et al. (2017)",
                "Originally drafted by": "** Norbert Vanek",
                "Reviewed (or Edited) by": "** Jamie P. Cockcroft; Aleksandra Lazić; Charlotte R. Pennington; Sam Parsons, Elif Bastan",
                "Translated by": "Helena Hartmann",
                "Translation reviewed by": "Susanne Vogel  ####",
                "Translation": "Eine kostenlose, quelloffene Anonymisierungssoftware, die benannte Entitäten (z. B. Personen, Orte, Zeiten, Daten) identifiziert und verändert. Ihr Hauptmerkmal besteht darin, dass sie den für Sekundäranalysen benötigten kritischen Kontext bewahrt. Ziel ist es, Forscher:innen bei der gemeinsamen Nutzung ihrer ursprünglichen Textdaten zu unterstützen und sich dabei an Forschungsethik zu halten.",
                "Related_terms": "** Anonymity; Confidentiality; Data sharing; Research ethics"
            },
            {
                "Title": "Non-Intervention, Reproducible, and Open Systematic Reviews (NIRO-SR) *",
                "Definition": "** A comprehensive set of tools to facilitate the development, preregistration and dissemination of systematic literature reviews for non-intervention research. Part A represents detailed guidelines for creating and preregistering a systematic review protocol in the context of non-intervention research whilst preparing for transparency. Part B represents guidelines for writing up the completed systematic review, with a focus on enhancing reproducibility.",
                "Reference(s)": "** Topor et al. (2021)",
                "Drafted by": "** Asma Assaneea",
                "Reviewed (or Edited) by": "** Tsvetomira Dumbalska; Thomas Rhys Evans; Tamara Kalandadze; Jade Pickering; Mirela Zaneva",
                "Translated by": "Helena Hartmann",
                "Translation reviewed by": "Susanne Vogel, Jennifer Mattschey",
                "Translation": "Ein umfassender Satz von Instrumenten zur Erleichterung der Entwicklung, Präregistrierung und Veröffentlichung von systematischen Literaturarbeiten für nicht-interventionelle Forschung. Teil A enthält detaillierte Leitlinien für die Erstellung und Präregistrierung eines systematischen Reviewprotokolls im Kontext der nicht-interventionellen Forschung und kreiert dabei gleichzeitig die Rahmenbedingungen für spätere Transparenz. Teil B enthält Leitlinien für das Verfassen der fertigen systematischen Literaturarbeit, wobei der Schwerpunkt auf der Verbesserung der Reproduzierbarkeit liegt.",
                "Related_terms": "** Knowledge accumulation; Systematic review; Systematic Review Protocol"
            },
            {
                "Title": "Null Hypothesis Significance Testing (NHST, Nullhypothesen-Signifikanztestung) *",
                "Definition": "** A frequentist approach to inference used to test the probability of an observed effect against the null hypothesis of no effect/relationship (Pernet, 2015). Such a conclusion is arrived at through use of an index called the *p*\\-value. Specifically, researchers will conclude an effect is present when an a priori alpha threshold, set by the researchers, is satisfied; this determines the acceptable level of uncertainty and is closely related to Type I error.",
                "Reference": "** Lakens et al. (2018); Pernet (2015); Spence and Stanley (2018)",
                "Originally drafted by": "** Alaa AlDoh",
                "Reviewed (or Edited) by": "** Jamie P. Cockcroft; Annalise A. LaPlume; Charlotte R. Pennington; Sonia Rishi",
                "Translated by": "Helena Hartmann",
                "Translation reviewed by": "Susanne Vogel, Jennifer Mattschey  ### **O** {#o}",
                "Translation": "Ein frequentistischer Ansatz für Schlussfolgerungen, der verwendet wird, um die Wahrscheinlichkeit eines beobachteten Effekts angesichts der Nullhypothese eines fehlenden Effekts/Zusammenhangs zu testen (Pernet, 2015). Eine solche Schlussfolgerung wird durch die Verwendung eines Indexes, des sogenannten p-Wertes, erreicht. Insbesondere schließen Forscher:innen auf das Vorhandensein eines Effekts, wenn ein von den Forscher:innen vorher festgelegter Alpha-Schwellenwert erfüllt ist; dieser bestimmt das akzeptable Maß an Unsicherheit und ist eng mit dem Typ-I-Fehler verbunden.",
                "Related_terms": "** Inference; P-value; Statistical significance; Type I error"
            },
            {
                "Title": "Objectivity (Objektivität) *",
                "Definition": "** The idea that scientific claims, methods, results and scientists themselves should remain value-free and unbiased, and thus not be affected by cultural, political, racial or religious bias as well as any personal interests (Merton, 1942).",
                "Reference(s)": "** Macfarlane and Cheng (2008); Merton (1942)",
                "Originally drafted by": "** Ryan Millager",
                "Reviewed (or Edited) by": "** Mahmoud Elsherif; Madeleine Ingham; Kai Krautter; Sam Parsons; Charlotte R. Pennington",
                "Translated by": "Helena Hartmann",
                "Translation reviewed by": "Susanne Vogel, Jennifer Mattschey",
                "Translation": "Die Idee, dass wissenschaftliche Behauptungen, Methoden, Ergebnisse und Wissenschaftler:innen selbst wertfrei und unvoreingenommen bleiben sollten und somit nicht von kulturellen, politischen, herkunftsbezogenen oder religiösen Vorurteilen sowie persönlichen Interessen beeinflusst werden sollten (Merton, 1942).",
                "Related_terms": "** Communality; Mertonian norms; Neutrality"
            },
            {
                "Title": "Ontology (Artificial Intelligence) (Ontologie in der Künstlichen Intelligenz) *",
                "Definition": "** A set of axioms in a subject area that help classify and explain the nature of the entities under study and the relationships between them.",
                "Reference": "** Noy and McGuinness (2001)",
                "Originally drafted by": "** Emma Norris",
                "Reviewed (or Edited) by": "** Charlotte R. Pennington; Graham Reid",
                "Translated by": "Susanne Vogel",
                "Translation reviewed by": "Helena Hartmann, Jennifer Mattschey",
                "Translation": "Eine Sammlung von Grundregeln (axioms) in einem Fachgebiet, die zur Klassifizierung und Erklärung der Art der untersuchten Einheiten und der Beziehungen zwischen ihnen beitragen.",
                "Related_terms": "** Axiology; Epistemology; Taxonomy"
            },
            {
                "Title": "Open access *",
                "Definition": "** “Free availability of scholarship on the public internet, permitting any users to read, download, copy, distribute, print, search, or link to the full texts of these research articles, crawl them for indexing, pass them as data to software, or use them for any other lawful purpose, without financial, legal, or technical barriers other than those inseparable from gaining access to the internet itself” (Boai, 2002). Different methods of achieving open access (OA) are often referred to by color, including Green Open Access (when the work is openly accessible from a public repository), Gold Open Access (when the work is immediately openly accessible upon publication via a journal website), and Platinum (or Diamond) Open Access (a subset of Gold OA in which all works in the journal are immediately accessible after publication from the journal website without the authors needing to pay an article processing fee \\[APC\\]).",
                "Reference(s)": "** [Budapest Open Access Initiative (2002)](https://www.budapestopenaccessinitiative.org/read); Suber (2015)",
                "Drafted by": "** Mahmoud Elsherif",
                "Reviewed (or Edited) by": "** Nick Ballou; Helena Hartmann; Aoife O’Mahony; Ross Mounce; Mariella Paul; Charlotte R. Pennington",
                "Translated by": "Helena Hartmann",
                "Translation reviewed by": "Susanne Vogel, Jennifer Mattschey",
                "Translation": "Free availability of scholarship on the public internet, permitting any users to read, download, copy, distribute, print, search, or link to the full texts of these research articles, crawl them for indexing, pass them as data to software, or use them for any other lawful purpose, without financial, legal, or technical barriers other than those inseparable from gaining access to the internet itself” (Boai, 2002; dt.: Freie Verfügbarkeit von wissenschaftlichen Arbeiten im öffentlichen Internet, die es allen Nutzer:innen erlaubt, die Volltexte dieser Forschungsartikel zu lesen, herunterzuladen, zu kopieren, zu verbreiten, zu drucken, zu durchsuchen oder mit ihnen zu verlinken, sie für die Indexierung zu durchsuchen, sie als Daten an Software weiterzugeben oder sie für jeden anderen rechtmäßigen Zweck zu nutzen, ohne andere finanzielle, rechtliche oder technische Hindernisse als die, die untrennbar mit dem Zugang zum Internet selbst verbunden sind). Verschiedene Methoden zur Erreichung von Open Access werden oft farblich gekennzeichnet, darunter Green Open Access (wenn das Werk über ein öffentliches Repositorium zugänglich ist), Gold Open Access (wenn das Werk unmittelbar nach der Veröffentlichung über eine Zeitschriften-Website zugänglich ist) und Platin (oder Diamant) Open Access (eine Untergruppe von Gold Open Access, bei der alle Werke in der Zeitschrift unmittelbar nach der Veröffentlichung über die Zeitschriften-Website zugänglich sind, ohne dass die Autor:innen eine Gebühr für die Bearbeitung von Artikeln \\[article processing fee; APC\\] zahlen müssen).",
                "Related_terms": "** Article Processing Charge; FAIR principles; Paywall; Preprint; Repository"
            },
            {
                "Title": "Open Code (Offener Code) *",
                "Definition": "** Making computer code (e.g., programming, analysis code, stimuli generation) freely and publicly available in order to make research methodology and analysis transparent and allow for reproducibility and collaboration. Code can be made available via open code websites, such as GitHub, the Open Science Framework, and Codeshare (to name a few), enabling others to evaluate and correct errors and re-use and modify the code for subsequent research.",
                "Reference": "** Easterbrook (2014)",
                "Originally drafted by": "** Charlotte R. Pennington",
                "Reviewed (or Edited) by": "** Elizabeth Collins; Mahmoud Elsherif; Christopher Graham; Emma Henderson",
                "Translated by": "Susanne Vogel",
                "Translation reviewed by": "Helena Hartmann, Jennifer Mattschey",
                "Translation": "Freier und öffentlicher Zugang zu Computercode (z. B. Programmierung, Analysecode, Stimulusgenerierung), um die Forschungsmethodik und \\-analyse transparent zu machen und Reproduzierbarkeit und Zusammenarbeit zu ermöglichen. Der Code kann z. B. über Open-Code-Websites wie GitHub, das Open Science Framework und Codeshare zur Verfügung gestellt werden, so dass andere den Code bewerten, Fehler korrigieren und ihn für spätere Forschungsarbeiten wiederverwenden und verändern können.",
                "Related_terms": "** Computational Reproducibility; Open Access; Open Licensing; Open Material; Open Source; Open Source Software; Reproducibility; Syntax"
            },
            {
                "Title": "Open Data (Offene Daten) *",
                "Definition": "** Open data refers to data that is freely available and readily accessible for use by others without restriction, “Open data and content can be freely used, modified, and shared by anyone for any purpose” ([https://opendefinition.org/](https://opendefinition.org/)). Open data are subject to the requirement to attribute and share alike, thus it is important to consider appropriate Open Licenses. Sensitive or time-sensitive datasets can be embargoed or shared with more selective access options to ensure data integrity is upheld.",
                "Reference": "** [https://opendefinition.org/](https://opendefinition.org/) (version 2.1); [https://opendatahandbook.org/guide/en/what-is-open-data/](https://opendatahandbook.org/guide/en/what-is-open-data/)",
                "Originally drafted by": "** Lisa Spitzer",
                "Reviewed (or Edited) by": "** Sarah Ashcroft-Jones; Mahmoud Elsherif; Helena Hartmann; Matt Jaquiery; Flávio Azevedo; Ross Mounce; Charlotte R. Pennington; Steven Verheyen",
                "Translated by": "Susanne Vogel",
                "Translation reviewed by": "Helena Hartmann, Jennifer Mattschey",
                "Translation": "Offene Daten sind frei verfügbare und direkt zugängliche Daten, die von anderen uneingeschränkt genutzt werden können. \"Open data and content can be freely used, modified, and shared by anyone for any purpose” (dt. Offene Daten und Inhalte können von jeder:m zu jedem Zweck frei verwendet, verändert und weitergegeben werden\"; https://opendefinition.org/). Offene Daten unterliegen sowohl dem Erfordernis der Kennzeichnung als auch des Teilens, daher ist es wichtig, geeignete Lizenzen (Open Licenses) in Betracht zu ziehen. Sensible oder zeitkritische Datensätze können mit einer Sperrfrist (embargo) belegt oder mit selektiveren Zugriffsoptionen freigegeben werden, um die Integrität der Daten zu gewährleisten.",
                "Related_terms": "** Badges (Open Science); Data availability; FAIR principles; Metadata; Open Licenses; Open Material; Reproducibility; Secondary data analysis"
            },
            {
                "Title": "Open Educational Resources (OERs, Offene Bildungsressourcen) *",
                "Definition": "** Learning materials that can be modified and enhanced because their creators have given others permission to do so. The individuals or organizations that create OERs—which can include materials such as presentation slides, podcasts, syllabi, images, lesson plans, lecture videos, maps, worksheets, and even entire textbooks—waive some (if not all) of the copyright associated with their works, typically via legal tools like Creative Commons licenses, so others can freely access, reuse, translate, and modify them.",
                "Reference": "** [https://opensource.com/resources/what-open-education](https://opensource.com/resources/what-open-education); [https://en.unesco.org/themes/building-knowledge-societies/oer](https://en.unesco.org/themes/building-knowledge-societies/oer)",
                "Originally drafted by": "** Aleksandra Lazić",
                "Reviewed (or Edited) by": "** Sam Parsons; Charlotte R. Pennington; Steven Verheyen; Elizabeth Collins",
                "Translated by": "Helena Hartmann",
                "Translation reviewed by": "Susanne Vogel, Jennifer Mattschey",
                "Translation": "Lernmaterialien, die verändert und verbessert werden können, weil ihre Urheber:innen anderen die Erlaubnis dazu erteilt haben. Die Personen oder Organisationen, die OER erstellen \\- zu denen Materialien wie Präsentationsfolien, Podcasts, Lehrpläne, Bilder, Unterrichtspläne, Vorlesungsvideos, Karten, Arbeitsblätter und sogar ganze Lehrbücher gehören können \\- verzichten auf einige (oder alle) der mit ihren Werken verbundenen Urheberrechte, in der Regel über rechtliche Instrumente wie Creative-Commons-Lizenzen, so dass andere frei darauf zugreifen, sie wiederverwenden, übersetzen und verändern können.",
                "Related_terms": "** Accessibility; FORRT; Open access; Open Licenses; Open Material"
            },
            {
                "Title": "Open Educational Resources (OER) Commons  *",
                "Definition": "** OER Commons (with OER standing for open educational resources) is a freely accessible online library allowing teachers to create, share and remix educational resources. The goal of the OER movement is to stimulate “collaborative teaching and learning” ([https://www.oercommons.org/about](https://www.oercommons.org/about)) and provide high-quality educational resources that are accessible for everyone.",
                "Reference(s)": "** www.oercommons.org",
                "Drafted by": "** Ali H. Al-Hoorie",
                "Reviewed (or Edited) by": "** Mahmoud Elsherif, Gisela H. Govaart",
                "Translated by": "Helena Hartmann",
                "Translation reviewed by": "Susanne Vogel, Jennifer Mattschey",
                "Translation": "OER Commons (OER steht für Open Educational Resources, aus dem Engl. offene Bildungsressourcen) ist eine frei zugängliche Online-Bibliothek, die es Lehrkräften ermöglicht, Bildungsressourcen zu erstellen, zu teilen und neu zu kombinieren. Das Ziel der OER-Bewegung ist es, kollaboratives Lehren und Lernen (https://www.oercommons.org/about) zu fördern und qualitativ hochwertige Bildungsressourcen bereitzustellen, die für jeden zugänglich sind.",
                "Related_terms": "** Equity; FORRT; Inclusion; Open Scholarship Knowledge Base; Open Science Framework"
            },
            {
                "Title": "Open Licenses (Offene Lizenzen) *",
                "Definition": "** Open licenses are provided with open data and open software (e.g., analysis code) to define how others can (re)use the licensed material. In setting out the permissions and restrictions, open licenses often permit the unrestricted access, reuse and retribution of an author’s original work. Datasets are typically licensed under a type of open licence known as a Creative Commons license (e.g., MIT, Apache, and GPL). These can differ in relatively subtle ways with GPL licenses (and their variants) being Copyleft licenses that require that any derivative work is licensed under the same terms as the original.",
                "Reference(s)": "** [https://opensource.org/licenses](https://opensource.org/licenses)",
                "Originally drafted by": "** Andrew J. Stewart",
                "Reviewed (or Edited) by": "** Elizabeth Collins; Sam Parsons; Graham Reid; Steven Verheyen",
                "Translated by": "Susanne Vogel",
                "Translation reviewed by": "Helena Hartmann, Jennifer Mattschey",
                "Translation": "Offene Lizenzen werden mit offenen Daten und offener Software (z. B. Analysecode) bereitgestellt, um festzulegen, wie andere das lizenzierte Material (wieder-)verwenden können. Indem sie die Rechte und Einschränkungen festlegen, erlauben offene Lizenzen oft den uneingeschränkten Zugang, die Wiederverwendung und die Weitergabe der Originalarbeit einer:s Autor:in. Datensätze werden in der Regel unter einer Art von offener Lizenz lizenziert, die als Creative-Commons-Lizenz bekannt ist (z. B. MIT, Apache und GPL). Diese können sich auf relativ subtile Weise unterscheiden, wobei die GPL-Lizenzen (und ihre Varianten) Copyleft-Lizenzen sind, die verlangen, dass jede abgeleitete Arbeit unter denselben Bedingungen wie das Original lizenziert wird.",
                "Related_terms": "** Creative Commons (CC) License; Copyleft; Copyright; Licence; Open Data; Open Source"
            },
            {
                "Title": "Open Material (offene Materialien) *",
                "Definition": "** Author’s public sharing of materials that were used in a study, “such as survey items, stimulus materials, and experiment programs” (Kidwell et al., 2016, p. 3). Digitally-shareable materials are posted on open access repositories, which makes them publicly available and accessible. Depending on licensing, the material can be reused by other authors for their own studies. Components that are not digitally-shareable (e.g. biological materials, equipment) must be described in sufficient detail to allow reproducibility.",
                "Reference": "** Blohowiak et al. (2020); Kidwell et al. (2016)",
                "Originally drafted by": "** Lisa Spitzer",
                "Reviewed (or Edited) by": "** Sam Parsons; Charlotte R. Pennington; Olly Robertson; Emily A. Williams; Flávio Azevedo",
                "Translated by": "Helena Hartmann",
                "Translation reviewed by": "Susanne Vogel, Jennifer Mattschey",
                "Translation": "Die öffentliche Freigabe von Materialien, die in einer Studie verwendet wurden, “such as survey items, stimulus materials, and experiment programs” (Kidwell et al., 2016, S. 3, aus dem Engl. \"wie z. B. Umfrageelemente, Stimulusmaterialien und Experimental-Programme\") durch den/die Autor:in. Digital teilbare Materialien werden auf Open-Access-Repositorien veröffentlicht, wodurch sie öffentlich verfügbar und zugänglich werden. Je nach Lizenzierung kann das Material von anderen Autor:innen für ihre eigenen Studien wiederverwendet werden. Komponenten, die nicht digital weitergegeben werden können (z. B. biologische Materialien, Geräte), müssen ausreichend detailliert beschrieben werden, um eine Reproduzierbarkeit zu ermöglichen.",
                "Related_terms": "** Badges (Open Science); Credibility of scientific claims; FAIR principles; Open Access; Open Code; Open Data; Reproducibility; Transparency"
            },
            {
                "Title": "OpenNeuro *",
                "Definition": "** A free platform where researchers can freely and openly share, browse, download and re-use brain imaging data (e.g., MRI, MEG, EEG, iEEG, ECoG, ASL, and PET data).",
                "Reference(s)": "** Poldrack et al. (2013); Poldrack and Gorgolewski (2014) https://openneuro.org/",
                "Drafted by": "** Ali H. Al-Hoorie",
                "Reviewed (or Edited) by": "** Leticia Micheli, Gisela H. Govaart",
                "Translated by": "Helena Hartmann",
                "Translation reviewed by": "Susanne Vogel, Jennifer Mattschey",
                "Translation": "Eine kostenlose Plattform, auf der Forscher:innen Daten aus Bildgebungsverfahren des Gehirns (z. B. MRI-, MEG-, EEG-, iEEG-, EKoG-, ASL- und PET-Daten) frei und offen austauschen, durchsuchen, herunterladen und wiederverwenden können.",
                "Related_terms": "** BIDS data structure; Open data; OpenfMRI"
            },
            {
                "Title": "Open Peer Review (Offene Begutachtung) *",
                "Definition": "** A scholarly review mechanism providing disclosure of any combination of author and referee identities, as well as peer-review reports and editorial decision letters, to one another or publicly at any point during or after the peer review or publication process. It may also refer to the removal of restrictions on who can participate in peer review and the platforms for doing so. Note that ‘open peer review’ has been used interchangeably to refer to any, or all, of the above practices.",
                "Reference(s)": "Ross-Hellauer (2017)",
                "Originally drafted by": "** Sonia Rishi",
                "Reviewed (or Edited) by": "** Mahmoud Elsherif; Sam Parsons; Charlotte R. Pennington; Yuki Yamada; Flávio Azevedo",
                "Translated by": "Susanne Vogel",
                "Translation reviewed by": "Helena Hartmann, Jennifer Mattschey",
                "Translation": "Ein wissenschaftlicher Begutachtungsmechanismus, der die Offenlegung der Identität von Autor:innen und Gutachter:innen sowie von Peer-Review-Gutachten und editorischen Entscheidungsschreiben untereinander oder gegenüber der Öffentlichkeit zu jedem Zeitpunkt während oder nach dem Peer-Review- oder Veröffentlichungsprozess ermöglicht. Der Begriff kann sich auch auf die Aufhebung von Beschränkungen für die Teilnahme am Peer-Review-Verfahren und die entsprechenden Plattformen dafür beziehen. Beachten Sie, dass der Begriff \"offene Begutachtung\" austauschbar verwendet wird, um sich auf jede der oben genannten Praktiken oder auf alle zu beziehen.",
                "Related_terms": "** Non-anonymised peer review; Open science; PRO (peer review openness) initiative; Transparent peer review"
            },
            {
                "Title": "Open Scholarship *",
                "Definition": "‘**Open scholarship’ is often used synonymously with ‘open science’, but extends to all disciplines, drawing in those which might not traditionally identify as science-based. It reflects the idea that knowledge of all kinds should be openly shared, transparent, rigorous, reproducible, replicable, accumulative, and inclusive (allowing for all knowledge systems). Open scholarship includes all scholarly activities that are not solely limited to research such as teaching and pedagogy.",
                "Reference(s)": "** Tennant et al. (2019) Foundations for Open Scholarship Strategy Development https://www.researchgate.net/publication/330742805\\_Foundations\\_for\\_Open\\_Scholarship\\_Strategy\\_Development",
                "Drafted by": "** Gerald Vineyard",
                "Reviewed (or Edited) by": "** Mahmoud Elsherif; Zoe Flack; Sam Parsons; Charlotte R. Pennington",
                "Translated by": "Helena Hartmann",
                "Translation reviewed by": "Susanne Vogel, Jennifer Mattschey",
                "Translation": "Der Begriff \"Open Scholarship\" wird oft synonym mit \"Open Science\" verwendet, erstreckt sich aber auf alle Disziplinen und schließt auch solche ein, die sich selbst traditionell nicht als wissenschaftlich bezeichnen. Er spiegelt die Idee wider, dass Wissen aller Art offen, transparent, rigoros (genau), reproduzierbar, replizierbar, akkumulierbar und inklusiv sein sollte (unter Berücksichtigung aller Wissenssysteme). Open Scholarship umfasst alle wissenschaftlichen Aktivitäten, die sich nicht nur auf die Forschung beschränken, wie Lehre und Pädagogik.",
                "Related_terms": "** Bropenscience; Decolonisation; Knowledge; Open Research; Open Science"
            },
            {
                "Title": "Open Scholarship Knowledge Base  *",
                "Definition": "** The Open Scholarship Knowledge Base (OSKB) is a collaborative initiative to share knowledge on the what, why and how of open scholarship to make this knowledge easy to find and apply. Information is curated and created by the community. The OSKB is a community under the Center for Open Science (COS).",
                "Reference(s)": "** www.oercommons.org/hubs/OSKB",
                "Drafted by": "** Ali H. Al-Hoorie",
                "Reviewed (or Edited) by": "** Mahmoud Elsherif; Samuel Guay; Tamara Kalandadze",
                "Translated by": "Susanne Vogel",
                "Translation reviewed by": "Helena Hartmann",
                "Translation": "Die Open Scholarship Knowledge Base (OSKB) ist eine gemeinschaftliche Initiative zum Austausch von Wissen über das Was, Warum und Wie von Open Scholarship, damit dieses Wissen leicht zu finden und anzuwenden ist. Die Informationen werden von der Gemeinschaft kuratiert und erstellt. Die OSKB ist eine Gemeinschaft im Rahmen des Center for Open Science (COS).",
                "Related_terms": "** Center for Open Science (COS), Open Educational Resources (OERs); Open scholarship; Open Science"
            },
            {
                "Title": "Open Science (Offene Forschung) *",
                "Definition": "** An umbrella term reflecting the idea that scientific knowledge of all kinds, where appropriate, should be openly accessible, transparent, rigorous, reproducible, replicable, accumulative, and inclusive, all which are considered fundamental features of the scientific endeavour. Open science consists of principles and behaviors that promote transparent, credible, reproducible, and accessible science. Open science has six major aspects: open data, open methodology, open source, open access, open peer review, and open educational resources.",
                "Reference(s)": "** Abele-Brehm et al. (2019); Crüwell et al. (2019); Kathawalla et al. (2020); Syed (2019); Woelfe et al. (2011)",
                "Drafted by": "** Mahmoud Elsherif",
                "Reviewed (or Edited) by": "** Zoe Flack; Tamara Kalandadze; Charlotte R. Pennington; Qinyu Xiao",
                "Translated by": "Helena Hartmann",
                "Translation reviewed by": "Susanne Vogel, Jennifer Mattschey",
                "Translation": "Ein Oberbegriff, der die Idee widerspiegelt, dass wissenschaftliche Erkenntnisse aller Art, wenn möglich, offen zugänglich, transparent, rigoros (genau), reproduzierbar, replizierbar, akkumulierbar und inklusiv sein sollten. Diese Eigenschaften werden als  grundlegende Merkmale wissenschaftlicher Bestrebungen angesehen. Offene Wissenschaft besteht aus Prinzipien und Verhaltensweisen, die eine transparente, glaubwürdige, reproduzierbare und zugängliche Wissenschaft fördern. Offene Wissenschaft hat sechs Hauptaspekte: offene Daten, offene Methodik, offene Quellcodes, offener Zugang (Open Access), offenes Peer Review und offene Bildungsressourcen.",
                "Related_terms": "** Accessibility; Credibility; Open Data; Open Material; Open Peer Review; Open Research; Open Science Practices; Open Scholarship; Reproducibility crisis (aka Replicability or replication crisis); Reproducibility; Transparency"
            },
            {
                "Title": "Open Science Framework *",
                "Definition": "** A free and open source platform for researchers to organize and share their research project and to encourage collaboration. Often used as an open repository for research code, data and materials, preprints and preregistrations, while managing a more efficient workflow. Created and maintained by the Center for Open Science.",
                "Reference(s)": "** Foster and Deardorff (2017); https://osf.io/",
                "Drafted by": "** William Ngiam",
                "Reviewed (or Edited) by": "** Mahmoud Elsherif; Charlotte R. Pennington; Lisa Spitzer",
                "Translated by": "Susanne Vogel",
                "Translation reviewed by": "Helena Hartmann, Jennifer Mattschey  ### ---",
                "Translation": "Eine kostenlose open-source Plattform für Forschende zur Organisation und gemeinsamen Nutzung ihrer Forschungsprojekte und zur Förderung der Zusammenarbeit. Sie wird häufig als offenes Repositorium für Forschungscode, Daten und Materialien, Preprints und Präregistrierungen verwendet und ermöglicht gleichzeitig einen effizienteren Arbeitsablauf. Erstellt und gepflegt durch das Center for Open Science.",
                "Related_terms": "** Archive; Center for Open Science (COS); Open Code; Open Data; Preprint; Preregistration"
            },
            {
                "Title": "Open Source software (Software mit offenem Quellcode) *",
                "Definition": "** A type of computer software in which source code is released under a license that permits others to use, change, and distribute the software to anyone and for any purpose. Open source is more than openly accessible: the distribution terms of open-source software must comply with 10 specific criteria (see: [https://opensource.org/osd](https://opensource.org/osd)).",
                "Reference": "** [https://opensource.org/osd](https://opensource.org/osd); [https://www.fosteropenscience.eu/foster-taxonomy/open-source-open-science](https://www.fosteropenscience.eu/foster-taxonomy/open-source-open-science)",
                "Originally drafted by": "** Connor Keating",
                "Reviewed (or Edited) by": "** Jamie P. Cockcroft; Helena Hartmann; Charlotte R. Pennington; Andrew J. Stewart",
                "Translated by": "Helena Hartmann",
                "Translation reviewed by": "Susanne Vogel, Jennifer Mattschey",
                "Translation": "Eine Art von Computersoftware, bei der der Quellcode unter einer Lizenz veröffentlicht wird, die es anderen erlaubt, die Software zu nutzen, zu verändern und an jede Person und für jeglichen Zweck weiterzugeben. Open Source ist mehr als nur offen zugänglich: Die Vertriebsbedingungen von Open-Source-Software müssen 10 spezifische Kriterien erfüllen (siehe: https://opensource.org/osd).",
                "Related_terms": "** Github; Open Access; Open Code; Open Data; Open Licenses; Python; R; Repository"
            },
            {
                "Title": "Open washing *",
                "Definition": "** Open washing, termed after “greenwashing”, refers to the act of claiming openness to secure perceptions of rigor or prestige associated with open practices. It has been used to characterise the marketing strategy of software companies that have the appearance of open-source and open-licensing, while engaging in proprietary practices. Open washing is a growing concern for those adopting open science practices as their actions are undermined by misleading uses of the practices, and actions designed to facilitate progressive developments are reduced to ‘ticking the box’ without clear quality control.",
                "Reference": "** Farrow (2017); Moretti (2020); Villum (2016); Vlaeminck and Podkrajac (2017)",
                "Originally drafted by": "** Meng Liu",
                "Reviewed (or Edited) by": "** Thomas Rhys Evans; Sam Guay; Sam Parsons; Charlotte R. Pennington; Beatrice Valentini",
                "Translated by": "Helena Hartmann",
                "Translation reviewed by": "Susanne Vogel, Jennifer Mattschey",
                "Translation": "Open Washing, benannt nach dem Ausdruck \"Greenwashing\", bezieht sich auf die Behauptung von Offenheit, um den Eindruck von Rigorosität (Sorgfalt) oder Prestige zu erwecken, die mit offenen Praktiken verbunden sind. Der Begriff wurde verwendet, um die Marketingstrategie von Softwareunternehmen zu charakterisieren, die den Anschein von Open-Source und Open-Licensing erwecken, während sie proprietäre Praktiken anwenden. Open Washing ist ein wachsendes Problem für diejenigen, die offene wissenschaftliche Praktiken anwenden, da ihre Maßnahmen durch eine irreführende Verwendung der Praktiken untergraben werden und Maßnahmen zur Erleichterung fortschrittlicher Entwicklungen auf ein \"Abhaken von Checklisten\" ohne klare Qualitätskontrolle reduziert werden.",
                "Related_terms": "** Open Access; Open Data; Open Source"
            },
            {
                "Title": "Optional Stopping (Optionales Stoppen) *",
                "Definition": "** The practice of (repeatedly) analyzing data during the data collection process and deciding to stop data collection if a statistical criterion (e.g. *p*\\-value, or bayes factor) reaches a specified threshold. If appropriate methodological precautions are taken to control the type 1 error rate, this can be an efficient analysis procedure (e.g. Lakens, 2014). However, without transparent reporting or appropriate error control the type 1 error can increase greatly and optional stopping could be considered a Questionable Research Practice (QRP) or a form of p-hacking.",
                "Reference(s)": "** Beffara Bret et al. (2021); Lakens (2014); Sagarin et al. (2014); Schönbrodt et al. (2017)",
                "Originally Drafted by": "** Brice Beffara Bret; Bettina M. J. Kern",
                "Reviewed (or Edited) by": "** Ali H. Al-Hoorie; Helena Hartmann; Catia M. Oliveira; Sam Parsons",
                "Translated by": "Susanne Vogel",
                "Translation reviewed by": "Helena Hartmann, Jennifer Mattschey",
                "Translation": "Die Praxis der (wiederholten) Analyse von Daten während der Datenerhebung und die Entscheidung, die Datenerhebung zu beenden, wenn ein statistisches Kriterium (z. B. der *p*\\-Wert oder der Bayes-Faktor) einen bestimmten Schwellenwert erreicht. Wenn geeignete methodische Vorkehrungen zur Kontrolle der Fehlerquote vom Typ 1 getroffen werden, kann dies ein effizientes Analyseverfahren sein (z. B. Lakens, 2014). Ohne eine transparente Berichterstattung oder eine angemessene Fehlerkontrolle kann der Typ-1-Fehler jedoch stark ansteigen, und Optionales Stoppen könnte als fragwürdige Forschungspraxis (questionable research practice; QRP) oder als eine Form des *p*\\-hacking betrachtet werden.",
                "Related_terms": "** *P*\\-hacking; Questionable Research Practices or Questionable Reporting Practices (QRPs); Sequential testing"
            },
            {
                "Title": "ORCID (Open Researcher and Contributor ID) *",
                "Definition": "** A organisation that provides a registry of persistent unique identifiers (ORCID iDs) for researchers and scholars, allowing these users to link their digital research documents and other contributions to their ORCID record. This avoids the name ambiguity problem in scholarly communication. ORCID iDs provide unique, persistent identifiers connecting researchers and their scholarly work. It is free to register for an ORCID iD at [https://orcid.org/register](https://orcid.org/register).",
                "Reference(s)": "** Haak et al. (2012); [https://orcid.org/](https://orcid.org/)",
                "Drafted by": "** Martin Vasilev",
                "Reviewed (or Edited) by": "** Bradley Baker; Mahmoud Elsherif; Shannon Francis; Charlotte R. Pennington; Emily A. Williams; Flávio Azevedo",
                "Translated by": "Helena Hartmann",
                "Translation reviewed by": "Susanne Vogel, Jennifer Mattschey  ### ---",
                "Translation": "Eine Organisation, die ein Register mit dauerhaften eindeutigen Identifikatoren (ORCID iDs) für Forscher:innen und Akademiker:innen bereitstellt, so dass diese Nutzer:innen ihre digitalen Forschungsdokumente und andere Beiträge mit ihrem ORCID-Eintrag verknüpfen können. Die Registrierung für eine ORCID iD ist kostenlos aufhttps://orcid.org/register.",
                "Related_terms": "** Authorship; DOI (digital object identifier); Name Ambiguity Problem"
            },
            {
                "Title": "Overlay Journal (Overlay Zeitschrift) *",
                "Definition": "** Open access electronic journals that collect and curate articles available from other sources (typically preprint servers, such as arXiv). Article curation may include (post-publication) peer review or editorial selection. Overlay journals do not publish novel material; rather, they organize and collate articles available in existing repositories.",
                "Reference": "** Ginsparg (1997, 2001); [https://discovery.ucl.ac.uk/id/eprint/19081/](https://discovery.ucl.ac.uk/id/eprint/19081/)",
                "Originally drafted by": "** Bradley Baker",
                "Reviewed (or Edited) by": "** Christopher Graham; Helena Hartmann; Sam Parsons; Charlotte R. Pennington",
                "Translated by": "Susanne Vogel",
                "Translation reviewed by": "Helena Hartmann, Jennifer Mattschey   ###  ### **P** {#p}",
                "Translation": "Elektronische Open-Access-Zeitschriften, die Artikel aus anderen Quellen (in der Regel Preprint-Server wie arXiv) sammeln und kuratieren. Die Kuratierung der Artikel kann eine (nach der Veröffentlichung erfolgende) Peer-Review-Begutachtung oder redaktionelle Auswahl beinhalten. Overlay-Zeitschriften veröffentlichen kein neues Material, sondern organisieren und sammeln Artikel, die in bestehenden Repositories verfügbar sind.",
                "Related_terms": "** Open access; Preprint"
            },
            {
                "Title": "P-curve (P-Kurve) *",
                "Definition": "** P-curve is a tool for identifying potential publication bias and makes use of the distribution of significant *p*\\-values in a series of independent findings. The deviation from the expected right-skewed distribution can be used to assess the existence and degree of publication bias: if the curve is right-skewed, there are more low, highly significant *p*\\-values, reflecting an underlying true effect. If the curve is left-skewed, there are many barely significant results just under the 0.05-threshold. This suggests that the studies lack evidential value and may be underpinned by questionable research practices (QRPs; e.g., *p*\\-hacking). In the case of no true effect present (true null hypothesis) and unbiased *p*\\-value reporting, the *p*\\-curve should be a flat, horizontal line, representing the typical distribution of *p*\\-values.",
                "Reference": "** Bruns and Ioannidis (2016); Simonsohn et al. (2014a); Simonsohn et al.(2014b); Simonsohn et al. (2019)",
                "Originally drafted by": "** Bettina M. J. Kern",
                "Reviewed (or Edited) by": "** Sam Guay; Kamil Izydorczak; Charlotte R. Pennington; Robert M. Ross; Olmo van den Akker",
                "Translated by": "Helena Hartmann",
                "Translation reviewed by": "Susanne Vogel, Jennifer Mattschey",
                "Translation": "Die P-Kurve ist ein Instrument zur Erkennung potenzieller Publikationsverzerrungen (bias) und nutzt die Verteilung signifikanter *p*\\-Werte in einer Reihe unabhängiger Befunde. Die Abweichung von der erwarteten rechtsschiefen Verteilung kann verwendet werden, um das Vorhandensein und das Ausmaß von Publikationsverzerrungen zu bewerten: Wenn die Kurve rechtsschief ist, gibt es mehr niedrige, hochsignifikante *p*\\-Werte, die einen zugrunde liegenden wahren Effekt widerspiegeln. Wenn die Kurve linksschief ist, gibt es viele kaum signifikante Ergebnisse knapp unter der .05-Schwelle. Dies deutet darauf hin, dass die Studien nicht aussagekräftig sind und möglicherweise durch fragwürdige Forschungspraktiken (questionable research practices, QRPs; z. B. *p*\\-hacking) untermauert werden. Wenn kein echter Effekt vorliegt (wahre Nullhypothese) und der p-Wert unverzerrt angegeben wird, sollte die P-Kurve eine flache, horizontale Linie sein, was die typische Verteilung der *p*\\-Werte darstellt.",
                "Related_terms": "** File-drawer; Hypothesis; *P*\\-hacking; *p*\\-value; Publication bias (File Drawer Problem); Questionable Research Practices or Questionable Reporting Practices (QRPs); Selective reporting; Z-curve"
            },
            {
                "Title": "*P*****-hacking *",
                "Definition": "** Exploiting techniques that may artificially increase the likelihood of obtaining a statistically significant result by meeting the standard statistical significance criterion (typically α \\= .05). For example, performing multiple analyses and reporting only those at *p* \\< .05, selectively removing data until *p* \\< .05, selecting variables for use in analyses based on whether those parameters are statistically significant.",
                "Reference(s)": "** Hardwicke et al. (2014); Neuroskeptic (2012)",
                "Drafted by": "** Mahmoud Elsherif",
                "Reviewed (or Edited) by": "** Tamara Kalandadze; William Ngiam; Sam Parsons; Martin Vasilev",
                "Translated by": "Susanne Vogel",
                "Translation reviewed by": "Helena Hartmann, Jennifer Mattschey",
                "Translation": "Nutzung von Techniken, die die Wahrscheinlichkeit eines statistisch signifikanten Ergebnisses künstlich erhöhen können, indem sie das statistische Standard-Signifikanzkriterium (in der Regel α \\= .05) erfüllen. Zum Beispiel die Durchführung mehrerer Analysen und die Angabe nur derjenigen mit *p* \\< .05, das selektive Entfernen von Daten bis *p* \\< .05, die Auswahl von Variablen für die Verwendung in Analysen auf der Grundlage, ob diese Parameter statistisch signifikant sind.",
                "Related_terms": "** Analytic flexibility; Fishing; Garden of forking paths; HARKing; Questionable Research Practices or Questionable Reporting Practices (QRPs); Selective reporting"
            },
            {
                "Title": "*P*****-value (*p*-Wert) *",
                "Definition": "** A statistic used to evaluate the outcome of a hypothesis test in Null Hypothesis Significance Testing (NHST). It refers to the probability of observing an effect, or more extreme effect, assuming the null hypothesis is true (Lakens, 2021b). The American Statistical Association’s statement on p-values (Wasserstein & Lazar, 2016\\) notes that p-values are not an indicator of the truth of the null hypothesis and instead defines p-values in this way: “Informally, a p-value is the probability under a specified statistical model that a statistical summary of the data (e.g., the sample mean difference between two compared groups) would be equal to or more extreme than its observed value” (p. 131).",
                "Reference(s)": "** [https://psyteachr.github.io/glossary/p.html](https://psyteachr.github.io/glossary/p.html); Lakens (2021); Wasserstein and Lazar (2016)",
                "Drafted by": "** Alaa AlDoh; Flávio Azevedo",
                "Reviewed (or Edited) by": "** Jamie P. Cockcroft; Charlotte R. Pennington; Suzanne L. K. Stewart; Robbie C.M. van Aert; Marcel A.L.M. van Assen; Martin Vasilev",
                "Translated by": "Susanne Vogel",
                "Translation reviewed by": "Helena Hartmann, Jennifer Mattschey",
                "Translation": "Eine Statistik, die zur Bewertung des Ergebnisses eines Hypothesentests bei der Nullhypothesen-Signifikanzprüfung (null hypothesis significance testing; NHST) verwendet wird. Sie bezieht sich auf die Wahrscheinlichkeit, einen Effekt oder einen extremeren Effekt zu beobachten, unter der Annahme, dass die Nullhypothese wahr ist (Lakens, 2021). In der Erklärung der American Statistical Association zu *p*\\-Werten (Wasserstein & Lazar, 2016\\) wird darauf hingewiesen, dass *p*\\-Werte kein Indikator für die Wahrheit der Nullhypothese sind, und stattdessen werden *p*\\-Werte wie folgt definiert: \"Informally, a *p*\\-value is the probability under a specified statistical model that a statistical summary of the data (e.g., the sample mean difference between two compared groups) would be equal to or more extreme than its observed value” (dt. Informell ist ein *p*\\-Wert die Wahrscheinlichkeit, dass eine statistische Zusammenfassung der Daten (z. B. die Stichprobenmittelwertdifferenz zwischen zwei verglichenen Gruppen) unter einem bestimmten statistischen Modell gleich oder extremer als der beobachtete Wert ist (S. 131).",
                "Related_terms": "** Null Hypothesis Statistical Testing (NHST); statistical significance"
            },
            {
                "Title": "Papermill (Papierfabrik) *",
                "Definition": "** An organization that is engaged in scientific misconduct wherein multiple papers are produced by falsifying or fabricating data, e.g. by editing figures or numerical data or plagiarizing written text. Papermills are “alleged to offer products ranging from research data through to ghostwritten fraudulent or fabricated manuscripts and submission services” (Byrne & Christopher, 2020, p. 583). A papermill relates to the fast production and dissemination of multiple allegedly new papers. These are often not detected in the scientific publishing process and therefore either never found or retracted if discovered (e.g. through plagiarism software).",
                "Reference(s)": "** Byrne and Christopher (2020); Hackett and Kelly (2020)",
                "Originally drafted by": "** Helena Hartmann",
                "Reviewed (or Edited) by": "** Sarah Ashcroft-Jones; Elizabeth Collins; Mahmoud Elsherif; Charlotte R. Pennington",
                "Translated by": "Helena Hartmann",
                "Translation reviewed by": "Susanne Vogel, Jennifer Mattschey",
                "Translation": "Eine Organisation, die wissenschaftliches Fehlverhalten begeht, indem sie mehrere Arbeiten durch Fälschung oder Fabrikation von Daten erstellt, z. B. durch Bearbeitung von Abbildungen oder numerischen Daten oder durch Plagiat von geschriebenem Text. Es wird behauptet, dass Papierfabriken (aus dem Engl. Paper Mills) Produkte anbieten, die von Forschungsdaten bis hin zu betrügerischen oder gefälschten Ghostwriting-Manuskripten und Einreichungsdiensten reichen (“alleged to offer products ranging from research data through to ghostwritten fraudulent or fabricated manuscripts and submission services”, Byrne & Christopher, 2020, S. 583). Bei einer Papierfabrik geht es um die schnelle Produktion und Verbreitung einer Vielzahl angeblich neuer Arbeiten. Diese werden im wissenschaftlichen Veröffentlichungsprozess oft nicht entdeckt und daher entweder nie gefunden oder zurückgezogen, wenn sie entdeckt werden (z. B. durch Plagiaterkennungssoftware).",
                "Related_terms": "** Data fabrication; Data falsification; Fraud; Plagiarism; Questionable Research Practices or Questionable Reporting Practices (QRPs); Scientific misconduct; Scientific publishing"
            },
            {
                "Title": "Paradata (Paradaten) *",
                "Definition": "** Data that are captured about the characteristics and context of primary data collected from an individual \\- distinct from metadata. Paradata can be used to investigate a respondent’s interaction with a survey or an experiment on a micro-level. They can be most easily collected during computer mediated surveys but are not limited to them. Examples include response times to survey questions, repeated patterns of responses such as choosing the same answer for all questions, contextual characteristics of the participant such as injuries that prevent good performance on tasks, the number of premature responses to stimuli in an experiment. Paradata have been used for the investigation and adjustment of measurement and sampling errors.",
                "Reference": "** Kreuter (2013)",
                "Originally drafted by": "** Alexander Hart; Graham Reid",
                "Reviewed (or Edited) by": "** Helena Hartmann; Charlotte R. Pennington; Marta Topor; Flávio Azevedo",
                "Translated by": "Helena Hartmann",
                "Translation reviewed by": "Susanne Vogel, Jennifer Mattschey",
                "Translation": "Daten, die über die Merkmale und den Kontext der von einer Person erhobenen Primärdaten erfasst werden \\- im Unterschied zu Metadaten. Paradaten können verwendet werden, um die Interaktion eines Befragten mit einer Umfrage oder einem Experiment auf einer Mikroebene zu untersuchen. Sie lassen sich am einfachsten bei computergestützten Umfragen erfassen, sind aber nicht darauf beschränkt. Beispiele hierfür sind die Antwortzeiten auf Umfrageelemente, wiederholte Antwortmuster wie die Wahl der gleichen Antwort für alle Fragen, kontextuelle Merkmale des Teilnehmers wie Verletzungen, die eine gute Leistung bei Aufgaben verhindern, die Anzahl der vorzeitigen Reaktionen auf Stimuli in einem Experiment. Paradaten wurden für die Untersuchung und Korrektur von Mess- und Stichprobenfehlern verwendet.",
                "Related_terms": "** Auxiliary data; Data collection; Data quality; Metadata; Process information"
            },
            {
                "Title": "PARKing *",
                "Definition": "** PARKing (preregistering after results are known) is defined as the practice where researchers complete an experiment (possibly with infinite re-experimentation) before preregistering. This practice invalidates the purpose of preregistration, and is one of the QRPs (or, even scientific misconduct) that try to gain only \"credibility that it has been preregistered.\"",
                "Reference": "** [Ikeda et al. (2019)](https://www.jstage.jst.go.jp/article/sjpr/62/3/62_281/_pdf/-char/ja); [Yamada (2018)](https://www.frontiersin.org/articles/10.3389/fpsyg.2018.01831/full)",
                "Originally drafted by": "** Qinyu Xiao",
                "Reviewed (or Edited) by": "Helena Hartmann; Sam Parsons; Yuki Yamada",
                "Translated by": "Susanne Vogel",
                "Translation reviewed by": "Helena Hartmann, Jennifer Mattschey",
                "Translation": "PARKing (preregistering after results are known; dt. Präregistrieren nach Bekanntwerden der Ergebnisse) wird definiert als die Praxis, bei der Forschende ein Experiment (möglicherweise mit unendlichen Wiederholungsexperimenten) abschließen, bevor sie es präregistrieren. Diese Praxis macht den Zweck der Präregistrierung zunichte und ist eine der QRPs (questionable research practices; oder sogar wissenschaftliches Fehlverhalten), die nur versuchen, \"credibility that it has been preregistered\" (dt. die Glaubwürdigkeit, dass es präregistriert wurde\" zu erlangen.",
                "Related_terms": "** CARKing; HARKing; Preregistration; Questionable Research Practices or Questionable Reporting Practices (QRPs); SPARKing"
            },
            {
                "Title": "Participatory Research (Partizipative Forschung) *",
                "Definition": "** Participatory research refers to incorporating the views of people from relevant communities in the entire research process to achieve shared goals between researchers and the communities. This approach takes a collaborative stance that seeks to reduce the power imbalance between the researcher and those researched through a “systematic cocreation of new knowledge” (Andersson, 2018).",
                "Reference": "** Cornwall and Jewkes (1995); Fletcher-Watson et al. (2019); Kiernan (1999); Leavy (2017); Ottmann et al. (2011); Rose (2018)",
                "Originally drafted by": "** Tamara Kalandadze",
                "Reviewed (or Edited) by": "** Jamie P. Cockcroft; Bethan Iley; Halil E. Kocalar; Michele C. Lim",
                "Translated by": "Helena Hartmann",
                "Translation reviewed by": "Susanne Vogel, Jennifer Mattschey",
                "Translation": "Partizipative Forschung bedeutet, die Ansichten von Menschen aus relevanten Gemeinschaften in den gesamten Forschungsprozess einzubeziehen, um gemeinsame Ziele zwischen Forscher:innen und diesen Gemeinschaften zu erreichen. Dieser Ansatz nimmt eine kollaborative Haltung ein, die darauf abzielt, das Machtungleichgewicht zwischen dem Forschenden und den Erforschten durch eine systematische Miterschaffung von neuem Wissen zu verringern (“systematic cocreation of new knowledge”, Andersson, 2018).",
                "Related_terms": "** Collaborative research; Inclusion; Neurodiversity; Patient and Public Involvement (PPI); Transformative paradigm"
            },
            {
                "Title": "Patient and Public Involvement (PPI, Einbindung von Patient:innen und Öffentlichkeit) *",
                "Definition": "** Active research collaboration with the population of interest, as opposed to conducting research “about” them. Researchers can incorporate the lived experience and expertise of patients and the public at all stages of the research process. For example, patients can help to develop a set of research questions, review the suitability of a study design, approve plain English summaries for grant/ethics applications and dissemination, collect and analyse data, and assist with writing up a project for publication. This is becoming highly recommended and even required by funders (Boivin et al., 2018).",
                "Reference": "** Boivin et al. (2018); [https://www.invo.org.uk/](https://www.invo.org.uk/)",
                "Originally drafted by": "** Jade Pickering",
                "Reviewed (or Edited) by": "** Mahmoud Elsherif; Sam Parsons; Catia M. Oliveira",
                "Translated by": "Susanne Vogel",
                "Translation reviewed by": "Helena Hartmann, Jennifer Mattschey",
                "Translation": "Aktive Forschungszusammenarbeit mit der interessierenden Population, im Gegensatz zur Forschung \"über\" sie. Forschende können die Lebenserfahrung und das Fachwissen von Patient:innen und der Öffentlichkeit in allen Phasen des Forschungsprozesses einbeziehen. So können Patient:innen beispielsweise bei der Entwicklung von Forschungsfragen helfen, die Eignung eines Studiendesigns überprüfen, Zusammenfassungen in einfacher Sprache für Drittmittel-/Ethikanträge und die Verbreitung genehmigen, Daten erheben und analysieren und bei der Veröffentlichung eines Projekts helfen. Dies wird zunehmend empfohlen und sogar von Drittmittelgebern gefordert (Boivin et al., 2018).",
                "Related_terms": "** Co-production; Participatory research"
            },
            {
                "Title": "Paywall (Bezahlschranke) *",
                "Definition": "** A technological barrier that permits access to information only to individuals who have paid \\- either personally, or via an organisation \\- a designated fee or subscription.",
                "Reference": "** Day et al. (2020); [https://casrai.org/term/closed-access/](https://casrai.org/term/closed-access/);",
                "Originally drafted by": "** Bradley Baker",
                "Reviewed (or Edited) by": "** Mahmoud Elsherif; Sam Parsons; Charlotte R. Pennington; Julia Wolska",
                "Translated by": "Helena Hartmann",
                "Translation reviewed by": "Susanne Vogel, Jennifer Mattschey",
                "Translation": "Eine technische Hürde, die den Zugang zu Informationen nur für Personen ermöglicht, die \\- entweder persönlich oder über eine Organisation \\- eine bestimmte Gebühr oder ein Abonnement bezahlt haben.",
                "Related_terms": "** Accessibility; Open Access"
            },
            {
                "Title": "PCI (Peer Community In) *",
                "Definition": "** PCI is a non-profit organisation that creates communities of researchers who review and recommend unpublished preprints based upon high-quality peer review from at least two researchers in their field. These preprints are then assigned a DOI, similarly to a journal article. PCI was developed to establish a free, transparent and public scientific publication system based on the review and recommendation of preprints.",
                "Reference(s)": "** https://peercommunityin.org/",
                "Originally drafted by": "** Emma Henderson",
                "Reviewed (or Edited) by": "** Jamie P. Cockcroft; Christopher Graham; Bethan Iley; Aleksandra Lazić; Charlotte R. Pennington",
                "Translated by": "Helena Hartmann",
                "Translation reviewed by": "Susanne Vogel, Jennifer Mattschey",
                "Translation": "PCI ist eine gemeinnützige Organisation, die Gemeinschaften von Forschenden schafft, die unveröffentlichte Preprints auf der Grundlage eines hochwertigen Peer-Reviews von mindestens zwei Forschenden aus ihrem Fachgebiet überprüfen und empfehlen. Diesen Preprints wird dann ein DOI zugewiesen, ähnlich wie bei einem Zeitschriftenartikel. PCI wurde entwickelt, um ein kostenloses, transparentes und öffentliches wissenschaftliches Publikationssystem zu schaffen, das auf der Überprüfung und Empfehlung von Preprints beruht.",
                "Related_terms": "** Open Access; Open Archives; Open Peer Review; PCI Registered Reports; Peer review; Preprints"
            },
            {
                "Title": "PCI Registered Reports *",
                "Definition": "** An initiative launched in 2021 dedicated to receiving, reviewing, and recommending Registered Reports (RRs) across the full spectrum of Science, technology, engineering, and mathematics (STEM), medicine, social sciences and humanities. Peer Community In (PCI) RRs are overseen by a ‘Recommender’ (equivalent to an Action Editor) and reviewed by at least two experts in the relevant field. It provides free and transparent pre- (Stage 1\\) and post-study (Stage 2\\) reviews across research fields. A network of PCI RR-friendly journals endorse the PCI RR review criteria and commit to accepting, without further peer review, RRs that receive a positive final recommendation from PCI RR.",
                "Reference(s)": "** [https://rr.peercommunityin.org/about/about](https://rr.peercommunityin.org/about/about)",
                "Originally drafted by": "** Charlotte R. Pennington",
                "Reviewed (or Edited) by": "** Sarah Ashcroft-Jones; Jamie P. Cockcroft; Mahmoud Elsherif; Helena Hartmann",
                "Translated by": "Helena Hartmann",
                "Translation reviewed by": "Jennifer Mattschey, Susanne Vogel",
                "Translation": "Eine 2021 ins Leben gerufene Initiative, die sich der Aufnahme, Überprüfung und Empfehlung von Registered Reports (RRs) aus dem gesamten Spektrum der Naturwissenschaften, Technik, Ingenieurwissenschaften und Mathematik (MINT), Medizin, Sozial- und Geisteswissenschaften widmet. Peer Community In (PCI) RRs werden von einem \"Recommender\" (dt. Empfehlende:r, entspricht einem Action Editor) betreut und von mindestens zwei Expert:innen auf dem jeweiligen Gebiet begutachtet. Es bietet kostenlose und transparente Begutachtungen vor (Stufe 1\\) und nach der Studiendurchführung (Stufe 2\\) über alle Forschungsbereiche hinweg. Ein Netzwerk von PCI RR-freundlichen Zeitschriften unterstützt die PCI RR-Begutachtungskriterien und verpflichtet sich, RRs, die eine positive abschließende Empfehlung von PCI RR erhalten, ohne weitere Begutachtung zu akzeptieren.",
                "Related_terms": "** In Principle Acceptance (IPA); Open Access; PCI (Peer Community In); Publication bias (File Drawer Problem); Registered Report; Results blind; Stage 1 study review; Stage 2 study review; Transparency"
            },
            {
                "Title": "Plan S *",
                "Definition": "** Plan S is an initiative, launched in September 2018 by cOAlition S, a consortium of research funding organisations, which aims to accelerate the transition to full and immediate Open Access. Participating funders require recipients of research grants to publish their research in compliant Open Access journals or platforms, or make their work openly and immediately available in an Open Access repository, from 2021 onwards. cOAlition S funders have commited to not financially support ‘hybrid’ Open Access publication fees in subscription venues. However, authors can comply with plan S through publishing Open Access in a subscription journal under a “transformative arrangement” as further described in the implementation guidance. The “S” in Plan S stands for shock.",
                "Reference": "** [https://www.coalition-s.org](https://www.coalition-s.org/)",
                "Originally drafted by": "** Olmo van den Akker",
                "Reviewed (or Edited) by": "** Jamie P. Cockcroft; Helena Hartmann; Halil E. Kocalar; Birgit Schmidt",
                "Translated by": "Helena Hartmann",
                "Translation reviewed by": "Jennifer Mattschey, Susanne Vogel",
                "Translation": "Plan S ist eine im September 2018 von cOAlition S, einem Konsortium von drittmittelgebenden Organisationen, gestartete Initiative, die den Übergang zu vollständigem und sofortigem Open Access beschleunigen soll. Die teilnehmenden Förderorganisationen verlangen von den Empfänger:innen von Forschungsgeldern, dass sie ab 2021 in Open-Access-konformen Zeitschriften publizieren oder ihre Arbeiten sofort offen zugänglich machen. Die Förderer von cOAlition S haben sich verpflichtet, keine \"hybriden\" Open-Access-Publikationsgebühren in auf Abonnementen basierenden Publikationsplattformen finanziell zu unterstützen. Autor:innen können jedoch Plan S einhalten, indem sie Open Access in einer zu abonnierenden Zeitschrift im Rahmen einer \"transformativen Vereinbarung\" veröffentlichen, wie in den Durchführungsleitlinien näher beschrieben. Das \"S\" in Plan S steht für Schock.",
                "Related_terms": "** Open Access; DORA; Repository"
            },
            {
                "Title": "Positionality (Positionalität) *",
                "Definition": "** The contextualization of both the research environment and the researcher, to define the boundaries within the research was produced (Jaraf, 2018). Positionality is typically centred and celebrated in qualitative research, but there have been recent calls for it to also be used in quantitative research as well. Positionality statements, whereby a researcher outlines their background and ‘position’ within and towards the research, have been suggested as one method of recognising and centring researcher bias.",
                "Reference(s)": "** Jafar (2018); Oxford Dictionaries (2017)",
                "Originally drafted by": "** Joanne McCuaig",
                "Reviewed (or Edited) by": "** Helena Hartmann; Aoife O’Mahony; Madeleine Pownall; Graham Reid",
                "Translated by": "Susanne Vogel",
                "Translation reviewed by": "Jennifer Mattschey, Helena Hartmann",
                "Translation": "Die Kontextualisierung sowohl der Forschungsumgebung als auch der Forschenden, um zu definieren, innerhalb welcher Grenzen die Forschungsarbeit durchgeführt wurde (Jaraf, 2018). Positionalität wird in der Regel in der qualitativen Forschung in den Mittelpunkt gestellt und gewürdigt, aber in jüngster Zeit wurde gefordert, sie auch in der quantitativen Forschung zu verwenden. Positionalitätserklärungen, in denen Forschende ihren Hintergrund und ihre \"Position\" innerhalb und gegenüber der Forschung darlegen, wurden als eine Methode zur Erkennung und Zentrierung des Bias (Voreingenommenheit) von Forschenden vorgeschlagen.",
                "Related_terms": "** Bias; Reflexivity; Perspective"
            },
            {
                "Title": "Positionality Map (Positionalitätskarte) *",
                "Definition": "** A reflexive tool for practicing explicit positionality in critical qualitative research. The map is to be used “as a flexible starting point to guide researchers to reflect and be reflexive about their social location. The map involves three tiers: the identification of social identities (Tier 1), how these positions impact our life (Tier 2), and details that may be tied to the particularities of our social identity (Tier 3).” (Jacobson and Mustafa 2019, p. 1). The aim of the map is “for researchers to be able to better identify and understand their social locations and how they may pose challenges and aspects of ease within the qualitative research process.”",
                "Reference": "** Jacobson and Mustafa (2019)",
                "Originally drafted by": "** Joanne McCuaig",
                "Reviewed (or Edited) by": "** Helena Hartmann; Michele C. Lim; Charlotte R. Pennington; Graham Reid",
                "Translated by": "Helena Hartmann",
                "Translation reviewed by": "Jennifer Mattschey, Susanne Vogel",
                "Translation": "Ein reflexives Instrument zur Anwendung von expliziter Positionalität in der qualitativ-kritischen Forschung. Die Karte soll als flexibler Ausgangspunkt verwendet werden, um Forscher:innen anzuleiten, über ihren sozialen Standort nachzudenken und ihn zu reflektieren. Die Karte umfasst drei Ebenen: die Identifizierung sozialer Identitäten (Ebene 1), wie diese Positionen unser Leben beeinflussen (Ebene 2\\) und Details, die mit den Besonderheiten unserer sozialen Identität verbunden sein können (Ebene 3). (aus dem Engl. “as a flexible starting point to guide researchers to reflect and be reflexive about their social location. The map involves three tiers: the identification of social identities (Tier 1), how these positions impact our life (Tier 2), and details that may be tied to the particularities of our social identity (Tier 3).”; Jacobson und Mustafa 2019, S. 1). Das Ziel der Karte ist es, dass Forschende ihre sozialen Positionen besser identifizieren und verstehen können, wie diese Herausforderungen und Aspekte der Einfachheit innerhalb des qualitativen Forschungsprozesses darstellen können (aus dem Engl. “for researchers to be able to better identify and understand their social locations and how they may pose challenges and aspects of ease within the qualitative research process.”)",
                "Related_terms": "** Positionality; Qualitative research; Social identity map; Transparency"
            },
            {
                "Title": "Post Hoc (Im Nachhinein) *",
                "Definition": "** Post hoc is borrowed from Latin, meaning “after this”. In statistics, post hoc (or post hoc analysis) refers to the testing of hypotheses not specified prior to data analysis. In frequentist statistics, the procedure differs based on whether the analysis was planned or post-hoc, for example by applying more stringent error control. In contrast, Bayesian and likelihood approaches do not differ as a function of when the hypothesis was specified.",
                "Reference(s)": "** Dienes (p.166, 2008\\)",
                "Drafted by": "** Alaa Aldoh",
                "Reviewed (or Edited) by": "** Sam Parsons; Jamie P. Cockcroft; Bethan Iley; Halil E. Kocalar; Graham Reid; Flávio Azevedo",
                "Translated by": "Susanne Vogel",
                "Translation reviewed by": "Helena Hartmann, Jennifer Mattschey",
                "Translation": "Post hoc kommt aus dem Lateinischen und bedeutet \"im Nachhinein\". In der Statistik bezieht sich post hoc (oder Post-hoc-Analyse) auf die Prüfung von Hypothesen, die nicht vor der Datenanalyse festgelegt wurden. In der frequentistischen Statistik unterscheidet sich das Verfahren je nachdem, ob die Analyse geplant oder post hoc durchgeführt wurde, z. B. durch Anwendung einer strengeren Fehlerkontrolle. Im Gegensatz dazu unterscheiden sich Bayes'sche und Likelihood-Ansätze nicht in Abhängigkeit davon, wann die Hypothese festgelegt wurde.",
                "Related_terms": "** A priori, Ad hoc; HARKing; P-hacking"
            },
            {
                "Title": "Post Publication Peer Review (Peer-Review nach der Veröffentlichung) *",
                "Definition": "** Peer review that takes place after research has been published. It is typically posted on a dedicated platform (e.g., PubPeer). It is distinct from the traditional commentary which is published in the same journal and which is itself usually peer reviewed.",
                "Drafted by": "** Ali H. Al-Hoorie",
                "Reviewed (or Edited) by": "** Mahmoud Elsherif; Sam Parsons",
                "Translated by": "Helena Hartmann",
                "Translation reviewed by": "Susanne Vogel, Jennifer Mattschey",
                "Translation": "Peer-Review, das nach der Veröffentlichung einer Forschungsarbeit stattfindet. Es wird in der Regel auf einer speziellen Plattform (z. B. PubPeer) veröffentlicht. Es unterscheidet sich vom traditionellen Kommentar, der in der gleichen Zeitschrift veröffentlicht wird und in der Regel selbst einem Peer Review unterzogen wird.",
                "Related_terms": "** Open Peer Review; PeerPub; Peer review"
            },
            {
                "Title": "Posterior distribution *",
                "Definition": "** A way to summarize one’s updated knowledge in Bayesian inference, balancing prior knowledge with observed data. In statistical terms, posterior distributions are proportional to the product of the likelihood function and the prior. A posterior probability distribution captures (un)certainty about a given parameter value.",
                "Reference(s)": "** Dienes (2014); Lüdtke et al. (2020); van de Schoot et al. (2021)",
                "Drafted by": "** Alaa AlDoh",
                "Reviewed (or Edited) by": "** Adam Parker; Jamie P. Cockcroft; Julia Wolska; Yu-Fang Yang; Charlotte R. Pennington",
                "Translated by": "Susanne Vogel",
                "Translation reviewed by": "Helena Hartmann, Jennifer Mattschey",
                "Translation": "Eine Möglichkeit, das aktualisierte Wissen bei der Bayes'schen Inferenz zusammenzufassen, wobei das vorherige Wissen (prior knowledge) mit den beobachteten Daten abgeglichen wird. In der Statistik sind Posterior-Verteilungen proportional zum Produkt aus der Wahrscheinlichkeitsfunktion und dem Prior. Eine Posterior-Wahrscheinlichkeitsverteilung gibt die (Un-)Gewissheit über einen bestimmten Parameterwert an.",
                "Related_terms": "** Bayes Factor; Bayesian inference; Bayesian parameter estimation; Likelihood function; Prior distribution"
            },
            {
                "Title": "Predatory Publishing (Predatory Verlagswesen) *",
                "Definition": "** Predatory (sometimes “vanity”) publishing describes a range of business practices in which publishers seek to profit, primarily by collecting article processing charges (APCs), from publishing scientific works without necessarily providing legitimate quality checks (e.g., peer review) or editorial services. In its most extreme form, predatory publishers will publish any work, so long as charges are paid. Other less extreme strategies, such as sending out high numbers of unsolicited requests for editing or publishing in fee-driven special issues, have also been accused as predatory (Crosetto, 2021).",
                "Reference": "** Crosetto (2021); Xia et al. (2015)",
                "Originally drafted by": "** Nick Ballou",
                "Reviewed (or Edited) by": "** Olmo van den Akker; Helena Hartmann; Aleksandra Lazić; Graham Reid; Flávio Azevedo",
                "Translated by": "Helena Hartmann",
                "Translation reviewed by": "Susanne Vogel, Jennifer Mattschey",
                "Translation": "Unter Predatory (manchmal auch \"Vanity\") Veröffentlichung versteht man eine Reihe von Geschäftspraktiken, bei denen Verlage versuchen, in erster Linie durch die Erhebung von Bearbeitungsgebühren für Artikel (article processing charges, APCs) von der Veröffentlichung wissenschaftlicher Arbeiten zu profitieren, ohne notwendigerweise legitime Qualitätskontrollen (z. B. Peer Review) oder redaktionelle Dienstleistungen anzubieten. In der extremsten Form veröffentlichen solche Verlage jede Arbeit, solange die Gebühren bezahlt werden. Andere, weniger extreme Strategien, wie das Versenden einer großen Anzahl von unaufgeforderten Anfragen zum (Mit-)Herausgeben oder Veröffentlichen in kostenpflichtigen Sonderausgaben werden ebenfalls als \"predatory\" (dt. räuberisch) bezeichnet (Crosetto, 2021).",
                "Related_terms": "** Article Processing Charge (APC); Gaming (the system)"
            },
            {
                "Title": "PREPARE Guidelines (PREPARE Leitlinien) *",
                "Definition": "** The PREPARE guidelines and checklist (Planning Research and Experimental Procedures on Animals: Recommendations for Excellence) aim to help the planning of animal research, and support adherence to the 3Rs (Replacement, Reduction or Refinement) and facilitate the reproducibility of animal research.",
                "Reference(s)": "** Smith et al. (2018)",
                "Drafted by": "** Ben Farrar",
                "Reviewed (or Edited) by": "** Mahmoud Elsherif; Gilad Feldman; Elias Garcia-Pelegrin",
                "Translated by": "Susanne Vogel",
                "Translation reviewed by": "Helena Hartmann, Jennifer Mattschey",
                "Translation": "Die PREPARE-Leitlinien und die Checkliste (Planning Research and Experimental Procedures on Animals: Recommendations for Excellence) sollen bei der Planung von Tierversuchen helfen, die Einhaltung der sogenannten 3Rs (Replacement, Reduction or Refinement, dt. Ersetzen, Reduzieren und Verfeinern) zu unterstützen und die Reproduzierbarkeit von Tierversuchen zu erleichtern.",
                "Related_terms": "** ARRIVE Guidelines; Reporting Guideline; STRANGE"
            },
            {
                "Title": "Preprint (Vorabdruck) *",
                "Definition": "** A publicly available version of any type of scientific manuscript/research output preceding formal publication, considered a form of Green Open Access. Preprints are usually hosted on a repository (e.g. arXiv) that facilitates dissemination by sharing research results more quickly than through traditional publication. Preprint repositories typically provide persistent identifiers (e.g. DOIs) to preprints. Preprints can be published at any point during the research cycle, but are most commonly published upon submission (i.e., before peer-review). Accepted and peer-reviewed versions of articles are also often uploaded to preprint servers, and are called postprints.",
                "Reference(s)": "** Bourne et al. (2017); Elmore (2018)",
                "Drafted by": "** Mariella Paul",
                "Reviewed (or Edited) by": "** Gisela H. Govaart; Helena Hartmann; Sam Parsons; Tobias Wingen; Flávio Azevedo",
                "Translated by": "Helena Hartmann",
                "Translation reviewed by": "Susanne Vogel, Jennifer Mattschey",
                "Translation": "Eine öffentlich zugängliche Version eines wissenschaftlichen Manuskripts/Forschungsergebnisses, die der offiziellen Veröffentlichung vorausgeht und als eine Form des Grünen Open Access gilt. Preprints werden in der Regel auf einem Repository (z. B. arXiv) bereitgestellt, das die Verbreitung von Forschungsergebnissen durch eine schnellere Weitergabe als bei einer herkömmlichen Veröffentlichung erleichtert. Preprint-Repositorien vergeben in der Regel dauerhafte Identifikatoren (z. B. DOIs) für Preprints. Preprints können zu jedem Zeitpunkt des Forschungszyklus veröffentlicht werden, werden aber meist bei der Einreichung (d. h. vor der Begutachtung durch Fachkollegen über Peer Review) veröffentlicht. Angenommene und begutachtete Versionen von Artikeln werden ebenfalls häufig auf Preprintserver hochgeladen und als Postprints bezeichnet.",
                "Related_terms": "** Open Access; DOI (digital object identifier); Postprint; Working Paper"
            },
            {
                "Title": "Preregistration (Präregistrierung) *",
                "Definition": "** The practice of publishing the plan for a study, including research questions/hypotheses, research design, data analysis before the data has been collected or examined. It is also possible to preregister secondary data analyses (Merten & Krypotos, 2019). A preregistration document is time-stamped and typically registered with an independent party (e.g., a repository) so that it can be publicly shared with others (possibly after an embargo period). Preregistration provides a transparent documentation of what was planned at a certain time point, and allows third parties to assess what changes may have occurred afterwards. The more detailed a preregistration is, the better third parties can assess these changes and with that the validity of the performed analyses. Preregistration aims to clearly distinguish confirmatory from exploratory research.",
                "Reference(s)": "** Haven and van Grootel (2019); Lewandowsky and Bishop (2016); Merten and Krypotos (2019); Navarro (2020); Nosek et al. (2018); Simmons et al. (2021)",
                "Drafted by": "** Mahmoud Elsherif",
                "Reviewed (or Edited) by": "** Gisela H. Govaart; Helena Hartmann; Tina Lonsdorf; William Ngiam; Eike Mark Rinke; Lisa Spitzer; Olmo van den Akker; Flávio Azevedo",
                "Translated by": "Susanne Vogel",
                "Translation reviewed by": "Helena Hartmann, Jennifer Mattschey",
                "Translation": "Die Praxis der Veröffentlichung des Plans für eine Studie, einschließlich der Forschungsfragen/Hypothesen, des Forschungsdesigns und der Datenanalyse, bevor die Daten erhoben oder untersucht wurden. Es ist auch möglich, Sekundärdatenanalysen zu präregistrieren (Merten & Krypotos, 2019). Eine Präregistrierung wird mit einem Zeitstempel versehen und in der Regel bei einer unabhängigen Partei (z. B. einem Repositorium) registriert, so dass es öffentlich mit anderen geteilt werden kann (möglicherweise nach einer Sperrfrist). Die Präregistrierung bietet eine transparente Dokumentation dessen, was zu einem bestimmten Zeitpunkt geplant war, und ermöglicht es Dritten, zu beurteilen, welche Änderungen sich im Nachhinein ergeben haben könnten. Je detaillierter eine Präregistrierung ist, desto besser können Dritte diese Änderungen und damit auch die Validität der durchgeführten Analysen beurteilen. Die Präregistrierung zielt darauf ab, konfirmatorische von explorativer Forschung klar zu unterscheiden.",
                "Related_terms": "** Confirmation bias; Confirmatory analyses; Exploratory Data Analysis; HARKing; Pre-analysis plan; Questionable Research Practices or Questionable Reporting Practices (QRPs); Registered Report; Research Protocol; Transparency"
            },
            {
                "Title": "Preregistration Pledge (Präregistrierungs-Versprechen) *",
                "Definition": "** In a “collective action in support of open and reproducible research practices'', the preregistration pledge is a campaign from the Project Free Our Knowledge that asks a researcher to commit to preregistering at least one study in the next two years ([https://freeourknowledge.org/about/](https://freeourknowledge.org/about/)). The project is a grassroots movement initiated by early career researchers (ECRs).",
                "Reference(s)": "** https://freeourknowledge.org/2020-12-03-preregistration-pledge/",
                "Originally drafted by": "** Helena Hartmann",
                "Reviewed (or Edited) by": "** Jamie P. Cockcroft; Mahmoud Elsherif; Aleksandra Lazić, Steven Verheyen",
                "Translated by": "Helena Hartmann",
                "Translation reviewed by": "Susanne Vogelt, Jennifer Mattschey",
                "Translation": "In einer \"kollektiven Aktion zur Unterstützung offener und reproduzierbarer Forschungspraktiken\" (aus dem Engl. “collective action in support of open and reproducible research practices'') ist der Preregistration Pledge eine Kampagne des Projekts Free Our Knowledge, die Forscher:innen auffordert, sich zu verpflichten, in den nächsten zwei Jahren mindestens eine Studie zu präregistrieren ([https://freeourknowledge.org/about/](https://freeourknowledge.org/about/)). Das Projekt ist eine Bewegung, die von Nachwuchsforscher:innen (early career researchers, ECRs) initiiert wurde.",
                "Related_terms": "** Preregistration"
            },
            {
                "Title": "PRO (peer review openness) initiative (PRO Initiative)*",
                "Definition": "** The agreement made by several academics that they will not provide a peer review of a manuscript unless certain conditions are met. Specifically, the manuscript authors should ensure the data and materials will be made publically available (or give a justification as to why they are not freely available or shared), provide documentation detailing how to interpret and run any files or code and detail where these files can be located via the manuscript itself.",
                "Reference": "** Morey et al. (2016)",
                "Originally drafted by": "** Jamie P. Cockcroft",
                "Reviewed (or Edited) by": "** Sarah Ashcroft-Jones; Mahmoud Elsherif; Helena Hartmann; Steven Verheyen",
                "Translated by": "Susanne Vogel",
                "Translation reviewed by": "Helena Hartmann, Jennifer Mattschey",
                "Translation": "Die Vereinbarung mehrerer Forschender, dass sie ein Manuskript nur dann begutachten, wenn bestimmte Bedingungen erfüllt sind. Insbesondere sollten die Autor:innen des Manuskripts sicherstellen, dass die Daten und Materialien öffentlich zugänglich gemacht werden (oder begründen, warum sie nicht frei verfügbar sind oder weitergegeben werden), eine Dokumentation bereitstellen, in der detailliert beschrieben wird, wie Dateien oder Code zu interpretieren und auszuführen sind, und im Manuskript angeben, wo diese Dateien zu finden sind.",
                "Related_terms": "** Non-anonymised peer review; Open Science; Open Peer Review; Transparent peer review"
            },
            {
                "Title": "Prior distribution (a-priori Verteilung) *",
                "Definition": "** Beliefs held by researchers about the parameters in a statistical model before further evidence is taken into account. A ‘prior’ is expressed as a probability distribution and can be determined in a number of ways (e.g., previous research, subjective assessment, principles such as maximising entropy given constraints), and is typically combined with the likelihood function using Bayes’ theorem to obtain a posterior distribution.",
                "Reference(s)": "** van de Schoot et al. (2021)",
                "Drafted by": "** Alaa AlDoh",
                "Reviewed (or Edited) by": "** Charlotte R. Pennington; Martin Vasilev",
                "Translated by": "Helena Hartmann",
                "Translation reviewed by": "Susanne Vogel, Jennifer Mattschey",
                "Translation": "Überzeugungen, die Forscher:innen über die Parameter eines statistischen Modells haben, bevor weitere Erkenntnisse berücksichtigt werden. Ein \"Prior\" wird als Wahrscheinlichkeitsverteilung ausgedrückt und kann auf verschiedene Weise bestimmt werden (z. B. frühere Forschung, subjektive Einschätzung, Grundsätze wie die Maximierung der Entropie bei gegebenen Einschränkungen) und wird in der Regel mit der Likelihood Funktion unter Verwendung des Satzes von Bayes kombiniert, um eine Posterior-Verteilung zu erhalten.",
                "Related_terms": "** Bayes Factor; Bayesian inference; Bayesian Parameter Estimation; Likelihood function; Posterior distribution"
            },
            {
                "Title": "Pseudonymisation (Pseudonymisierung) *",
                "Definition": "** Pseudonymisation refers to a technique that involves replacing or removing any information that could lead to identification of research subjects’ identity whilst still being able to make them identifiable through the use of the combination of code number and identifiers. This process comprises the following steps: removal of all identifiers from the research dataset; attribution of a specific identifier (pseudonym) for each participant and using it to label each research record; and maintenance of a cipher that links the code number to the participant in a document physically separate from the dataset. Pseudonymisation is typically a minimum requirement from ethical committees when conducting research, especially on human participants or involving confidential information, in order to ensure upholding of data privacy.",
                "Reference": "** Mourby et al. (2018); UKRI ([https://mrc.ukri.org/documents/pdf/gdpr-guidance-note-5-identifiability-anonymisation-and-pseudonymisation/](https://mrc.ukri.org/documents/pdf/gdpr-guidance-note-5-identifiability-anonymisation-and-pseudonymisation/))",
                "Originally drafted by": "** Catia M. Oliveira",
                "Reviewed (or Edited) by": "** Helena Hartmann; Sam Parsons; Charlotte R. Pennington; Birgit Schmidt",
                "Translated by": "Helena Hartmann",
                "Translation reviewed by": "Susanne Vogel, Jennifer Mattschey",
                "Translation": "Pseudonymisierung bezieht sich auf eine Technik, bei der alle Informationen, die zur Identifizierung von Versuchspersonen führen könnten, ersetzt oder entfernt werden, während sie durch die Kombination von Codenummer und Identifikatoren weiterhin identifizierbar bleiben. Dieser Prozess umfasst die folgenden Schritte: Entfernung aller Identifikatoren aus dem Forschungsdatensatz; Zuweisung eines spezifischen Identifikators (Pseudonym) für jede Versuchsperson und Verwendung dieses Pseudonyms zur Kennzeichnung jeweiligen Datensatzes; und Beibehaltung einer Verschlüsselung (z. B. einer Kodierliste), die die Codenummer mit der Identität der Versuchsperson in einem physisch vom Datensatz getrennten Dokument verknüpft. Die Pseudonymisierung ist in der Regel eine Mindestanforderung der Ethikkommissionen bei der Durchführung von Forschungsarbeiten, insbesondere bei menschlichen Teilnehmenden oder bei vertraulichen Informationen, um den Datenschutz zu gewährleisten.",
                "Related_terms": "** Anonymity; Confidentiality; Data privacy; De-identification; Pseudonymisation; Research ethics"
            },
            {
                "Title": "Pseudoreplication (Pseudoreplikation) *",
                "Definition": "** When there is a lack of statistical independence presented in the data and thus artificially inflating the number of samples (i.e. replicates). For instance, collecting more than one data point from the same experimental unit (e.g. participant or crops). Numerous methods can overcome this, such as averaging across replicates (e.g., taking the mean RT for a participant) or implementing mixed effects models with the random effects structure accounting for the pseudoreplication (e.g., specifying each individual RT as belonging to the same subject). Note, the former option would be associated with a loss of information and statistical power.",
                "Reference(s)": "** Davies and Gray (2015); Hurlbert (1984); Lazic (2019)",
                "Drafted by": "** Ben Farrar",
                "Reviewed (or Edited) by": "** Jamie P. Cockcroft; Mahmoud Elsherif; Elias Garcia-Pelegrin; Annalise A. LaPlume",
                "Translated by": "Susanne Vogel",
                "Translation reviewed by": "Helena Hartmann, Jennifer Mattschey",
                "Translation": "Wenn die Daten nicht statistisch unabhängig sind und somit die Anzahl der Stichproben (d. h. der Wiederholungen) künstlich vergrößert wird. Zum Beispiel, wenn mehr als ein Datenpunkt von der gleichen Versuchseinheit (z. B. Versuchsperson oder Kulturpflanze) gesammelt wird. Es gibt zahlreiche Methoden, um dieses Problem zu lösen, wie z. B. die Mittelwertbildung über die Wiederholungen (z. B. die mittlere Reaktionszeit einer Versuchsperson) oder die Implementierung von Modellen mit gemischten Effekten (mixed effects), bei denen die Struktur der zufälligen Effekte die Pseudo-Vervielfältigung berücksichtigt (z. B. die Spezifizierung jeder einzelnen Reaktionszeit als zu derselben Person). Beachten Sie, dass die erste Option mit einem Verlust an Information und statistischer Aussagekraft verbunden wäre.",
                "Related_terms": "** Confounding; Generalizability; Replication; Validity"
            },
            {
                "Title": "Psychometric meta-analysis (Psychometrische Metaanalyse) *",
                "Definition": "** Psychometric meta-analyses aim to correct for attenuation of the effect sizes of interest due to measurement error and other artifacts by using procedures based on psychometric principles, e.g. reliability of the measures. These procedures should be implemented before using the synthesised effect sizes in correlational or experimental meta-analysis, as making these corrections tends to lead to larger and less variable effect sizes.",
                "Reference(s)": "** Borenstein et al. (2009); Schmidt and Hunter (2014)",
                "Drafted by": "** Adrien Fillon",
                "Reviewed (or Edited) by": "** Mahmoud Elsherif; Eduardo Garcia-Garzon; Helena Hartmann; Catia M. Oliveira; Flávio Azevedo",
                "Translated by": "Helena Hartmann",
                "Translation reviewed by": "Susanne Vogel, Jennifer Mattschey",
                "Translation": "Psychometrische Meta-Analysen zielen darauf ab, die Abschwächung der interessierenden Effektgrößen aufgrund von Messfehlern und anderen Artefakten durch Verfahren zu korrigieren, die auf psychometrischen Grundsätzen beruhen, z. B. der Zuverlässigkeit (Reliabilität) der Messgrößen. Diese Verfahren sollten durchgeführt werden, bevor die synthetisierten Effektgrößen in Korrelations- oder experimentellen Meta-Analysen verwendet werden, da die Durchführung dieser Korrekturen tendenziell zu größeren und weniger variablen Effektgrößen führt.",
                "Related_terms": "** Correlational meta-analysis; Hunter-Schmidt meta-analysis; Meta-analysis; Non-Intervention, Reproducible, and Open Systematic Reviews (NIRO-SR); Publication bias (File Drawer Problem); Validity generalization"
            },
            {
                "Title": "Publication bias (File Drawer Problem) (Publikationsverzerrung; Aktenschubladenproblem) *",
                "Definition": "** The failure to publish results based on the \"direction or strength of the study findings\" (Dickersin & Min, 1993, p. 135). The bias arises when the evaluation of a study’s publishability disproportionately hinges on the outcome of the study, often with the inclination that novel and significant results are worth publishing more than replications and null results. This bias typically materializes through a disproportionate number of significant findings and inflated effect sizes. This process leads to the published scientific literature not being representative of the full extent of all research, and specifically underrepresents null finding. Such findings, in turn, land in the so called “file drawer”, where they are never published and have no findable documentation.",
                "Reference(s)": "** Dickersin and Min (1993); Devito and Goldacre (2019); Duval and Tweedie (2000a, 2000b); Franco et al. (2014); Lindsay (2020); Rothstein et al. (2005)",
                "Drafted by": "** Mahmoud Elsherif",
                "Reviewed (or Edited) by": "** Jamie P. Cockcroft; Gilad Feldman; Adrien Fillon; Helena Hartmann; Tamara Kalandadze; William Ngiam; Martin Vasilev; Olmo van den Akker; Flávio Azevedo",
                "Translated by": "Susanne Vogel",
                "Translation reviewed by": "Helena Hartmann, Jennifer Mattschey",
                "Translation": "Der Fehler, Ergebnisse nur auf der Grundlage der \"direction or strength of the study findings\" (dt. Richtung oder Stärke der Studienergebnisse) zu veröffentlichen (Dickersin & Min, 1993, S. 135). Die Voreingenommenheit (Bias) entsteht, wenn die Bewertung der Veröffentlichungswürdigkeit einer Studie unverhältnismäßig stark vom Ergebnis der Studie abhängt, oft mit der Tendenz, dass neuartige und signifikante Ergebnisse es mehr wert seien, veröffentlicht zu werden als Replikationen und Null-Befunde. Diese Voreingenommenheit äußert sich in der Regel durch eine unverhältnismäßig hohe Anzahl signifikanter Ergebnisse und überhöhte Effektgrößen. Dieser Prozess führt dazu, dass die veröffentlichte wissenschaftliche Literatur nicht repräsentativ für das gesamte Ausmaß der Forschung ist und insbesondere Null-Befunde unterrepräsentiert sind. Solche Ergebnisse wiederum landen in der so genannten \"Aktenschublade\" (File Drawer), wo sie nie veröffentlicht werden und keine auffindbare Dokumentation haben.",
                "Related_terms": "** Dissemination bias; P-curve; P-hacking; Selective reporting; Statistical significance; Trim and fill **Alternative definition:** In the context of meta-analysis, publication bias “...occurs whenever the research that appears in the published literature is systematically unrepresentative of the population of completed studies. Simply put, when the research that is readily available differs in its results from the results of all the research that has been done in an area, readers and reviewers of that research are in danger of drawing the wrong conclusion about what that body of research shows.” (Rothstein et al., 2005, p. 1\\) \\[GERMAN:\\] Im Zusammenhang mit Meta-Analysen tritt Publikationsverzerrung (Bias) immer dann auf \"... whenever the research that appears in the published literature is systematically unrepresentative of the population of completed studies. Simply put, when the research that is readily available differs in its results from the results of all the research that has been done in an area, readers and reviewers of that research are in danger of drawing the wrong conclusion about what that body of research shows” (dt. wenn die in der veröffentlichten Literatur erscheinende Forschung systematisch nicht repräsentativ für die Grundgesamtheit der abgeschlossenen Studien ist. Einfach ausgedrückt: Wenn die Ergebnisse der verfügbaren Forschungsarbeiten von den Ergebnissen aller in einem bestimmten Bereich durchgeführten Forschungsarbeiten abweichen, besteht für Lesende und Begutachtende dieser Forschungsarbeiten die Gefahr, dass sie falsche Schlussfolgerungen über die Ergebnisse dieser Forschungsarbeiten ziehen.\" (Rothstein et al., 2005, S. 1\\) **Related terms to alternative definition:** meta-analysis"
            },
            {
                "Title": "Public Trust in Science (öffentliches Vertrauen in die Wissenschaft) *",
                "Definition": "** Trust in the knowledge, guidelines and recommendations that has been produced or provided by scientists to the benefit of civil society (Hendriks et al., 2016). These may also refer to trust in scientific-based recommendations on public health (e.g., universal health-care, stem cell research, federal funds for women’s reproductive rights, preventive measures of contagious diseases, and vaccination), climate change, economic policies (e.g., welfare, inequality- and poverty-control) and their intersections. The trust a member of the public has in science has been shown to be influenced by a vast number of factors such as age (Anderson et al., 2012), gender (Von Roten, 2004), rejection of scientific norms (Lewandowsky & Oberauer, 2021), political ideology (Azevedo & Jost, 2021; Brewer & Ley, 2012; Leiserowitz et al., 2010), right-wing authoritarianism and social dominance (Kerr & Wilson, 2021), education (Bak, 2001; Hayes & Tariq, 2000), income (Anderson et al., 2012), science knowledge (Evans & Durant, 1995; Nisbet et al., 2002), social media use (Huber et al., 2019), and religiosity (Azevedo, 2021; Brewer & Ley, 2013; Liu & Priest, 2009).",
                "Reference(s)": "** Anderson et al. (2012); Azevedo (2021); Azevedo and Jost (2021); Bak (2001); Brewer and Ley (2013); Evans and Durant (1995); Hayes and Tariq (2000); Hendriks et al. (2016); Huber et al. (2019); Kerr and Wilson (2021); Lewandowsky and Oberauer (2021); Liu and Priest (2009); Nisbet et al. (2002); Schneider et al., (2019); Wingen et al. (2020)",
                "Originally drafted by": "** Tobias Wingen; Flávio Azevedo",
                "Reviewed (or Edited) by": "** Elias Garcia-Pelegrin; Helena Hartmann; Catia M. Oliveira; Olmo van den Akker",
                "Translated by": "Helena Hartmann",
                "Translation reviewed by": "Susanne Vogel, Jennifer Mattschey",
                "Translation": "Vertrauen in das Wissen, die Leitlinien und Empfehlungen, die von Wissenschaftler:innen zum Nutzen der Zivilgesellschaft erarbeitet oder bereitgestellt wurden (Hendriks et al., 2016). Dies kann sich auch auf das Vertrauen in wissenschaftlich fundierte Empfehlungen zur öffentlichen Gesundheit (z. B. universelle Gesundheitsversorgung, Stammzellenforschung, Bundesmittel für die reproduktiven Rechte von Frauen, Präventivmaßnahmen für ansteckende Krankheiten und Impfungen), zum Klimawandel, zur Wirtschaftspolitik (z. B. Sozial-/Transferleistungen, Bekämpfung von Ungleichheit und Armut) und deren Überschneidungen beziehen. Das Vertrauen der Öffentlichkeit in die Wissenschaft wird nachweislich durch eine Vielzahl von Faktoren beeinflusst, wie Alter (Anderson et al., 2012), Geschlecht (Von Roten, 2004), Ablehnung wissenschaftlicher Normen (Lewandowsky & Oberauer, 2021), politische Ideologie (Azevedo & Jost, 2021; Brewer & Ley, 2012; Leiserowitz et al, 2010), Rechtsautoritarismus und soziale Dominanz (Kerr & Wilson, 2021), Bildung (Bak, 2001; Hayes & Tariq, 2000), Einkommen (Anderson et al., 2012), wissenschaftliche Kenntnisse (Evans & Durant, 1995; Nisbet et al., 2002), Nutzung sozialer Medien (Huber et al., 2019\\) und Religiosität (Azevedo, 2021; Brewer & Ley, 2013; Liu & Priest, 2009).",
                "Related_terms": "** Credibility of scientific claims; Epistemic Trust"
            },
            {
                "Title": "Publish or Perish (Publizieren oder untergehen) *",
                "Definition": "** An aphorism describing the pressure researchers feel to publish academic manuscripts, often in high prestige academic journals, in order to have a successful academic career. This pressure to publish a high quantity of manuscripts can go at the expense of the quality of the manuscripts. This institutional pressure is exacerbated by hiring procedures and funding decisions strongly focusing on the number and impact of publications.",
                "Reference(s)": "** Case (1928); Fanelli (2010)",
                "Drafted by": "** Eliza Woodward",
                "Reviewed (or Edited) by": "** Nick Ballou; Mahmoud Elsherif; Helena Hartmann; Annalise A. LaPlume; Sam Parsons; Timo Roettger; Olmo van den Akker",
                "Translated by": "Susanne Vogel",
                "Translation reviewed by": "Helena Hartmann, Jennifer Mattschey",
                "Translation": "Ein Aphorismus, der den Druck beschreibt, unter dem Forschende stehen, akademische Manuskripte zu veröffentlichen, oft in hoch angesehenen akademischen Zeitschriften, um eine erfolgreiche akademische Karriere zu haben. Dieser Druck, eine große Anzahl von Manuskripten zu veröffentlichen, kann auf Kosten der Qualität der Manuskripte gehen. Dieser institutionelle Druck wird durch Einstellungs- und Berufungsverfahren und Finanzierungsentscheidungen verschärft, die sich stark an der Anzahl und dem Einfluss der Veröffentlichungen orientieren.",
                "Related_terms": "** Incentive structure; Journal Impact Factor; Reproducibility crisis (aka Replicability or replication crisis); Salami slicing; Slow Science"
            },
            {
                "Title": "PubPeer *",
                "Definition": "** A website that allows users to post anonymous peer reviews of research that has been published (i.e. post-publication peer review).",
                "Reference(s)": "** www.pubpeer.com",
                "Drafted by": "** Ali H. Al-Hoorie",
                "Reviewed (or Edited) by": "** Mahmoud ELsherif",
                "Translated by": "Helena Hartmann",
                "Translation reviewed by": "Susanne Vogel, Jennifer Mattschey",
                "Translation": "Eine Website, die es Nutzenden ermöglicht, anonyme Peer-Reviews zu publizierten Forschungsergebnissen zu veröffentlichen (d.h. Post-Publication Peer Review).",
                "Related_terms": "** Open Peer Review"
            },
            {
                "Title": "Python *",
                "Definition": "** An interpreted general-purpose programming language, intended to be user-friendly and easily readable, originally created by Guido van Rossum in 1991\\. Python has an extensive library of additional features with accessible documentation for tasks ranging from data analysis to experiment creation. It is a popular programming language in data science, machine learning and web development. Similar to R Markdown, Python can be presented in an interactive online format called a Jupyter notebook, combining code, data, and text.",
                "Reference": "** Lutz (2001)",
                "Originally drafted by": "** Shannon Francis",
                "Reviewed (or Edited) by": "** James E. Bartlett; Alexander Hart; Helena Hartmann; Dominik Kiersz; Graham Reid; Andrew J. Stewart",
                "Translated by": "Susanne Vogel",
                "Translation reviewed by": "Helena Hartmann, Jennifer Mattschey  ### **Q** {#q}",
                "Translation": "Eine interpretierte Allzweckprogrammiersprache, die benutzerfreundlich und leicht lesbar sein soll und ursprünglich 1991 von Guido van Rossum entwickelt wurde. Python verfügt über eine umfangreiche Bibliothek mit zusätzlichen Funktionen und einer leicht zugänglichen Dokumentation für Aufgaben, die von der Datenanalyse bis zur Erstellung von Experimenten reichen. Sie ist eine beliebte Programmiersprache in den Bereichen Datenwissenschaft, maschinelles Lernen und Webentwicklung. Ähnlich wie R Markdown kann Python in einem interaktiven Online-Format, dem Jupyter-Notebook, präsentiert werden, das Code, Daten und Text kombiniert.",
                "Related_terms": "** Jupyter; Matplotlib; NumPy; OpenSesame; PsychoPy; R"
            },
            {
                "Title": "Qualitative research (Qualitative Forschung) *",
                "Definition": "** Research which uses non-numerical data, such as textual responses, images, videos or other artefacts, to explore in-depth concepts, theories, or experiences. There are a wide range of qualitative approaches, from micro-detailed exploration of language or focusing on personal subjective experiences, to those which explore macro-level social experiences and opinions.",
                "Reference(s)": "** Aspers and Corte (2019); Levitt et al. (2017)",
                "Drafted by": "** Madeleine Pownall",
                "Reviewed (or Edited) by": "** Mahmoud Elsherif; Helena Hartmann; Oscar Lecuona; Claire Melia; Flávio Azevedo",
                "Translated by": "Helena Hartmann",
                "Translation reviewed by": "Susanne Vogel, Jennifer Mattschey",
                "Translation": "Forschung, die nicht-numerische Daten verwendet, wie z. B. Textantworten, Bilder, Videos oder andere Objekte, um Konzepte, Theorien oder Erfahrungen genauer zu untersuchen. Es gibt ein breites Spektrum an qualitativen Ansätzen, von der mikro-detaillierten Erforschung von Sprache oder der Konzentration auf persönliche subjektive Erfahrungen bis hin zu solchen, die soziale Erfahrungen und Meinungen auf der Makroebene untersuchen.",
                "Related_terms": "** Bracketing Interviews; Positionality; Quantitative research; Reflexivity **Alternative definition:** (if applicable) In Psychology, the **epistemology** of qualitative research is typically concerned with understanding people’s perspectives. Such epistemology proposes assuming the equity of researchers and participants as human beings, and in consequence, the need of sympathetic human understanding instead of data-driven conclusions. \\[GERMAN:\\] In der Psychologie befasst sich die Erkenntnistheorie der qualitativen Forschung in der Regel mit dem Verständnis der Perspektive der Menschen. Eine solche Erkenntnistheorie geht davon aus, dass Forschende und Teilnehmende gleichwertige menschliche Wesen sind und folglich ein mitfühlendes menschliches Verständnis anstelle von datengesteuerten Schlussfolgerungen erforderlich ist."
            },
            {
                "Title": "Quantitative research (Quantitative Forschung)  (translated, reviewed)**",
                "Definition": "** Quantitative research encompasses a diverse range of methods to systematically investigate a range of phenomena via the use of numerical data which can be analysed with statistics.",
                "Reference(s)": "** Goertzen (2017)",
                "Drafted by": "** Aoife O’Mahony",
                "Reviewed (or Edited) by": "** Valeria Agostini; Tamara Kalandadze; Adam Parker",
                "Translated by": "Susanne Vogel",
                "Translation reviewed by": "Helena Hartmann, Jennifer Mattschey",
                "Translation": "Quantitative Forschung umfasst ein breites Spektrum von Methoden zur systematischen Untersuchung einer Reihe von Phänomenen mit Hilfe von numerischen Daten, die mit Hilfe von Statistik analysiert werden können.",
                "Related_terms": "** Measuring; Qualitative research; Sample size; Statistical power; Statistics"
            },
            {
                "Title": "Questionable Research Practices or Questionable Reporting Practices (QRPs) (Fragwürdige Forschungs- oder Veröffentlichungspraktiken) *",
                "Definition": "** A range of activities that intentionally or unintentionally distort data in favour of a researcher’s own hypotheses \\- or omissions in reporting such practices \\- including; selective inclusion of data, hypothesising after the results are known (HARKing), and *p*\\-hacking. Popularized by John et al. (2012).",
                "Reference(s)": "** Banks et al. (2016); Fiedler and Schwartz (2016); Hardwicke et al. (2014); John et al. (2012); Neuroskeptic (2012); Sijtsma (2016); Simonsohn et al. (2011)",
                "Drafted by": "** Mahmoud Elsherif",
                "Reviewed (or Edited) by": "** Tamara Kalandadze; William Ngiam; Sam Parsons; Mariella Paul; Eike Mark Rinke; Timo Roettger; Flávio Azevedo",
                "Translated by": "Helena Hartmann",
                "Translation reviewed by": "Susanne Vogel, Jennifer Mattschey",
                "Translation": "Eine Reihe von Aktivitäten, die absichtlich oder unabsichtlich Daten zugunsten der eigenen Hypothesen eines Forschenden verzerren \\- oder Auslassungen bei der Berichterstattung über solche Praktiken \\- einschließlich selektiver Einbeziehung von Daten, Hypothesenbildung nach Bekanntwerden der Ergebnisse (HARKing) und p-hacking. Verbreitet von John et al. (2012).",
                "Related_terms": "** Creative use of outliers; Fabrication; File-drawer; Garden of forking paths; HARKing; Nonpublication of data; *P*\\-hacking; *P*\\-value fishing; Partial publication of data; Post-hoc storytelling; Preregistration; Questionable Measurement Practices (QMP); Researcher degrees of freedom; Reverse *p*\\-hacking; Salami slicing"
            },
            {
                "Title": "Questionable Measurement Practices (QMP; Fragwürdige Forschungspraktiken)) *",
                "Definition": "** Decisions researchers make that raise doubts about the validity of measures used in a study, and ultimately the study’s final conclusions (Flake & Fried, 2020). Issues arise from a lack of transparency in reporting measurement practices, a failure to address construct validity, negligence, ignorance, or deliberate misrepresentation of information.",
                "Reference": "** Flake and Fried (2020)",
                "Originally drafted by": "** Halil Emre Kocalar",
                "Reviewed (or Edited) by": "** Jamie P. Cockcroft; Annalise A. LaPlume; Sam Parsons; Mirela Zaneva; Flávio Azevedo",
                "Translated by": "Susanne Vogel",
                "Translation reviewed by": "Helena Hartmann, Jennifer Mattschey   ###  ### **R** {#r}",
                "Translation": "Entscheidungen von Forschenden, die Zweifel an der Gültigkeit der in einer Studie verwendeten Messgrößen und letztlich an den endgültigen Schlussfolgerungen der Studie aufkommen lassen (Flake & Fried, 2020). Probleme ergeben sich aus mangelnder Transparenz bei der Berichterstattung über Messpraktiken, einem Versäumnis, die Konstruktvalidität zu berücksichtigen, Nachlässigkeit, Unwissenheit oder einer absichtlichen Falschdarstellung von Informationen.",
                "Related_terms": "** Construct validity; Measurement schmeasurement; *P*\\-hacking; Psychometrics; Questionable Research Practices or Questionable Reporting Practices (QRPs); Validity"
            },
            {
                "Title": "R *",
                "Definition": "** R is a free, open-source programming language and software environment that can be used to conduct statistical analyses and plot data. R was created by Ross Ihaka and Robert Gentleman at the University of Auckland. R enables authors to share reproducible analysis scripts, which increases the transparency of a study. Often, R is used in conjunction with an integrated development environment (IDE) which simplifies working with the language, for example RStudio or Visual Studio Code, or Tinn-R.",
                "Reference": "** [https://www.r-project.org/](https://www.r-project.org/); R Core Team (2020)",
                "Originally drafted by": "** Lisa Spitzer",
                "Reviewed (or Edited) by": "** Bradley Baker; Alexander Hart; Joanne McCuaig; Andrew J. Stewart",
                "Translated by": "Helena Hartmann",
                "Translation reviewed by": "Susanne Vogel, Jennifer Mattschey",
                "Translation": "R ist eine freie, quelloffene Programmiersprache und Softwareumgebung, die zur Durchführung statistischer Analysen und zur Darstellung von Daten verwendet werden kann. R wurde von Ross Ihaka und Robert Gentleman an der Universität von Auckland entwickelt. R ermöglicht es Forschenden , reproduzierbare Analyseskripte weiterzugeben, was die Transparenz einer Studie erhöht. Häufig wird R in Verbindung mit einer integrierten Entwicklungsumgebung (integrated development environment, IDE) verwendet, die die Arbeit mit der Sprache vereinfacht, z. B. RStudio oder Visual Studio Code oder Tinn-R.",
                "Related_terms": "** Open-source; Statistical analysis"
            },
            {
                "Title": "Red Teams (Rote Teams)*",
                "Definition": "** An approach that integrates external criticism by colleagues and peers into the research process. Red teams are based on the idea that research that is more critically and widely evaluated is more reliable. The term originates from a military practice: One group (the red team) attacks something, and another group (the blue team) defends it. The practice has been applied to open science, by giving a red team (designated critical individuals) financial incentives to find errors in or identify improvements to the materials or content of a research project (in the materials, code, writing, etc.; Coles et al., 2020).",
                "Reference": "** Coles et al. (2020); Lakens (2020)",
                "Originally drafted by": "** Annalise A. LaPlume",
                "Reviewed (or Edited) by": "** Nick Ballou; Mahmoud Elsherif**;** Thomas Rhys Evans; Helena Hartmann; Timo Roettger",
                "Translated by": "Susanne Vogel",
                "Translation reviewed by": "Helena Hartmann, Jennifer Mattschey",
                "Translation": "Ein Ansatz, der die externe Kritik von Kolleg:innen und Peers in den Forschungsprozess integriert. Red Teams (Rote Teams) beruhen auf der Idee, dass Forschung, die kritischer und umfassender bewertet wird, zuverlässiger ist. Der Begriff geht auf eine militärische Praxis zurück: Eine Gruppe (das rote Team) greift etwas an, und eine andere Gruppe (das blaue Team) verteidigt es. Diese Praxis wurde auf Open Science angewandt, indem ein rotes Team (benannte kritische Einzelpersonen) finanzielle Anreize erhält, um Fehler in den Materialien oder Inhalten eines Forschungsprojekts (in den Materialien, im Code, in der Schrift usw.) zu finden oder Verbesserungen daran vorzuschlagen (Coles et al., 2020).",
                "Related_terms": "** Adversarial collaboration"
            },
            {
                "Title": "Reflexivity (Reflexivität) *",
                "Definition": "** The process of reflexivity refers to critically considering the knowledge that we produce through research, how it is produced, and our own role as researchers in producing this knowledge. There are different forms of reflexivity; personal reflexivity whereby researchers consider the impact of their own personal experiences, and functional whereby researchers consider the way in which our research tools and methods may have impacted knowledge production. Reflexivity aims to bring attention to underlying factors which may impact the research process, including development of research questions, data collection, and the analysis.",
                "Reference(s)": "** Braun and Clarke (2013); Finlay and Gough (2008)",
                "Drafted by": "** Claire Melia",
                "Reviewed (or Edited) by": "** Gilad Feldman; Annalise A. LaPlume",
                "Translated by": "Helena Hartmann",
                "Translation reviewed by": "Susanne Vogel, Jennifer Mattschey",
                "Translation": "Der Prozess der Reflexivität bezieht sich auf die kritische Betrachtung des Wissens, das wir durch Forschung produzieren, wie es produziert wird und unserer eigenen Rolle als Forschende bei der Produktion dieses Wissens. Es gibt verschiedene Formen der Reflexivität: die persönliche Reflexivität, bei der Forschende die Auswirkungen ihrer eigenen persönlichen Erfahrungen berücksichtigen, und die funktionale Reflexivität, bei der Forschende die Art und Weise betrachten, in der unsere Forschungsinstrumente und \\-methoden die Wissensproduktion beeinflusst haben könnten. Reflexivität zielt darauf ab, die Aufmerksamkeit auf die zugrundeliegenden Faktoren zu lenken, die den Forschungsprozess beeinflussen können, einschließlich der Entwicklung von Forschungsfragen, der Datenerhebung und der Analyse.",
                "Related_terms": "** Bracketing Interviews; Qualitative Research"
            },
            {
                "Title": "Registered Report *",
                "Definition": "** A scientific publishing format that includes an initial round of peer review of the background and methods (study design, measurement, and analysis plan); sufficiently high quality manuscripts are accepted for in-principle acceptance (IPA) at this stage. Typically, this stage 1 review occurs before data collection, however secondary data analyses are possible in this publishing format. Following data analyses and write up of results and discussion sections, the stage 2 review assesses whether authors sufficiently followed their study plan and reported deviations from it (and remains indifferent to the results). This shifts the focus of the review to the study’s proposed research question and methodology and away from the perceived interest in the study’s results.",
                "Reference(s)": "** Chambers (2013); Chambers et al. (2015); Chambers and Tzavella (2020); Findley et al. (2016); [https://www.cos.io/initiatives/registered-reports](https://www.cos.io/initiatives/registered-reports)",
                "Drafted by": "** Madeleine Pownall",
                "Reviewed (or Edited) by": "** Gilad Feldman; Emma Henderson; Aoife O’Mahony; Sam Parsons; Mariella Paul; Charlotte R. Pennington; Eike Mark Rinke; Timo Roettger; Olmo van den Akker; Yuki Yamada; Flávio Azevedo",
                "Translated by": "Susanne Vogel",
                "Translation reviewed by": "Helena Hartmann, Jennifer Mattschey",
                "Translation": "Ein wissenschaftliches Publikationsformat, das eine erste Runde der Begutachtung (Peer Review) des Hintergrunds und der Methoden (Studiendesign, Messung und Analyseplan) umfasst; Manuskripte von hinreichend hoher Qualität werden in dieser Phase zur In-Principle Acceptance (IPA) angenommen. In der Regel findet diese Phase 1 vor der Datenerhebung statt, jedoch sind Sekundärdatenanalysen in diesem Veröffentlichungsformat möglich. Im Anschluss an die Datenanalyse und die Verschriftlichung der Ergebnis- und Diskussionsabschnitte wird in Phase 2 bewertet, ob die Autor:innen ihren Studienplan hinreichend befolgt und über Abweichungen davon berichtet haben (diese Begutachtung erfolgt unabhängig von den Ergebnissen selbst). Damit verschiebt sich der Schwerpunkt der Überprüfung auf die vorgeschlagene Forschungsfrage und die Methodik der Studie und weg von dem wahrgenommenen Interesse an den Ergebnissen der Studie.",
                "Related_terms": "** Preregistration; Publication bias (File Drawer Problem); Results-free review; PCI (Peer Community In); Research Protocol"
            },
            {
                "Title": "Registry of Research Data Repositories *",
                "Definition": "** A global registry of research data repositories from different academic disciplines. It includes repositories that enable permanent storage of, description via metadata and access to, data sets by researchers, funding bodies, publishers, and scholarly institutions.",
                "Reference": "** [https://www.re3data.org/](https://www.re3data.org/) \\- Registry of Research Data Repositories.",
                "Originally drafted by": "** Aleksandra Lazić",
                "Reviewed (or Edited) by": "** Mahmoud Elsherif; Sam Parsons; Charlotte R. Pennington; Helena Hartmann",
                "Translated by": "Helena Hartmann",
                "Translation reviewed by": "Susanne Vogel, Jennifer Mattschey",
                "Translation": "Ein globales Register von Forschungsdaten-Repositorien aus verschiedenen akademischen Disziplinen. Es umfasst Repositorien, die Forschenden, Fördereinrichtungen, Verlagen und wissenschaftlichen Einrichtungen die dauerhafte Speicherung von Datensätzen, deren Beschreibung durch Metadaten und den Zugriff darauf ermöglichen.",
                "Related_terms": "** Metadata; Open Access; Open Data; Open Material; Repository"
            },
            {
                "Title": "Reliability (Reliabilität) *",
                "Definition": "** The extent to which repeated measurements lead to the same results. In psychometrics, reliability refers to the extent to which respondents have similar scores when they take a questionnaire on multiple occasions. Noteworthy, reliability does not imply validity. Furthermore, additional types of reliability besides internal consistency exist, including: test-retest reliability, parallel forms reliability and interrater reliability.",
                "Reference": "** Bollen (1989); Drost (2011)",
                "Originally drafted by": "** Annalise A. LaPlume",
                "Reviewed (or Edited) by": "** Mahmoud Elsherif**;** Eduardo Garcia-Garzon; Kai Krautter; Olmo van den Akker",
                "Translated by": "Susanne Vogel",
                "Translation reviewed by": "Helena Hartmann, Jennifer Mattschey",
                "Translation": "Das Ausmaß, in dem wiederholte Messungen zu den gleichen Ergebnissen führen. In der Psychometrik bezieht sich die Reliabilität auf das Ausmaß, in dem die Befragten ähnliche Ergebnisse erzielen, wenn sie einen Fragebogen mehrmals ausfüllen. Es sei darauf hingewiesen, dass Reliabilität nicht Validität impliziert. Darüber hinaus gibt es neben der Test-Retest-Reliabilität noch weitere Arten der Reliabilität, darunter die interne Konsistenz, die Paralleltest-Reliabilität und die Interrater-Reliabilität.",
                "Related_terms": "** Consistency; Internal consistency; Quality Criteria; Replicability; Reproducibility; Validity"
            },
            {
                "Title": "Repeatability (Wiederholbarkeit) *",
                "Definition": "** Synonymous with *test-retest* *reliability*. It refers to the agreement between the results of successive measurements of the same measure. Repeatability requires the same experimental tools, the same observer, the same measuring instrument administered under the same conditions, the same location, repetition over a short period of time, and the same objectives (Joint Committee for Guidelines in Metrology, 2008\\)",
                "Reference(s)": "** ISO (1993); Stodden (2011)",
                "Drafted by": "** Mahmoud Elsherif, Adam Parker",
                "Reviewed (or Edited) by": "** Gilad Feldman; Helena Hartmann; Joanne McCuaig; Sam Parsons",
                "Translated by": "Susanne Vogel",
                "Translation reviewed by": "Helena Hartmann, Jennifer Mattschey",
                "Translation": "Synonym für die Test-Retest-Reliabilität. Sie bezieht sich auf die Übereinstimmung zwischen den Ergebnissen aufeinanderfolgender Messungen derselben Maße. Die Wiederholbarkeit erfordert die gleichen Versuchswerkzeuge, die gleichen Beobachtenden, das gleiche Messinstrument unter den gleichen Bedingungen, den gleichen Ort, Wiederholungen über einen kurzen Zeitraum und die gleichen Ziele (Joint Committee for Guidelines in Metrology, 2008\\)",
                "Related_terms": "** Reliability"
            },
            {
                "Title": "Replicability (Replikabilität)*",
                "Definition": "** An umbrella term, used differently across fields, covering concepts of: direct and conceptual replication, computational reproducibility/replicability, generalizability analysis and robustness analyses. Some of the definitions used previously include: a different team arriving at the same results using the original author's artifacts (Barba 2018); a study arriving at the same conclusion after collecting new data (Claerbout and Karrenbach, 1992); as well as studies for which any outcome would be considered diagnostic evidence about a claim from prior research (Nosek & Errington, 2020).",
                "Reference(s)": "** Barba (2018); Crüwell et al. (2019); King (1996); National Academies of Sciences et al. (2011); Nosek and Errington (2020)",
                "Drafted by": "** Mahmoud Elsherif",
                "Reviewed (or Edited) by": "** Jamie P. Cockcroft; Adrien Fillon; Gilad Feldman; Annalise A. LaPlume; Tina B. Lonsdorf; Sam Parsons; Eike Mark Rinke; Tobias Wingen",
                "Translated by": "Susanne Vogel",
                "Translation reviewed by": "Helena Hartmann, Jennifer Mattschey",
                "Translation": "Ein Oberbegriff, der in verschiedenen Bereichen unterschiedlich verwendet wird und folgende Konzepte umfasst: direkte und konzeptionelle Replikation, komputationale Reproduzierbarkeit/Replizierbarkeit, Generalisierungsanalyse und Robustheitsanalysen. Einige der zuvor verwendeten Definitionen umfassen: ein anderes Team, das unter Verwendung der Materialien der ursprünglichen Autor:in zu denselben Ergebnissen gelangt (Barba 2018); eine Studie, die nach der Erhebung neuer Daten zu denselben Schlussfolgerungen gelangt (Claerbout und Karrenbach, 1992); sowie Studien, bei denen jedes Ergebnis als diagnostischer Beweis für eine Behauptung aus einer früheren Forschung angesehen wird (Nosek & Errington, 2020).",
                "Related_terms": "** Conceptual replication; Direct Replication; Generalizability; Reproducibility; Reliability; Robustness (analyses)"
            },
            {
                "Title": "Replication Markets (Replikationsmärkte) *",
                "Definition": "** A replication market is an environment where users bet on the replicability of certain effects. Forecasters are incentivized to make accurate predictions and the top successful forecasters receive monetary compensation or contributorship for their bets. The rationale behind a replication market is that it leverages the collective wisdom of the scientific community to predict which effect will most likely replicate, thus encouraging researchers to channel their limited resources to replicating these effects.",
                "Reference": "** Liu et al. (2020); Tierney et al. (2020); Tierney et al. (2021); www.replicationmarkets.com",
                "Originally drafted by": "** Ali H. Al-Hoorie",
                "Reviewed (or Edited) by": "** Mahmoud Elsherif; Leticia Micheli; Sam Parsons",
                "Translated by": "Helena Hartmann",
                "Translation reviewed by": "Susanne Vogel, Jennifer Mattschey",
                "Translation": "Ein Replikationsmarkt ist eine Umgebung, in der Nutzer:innen auf die Replizierbarkeit bestimmter Effekte wetten. Für die Vorhersagenden besteht ein Anreiz, genaue Vorhersagen zu treffen, und die erfolgreichsten Vorhersagenden erhalten für ihre Wetten eine finanzielle Entlohnung die Erwähnung als Beitragende (Contributorship). Der Grundgedanke hinter einem Replikationsmarkt ist, dass er die kollektive Weisheit der wissenschaftlichen Gemeinschaft nutzt, um vorherzusagen, welcher Effekt sich am wahrscheinlichsten replizieren lässt, und so die Forschenden ermutigt, ihre begrenzten Ressourcen für die Replikation dieser Effekte einzusetzen.",
                "Related_terms": "** Citizen science; Crowdsourcing; Replicability; Reproducibility"
            },
            {
                "Title": "Reporting Guideline (Berichtleitlinien) *",
                "Definition": "** A reporting guideline is a “checklist, flow diagram, or structured text to guide authors in reporting a specific type of research, developed using explicit methodology.” (EQUATOR Network, n.d.). Reporting guidelines provide the minimum guidance required to ensure that research findings can be appropriately interpreted, appraised, synthesized and replicated. Their use often differs per scientific journal or publisher.",
                "Reference": "** Moher et al. (2009) Schulz et al. (2010); Torpor et al. (2021); Von Elm et al. (2007); [https://www.equator-network.org/about-us/what-is-a-reporting-guideline/](https://www.equator-network.org/about-us/what-is-a-reporting-guideline/)",
                "Originally drafted by": "** Aidan Cashin",
                "Reviewed (or Edited) by": "** Gilad Feldman; Helena Hartmann; Joanne McCuaig",
                "Translated by": "Susanne Vogel",
                "Translation reviewed by": "Helena Hartmann, Jennifer Mattschey",
                "Translation": "Ein Leitfaden für die Berichterstattung ist eine “checklist, flow diagram, or structured text to guide authors in reporting a specific type of research, developed using explicit methodology.” (dt. Checkliste, ein Flussdiagramm oder ein strukturierter Text, der die Autor:innen bei der Verschriftlichung einer bestimmten Art von Forschung anleitet und unter Verwendung einer expliziten Methodik entwickelt wurde, EQUATOR-Netzwerk, n.d.). Reporting Guidelines bieten ein Mindestmaß an Anleitung, um sicherzustellen, dass Forschungsergebnisse angemessen interpretiert, bewertet, zusammengefasst und reproduziert werden können. Ihre Anwendung ist oft in verschiedenen Journals oder Verlagen unterschiedlich..",
                "Related_terms": "** CONSORT; Non-Intervention, Reproducible, and Open Systematic Reviews (NIRO-SR); PRISMA; STROBE"
            },
            {
                "Title": "Repository (Repositorium) *",
                "Definition": "** An online archive for the storage of digital objects including research outputs, manuscripts, analysis code and/or data. Examples include preprint servers such as bioRxiv, MetaArXiv, PsyArXiv, institutional research repositories, as well as data repositories that collect and store datasets including zenodo.org, PsychData, and code repositories such as Github, or more general repositories for all kinds of research data, such as the Open Science Framework (OSF). Digital objects stored in repositories are typically described through metadata which enables discovery across different storage locations.",
                "Reference(s)": "** [https://www.nature.com/sdata/policies/repositories](https://www.nature.com/sdata/policies/repositories)",
                "Originally drafted by": "** Tina Lonsdorf",
                "Reviewed (or Edited) by": "** Gilad Feldman; Connor Keating; Mariella Paul; Charlotte R. Pennington; Flávio Azevedo",
                "Translated by": "Helena Hartmann",
                "Translation reviewed by": "Susanne Vogel, Jennifer Mattschey",
                "Translation": "Ein Online-Archiv für die Speicherung digitaler Objekte wie Forschungsergebnisse, Manuskripte, Analysecodes und/oder Daten. Beispiele sind Preprint-Server wie bioRxiv, MetaArXiv, PsyArXiv, institutionelle Forschungsrepositorien sowie Datenrepositorien, die Datensätze sammeln und speichern, z. B. zenodo.org, PsychData und Code-Repositorien wie Github, oder allgemeinere Repositorien für alle Arten von Forschungsdaten wie das Open Science Framework (OSF). Digitale Objekte, die in Repositorien gespeichert werden, werden in der Regel durch Metadaten beschrieben, die das Auffinden in verschiedenen Speicherorten erlauben.",
                "Related_terms": "** Data sharing; Github; Metadata; Open Access; Open data; Open Material; Open Science Framework; Open Source; Preprint"
            },
            {
                "Title": "ReproducibiliTea *",
                "Definition": "A grassroots initiative that helps researchers create local journal clubs at their universities to discuss a range of topics relating to open research and scholarship. Each meeting usually centres around a specific paper that discusses, for example, reproducibility, research practice, research quality, social justice and inclusion, and ideas for improving science.",
                "Reference": "** [https://reproducibilitea.org/](https://reproducibilitea.org/); Orben (2019)",
                "Originally drafted by": "** Emma Norris",
                "Reviewed (or Edited) by": "** Mahmoud Elsherif; Gilad Feldman; Connor Keating; Charlotte R. Pennington; Sam Parsons; Flávio Azevedo",
                "Translated by": "Helena Hartmann",
                "Translation reviewed by": "Susanne Vogel, Jennifer Mattschey",
                "Translation": "Eine basisdemokratische Initiative, die Forschenden hilft, an ihren Universitäten lokale Journal Clubs zu gründen, um verschiedene Themen im Zusammenhang mit offener Forschung und Scholarship zu diskutieren. Im Mittelpunkt eines jeden Treffens steht in der Regel eine bestimmte Veröffentlichung, in der z. B. Reproduzierbarkeit, Forschungspraxis, Forschungsqualität, soziale Gerechtigkeit und Inklusion sowie Ideen zur Verbesserung der Wissenschaft erörtert werden.",
                "Related_terms": "** Grassroots initiative; Journal club; Open science; Reproducibility"
            },
            {
                "Title": "Reproducibility (Reproduzierbarkeit)*",
                "Definition": "** A minimum standard on a spectrum of activities (\"reproducibility spectrum\") for assessing the value or accuracy of scientific claims based on the original methods, data, and code. For instance, where the original researcher's data and computer codes are used to regenerate the results (Barba, 2018), often referred to as computational reproducibility. Reproducibility does not guarantee the quality, correctness, or validity of the published results (Peng, 2011). In some fields, this meaning is, instead, associated with the term “replicability” or ‘repeatability’.",
                "Reference(s)": "** Barba (2018); Cruwell et al. (2019); Peng (2011), Stodden (2011); Syed (2019); National Academies of Sciences, Engineering, and Medicine (2019)",
                "Drafted by": "** Mahmoud Elsherif",
                "Reviewed (or Edited) by": "** Helena Hartmann; Annalise A. LaPlume; Tina B. Lonsdorf; Sam Parsons; Charlotte R. Pennington; Suzanne L. K. Stewart",
                "Translated by": "Susanne Vogel",
                "Translation reviewed by": "Helena Hartmann, Jennifer Mattschey",
                "Translation": "Ein Mindeststandard auf einem Spektrum von Aktivitäten (\"Reproduzierbarkeitsspektrum\") zur Bewertung des Wertes oder der Genauigkeit wissenschaftlicher Aussagen auf der Grundlage der ursprünglichen Methoden, Daten und Codes. Zum Beispiel, wenn die Daten und Computercodes der ursprünglichen Forschenden verwendet werden, um die Ergebnisse wieder zu erzeugen (Barba, 2018), was oft als komputationale Reproduzierbarkeit bezeichnet wird. Die Reproduzierbarkeit garantiert nicht die Qualität, Korrektheit oder Gültigkeit der veröffentlichten Ergebnisse (Peng, 2011). In einigen Bereichen wird diese Bedeutung stattdessen mit dem Begriff \"Replizierbarkeit\" (Replicability) oder \"Wiederholbarkeit\" (Repeatability) in Verbindung gebracht.",
                "Related_terms": "** Computational reproducibility; Replicability; repeatability"
            },
            {
                "Title": "Reproducibility crisis (aka Replicability or replication crisis) (Reproduzierbarkeitskrise, auch Replizierbarkeitskrise oder Replikationskrise) *",
                "Definition": "** The finding, and related shift in academic culture and thinking, that a large proportion of scientific studies published across disciplines do not replicate (e.g. Open Science Collaboration, 2015). This is considered to be due to a lack of quality and integrity of research and publication practices, such as publication bias, QRPs and a lack of transparency, leading to an inflated rate of false positive results. Others have described this process as a ‘Credibility revolution’ towards improving these practices.",
                "Reference(s)": "** Fanelli (2018); Open Science Collaboration (2015)",
                "Drafted by": "** Mahmoud Elsherif",
                "Reviewed (or Edited) by": "** Helena Hartmann; Annalise A. LaPlume; Mariella Paul; Sonia Rishi; Lisa Spitzer",
                "Translated by": "Helena Hartmann",
                "Translation reviewed by": "Susanne Vogel, Jennifer Mattschey",
                "Translation": "Die Feststellung und der damit verbundene Wandel in der akademischen Kultur und Denkweise, dass ein großer Teil der in verschiedenen Disziplinen veröffentlichten wissenschaftlichen Studien nicht replizierbar ist (z. B. Open Science Collaboration, 2015). Man geht davon aus, dass dies auf einen Mangel an Qualität und Integrität der Forschungs- und Veröffentlichungspraktiken zurückzuführen ist, z. B. Publikationsverzerrungen, QRPs und mangelnde Transparenz, was zu einer überhöhten Rate falsch-positiver Ergebnisse führt. Andere haben diesen Prozess als eine \"Glaubwürdigkeitsrevolution\" zur Verbesserung dieser Praktiken beschrieben.",
                "Related_terms": "** Credibility crisis; Publication bias (File Drawer Problem); Questionable Research Practices or Questionable Reporting Practices (QRPs); Replicability; Reproducibility"
            },
            {
                "Title": "Reproducibility Network (Reproduzierbarkeitsnetzwerk) *",
                "Definition": "** A reproducibility network is a consortium of open research working groups, often peer-led. The groups operate on a wheel-and-spoke model across a particular country, in which the network connects local cross-disciplinary researchers, groups, and institutions with a central steering group, who also connect with external stakeholders in the research ecosystem. The goals of reproducibility networks include; advocating for greater awareness, promoting training activities, and disseminating best-practices at grassroots, institutional, and research ecosystem levels. Such networks exist in the UK, Germany, Switzerland, Slovakia, and Australia (as of March 2021).",
                "Reference": "** [https://www.ukrn.org/](https://www.ukrn.org/) ; [https://reproducibilitynetwork.de/](https://reproducibilitynetwork.de/); [https://www.swissrn.org/](https://www.swissrn.org/); [https://slovakrn.wixsite.com/skrn](https://slovakrn.wixsite.com/skrn); [https://www.aus-rn.org/](https://www.aus-rn.org/)",
                "Originally drafted by": "** Suzanne L. K. Stewart",
                "Reviewed (or Edited) by": "** Annalise A. LaPlume; Sam Parsons; Flávio Azevedo",
                "Translated by": "Susanne Vogel",
                "Translation reviewed by": "Helena Hartmann, Jennifer Mattschey",
                "Translation": "Ein Reproduzierbarkeitsnetzwerk ist ein Konsortium offener Forschungsarbeitsgruppen, die häufig von Peers geleitet werden. Die Gruppen arbeiten in einem bestimmten Land nach dem wheel-and-spoke-Modell (dt. Rad-und-Speichen-Modell), bei dem das Netzwerk lokale disziplinübergreifende Forschende, Gruppen und Einrichtungen mit einer zentralen Lenkungsgruppe verbindet, die auch mit externen Akteuren im Forschungsökosystem in Verbindung steht. Zu den Zielen von Reproduzierbarkeitsnetzwerken gehören die Sensibilisierung für das Thema, die Förderung von Trainingsmaßnahmen und die Verbreitung bewährter Praktiken an der Basis, in den Institutionen und im Forschungsumfeld. Solche Netzwerke gibt es im Vereinigten Königreich, in Deutschland, in der Schweiz, in der Slowakei und in Australien (Stand: März 2021)."
            },
            {
                "Title": "Research Contribution Metric (*p*) *",
                "Definition": "** Type of semantometric measure assessing similarity of publications connected in a citation network. This method uses a simple formula to assess authors’ contributions. Publication *p* can be estimated based on the semantic distance from the publications cited by *p* to publications citing *p*.",
                "Reference": "** Knoth and Herrmannova (2014); Holcombe (2019); Larivière et al. (2016)",
                "Originally drafted by": "** Alaa AlDoh",
                "Reviewed (or Edited) by": "** Michele C. Lim; Jamie P. Cockcroft; Micah Vandegrift; Dominik Kiersz",
                "Translated by": "Helena Hartmann",
                "Translation reviewed by": "Susanne Vogel, Jennifer Mattschey  ### ---",
                "Translation": "Eine Art semantometrisches Maß zur Bewertung der Ähnlichkeit von Veröffentlichungen, die in einem Zitationsnetzwerk miteinander verbunden sind. Diese Methode verwendet eine einfache Formel, um die Beiträge der Autor:innen zu bewerten. Die Publikation *p* kann auf der Grundlage der semantischen Distanz zwischen den von p zitierten Publikationen und den Publikationen, die *p* zitieren, geschätzt werden.",
                "Related_terms": "** Semantometrics"
            },
            {
                "Title": "Research Cycle (Forschungszyklus) *",
                "Definition": "** Describes the circular process of conducting scientific research, with “researchers working at various stages of inquiry, from more tentative and exploratory investigations to the testing of more definitive and well-supported claims” (Lieberman, 2020, p. 42). The cycle includes literature research and hypothesis generation, data collection and analysis, as well as dissemination of results (e.g. through publication in peer-reviewed journals), which again informs theory and new hypotheses/research.",
                "Reference(s)": "** Bramoullé and Saint Paul (2010); Lieberman (2020)",
                "Originally drafted by": "** Helena Hartmann",
                "Reviewed (or Edited) by": "** Jamie P. Cockcroft; Aleksandra Lazić; Graham Reid; Beatrice Valentini",
                "Translated by": "Helena Hartmann",
                "Translation reviewed by": "Susanne Vogel, Jennifer Mattschey",
                "Translation": "Beschreibt den zirkulären Prozess der wissenschaftlichen Forschung, bei dem Forschende in verschiedenen Stadien der Untersuchung arbeiten, von eher vorsichtigen und explorativen Untersuchungen bis hin zur Prüfung definierterer und gut belegter Behauptungen (aus dem Engl. “researchers working at various stages of inquiry, from more tentative and exploratory investigations to the testing of more definitive and well-supported claims”, Lieberman, 2020, S. 42). Der Zyklus umfasst die Literaturrecherche und Hypothesenbildung, die Datenerhebung und \\-analyse sowie die Verbreitung der Ergebnisse (z. B. durch die Veröffentlichung in Fachzeitschriften mit Peer-Review), aus der wiederum die Theorie und neue Hypothesen/Forschungen hervorgehen.",
                "Related_terms": "** Research process"
            },
            {
                "Title": "Research Data Management (Forschungsdatenmanagement) *",
                "Definition": "** Research Data Management (RDM) is a broad concept that includes processes undertaken to create organized, documented, accessible, and reusable quality research data. Adequate research data management provides many benefits including, but not limited to, reduced likelihood of data loss, greater visibility and collaborations due to data sharing, demonstration of research integrity and accountability.",
                "Reference(s)": "** CESSDA; Corti et al. (2019)",
                "Drafted by": "** Micah Vandegrift",
                "Reviewed (or Edited) by": "** Helena Hartmann; Tina B. Lonsdorf; Catia M. Oliveira; Julia Wolska",
                "Translated by": "Susanne Vogel",
                "Translation reviewed by": "Helena Hartmann, Jennifer Mattschey",
                "Translation": "Research Data Management (dt. Forschungsdatenmanagement, FDM) ist ein weit gefasstes Konzept, das Prozesse zur Erstellung organisierter, dokumentierter, zugänglicher und wiederverwendbarer hochwertiger Forschungsdaten umfasst. Ein angemessenes Forschungsdatenmanagement bietet viele Vorteile, unter anderem eine geringere Wahrscheinlichkeit von Datenverlusten, eine größere Sichtbarkeit und Zusammenarbeit durch die gemeinsame Nutzung von Daten sowie die Demonstration von Integrität und Rechenschaft in der Forschung.",
                "Related_terms": "** Data curation; Data documentation; Data management plan (DMP); Data sharing; Metadata; Research data management"
            },
            {
                "Title": "Research integrity (Forschungsintegrität) *",
                "Definition": "** Research integrity is defined by a set of good research practices based on fundamental principles: honesty, reliability, respect and accountability (ALLEA, 2017). Good research practices —which are based on fundamental principles of research integrity and should guide researchers in their work as well as in their engagement with the practical, ethical and intellectual challenges inherent in research— refer to areas such as: research environment (e.g., research institutions and organisations promote awareness and ensure a prevailing culture of research integrity), training, supervision and mentoring (e.g., Research institutions and organisations develop appropriate and adequate training in ethics and research integrity to ensure that all concerned are made aware of the relevant codes and regulations), research procedures (e.g., researchers report their results in a way that is compatible with the standards of the discipline and, where applicable, can be verified and reproduced), safeguards (e.g., researchers have due regard for the health, safety and welfare of the community, of collaborators and others connected with their research), data practices and management (e.g., researchers, research institutions and organisations provide transparency about how to access or make use of their data and research materials), collaborative working, publication and dissemination (e.g., authors and publishers consider negative results to be as valid as positive findings for publication and dissemination), reviewing, evaluating and editing (e.g., researchers review and evaluate submissions for publication, funding, appointment, promotion or reward in a transparent and justifiable manner).",
                "Reference(s)": "** ALLEA (2017); Medin (2012); Moher et al. (2020)",
                "Drafted by": "** Ana Barbosa Mendes; Flávio Azevedo",
                "Reviewed (or Edited) by": "** Valeria Agostini; Bradley Baker; Gilad Feldman; Tamara Kalandadze; Charlotte R. Pennington",
                "Translated by": "Helena Hartmann",
                "Translation reviewed by": "Susanne Vogel, Jennifer Mattschey",
                "Translation": "Die Integrität der Forschung wird durch eine Reihe von guten Forschungspraktiken definiert, die auf den folgenden Grundprinzipien beruhen: Ehrlichkeit, Zuverlässigkeit, Respekt und Verantwortlichkeit (ALLEA, 2017). Gute Forschungspraktiken \\- die auf den Grundprinzipien der Integrität in der Forschung beruhen und die Forschende bei ihrer Arbeit sowie bei der Auseinandersetzung mit den praktischen, ethischen und intellektuellen Herausforderungen der Forschung leiten sollten \\- beziehen sich auf Bereiche wie: Forschungsumfeld (z. B. Forschungseinrichtungen und \\-organisationen fördern das Bewusstsein und sorgen für eine vorherrschende Kultur der Integrität in der Forschung), Ausbildung, Supervision und Betreuung (z. B. Forschungseinrichtungen und \\-organisationen entwickeln eine geeignete und angemessene Ausbildung in den Bereichen Ethik und Integrität in der Forschung, um sicherzustellen, dass alle Beteiligten mit den einschlägigen Kodizes und Vorschriften vertraut gemacht werden), Forschungsverfahren (z. B., Forschende berichten über ihre Ergebnisse in einer Art und Weise, die mit den Standards des Fachgebiets vereinbar ist und gegebenenfalls überprüft und reproduziert werden kann), Schutzmaßnahmen (z. B. achten Forschende in angemessener Weise auf die Gesundheit, die Sicherheit und das Wohlergehen der Gemeinschaft, der Mitarbeitenden und anderer Personen, die mit ihrer Forschung in Verbindung stehen), Datenpraktiken und \\-management (z. B. bieten Forschende, Forschungseinrichtungen und Organisationen Transparenz darüber, wie ihre Daten und Forschungsmaterialien zugänglich sind oder genutzt werden können), Zusammenarbeit, Veröffentlichung und Verbreitung (z. B. betrachten Autor:innen und Verleger:innen Null-Befunde (auch “negative Ergebnisse”) als ebenso gültig wie positive Ergebnisse für die Veröffentlichung und Verbreitung), Überprüfung, Bewertung und Bearbeitung (z. B. überprüfen und bewerten Forschende Einreichungen für die Veröffentlichung, Finanzierung, Ernennung, Beförderung oder Belohnung auf transparente und vertretbare Weise).",
                "Related_terms": "** Credibility of scientific claims; Error detection; Ethics; Open research; Questionable Research Practices or Questionable Reporting Practices (QRPs); Responsible Research Practices; Rigour; Transparency; Trustworthy research"
            },
            {
                "Title": "Research Protocol (Forschungsprotokoll) *",
                "Definition": "** A detailed document prepared before conducting a study, often written as part of ethics and funding applications. The protocol should include information relating to the background, rationale and aims of the study, as well as hypotheses which reflect the researchers’ expectations. The protocol should also provide a “recipe” for conducting the study, including methodological details and clear analysis plans. Best practice guidelines for creating a study protocol should be used for specific methodologies and fields. It is possible to publically share research protocols to attract new collaborators or facilitate efficient collaboration across labs (e.g. [https://www.protocols.io/](https://www.protocols.io/)). In medical and educational fields, protocols are often a separate article type suitable for publication in journals. Where protocol sharing or publication is not common practice, researchers can choose preregistration.",
                "Reference": "** BMJ (2015); Nosek et al. (2018)",
                "Originally drafted by": "** Marta Topor",
                "Reviewed (or Edited) by": "** Helena Hartmann; Bethan Iley; Annalise A. LaPlume; Charlotte Pennington",
                "Translated by": "Susanne Vogel",
                "Translation reviewed by": "Helena Hartmann, Jennifer Mattschey",
                "Translation": "Ein detailliertes Dokument, das vor der Durchführung einer Studie erstellt wird und häufig Teil von Ethik- und Drittmittelanträgen ist. Das Protokoll sollte Informationen über den Hintergrund, die Begründung und die Ziele der Studie sowie Hypothesen enthalten, die die Erwartungen der Forschenden widerspiegeln. Das Protokoll sollte auch ein \"Rezept\" für die Durchführung der Studie enthalten, einschließlich methodischer Details und klarer Analysepläne. Für bestimmte Methoden und Bereiche sollten Best-Practice-Leitlinien für die Erstellung eines Studienprotokolls verwendet werden. Es ist möglich, Forschungsprotokolle öffentlich zugänglich zu machen, um neue Kooperationspartner:innen zu gewinnen oder die effiziente Zusammenarbeit zwischen Gruppen zu erleichtern (z. B. https://www.protocols.io/). In der Medizin und im Bildungsbereich sind Protokolle oft ein eigener Artikeltyp, der sich für die Veröffentlichung in Fachzeitschriften eignet. Wo die gemeinsame Nutzung oder Veröffentlichung von Protokollen nicht üblich ist, können sich Forschende für eine Präregistrierung entscheiden.",
                "Related_terms": "** Many Labs; Preregistration"
            },
            {
                "Title": "Research workflow (Forschungs-Arbeitsablauf) *",
                "Definition": "** The process of conducting research from conceptualisation to dissemination. A typical workflow may look like the following: Starting with conceptualisation to identify a research question and design a study. After study design, researchers need to gain ethical approval (if necessary) and may decide to preregister the final version. Researchers then collect and analyse their data. Finally, the process ends with dissemination; moving between pre-print and post-print stages as the manuscript is submitted to a journal.",
                "Reference(s)": "** Kathawalla et al. (2021); Stodden (2011)",
                "Drafted by": "** James E Bartlett",
                "Reviewed (or Edited) by": "** Gilad Feldman; Helena Hartmann; Aleksandra Lazić; Joanne McCuaig; Timo Roettger; Sam Parsons; Steven Verheyen",
                "Translated by": "Helena Hartmann",
                "Translation reviewed by": "Susanne Vogel, Jennifer Mattschey",
                "Translation": "Der Prozess der Durchführung von Forschungsarbeiten von der Konzeption bis zur Verbreitung. Ein typischer Arbeitsablauf (workflow) könnte folgendermaßen aussehen: Beginnend mit der Konzeptualisierung, um eine Forschungsfrage zu ermitteln und eine Studie zu entwerfen. Nach der Konzeption der Studie müssen die Forschenden eine ethische Genehmigung einholen (falls erforderlich) und können beschließen, die endgültige Version zu präregistrieren. Anschließend erheben die Forschenden ihre Daten und analysieren sie. Schließlich endet der Prozess mit der Verbreitung, wobei zwischen Preprint- und Postprint-Phasen unterschieden wird, wenn das Manuskript bei einer Zeitschrift eingereicht wird.",
                "Related_terms": "** Open Research Workflow; Research cycle; Research pipeline"
            },
            {
                "Title": "Researcher degrees of freedom (Freiheitsgrade von Forschenden) *",
                "Definition": "** Refers to the flexibility often inherent in the scientific process, from hypothesis generation, designing and conducting a research study to processing the data and analyzing as well as interpreting and reporting results. Due to a lack of precisely defined theories and/or empirical evidence, multiple decisions are often equally justifiable. The term is sometimes used to refer to the opportunistic (ab-)use of this flexibility aiming to achieve desired results —e.g., when in- or excluding certain data— albeit the fact that technically the term is not inherently value-laden.",
                "Reference": "** Gelman and Loken (2013); Simmons et al. (2011); Wicherts et al. (2016)",
                "Originally drafted by": "** Tina Lonsdorf",
                "Reviewed (or Edited) by": "** Gilad Feldman; Helena Hartmann; Timo Roettger; Robbie C.M. van Aert; Flávio Azevedo",
                "Translated by": "Helena Hartmann",
                "Translation reviewed by": "Susanne Vogel, Jennifer Mattschey",
                "Translation": "Bezieht sich auf die Flexibilität, die dem wissenschaftlichen Prozess oft innewohnt, von der Hypothesenbildung über die Konzeption und Durchführung einer Forschungsstudie bis hin zur Verarbeitung der Daten und der Analyse sowie der Interpretation und Berichterstattung der Ergebnisse. Da es an genau definierten Theorien und/oder empirischen Beweisen mangelt, sind häufig mehrere Entscheidungen gleichermaßen vertretbar. Der Begriff wird manchmal verwendet, um auf die opportunistische (missbräuchliche) Nutzung dieser Flexibilität hinzuweisen, die darauf abzielt, gewünschte Ergebnisse zu erzielen \\- z. B. bei der Aufnahme oder dem Ausschluss bestimmter Daten \\-, auch wenn der Begriff technisch gesehen nicht von Natur aus wertend ist.",
                "Related_terms": "** Analytic Flexibility; Garden of forking paths; Model uncertainty; Multiverse analysis; *P*\\-hacking; Robustness (analyses); Specification curve analysis"
            },
            {
                "Title": "RepliCATs project *",
                "Definition": "** Collaborative Assessment for Trustworthy Science. The repliCATS project’s aim is to crowdsource predictions about the reliability and replicability of published research in eight social science fields: business research, criminology, economics, education, political science, psychology, public administration, and sociology.",
                "Reference": "** Fraser et al.(2021); [https://replicats.research.unimelb.edu.au/](https://replicats.research.unimelb.edu.au/)",
                "Originally drafted by": "** Tamara Kalandadze",
                "Reviewed (or Edited) by": "** Sarah Ashcroft-Jones; Mahmoud Elsherif; Gilad Feldman; Helena Hartmann; Charlotte R. Pennington",
                "Translated by": "Helena Hartmann",
                "Translation reviewed by": "Susanne Vogel, Jennifer Mattschey",
                "Translation": "Kollaborative Bewertung für vertrauenswürdige Wissenschaft (aus dem Engl. Collaborative Assessment for Trustworthy Science). Ziel des repliCATS-Projekts ist es, Vorhersagen über die Zuverlässigkeit und Replizierbarkeit veröffentlichter Forschung in acht sozialwissenschaftlichen Bereichen zu treffen: Volkswirtschaft, Kriminologie, Betriebswirtschaft, Bildung, Politikwissenschaft, Psychologie, öffentliche Verwaltung und Soziologie.",
                "Related_terms": "** Replicability; Trustworthiness"
            },
            {
                "Title": "Responsible Research and Innovation (Verantwortungsvolle Forschung und Innovation) *",
                "Definition": "** An approach that considers societal implications and expectations, relating to research and innovation, with the aim to foster inclusivity and sustainability. It accounts for the fact that scientific endeavours are not isolated from their wider effects and that research is motivated by factors beyond the pursuit of knowledge. As such, many parties are important in fostering responsible research, including funding bodies, research teams, stakeholders, activists, and members of the public.",
                "Reference(s)": "** European Commission (2021)",
                "Drafted by": "** Ana Barbosa Mendes",
                "Reviewed (or Edited) by": "** Helena Hartmann; Joanne McCuaig; Sam Parsons; Graham Reid",
                "Translated by": "Susanne Vogel",
                "Translation reviewed by": "Helena Hartmann, Jennifer Mattschey",
                "Translation": "Ein Ansatz, der die gesellschaftlichen Auswirkungen und Erwartungen in Bezug auf Forschung und Innovation berücksichtigt, mit dem Ziel, Inklusivität und Nachhaltigkeit zu fördern. Er trägt der Tatsache Rechnung, dass wissenschaftliche Bemühungen nicht von ihren breiteren Auswirkungen isoliert sind und dass die Forschung durch Faktoren motiviert wird, die über das Streben nach Wissen hinausgehen. Bei der Förderung einer verantwortungsvollen Forschung spielen daher viele Parteien eine wichtige Rolle, darunter Drittmittelgeber:innen, Forschungsteams, Interessensgruppen, Aktivist:innen und Mitglieder der Öffentlichkeit.",
                "Related_terms": "** Citizen Science; Public Engagement; Transdisciplinary Research"
            },
            {
                "Title": "Reverse p-hacking *",
                "Definition": "** Exploiting researcher degrees of freedom during statistical analysis in order to increase the likelihood of accepting the null hypothesis (for instance, *p* \\> .05).",
                "Reference": "** Chuard et al. (2019)",
                "Originally drafted by": "** Robert M. Ross",
                "Reviewed (or Edited) by": "** Mahmoud Elsherif; Alexander Hart; Sam Parsons; Timo Roettger",
                "Translated by": "Helena Hartmann",
                "Translation reviewed by": "Susanne Vogel, Jennifer Mattschey",
                "Translation": "Ausnutzung der Freiheitsgrade des Forschenden bei der statistischen Analyse, um die Wahrscheinlichkeit zu erhöhen, dass die Nullhypothese beibehalten wird (z. B. *p* \\> .05).",
                "Related_terms": "** Analytic flexibility; HARKing; P-hacking; Questionable Research Practices or Questionable Reporting Practices (QRPs); Researcher degrees of freedom; Selective reporting"
            },
            {
                "Title": "RIOT Science Club *",
                "Definition": "** The RIOT Science Club is a multi-site seminar series that raises awareness and provides training in Reproducible, Interpretable, Open & Transparent science practices. It provides regular talks, workshops and conferences, all of which are openly available and rewatchable on the respective location’s websites and Youtube.",
                "Reference": "** [http://riotscience.co.uk/](http://riotscience.co.uk/)",
                "Originally drafted by": "** Tamara Kalandadze",
                "Reviewed (or Edited) by": "** Helena Hartmann; Emma Henderson; Joanne McCuaig; Flávio Azevedo",
                "Translated by": "Helena Hartmann",
                "Translation reviewed by": "Susanne Vogel, Jennifer Mattschey",
                "Translation": "Der RIOT Science Club ist eine standortübergreifende Seminarreihe, die das Bewusstsein für reproduzierbare, interpretierbare, offene und transparente wissenschaftliche Praktiken schärft und Schulungen dazu anbietet. Es werden regelmäßig Vorträge, Workshops und Konferenzen angeboten, die alle offen zugänglich sind und auf den Websites der jeweiligen Standorte und auf Youtube nachverfolgt werden können.",
                "Related_terms": "** Early career researchers (ECRs); Interpretability; Openness; Reproducibility; Transparency"
            },
            {
                "Title": "Robustness (analyses) (Robustheit(-sanalysen)) *",
                "Definition": "** The persistence of support for a hypothesis under perturbations of the methodological/analytical pipeline. In other words, applying different methods/analysis pipelines to examine if the same conclusion is supported under analytical different conditions.",
                "Reference(s)": "** Goodman et al. (2016) (alternative); Nosek and Errington (2020)",
                "Drafted by": "** Tina Lonsdorf; Flávio Azevedo",
                "Reviewed (or Edited) by": "** Gilad Feldman; Adrien Fillon; Helena Hartmann; Timo Roettger",
                "Translated by": "Helena Hartmann",
                "Translation reviewed by": "Susanne Vogel, Jennifer Mattschey  ### **Looking for something else?**  We have separated the glossary project across several documents, see links below:  Landing page:\t\t\t[Glossary Translations German template](https://docs.google.com/document/d/1IIZK-F9SX1P4UrPlZeEKgUAOyiEjNbGFSWafr0ADb0M/edit) Letters A \\- F:\t\t\t[German Glossary | Phase 2 | A-F](https://docs.google.com/document/d/1yQH_iYm7FgjVGtJyQWGv7iv1kt4GkUWeRbXHch57Fuo/edit) Letters G \\- L:\t\t\t[German Glossary | Phase 2 | G-L](https://docs.google.com/document/d/1MbfcDK3G6ybzkkq2jVz36q4TcqQMGpNG2cun6GOeuSQ/edit#) Letters M \\- R: \t\t\t[German Glossary | Phase 2 | M - R](https://docs.google.com/document/d/1sv2C1Y-z3WeiYjvhn2B8rhFyPK5YQ8G3sCCh3ZYwV2Q/edit#heading=h.w0bgiwj800db) Letters S \\- Z:\t\t\t[German Glossary | Phase 2 | S - Z](https://docs.google.com/document/d/1pORanWNHkMRkGYs8vNxdetx907gBv-fm8l-tV8XRIiE/edit) References: \t\t\t[Glossary | Phase 2 | References](https://docs.google.com/document/d/12_F8kbnl2GP66iBkIejJeX2KnkZ13iC_jj7VVYZMxpA/edit?usp=sharing) Terms not yet defined: \t\t[Glossary | Phase 2 | Terms not yet defined](https://docs.google.com/document/d/16FGodUkNoNrBYyq2JqHJMNzYuZf-QbsigooMpB_ns_E/edit?usp=sharing) Contributors spreadsheet: [Glossary Translations German tenzing contributions \\[FORRT\\]](https://docs.google.com/spreadsheets/d/1UEM7s27b5pOlrIYX9-fXYdPOmPpZZyoggJOfOIrOhko/edit#gid=0)  [FORRT slack](https://join.slack.com/t/forrt/shared_invite/zt-alobr3z7-NOR0mTBfD1vKXn9qlOKqaQ) \\- join us\\! [FORRT email](mailto:FORRTproject@gmail.com) \\- contact us\\!  ###",
                "Translation": "Die Beständigkeit der Unterstützung für eine Hypothese bei Störungen der methodischen/analytischen Pipeline. Mit anderen Worten: Anwendung verschiedener Methoden/Analyseverfahren, um zu prüfen, ob dieselbe Schlussfolgerung unter verschiedenen analytischen Bedingungen unterstützt wird.",
                "Related_terms": "** Many Labs; Multiverse analysis; Sensitivity analyses; Specification Curve Analysis **Alternative definition:** “Robustness refers to the stability of experimental conclusions to variations in either baseline assumptions or experimental procedures. It is somewhat related to the concept of generalizability (also known as transportability), which refers to the persistence of an effect in settings different from and outside of an experimental framework \\[...\\] Whether a study design is similar enough to the original to be considered a replication, a “robustness test,” or some of many variations of pure replication that have been identified, particularly in the social sciences (for example, conceptual replication, pseudoreplication), is an unsettled question” (Goodman et al., 2016). \\[GERMAN:\\] Robustheit bezieht sich auf die Stabilität der experimentellen Schlussfolgerungen gegenüber Variationen in den Grundannahmen oder experimentellen Verfahren. Sie ist in gewisser Weise mit dem Konzept der Verallgemeinerbarkeit (auch als Transportierbarkeit bezeichnet) verwandt, das sich auf die Persistenz eines Effekts in anderen Umgebungen und außerhalb eines experimentellen Rahmens bezieht \\[...\\] Ob ein Studiendesign dem Original ähnlich genug ist, um als Replikation, als \"Robustheitstest\" oder als eine der vielen Varianten der reinen Replikation zu gelten, die insbesondere in den Sozialwissenschaften identifiziert wurden (z. B. konzeptionelle Replikation, Pseudoreplikation), ist eine ungeklärte Frage (aus dem Engl. “Robustness refers to the stability of experimental conclusions to variations in either baseline assumptions or experimental procedures. It is somewhat related to the concept of generalizability (also known as transportability), which refers to the persistence of an effect in settings different from and outside of an experimental framework \\[...\\] Whether a study design is similar enough to the original to be considered a replication, a “robustness test,” or some of many variations of pure replication that have been identified, particularly in the social sciences (for example, conceptual replication, pseudoreplication), is an unsettled question”, Goodman et al., 2016)."
            },
            {
                "Title": "Salami slicing (Salamischneiden) *",
                "Definition": "** A questionable research/reporting practice strategy, often done *post hoc*, to increase the number of publishable manuscripts by ‘slicing’ up the data from a single study \\- one example of a method of ‘gaming the system’ of academic incentives. For instance, this may involve publishing multiple studies based on a single dataset, or publishing multiple studies from different data collection sites without transparently stating where the data originally derives from. Such practices distort the literature, and particularly meta-analyses, because it is unclear that the findings were obtained from the same dataset, thereby concealing the dependencies across the separately published papers.",
                "Reference(s)": "** Fanelli (2018)",
                "Drafted by": "** Mahmoud Elsherif",
                "Reviewed (or Edited) by": "** Sarah Ashcroft-Jones; Adrien Fillon; Helena Hartmann; Matt Jaquiery; Tamara Kalandadze; Charlotte R. Pennington; Graham Reid; Suzanne L. K. Stewart",
                "Translated by": "Bettina M.J. Kern",
                "Translation reviewed by": "Helena Hartmann, Susanne Vogel; Joris Frese",
                "Translation": "Eine fragwürdige Forschungspraktik mit dem Ziel, im Nachhinein einer Datenerhebung (post-hoc) die Anzahl an Publikationen zu erhöhen. Beispielsweise werden die Daten einer einzelnen Erhebung mehrmals für unterschiedliche Publikationen in unterschiedlichen Zeitschriften verwendet, ohne dass dieser Zusammenhang zwischen den Publikationen und ihren Ergebnissen hinreichend transparent gemacht wird. Ein Datensatz (die *Salami*) wird also in mehrere “Publikationsscheiben” zerschnitten (*slicing*). Dieses Vorgehen führt zu einer Verzerrung in der Forschungsliteratur zu einem Thema, da es eine höhere Menge an unabhängigen Forschungsbefunden vortäuscht. Das wirkt sich auch stark auf die Ergebnisse von Meta-Analysen aus, bei denen davon ausgegangen wird, dass die einzelnen Effektmaße in den einzelnen Publikationen von unabhängigen Stichproben stammen. Dies ist bei Salami Slicing nicht der Fall.",
                "Related_terms": "** Gaming (the system); Questionable Research Practices or Questionable Reporting Practices (QRPs); Partial publication"
            },
            {
                "Title": "Scooping (Untergrabung) *",
                "Definition": "** The act of reporting or publishing a novel finding prior to another researcher/team. Survey-based research indicates that fear of being scooped is an important fear-related barrier for data sharing in psychology, and agent-based models suggest that competition for priority harms scientific reliability (Tiokhin et al. 2021).",
                "Reference(s)": "** Houtkoop et al. (2018); Laine (2017); Tiokhin et al. (2021)",
                "Drafted by": "** William Ngiam",
                "Reviewed (or Edited) by": "** Ashley Blake; Thomas Rhys Evans; Connor Keating; Graham Reid; Timo Roettger; Robert M. Ross; Flávio Azevedo",
                "Translated by": "Susanne Vogel",
                "Translation reviewed by": "Helena Hartmann; Joris Frese",
                "Translation": "Der Akt der Berichterstattung über oder Veröffentlichung eines neuen Ergebnisses vor anderen Forschenden/Teams. Umfragebasierte Untersuchungen deuten darauf hin, dass die Furcht davor, untergraben (scooped) zu werden, ein wichtiges angstbezogenes Hindernis für die gemeinsame Nutzung von Daten in der Psychologie ist, und agentenbasierte Modelle deuten darauf hin, dass der Wettbewerb um Prioritäten die wissenschaftliche Reliabilität beeinträchtigt (Tiokhin et al. 2021).",
                "Related_terms": "** Novelty; Open data; Preregistration"
            },
            {
                "Title": "Semantometrics (Semantometrie) *",
                "Definition": "** A class of metrics for evaluating research using full publication text to measure semantic similarity of publications and highlighting an article’s contribution to the progress of scholarly discussion. It is an extension of tools such as bibliometrics, webometrics, and altmetrics.",
                "Reference": "** Herrmannova and Knoth (2016); Knoth and Herrmannova (2014)",
                "Originally drafted by": "** Alaa AlDoh",
                "Reviewed (or Edited) by": "** Jamie P. Cockcroft; Mahmoud Elsherif; Christopher Graham; Charlotte R. Pennington",
                "Translated by": "Susanne Vogel",
                "Translation reviewed by": "Helena Hartmann; Joris Frese",
                "Translation": "Eine Klasse von Metriken zur Bewertung von Forschung unter Verwendung des vollständigen Veröffentlichungstextes zur Messung der semantischen Ähnlichkeit von Veröffentlichungen und zur Hervorhebung des Beitrags eines Artikels zum Fortschritt der wissenschaftlichen Diskussion. Sie ist eine Erweiterung von Instrumenten wie Bibliometrie, Webometrie und Altmetrik.",
                "Related_terms": "** Bibliometrics; Contribution(p)"
            },
            {
                "Title": "Sensitive research (Sensitive Forschung) *",
                "Definition": "** Research that poses a threat to those who are or have been involved in it, including the researchers, the participants, and the wider society. This threat can be physical danger (e.g. suicide) or a negative emotional response (e.g. depression) to those who are involved in the research process. For instance, research conducted on victims of suicide, the researcher might be emotionally traumatised by the descriptions of the suicidal behaviours. Indeed, the communication with the victims might also make them re-experience the traumatic memories, leading to negative psychological responses.",
                "Reference": "** Lee (1993); Albayrak-Aydemir (2019)",
                "Originally drafted by": "** Nihan Albayrak-Aydemir",
                "Reviewed (or Edited) by": "** Valeria Agostini; Mahmoud Elsherif; Helena Hartmann; Graham Reid",
                "Translated by": "Susanne Vogel",
                "Translation reviewed by": "Helena Hartmann; Joris Frese",
                "Translation": "Forschung, die eine Bedrohung für diejenigen darstellt, die daran beteiligt sind oder waren, einschließlich der Forschenden, der Versuchspersonen und der breiteren Gesellschaft. Diese Bedrohung kann eine physische Gefahr (z. B. Selbstmord) oder eine negative emotionale Reaktion (z. B. Depression) für die am Forschungsprozess Beteiligten sein. Bei Forschungsarbeiten, die an Opfern von Selbstmord durchgeführt werden, könnten Forschende beispielsweise durch die Beschreibungen des suizidalen Verhaltens emotional traumatisiert werden. Die Kommunikation mit den Opfern könnte auch dazu führen, dass sie die traumatischen Erinnerungen wiedererleben, was zu negativen psychischen Reaktionen führt.",
                "Related_terms": "** Anonymity"
            },
            {
                "Title": "Sequence-determines-credit approach (SDC) *",
                "Definition": "** An authorship system that assigns authorship order based on the contribution of each author. The names of the authors are listed according to their contribution in descending order with the most contributing author first and the least contributing author last.",
                "Reference": "** Schmidt (1987); Tscharntke et al. (2007)",
                "Originally drafted by": "** Myriam A. Baum",
                "Reviewed (or Edited) by": "** Sam Parsons; Charlotte R. Pennington",
                "Translated by": "Helena Hartmann",
                "Translation reviewed by": "Susanne Vogel; Joris Frese",
                "Translation": "Ein Autor:innenschaftssystem, das die Reihenfolge der Autor:innenschaft auf der Grundlage des Beitrags der einzelnen Autor:innen festlegt. Die Namen der Autor:innen werden entsprechend ihres Beitrags in absteigender Reihenfolge aufgeführt, wobei der/die Autor:in mit dem größten Beitrag an erster Stelle und der/die Autor:in mit dem geringsten Beitrag an letzter Stelle steht.",
                "Related_terms": "** Authorship; First-last-author-emphasis norm (FLAE)"
            },
            {
                "Title": "Sherpa Romeo *",
                "Definition": "** An online resource that collects and presents open access policies from publishers, from across the world, providing summaries of individual journal's copyright and open access archiving policies.",
                "Reference": "** [https://v2.sherpa.ac.uk/romeo/](https://v2.sherpa.ac.uk/romeo/)",
                "Originally drafted by": "** Aleksandra Lazić",
                "Reviewed (or Edited) by": "** Mahmoud Elsherif; Christopher Graham; Sam Parsons; Martin Vasilev",
                "Translated by": "Bettina M. J. Kern",
                "Translation reviewed by": "Helena Hartmann, Susanne Vogel; Joris Frese",
                "Translation": "Eine Online-Ressource, die die Open-Access-Richtlinien von wissenschaftlichen Verlagen aus der ganzen Welt sammelt und Zusammenfassungen der Urheberrechts- und Open-Access-Archivierungsrichtlinien der einzelnen Zeitschriften bereitstellt.",
                "Related_terms": "** Embargo period; Open access; Paywall; Preprint; Repository"
            },
            {
                "Title": "Single-blind peer review (Einfachblinde Peer Begutachtung) *",
                "Definition": "** Evaluation of research products by qualified experts where the reviewer(s) knows the identity of the author(s), but the reviewer(s) remains anonymous to the author(s).",
                "Reference": "** Largent and Snodgrass (2016)",
                "Originally drafted by": "** Bradley Baker",
                "Reviewed (or Edited) by": "** Ashley Blake; Christopher Graham; Helena Hartmann; Graham Reid",
                "Translated by": "Susanne Vogel",
                "Translation reviewed by": "Helena Hartmann; Joris Frese",
                "Translation": "Bewertung von Forschungsergebnissen durch qualifizierte Expert:innen, wobei die Gutachter:innen die Identität der Autor:innen kennen, die Begutachtenden aber gegenüber den Autor:innen anonym bleiben.",
                "Related_terms": "** Anonymous review; Double-blind peer review; Masked review; Open Peer Review; Peer review; Triple-blind peer review"
            },
            {
                "Title": "Slow science (Langsame Forschung) *",
                "Definition": "** Adopting Open Scholarship practices leads to a longer research process overall, with more focus on transparency, reproducibility, replicability and quality, over the quantity of outputs. Slow Science opposes publish-or-perish culture and describes an academic system that allows time and resources to produce fewer higher-quality and transparent outputs, for instance prioritising researcher time towards collecting more data, more time to read the literature, think about how their findings fit the literature and documenting and sharing research materials instead of running additional studies.",
                "Reference(s)": "** http://slow-science.org/; Nelson et al., (2012); Frith (2020)",
                "Drafted by": "** Sonia Rishi",
                "Reviewed (or Edited) by": "** Adrien Fillon; Tamara Kalandadze; Sam Parsons Charlotte R. Pennington; Robert M Ross; Timo Roettger",
                "Translated by": "** Bettina M. J. Kern",
                "Translation reviewed by": "** Helena Hartmann, Susanne Vogel; Joris Frese",
                "Translation": "Open Scholarship-Praktiken umzusetzen führt zu einem längeren Forschungsprozess, bei dem mehr Wert auf Transparenz, Reproduzierbarkeit, Replizierbarkeit und Qualität als auf die Quantität der Ergebnisse gelegt wird. Langsame Forschung (Slow science) wendet sich gegen die \"*publish-or-perish*\"-Kultur (dt. Publizieren oder untergehen) und bezeichnet ein akademisches System, das Zeit und Ressourcen für die Produktion von hochwertigen und transparenten Ergebnissen bereitstellt, beispielsweise indem Forschende mehr Zeit für Datenerhebung, für das Lesen der Literatur, für das Nachdenken darüber, wie ihre Ergebnisse zur bestehenden Literatur passen, und für die Dokumentation, Bereitstellung und Austausch von Forschungsmaterialien aufwenden, anstatt zusätzliche Studien durchzuführen.",
                "Related_terms": "** collaboration; Incentive structure; Publish or Perish; research culture; research quality"
            },
            {
                "Title": "Society for Open, Reliable, and Transparent Ecology and Evolutionary biology (SORTEE) *",
                "Definition": "** SORTEE ([https://www.sortee.org/](https://www.sortee.org/)) is an international society with the aim of improving the transparency and reliability of research results in the fields of ecology, evolution, and related disciplines through cultural and institutional changes. SORTEE was launched in December 2020 to anyone interested in improving research in these disciplines, regardless of experience. The society is international in scope, membership, and objectives. As of May 2021, SORTEE comprises of over 600 members.",
                "Reference(s)": "** https://www.sortee.org/",
                "Drafted by": "** Brice Beffara Bret; Dominique Roche",
                "Reviewed (or Edited) by": "** Sarah Ashcroft-Jones; Mahmoud Elsherif; Charlotte R. Pennington; Graham Reid",
                "Translated by": "Helena Hartmann",
                "Translation reviewed by": "Susanne Vogel; Joris Frese",
                "Translation": "SORTEE (https://www.sortee.org/) ist eine internationale Gesellschaft mit dem Ziel, die Transparenz und Zuverlässigkeit von Forschungsergebnissen in den Bereichen Ökologie, Evolution und verwandten Disziplinen durch kulturelle und institutionelle Veränderungen zu verbessern. SORTEE wurde im Dezember 2020 gegründet und richtet sich an alle, die an der Verbesserung der Forschung in diesen Disziplinen interessiert sind, unabhängig von ihrer Erfahrung. Die Gesellschaft ist in Bezug auf Umfang, Mitgliedschaft und Ziele international. SORTEE umfasst über 600 Mitglieder (Stand Mai, 2021).",
                "Related_terms": "** Society for the Improvement of Psychological Science (SIPS)"
            },
            {
                "Title": "Society for the Improvement of Psychological Science (SIPS) *",
                "Definition": "** A membership society founded to further promote improved methods and practices in the psychological research field. The society aims to complete its mission statement by enhancing the training of psychological researchers; by promoting research cultures that are more conducive to better quality research; by quantifying and empirically assessing the impact of such reforms; and by leading outreach events within and outside psychology to better the current state of research norms.",
                "Reference(s)": "** https://improvingpsych.org/",
                "Drafted by": "** Mahmoud Elsherif",
                "Reviewed (or Edited) by": "** Ashley Blake; Jade Pickering; Graham Reid; Flávio Azevedo",
                "Translated by": "Bettina MJ Kern",
                "Translation reviewed by": "Helena Hartmann, Susanne Vogel; Joris Frese",
                "Translation": "Die SIPS ist eine Mitgliedsgesellschaft und wurde mit dem Ziel gegründet, bessere Praktiken und Methoden im Bereich der psychologischen Forschung zu fördern. Diese Mission möchte die SIPS durch verschiedene Initiativen, Maßnahmen und Kooperationen innerhalb und außerhalb der Psychologie erfüllen. Diese sollen dazu beitragen, die wissenschaftliche Ausbildung von Forschenden und Studierenden zu verbessern und eine Forschungskultur zu verbreiten, die eine bessere Qualität der Forschung ermöglicht. Zudem geht es auch darum, die Auswirkungen solcher Reformen empirisch zu bewerten und zu quantifizieren.",
                "Related_terms": "** Society for Open, Reliable, and Transparent Ecology and Evolutionary biology (SORTEE)"
            },
            {
                "Title": "Social class (Soziale Schicht) *",
                "Definition": "** Social class is usually measured using both objective and subjective measurements, as recommended by the American Psychological Association (American Psychological Association,Task Force on Socioeconomic Status, 2007). Unlike the conventional concept, which only considers one factor, either education or income (e.g., economic variables), an individual's social class is considered to be a combination of their education, income, occupational prestige, subjective social status, and self-identified social class. Social class is partly a cultural variable, as it is a stable variable and likely to change slowly over the years. Social class can have important implications to academic outcomes. An individual may have a high socio-economic status yet identify as a working class individual. Working class students tend to have different life circumstances and often more restrictive commitments than middle-class students, which make their integration with other students more difficult (Rubin, 2021). The lack of time and money is obstructive to their social experience at university. Working class students are more likely to work to support themselves, resulting in less time for academic activities and for socializing with other students as well as less money to purchase items linked to social experiences (e.g. food).",
                "Reference(s)": "** Evans and Rubin (2021); Rubin et al. (2019); Rubin (2021); Saegert et al. (2007)",
                "Drafted by": "** Mahmoud Elsherif",
                "Reviewed (or Edited) by": "** Leticia Micheli; Eliza Woodward; Julika Wolska; Gerald Vineyard**;** Yu-Fang Yang",
                "Translated by": "Helena Hartmann",
                "Translation reviewed by": "Susanne Vogel; Joris Frese",
                "Translation": "Die soziale Schicht wird in der Regel sowohl durch objektive als auch durch subjektive Messungen erfasst, wie von der American Psychological Association empfohlen (American Psychological Association, Task Force on Socioeconomic Status, 2007). Im Gegensatz zum herkömmlichen Konzept, bei dem nur ein Faktor, entweder Bildung oder Einkommen (d. h. wirtschaftliche Variablen), berücksichtigt wird, wird die soziale Schicht einer Person als eine Kombination aus Bildung, Einkommen, beruflichem Prestige, subjektivem sozialem Status und selbst identifizierter sozialer Klasse betrachtet. Die soziale Schicht ist zum Teil eine kulturelle Variable, da es sich um eine stabile Variable handelt, die sich im Laufe der Jahre wahrscheinlich nur langsam verändert. Die soziale Schicht kann wichtige Auswirkungen auf die schulischen Leistungen haben. Eine Person kann einen hohen sozioökonomischen Status haben und sich dennoch als Arbeiter bezeichnen. Studierende mit nicht-akademischem Hintergrund haben in der Regel andere Lebensumstände und oft restriktivere Verpflichtungen als Studierende aus der Mittelschicht, was ihre Integration mit anderen Studierenden erschwert (Rubin, 2021). Der Mangel an Zeit und Geld behindert ihre sozialen Erfahrungen an der Universität. Studierende mit nicht-akademischem Hintergrund müssen eher arbeiten, um ihren Lebensunterhalt zu bestreiten, was dazu führt, dass sie weniger Zeit für akademische Aktivitäten und soziale Kontakte mit anderen Studierenden und weniger Geld für den Kauf von Waren haben, die mit sozialen Erfahrungen verbunden sind (z. B. Essen).",
                "Related_terms": "** Social integration"
            },
            {
                "Title": "Social integration (Soziale Integration) *",
                "Definition": "** Social integration is a multi-dimensional construct. In an academic context, social integration is related to the quantity and quality of the social interactions with staff and students, as well as the sense of connection and belonging to the university and the people within the institute. To be more specific, social support, trust, and connectedness are all variables that contribute to social integration. Social integration has important implications for academic outcomes and mental wellbeing (Evans & Rubin, 2021). Working class students are less likely to integrate with other students, since they have differing social and economic backgrounds and less disposable income. Thus they are not able to experience as many educational and fiscal opportunities than others. In turn, this can lead to poor mental health and feelings of ostracism (Rubin, 2021).",
                "Reference(s)": "** Evans and Rubin (2021); Rubin et al. (2019); Rubin (2021)",
                "Drafted by": "** Mahmoud Elsherif",
                "Reviewed (or Edited) by": "** Leticia Micheli; Eliza Woodward; Julika Wolska**;** Gerald Vineyard; Yu-Fang Yang; Flávio Azevedo",
                "Translated by": "Susanne Vogel",
                "Translation reviewed by": "Helena Hartmann; Joris Frese",
                "Translation": "Soziale Integration ist ein mehrdimensionales Konstrukt. Im akademischen Kontext bezieht sich die soziale Integration auf die Quantität und Qualität der sozialen Interaktionen mit Mitarbeitenden und Studierenden sowie auf das Gefühl der Verbundenheit und Zugehörigkeit zur Universität und zu den Menschen innerhalb des Instituts. Genauer gesagt, sind soziale Unterstützung, Vertrauen und Verbundenheit allesamt Variablen, die zur sozialen Integration beitragen. Die soziale Integration hat wichtige Auswirkungen auf die akademischen Ergebnisse und das psychische Wohlbefinden (Evans & Rubin, 2021). Bei Studierenden mit nicht-akademischem Hintergrund ist die Wahrscheinlichkeit geringer, dass sie sich mit anderen Studierenden zusammenschließen, da sie einen unterschiedlichen sozialen und ökonomischen Hintergrund und ein geringeres verfügbares Einkommen haben. Daher können sie nicht so viele Bildungsmöglichkeiten und finanzielle Hilfen in Anspruch nehmen wie andere. Dies wiederum kann zu einer schlechten psychischen Gesundheit und dem Gefühl der Ausgrenzung führen (Rubin, 2021).",
                "Related_terms": "** Social class"
            },
            {
                "Title": "Specification Curve Analysis (Spezifikationskurvenanalyse) *",
                "Definition": "** An analytic approach that consists of identifying, calculating, visualising and interpreting results (through inferential statistics) for *all* reasonable specifications for a particular research question (see Simonsohn et al. 2015). Specification curve analysis helps make transparent the influence of presumably arbitrary decisions during the scientific progress (e.g., experimental design, construct operationalization, statistical models or several of these) made by a researcher by comprehensively reporting all non-redundant, sensible tests of the research question. Voracek et al. (2019) suggest that SCA differs from multiverse analysis with regards to the graphical displays (a specification curve plot rather than a histogram and tile plot) and the use of inferential statistics to interpret findings.",
                "Reference(s)": "** Simonsohn et al. (2015); Simonsohn (2020); Voracek et al. (2019)",
                "Drafted by": "** Bradley Baker",
                "Reviewed (or Edited) by": "** Tina B. Lonsdorf; Sam Parsons; Charlotte R. Pennington",
                "Translated by": "Helena Hartmann",
                "Translation reviewed by": "Susanne Vogel; Joris Frese",
                "Translation": "Ein analytischer Ansatz, der darin besteht, alle sinnvollen Spezifikationen für eine bestimmte Forschungsfrage (mittels Inferenzstatistik) zu identifizieren, zu berechnen, zu visualisieren und zu interpretieren (siehe Simonsohn et al. 2015). Die Spezifikationskurvenanalyse hilft, den Einfluss von vermutlich willkürlichen Entscheidungen im wissenschaftlichen Verlauf (z. B. Versuchsplanung, Konstruktoperationalisierung, statistische Modelle oder mehrere davon), die Forschende getroffen haben, transparent zu machen, indem sie alle nicht redundanten, sinnvollen Tests der Forschungsfrage umfassend darstellt. Voracek et al. (2019) weisen darauf hin, dass sich die Spezifikationskurvenanalyse von der Multiversumsanalyse in Bezug auf die grafischen Darstellungen (ein Spezifikationskurvendiagramm anstelle eines Histogramms und Kacheldiagramms) und die Verwendung von Inferenzstatistiken zur Interpretation der Ergebnisse unterscheidet.",
                "Related_terms": "** Multiverse analysis; Research synthesis; Robustness (analyses); Selective reporting; Vibration of effects"
            },
            {
                "Title": "Statistical Assumptions (Statistische Vorannahmen) *",
                "Definition": "** Analytical approaches and models assume certain characteristics of one’s data (e.g., statistical independence, random samples, normality, equal variance,...). Before running an analysis, these assumptions should be checked since their violation can change the results and conclusion of a study. Good practice in open and reproducible science is to report assumption testing in terms of the assumptions verified and the results of such checks or corrections applied.",
                "Reference": "** Garson (2012); Hahn and Meeker (1993); Hoekstra et al. (2012); Nimon (2012)",
                "Originally drafted by": "** Graham Reid",
                "Reviewed (or Edited) by": "** Jamie P. Cockcroft; Sam Parsons; Martin Vasilev; Julia Wolska",
                "Translated by": "** Bettina M.J. Kern **** Helena Hartmann, Susanne Vogel; Joris Frese",
                "Translation": "Analytische Ansätze und Modelle gehen davon aus, dass die Daten, auf die sie angewendet werden, bestimmte Eigenschaften aufweisen (beispielsweise statistische Unabhängigkeit, Ziehung einer Zufallsstichprobe, Normalverteilung, Varianzhomogenität, …). Vor der Datenanalyse sollten diese Annahmen überprüft werden, da es die Ergebnisse und die Schlussfolgerung der Analyse ändern kann, wenn die Annahmen nicht erfüllt sind. Im Rahmen von offener, reproduzierbarer Forschung gehört es zur guten Praxis, das Testen der Annahmen zu dokumentieren und zu berichten, welche Annahmen geprüft wurden, durch welche Tests sie geprüft wurden, was die Ergebnisse waren und welche Maßnahmen oder Korrekturen durchgeführt wurden.",
                "Related_terms": "** Null Hypothesis Significance Testing (NHST); Statistical Significance; Statistical Validity; Transparency; Type I error; Type II error; Type M error; Type S error"
            },
            {
                "Title": "Statistical power (Statistische Teststärke) *",
                "Definition": "** Statistical power is the long-run probability that a statistical test correctly rejects the null hypothesis if the alternative hypothesis is true. It ranges from 0 to 1, but is often expressed as a percentage. Power can be estimated using the significance criterion (alpha), effect size, and sample size used for a specific analysis technique. There are two main applications of statistical power. A priori power where the researcher asks the question “given an effect size, how many participants would I need for X% power?”. Sensitivity power asks the question “given a known sample size, what effect size could I detect with X% power?”.",
                "Reference(s)": "** Carter et al. (2021); Cohen (1962); Cohen (1988); Dienes (2008); Giner-Sorolla et al. (2019); Ioannidis (2005); Lakens (2021a)",
                "Drafted by": "** Thomas Rhys Evans",
                "Reviewed (or Edited) by": "** James E. Bartlett; Jamie P. Cockcroft; Adrien Fillon; Emma Henderson; Tamara Kalandadze; William Ngiam; Catia M. Oliveira; Charlotte R. Pennington; Graham Reid; Martin Vasilev; Qinyu Xiao; Flávio Azevedo",
                "Translated by": "** Bettina M. J. Kern",
                "Translation reviewed by": "** Helena Hartmann, Susanne Vogel; Joris Frese",
                "Translation": "Die Teststärke (power), manchmal auch als statistische Aussagekraft bezeichnet, ist die langfristige Wahrscheinlichkeit, dass die Alternativhypothese zutrifft und ein statistischer Test korrekterweise die Nullhypothese verwirft. Die Teststärke geht von 0 bis 1, wird häufig aber als Prozentzahl ausgedrückt. Berechnet wird sie aus dem gewählten Signifikanzniveau, der Effektstärke und der Stichprobengröße in Abhängigkeit der jeweiligen Analysemethode. Es gibt zwei Hauptanwendungsgebiete: Bei der a-priori Teststärke geht es um die Frage, wie groß die Stichprobe sein muss, damit ein Effekt mit X-prozentiger langfristiger Wahrscheinlichkeit auch tatsächlich entdeckt wird. Die Sensitivitäts-Teststärke  dreht sich um die Frage, welche Effektgröße man finden kann, wenn eine bestimmte Stichprobengröße gegeben ist.",
                "Related_terms": "** Effect Size; Meta-analysis; Null Hypothesis Significance Testing (NHST); Power Analysis; Positive Predictive Value; Quantitative research; Sample size; Significance criterion (alpha); Type I error; Type II error **Related terms to alternative definition:** Type II Error"
            },
            {
                "Title": "Statistical significance (Statistische Signifikanz) *",
                "Definition": "** A property of a result using Null Hypothesis Significance Testing (NHST) that, given a significance level, is deemed unlikely to have occurred given the null hypothesis. Tenny and Abdelgawad (2017) defined it as “a measure of the probability of obtaining your data or more extreme data assuming the null hypothesis is true, compared to a pre-selected acceptable level of uncertainty regarding the true answer” (p. 1). Conventions for determining the threshold vary between applications and disciplines but ultimately depend on the considerations of the researcher about an appropriate error margin. The American Statistical Association’s statement (Wasserstein & Lazar, 2016\\) notes that “Researchers often wish to turn a p-value into a statement about the truth of a null hypothesis, or about the probability that random chance produced the observed data. The p-value is neither. It is a statement about data in relation to a specified hypothetical explanation, and is not a statement about the explanation itself” (p. 131).",
                "Reference(s)": "** Cassidy et al. (2019); Tenny and Abdelgawad (2021); Wasserstein and Lazar (2016)",
                "Drafted by": "** Alaa AlDoh; Flávio Azevedo",
                "Reviewed (or Edited) by": "** James E. Bartlett; Alexander Hart**;** Annalise A. LaPlume; Charlotte R. Pennington; Graham Reid; Timo Roettger; Suzanne L. K. Stewart",
                "Translated by": "Susanne Vogel",
                "Translation reviewed by": "Helena Hartmann; Joris Frese",
                "Translation": "Eine Eigenschaft eines Ergebnisses unter Verwendung von Nullhypothesen-Signifikanztestung (Null Hypothesis Significance Testing, NHST), das gegeben eines Signifikanzniveaus als unwahrscheinlich angesehen wird wenn die Nullhypothese gilt. Tenny und Abdelgawad (2017) definierten sie als \"a measure of the probability of obtaining your data or more extreme data assuming the null hypothesis is true, compared to a pre-selected acceptable level of uncertainty regarding the true answer” (dt. ein Maß für die Wahrscheinlichkeit, die vorliegenden Daten oder extremere Daten zu erhalten, unter der Annahme, dass die Nullhypothese wahr ist, im Vergleich zu einem im Voraus gewählten akzeptierbaren Unsicherheitsgrad hinsichtlich der wahren Antwort\" (S. 1). Die Konventionen zur Bestimmung des Schwellenwerts variieren je nach Anwendung und Disziplin, hängen aber letztlich von den Überlegungen der Forschenden über eine angemessene Fehlerquote ab. In der Erklärung der American Statistical Association (Wasserstein & Lazar, 2016\\) heißt es: \"Researchers often wish to turn a *p*\\-value into a statement about the truth of a null hypothesis, or about the probability that random chance produced the observed data. The *p*\\-value is neither. It is a statement about data in relation to a specified hypothetical explanation, and is not a statement about the explanation itself” (dt. Forschende wollen einen *p*\\-Wert oft in eine Aussage über die Wahrheit einer Nullhypothese oder über die Wahrscheinlichkeit verwandeln, dass die beobachteten Daten durch Zufall entstanden sind. Der *p*\\-Wert ist weder das eine noch das andere. Er ist eine Aussage über die Daten in Bezug auf eine bestimmte hypothetische Erklärung und ist keine Aussage über die Erklärung selbst\" (S. 131).",
                "Related_terms": "** Alpha error; Frequentist statistics; Null hypothesis; Null Hypothesis Significance Testing (NHST); *P*\\-value; Type I error **Incorrect definition:** Statistical significance describes the likelihood of the observed result against chance (regardless of the null hypotheses)"
            },
            {
                "Title": "Statistical validity (Statistische Validität) *",
                "Definition": "** The extent to which conclusions from a statistical test are accurate and reflective of the true effect found in nature. In other words, whether or not a relationship exists between two variables and can be accurately detected with the conducted analyses. Threats to statistical validity include low power, violation of assumptions, reliability of measures, etc, affecting the reliability and generality of the conclusions.",
                "Reference(s)": "** Cook and Campbell (1979); Drost (2011)",
                "Drafted by": "** Annalise A. LaPlume",
                "Reviewed (or Edited) by": "** Jamie P. Cockcroft, Zoltan Kekecs; Graham Reid",
                "Translated by": "Helena Hartmann",
                "Translation reviewed by": "Susanne Vogel; Joris Frese",
                "Translation": "Das Ausmaß, in dem die Schlussfolgerungen aus einem statistischen Test akkurat sind und den in der Natur vorkommenden wahren Effekt widerspiegeln. Mit anderen Worten, ob ein Zusammenhang zwischen zwei Variablen besteht und mit den durchgeführten Analysen genau festgestellt werden kann. Zu den Gefahren für die statistische Validität gehören eine geringe Power (Teststärke), die Verletzung von Annahmen, die Zuverlässigkeit von Messungen usw., die die Zuverlässigkeit und Allgemeinheit der Schlussfolgerungen beeinträchtigen.",
                "Related_terms": "** Power; Validity; Statistical assumptions"
            },
            {
                "Title": "STRANGE *",
                "Definition": "** The STRANGE “framework” is a proposal and series of questions to help animal behaviour researchers consider sampling biases when planning, performing and interpreting research with animals. STRANGE is an acronym highlighting several possible sources of sampling bias in animal research, such as the animals’ Social background; Trappability and self-selection; Rearing history; Acclimation and habituation; Natural changes in responsiveness; Genetic make-up, and Experience.",
                "Reference(s)": "** Webster and Rutz (2020)",
                "Drafted by": "** Mahmoud Elsherif",
                "Reviewed (or Edited) by": "** Ben Farrar; Zoe Flack; Elias Garcia-Pelegrin; Charlotte R. Pennington; Graham Reid",
                "Translated by": "Helena Hartmann",
                "Translation reviewed by": "Susanne Vogel; Joris Frese  ### ---",
                "Translation": "Das STRANGE \"Framework\" ist ein Vorschlag und eine Reihe von Fragen, die Tierverhaltensforschenden helfen sollen, Stichprobenverzerrungen bei der Planung, Durchführung und Interpretation von Forschungsarbeiten mit Tieren zu berücksichtigen. STRANGE ist ein Akronym, das verschiedene mögliche Quellen für Stichprobenverzerrungen in der Tierforschung hervorhebt, wie z. B. den Sozialen Hintergrund der Tiere, die Einfangbarkeit (Trappability) und Selbstselektion, die Aufzuchtgeschichte (Rearing), die Akklimatisierung und Gewöhnung, Natürliche Veränderungen der Reaktionsfähigkeit, die Genetische Ausstattung und die Erfahrung.",
                "Related_terms": "** Bias; Constraints on Generality (COG); Populations; Sampling bias; WEIRD"
            },
            {
                "Title": "StudySwap *",
                "Definition": "** A free online platform through which researchers post brief descriptions of research projects or resources that are available for use (“haves”) or that they require and another researcher may have (“needs”). StudySwap is a crowdsourcing approach to research which can ensure that fewer research resources go unused and more researchers have access to the resources they need.",
                "Reference": "** Chartier et al. (2018); [https://osf.io/view/StudySwap](https://osf.io/view/StudySwap)",
                "Originally drafted by": "** Charlotte R. Pennington",
                "Reviewed (or Edited) by": "** Mahmoud Elsherif; Helena Hartmann; Emma Henderson; Graham Reid",
                "Translated by": "Helena Hartmann",
                "Translation reviewed by": "Susanne Vogel; Joris Frese",
                "Translation": "Eine kostenlose Online-Plattform, auf der Forschende kurze Beschreibungen von Forschungsprojekten oder Ressourcen einstellen, die sie zur Nutzung freigeben können (\"haves\") oder die sie benötigen und die ein/e andere/r Forschende/r haben könnte (\"needs\"). StudySwap ist ein Crowdsourcing-Ansatz für die Forschung, der dafür sorgen kann, dass weniger Forschungsressourcen ungenutzt bleiben und mehr Forschende Zugang zu den Ressourcen haben, die sie benötigen.",
                "Related_terms": "** Collaboration; Crowdsourcing; Team science"
            },
            {
                "Title": "Systematic Review (systematisches Review) *",
                "Definition": "** A form of literature review and evidence synthesis. A systematic review will usually include a thorough, repeatable (reproducible) search strategy including key terms and databases in order to find relevant literature on a given topic or research question. Systematic reviewers follow a process of screening the papers found through their search, until they have filtered down to a set of papers that fit their predefined inclusion criteria. These papers can then be synthesised in a written review which may optionally include statistical synthesis in the form of a meta-analysis as well. A systematic review should follow a standard set of guidelines to ensure that bias is kept to a minimum for example PRISMA (Moher et al., 2009; Page et al., 2021), Cochrane Systematic Reviews (Higgins et al., 2019), or NIRO-SR (Topor et al., 2021).",
                "Reference": "** Higgins et al. (2019); Moher et al. (2009); Page et al. (2021); Topor et al. (2021)",
                "Originally drafted by": "** Jade Pickering",
                "Reviewed (or Edited) by": "** Mahmoud Elsherif; Adam Parker; Charlotte R. Pennington; Timo Roettger; Marta Topor; Emily A. Williams; Flávio Azevedo",
                "Translated by": "Helena Hartmann",
                "Translation reviewed by": "Susanne Vogel; Joris Frese  ### ---  ###  ###  ### **T** {#t}",
                "Translation": "Eine Form der Literaturübersicht und Evidenzsynthese. Ein systematisches Review umfasst in der Regel eine gründliche, wiederholbare (reproduzierbare) Suchstrategie mit Suchbegriffen und Datenbanken, um relevante Literatur zu einem bestimmten Thema oder einer Forschungsfrage zu finden. Systematische Reviewer:innen führen ein Screening der durch ihre Suche gefundenen Arbeiten durch, bis sie eine Reihe von Arbeiten herausgefiltert haben, die ihren vordefinierten Einschlusskriterien entsprechen. Diese Arbeiten können dann in einer schriftlichen Übersichtsarbeit zusammengefasst werden, die optional auch eine statistische Synthese in Form einer Meta-Analyse enthalten kann. Ein systematisches Review sollte einem standardisierten Satz von Leitlinien folgen, um sicherzustellen, dass Verzerrungen auf ein Minimum reduziert werden, z. B. PRISMA (Moher et al., 2009; Page et al., 2021), Cochrane Systematic Reviews (Higgins et al., 2019\\) oder NIRO-SR (Topor et al., 2021).",
                "Related_terms": "** Meta-analysis; CONSORT; Non-Intervention, Reproducible, and Open Systematic Reviews (NIRO-SR); PRISMA"
            },
            {
                "Title": "Tenzing *",
                "Definition": "** *Tenzing* is an online webapp and R package that helps researchers to track and report the contributions of each team member using the CRediT taxonomy in an efficient way. Team members of a research project can indicate their contributions to each CRediT role using an online spreadsheet template, and provide any additional authors' information (e.g., name, affiliation, order in publication, email address, and ORCID iD). Upon writing the manuscript, *tenzing* can automatically create a list of contributors belonging to each CRediT role to be included in the contributions section and create the manuscript’s title page.",
                "Reference(s)": "** Holcombe et al. (2020)",
                "Drafted by": "** Marton Kovacs",
                "Reviewed (or Edited) by": "** Balazs Aczel; Mahmoud Elsherif; Helena Hartmann; Charlotte R. Pennington; Graham Reid; Flávio Azevedo",
                "Translated by": "Bettina M. J. Kern",
                "Translation reviewed by": "Helena Hartmann, Susanne Vogel; Joris Frese",
                "Translation": "Tenzing* ist eine Online-Webapp und ein R-Paket, das Forschenden hilft, die Beiträge jedes Teammitglieds unter Verwendung der CRediT-Taxonomie auf effiziente Weise zu verfolgen und zu melden. Die Teammitglieder eines Forschungsprojekts können ihre Beiträge zu jeder CRediT-Rolle mithilfe einer Online-Tabelle angeben und zusätzliche Autor:innenangaben machen (z. B. Name, Institution, Reihenfolge in der Veröffentlichung, E-Mail-Adresse und ORCID iD). Somit kann man mit Tenzing automatisiert eine Liste der Mitwirkenden am Manuskript erstellen, die zu einer CRediT-Rolle beigetragen haben und in die Contributions aufgenommen werden müssen, und auch die Titelseite des Manuskripts erstellen.",
                "Related_terms": "** Authorship; Consortium authorship; Contributions; CRediT"
            },
            {
                "Title": "Theory (Theorie) *",
                "Definition": "** A theory is a unifying explanation or description of a process or phenomenon, which is amenable to repeated testing and verifiable through scientific investigation, using various experiments led by several independent researchers. Theories may be rejected or deemed an unsatisfactory explanation of a phenomenon after rigorous testing of a new hypothesis that explains the phenomena better or seems to contradict them but is more generalisable to a wider array of findings.",
                "Reference(s)": "** Schafersman (1997); Wacker (1998)",
                "Drafted by": "** Aoife O’Mahony",
                "Reviewed (or Edited) by": "** Mahmoud Elsherif; Helena Hartmann; Charlotte R. Pennington; Graham Reid",
                "Translated by": "Helena Hartmann",
                "Translation reviewed by": "Susanne Vogel; Joris Frese",
                "Translation": "Eine Theorie ist eine vereinheitlichende Erklärung oder Beschreibung eines Prozesses oder Phänomens, die wiederholt getestet werden kann und durch wissenschaftliche Untersuchungen überprüfbar ist, wobei verschiedene Experimente von mehreren unabhängigen Forschenden durchgeführt werden. Theorien können verworfen oder als unzureichende Erklärung eines Phänomens angesehen werden, nachdem eine neue Hypothese, die die Phänomene besser erklärt oder ihnen zu widersprechen scheint, aber für eine breitere Palette von Erkenntnissen verallgemeinerbar ist, einer strengen Prüfung unterzogen wurde.",
                "Related_terms": "** Hypothesis; Model (philosophy); Theory building"
            },
            {
                "Title": "Theory building (Theoriebildung) *",
                "Definition": "** The process of creating and developing a statement of concepts and their interrelationships to show how and/or why a phenomenon occurs. Theory building leads to theory testing.",
                "Reference(s)": "** Borsboom et al. (2020); Corley and Gioia (2011); Gioia and Pitrie (1990); Wacker (1998)",
                "Drafted by": "** Filip Dechterenko",
                "Reviewed (or Edited) by": "** Mahmoud Elsherif; Helena Hartmann; Charlotte R. Pennington",
                "Translated by": "Helena Hartmann",
                "Translation reviewed by": "Susanne Vogel; Joris Frese",
                "Translation": "Der Prozess der Erstellung und Entwicklung einer Erklärung von Konzepten und deren Zusammenhängen, um zu zeigen, wie und/oder warum ein Phänomen auftritt. Die Theoriebildung ermöglicht dann die Überprüfung der Theorie.",
                "Related_terms": "** Hypothesis; Model (philosophy); Theory; Theoretical contribution; Theoretical model"
            },
            {
                "Title": "The Troubling Trio (Die dreisten Drei) *",
                "Definition": "** Described as a combination of low statistical power, a surprising result, and a *p*\\-value only slightly lower than .05.",
                "Reference(s)": "** Lindsay (2015)",
                "Drafted by": "** Halil Emre Kocalar",
                "Reviewed (or Edited) by": "**; Catia M. Oliveira; Adam Parker; Sam Parsons;Charlotte R. Pennington",
                "Translated by": "Helena Hartmann",
                "Translation reviewed by": "Susanne Vogel; Joris Frese",
                "Translation": "Beschrieben als eine Kombination aus geringer statistischer Teststärke (Power), einem überraschenden Ergebnis und einem *p*\\-Wert, der nur geringfügig niedriger als .05 ist.",
                "Related_terms": "** Replication; Reproducibility; Null Hypothesis Significance Testing (NHST); *P*\\-hacking; Questionable Research Practices or Questionable Reporting Practices (QRPs)"
            },
            {
                "Title": "Transparency (Transparenz) *",
                "Definition": "** Having one’s actions open and accessible for external evaluation. Transparency pertains to researchers being honest about theoretical, methodological, and analytical decisions made throughout the research cycle. Transparency can be usefully differentiated into “scientifically relevant transparency” and “socially relevant transparency”. While the former has been the focus of early Open Science discourses, the latter is needed to provide scientific information in ways that are relevant to decision makers and members of the public (Elliott & Resnik, 2019).",
                "Reference(s)": "** Elliott and Resnik (2019); Lyon (2016); [Syed (2019)](https://psyarxiv.com/cteyb/)",
                "Drafted by": "** William Ngiam",
                "Reviewed (or Edited) by": "** Tamara Kalandadze; Aoife O’Mahony; Eike Mark Rinke; Flávio Azevedo",
                "Translated by": "Bettina M. J. Kern",
                "Translation reviewed by": "Helena Hartmann, Susanne Vogel; Joris Frese",
                "Translation": "Transparenz bedeutet, das eigene Handeln offen und zugänglich für eine externe Bewertung zu machen. Transparent zu forschen bedeutet, dass Forschende sämtliche theoretischen, methodischen und analytischen Entscheidungen nachvollziehbar offenlegen, die sie im Laufe des Forschungsprozesses getroffen haben. Transparenz kann in \"wissenschaftlich relevante Transparenz\" und \"gesellschaftlich relevante Transparenz\" unterschieden werden. Während erstere im Mittelpunkt der frühen Open-Science Diskurse stand, ist letztere erforderlich, um wissenschaftliche Informationen in einer Weise bereitzustellen, die für Entscheidungsträger und die Öffentlichkeit relevant ist",
                "Related_terms": "** Credibility of scientific claims; Open science; Preregistration; Reproducibility; Trustworthiness"
            },
            {
                "Title": "Transparency Checklist (Transparenz-Checkliste) *",
                "Definition": "** The transparency checklist is a consensus-based, comprehensive checklist that contains 36 items that cover the prepregistration, methods, results and discussion and data, code and materials availability. A shortened 12-item version of the checklist is also available. Checklist responses can be submitted alongside a manuscript for review. While the checklist can also work for educational purposes, it mainly aims to support researchers to identify concrete actions that can increase the transparency of their research while a disclosed checklist can help the readers and reviewers gain critical information about different aspects of transparency of the submitted research.",
                "Reference(s)": "** Aczel et. al. (2021)",
                "Drafted by": "** Barnabas Szaszi",
                "Reviewed (or Edited) by": "** Sarah Ashcroft-Jones; Mahmoud Elsherif; Helena Hartmann; Graham Reid; Flávio Azevedo",
                "Translated by": "Helena Hartmann",
                "Translation reviewed by": "Susanne Vogel; Joris Frese",
                "Translation": "Die Transparenz-Checkliste ist eine konsensbasierte, umfassende Checkliste mit 36 Punkten, die die Präregistrierung, Methoden, Ergebnisse und Diskussion sowie die Verfügbarkeit von Daten, Analysecode und Materialien abdeckt. Eine verkürzte Version der Checkliste mit 12 Punkten ist ebenfalls verfügbar. Die Antworten auf die Checkliste können zusammen mit einem Manuskript zur Begutachtung eingereicht werden. Die Checkliste kann zwar auch zu Bildungszwecken eingesetzt werden, zielt aber vor allem darauf ab, Forschende bei der Ermittlung konkreter Maßnahmen zur Erhöhung der Transparenz ihrer Forschung zu unterstützen, während eine offengelegte Checkliste den Lesenden und Gutachter:innen helfen kann, kritische Informationen über verschiedene Aspekte der Transparenz der eingereichten Forschungsarbeiten zu gewinnen.",
                "Related_terms": "** Credibility of scientific claims; Open science; Preregistration; Reproducibility; Trustworthiness"
            },
            {
                "Title": "Triple-blind peer review (Dreifach-blindes Peer Review) *",
                "Definition": "** Evaluation of research products by qualified experts where the author(s) are anonymous to both the reviewer(s) and editor(s). **“**Blinding of the authors and their affiliations to both editors and reviewers. This approach aims to eliminate institutional, personal, and gender biases” (Tvina et al., 2019, p. 1082).",
                "Reference(s)": "** Largent and Snodgrass (2016); Tvina et al. (2019)",
                "Drafted by": "** Mahmoud Elsherif",
                "Reviewed (or Edited) by": "** Bradley Baker; Helena Hartmann; Charlotte R. Pennington; Christopher Graham",
                "Translated by": "Bettina M.J. Kern",
                "Translation reviewed by": "Helena Hartmann, Susanne Vogel; Joris Frese",
                "Translation": "Bewertung von Forschungsergebnissen durch qualifizierte Expert:innen im Begutachtungsprozess (Peer-Review), wobei die verfassenden Personen sowohl für die Begutachtenden als auch für den/die Herausgebenden (Editor:innen) anonym bleiben. Die Verblindung der verfassenden Personen und ihrer institutionellen Zugehörigkeit sowohl gegenüber den Herausgebenden als auch den Gutachtenden zielt darauf ab, zu verhindern, dass institutionelle, persönliche und geschlechtsspezifische Vorurteile bei der Bewertung des Manuskripts eine Rolle spielen (Tvina et al., 2019, S. 1082).",
                "Related_terms": "** Double-blind peer review; Open Peer Review; Single-blind peer review"
            },
            {
                "Title": "Trim-and-fill method *",
                "Definition": "** The trim-and-fill method estimates missing studies as a result of publication bias in the funnel plot and adjusts the overall magnitude of the effect size. The studies with the most extreme effect sizes on the left and on the right side are suppressed. The number of studies excluded is conducted based on a case-by-case basis.  This method is to first trim the studies producing an asymmetrical funnel plot in order to ensure the overall magnitude of the effect size influenced by the remaining studies is not influenced by the remaining studies by publication bias. However, if there is no true overall effect size, it is infeasible to identify the studies to be trimmed at the first step. Following this, the missing studies are imputed in the funnel plot based on the overall estimate that is corrected in terms of bias.",
                "Reference(s)": "**",
                "Drafted by": "** Mahmoud Elsherif",
                "Reviewed (or Edited) by": "**",
                "Translated by": "Helena Hartmann",
                "Translation reviewed by": "Susanne Vogel; Joris Frese",
                "Translation": "Die Trim-and-Fill-Methode schätzt fehlende Studien in Folge von Publikationsverzerrungen im Trichterdiagramm (Funnel Plot) und passt die Gesamtgröße der Effektstärke an. Die Studien mit den extremsten Effektgrößen auf der linken und rechten Seite werden unterdrückt. Die Anzahl der ausgeschlossenen Studien wird von Fall zu Fall festgelegt. Bei dieser Methode werden zunächst die Studien herausgefiltert, die einen asymmetrischen Funnel Plot ergeben, um sicherzustellen, dass die Gesamtgröße der Effektstärke, die von den verbleibenden Studien beeinflusst wird, nicht von den verbleibenden Studien durch Publikationsverzerrungen beeinflusst wird. Wenn es jedoch keinen wahren Effekt gibt, ist es nicht möglich, die zu eliminierenden Studien in einem ersten Schritt zu identifizieren. Anschließend werden die fehlenden Studien im Funnel Plot auf der Grundlage der hinsichtlich der Verzerrung korrigierten Gesamtschätzung imputiert.",
                "Related_terms": "** meta analysis"
            },
            {
                "Title": "TRUST Principles (TRUST Prinzipien) *",
                "Definition": "** A set of guiding principles that consider Transparency, Responsibility, User focus, Sustainability, and Technology (TRUST) as the essential components for assessing, developing, and sustaining the trustworthiness of digital data repositories (especially those that store research data). They are complementary to the FAIR Data Principles.",
                "Reference": "** Lin et al. (2020)",
                "Originally drafted by": "** Aleksandra Lazić",
                "Reviewed (or Edited) by": "** Jamie P. Cockcroft; Mahmoud Elsherif; Helena Hartmann; Sam Parsons",
                "Translated by": "Susanne Vogel",
                "Translation reviewed by": "Helena Hartmann; Joris Frese",
                "Translation": "Eine Reihe von Leitprinzipien, die Transparenz, Verantwortung, Nutzer:innenorientierung, Nachhaltigkeit und Technologie (Transparency, Responsibility, User focus, Sustainability, and Technology, TRUST) als die wesentlichen Komponenten für die Bewertung, Entwicklung und Aufrechterhaltung der Vertrauenswürdigkeit digitaler Datenrepositorien (insbesondere solcher, die Forschungsdaten speichern) betrachten. Sie ergänzen die FAIR Daten Prinzipien.",
                "Related_terms": "** FAIR principles; Metadata; Open Access; Open Data; Open Material; Repository"
            },
            {
                "Title": "Type I error (Typ-I-Fehler) *",
                "Definition": "** “Incorrect rejection of a null hypothesis” (Simmons et al., 2011, p. 1359), i.e. finding evidence to reject the null hypothesis that there is no effect when the evidence is actually in favouring of retaining the null that there is no effect (For example, a judge imprisoning an innocent person). Concluding that there is a significant effect and rejecting the null hypothesis when your findings actually occured by chance.",
                "Reference": "** Simmons et al., (2011)",
                "Originally drafted by": "** Lisa Spitzer",
                "Reviewed (or Edited) by": "** Mahmoud Elsherif; Adrien Fillon; Helena Hartmann; Matt Jaquiery; Mariella Paul; Charlotte R. Pennington; Graham Reid; Olly Robertson; Mirela Zaneva",
                "Translated by": "Bettina M.J. Kern",
                "Translation reviewed by": "Helena Hartmann, Susanne Vogel; Joris Frese",
                "Translation": "Incorrect rejection of a null hypothesis” (dt. fälschliches Verwerfen einer Nullhypothese; Simmons et al., 2011, S. 1359), d.h. das Auffinden von Beweisen, um die Nullhypothese, dass es keinen Effekt gibt, zu verwerfen, wenn die Beweise tatsächlich für die Beibehaltung der Nullhypothese sprechen, dass es keinen Effekt gibt (z. B. ein Richter, der eine unschuldige Person inhaftiert). Die Schlussfolgerung, dass ein signifikanter Effekt vorliegt, und die Zurückweisung der Nullhypothese, wenn die Ergebnisse tatsächlich zufällig entstanden sind.",
                "Related_terms": "** Frequentist statistics; Null Hypothesis Significance Testing (NHST); Null Result; *P* value; Questionable Research Practices or Questionable Reporting Practices (QRPs); Reproducibility crisis (aka Replicability or replication crisis); Scientific integrity; Statistical power; True positive result; Type II error"
            },
            {
                "Title": "Type II error (Typ-II-Fehler) *",
                "Definition": "** A false negative result occurs when the alternative hypothesis is true in the population but the null hypothesis is accepted as part of the analysis (Hartgerink et al., 2017). That is, finding a non-significant statistical result when the effect is true (For example, a judge passing an innocent verdict on a guilty person). False negatives are less likely to be the subject of replications than positive results (Fiedler et al., 2012), and remain an unresolved issue in scientific research (Hartgerink et al., 2017).",
                "Reference(s)": "** Fiedler et al. (2012); Hartgerink et al. (2017)",
                "Originally drafted by": "** Olly Robertson",
                "Reviewed (or Edited) by": "** Mahmoud Elsherif; Charlotte R. Pennington",
                "Translated by": "Bettina M.J. Kern",
                "Translation reviewed by": "Helena Hartmann, Susanne Vogel; Joris Frese",
                "Translation": "Ein Fehler 2\\. Art liegt vor, wenn die Alternativhypothese in der Population wahr ist, fälschlicherweise aber die Nullhypothese angenommen wird (Hartgerink et al., 2017). Es wird also ein nicht-signifikantes statistisches Ergebnis gefunden, obwohl es einen wahren Effekt gibt (z. B. ein Arzt, der eine vorliegende Schwangerschaft nicht erkennt). Falsch-negative Ergebnisse sind weniger wahrscheinlich Gegenstand von Replikationen als positive Ergebnisse (Fiedler et al., 2012\\) und bleiben ein ungelöstes Problem in der wissenschaftlichen Forschung (Hartgerink et al., 2017).",
                "Related_terms": "** Effect size; Null Hypothesis Significance Testing (NHST); Questionable Research Practices or Questionable Reporting Practices (QRPs); Reproducibility crisis (aka Replicability or replication crisis); Scientific integrity; Statistical power; True positive result; Type I error"
            },
            {
                "Title": "Type M error (Typ-M-Fehler) *",
                "Definition": "** A Type M error occurs when a researcher concludes that an effect was observed with magnitude lower or higher than the real one. For example, a type M error occurs when a researcher claims that an effect of small magnitude was observed when it is large in truth or vice versa.",
                "Reference(s)": "** Gelman and Carlin (2014); Lu et al.(2018)",
                "Originally drafted by": "** Eduardo Garcia-Garzon",
                "Reviewed (or Edited) by": "** Mahmoud Elsherif; Helena Hartmann; Graham Reid; Mirela Zaneva",
                "Translated by": "Helena Hartmann",
                "Translation reviewed by": "Susanne Vogel; Joris Frese  ### ---",
                "Translation": "Ein Typ-M-Fehler tritt auf, wenn ein/e Forscher:in zu dem Schluss kommt, dass ein Effekt beobachtet wurde, dessen Ausmaß geringer oder größer ist als der wahre Effekt. Ein Typ-M-Fehler tritt beispielsweise auf, wenn ein/e Forscher:in behauptet, dass ein Effekt von geringer Größe beobachtet wurde, obwohl er in Wahrheit groß ist, oder umgekehrt.",
                "Related_terms": "** Statistical power; Type S error; Type I error; Type II error"
            },
            {
                "Title": "Type S error (Typ S Fehler) *",
                "Definition": "** A Type S error occurs when a researcher concludes that an effect was observed with an opposite sign than real one. For example, a type S error occurs when a researcher claims that a positive effect was observed when it is negative in reality or vice versa.",
                "Reference(s)": "** Gelman and Carlin (2014); Lu et al. (2018)",
                "Originally drafted by": "** Eduardo Garcia-Garzon",
                "Reviewed (or Edited) by": "** Helena Hartmann; Sam Parsons; Graham Reid; Mirela Zaneva",
                "Translated by": "Susanne Vogel",
                "Translation reviewed by": "Helena Hartmann; Joris Frese   ### **U** {#u}",
                "Translation": "Ein Fehler vom Typ S tritt auf, wenn ein:e Forschende:r zu dem Schluss kommt, dass ein Effekt mit einem anderen Vorzeichen als dem tatsächlichen beobachtet wurde. Ein Fehler vom Typ S tritt beispielsweise auf, wenn ein:e Forschende:r behauptet, dass ein positiver Effekt beobachtet wurde, obwohl er in Wirklichkeit negativ ist, oder umgekehrt.",
                "Related_terms": "** Statistical power; Type M error; Type I error; Type II error"
            },
            {
                "Title": "Under-representation (Unterrepräsentation) *",
                "Definition": "** Not all voices, perspectives, and members of the community are adequately represented. Under-representation typically occurs when the voices or perspectives of one group dominate, resulting in the marginalization of another. This often affects groups who are a minority in relation to certain personal characteristics.",
                "Drafted by": "** Madeleine Pownall",
                "Reviewed (or Edited) by": "** Mahmoud Elsherif; Helena Hartmann; Bethan Iley; Adam Parker; Charlotte R. Pennington, Mirela Zaneva",
                "Translated by": "Susanne Vogel",
                "Translation reviewed by": "Helena Hartmann; Joris Frese",
                "Translation": "Nicht alle Stimmen, Perspektiven und Mitglieder der Gemeinschaft sind angemessen vertreten. Unterrepräsentation tritt typischerweise dann auf, wenn die Stimmen oder Perspektiven einer Gruppe dominieren, was zur Marginalisierung einer anderen führt. Dies betrifft häufig Gruppen, die in Bezug auf bestimmte persönliche Merkmale in der Minderheit sind.",
                "Related_terms": "** Equity; Fairness; Inequality; WEIRD"
            },
            {
                "Title": "Universal design for learning (UDL) *",
                "Definition": "** A framework for improving learning and optimising teaching based upon scientific insights of how humans learn. It aims to make learning inclusive and transformative for all people in which the focus is on catering to the differing needs of different students. It is often regarded as an evidence-based and scientifically valid framework to guide educational practice, consisting of three key principles: engagement, representation, and action and expression. In addition, UDL is included in the Higher Education Opportunity Act of 2008 (Edyburn, 2010).",
                "Reference(s)": "** Hitchcock et al. (2002); Rose (2000); Rose and Meyer (2002)",
                "Drafted by": "** Charlotte R. Pennington",
                "Reviewed (or Edited) by": "** Valeria Agostini; Mahmoud Elsherif; Graham Reid; Mirela Zaneva; Flávio Azevedo",
                "Translated by": "Helena Hartmann",
                "Translation reviewed by": "Susanne Vogel; Joris Frese  ### ---  ### **V** {#v}",
                "Translation": "Ein Konzept zur Verbesserung des Lernens und zur Optimierung des Unterrichtens, das auf wissenschaftlichen Erkenntnissen darüber beruht, wie Menschen lernen. Es wird häufig als evidenzbasierter und wissenschaftlich fundierter Rahmen für die pädagogische Praxis angesehen, der aus drei Schlüsselprinzipien besteht: Engagement, Repräsentation sowie Aktion und Ausdruck. Darüber hinaus ist UDL in den Higher Education Opportunity Act von 2008 aufgenommen worden (Edyburn, 2010).",
                "Related_terms": "** Equal opportunities; Inclusivity; Pedagogy; Teaching practice"
            },
            {
                "Title": "Validity (Validität) *",
                "Definition": "** Validity refers to the application of statistical principles to arrive at well-founded —i.e., likely corresponding accurately to the real world— concepts, conclusions or measurement. In psychometrics, validity refers to the extent to which something measures what it intends to or claims to measure. Under this generic term, there are different types of validity (e.g., internal validity, construct validity, face validity, criterion validity, diagnostic validity, discriminant validity, concurrent validity, convergent validity, predictive validity, external validity).",
                "Reference(s)": "** Campbell (1957); Boorsboom et al. (2004); Kelley (1927)",
                "Drafted by": "** Tamara Kalandadze; Madeleine Pownall; Flávio Azevedo",
                "Reviewed (or Edited) by": "** Eduardo Garcia-Garzon; Halil E. Kocalar; Annalise A. LaPlume; Joanne McCuaig; Adam Parker; Charlotte R. Pennington",
                "Translated by": "Susanne Vogel",
                "Translation reviewed by": "Helena Hartmann; Joris Frese",
                "Translation": "Validität bezieht sich auf die Anwendung statistischer Prinzipien, um zu fundierten \\- d. h. wahrscheinlich genau der realen Welt entsprechenden \\- Konzepten, Schlussfolgerungen oder Messungen zu gelangen. In der Psychometrie bezieht sich die Validität auf das Ausmaß, in dem etwas das misst, was es zu messen beabsichtigt oder vorgibt zu messen. Unter diesem Oberbegriff gibt es verschiedene Arten von Validität (z. B. interne Validität, Konstruktvalidität, Augenscheinvalidität, Kriteriumsvalidität, diagnostische Validität, diskriminante Validität, übereinstimmende Validität, konvergente Validität, prädiktive Validität, externe Validität).",
                "Related_terms": "** Causality; Construct validity; Content validity; Criterion validity; External validity; Face validity; Internal validity; Measurement; Questionable Measurement Practices (QMP); Psychometry; Reliability; Statistical power; Statistical validity; Test"
            },
            {
                "Title": "Version control (Versionskontrolle) *",
                "Definition": "** The practice of managing and recording changes to digital resources (e.g. files, websites, programmes, etc.) over time so that you can recall specific versions later. Version control systems are designed to record the history of changes (who, what and when), and help to avoid human errors (e.g. working on the wrong version). For example, the Git version control system is a widely used software tool that originally helped software developers to version control shared code and is now used across many scientific disciplines to manage and share files.",
                "Reference": "** [https://git-scm.com/book/en/v2/Getting-Started-About-Version-Control](https://git-scm.com/book/en/v2/Getting-Started-About-Version-Control)",
                "Drafted by": "** Mahmoud Elsherif",
                "Reviewed (or Edited) by": "** Sarah Ashcroft-Jones; Thomas Rhys Evans; Helena Hartmann; Matt Jaquiery; Adam Parker; Charlotte R. Pennington; Robert M. Ross; Timo Roettger; Andrew J. Stewart",
                "Translated by": "Helena Hartmann",
                "Translation reviewed by": "Susanne Vogel; Joris Frese  ###  ### **W** {#w}",
                "Translation": "Die Praxis der Verwaltung und Aufzeichnung von Änderungen an digitalen Ressourcen (z. B. Dateien, Websites, Programme usw.) im Laufe der Zeit, sodass Sie bestimmte Versionen später wieder aufrufen können. Versionskontrollsysteme sind so konzipiert, dass sie den Verlauf der Änderungen (wer, was und wann) aufzeichnen und dazu beitragen, menschliche Fehler (z. B. die Arbeit an der falschen Version) zu vermeiden. Das Versionskontrollsystem Git beispielsweise ist ein weit verbreitetes Softwaretool, das ursprünglich Softwareentwickelnden bei der Versionskontrolle von gemeinsamem Code half und heute in vielen wissenschaftlichen Disziplinen zur Verwaltung und gemeinsamen Nutzung von Dateien eingesetzt wird.",
                "Related_terms": "** Git; Reproducibility; Software configuration management; Source code management; Source control"
            },
            {
                "Title": "Webometrics (Webometrie) *",
                "Definition": "** Webometrics involves the study of online content. Webometrics focuses on the numbers and types of hyperlinks between different online sites. Such approaches have been considered as a type of altmetrics. “The study of the quantitative aspects of the construction and use of information resources, structures and technologies on the Web drawing on [bibliometric](https://en.wikipedia.org/wiki/Bibliometrics) and [informetric](https://en.wikipedia.org/wiki/Informetrics) approaches” (Björneborn & Ingwersen, 2004).",
                "Reference(s)": "** Björneborn and Ingwersen (2004)",
                "Drafted by": "** Charlotte R. Pennington",
                "Reviewed (or Edited) by": "** Christopher Graham; Mirela Zaneva",
                "Translated by": "Susanne Vogel",
                "Translation reviewed by": "Helena Hartmann; Joris Frese",
                "Translation": "Die Webometrie befasst sich mit der Untersuchung von Online-Inhalten. Die Webometrie konzentriert sich auf die Anzahl und Art der Hyperlinks zwischen verschiedenen Online-Seiten. Solche Ansätze werden als eine Art von Altmetrik betrachtet. \"The study of the quantitative aspects of the construction and use of information resources, structures and technologies on the Web drawing on [bibliometric](https://en.wikipedia.org/wiki/Bibliometrics) and [informetric](https://en.wikipedia.org/wiki/Informetrics) approaches” (dt. Die Untersuchung der quantitativen Aspekte des Aufbaus und der Nutzung von Informationsressourcen, \\-strukturen und \\-technologien im Web auf der Grundlage bibliometrischer und informatischer Ansätze; Björneborn & Ingwersen, 2004).",
                "Related_terms": "** Altmetrics; Bibliometrics"
            },
            {
                "Title": "WEIRD *",
                "Definition": "** This acronym refers to Western, Educated, Industrialized, Rich and Democratic societies. Most research is conducted on, and conducted by, relatively homogeneous samples from WEIRD societies. This limits the generalizability of a large number of research findings, particularly given that WEIRD people are often psychological outliers. It has been argued that “WEIRD psychology ” started to evolve culturally as a result of societal changes and religious beliefs in the Middle Ages in Europe. Critics of this term suggest it presents a binary view of the global population and erases variation that exists both between and within societies, and that other aspects of diversity are not captured.",
                "Reference(s)": "** Henrich (2020); Henrich et al. (2010); Muthukrishna et al., (2020); Syed and Kathawalla (2020)",
                "Drafted by": "** Mahmoud Elsherif",
                "Reviewed (or Edited) by": "Zoe Flack; Matt Jaquiery; Bettina M. J. Kern; Adam Parker; Charlotte R. Pennington; Robert M. Ross; Suzanne L. K. Stewart",
                "Translated by": "Helena Hartmann",
                "Translation reviewed by": "Susanne Vogel; Joris Frese  ###  ### **X** {#x}  ###  ### **Y** {#y}  ###  ### **Z** {#z}",
                "Translation": "Dieses Akronym steht für westliche, gebildete, industrialisierte, reiche und demokratische (Western, Educated, Industrialized, Rich and Democratic) Gesellschaften. Die meisten Forschungsarbeiten werden an relativ homogenen Stichproben aus WEIRD-Gesellschaften durchgeführt und von diesen durchgeführt. Dies schränkt die Verallgemeinerbarkeit zahlreicher Forschungsergebnisse ein, zumal WEIRD-Personen oft psychologische Ausreißer sind. Es wurde argumentiert, dass sich die \"WEIRD Psychologie\" kulturell als Ergebnis gesellschaftlicher Veränderungen und religiöser Überzeugungen im Mittelalter in Europa zu entwickeln begann. Kritiker\\*innen dieses Begriffs sind der Meinung, dass er ein binäres Bild der Weltbevölkerung zeichnet und die Unterschiede zwischen und innerhalb von Gesellschaften ausblendet und dass andere Aspekte der Vielfalt nicht erfasst werden.",
                "Related_terms": "** Bias; BIZARRE; Diversity; Generalizability; Populations; Sampling bias; STRANGE"
            },
            {
                "Title": "Z-Curve (Z-Kurve) *",
                "Definition": "** Computing a Z-score is a statistical approach mainly used to obtain the ‘Estimated Replication Rate’ (ERR) and ‘Expected Discovery Rate’ (EDR) for a set of reported studies. Calculating a *z*\\-curve for a set of statistically significant studies involves converting reported *p*\\-values to *z*\\-scores, fitting a finite mixture model to the distribution of *z*\\-scores, and estimating mean power based on the mixture model. The Z-curve analysis can be performed in R through a dedicated package \\- [https://cran.r-project.org/web/packages/zcurve/index.html](https://cran.r-project.org/web/packages/zcurve/index.html).",
                "Reference(s)": "** Bartoš and Schimmack (2020); Brunner and Schimmack (2020)",
                "Drafted by": "** Bradley J. Baker",
                "Reviewed (or Edited) by": "** Kamil Izydorczak; Sam Parsons; Charlotte R. Pennington; Mirela Zaneva",
                "Translated by": "Susanne Vogel",
                "Translation reviewed by": "Helena Hartmann; Joris Frese",
                "Translation": "Die Berechnung einer z-Kurve ist ein statistischer Ansatz, der hauptsächlich verwendet wird, um die \"geschätzte Replikationsrate\" (Estimated Replication Rate, ERR) und die \"erwartete Entdeckungsrate\" (Expected Discovery Rate, EDR) für eine Reihe von berichteten Studien zu ermitteln. Die Berechnung einer *z*\\-Kurve für eine Sammlung statistisch signifikanter Studien beinhaltet die Umwandlung der berichteten *p*\\-Werte in *z*\\-Werte, die Anpassung eines Finite Mixture Modell an die Verteilung der *z*\\-Werte und die Schätzung der mittleren Teststärke (Power) auf der Grundlage des Modells. Die *z*\\-Kurven-Analyse kann in R mit einem speziellen Paket durchgeführt werden \\- [https://cran.r-project.org/web/packages/zcurve/index.html](https://cran.r-project.org/web/packages/zcurve/index.html).",
                "Related_terms": "** Altmetrics; File drawer ratio; P-curve; P-hacking; Replication; Statistical power"
            },
            {
                "Title": "Zenodo *",
                "Definition": "** An open science repository where researchers can deposit research papers, reports, data sets, research software, and any other research-related digital artifacts. Zenodo creates a persistent digital object identifier (DOI) for each submission to make it citable. This platform was developed under the European OpenAIRE program and operated by CERN.",
                "Reference": "** www.zenodo.org",
                "Originally drafted by": "** Ali H. Al-Hoorie",
                "Reviewed (or Edited) by": "** Sara Middleton",
                "Translated by": "Helena Hartmann",
                "Translation reviewed by": "Susanne Vogel; Joris Frese  ### **Looking for something else?**  We have separated the glossary project across several documents, see links below:  Landing page:\t\t\t[Glossary Translations German template](https://docs.google.com/document/d/1IIZK-F9SX1P4UrPlZeEKgUAOyiEjNbGFSWafr0ADb0M/edit) Letters A \\- F:\t\t\t[German Glossary | Phase 2 | A-F](https://docs.google.com/document/d/1yQH_iYm7FgjVGtJyQWGv7iv1kt4GkUWeRbXHch57Fuo/edit) Letters G \\- L:\t\t\t[German Glossary | Phase 2 | G-L](https://docs.google.com/document/d/1MbfcDK3G6ybzkkq2jVz36q4TcqQMGpNG2cun6GOeuSQ/edit#) Letters M \\- R: \t\t\t[German Glossary | Phase 2 | M - R](https://docs.google.com/document/d/1sv2C1Y-z3WeiYjvhn2B8rhFyPK5YQ8G3sCCh3ZYwV2Q/edit#heading=h.w0bgiwj800db) Letters S \\- Z:\t\t\t[German Glossary | Phase 2 | S - Z](https://docs.google.com/document/d/1pORanWNHkMRkGYs8vNxdetx907gBv-fm8l-tV8XRIiE/edit) References: \t\t\t[Glossary | Phase 2 | References](https://docs.google.com/document/d/12_F8kbnl2GP66iBkIejJeX2KnkZ13iC_jj7VVYZMxpA/edit?usp=sharing) Terms not yet defined: \t\t[Glossary | Phase 2 | Terms not yet defined](https://docs.google.com/document/d/16FGodUkNoNrBYyq2JqHJMNzYuZf-QbsigooMpB_ns_E/edit?usp=sharing) Contributors spreadsheet: [Glossary Translations German tenzing contributions \\[FORRT\\]](https://docs.google.com/spreadsheets/d/1UEM7s27b5pOlrIYX9-fXYdPOmPpZZyoggJOfOIrOhko/edit#gid=0)  [FORRT slack](https://join.slack.com/t/forrt/shared_invite/zt-alobr3z7-NOR0mTBfD1vKXn9qlOKqaQ) \\- join us\\! [FORRT email](mailto:FORRTproject@gmail.com) \\- contact us\\!",
                "Translation": "Ein offenes wissenschaftliches Repositorium, in dem Forschende Forschungsarbeiten, Berichte, Datensätze, Forschungssoftware und jedwede andere forschungsbezogene digitale Artefakte hinterlegen können. Zenodo erstellt eine dauerhafte digitale Objektkennung (DOI) für jeden Beitrag, um ihn zitierfähig zu machen. Diese Plattform wurde im Rahmen des europäischen Programms OpenAIRE entwickelt und wird vom CERN betrieben.",
                "Related_terms": "** DOI (digital object identifier); figshare; Open data; Open Science Framework; Preprint"
            }
        ]
    }
]