[
    {
        "arabic": [
            {
                "type": "glossary",
                "title": "انحياز المستخلص (Abstract Bias)",
                "definition": "الميل نحو الإشارة إلى النَّتائج ذات الدِّلالة الإحصائيَّة في المستخلص فقط، والاكتفاء بالنَّتائج التي لا تصل بالدِّلالة الإحصائيَّة إلى داخل النَّص الرَّئيسيّ. للبحث (حيث إنَّ عدم الإبلاغ عن هذه النَّتائج يعدُّ تقريرًا انتقائيًا). يؤدي هذا التَّحيُّز إلى عدم اكتشاف الدِّراسات التي تحوي نتائج لا تصل للدِّلالة الإحصائيَّة باستخدام إجراءات الأبحاث البعديَّة المعتادة (التي تعتمد على المعلومات الموجودة في العنوان والملخَّص والكلمات الرَّئيسيَّة)، وبالتَّالي إلى التَّحيُّز في نتائج التَّحليلات البعديَّة. **المصطلحات ذات الصِّلة:** قطف الكرز (الانتقائيّة)، تحيُّز النشر (مشكلة درج الملفَّات)، التقرير الانتقائي.",
                "related_terms": [
                    "Cherry-picking",
                    "Publication bias (File Drawer Problem)",
                    "Selective reporting"
                ],
                "references": "Duyx, B., Swaen, G. M., Urlings, M. J., Bouter, L. M., & Zeegers, M. P. (2019). The strong focus on positive results in abstracts may cause bias in systematic reviews: A case study on abstract reporting bias. Systematic Reviews, 8(1), 1–8.",
                "drafted_by": [
                    "Ali H. Al-Hoorie"
                ],
                "reviewed_by": [
                    "Mahmoud Elsherif",
                    "Bethan Iley",
                    "Sam Parsons",
                    "Gerald Vineyard",
                    "Eliza Woodward",
                    "Flávio Azevedo"
                ],
                "alt_related_terms": [
                    null
                ],
                "language": "arabic"
            },
            {
                "type": "glossary",
                "title": "التَّأثير الأكاديميّ (Academic Impact)",
                "definition": "يشير هذا المصطلح إلى الإسهام الذي يحدثه النَّتاج البحثي، مثل: مقالة منشورة في التحوُّل المفاهيمي، وتطوير النَّظريات، والأساليب، والتَّطبيقات العمليَّة، سواء داخل التَّخصص  المعرفي الواحد، أو عبر التَّخصُّصات  المعرفيَّة المختلفة. ويدلُّ التَّأثير الأكاديمي كذلك على درجة التَّغيير الذي يحدثه أي نتاج، أو برنامج بحثي خارج المجال الأكاديمي، مثل التَّأثير الاجتماعي والاقتصادي. (راجع: [https://esrc.ukri.org/research/impact-toolkit/what-is-impact/](https://esrc.ukri.org/research/impact-toolkit/what-is-impact/)). **المصطلحات ذات الصّلة:** المستفيدين، **إعلان سان فرانسيسكو بشأن تقييم البحوث** ; REF",
                "related_terms": [
                    "Beneficiaries",
                    "DORA",
                    "Reach",
                    "REF"
                ],
                "references": "Anon. (2021). What is impact?. Retrieved from https://esrc.ukri.org/research/impact-toolkit/what-is-impact/",
                "drafted_by": [
                    "Connor Keating"
                ],
                "reviewed_by": [
                    "Myriam A. Baum",
                    "Adam Parker",
                    "Charlotte R. Pennington",
                    "Suzanne L. K. Stewart",
                    "Flávio Azevedo"
                ],
                "alt_related_terms": [
                    null
                ],
                "language": "arabic"
            },
            {
                "type": "glossary",
                "title": "إمكانيَّة الوصول (Accessibility)",
                "definition": "الَّتعريف:**  يُعرّف هذا المصطلح بسهولة الوصول، أو إمكانيَّة إعادة استخدام المواد، كالبيانات، والتَّعليمات البرمجيَّة، والمطبوعات، والمنشورات لأغراض أكاديمية، وتحديدًا للأشخاص ذوي الأمراض المزمنة، و الإعاقات، أو الاضطرابات العصبيَّة. تواجه هذه المجموعات في عملية البحث العديد من العوائق الماديَّة، والقانونيَّة، والتقنية، فمثلًا، هناك معوّق الحصول على مواد ملائمة لحالتهم، وكذلك معوّق الدخول البدني إلى المرافق. يشمل مصطلح إمكانية الوصول أيضًا: اعتبارات هيكليَّة تتعلَّق بالتَّنوع، والمساواة، والشُّمولَّية والَّتمثيل  (Pownall et al., 2021\\). عند تجهيز الواجهات، والمناسبات، والمرافق يجب أخذ إمكانية الوصول بعين الاعتبار؛ لضمان المشاركة الكاملة، فمثلًا يجب التأكد من أنَّ الصُّور الإلكترونيَّة موائمة لذوي عمى الألوان، وأن تحتوي هذه الصورعلى نص بديل يوضح محتوى الصورة، أو استخدام السطرجة الفورية، أو التَّرجمة الحيَّة في المناسبات. (Brown et al., 2018; Pollet & Bond, 2021; World Wide Web Consortium, 2021). **المصطلحات ذات الصِّلة:** التَّوفر، بيان توفر البيانات، الشُّمول، الوصول المفتوح، نقص التَّمثيل، التصميم الشامل للتَّعليم.",
                "related_terms": [
                    "Availability",
                    "Data availability statements",
                    "Inclusion",
                    "Open Access",
                    "Under-representation",
                    "Universal design for learning (UDL)"
                ],
                "references": "Brown, N., Thompson, P., & Leigh, J. S. (2018). Making academia more accessible. Journal of Perspectives in Applied Academic Practice, 6(2), 82–90. https://doi.org/10.14297/jpaap.v6i2.348\n\nPollet, I. L., & Bond, A. L. (2021). Evaluation and recommendations for greater accessibility of colour figures in ornithology. Ibis, 163, 292–295. https://doi.org/10.1111/ibi.12887\n\nSuber, P. (2004). The primacy of authors in achieving Open Access. In Nature. Retrieved from http://dash.harvard.edu/handle/1/4391161)\n\nPownall, M., Talbot, C. V., Henschel, A., Lautarescu, A., Lloyd, K. E., Hartmann, H., Darda, K. M., Tang, K. T. Y., Carmichael-Murphy, P., & Siegel, J. A. (2021). Navigating Open Science as Early Career Feminist Researchers. Psychology of Women Quarterly, 45(4), 526–539. https://doi.org/10.1177/03616843211029255 Retrieved from https://journals.sagepub.com/doi/10.1177/03616843211029255",
                "drafted_by": [
                    "Kai Krautter"
                ],
                "reviewed_by": [
                    "Valeria Agostini",
                    "Myriam A. Baum",
                    "Mahmoud Elsherif",
                    "Bethan Iley",
                    "Tamara Kalandadze",
                    "Ryan Millager",
                    "Sara Middleton",
                    "Charlotte R. Pennington",
                    "Madeleine Pownall",
                    "Robert M. Ross",
                    "Flávio Azevedo"
                ],
                "alt_related_terms": [
                    null
                ],
                "language": "arabic"
            },
            {
                "type": "glossary",
                "title": "تحيَّز الشَّخصنة (Ad hominem bias)",
                "definition": "مأخوذ من المعنى اللاتيني \"إلى الشَّخص\"، وهو حينما يتأثر الحكم على فكرة، أو  عمل بخصائص الشَّخص الذي قام به، وليس بخصائص العمل نفسه. يمكن أن يكون هذا التَّحيز سلبيًا، كما هو الحال عندما يقوم منافس، أو شخص له عداء شخصي بتوجيه انتقاد أقسى مما تستوجبه جودة العمل، وقد يكون إيجابيًا، كما هو الحال عندما يحصل العمل من صديق على تقييم إيجابي للغاية.  **المصطلحات ذات الصلة:** تحكيم الأقران.",
                "related_terms": [
                    "Peer review"
                ],
                "references": "Barnes, R. M., Johnston, H. M., MacKenzie, N., Tobin, S. J., & Taglang, C. M. (2018). The effect of ad hominem attacks on the evaluation of claims promoted by scientists. PloS One, 13(1), e0192025. https://doi.org/10.1371/journal.pone.0192025\n\nTvina, A., Spellecy, R., & Palatnik, A. (2019). Bias in the peer review process: can we do better? Obstetrics & Gynecology, 133(6), 1081–1083. https://doi.org/10.1097/AOG.0000000000003260",
                "drafted_by": [
                    "Mahmoud Elsherif"
                ],
                "reviewed_by": [
                    "Bradley Baker",
                    "Filip Dechterenko",
                    "Bethan Iley",
                    "Madeleine Ingham",
                    "Graham Reid"
                ],
                "alt_related_terms": [
                    null
                ],
                "language": "arabic"
            },
            {
                "type": "glossary",
                "title": "التَّشارك العدائي (Adversarial collaboration)",
                "definition": "أن يشترك في مشروع واحد باحثان أو أكثر من مؤيدي نظريات مختلفة أو متناقضة، والذي يؤدي إلى تنبؤات متباينة محتملة حول نتائج الدِّراسة؛ والهدف من ذلك هو تقليل التَّحيزات، ونقاط الضعف المنهجيَّة إلى الحد الأدنى، وكذلك إنشاء قاعدة مشتركة من الحقائق التي يجب أن تأخذها النَّظريات المنافسة في الاعتبار.  **المصطلحات ذات الصلة:** التَّعاون، العديد من المحللين، المعامل المتعددة، التسجيل المسبق، تحيز النَّشر (مشكلة درج الملفات)",
                "related_terms": [
                    "Collaboration",
                    "Many Analysts",
                    "Many Labs",
                    "Preregistration",
                    "Publication bias (File Drawer Problem)"
                ],
                "references": "Bateman, I., Kahneman, D., Munro, A., Starmer, C., & Sugden, R. (2005). Testing competing models of loss aversion: An adversarial collaboration. Journal of Public Economics, 89(8), 1561–1580. https://doi.org/10.1016/j.jpubeco.2004.06.013\n\nCowan, N., Belletier, C., Doherty, J. M., Jaroslawska, A. J., Rhodes, S., Forsberg, A., & Logie, R. H. (2020). How do scientific views change? Notes from an extended adversarial collaboration. Perspectives on Psychological Science, 15(4), 1011–1025. https://doi.org/10.1177/1745691620906415\n\nKerr, N. L., Ao, X., Hogg, M. A., & Zhang, J. (2018). Addressing replicability concerns via adversarial collaboration: Discovering hidden moderators of the minimal intergroup discrimination effect. Journal of Experimental Social Psychology, 78, 66–76. https://doi.org/10.1016/j.jesp.2018.05.001\n\nMellers, B., Hertwig, R., & Kahneman, D. (2001). Do frequency representations eliminate conjunction effects? An exercise in adversarial collaboration. Psychological Science, 12(4), 269–275. https://doi.org/10.1111/1467-9280.00350\n\nRakow, T., Thompson, V., Ball, L., & Markovits, H. (2014). Rationale and guidelines for empirical adversarial collaboration: A Thinking & Reasoning initiative. Thinking & Reasoning, 21(2), 167–175. https://doi.org/10.1080/13546783.2015.975405",
                "drafted_by": [
                    "Siu Kit Yeung"
                ],
                "reviewed_by": [
                    "Matt Jaquiery",
                    "Aoife O’Mahony",
                    "Charlotte R. Pennington",
                    "Flávio Azevedo",
                    "Madeleine Pownall**;** Martin Vasilev"
                ],
                "alt_related_terms": [
                    null
                ],
                "language": "arabic"
            },
            {
                "type": "glossary",
                "title": "collaborative commentary التَّعليق التَّعاوني العدائي (Adversarial (collaborative) commentary)",
                "definition": "الَّتعريف:** تعليق يتعاون فيه المؤلفون الأصليون للعمل مع نُقّاده؛ لصياغة بيان تتوافق فيه الآراء. ويهدف الَّتعليق التَّعاوني إلى صياغة تعليق خالٍ من تحيُّز الشَّخصنة، وينقل فهمًا مشتركًا، أو على الأقل يُحدد أين يتفق الطَّرفان، وأين يختلفان. تقدّم هذه الطَّريقة رسالة واضحة ومسارًا إلى الأمام، بدلًا من ترك القارئ يختار بين وجهات النَّظر المتعارضة التي يتم نقلها في تعليقات منفصلة.  **المصطلحات ذات الصِّلة:** التَّشارك العدائي، التَّعليق التَّعاوني.",
                "related_terms": [
                    "Adversarial collaboration",
                    "Collaborative commentary"
                ],
                "references": "Heyman, T., Moors, P., & Rabagliati, H. (2020). The benefits of adversarial collaboration for commentaries. Nature Human Behavior, 4, 1217. https://doi.org/10.1038/s41562-020-00978-6\n\nRabagliati, H., Moors, P., & Heyman, T. (2019). Can item effects explain away the evidence for unconscious sound symbolism? An adversarial commentary on Heyman, Maerten, Vankrunkelsven, Voorspoels, and Moors (2019). Psychological Science, 31(9), 1200–1204. https://doi.org/10.1177/0956797620949461\n\nSilberzahn, R., Simonsohn, U., & Ulhmann, E. L. (2014). Matched-names analysis reveals no evidence of name-meaning effects: A collaborative commentary on Silberzahn and Uhlmann (2013). Psychological Science, 25(7), 1504–1505. https://doi.org/10.1177/0956797614533802",
                "drafted_by": [
                    "Steven Verheyen"
                ],
                "reviewed_by": [
                    "Mahmoud Elsherif",
                    "Sam Parsons",
                    "Emma Henderson",
                    "Michele C. Lim",
                    "Flávio Azevedo"
                ],
                "alt_related_terms": [
                    null
                ],
                "language": "arabic"
            },
            {
                "type": "glossary",
                "title": "تحيُّز الانتماء (Affiliation bias)",
                "definition": "يحدث هذا النُّوع من التَّحيُّز عندما يتأثر حكم أو وجهة نظر الفرد حول جودة البحوث بالمؤسسة التي ينتمي إليها الباحث. ومثال هذا النُّوع من التَّحيُّز هو عندما يفضل محررو المجلات العلميَّة نشر بحوث قادمة من مؤسسات مرموقة. **المصطلحات ذات الصِّلة:** تحكيم الأقران.",
                "related_terms": [
                    "Peer review"
                ],
                "references": "Tvina, A., Spellecy, R., & Palatnik, A. (2019). Bias in the peer review process: can we do better? Obstetrics & Gynecology, 133(6), 1081–1083. https://doi.org/10.1097/AOG.0000000000003260",
                "drafted_by": [
                    "Mahmoud Elsherif"
                ],
                "reviewed_by": [
                    "Christopher Graham",
                    "Madeleine Ingham",
                    "Adam Parker",
                    "Graham Reid"
                ],
                "alt_related_terms": [
                    null
                ],
                "language": "arabic"
            },
            {
                "type": "glossary",
                "title": "الاحتمال العشوائي (Aleatoric uncertainty)",
                "definition": "التَّباين في النَّتائج بسبب عوامل غير معروفة، أو عشوائيَّة بطبيعتها، وهذه العشوائيَّة في النَّتائج لا يمكن تقليصها من خلال مصادر إضافيَّة للمعلومات. فعلى سبيل المثال: عند قلب عملة معدنيَّة، سيكون هناك احتمال عشوائي بشأن ما إذا كانت العملة ستستقر على الصُّورة، أم الكتابة.  **مصطلحات ذات صلة:** اللايقين المعرفي; عدم اليقين النايتي (نسبة إلى العالم فرانك نايت)",
                "related_terms": [
                    "Epistemic uncertainty",
                    "Knightian uncertainty"
                ],
                "references": "Der Kiureghian, A., & Ditlevsen, O. (2009). Aleatory or epistemic? Does it matter? Structural Safety, 31(2), 105–112. https://doi.org/10.1016/j.strusafe.2008.06.020",
                "drafted_by": [
                    "Bradley Baker"
                ],
                "reviewed_by": [
                    "Nihan Albayrak-Aydemir**;** Brett Gall",
                    "Magdalena Grose-Hodge",
                    "Bethan Iley",
                    "Charlotte R. Pennington"
                ],
                "alt_related_terms": [
                    null
                ],
                "language": "arabic"
            },
            {
                "type": "glossary",
                "title": "المقاييس البديلة (Altmetrics)",
                "definition": "بالابتعاد عن مقاييس الاستشهاد التَّقليديَّة، توفر المقاييس البديلة تقييمًا للاهتمام والتَّأثير الأوسع للعمل البحثي استنادًا إلى مصادر متنوعة، مثل: وسائل التَّواصل الاجتماعي (مثل X)، ووسائل الإعلام الإخباريَّة الرَّقميَّة، وعدد تنزيلات مقالات الطِّباعة الأولية، وما إلى ذلك.  أحد أوجه النَّقد لهذه المقاييس البديلة  هو أنَّ الإدِّعاءات المثيرة عادة ما تحظى باهتمام أكبر من البحث الجاد (Ali, 2021).  **المصطلحات ذات الصِّلة:** التَّأثير الأكاديميّ، المقاييس البديلة، الببليومتريّات، مؤشر إتش، معامل تأثير المجلَّة.",
                "related_terms": [
                    "Academic impact",
                    "Alternative metrics",
                    "Bibliometrics",
                    "H-index",
                    "Impact assessment",
                    "Journal impact factor"
                ],
                "references": "Ali, M. J. (2021). Understanding the Altmetrics. Seminars in Ophthalmology. https://doi.org/10.1080/08820538.2021.1930806\n\nGalligan, F., & Dyas-Correia, S. (2013). Altmetrics: rethinking the way we measure. Serials Review, 39(1), 56–61. https://doi.org/10.1016/j.serrev.2013.01.003",
                "drafted_by": [
                    "Mirela Zaneva"
                ],
                "reviewed_by": [
                    "Ali H. Al-Hoorie",
                    "Charlotte R. Pennington",
                    "Birgit Schmidt",
                    "Flávio Azevedo"
                ],
                "alt_related_terms": [
                    null
                ],
                "language": "arabic"
            },
            {
                "type": "glossary",
                "title": "أمنيجا (AMNESIA)",
                "definition": "أمنيجا (Amnesia) هي أداة مجانيَّة لإخفاء الهويَّة، تقوم بإزالة المعلومات التَّعريفيَّة من البيانات، بعد تحميل مجموعة البيانات التي تحتوي على معلومات شخصيَّة، يتم تحويلها عن طريق الأداة فينتج عن ذلك بيانات خالية من المعلومات الشَّخصيَّة والحساسَّة.  **المصطلحات ذات الصِّلة:** التّعمية، السِّريَّة، أخلاقيَّات البحث.",
                "related_terms": [
                    "Anonymity",
                    "Confidentiality",
                    "Research ethics"
                ],
                "references": "",
                "drafted_by": [
                    "Norbert Vanek"
                ],
                "reviewed_by": [
                    "Ali H. Al-Hoorie",
                    "Myriam A. Baum",
                    "Charlotte R. Pennington"
                ],
                "alt_related_terms": [
                    null
                ],
                "language": "arabic"
            },
            {
                "type": "glossary",
                "title": "المرونة التَّحليليَّة (Analytic Flexibility)",
                "definition": "الَّتعريف**: نوع من درجات حريَّة الباحثين (Simmons, Nelson, & Simonsohn, 2011\\) التي تشير تحديدًا إلى العدد الكبير من الخيارات التي يتم إجراؤها أثناء المعالجة المسبقة للبيانات، والتَّحليل الإحصائيّ. وتعرف كذلك بأنها \"مجموعة من نتائج التَّحليل عبر طرق تحليل مختلفة، و مقبولة\" (Carp, 2012, p. 1). يمكن أن تكون المرونة التَّحليليَّة مشكلة، حيث يمكن أن يُترجم هذا التَّباين في الاستراتيجيات التَّحليليَّة إلى تباين في نتائج البحث تحديدًا عند تطبيق العديد من الاستراتيجيَّات دون الإفصاح عنها بشفافية (Masur, 2021).  **المصطلحات ذات الصِّلة:** حديقة المسارات المتشعِّبة،  تحليل الأكوان المتعدِّدة ، درجات حرية الباحث.",
                "related_terms": [
                    "Garden of forking paths",
                    "Multiverse analysis",
                    "Researcher degrees of freedom"
                ],
                "references": "Breznau, N. (2021). I saw you in the crowd: Credibility, reproducibility, and meta-utility. PS: Political Science & Politics, 54(2), 309–313. https://doi.org/10.1017/S1049096520000980\n\nJones, A., Dr, J., Duckworth, & Christiansen, P. (2020). May I have your attention, please? Methodological and Analytical Flexibility in the Addiction Stroop. https://doi.org/10.31234/osf.io/ws8xp\n\nMasur, P. K. (2020). Understanding the Effects of Analytical Choices on Finding the Privacy Paradox: A Specification Curve Analysis of Large-Scale Survey Data. Preprint. https://osf.io/m72gb/\n\nSimmons, J. P., Nelson, L. D., & Simonsohn, U. (2011). False-positive psychology: Undisclosed flexibility in data collection and analysis allows presenting anything as significant. Psychological Science, 22(11), 1359–1366. https://doi.org/10.1177/0956797611417632",
                "drafted_by": [
                    "Mariella Paul"
                ],
                "reviewed_by": [
                    "Adrien Fillon",
                    "Bettina M. J . Kern",
                    "Adam Parker",
                    "Charlotte R. Pennington",
                    "Flávio Azevedo"
                ],
                "alt_related_terms": [
                    null
                ],
                "language": "arabic"
            },
            {
                "type": "glossary",
                "title": "التّعمية (Anonymity)",
                "definition": "تشير عملية تعمية البيانات إلى مسح وحجب، أو تعميم، أو تغيير أي معلومات تعريفيَّة قد تشير إلى هوية المشاركين، والمحكِّمين، والمؤلِّفين وغيرهم إذ إنَّه يجب إخفاء البيانات الشَّخصيَّة لأي مشارك، حتى لا يتم التَّعرف على هويته.  تتمثل واحدة من أبسط الطُّرق في إخفاء هُوية المشاركين في تبديل أسمائهم بأسماء وهميَّة وحجب ما يشير إلى أي موقع جغرافي. تعد عملية التّعمية مهمَّة في مصادر البيانات المفتوحة، حيث إن بعض مصادر البيانات قد تبقى محجوبة؛ لأسباب تتعلق بالهُوية. ونوقش موضوع التّعمية ومصادر البيانات المفتوحة في البحوث الكيفيَّة التي تركز على التَّجارب والآراء الشَّخصيَّة، وأيضًا في البحوث الكميَّة التي تتضمَّن عينتها مشاركين في الدِّراسات السَّريريَّة. **المصطلحات ذات الصِّلة:** مجهول الهُوية، عيِّنة الدِّراسات السَّريريَّة، السِّريَّة ، أخلاقيات البحث، عيِّنة البحث، العيّنة الأكثر عُرضة للضَّرر.",
                "related_terms": [
                    "Anonymising",
                    "Clinical populations",
                    "Confidentiality",
                    "Research ethics",
                    "Research participants",
                    "Vulnerable population"
                ],
                "references": "Braun, V., & Clarke, V. (2013). Successful Qualitative Research. SAGE Publications.",
                "drafted_by": [
                    "Claire Melia"
                ],
                "reviewed_by": [
                    "Tsvetomira Dumbalska",
                    "Bethan Iley",
                    "Tamara Kalandadze",
                    "Bettina M.J. Kern",
                    "Sam Parsons",
                    "Charlotte R. Pennington",
                    "Flávio Azevedo",
                    "Madeleine Pownall",
                    "Birgit Schmidt"
                ],
                "alt_related_terms": [
                    null
                ],
                "language": "arabic"
            },
            {
                "type": "glossary",
                "title": "إرشادات أرايف (ARRIVE Guidelines)",
                "definition": "إرشادات أرايف (اختصار للبحوث الحيوانيَّة: الإبلاغ عن التَّجارب في الجسم الحي) هي مجموعة من المبادئ التَّوجيهيَّة لإعداد التقارير بناء على قائمة تحقق تم تطويرها؛ لتحسين معايير الإبلاغ، وتعزيز قابليَّة التِّكرار في بحوث الحيوانات الحية.  صدر الجيل الثَّاني من هذه الإرشادات في عام 2020\\. وفي هذه المبادئ التَّوجيهيَّة الجديدة، تم إيضاح هذه البنود أكثر، وأُعطي لكلٍ منها الأولويَّة، وأُضيفت معلومات جديدة في وثيقتي \"الشَّرح\" و \"التَّفصيل\" المصاحبتين لها؛ لتوفير الأساس المنطقي لكل بند، ومجموعة من التَّوصيات لإضفاء سياق على الدراسة التي يجري وصفها.  **المصطلحات ذات الصِّلة:** إرشادات الإعداد، دليل إعداد التّقارير، سترينج:الخلفية الاجتماعية والقابلية للتتبع والاختيار الذاتي وتارخ التربية والتأقلم والتعود.",
                "related_terms": [
                    "PREPARE Guidelines",
                    "Reporting Guideline",
                    "STRANGE"
                ],
                "references": "",
                "drafted_by": [
                    "Ben Farrar"
                ],
                "reviewed_by": [
                    "Jamie P. Cockcroft",
                    "Mahmoud Elsherif",
                    "Elias Garcia-Pelegrin",
                    "Helena Hartmann",
                    "Wanyin Li",
                    "Charlotte R. Pennington"
                ],
                "alt_related_terms": [
                    null
                ],
                "language": "arabic"
            },
            {
                "type": "glossary",
                "title": "APC رسوم النَّشر (Article Processing Charge (APC))",
                "definition": "رسوم النَّشر هي رسوم يفرضها النَّاشر على المؤلفين مقابل نشر واستضافة مقال مفتوح الوصول. غالبًا ما تهدف رسوم النَّشر إلى التَّعويض عن الخسارة المحتملة في الإيرادات التي قد تتعرَّض لها المجلة عند الانتقال من نماذج النَّشر التَّقليديَّة، مثل خدمات الاشتراك، أو الدَّفع لكل عرض،  والوصول المفتوح أيضًا. في حين أنَّ بعض المجلات تتقاضى حوالي 300 دولار أمريكي فقط، فإنَّ رسوم النَّشر تتباين كثيرًا، من 1000 دولار أمريكي (مثل مجلة \"التَّقدم في الأساليب، والممارسات في العلوم النَّفسيَّة\") أو أقل، إلى أكثر من 10000 دولار أمريكي (مثل مجلة \"الطَّبيعة\"). في حين أنَّ بعض النَّاشرين يقدمون إعفاءات للباحثين من مناطق معيَّنة من العالم، أو الذين لا يملكون الدَّعم، فقد تم انتقاد بعض رسوم النشر كونها لا تتناسب مع المعالجة الفعلية للبحوث والاستضافة الفعليَّة (Grossmann & Brembs, 2021\\) ولكونهم قد يخلقون عدم مساواة  بين العلماء حيث أن البعض منهم  يمكنهم تحمل تكلفة إتاحة أعمالهم مجانًا (Smith et al., 2020).  **المصطلحات ذات الصِّلة:**  الوصول المفتوح، نقص التَّمثيل.",
                "related_terms": [
                    "Open Access",
                    "Under-representation"
                ],
                "references": "Grossmann, A., & Brembs, B. (2021). Current market rates for scholarly publishing services. F1000Research, 10(20), 20. https://doi.org/10.12688/f1000research.27468.1\n\nSmith, A. C., Merz, L., Borden, J. B., Gulick, C., Kshirsagar, A. R., & Bruna, E. M. (2020). Assessing the effect of article processing charges on the geographic diversity of authors using Elsevier’s ‘Mirror Journal’ system. https://doi.org/10.31222/osf.io/s7cx4",
                "drafted_by": [
                    "Nick Ballou"
                ],
                "reviewed_by": [
                    "Ali H. Al-Hoorie",
                    "Bethan Iley",
                    "Flávio Azevedo",
                    "Robert Ross",
                    "Tobias Wingen \\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_"
                ],
                "alt_related_terms": [
                    null
                ],
                "language": "arabic"
            },
            {
                "type": "glossary",
                "title": "التَّأليف (Authorship)",
                "definition": "التَّأليف هو نسبة المخرجات البحثيَّة (مثل المخطوطات، والبيانات، والبرامج) لشخص أو أكثر، وكذلك تحمّل مسؤوليَّة محتواها (McNutt et al., 2018; Patience et al., 2019). تختلف الأعراف بين التَّخصصات، والثَّقافات وحتى المجموعات البحثيَّة في توقعاتها بشأن ماهية الجهد الذي يستحق أن يندرج تحت \"التَّأليف\"، وما يعنيه ترتيب المؤلفين \\- إن وجد \\- وفي مدى المسؤوليَّة التي يتحملُّها المؤلِّف عن البحث ،وعن جوانب العمل التي لم يقم بتنفيذها شخصيًا. **المصطلحات ذات الصِّلة:** التَّأليف المشترك، التحالف التَّأليفي، الإسهام، تصنيف أدوار المؤلفين، مبدأ التأكيد على المؤلِّفين الأول والأخير، التَّأليف المُهدى (أو المؤلف الضيف)، نهج التسلسل الذي يحدد المساهمة.",
                "related_terms": [
                    "Co-authorship",
                    "Consortium authorship",
                    "Contributorship",
                    "CRediT",
                    "First-last-author-emphasis norm (FLAE)",
                    "Gift (or Guest) Authorship",
                    "Sequence-determines-credit approach (SDC)"
                ],
                "references": "Academies, A. A. E. (2017). The European Code of Conduct for Research Integrity. Revised Edition. Retrieved from https://allea.org/code-of-conduct/\n\nGerman Research Foundation. (2019). Guidelines for Safeguarding Good Research Practice. Code of Conduct.\n\nMcNutt, M. K., Bradford, M., Drazen, J. M., Hanson, B., Howard, B., Jamieson, K. H., Kiermer, V., Marcus, E., Pope, B. K., Schekman, R., Swaminathan, S., Stang, P. J., & Verma, I. M. (2018). Transparency in authors’ contributions and responsibilities to promote integrity in scientific publication. Proceedings of the National Academy of Sciences of the United States of America, 115(11), 2557–2560. https://doi.org/10.1073/pnas.1715374115\n\nPatience, G. S., Galli, F., Patience, P. A., & Boffito, D. C. (2019). Intellectual contributions meriting authorship: Survey results from the top cited authors across all science categories. PLoS One, 14(1), e0198117. https://doi.org/10.1371/journal.pone.0198117",
                "drafted_by": [
                    "Jacob Miranda"
                ],
                "reviewed_by": [
                    "Bradley Baker",
                    "Brett J. Gall",
                    "Matt Jaquiery",
                    "Charlotte R. Pennington",
                    "Flávio Azevedo",
                    "Birgit Schmidt",
                    "Yuki Yamada"
                ],
                "alt_related_terms": [
                    null
                ],
                "language": "arabic"
            },
            {
                "type": "glossary",
                "title": "الفرضية المساعدة (Auxiliary Hypothesis)",
                "definition": "التعريف**: تحتوي جميع النظريات على افتراضات حول طبيعة المفاهيم وكيفية قياسها. ومع ذلك  لايتم اشتقاق جميع التنبؤات  من النظريات والافتراضات، فقد يتم استنتاج الافتراضات أحيانًا من مبادئ أخرى. يتم وضع افتراضات إضافية لاستنتاج التنبؤ ثم اختبارها عن طريق ربطها بالبيانات التي يمكن ملاحظتها. كما أنه يتم في بعض الأحيان تبرير سبب فشل محاولة تكرار النتائج بهذه الفرضيات المساعدة.  **المصطلحات ذات الصلة:** اللايقين المعرفي؛ الفرضية؛ الافتراضات الإحصائية؛ المتغيرات الوسيطة الخفية.",
                "related_terms": [
                    "Epistemic uncertainty",
                    "Hypothesis",
                    "Statistical assumptions",
                    "Hidden moderators"
                ],
                "references": "Dienes, Z. (2008). Understanding psychology as a science: An introduction to scientific and statistical inference. Macmillan International Higher Education.\n\nLakatos, I. (1978). The Methodology of Scientific Research Programs: Vol. I. Cambridge University Press.",
                "drafted_by": [
                    "Alaa Aldoh"
                ],
                "reviewed_by": [
                    "Ali H. Al-Hoorie",
                    "Nihan Albayrak-Aydemir",
                    "Mahmoud Elsherif",
                    "Bethan Iley",
                    "Sam Parsons",
                    "Flávio Azevedo"
                ],
                "alt_related_terms": [
                    null
                ],
                "language": "arabic"
            },
            {
                "type": "glossary",
                "title": "Open Science الشَّارات العلم المفتوح (Badges (Open Science))",
                "definition": "التعريف:** هي رموز تضيفها فرق التَّحرير للمخطوطات المنشورة؛ للإقرار بممارسات العلم المفتوح. وتكون بمثابة حوافز للباحثين؛ لمشاركة البيانات، والمواد، أو لتضمين التَّسجيل المسبق للدِّراسة، ولكونها رموزًا واضحة فإنَّ هذه الشَّارات تهدف إلى التَّوضيح للقارئ بأنَّ المحتوى قد استوفى معايير البحث المفتوح المطلوب لتلقي الشَّارة \\- المقدمة عادة من تلك المجلة- قد يتم تعيين شارات مختلِّفة لممارسات مختلِّفة، مثل إتاحة البحث وإمكانيَّة الوصول للمحتوى في موقع ثابت (\"شارة المحتوى المفتوح\" و \"شارة البيانات المفتوحة\") أو التَّسجيل المسبق للدِّراسة (شارة التَّسجيل المسبق).  **المصطلحات ذات الصِّلة**: حوافز، شارة البيانات المفتوحة، التَّسجيل المسبق، شارة ثلاثيَّة.",
                "related_terms": [
                    "Incentives",
                    "Open Data badge",
                    "Preregistration",
                    "Triple badge"
                ],
                "references": "Hardwicke, T. E., Bohn, M., MacDonald, K., Hembacher, E., Nuijten, M. B., Peloquin, B. N., & others. (2020). Analytic reproducibility in articles receiving open data badges at the journal Psychological Science: an observational study. Royal Society Open Science, 8(1), 201494. https://doi.org/10.1098/rsos.201494\n\nKidwell, M. C., Lazarević, L. B., Baranski, E., Hardwicke, T. E., Piechowski, S., Falkenberg, L. S., & Nosek, B. A. (2016). Badges to acknowledge open practices: A simple, low-cost, effective method for increasing transparency. PLoS Biology, 14(5), e1002456. https://doi.org/10.1371/journal.pbio.1002456",
                "drafted_by": [
                    "Jacob Miranda"
                ],
                "reviewed_by": [
                    "Brett Gall",
                    "Helena Hartmann",
                    "Mariella Paul",
                    "Charlotte R. Pennington",
                    "Lisa Spitzer",
                    "Suzanne L. K. Stewart"
                ],
                "alt_related_terms": [
                    null
                ],
                "language": "arabic"
            },
            {
                "type": "glossary",
                "title": "معامل بايز (Bayes Factor)",
                "definition": "مقياس إحصائيّ مستمر؛ لاختيار النَّموذج المستخدم في الاستدلال البايزي، يصف الأدلَّة النِّسبيَّة لنموذج ما على آخر، بغض النَّظر عما إذا كانت النَّماذج صحيحة أم لا. تتراوح معاملات بايز من 0 إلى ما لا نهاية، ممَّا يشير إلى القوة النِّسبيَّة للأدلَّة، وحيث 1 هي نقطة محايدة لعدم وجود دليل. وعلى النَّقيض من القيمة الاحتماليَّة، تسمح معاملات بايز بثلاثة أنواع  من الاستنتاجات:  (أ) دليل على الفرضيَّة البديلة. (ب) دليل على الفرضيَّة الصِّفريَّة. (ج) لا يوجد دليل كاف لأي منهما. وعليه يعبر BF10 عن وجود دليل لصالح الفرضيَّة البديلة مقارنة بالفرضيَّة الصِّفريَّة، و BF01 عن دليل لصالح الفرضيَّة الصِّفرية مقارنة بالفرضيَّة البديلة. **المصطلحات ذات الصِّلة:** الاستدلال البايزي، إحصاءات بايزية، دالة الاحتمالية، اختبار دلالة الفرضيَّة الصِّفريَّة، القيمة الاحتماليَّة.",
                "related_terms": [
                    "Bayesian inference",
                    "Bayesian statistics",
                    "Likelihood function",
                    "Null Hypothesis Significance Testing (NHST)",
                    "*p*\\-value"
                ],
                "references": "Hoijtink, H., Mulder, J., van Lissa, C., & Gu, X. (2019). A tutorial on testing hypotheses using the Bayes factor. Psychological Methods, 24(5), 539–556. https://doi.org/10.1037/met0000201\n\nMakowski, D., Ben-Shachar, M. S., Chen, S. H. A., & Lüdecke, D. (2019). Indices of Effect Existence and Significance in the Bayesian Framework. https://doi.org/10.3389/fpsyg.2019.02767",
                "drafted_by": [
                    "Meng Liu"
                ],
                "reviewed_by": [
                    "Alaa AlDoh",
                    "Helena Hartmann",
                    "Connor Keating",
                    "Kai Krautter",
                    "Michele C. Lim",
                    "Suzanne L. K. Stewart",
                    "Ana Todorovic"
                ],
                "alt_related_terms": [
                    null
                ],
                "language": "arabic"
            },
            {
                "type": "glossary",
                "title": "الاستدلال البايزي (Bayesian Inference)",
                "definition": "التعريف:** طريقة للاستدلال الإحصائي تعتمد على نظرية بايز، والتي تستخدم (عدم) اليقين المعرفي باستخدام اللغة الرياضية للاحتمالات. يعتمد الاستدلال البايزي على تخصيص (وإعادة تخصيص ، بناء على البيانات أو الأدلة التي تمت ملاحظتها حديثا) والمصداقية عبر الاحتمالات. هناك طريقتان للاستدلال البايزي يشملان \"عوامل بايز\" وتقدير معامل بايزي.  **المصطلحات ذات الصلة:** معامل بايز;  إحصاءات بايزية، تقدير المعاملات باستخدام النهج البايزي.",
                "related_terms": [
                    "Bayes Factor",
                    "Bayesian statistics",
                    "Bayesian Parameter Estimation"
                ],
                "references": "Dienes, Z. (2011). Bayesian versus orthodox statistics: Which side are you on? Perspectives on Psychological Science, 6(3), 274–290. https://doi.org/10.1177/1745691611406920\n\nDienes, Z. (2014). Using Bayes to get the most out of non-significant results. Frontiers in Psychology, 5, 781. https://doi.org/10.3389/fpsyg.2014.00781\n\nDienes, Z. (2016). How Bayes factors change scientific practice. Journal of Mathematical Psychology, 72, 78–89. https://doi.org/10.1016/j.jmp.2015.10.003\n\nEtz, A., Gronau, Q. F., Dablander, F., & others. (2018). How to become a Bayesian in eight easy steps: An annotated reading list. Psychonomic Bulletin & Review, 25, 219–234. https://doi.org/10.3758/s13423-017-1317-5\n\nKruschke, J. K. (2015). Doing Bayesian data analysis: A tutorial with R, JAGS, and Stan (2nd ed.). Academic Press.\n\nMcElreath, R. (2020). Statistical rethinking: A Bayesian course with examples in R and Stan (2nd ed.). Taylor.\n\nWagenmakers, E.-J., Marsman, M., Jamil, T., Ly, A., Verhagen, J., Love, J., Selker, R., Gronau, Q. F., Šmíra, M., Epskamp, S., Matzke, D., Rouder, J. N., & Morey, R. D. (2018). Bayesian inference for psychology. Part I: Theoretical advantages and practical ramifications. Psychonomic Bulletin & Review, 25(1), 35–57. https://doi.org/10.3758/s13423-017-1343-3",
                "drafted_by": [
                    "Charlotte R. Pennington"
                ],
                "reviewed_by": [
                    "Alaa AlDoh",
                    "Bradley Baker",
                    "Robert Ross",
                    "Markus Weinmann",
                    "Tobias Wingen",
                    "Steven Verheyen"
                ],
                "alt_related_terms": [
                    null
                ],
                "language": "arabic"
            },
            {
                "type": "glossary",
                "title": "تقدير المعاملات باستخدام النَّهج البايزي (Bayesian Parameter Estimation)",
                "definition": "يقوم النَّهج البايزي بتقدير قيم المعاملات من خلال تحديث اعتقاد مسبق حول معاملات النَّموذج \\_ وبعبارة أخرى حول التَّوزيع المسبق\\_ وذلك لوجود أدلَّة جديدة (أي البيانات الملاحظة) وذلك باستخدام دالة الإمكان؛ ممَّا يؤدي إلى التَّوزيع اللَّاحق.  يمكن تلخيص التَّوزيع اللَّاحق بعدَّة طرق بما في ذلك:  تقدير دقيق لمتوسط، ووسيط، ومنوال التَّوزيع البعدي المحتمل، والفترات المتقطِّعة للحدود، وللكتلة المحدَّدة (يُشار للفترات المتقطِّعة للكتلة المحدَّدة على أنها الفترات الموثوق بها)، وفي المقابل قد يصبح التَّوزيع اللَّاحق توزيعًا مسبقًا في مرحلة تالية، كما يمكن أخذ عيّنات من التَّوزيع اللَّاحق باستخدام سلسلة ماركوف مونتي كارلو والتي يمكن استخدامها لتحديد أوجه الشَّك المعقّدة في النَّموذج (Foreman-Mackey et al., 2013). **المصطلحات ذات الصِّلة:** معامل بايز، الاستدلال البايزي، الإحصاءات البايزية، اختبار دلالة الفرضية الصِّفريَّة.",
                "related_terms": [
                    "Bayes Factor",
                    "Bayesian inference",
                    "Bayesian statistics",
                    "Null Hypothesis Significance Testing (NHST)"
                ],
                "references": "Foreman-Mackey, D., Hogg, D. W., Lang, D., & Goodman, J. (2013). emcee: The MCMC Hammer. Publications of the Astronomical Society of the Pacific, 125(925), 306–312. https://doi.org/10.1086/670067\n\nMcElreath, R. (2020). Statistical rethinking: A Bayesian course with examples in R and Stan (2nd ed.). Taylor.\n\nPress, W. (2007). Numerical recipes: the art of scientific computing, 3rd edition.\n\nHuber, C. (2016). Introduction to Bayesian statistics, part 2: MCMC and the Metropolis–Hastings algorithm. In The Stata Blog. https://blog.stata.com/2016/11/15/introduction-to-bayesian-statistics-part-2-mcmc-and-the-metropolis-hastings-algorithm/",
                "drafted_by": [
                    "Alaa AlDoh"
                ],
                "reviewed_by": [
                    "Mahmoud Elsherif",
                    "Helena Hartmann",
                    "Dominik Kiersz",
                    "Meng Liu",
                    "Ana Todorovic",
                    "Markus Weinmann"
                ],
                "alt_related_terms": [
                    null
                ],
                "language": "arabic"
            },
            {
                "type": "glossary",
                "title": "بنية بيانات تصوير الدِّماغ (BIDS data structure)",
                "definition": "تصف بنية بيانات تصوير الدِّماغ طريقة بسيطة، وسهلة التَّبني؛ لتنظيم التَّصوير العصبيّ و الفيزيولوجيا الكهربيّة، والبيانات السُّلوكيَّة (أي تنسيقات الملفات، وهياكل المجلَّدات).  وهذه البنية جهد مجتمعي طوَّره المجتمع من أجل المجتمع، وكان مستوحى من التَّنسيق المستخدم داخليًا بواسطة مستودع OpenfMRI المعروف باسم OpenNeuro. بعد أن تم تطويره في البداية لبيانات التَّصوير بالرَّنين المغناطيسيّ الوظيفيّ، تم توسيع البنية للعديد من التَّدابير الأخرى، مثل مخطط كهربية الدِّماغ (Pernet et al., 2019). **المصطلحات ذات الصِّلة**: البيانات المفتوحة.",
                "related_terms": [
                    "Open Data"
                ],
                "references": "Gorgolewski, K., Auer, T., Calhoun, V., & others. (2016). The brain imaging data structure, a format for organizing and describing outputs of neuroimaging experiments. Scientific Data, 3, 160044. https://doi.org/10.1038/sdata.2016.44\n\nBIDS. (2020). About BIDS. Retrieved from https://bids.neuroimaging.io",
                "drafted_by": [
                    "Tina Lonsdorf"
                ],
                "reviewed_by": [
                    "Ali H. Al-Hoorie",
                    "David Moreau",
                    "Mariella Paul",
                    "Charlotte R. Pennington"
                ],
                "alt_related_terms": [
                    null
                ],
                "language": "arabic"
            },
            {
                "type": "glossary",
                "title": "بيزار (BIZARRE)",
                "definition": "يشير مصطلح (BIZARRE) إلى اختصار البيئات القاحلة، المؤسساتيَّة، وحدائق الحيوان، وغيرها من البيئات المناخيَّة لتربية الحيوانات.  يتم إجراء معظم البحوث في هذه العينة بالتَّحديد عن الشَّمبانزي، والذي يحد بدوره من قابليَّة تعميم عدد كبير من نتائج البحوث على مجتمع الشَّمبانزي. يُعتقد أن بيزار يعكس المفهوم العالمي لكلِّ ماهو شمبانزي (راجع أيضًا وِيرد، والذي قيل إنه مفهوم عالمي لما هو إنسان). **المصطلحات ذات الصِّلة**: المجتمع، سترينج: الخلفية الاجتماعية والقابلية للتتبع والاختيار الذاتي وتاريخ التربية والتأقلم والتّعود، وِيرد",
                "related_terms": [
                    "Populations",
                    "STRANGE",
                    "WEIRD"
                ],
                "references": "Clark, H., Elsherif, M. M., & Leavens, D. A. (2019). Ontogeny vs. phylogeny in primate/canid comparisons: a meta-analysis of the object choice task. Neuroscience & Biobehavioral Reviews, 105, 178–189. https://doi.org/10.1016/j.neubiorev.2019.06.001\n\nLeavens, D. A., Bard, K. A., & Hopkins, W. D. (2010). BIZARRE chimpanzees do not represent “the chimpanzee.” Behavioral and Brain Sciences, 33(2–3), 100–101. https://doi.org/10.1017/S0140525X10000166",
                "drafted_by": [
                    "Mahmoud Elsherif"
                ],
                "reviewed_by": [
                    "Zoe Flack",
                    "Charlotte R. Pennington"
                ],
                "alt_related_terms": [
                    null
                ],
                "language": "arabic"
            },
            {
                "type": "glossary",
                "title": "to Open Scholarship النَّهج التَّصاعديّ (Bottom-up approach (to Open Scholarship))",
                "definition": "في الثَّقافة الأكاديميَّة يركز النَّهج التَّصاعديّ على الاهتمام الجوهريّ للأكاديميين بتحسين جودة البحوث وثقافة البحث، عبر جعلها داعمة، وتعاونيّة، ومبدعة، وشاملة.  وفي الغالب يشير النَّهج التَّصاعديّ لقيادة الباحثين حديثي الانضمام للحقل البحثيّ في بداية حياتهم المهنيَّة كصانعي التَّغيير الذين يقودون التَّحولات، والتَّغيير في المنهجيَّة العلميَّة من خلال الحماس والابتكار. ومقارنة بالنَّهج التنازلي والذي بدأه كبار الباحثين، فإنَّ النَّهج التَّصاعدي من أسفل إلى أعلى يأخذ بالحسبان الظُّروف المحليَّة والمحدَّدة بكلِّ حالة، والتي في كثير من الأحيان تستخدم البيانات التَّطبيقيَّة، والخبرات الحياتيَّة المعاشة، والاعتبارات الشَّخصية، والظُّروف كنقطة بداية  لتطوير حلول السِّياسات. **المصطلحات ذات الصِّلة:** الباحثون المبتدئون، المبادرات الشَّعبيَّة.",
                "related_terms": [
                    "Early Career Researchers (ECRs)",
                    "Grassroot initiatives"
                ],
                "references": "Button, K. S., Lawrence, N. S., Chambers, C. D., & Munafò, M. R. (2016). Instilling scientific rigour at the grassroots. Psychologist, 29(3), 158–159.\n\nButton, K. S., Chambers, C. D., Lawrence, N., & Munafò, M. R. (2020). Grassroots training for reproducible science: a consortium-based approach to the empirical dissertation. Psychology Learning & Teaching, 19(1), 77–90. https://doi.org/10.1177/1475725719857659\n\nHart, D. D., & Silka, L. (2020). Rebuilding the Ivory Tower: A Bottom-Up Experiment in Aligning Research with Societal Needs. Issues in Science and Technology, 79–85. Retrieved from https://issues.org/aligning-research-with-societal-needs/\n\nMeslin, E. M. (2009). Achieving global justice in health through global research ethics: supplementing Macklin’s ‘top-down’ approach with one from the ‘ground up’. In R. M. Green, A. Donovan, & S. A. Jauss (Eds.), Global Bioethics: Issues of Conscience for the Twenty-First Century (pp. 163–177). University Press.\n\nMoran, H., Karlin, L., Lauchlan, E., Rappaport, S. J., Bleasdale, B., Wild, L., & Dorr, J. (2020). Understanding Research Culture: What researchers think about the culture they work in. Wellcome Open Research, 5, 201. https://doi.org/10.12688/wellcomeopenres.15832.1\n\nNosek, B. A. (2019). Strategy for Culture Change. Center for Open Science. https://www.cos.io/blog/strategy-for-culture-change",
                "drafted_by": [
                    "Catherine Laverty"
                ],
                "reviewed_by": [
                    "Helena Hartmann",
                    "Michele C. Lim",
                    "Adam Parker",
                    "Charlotte R. Pennington",
                    "Birgit Schmidt",
                    "Marta Topor",
                    "Flávio Azevedo"
                ],
                "alt_related_terms": [
                    null
                ],
                "language": "arabic"
            },
            {
                "type": "glossary",
                "title": "تأطير المقابلات (Bracketing Interviews)",
                "definition": "يستخدم تأطير المقابلات على نحوِ شائع في المنهج النَّوعي.  يحاول الباحثون خلالها استكشاف تحيزاتهم الشَّخصيَّة، وافتراضاتهم المتعلِّقة بالبحوث التي يجرونها. ويتيح ذلك للباحثين أن يكونوا على دراية باهتماماتهم الخاصّة، ويساعدهم على أن يصبحوا أكثر تأملًا في بحوثهم وأكثر انتقادًا لها، مع الأخذ في الاعتبار كيفيَّة تأثير تجاربهم الخاصَّة على عمليَّة البحث. ويمكن أن يخضع تأطير المقابلات نفسه للتَّحليل النَّوعي. **المصطلحات ذات الصِّلة**: البحث النَّوعي، الانعكاسيَّة، تحيُّز الباحث.",
                "related_terms": [
                    "Qualitative research",
                    "Reflexivity",
                    "Researcher bias **Reference (s)**:  \\[@RollsRelf2006\\], \\[@Sorsa2015\\]"
                ],
                "references": "",
                "drafted_by": [
                    "Claire Melia"
                ],
                "reviewed_by": [
                    "Tamara Kalandadze",
                    "Charlotte R. Pennington",
                    "Graham Reid",
                    "Marta Topor"
                ],
                "alt_related_terms": [
                    null
                ],
                "language": "arabic"
            },
            {
                "type": "glossary",
                "title": "بروبنساينس (Bropenscience)",
                "definition": "تعبير ساخر يهدف إلى زيادة الوعي بنقص الأصوات المتنوِّعة في العلوم المفتوحة (Bahlai, Bartlett, Burgio et al., 2019; Onie, 2020)، بالإضافة إلى وجود أنماط سلوكيَّة، وتواصليَّة يمكن أن تكون سامّة، أو إقصائيَّة.  الأهم من ذلك، ليس بالضَّرورة أن يكون كل هؤلاء الأفراد ذكورًا، بل هم أفراد يظهرون تفكيرًا جامدًا، ويفتقرون إلى الوعي الذَّاتي، ويميلون إلى العداء، والقسوة، والإقصاء (Pownall et al., 2021; Whitaker & Guest, 2020). و ينتمون عمومًا إلى مجموعات مهيمنة تستفيد من الامتيازات الهيكليَّة. لمعالجة هذه الظاهرة يجب على الباحثين دراسة وفحص ومعالجة التباينات واللامساواة الهيكليَّة داخل الأنظمة، والمؤسسات الأكاديميَّة. **المصطلحات ذات الصِّلة**: التَّنوع، الشمول، التَّقاطعيَّة، العلم المفتوح.",
                "related_terms": [
                    "Diversity",
                    "Inclusion",
                    "Intersectionality",
                    "Open Science **Reference (s)**: \\[@GuestTweet2017\\], \\[@Whitaker2020\\], \\[@Pownall20210\\]"
                ],
                "references": "",
                "drafted_by": [
                    "Zoe Flack"
                ],
                "reviewed_by": [
                    "Magdalena Grose-Hodge",
                    "Helena Hartmann",
                    "Bethan Iley",
                    "Tamara Kalandadze",
                    "Adam Parker",
                    "Charlotte R. Pennington",
                    "Flávio Azevedo",
                    "Bradley Baker",
                    "Mahmoud Elsherif"
                ],
                "alt_related_terms": [
                    null
                ],
                "language": "arabic"
            },
            {
                "type": "glossary",
                "title": "النقد بعد معرفة النتائج (CARKing)",
                "definition": "يشير هذا المصطلح إلى تقديم نقد لمنهجيَّة الدِّراسة، كما لو كان الشَّخص سيقدمه قبل معرفة النَّتائج، وعادة ما يشكِّل رد فعل، أو نقدًا لنتائج غير مرحَّب بها، أو غير مرغوب فيها، سواء كان النَّاقد واعيًا بهذه الحقيقة أم لا.  **المصطلحات ذات الصِّلة:** الافتراض بعد معرفة النتائج، التسجيل بعد معرفة النتائج، التَّسجيل المسبق، التَّقرير المسجَّل، السباركينج",
                "related_terms": [
                    "HARKing",
                    "Preregistration",
                    "Registered Report"
                ],
                "references": "Bardsley, N. (2018). What lessons does the “replication crisis” in psychology hold for experimental economics? In Handbook of Psychology and Economic Behaviour, 2nd edition. Cambridge University Press. Retrieved from http://centaur.reading.ac.uk/69874/\n\nNosek, B. A., & Lakens, D. (2014). Registered reports. Social Psychology, 45, 137–141. https://doi.org/10.1027/1864-9335/a000192",
                "drafted_by": [
                    "Mahmoud Elsherif"
                ],
                "reviewed_by": [
                    "Ali H. Al-Hoorie",
                    "Ashley Blake",
                    "Adrien Fillon",
                    "Charlotte R. Pennington"
                ],
                "alt_related_terms": [
                    null
                ],
                "language": "arabic"
            },
            {
                "type": "glossary",
                "title": "COS مركز العلم المفتوح (Center for Open Science (COS))",
                "definition": "منظمة غير ربحيَّة،  تعنى بالتُّكنولوجيا، ومقرها في شارلوتسفيل بولاية فيرجينيا، وتهتم بزيادة الشَّفافيَّة، والنَّزاهة، وإمكانيَّة تكرار البحوث. ويستضيف المركز إطار العلوم المفتوحة وقاعدة المعرفة المفتوحة بالإضافة إلى مصادر أخرى.  **المصطلحات ذات الصِّلة**: شارات العلوم المفتوحة، إطار العلوم المفتوحة، مجموعات OSF ، مؤسسات OSF، اجتماعات OSF، طبعات أولية OSF، سجلات OSF، الَّتسجيلات (التسجيلات المسبقة، والتَّقارير المسجلة)، إرشادات تعزيز الشَّفافيَّة والانفتاح (TOP)",
                "related_terms": [
                    "Open Science badges",
                    "Open Science Framework",
                    "OSF collections",
                    "OSF institutions",
                    "OSF meetings",
                    "OSF preprints",
                    "OSF registries",
                    "Registrations (Preregistrations & Registered Reports)",
                    "Transparency and Openness Promotion Guidelines (TOP)"
                ],
                "references": "for Open Science, C. (n.d.). Show Your Work. Share Your Work. Advance Science. That’s Open Science. https://www.cos.io/",
                "drafted_by": [
                    "Beatrix Arendt"
                ],
                "reviewed_by": [
                    "Ali H. Al-Hoorie",
                    "Mariella Paul",
                    "Charlotte R. Pennington",
                    "Lisa Spitzer"
                ],
                "alt_related_terms": [
                    null
                ],
                "language": "arabic"
            },
            {
                "type": "glossary",
                "title": "تحيُّز الاستشهاد (Citation bias)",
                "definition": "هو اختيار متحيِّز للبحوث أو المؤلفين الذين يتم الاستشهاد بهم، وإدراجهم في قسم المراجع. عندما يكون هناك تحيُّز في الاستشهاد، فغالبًا ما يكون ذلك بطريقة تفيد المؤلِّفين، أو المحكِّمين، أو تبالغ في تمثيل الدِّراسات ذات الدِّلالة الإحصائيَّة، أو تعكس التَّحيُّزات المتعلقة بالنوع البشري أو التحيزات العرقيَّة (Brooks, 1985; Jannot et al., 2013; Zurn et al., 2020). ولعل أحد الحلول المقترحة هو استخدام \"بيان تنوع الاستشهاد\" بحيث ينظر المؤلِّفون لممارساتهم في الاستشهاد، ويحدِّدون مواطن التَّحيُّز التي قد تظهر.  **المصطلحات ذات الصلة:** بيان تنُّوع  الاستشهادات، تقرير التَّحيُّز.",
                "related_terms": [
                    "Citation diversity statement",
                    "Reporting bias"
                ],
                "references": "Brooks, T. A. (1985). Private acts and public objects: An investigation of citer motivations. Journal of the American Society for Information Science, 36(4), 223–229. https://doi.org/10.1002/asi.4630360402\n\nJannot, A. S., Agoritsas, T., Gayet-Ageron, A., & Perneger, T. V. (2013). Citation bias favoring statistically significant studies was present in medical research. Journal of Clinical Epidemiology, 66(3), 296–301. https://doi.org/10.1016/j.jclinepi.2012.09.015\n\nThombs, B. D., Levis, A. W., Razykov, I., Syamchandra, A., Leentjens, A. F., Levenson, J. L., & Lumley, M. A. (2015). Potentially coercive self-citation by peer reviewers: a cross-sectional study. Journal of Psychosomatic Research, 78(1), 1–6. https://doi.org/10.1016/j.jpsychores.2014.09.015\n\nZurn, P., Bassett, D. S., & Rust, N. C. (2020). The Citation Diversity Statement: A Practice of Transparency, A Way of Life. Trends in Cognitive Sciences, 24(9), 669–672. https://doi.org/10.1016/j.tics.2020.06.009",
                "drafted_by": [
                    "Bettina M. J. Kern"
                ],
                "reviewed_by": [
                    "Mahmoud Elsherif",
                    "Annalise A. LaPlume",
                    "Helena Hartmann",
                    "Bethan Iley",
                    "Charlotte R. Pennington",
                    "Timo Roettger",
                    "Tobias Wingen"
                ],
                "alt_related_terms": [
                    null
                ],
                "language": "arabic"
            },
            {
                "type": "glossary",
                "title": "بيان تنوُّع الاستشهادات (Citation Diversity Statement)",
                "definition": "هو جهود جارية لزيادة الوعي، وتخفيف تحيُّز الاستشهاد فيما يتعلَّق بالنوع البشري والعرق، وهو عبارة عن فقرة قصيرة، حيث \"يأخذ المؤلِّفون في الاعتبار تحيُّزهم الشَّخصي، ويقيسون مدى إنصاف قوائمهم المرجعيَّة. تنصُّ الفقرة على: (1) أهميَّة تنوُّع الاستشهادات، (2) النِّسبة المئويَّة للتَّفصيل (أو مؤشرات التَّنوع الأخرى) للاستشهادات في الورقة، (3) الطَّريقة التي تم من خلالها تقييم النِّسب المئويَّة وقيودها،(4) الالتزام بتحسين الممارسات العادلة في العلوم\" (Zurn et al., 2020, p. 669). **المصطلحات ذات الصِّلة:** تحيُّز الاستشهاد، التنوُّع،  نقص التَّمثيل.",
                "related_terms": [
                    "Citation bias",
                    "Diversity",
                    "Under-representation"
                ],
                "references": "Zurn, P., Bassett, D. S., & Rust, N. C. (2020). The Citation Diversity Statement: A Practice of Transparency, A Way of Life. Trends in Cognitive Sciences, 24(9), 669–672. https://doi.org/10.1016/j.tics.2020.06.009",
                "drafted_by": [
                    "Helena Hartmann"
                ],
                "reviewed_by": [
                    "Mahmoud Elsherif",
                    "Magdalena Grose-Hodge",
                    "Sam Parsons",
                    "Timo Roettger"
                ],
                "alt_related_terms": [
                    null
                ],
                "language": "arabic"
            },
            {
                "type": "glossary",
                "title": "علم المواطن (Citizen Science)",
                "definition": "يشير مصطلح علم المواطن إلى المشاريع العلميَّة التي تتضمَّن مشاركة فعَّالة من قبل الجمهور في المساعي العلميَّة بهدف إضفاء الطَّابع الدِّيمقراطي على العلوم.  يمكن أن يشارك العلماء المواطنون في جميع مراحل البحث، حيث يعملون كمتعاونين، أو مساهمين أو قادة مشاريع. من الأمثلة على مشروع علمي كبير للمواطنين هو تحديد الأفراد للأجسام الفلكيَّة (Lintott, 2008). ال**مصطلحات ذات الصِّلة:** علم الحشود، توظيف الجماهير. التَّعريف البديل: (إن أمكن) في الماضي كان علم المواطن يشير في الغالب إلى المتطوعين الذين يشاركون كمساعدين ميدانيين في الدِّراسات العلميَّة (Cohn, 2008, p. 193).",
                "related_terms": [
                    "Crowd science",
                    "Crowdsourcing **Alternative definition:** (if applicable) In the past, citizen science mostly referred to volunteers who participate as field assistants in scientific studies (Cohn, 2008, p. 193)."
                ],
                "references": "Cohn, J. P. (2008). Citizen science: Can volunteers do real research? BioScience, 58(3), 192–197. https://doi.org/10.1641/B580303\n\nLintott, C. J., Schawinski, K., Slosar, A., Land, K., Bamford, S., Thomas, D., & Vandenberg, J. (2008). Galaxy Zoo: morphologies derived from visual inspection of galaxies from the Sloan Digital Sky Survey. Monthly Notices of the Royal Astronomical Society, 389(3), 1179–1189. https://doi.org/10.1111/j.1365-2966.2008.13689.x",
                "drafted_by": [
                    "Mahmoud Elsherif; Ana Barbosa Mendes"
                ],
                "reviewed_by": [
                    "Gisela H. Govaart",
                    "Tamara Kalandadze",
                    "Dominik Kiersz",
                    "Charlotte R. Pennington",
                    "Robert M. Ross"
                ],
                "alt_related_terms": [
                    null
                ],
                "language": "arabic"
            },
            {
                "type": "glossary",
                "title": "شبكة أرشيف المعرفة الشَّاملة (CKAN)",
                "definition": "توفِّر شبكة أرشيف المعرفة الشَّاملة منصَّة مفتوحة المصدر، وبرمجيات مجانيَّة تهدف إلى توفير أدوات من شأنها تسهيل نشر البيانات، ومشاركتها، كما تدعم الشَّبكة الحكومات، ومؤسَّسات البحث، ومنظَّمات أخرى في إدارة ونشر كميَّات هائلة من البيانات.  **المصطلحات ذات الصِّلة:** منصَّات البيانات، مشاركة البيانات.",
                "related_terms": [
                    "Data platforms",
                    "Data sharing"
                ],
                "references": "Anon. (n.d.). Ckan. Retrieved from https://ckan.org/",
                "drafted_by": [
                    "Tsvetomira Dumbalska"
                ],
                "reviewed_by": [
                    "Ali H. Al-Hoorie",
                    "Kai Krautter",
                    "Charlotte R. Pennington"
                ],
                "alt_related_terms": [
                    null
                ],
                "language": "arabic"
            },
            {
                "type": "glossary",
                "title": "الإطار المجتمعي للممارسات الجيِّدة في المستودعات (COAR Community Framework for Good Practices in Repositories)",
                "definition": "إطار يحدد أفضل الممارسات للمستودعات العلميَّة، ومعايير التَّقييم لهذه الممارسات. ويتميَّز نهجه \\- المرن والمتعدد الأبعاد \\- بإمكانيَّة تطبيقه على أنواع مختلفة من المستودعات، بما في ذلك تلك التي تستضيف منشورات، أو بيانات، وذلك باختلاف السِّياقات الجغرافيَّة، والموضوعيَّة.  **المصطلحات ذات الصِّلة:** البيانات الوصفيَّة، الوصول المفتوح، البيانات المفتوحة، المواد المفتوحة، مستودع البيانات، مبادئ الثِّقة.",
                "related_terms": [
                    "Metadata",
                    "Open Access",
                    "Open Data",
                    "Open Material",
                    "Repository",
                    "TRUST principles"
                ],
                "references": "of Open Access Repositories, C. (2020). COAR Community Framework for Best Practices in Repositories (Version 1). Zenodo. https://doi.org/10.5281/zenodo.4110829",
                "drafted_by": [
                    "Aleksandra Lazić"
                ],
                "reviewed_by": [
                    "Ashley Blake",
                    "Jamie P. Cockcroft",
                    "Bethan Iley",
                    "Sam Parsons"
                ],
                "alt_related_terms": [
                    null
                ],
                "language": "arabic"
            },
            {
                "type": "glossary",
                "title": "دليل الرُّموز (Codebook)",
                "definition": "يُعد دليل الرُّموز ملخصًا عالي المستوى، يصف محتويات مجموعة البيانات، وهيكلها وطبيعتها، وشكلها.  يحتوي دليل الرُّموز الموثَّق جيداً على معلومات تهدف إلى أن تكون كاملة، وواضحة في حدِّ ذاتها لكلِّ متغيِّر في ملف البيانات، مثل تسمية وترميز العنصر، والمفهوم الذي يمثِّله. يوفر دليل الرُّموز الشَّفافيَّة للباحثين الذين قد لا يكونون على دراية بالبيانات، ولكنَّهم يرغبون في إعادة إنتاج التَّحليلات، أو إعادة استخدام البيانات. **المصطلحات ذات الصِّلة**: قاموس البيانات،  البيانات الوصفيَّة.",
                "related_terms": [
                    "Data dictionary",
                    "Metadata"
                ],
                "references": "Arslan, R. C. (2019). How to Automatically Document Data With the codebook Package to Facilitate Data Reuse. Advances in Methods and Practices in Psychological Science, 2(2), 169–187. https://doi.org/10.1177/2515245919838783",
                "drafted_by": [
                    "Tina Lonsdorf"
                ],
                "reviewed_by": [
                    "Ali H. Al-Hoorie",
                    "Ashley Blake, Kai Krautter",
                    "Charlotte R. Pennington",
                    "Flávio Azevedo"
                ],
                "alt_related_terms": [
                    null
                ],
                "language": "arabic"
            },
            {
                "type": "glossary",
                "title": "مراجعة النَّص البرمجي (Code review)",
                "definition": "عمليَّة التَّحقق من برمجة باحث آخر (على وجه التَّحديد، شفرة مصدر الحاسوب) بما في ذلك على سبيل المثال لا الحصر، نص البرمجي الإحصائي، ونمذجة البيانات. تم تصميم هذه العمليَّة؛ لاكتشاف الأخطاء وحلها، مما يحسّن جودة التَّعليمات البرمجية من النَّاحية العمليَّة ، قد تتم عملية مراجعة النُّظراء الحديثة عبر مستودع مستضاف على الإنترنت مثل GitHub أو GitLab أو SourceForge. **المصطلحات ذات الصِّلة:** قابلية إعادة الإنتاج، التَّحكم في الإصدار",
                "related_terms": [
                    "Reproducibility",
                    "Version control"
                ],
                "references": "Petre, M., & Wilson, G. (2014). Code review for and by scientists. arXiv Preprint arXiv:1407.5648. https://arxiv.org/abs/1407.5648\n\nScopatz, A. M., & Huff, K. D. (2015). Effective Computation in Physics: Field Guide to Research with Python (1st ed.). O’Reilly Media. http://shop.oreilly.com/product/0636920033424.do",
                "drafted_by": [
                    "Filip Dechterenko"
                ],
                "reviewed_by": [
                    "Ali H. Al-Hoorie",
                    "Dominik Kiersz",
                    "Sam Parsons",
                    "Charlotte R. Pennington"
                ],
                "alt_related_terms": [
                    null
                ],
                "language": "arabic"
            },
            {
                "type": "glossary",
                "title": "CREP مشروع التِّكرار والتَّعليم التَّعاوني (Collaborative Replication and Education Project (CREP))",
                "definition": "مبادرة مصمَّمة لتنظيم، وهيكلة جهود التِّكرار للدراسات التَّطبيقية ذات الاستشهاد العالي في علم النَّفس وذلك لتلبية الاحتياج لكل من الدراسات التكريرية المباشرة ذات الجودة العالية والمزيد من التَّدريب على تقنيات البحوث التجريبية لطلبة علم النَّفس.  يهدف المشروع إلى تلبية الحاجة؛ لتكرار الدِّراسات التي تم الاستشهاد بها بشكل كبير، وتوفير التَّدريب والدَّعم، وفرص النُّمو المهني للأكاديميين الذين يكملون مشاريع البحوث التِّكرارية.  **المصطلحات ذات الصِّلة:** التِّكرار المباشر، التِّكرار الدَّقيق.",
                "related_terms": [
                    "Direct replication",
                    "Exact replication"
                ],
                "references": "Wagge, J. R., Baciu, C., Banas, K., Nadler, J. T., Schwarz, S., Weisberg, Y., & others. (2019). A demonstration of the collaborative replication and education project: Replication attempts of the red-romance effect. Collabra: Psychology, 5(1). https://doi.org/10.1525/collabra.177",
                "drafted_by": [
                    "Connor Keating"
                ],
                "reviewed_by": [
                    "Bradley Baker",
                    "Mahmoud Elsherif",
                    "Zoe Flack",
                    "Charlotte R. Pennington"
                ],
                "alt_related_terms": [
                    null
                ],
                "language": "arabic"
            },
            {
                "type": "glossary",
                "title": "COBIDAS لجنة أفضل الممارسات في تحليل البيانات ومشاركتها (Committee on Best Practices in Data Analysis and Sharing (COBIDAS))",
                "definition": "طور مجتمع التَّصوير العصبي التابع لمنظمة رسم خرائط الدِّماغ البشري دليلًا لأفضل الممارسات في جمع بيانات التَّصوير العصبي، وتحليلها وإعداد التَّقارير الخاصّة بها، ومشاركة كلّ من البيانات والنَّص البرمجي للتَّحليل.  يتضمن هذا المبدأ ثمانية عناصر يجب استيفاؤها عند كتابة أو تقديم مخطوطة من أجل تحسين طرق إعداد التَّقارير، والصُّور العصبيَّة النَّاتجة لتحقيق أقصى درجات الشَّفافيَّة وإعادة الإنتاج.  **تعريف بديل: (إن أمكن)** قائمة مرجعية لتحليل البيانات ومشاركتها.",
                "related_terms": [],
                "references": "Nichols, T. E., Das, S., Eickhoff, S. B., Evans, A. C., Glatard, T., Hanke, M., & others. (2017). Best practices in data analysis and sharing in neuroimaging using MRI. Nature Neuroscience, 20(3), 299–303. https://doi.org/10.1038/nn.4500\n\nPernet, C., Garrido, M. I., Gramfort, A., Maurits, N., Michel, C. M., Pang, E., & others. (2020). Issues and recommendations from the OHBM COBIDAS MEEG committee for reproducible EEG and MEG research. Nature Neuroscience, 23(12), 1473–1483. https://doi.org/10.1038/s41593-020-00709-0",
                "drafted_by": [
                    "Yu-Fang Yang"
                ],
                "reviewed_by": [
                    "Jamie P. Cockcroft",
                    "Helena Hartmann",
                    "Adam Parker",
                    "Sam Parsons"
                ],
                "alt_related_terms": [
                    null
                ],
                "language": "arabic"
            },
            {
                "type": "glossary",
                "title": "التَّشاركيَّة (Communality)",
                "definition": "يشير هذا المصطلح للملكيَّة المشتركة للنتائج والأساليب العلميَّة، وما يترتَّب على ذلك من ضرورة مشاركتها بحريّة.  تؤمن التَّشاركيَّة  بأنَّ كلَّ اكتشافٍ علميّ هو نتيجة جهد عدد من أفراد المجتمع، وبناءً عليه، فإنَّ على الباحثين مشاركة نتائج بحوثهم بشكل علني  مع زملائهم.  **المصطلحات ذات الصِّلة: ا**لمعايير الميرتونية، الموضوعيَّة.  **تعريف بديل:** الاشتراكيَّة (Merton, 1942)  تعتبر الطرق والنتائج العلمية في البحث ملكية عامة وإرثًا مشتركًا لذا فإنه من الضرورة ان يتشارك الجميع هذه الطرق والنتائج  بحرية ومصداقية.",
                "related_terms": [
                    "Mertonian norms",
                    "Objectivity **Alternative definition:** Communism (in Merton, 1942\\) **Related terms to alternative definition** (if applicable)"
                ],
                "references": "Anderson, M. S., Ronning, E. A., Devries, R., & Martinson, B. C. (2010). Extending the Mertonian norms: Scientists’ subscription to norms of research. Journal of Higher Education, 81(3), 366–393. https://doi.org/10.1353/jhe.0.0095\n\nHardwicke, T. E., Jameel, L., Jones, M., Walczak, E. J., & Weinberg, L. M. (2014). Only human: Scientists, systems, and suspect statistics. Opticon1826, 16, 25. https://doi.org/10.5334/OPT.CH\n\nMerton, R. K. (1938). Science and the social order. Philosophy of Science, 5(3), 321–337. https://doi.org/10.1086/286513\n\nMerton, R. K. (1942). A note on science and democracy. Journal of Legal and Political Sociology, 1, 115–126. https://doi.org/10.1515/9783110375008-013",
                "drafted_by": [
                    "David Moreau"
                ],
                "reviewed_by": [
                    "Ashley Blake",
                    "Mahmoud Elsherif",
                    "Charlotte R. Pennington",
                    "Beatrice Valentini"
                ],
                "alt_related_terms": [
                    null
                ],
                "language": "arabic"
            },
            {
                "type": "glossary",
                "title": "المشاريع المجتمعيَّة (Community Projects)",
                "definition": "هي المشاريع التَّعاونية التي يتشارك فيها باحثون من مختلف المستويات المهنيَّة، أو التَّخصُّصات، أو المؤسسات، أو البلدان.  وتختلف هذه المشاريع باختلاف الأهداف التي تُقام من أجلها، مثل: مشاريع مساعدة الأقران على التَّعلم، إجراء البحوث ووالتعليم والتَّربية.  وتختلف مدَّة هذه المشاريع بين مشاريع قصيرة المدى (كالمؤتمرات، أو فعاليات الهاكاثون)، أو طويلة المدى كأنشطة نوادي المجلات، أو المشاريع البحثيَّة التي تنظمها التَّحالفات. وتعتمد هذه المشاريع المجتمعيَّة على الثَّقافة التَّعاونيَّة، والسَّعي لبناء المجتمع كأساس لتحقيق أهدافها.  **المصطلحات ذات الصِّلة:** النَّهج التَّصاعدي، البحث الجماعيَّ، هاكثون، المعامل المتعدِّدة، نوادي الأبحاث التكرارية.",
                "related_terms": [
                    "Bottom-up approach (to Open Scholarship)",
                    "Crowdsourced research",
                    "Hackathon",
                    "Many Labs",
                    "ReproducibiliTea"
                ],
                "references": "Ellemers, N. (2021). Science as collaborative knowledge generation. British Journal of Social Psychology, 60(1), 1–28. https://doi.org/10.1111/bjso.12430\n\nOrben, A. (2019). A journal club to fix science. Nature, 573(7775), 465–466. https://doi.org/10.1038/d41586-019-02842-8\n\nShepard, B. (2015). Community projects as social activism. SAGE.",
                "drafted_by": [
                    "Marta Topor"
                ],
                "reviewed_by": [
                    "Ali H. Al-Hoorie",
                    "Jamie P. Cockcroft",
                    "Mahmoud Elsherif",
                    "Kai Krautter",
                    "Gerald Vineyard"
                ],
                "alt_related_terms": [
                    null
                ],
                "language": "arabic"
            },
            {
                "type": "glossary",
                "title": "خلاصة وافية (Compendium)",
                "definition": "هي مجموعة من الملفّات التي يعدها الباحث؛ لدعم تقرير، أو بحث منشور. تتضمَّن البيانات الأصليَّة، والبيانات الوصفيَّة، والنَّص البرمجي، والبرامج التَّابعة، والتَّراخيص، والتَّعليمات الأخرى اللّازمة لباحث آخر؛ لإعادة إنتاج النَّتائج المذكورة في التَّقرير، أو البحث بشكل مستقل.  **المصطلحات ذات الصلة:** موسوعات؛ تكرار؛ قابلية إعادة الإنتاج، خلاصة البحوث",
                "related_terms": [
                    "Compendia",
                    "Replication",
                    "Reproducibility",
                    "Research compendium",
                    "**References:** \\[@Claerbout1992\\], \\[@Gentleman2005\\], \\[@Marwick2018\\], \\[@Nust2018\\]"
                ],
                "references": "",
                "drafted_by": [
                    "Ben Marwick"
                ],
                "reviewed_by": [
                    "Ali H. Al-Hoorie",
                    "Charlotte R. Pennington"
                ],
                "alt_related_terms": [
                    null
                ],
                "language": "arabic"
            },
            {
                "type": "glossary",
                "title": "الاستنساخ الحسابي (Computational reproducibility)",
                "definition": "القدرة على إعادة إنتاج نفس نتائج الدِّراسة الأصليَّة، بما في ذلك الجداول، والرُّسوم البيانيَّة، والنَّتائج الكميَّة باستخدام نفس البيانات، والأساليب الحسابيَّة، وظروف التَّحليل.  يسهم توافر التَّعليمات البرمجيَّة والبيانات في إعادة التِّكرار الحسابي، وكذلك إعداد هذه الأدوات (تحليل البيانات، تحديد إصدارات البرامج المستخدمة، مشاركة البيئات الحسابيَّة، وغيرها).  من النَّاحية المثاليَّة، يجب أن تكون إمكانيَّة التِّكرار الحسابيّ قابلة للتَّحقيق من قبل باحث ثانٍ \\_أو الباحث الأصلي في وقت لاحق\\_ باستخدام مجموعة من الملفّات، والتَّعليمات المكتوبة فقط.  يعرف الاستنساخ الحسابي أيضًا باسم الاستنساخ التَّحليليّ (LeBel et al., 2018).  **المصطلحات ذات الصِّلة**: مبادئ فير، ، قابليَّة التِّكرار، قابليَّة إعادة الإنتاج.",
                "related_terms": [
                    "FAIR principles",
                    "Replicability",
                    "Reproducibility"
                ],
                "references": "on Reproducibility, C., & in Science et al., R. (2019). Reproducibility and Replicability in Science (p. 25303). National Academies Press. https://doi.org/10.17226/25303\n\nKitzes, J., Turek, D., & Deniz, F. (2017). The practice of reproducible research: Case studies and lessons from the data-intensive sciences. University of California Press.\n\nLeBel, E. P., McCarthy, R. J., Earp, B. D., Elson, M., & Vanpaemel, W. (2018). A unified framework to quantify the credibility of scientific findings. Advances in Methods and Practices in Psychological Science, 1(3), 389–402. https://doi.org/10.1177/2515245918787489\n\nNosek, B. A., & Errington, T. M. (2020). What is replication? PLOS Biology, 18(3), e3000691. https://doi.org/10.1371/journal.pbio.3000691\n\nObels, P., Lakens, D., Coles, N. A., Gottfried, J., & Green, S. A. (2020). Analysis of open data and computational reproducibility in registered reports in psychology. Advances in Methods and Practices in Psychological Science, 3(2), 229–237. https://doi.org/10.1177/2515245920917961",
                "drafted_by": [
                    "Tina Lonsdorf"
                ],
                "reviewed_by": [
                    "Sarah Ashcroft-Jones",
                    "Helena Hartmann",
                    "Annalise A. LaPlume",
                    "Adam Parker",
                    "Charlotte R. Pennington",
                    "Eike Mark Rinke"
                ],
                "alt_related_terms": [
                    null
                ],
                "language": "arabic"
            },
            {
                "type": "glossary",
                "title": "التِّكرار المفاهيمي (Conceptual replication)",
                "definition": "محاولة تكرار بحيث يكون التَّأثير الأساسيّ المراد اختباره هو نفسه، ولكنَّه يتم اختباره في عينة مختلفة، وتحليله بطريقة مختلفة عن تلك التي بُحثت في الدراسة الأصلية ، كاستعمال طرق تقنين مختلفة أو معالجة بيانات، أو أساليب إحصائيَّة، أو مفاهيم مختلفة (LeBel et al., 2018).  غالبًا ما يكون الهدف من تكرار المفاهيم هو استكشاف الظُّروف التي تحدُّ من مدى إمكانيَّة  ملاحظة التَّأثير وتعميمه في سياقات معيّنة، وعيّنات معيّنة، وباستعمال أساليب قياس معيّنة نحو تقييم وتطوير النَّظريَّة. (Hüffmeier et al., 2016).  **المصطلحات ذات الصِّلة**: التِّكرار المباشر، القابليَّة للتَّعميم.",
                "related_terms": [
                    "Direct replication",
                    "Generalizability"
                ],
                "references": "Crüwell, S., van Doorn, J., Etz, A., Makel, M. C., Moshontz, H., Niebaum, J. C., Orben, A., Parsons, S., & Schulte-Mecklenbeck, M. (2019). Seven Easy Steps to Open Science: An Annotated Reading List. Zeitschrift Für Psychologie, 227(4), 237–248. https://doi.org/10.1027/2151-2604/a000387\n\nHüffmeier, J., Mazei, J., & Schultze, T. (2016). Reconceptualizing replication as a sequence of different studies: A replication typology. Journal of Experimental Social Psychology, 66, 81–92. https://doi.org/10.1016/j.jesp.2015.09.009\n\nLeBel, E. P., McCarthy, R. J., Earp, B. D., Elson, M., & Vanpaemel, W. (2018). A unified framework to quantify the credibility of scientific findings. Advances in Methods and Practices in Psychological Science, 1(3), 389–402. https://doi.org/10.1177/2515245918787489",
                "drafted_by": [
                    "Mahmoud Elsherif; Thomas Rhys Evans"
                ],
                "reviewed_by": [
                    "Adrien Fillon",
                    "Helena Hartmann",
                    "Matt Jaquiery",
                    "Tina B. Lonsdorf",
                    "Catia M. Oliveira",
                    "Charlotte R. Pennington",
                    "Graham Reid",
                    "Timo Roettger",
                    "Lisa Spitzer",
                    "Suzanne L. K. Stewart",
                    "Flávio Azevedo"
                ],
                "alt_related_terms": [
                    null
                ],
                "language": "arabic"
            },
            {
                "type": "glossary",
                "title": "**ا**لانحياز التَّوكيديّ (Confirmation bias)",
                "definition": "The tendency to seek out, interpret, favor and recall information in a way that supports one’s prior values, beliefs, expectations, or hypothesis. **الميل للبحث عن المعلومات، وتفسيرها، وتمييزها، واسترجاعها بطريقة تدعم قيم الشَّخص ومعتقداته، وتوقعاته، أو فرضياته. **المصطلحات ذات الصِّلة:** الانحياز التَّأكيدي، الانحياز الاجتماعيّ (المتوافق)، التَّحيُّز الشَّخصي",
                "related_terms": [
                    "Confirmatory bias",
                    "Congeniality bias",
                    "Myside bias"
                ],
                "references": "Bishop, D. V. (2020). The psychology of experimental psychologists: Overcoming cognitive constraints to improve research: The 47th Sir Frederic Bartlett Lecture. Quarterly Journal of Experimental Psychology, 73(1), 1–19. https://doi.org/10.1177/1747021819886519\n\nNickerson, R. S. (1998). Confirmation bias: A ubiquitous phenomenon in many guises. Review of General Psychology, 2(2), 175–220. https://doi.org/10.1037/1089-2680.2.2.175\n\nSpencer, E. A., & Heneghan, C. (2018). Confirmation bias. Catalogue Of Bias. https://catalogofbias.org/biases/confirmation-bias/\n\nWason, P. C. (1960). On the failure to eliminate hypotheses in a conceptual task. Quarterly Journal of Experimental Psychology, 12(3), 129–140. https://doi.org/10.1080/17470216008416717",
                "drafted_by": [
                    "Barnabas Szaszi; Jenny Terry"
                ],
                "reviewed_by": [
                    "Mahmoud Elsherif",
                    "Tamara Kalandadze",
                    "Sam Parsons",
                    "Charlotte R. Pennington"
                ],
                "alt_related_terms": [
                    null
                ],
                "language": "arabic"
            },
            {
                "type": "glossary",
                "title": "التَّحليلات التَّوكيديَّة (Confirmatory analyses)",
                "definition": "هي جزء من التَّمييز بين البحوث التَّوكيديّة، والاستكشافيَّة (Wagenmakers et al., 2012)، حيث تشير التَّحاليل التَّوكيديَّة إلى التَّحاليل التي تم تحديدها مسبقًا، وتختبر الفرضيّات الموجودة.  يرى البعض أن عدم الالتفات لهذا التَّمييز في نتائج البحوث المنشورة يفسر سبب مشاكل القابليَّة للتِّكرار، ويمكن التغلب عليها من خلال التَّسجيل المسبق للدِّراسة والذي يمِّيز بوضوح التَّحاليل التَّوكيديّة عن الاستكشافيّة. وقد شكك بعض الباحثين في هذه المصطلحات وأوصوا باستبدالها بـ \"بحوث استكشافية\" و\"بحوث اختبار النَّظريات\". (Oberauer & Lewandowsky, 2019; Szollosi & Donkin, 2019).  **المصطلحات ذات الصِّلة:** تحليل البيانات الاستكشافي، التَّسجيل المسبق",
                "related_terms": [
                    "Exploratory data analysis",
                    "Preregistration"
                ],
                "references": "Box, G. E. P. (1976). Science and statistics. Journal of the American Statistical Association, 71(356), 791–799.\n\nOberauer, K., & Lewandowsky, S. (2019). Addressing the theory crisis in psychology. Psychonomic Bulletin & Review, 26(5), 1596–1618. https://doi.org/10.3758/s13423-019-01645-2\n\nSzollosi, A., & Donkin, C. (2019). Arrested theory development: The misguided distinction between exploratory and confirmatory research. PsyArXiv. https://doi.org/10.31234/osf.io/your_doi_placeholder\n\nTukey, J. W. (1977). Exploratory data analysis. Addison-Wesley.\n\nWagenmakers, E. J., Wetzels, R., Borsboom, D., van der Maas, H. L., & Kievit, R. A. (2012). An agenda for purely confirmatory research. Perspectives on Psychological Science, 7(6), 632–638. https://doi.org/10.1177/1745691612463078",
                "drafted_by": [
                    "Jenny Terry"
                ],
                "reviewed_by": [
                    "Mahmoud Elsherif",
                    "Eduardo Garcia-Garzon",
                    "Helena Hartmann",
                    "Mariella Paul",
                    "Charlotte R. Pennington",
                    "Timo Roettger",
                    "Lisa Spitzer"
                ],
                "alt_related_terms": [
                    null
                ],
                "language": "arabic"
            },
            {
                "type": "glossary",
                "title": "تعارض المصالح (Conflict of interest)",
                "definition": "تعارض المصالح ويطلق عليه أيضًا \"المصالح المتنافسة\": هو علاقة ماليَّة، أو غير ماليَّة أو نشاط، أو مصلحة أخرى قد تضعف الموضوعية أو الحكم المهني من جانب المؤلِّف، أو المراجع، أو المحرِّر، أو فريق التَّحرير.  تنصُّ مبادئ الشَّفافيَّة \\_وأفضل الممارسات في النَّشر العلميّ التي وضعتها لجنة أخلاقيَّات النَّشر، ودليل المجلَّات المفتوحة، وجمعيَّة النَّاشرين الأكاديميين بالوصول المفتوح، وجمعيَّة محرِّري الطِّب العالميَّة\\_ على أنَّ الدَّوريّات يجب أنْ تتبنَّى سياسات حول أخلاقيَّات النَّشر، بما في ذلك سياسات حول تعارض المصالح (DOA, 2018). يجب أن يتم الإفصاح عن تعارضات المصالح بشفافية حتى يتمكَّن القراء من تقييم البحث بشكل صحيح، وتقييم الانحياز المحتمل أو الفعلي،  بالإضافة إلى النَّشر، يجب أيضًا على مقدمي العروض الأكاديميَّة وأعضاء اللِّجان والمعلِّمين أن يعلنوا عن تعارضات المصالح.  يمكن اعتبار عدم الكشف عن تعارض المصالح عمدًا شكلًا من أشكال سوء السُّلوك.  **المصطلحات ذات الصِّلة**: الموضوعية، تحكيم الأقران، ثقة الجمهور في العلوم، أخلاقيَّات النَّشر، الشَّفافية",
                "related_terms": [
                    "Objectivity",
                    "Peer review",
                    "Public Trust in Science",
                    "Publication ethics",
                    "Transparency"
                ],
                "references": "Directory of Open Access Journals. (n.d.). https://doaj.org/apply/transparency/",
                "drafted_by": [
                    "Christopher Graham"
                ],
                "reviewed_by": [
                    "Flávio Azevedo"
                ],
                "alt_related_terms": [
                    null
                ],
                "language": "arabic"
            },
            {
                "type": "glossary",
                "title": "التَّحالف التَّأليفي (Consortium authorship)",
                "definition": "في التَّحالف التَّأليفي يظهر اسم الاتحاد، أو المنظمة فقط في عمود المؤلِّف، ولا تظهر أسماء الأفراد، فعلى سبيل المثال: يكون \"فورت\" هو مؤلف.  يمكن ملاحظة هذا النمط في منتجات المشاريع التَّعاونيَّة التي تضم عددًا كبيرًا من المساهمين، اعتمادًا على سياسة المجلَّة، يمكن تسجيل الباحثين الأفراد كمؤلفين للمنتج البحثيّ في قواعد بيانات الأدبيَّات مثل: أوركيد وسكوبس. يمكن أيضًا تسمية التَّحالف التَّأليفيّ بتأليف المجموعة، أو الشَّركة، أو المنظمة، أو التأليف الجماعي، مثل: [https://www.bmj.com/about-bmj/resources-authors/article-submission/authorship-contributorship](https://www.bmj.com/about-bmj/resources-authors/article-submission/authorship-contributorship) ) أو التَّأليف التَّعاوني مثل: ([https://support.jmir.org/hc/en-us/articles/115001449591-What-is-a-group-author-collaborative-author-and-does-it-need-an-ORCID](https://support.jmir.org/hc/en-us/articles/115001449591-What-is-a-group-author-collaborative-author-and-does-it-need-an-ORCID) ) **المصطلحات ذات الصِّلة:** التأليف، تصنيف أدوار المؤلفين.",
                "related_terms": [
                    "Authorship",
                    "CRediT"
                ],
                "references": "Tierney, W., Hardy III, J. H., Ebersole, C. R., Leavitt, K., Viganola, D., Clemente, E. G., & others. (2020). Creative destruction in science. Organizational Behavior and Human Decision Processes, 161, 291–309. https://doi.org/10.1016/j.obhdp.2020.07.002\n\nTierney, W., Hardy III, J., Ebersole, C. R., Viganola, D., Clemente, E. G., Gordon, M., & others. (2021). A creative destruction approach to replication: Implicit work and sex morality across cultures. Journal of Experimental Social Psychology, 93, 104060. https://doi.org/10.1016/j.jesp.2020.104060",
                "drafted_by": [
                    "Yuki Yamada"
                ],
                "reviewed_by": [
                    "Adam Parker",
                    "Charlotte R. Pennington",
                    "Beatrice Valentini",
                    "Qinyu Xiao",
                    "Flávio Azevedo"
                ],
                "alt_related_terms": [
                    null
                ],
                "language": "arabic"
            },
            {
                "type": "glossary",
                "title": "COG قيود التَّعميم (Constraints on Generality (COG))",
                "definition": "عبارة تحدّد بشكل صريح، وتبرِّر المجتمع المستهدف، وظروف النَّتائج التي دوِّنت. يجب أن يكون الباحثون صريحين بشأن القيود المحتملة لتعميم نتائج بحوثهم (Simons et al., 2017). وينبغي على الباحثين تقديم أوصاف تفصيليَّة لعيِّنة المجتمع والعوامل السِّياقيَّة التي قد تؤثرعلى النَّتائج بحيث يمكن لمحاولات التِّكرار المستقبليَّة أن تأخذ هذه العوامل بعين الاعتبار (Brandt et al., 2014).  من المفترض ألا يكون للشروط غير المدرجة صراحةً صلة نظريَّة بإمكانيَّة تكرار التَّأثير. **المصطلحات ذات الصِّلة:** بيزار، التَّنوع، العدالة، القابلية للتَّعميم، الشمول، قابليَّة إعادة الإنتاج، التِّكرار، سترينج: الخلفية الاجتماعية والقابلية للتتبع والاختبار الذاتي وتاريخ التربية والتأقلم والتّعود، وِيرد",
                "related_terms": [
                    "BIZARRE",
                    "Diversity",
                    "Equity",
                    "Generalizability",
                    "Inclusion",
                    "Reproducibility",
                    "Replication",
                    "STRANGE",
                    "WEIRD"
                ],
                "references": "Busse, C., Kach, A. P., & Wagner, S. M. (2017). Boundary Conditions: What They Are, How to Explore Them, Why We Need Them, and When to Consider Them. Organizational Research Methods, 20(4), 574–609. https://doi.org/10.1177/1094428116641191\n\nBrandt, M. J., IJzerman, H., Dijksterhuis, A., Farach, F. J., Geller, J., Giner-Sorolla, R., & others. (2014). The replication recipe: What makes for a convincing replication? Journal of Experimental Social Psychology, 50, 217–224. https://doi.org/10.1016/j.jesp.2013.10.005\n\nSimons, D. J., Shoda, Y., & Lindsay, D. S. (2017). Constraints on generality (COG): A proposed addition to all empirical papers. Perspectives on Psychological Science, 12(6), 1123–1128. https://doi.org/10.1177/1745691617708630\n\nYarkoni, T. (2020). The generalizability crisis. Behavioral and Brain Sciences, 1–37. https://doi.org/10.1017/S0140525X20001685",
                "drafted_by": [
                    "Mahmoud Elsherif"
                ],
                "reviewed_by": [
                    "Ali H. Al-Hoorie",
                    "Jamie P. Cockcroft",
                    "Sam Parsons",
                    "Charlotte R. Pennington",
                    "Timo Roettger"
                ],
                "alt_related_terms": [
                    null
                ],
                "language": "arabic"
            },
            {
                "type": "glossary",
                "title": "الصِّدق البنائي (Construct validity)",
                "definition": "عند استخدامه في سياق القياس والاختبار، يشير الصِّدق البنائي إلى مدى قياس اختبار معيّن ما يدّعي قياسه. أمّا في المجالات التي تُعنى بكيانات فرضيّة غير قابلة للقياس، يعد الصِّدق البنائي في الأساس اختبارًا نظريًا،لأنَّه يتضمَّن تحديد ما إذا كان المقياس الموضوعي (استبانة، مهمة معمليَّة، وما إلى ذلك) ممثلًا صادقًا للبنية الافتراضية (أي يتوافق مع نظريّة ما). وعند استخدامه بمعنى أوسع حول دراسة بحثيَّة، أو ادعاء، أو استنتاج، أو تأثير ملحوظ في دراسة بحثيَّة، فإنَّ صدق البناء يتعلَّق بمدى اتساق تفاصيل العيِّنة المستخدمة في الدِّراسة (المشاركون، والبيئات والتَّجارب، والمتغيِّرات التَّابعة) مع ~~البنى و~~المفاهيم العليا، والادعاء، والاستنتاج. ووفقًا لبعض العلماء، يمكن تعريف الصِّدق البنائي على أنه: \"الدَّرجة التي يتم فيها ضمان صحّة نسبة الاستدلالات من الأفراد، والسِّياقات، والنَّتائج ومسبباتها في دراسة معيّنة للمفاهيم التي قد تمثلها هذه الحالات\" (Shadish et al., 2002., p. 38). **المصطلحات ذات الصِّلة:** أزمة القياس، صدق القياس، ممارسات القياس المشكوك فيها، النظريّة، الصِّدق، التَّصديق.",
                "related_terms": [
                    "Measurement crisis",
                    "Measurement validity",
                    "Questionable Measurement Practices (QMP)",
                    "Theory",
                    "Validity",
                    "Validation"
                ],
                "references": "Cronbach, L. J., & Meehl, P. E. (1955). Construct validity in psychological tests. Psychological Bulletin, 52(4), 281–302. https://doi.org/10.1037/h0040957\n\nShadish, W. R., Cook, T. D., & Campbell, D. T. (2002). Experimental and quasi-experimental designs for generalized causal inference. Houghton Mifflin.\n\nSmith, G. T. (2005). On Construct Validity: Issues of Method and Measurement. Psychological Assessment, 17(4), 396–408. https://doi.org/10.1037/1040-3590.17.4.396",
                "drafted_by": [
                    "Annalise A. LaPlume"
                ],
                "reviewed_by": [
                    "Ali H. Al-Hoorie",
                    "Mahmoud Elsherif",
                    "Zoltan Kekecs",
                    "Charlotte R. Pennington"
                ],
                "alt_related_terms": [
                    null
                ],
                "language": "arabic"
            },
            {
                "type": "glossary",
                "title": "صدق المحتوى (Content validity)",
                "definition": "يشير هذا المصطلح إلى مدى شموليَّة القياس لجميع جوانب المفهوم التي يدعي الباحث قياسها: \"هو نوع من أنواع الصِّدق الكيفي الذي يكون فيه مجال المفهوم واضحًا بحيث يحكم المحلّل على ما إذا كانت المقاييس تمثل المجال كليةً \" (Bollen, 1989, p.185).  وهو أحد مكونات الصِّدق البنائي ويمكن تنفيذه باستخدام الطُّرق الكميَّة والكيفيّة، والتي غالبًا ما تتضمَّن تقييم الخبراء.  **المصطلحات ذات الصِّلة:** الصِّدق البنائي، الصِّدق",
                "related_terms": [
                    "Construct validity",
                    "Validity"
                ],
                "references": "Bollen, K. A. (1989). Structural Equations with Latent Variables (pp. 179–225). John Wiley & Sons.\n\nBrod, M., Tesler, L., & Christensen, T. (2009). Qualitative research and content validity: Developing best practices based on science and experience. Quality of Life Research, 18(9), 1263–1278. https://doi.org/10.1007/s11136-009-9540-9\n\nDrost, E. A. (2011). Validity and reliability in social science research. Education Research and Perspectives, 38(1), 105–123.\n\nHaynes, S. N., Richard, D. C. S., & Kubany, E. S. (1995). Content validity in psychological assessment: A functional approach to concepts and methods. Psychological Assessment, 7(3), 238–247. https://doi.org/10.1037/1040-3590.7.3.238",
                "drafted_by": [
                    "Annalise A. LaPlume"
                ],
                "reviewed_by": [
                    "Mahmoud Elsherif",
                    "Wanyin Li",
                    "Aoife O’Mahony",
                    "Eike Mark Rinke",
                    "Sam Parsons",
                    "Graham Reid"
                ],
                "alt_related_terms": [
                    null
                ],
                "language": "arabic"
            },
            {
                "type": "glossary",
                "title": "الإسهام (Contribution)",
                "definition": "هو إضافة رسميَّة، أو نشاط في سياق البحث. يتم إرفاق بيانات المساهمين، بما في ذلك أقسام الشُّكر في مقالات المجلات، بمنتجات البحث؛ لتصنيفها بشكل أفضل، والاعتراف بتنوع العمل الذي يتجاوز \"التَّأليف\" الذي يتطلبه أي عمل، أو إنتاج فكري.  إن المساهمة تتطوَّر باعتبارها \"مصدر بيانات لفهم العلاقة بين التَّأليف، والإنتاج المعرفي\" (Lariviere et al., p.430).  يُمكن اعتبار المساهمة في مجال تطوير البرمجيات مفتوحة المصدر بمثابة تغييرات يتم إدراجها في مخزن برامج المشروع بعد مراجعة الأقران المعروف تقنيًا باسم \"طلب السَّحب\". يعتبر NumPy مثالًا على مشروع مفتوح المصدر يقبل المساهمات (Harris et al., 2020). **المصطلحات ذات الصِّلة:** التَّأليف، تصنيف أدوار المؤلفين، القياسات الدِّلالية.",
                "related_terms": [
                    "authorship",
                    "CRediT",
                    "Semantometrics"
                ],
                "references": "Knoth, P., & Herrmannova, D. (2014). Towards semantometrics: A new semantic similarity based measure for assessing a research publication’s contribution. D-Lib Magazine, 20(11), 8. https://doi.org/10.1045/november2014-knoth\n\nLarivière, V., Desrochers, N., Macaluso, B., Mongeon, P., Paul-Hus, A., & Sugimoto, C. R. (2016). Contributorship and division of labor in knowledge production. Social Studies of Science, 46(3), 417–435. https://doi.org/10.1177/0306312716650046\n\nHolcombe, A. O. (2019). Contributorship, not authorship: Use CRediT to indicate who did what. Publications, 7(3), 48. https://doi.org/10.3390/publications7030048",
                "drafted_by": [
                    "Micah Vandegrift"
                ],
                "reviewed_by": [
                    "Jamie P. Cockcroft",
                    "Dominik Kiersz",
                    "Michele C. Lim",
                    "Leticia Micheli",
                    "Sam Parsons",
                    "Gerald Vineyard"
                ],
                "alt_related_terms": [
                    null
                ],
                "language": "arabic"
            },
            {
                "type": "glossary",
                "title": "البيان التصحيحي (Corrigendum)",
                "definition": "التعريف:** تُوثِّق التصحيحات خطأ ما أو أكثر في البحث المنشور والتي لا تغير جوهر البحث وفكرته الأساسية ولا استنتاجاته وبالتالي لا يرقى إلى مستوى سحب العمل. غالبًا ما تكون التصحيحات بجانب العمل الأصلي لغرض الشفافية. كما يطلق بعض الناشرين على هذا المستند مصلح \"خطأ\"، بينما يميز آخرون بين الاثنين بحيث تتعلق \"التصحيحات\" بأخطاء المؤلف  بينما تتعلق \"الأخطاء\" بأخطاء الناشر.  **المصطلحات ذات الصلة:** التصحيح، خطأ، التراجع",
                "related_terms": [
                    "Correction",
                    "Errata",
                    "Retraction"
                ],
                "references": "Anon. (2006). Correction or retraction? In Nature (Vol. 444, pp. 123–124). https://doi.org/10.1038/444123b",
                "drafted_by": [
                    "Charlotte R. Pennington"
                ],
                "reviewed_by": [
                    "Bradley Baker",
                    "Nick Ballou",
                    "Wanyin Li",
                    "Adam Parker",
                    "Emily A. Williams"
                ],
                "alt_related_terms": [
                    null
                ],
                "language": "arabic"
            },
            {
                "type": "glossary",
                "title": "الإنتاج المشترك (Co-production)",
                "definition": "‏نهج للبحث يمكِّن أصحاب المصلحة الذين لا يشاركون عادةً في عملية البحث من التَّعاون، إما في بداية المشروع، أو طوال دورة فترة البحث. على سبيل المثال: قد يتضمَّن الإنتاج المشترك في البحث الصِّحي التَّعاون بين المهنيين الصِّحيين والمرضى، بينما قد يشمل البحث التربوي أعضاء هيئة التَّدريس والطُّلاب. والدافع وراء هذا هو بعض المبادئ مثل: احترام وتقدير تجارب غير الباحثين، ومعالجة ديناميكيات السُّلطة، وبناء علاقات متبادلة المنفعة. **المصطلحات ذات الصلة**: علم المواطن.  التعاون؛  البحوث التعاونية،  علم الحشود،  منحة دراسية،  الترجمة المتكاملة للمعرفة (IKT)؛  النمط 2 لإنتاج المعرفة؛  البحوث التشاركية، عيّنة ومجتمع الدّراسة المساهمين",
                "related_terms": [
                    "Citizen science",
                    "Collaboration",
                    "Collaborative research",
                    "Crowd science",
                    "Engaged scholarship",
                    "Integrated Knowledge Translation (IKT)",
                    "Mode 2 of knowledge production",
                    "Participatory research",
                    "Patient and Public Involvement (PPI)"
                ],
                "references": "Filipe, A., Renedo, A., & Marston, C. (2017). The co-production of what? Knowledge, values, and social relations in health care. PLoS Biology, 15(5), e2001403. https://doi.org/10.1371/journal.pbio.2001403\n\nGraham, I. D., McCutcheon, C., & Kothari, A. (2019). Exploring the frontiers of research co-production: the Integrated Knowledge Translation Research Network concept papers. Health Research Policy and Systems, 17, 88. https://doi.org/10.1186/s12961-019-0501-7\n\nNIHR Guidance on Co-Producing a Research Project. (2021). https://www.learningforinvolvement.org.uk/?opportunity=nihr-guidance-on-co-producing-a-research-project\n\nCo-Production Collective. (n.d.). Our Approach. Co-Production Collective. https://www.coproductioncollective.co.uk/what-is-co-production/our-approach",
                "drafted_by": [
                    "Emma Norris"
                ],
                "reviewed_by": [
                    "Gisela H. Govaart",
                    "Magdalena Grose-Hodge",
                    "Helena Hartmann;Charlotte R. Pennington",
                    "Sonia Rishi",
                    "Emily A. Williams"
                ],
                "alt_related_terms": [
                    null
                ],
                "language": "arabic"
            },
            {
                "type": "glossary",
                "title": "CC license رخصة المشاع الإبداعيّ (Creative Commons (CC) license)",
                "definition": "‏مجموعة من التَّراخيص لحقوق النَّشر المجانيَّة سهلة الاستخدام، حيث تحدِّد حقوق المؤلِّفين، ومستخدمي البيانات، والمواد المفتوحة بطريقة موحّدة. تُمكِّن تراخيص المشاع الإبداعي المؤلِّفين من مشاركة العمل المحمي بموجب قانون حقوق الطَّبع والنَّشر مع الجمهور، ويأتي في أشكال مختلفة مع بنود أكثر أو أقل. فعلى سبيل المثال: تتيح لك رخصة \"الإسناد-غير تجاري-المشاركة بالمثل 4.0 الدولية\" مشاركة المواد وتعديلها بشرط أن ينسب العمل للكُتّاب الأصليين، والإشارة إلى تغييرات إن وجدت، والمشاركة بموجب نفس التَّرخيص، وأن لا تستخدم  المواد لأغراض تجاريَّة. **المصطلحات ذات الصِّلة**: ‏حقوق الطَّبع والنَّشر، التَّرخيص. **تعريف بديل:** (إن أمكن) المشاع الإبداعي هي منظمة دولية غير ربحية توفر تراخيص المشاع الإبداعي بهدف تقليل العقبات القانونية أمام مشاركة المعرفة والإبداع.",
                "related_terms": [
                    "Copyright",
                    "Licence **Alternative definition:** (if applicable) Creative Commons is an international nonprofit organization that provides Creative Commons licences, with the goal to minimize legal obstacles to the sharing of knowledge and creativity."
                ],
                "references": "Anon. (n.d.). About CC Licenses. Retrieved from https://creativecommons.org/about/cclicenses/",
                "drafted_by": [
                    "Tina Lonsdorf"
                ],
                "reviewed_by": [
                    "Adrien Fillon",
                    "Gisela H. Govaart",
                    "Annalise A. LaPlume",
                    "Sam Parsons",
                    "Charlotte R. Pennington"
                ],
                "alt_related_terms": [
                    null
                ],
                "language": "arabic"
            },
            {
                "type": "glossary",
                "title": "ثورة المصداقيَّة (Credibility revolution)",
                "definition": "تتعلَّق بالمشاكل والحلول النَّاتجة عن تزايد عدم الثِّقة في النَّتائج العلميَّة، على إثر المخاوف المتعلِّقة بمصداقيّة الادِّعاءات العلميَّة (مثل انخفاض قابليَّة التِّكرار). تم اقتراح هذا المصطلح كبديل أكثر إيجابيَّة لمصطلح أزمة قابليَّة التِّكرار، إذ يتضمَّن العديد من الحلول؛ لتحسين مصداقيَّة البحث، مثل التَّسجيل المسبق، والشَّفافيَّة والتِّكرار. **المصطلحات ذات الصِّلة:** مصداقيَّة الادِّعاءات العلميَّة، معايير عالية من الأدِّلَّة، العلم المفتوح، أزمة إعادة الإنتاج (أو أزمة التكرار)، ، الشَّفافيَّة. ‏",
                "related_terms": [
                    "Credibility of scientific claims",
                    "High standards of evidence",
                    "Openness",
                    "Open Science;Reproducibility crisis (aka Replicability or replication crisis)",
                    "Transparency"
                ],
                "references": "Angrist, J. D., & Pischke, J. S. (2010). The credibility revolution in empirical economics: How better research design is taking the con out of econometrics. Journal of Economic Perspectives, 24, 3–30. https://doi.org/10.1257/jep.24.2.3\n\nVazire, S. (2018). Implications of the Credibility Revolution for Productivity, Creativity, and Progress. Perspectives on Psychological Science, 13(4), 411–417. https://doi.org/10.1177/1745691617751884\n\nVazire, S., Schiavone, S. R., & Bottesini, J. G. (2020). Credibility Beyond Replicability: Improving the Four Validities in Psychological Science. https://doi.org/10.31234/osf.io/bu4d3",
                "drafted_by": [
                    "Tamara Kalandadze"
                ],
                "reviewed_by": [
                    "Bradley Baker",
                    "Mahmoud Elsherif",
                    "Helena Hartmann",
                    "Kai Krautter",
                    "Annalise A. LaPlume",
                    "Oscar Lecuona",
                    "Charlotte R. Pennington",
                    "Robert Ross",
                    "Tobias Wingen",
                    "Flávio Azevedo"
                ],
                "alt_related_terms": [
                    null
                ],
                "language": "arabic"
            },
            {
                "type": "glossary",
                "title": "نهج التَّدمير الإبداعي للتِّكرار (Creative destruction approach to replication)",
                "definition": "‏يجب أن تسعى جهود تكرار البحوث ليس فقط لدعم النَّتائج الأصليَّة، أو التَّشكيك فيها، بل أيضّا لاستبدالها بنظريات أقوى، وذات قوّة تفسيريّة أكبر. وبالتَّالي، فإنَّ هذا النَّهج يتضمَّن \"تشذيب\" النَّظريات الحاليَّة، ومقارنة جميع النَّظريَّات البديلة، وجعل جهود تكرار الأبحاث توليديَّة، ومشاركة في بناء النَّظريَّة أكثر (Tierney et al., 2020, 2021). **المصطلحات ذات الصِّلة**: البحث الجماعي، التَّزوير، النُّسخ المتماثلة، النظريّة.",
                "related_terms": [
                    "Crowdsourced research",
                    "Falsification",
                    "Replication",
                    "Theory"
                ],
                "references": "Tierney, W., Hardy III, J. H., Ebersole, C. R., Leavitt, K., Viganola, D., Clemente, E. G., & others. (2020). Creative destruction in science. Organizational Behavior and Human Decision Processes, 161, 291–309. https://doi.org/10.1016/j.obhdp.2020.07.002\n\nTierney, W., Hardy III, J., Ebersole, C. R., Viganola, D., Clemente, E. G., Gordon, M., & others. (2021). A creative destruction approach to replication: Implicit work and sex morality across cultures. Journal of Experimental Social Psychology, 93, 104060. https://doi.org/10.1016/j.jesp.2020.104060",
                "drafted_by": [
                    "Mahmoud Elsherif"
                ],
                "reviewed_by": [
                    "Magdalena Grose-Hodge",
                    "Aoife O’Mahony",
                    "Adam Parker",
                    "Charlotte R. Pennington",
                    "Sonia Rishi",
                    "Beatrice Valentini"
                ],
                "alt_related_terms": [
                    null
                ],
                "language": "arabic"
            },
            {
                "type": "glossary",
                "title": "تصنيف أدوار المؤلِّفين (CRediT)",
                "definition": "تصنيف أدوار المساهمين (https://credit.niso.org/) هو تصنيف رفيع المستوى يُستخدَّم للإشارة إلى الأدوار التي يعتمدها عادة المساهمون في الإنتاج العلمي. ويوجد حاليًا 14 دورًا يصف المساهمة المحدَّدة لكلِّ مساهم في الإنتاج العلمي. يمكن تعيين كل دور لمؤلِّفين مختلفين، ويمكن أيضًا تعيين أدوار متعدِّدة لمؤلف واحد يتضمَّن هذا التَّصنيف الأدوار الآتية: تصوّر الفكرة، وتنظيم البيانات، والتَّحليل الرَّسمي، والحصول على التَّمويل، والتَّحقيق، والمنهجيَّة، وإدارة المشروع، والموارد ، والبرامج، والإشراف، والتَّحقق من الصِّحة، والتَّصور، والكتابة \\- المسودة الأصليَّة، والكتابة \\- المراجعة والتَّحرير. وهناك وصف لكل من هذه الأدوار المختلفة (Brand et al., 2015). **المصطلحات ذات الصِّلة:** التَّأليف، الإسهام.",
                "related_terms": [
                    "Authorship",
                    "Contributions"
                ],
                "references": "Brand, A., Allen, L., Altman, M., Hlava, M., & Scott, J. (2015). Beyond authorship: attribution, contribution, collaboration, and credit. Learned Publishing, 28(2), 151–155. https://doi.org/10.1087/20150211\n\nHolcombe, A. O. (2019). Contributorship, not authorship: Use CRediT to indicate who did what. Publications, 7(3), 48. https://doi.org/10.3390/publications7030048",
                "drafted_by": [
                    "Sam Parsons"
                ],
                "reviewed_by": [
                    "Myriam A. Baum",
                    "Matt Jaquiery",
                    "Tamara Kalandadze",
                    "Connor Keating",
                    "Charlotte R. Pennington",
                    "Yuki Yamada",
                    "Flávio Azevedo"
                ],
                "alt_related_terms": [
                    null
                ],
                "language": "arabic"
            },
            {
                "type": "glossary",
                "title": "صدق المحك (Criterion validity)",
                "definition": "يشير هذا المصطلح إلى مدى توافق المقياس مع المقاييس الصَّادقة الأخرى للمفهوم ذاته.  وعادة ما يتم بيان صدق المحك عن طريق حساب معاملات الانحدار، أو الارتباطات ثنائيَّة المتغيِّر؛ لتقدير اتجاه وقوة العلاقة بين المقياس والمحك .  وغالبًا ما يتم الخلط بينه وبين الصِّدق البنائي على الرُّغم من اختلافه عنه في الغرض (فهو مجرد تنبؤي وليس نظريًا) والفائدة (التَّنبؤ بنتيجة يمكن ملاحظتها بدلًا من البنية الكامنة). وعادةً ما يؤدي عدم الموثوقيَّة في درجات المقياس أو المحك إلى تقليل صدق المحك . ويُطلق عليه أيضًا الصِّدق المتعلِّق بالمحكات أو الصِّدق الملموس. **المصطلحات ذات الصِّلة:** الصدق البنائي، الصِّدق.",
                "related_terms": [
                    "Construct validity",
                    "Validity"
                ],
                "references": "DeVellis, R. F. (2017). Scale development: Theory and applications (4th ed.). Sage.\n\nDrost, E. A. (2011). Validity and reliability in social science research. Education Research and Perspectives, 38(1), 105–123.",
                "drafted_by": [
                    "Annalise A. LaPlume"
                ],
                "reviewed_by": [
                    "Helena Hartmann",
                    "Kai Krautter",
                    "Sam Parsons",
                    "Eike Mark Rinke"
                ],
                "alt_related_terms": [
                    null
                ],
                "language": "arabic"
            },
            {
                "type": "glossary",
                "title": "البحث الجماعي (Crowdsourced Research)",
                "definition": "التعريف:** البحث الجماعي هو نموذج للتَّنظيم الاجتماعي؛ للبحث كتعاون واسع النِّطاق يتم فيه تنفيذ مشروع بحثي واحد، أو أكثر من قبل فرق متعدِّدة بطريقة مستقلَّة ولكن منسَّقة. يهدف البحث الجماعي إلى تحقيق مكاسب الكفاءة، وقابليَّة التَّوسع من خلال تجميع الموارد، وتعزيز الشَّفافيَّة، والاندماج الاجتماعي، بالإضافة إلى زيادة الدِّقة والموثوقية والجدارة من خلال تعزيز القوة الإحصائيَّة، والتَّحقق الاجتماعي المتبادل. يتعارض البحث الجماعي مع النَّموذج التَّقليدي لإنتاج البحث الأكاديمي، والذي يهيمن عليه العمل المستقلّ للأفراد، أو مجموعات صغيرة من الباحثين (\"العلوم الصغيرة\"). من أمثلة البحث الجماعي ما يسمى بدراسات \"التِّكرار المتعدِّد المعامل'' (Klein et al., 2018)، ودراسات \"العديد من المحلِّلين لمجموعة بيانات واحدة '' (Silberzahn et al., 2018)، والشَّبكات التَّعاونيَّة التَّوزيعيَّة (Moshontz et al., 2018\\) ومشاريع الكتابة التَّعاونية المفتوحة مثل: أوراق الإنترنت المفتوحة الضَّخمة (Himmelstein et al., 2019; Tennant et al., 2019). وكذلك يمكن أن يشير البحث الجماعي إلى استخدام عدد كبير من \"حشود العمال\" في جمع البيانات من خلال أسواق العمل عبر الإنترنت مثل Amazon Mechanical Turk أو Prolific، مثلًا في تحليل المحتوى (Benoit et al., 2016; Lind et al., 2017\\) أو البحث التَّجريبي (Peer et al., 2017). يُشار إلى البحث الجماعي المفتوح للمشاركة، والمفتوح من خلال المخرجات الوسيطة المشتركة باسم علم الحشود (Franzoni & Sauermann, 2014). **المصطلحات ذات الصِّلة:** علم المواطن، التَّعاون، توظيف الحشود، علم الفريق",
                "related_terms": [
                    "Citizen science",
                    "Collaboration",
                    "Crowdsourcing",
                    "Team science"
                ],
                "references": "Benoit, K., Conway, D., Lauderdale, B. E., Laver, M., & Mikhaylov, S. (2016). Crowd-sourced text analysis: Reproducible and agile production of political data. American Political Science Review, 110(2), 278–295. https://doi.org/10.1017/S0003055416000058\n\nBreznau, N. (2021). I saw you in the crowd: Credibility, reproducibility, and meta-utility. PS: Political Science & Politics, 54(2), 309–313. https://doi.org/10.1017/S1049096520000980\n\nFranzoni, C., & Sauermann, H. (2014). Crowd science: The organization of scientific research in open collaborative projects. Research Policy, 43(1), 1–20. https://doi.org/10.1016/j.respol.2013.07.005\n\nHimmelstein, D. S., Rubinetti, V., Slochower, D. R., Hu, D., Malladi, V. S., Greene, C. S., & Gitter, A. (2019). Open collaborative writing with Manubot. PLOS Computational Biology, 15(6), e1007128. https://doi.org/10.1371/journal.pcbi.1007128\n\nKlein, R. A., Vianello, M., Hasselman, F., Adams, B. G., Adams, R. B., Alper, S., & … Nosek, B. A. (2018). Many Labs 2: Investigating Variation in Replicability Across Samples and Settings. Advances in Methods and Practices in Psychological Science, 1(4), 443–490. https://doi.org/10.1177/2515245918810225\n\nLind, F., Gruber, M., & Boomgaarden, H. G. (2017). Content analysis by the crowd: Assessing the usability of crowdsourcing for coding latent constructs. Communication Methods and Measures, 11(3), 191–209. https://doi.org/10.1080/19312458.2017.1317338\n\nMoshontz, H., Campbell, L., Ebersole, C. R., IJzerman, H., Urry, H. L., Forscher, P. S., & Chartier, C. R. (2018). The Psychological Science Accelerator: Advancing psychology through a distributed collaborative network. Advances in Methods and Practices in Psychological Science, 1(4), 501–515. https://doi.org/10.1177/2515245918797607\n\nPeer, E., Brandimarte, L., Samat, S., & Acquisti, A. (2017). Beyond the Turk: Alternative platforms for crowdsourcing behavioral research. Journal of Experimental Social Psychology, 70, 153–163. https://doi.org/10.1016/j.jesp.2017.01.006\n\nSilberzahn, R., Uhlmann, E. L., Martin, D. P., Anselmi, P., Aust, F., Awtrey, E., Bahník, Š., Bai, F., Bannard, C., Bonnier, E., & others. (2018). Many Analysts, One Data Set: Making Transparent How Variations in Analytic Choices Affect Results. Advances in Methods and Practices in Psychological Science, 337–356. https://doi.org/10.1177/2515245917747646\n\nStewart, N., Chandler, J., & Paolacci, G. (2017). Crowdsourcing samples in cognitive science. Trends in Cognitive Sciences, 21(10), 736–748. https://doi.org/10.1016/j.tics.2017.06.007\n\nTennant, J., Bielczyk, N. Z., Cheplygina, V., Greshake Tzovaras, B., Hartgerink, C. H. J., Havemann, J., Masuzzo, P., & Steiner, T. (2019). Ten simple rules for researchers collaborating on Massively Open Online Papers (MOOPs). MetaArXiv. https://doi.org/10.31222/osf.io/et8ak\n\nUhlmann, E. L., Ebersole, C. R., Chartier, C. R., Errington, T. M., Kidwell, M. C., Lai, C. K., McCarthy, R. J., Riegelman, A., Silberzahn, R., & Nosek, B. A. (2019). Scientific utopia III: Crowdsourcing science. Perspectives on Psychological Science, 14(5), 711–733. https://doi.org/10.1177/1745691619850561\n\nWeek, C. (2021). What is Crowdsourcing? https://crowdsourcingweek.com/what-is-crowdsourcing/",
                "drafted_by": [
                    "Eike Mark Rinke"
                ],
                "reviewed_by": [
                    "Ali H. Al-Hoorie",
                    "Sam Parsons",
                    "Charlotte R. Pennington",
                    "Suzanne L. K. Stewart",
                    "Flávio Azevedo"
                ],
                "alt_related_terms": [
                    null
                ],
                "language": "arabic"
            },
            {
                "type": "glossary",
                "title": "الضَّريبة الثَّقافيَّة (Cultural taxation)",
                "definition": "العبء الإضافي المتوقع، أو المطلوب من أفراد الأقليَّات الممثَّلة تمثيلًا ناقصًا، أو المهمّشة، وخاصّة العلماء من غير العرق الأبيض. غالبًا ما يأتي هذا العبء من الأدوار الخدميّة التي توفر التَّمثيل، والتَّنوع العرقي، أو الثَّقافي، أو الجندري. يمكن أن تكون هذه الأدوار رسميَّة أو غير رسميَّة، وعادة ما تكون غير مجزية، أو غير مدفوعة الأجر. يشمل هذا العبء توفير الخبرة في مسائل التَّنوع الثَّقافي، وتثقيف أعضاء مجتمعات الأغلبيَّة، والعمل كحلقة وصل مع مجتمعات الأقليَّات، والأدوار الرَّسميَّة، وغير الرَّسميَّة كموجّه ونظام دعم لطلبة الأقليَّات. **المصطلحات ذات الصِّلة**: العمل غير المرئي، اختلالات السُّلطة، علاقات السُّلطة.",
                "related_terms": [
                    "Invisible labor",
                    "Power imbalances",
                    "Power relations"
                ],
                "references": "Joseph, T. D., & Hirshfield, L. E. (2011). `Why don’t you get somebody new to do it?’ Race and cultural taxation in the academy. Ethnic and Racial Studies, 34(1), 121–141. https://doi.org/10.1080/01419870.2010.496489\n\nLedgerwood, A., Hudson, S. T. J., Lewis, Jr., N. A., Maddox, K. B., Pickett, C., Remedios, J. D., & Wilkins, C. L. (2021). The Pandemic as a Portal: Reimagining Psychological Science as Truly Open and Inclusive. https://doi.org/10.31234/osf.io/gdzue\n\nPadilla, A. M. (1994). Research news and comment: Ethnic minority scholars; research, and mentoring: Current and future issues. Educational Researcher, 23(4), 24–27. https://doi.org/10.3102/0013189X023004024",
                "drafted_by": [
                    "Bradley Baker"
                ],
                "reviewed_by": [
                    "Helena Hartmann",
                    "Bethan Iley",
                    "Aoife O’Mahony",
                    "Charlotte R. Pennington",
                    "Flávio Azevedo"
                ],
                "alt_related_terms": [
                    null
                ],
                "language": "arabic"
            },
            {
                "type": "glossary",
                "title": "العلم التَّراكمي (Cumulative science)",
                "definition": "إنَّ غاية أي علم تطبيقي هي السَّعي نحو بناء قاعدة معرفيَّة تراكميَّة، تُمكّن مستقبل العلم أن يبنى عليها. (Curran, 2009, p. 1). وتعني هذه الفكرة أنَّ العلم سيقوم بخلق نظريَّات أكثر اكتمالًا ودقة كلَّما زادت كميَّة الأدلَّة والبيانات التي تم جمعها. يتطوَّر العلم التَّراكمي بخطى تدريجيَّة ومتزايدة، وليس عبر اكتشاف واحد مفاجئ. في حين أنَّ العلم الثَّوري نادر الحدوث، يعد العلم التَّراكمي أكثر أشكال العلوم شيوعًا. **المصطلحات ذات الصِّلة:** العلم البطيء.",
                "related_terms": [
                    "Slow Science"
                ],
                "references": "Curran, P. J. (2009). The seemingly quixotic pursuit of a cumulative psychological science: Introduction to the special issue. Psychological Methods, 14(2), 77–80. https://doi.org/10.1037/a0015972\n\nd’Espagnat, B. (2008). Is science cumulative? A physicist viewpoint. In Rethinking Scientific Change and Theory Comparison (pp. 145–151). Springer. https://doi.org/10.1007/978-1-4020-6279-7_10\n\nKuhn, T. (1962). The Structure of Scientific Revolutions. University of Chicago Press.\n\nMischel, W. (2009). Becoming a Cumulative Science. Association for Psychological Science. https://www.psychologicalscience.org/observer/becoming-a-cumulative-science",
                "drafted_by": [
                    "Beatrice Valentini"
                ],
                "reviewed_by": [
                    "Sarah Ashcroft-Jones",
                    "Mahmoud Elsherif",
                    "Helena Hartmann",
                    "Oscar Lecuona",
                    "Wanyin Li",
                    "Sonia Rishi",
                    "Flávio Azevedo"
                ],
                "alt_related_terms": [
                    null
                ],
                "language": "arabic"
            },
            {
                "type": "glossary",
                "title": "DA-RT الوصول للبيانات وشفافيَّة البحث (Data Access and Research Transparency (DA-RT))",
                "definition": "مبادرة تهدف إلى زيادة الوصول إلى البيانات وشفافيَّة البحوث في العلوم الاجتماعيَّة. وهي مبادرة متعدِّدة المعارف والأساليب، أنشأها مجلس جمعيَّة العلوم السِّياسيَّة الأمريكيَّة في عام 2014 لتعزيز دقة البحث الاجتماعي التجريبي . وبالإضافة إلى أنشطتها الأخرى، طورت هذه المبادرة بيان الشَّفافيَّة لمحرِّري المجلات والذي يتطلَّب التزام المجلات المشتركة بهذه المبادرة بالآتي: (أ) إتاحة البيانات ذات الصِّلة للعموم إذا تم نشر الدِّراسة. (ب) اتِّباع سياسة صارمة للاستشهاد بالبيانات. (ج) وصف الإجراءات التَّحليليَّة بشفافيَّة، وإن أمكن، إتاحة الوصول للنَّص البرمجي للتَّحليل. (د) تحديث أدلّة نمط المجلّة، وقواعد الأخلاقيَّات؛ لتشمل تسهيل الوصول إلى البيانات ومتطلّبات الشَّفافيَّة البحثيَّة. **المصطلحات ذات الصِّلة: إمكانيَّة الوصول، مشاركة البيانات، قابلية التكرار، قابليَّة إعادة الإنتاج.",
                "related_terms": [
                    "Accessibility",
                    "Data sharing",
                    "Replicability",
                    "Reproducibility"
                ],
                "references": "Carsey, T. M. (2014). Making DA-RT a reality. PS: Political Science & Politics, 47(1), 72–77. https://doi.org/10.1017/S1049096513001753\n\nMonroe, K. R. (2018). The rush to transparency: DA-RT and the potential dangers for qualitative research. Perspectives on Politics, 16(1), 141–148. https://doi.org/10.1017/S153759271700336X",
                "drafted_by": [
                    "Eike Mark Rinke"
                ],
                "reviewed_by": [
                    "Filip Dechterenko",
                    "Kai Krautter",
                    "Flávio Azevedo"
                ],
                "alt_related_terms": [
                    null
                ],
                "language": "arabic"
            },
            {
                "type": "glossary",
                "title": "DMP خطة إدارة البيانات (Data management plan (DMP))",
                "definition": "يشير هذا المصطلح إلى ملف منظم يصف عمليَّة الحصول على البيانات وتحليلها وإدارتها وتخزينها أثناء قيام بمشروع بحثي ما ، كما تصف ملكيَّة هذه البيانات وكيف سيتم حفظها ونشرها أثناء وبعد الانتهاء من المشروع. توفِّر قوالب إدارة البيانات إرشادات حول كيفيَّة جعل بيانات البحث متوافقة مع مبادئ فير ومتاحة للجميع إن أمكن ذلك. **المصطلحات ذات الصِّلة:** أرشفة البيانات، مشاركة البيانات، تخزين البيانات، مبادئ فير، البيانات المفتوحة.",
                "related_terms": [
                    "Data archiving",
                    "Data sharing",
                    "Data storage",
                    "FAIR principles",
                    "Open data"
                ],
                "references": "Burnette, M., Williams, S., & Imker, H. (2016). From Plan to Action: Successful Data Management Plan Implementation in a Multidisciplinary Project. Journal of eScience Librarianship, 5(1), e1101. https://doi.org/10.7191/jeslib.2016.1101\n\nMichener, W. K. (2015). Ten simple rules for creating a good data management plan. PLoS Computational Biology, 11(10), e1004525. https://doi.org/10.1371/journal.pcbi.1004525",
                "drafted_by": [
                    "Dominique Roche"
                ],
                "reviewed_by": [
                    "Charlotte R. Pennington",
                    "Sam Parsons",
                    "Birgit Schmidt",
                    "Flávio Azevedo"
                ],
                "alt_related_terms": [
                    null
                ],
                "language": "arabic"
            },
            {
                "type": "glossary",
                "title": "مشاركة البيانات (Data sharing)",
                "definition": "مجموعة من الممارسات، والتِّقنيَّات، والعناصر الثَّقافيَّة، والأطر القانونيَّة التي تتعلَّق بجعل البيانات المستخدمة في البحث العلمي متاحة للباحثين الآخرين. وصف بعض الباحثين (Gollwitzer et al., 2020\\) نوعين من مشاركة البيانات: النَّوع 1/ البيانات الضَّروريَّة لإعادة إنتاج نتائج مقالة بحثيَّة منشورة. النوع 2/ البيانات التي تم جمعها في مشروع بحثي ولكن لم يتم تحليلها (أو تم تحليلها جزئيًا فقط) أو لم يتم كتابة تقرير عنها بعد الانتهاء من المشروع، ومن ثم تتم مشاركتها عادةً في ظل فترة حظر محدَّدة. **المصطلحات ذات الصِّلة:** مبادئ فير؛ البيانات المفتوحة.",
                "related_terms": [
                    "FAIR principles",
                    "Open data"
                ],
                "references": "Abele-Brehm, A. E., Gollwitzer, M., Steinberg, U., & Schönbrodt, F. D. (2019). Attitudes toward open science and public data sharing. Social Psychology, 50, 252–260. https://doi.org/10.1027/1864-9335/a000384\n\nGollwitzer, M., Abele-Brehm, A., Fiebach, C., Ramthun, R., Scheel, A. M., Schönbrodt, F. D., & Steinberg, U. (2020). Data Management and Data Sharing in Psychological Science: Revision of the DGPs Recommendations.\n\nfor Data Sharing, S. C. (n.d.). What is data sharing? Retrieved 11 July 2021. https://eudatasharing.eu/what-data-sharing",
                "drafted_by": [
                    "Mahmoud Elsherif"
                ],
                "reviewed_by": [
                    "Helena Hartmann",
                    "Tina Lonsdorf",
                    "Charlotte R. Pennington",
                    "Timo Roettger",
                    "Flávio Azevedo"
                ],
                "alt_related_terms": [
                    null
                ],
                "language": "arabic"
            },
            {
                "type": "glossary",
                "title": "تصوير البيانات (Data visualisation)",
                "definition": "هو التَّمثيل الرُّسومي للبيانات أو المعلومات. تستند فعاليَّة تصوير البيانات إلى قدرة المعالجة البصريَّة المتطوِّرة لدى البشر في نقل الأفكار، وتوصيل المعلومات الأساسيَّة. يبرز تصوير البيانات غالبًا الرُّسوماتِ والبيانات الأوليَّة، والإحصائيّات الوصفيَّة، و الإحصائيَّات الاستدلاليَّة. **المصطلحات ذات الصِّلة:** شكل بياني، رسم بياني، مخطَّط بياني.",
                "related_terms": [
                    "Figure",
                    "Graph",
                    "Plot"
                ],
                "references": "Healy, K. (2018). Data visualization: A practical introduction. Princeton University Press.\n\nTufte, E. R. (1983). The visual display of quantitative information. Graphics Press.",
                "drafted_by": [
                    "Bradley Baker"
                ],
                "reviewed_by": [
                    "Mahmoud Elsherif",
                    "Charlotte R. Pennington",
                    "Suzanne L. K. Stewart;"
                ],
                "alt_related_terms": [
                    null
                ],
                "language": "arabic"
            },
            {
                "type": "glossary",
                "title": "إنهاء الاستعمار (Decolonisation)",
                "definition": "التعريف:** يمكن وصف الاستعمارية بأنها تطبيع لمفاهيم مثل الإمبريالية والرأسمالية والقومية. ويمكن اعتبار هذه المفاهيم مجتمعةً بمثابة مصفوفة من القوة (وعلاقات سلطوية) يمكن إرجاعها إلى الفترة الاستعمارية. تسعى عملية إنهاء الاستعمار إلى تفكيك هذه العلاقات السلطوية وإضعاف مركزيتها، بهدف فهم استمرارها وإعادة بناء معايير وقيم مجال معين. وفي البيئة الأكاديمية، يشير مفهوم إنهاء الاستعمار إلى إعادة التفكير في العدسة التي ندرس من خلالها ونبحث ونتعايش، بحيث تتجاوز العدسة المنظورات الغربية والاستعمارية. تتضمن عملية إنهاء الاستعمار في الأوساط الأكاديمية إعادة بناء الأطر التاريخية والثقافية، وإعادة توزيع الشعور بالانتماء داخلفي الجامعات، وتمكين وإشراك الأصوات وأنواع المعرفة التي تم استبعادها تاريخيًا من قبل الأوساط الأكاديمية. يتم ذلك عندما يتفاعل الأفرادالناس مع ماضيهم وحاضرهم ومستقبلهم من منظور مستقل عن المنظور المهيمن اجتماعيًا. ويتم ذلك أيضًا من خلال استيعاب، وليس رفض، القيم والعادات الداخلية للأفراد من مستعمرة معينة. **المصطلحات ذات الصِّلة:** التنوع؛ المساواة؛ الشمول",
                "related_terms": [
                    "Diversity",
                    "Equity",
                    "Inclusion"
                ],
                "references": "Albayrak, N. (2018). Diversity helps but decolonisation is the key to equality in higher education. Retrieved from https://lsepgcertcitl.wordpress.com/2018/04/16/diversity-helps-but-decolonisation-is-the-key-to-equality-in-higher-education/",
                "drafted_by": [
                    "Nihan Albayrak-Aydemir"
                ],
                "reviewed_by": [
                    "Sarah Ashcroft-Jones",
                    "Mahmoud Elsherif",
                    "Michele C. Lim",
                    "Emma Norris",
                    "Flávio Azevedo"
                ],
                "alt_related_terms": [
                    null
                ],
                "language": "arabic"
            },
            {
                "type": "glossary",
                "title": "معيار التَّمييز (Demarcation criterion)",
                "definition": "معيار لتمييز العلم عن غير العلم، يهدف إلى بيان الطريقة المثلى لنمو المعرفة في العالم. في النَّهج البوبري، يعدُّ معيار التمييز هو القابلية للتزييف، وتطبيق موقف التَّكذيب. تشمل الأساليب البديلة نهج \"كُون\"، الذي اعتقد بأنَّ المعيار هو حل الألغاز بهدف فهم الطبيعة، وكذلك \"لاكاتوس\" الذي رأى العلم يتميّز بالعمل ضمن برنامج بحث تقدمي. ال**مصطلحات ذات الصِّلة:** الفرضيَّة، التَّزييف.",
                "related_terms": [
                    "Hypothesis",
                    "Falsification"
                ],
                "references": "Dienes, Z. (2008). Understanding psychology as a science: An introduction to scientific and statistical inference. Macmillan International Higher Education.",
                "drafted_by": [
                    "Alaa AlDoh"
                ],
                "reviewed_by": [
                    "Bethan Iley",
                    "Sara Middleton"
                ],
                "alt_related_terms": [
                    null
                ],
                "language": "arabic"
            },
            {
                "type": "glossary",
                "title": "digital object identifier معرّف الكائن الرَّقمي (DOI (digital object identifier))",
                "definition": "معرفات الكائنات الرَّقميَّة هي سلاسل أبجديَّة رقميَّة يمكن تخصيصها لأي كيان بما في ذلك: المنشورات (ومنها المطبوعات الأوليَّة)، والمواد، ومجموعات البيانات، والأفلام. ولا يقتصر استخدام المعرّفات الرَّقميَّة على المواد الأكاديميَّة فقط. إنَّ معرفات الكائنات الرَّقميَّة توفِّر نظامًا لتحديد دائم، وقابل للتَّنفيذ، وتبادل قابل للتَّشغيل البيني للمعلومات المدارة على الشَّبكات الرَّقميَّة  (https://www.doi.org/doi-handbook/HTML/index.html). هناك العديد من وكالات لتسجيل معرّفات الكائنات الرَّقميَّة وإدارتها، ولكن من المرجَّح أنْ يواجه أغلب الباحثين اثنتين هما: Crossref و Datacite. :**المصطلحات ذات الصِّلة** arXiv and BibTex; Crossref, Datacite, ISBN, ISO, الأوركيد; Permalink.",
                "related_terms": [
                    "arXiv and BibTex",
                    "Crossref, Datacite, ISBN, ISO, ORCID",
                    "Permalink"
                ],
                "references": "Bilder, G. (2013). DOIs unambiguously and persistently identify published, trustworthy, citable online scholarly literature. Right? https://www.crossref.org/blog/dois-unambiguously-and-persistently-identify-published-trustworthy-citable-online-scholarly-literature-right/\n\nMorgan, C. (1998). The DOI (Digital Object Identifier). Serials, 11(1), 47–51. http://doi.org/10.1629/1147\n\nAnon. (2019). The DOI Handbook.",
                "drafted_by": [
                    "Tina Lonsdorf"
                ],
                "reviewed_by": [
                    "Ashley Blake",
                    "Helena Hartmann",
                    "Sam Parsons",
                    "Charlotte R. Pennington",
                    "Flávio Azevedo"
                ],
                "alt_related_terms": [
                    null
                ],
                "language": "arabic"
            },
            {
                "type": "glossary",
                "title": "التحكيم مزدوج التَّعمية (Double-blind peer review)",
                "definition": "هو تقييم نتائج البحوث من قِبل خبراء مؤهلين حيث يكون المؤلِّف (أو المؤلِّفون) والمحكّم (أو المحكّمون) مجهولًا (مجهولين) بالِّنسبة لبعضهم البعض، حيث يخفي هذا النَّهج هوية المؤلِّفين وانتماءاتهم عن المحكّمين ومن شأنه، من النَّاحية النَّظريَّة، إزالة التَّحيُّزات المتعلِّقة بالسُّمعة المهنيَّة، والجنس، والعِرق، والانتماء المؤسسي، ممّا يسمح للمحكّم بتجنب التَّحيُّز والتَّركيز على جودة النَّص وحده. (Tvina et al., 2019, p. 1082).  وكما هو الحال في جميع أنواع تحكيم الأقران فإنَّ التحكيم مزدوج التَّعمية لا يخلو من العيوب، فقد يكون إخفاء الهوية أمرًا صعبًا، إن لم يكن مستحيلًا، بالنِّسبة لبعض الباحثين العاملين في مجال متخصِّص.  **المصطلحات ذات الصِّلة:** تَّحيُّز الشَّخصنة، تحيُّز الانتماء ، المراجعة المجهولة، المراجعة المخفيَّة، التّحكيم المفتوح، تحكيم الأقران، التّحكيم أحاديَّ التَّعمية، المراجعة التَّقليديَّة، التّحكيم ثُّلاثيّ التعمية.",
                "related_terms": [
                    "Ad hominem bias",
                    "Affiliation bias",
                    "Anonymous review",
                    "Masked review",
                    "Open peer review",
                    "Peer review",
                    "Single-blind peer review",
                    "Traditional peer review",
                    "Triple-Blind peer review"
                ],
                "references": "Largent, E. A., & Snodgrass, R. T. (2016). Blind peer review by academic journals. In C. T. Robertson & A. S. Kesselheim (Eds.), Blinding as a solution to bias: Strengthening biomedical science, forensic science, and law (pp. 75–95). Academic Press. https://doi.org/10.1016/B978-0-12-802460-7.00005-X\n\nTvina, A., Spellecy, R., & Palatnik, A. (2019). Bias in the peer review process: can we do better? Obstetrics & Gynecology, 133(6), 1081–1083. https://doi.org/10.1097/AOG.0000000000003260",
                "drafted_by": [
                    "Mahmoud Elsherif"
                ],
                "reviewed_by": [
                    "Bradley Baker",
                    "Helena Hartmann",
                    "Meng Liu",
                    "Emma Norris"
                ],
                "alt_related_terms": [
                    null
                ],
                "language": "arabic"
            },
            {
                "type": "glossary",
                "title": "الوعي المزدوج (Double consciousness)",
                "definition": "حالة ارتباك في الهُويَّة، والتي يشعر خلالها الفرد بأنَّ لديه هويتين مختلفتين، هوية تسعى للاندماج مع الثَّقافة السَّائدة في المحيط الجامعي عندما يكون الفرد بين زملائه وأساتذته، وهوية أخرى عندما يكون بين أفراد عائلته.  قد يتسبب هذا التَّبادل المستمرّ بين الهويتين إلى ضعف ثقة الفرد بهويته واعتقاده بأنَّه لا ينتمي  إلى أي جهة.  ويمكن أن يؤدِّي شعور الفرد بضعف الانتماء إلى عدم قدرته على الاندماج الاجتماعي في الثَّقافة الأكاديميَّة، وهذا بدوره قد يؤدِّي  إلى تقليل فرص الفرد ويؤثر على صحته العقليَّة (Rubin, 2021; Rubin et al., 2019).  **المصطلحات ذات الصِّلة:** الطَّبقة الاجتماعيَّة، الاندماج الاجتماعي",
                "related_terms": [
                    "Social class",
                    "Social integration"
                ],
                "references": "Albayrak, N., & Okoroji, C. (2019). Facing the challenges of postgraduate study as a minority student. A Guide for Psychology Postgraduates, 63.\n\nDu Bois, W. E. B. (1968). The souls of black folk; essays and sketches. Johnson Reprint Corp.\n\nGilroy, P. (1993). The black Atlantic: Modernity and double consciousness. Harvard University Press.",
                "drafted_by": [
                    "Nihan Albayrak-Aydemir"
                ],
                "reviewed_by": [
                    "Mahmoud Elsherif",
                    "Wanyin Li",
                    "Michele C. Lim",
                    "Adam Parker"
                ],
                "alt_related_terms": [
                    null
                ],
                "language": "arabic"
            },
            {
                "type": "glossary",
                "title": "إعلان سان فرانسيسكو بشأن تقييم البحوث (DORA)",
                "definition": "اختصار لإعلان سان فرانسيسكو لتقييم البحث العلمي، وهو مبادرة عالميَّة تهدف إلى تقليل الاعتماد على مقاييس المجلات العلميَّة، مثل: معامل التأثير أو عدد الاستشهادات وتعزيز الثَّقافة التي تركِّز على القيمة الفعليَّة للبحث. يستهدف إعلان \"دورا\" كلًا من المانحين، والنَّاشرين، والمراكز البحثيَّة، وكذلك الباحثين. والَّتوقيع عليه يمثل تعهدًا بمواءمة الإجراءات والممارسات البحثية مع مبادئ الإعلان. **المصطلحات ذات الصِّة:** القابلية للتَّعميم، معامل تَّأثير المجلَّة، العلم المفتوح.",
                "related_terms": [
                    "Generalizability",
                    "Journal Impact Factor",
                    "Open Science"
                ],
                "references": "Health Research Board. (n.d.). Declaration on Research Assessment. Retrieved from https://www.hrb.ie/funding/funding-schemes/before-you-apply/how-we-assess-applications/declaration-on-research-assessment/",
                "drafted_by": [
                    "Aoife O’Mahony"
                ],
                "reviewed_by": [
                    "Connor Keating",
                    "Charlotte R. Pennington"
                ],
                "alt_related_terms": [
                    null
                ],
                "language": "arabic"
            },
            {
                "type": "glossary",
                "title": "التِّكرار المباشر (Direct replication)",
                "definition": "نظرًا لأنَّ مصطلح \"التِّكرار المباشر\" ليس له معنى تقني محدَّد متَّفق عليه على نطاق واسع، كما أنَّه لا يوجد تمييز واضح بين التِّكرار المباشر، والتِّكرار المفاهيمي، فإنَّنا ندرج أدناه العديد من المساهمات التي بُذلت للوصول إلى معنى متّفق عليه.  بدلًا من مناقشة \"دقّة\" التِّكرار، من المفيد أكثر مناقشة الاختلافات ذات الصِّلة بين الدِّراسة المكرَّرة والدِّراسة الأصليَّة، وتأثيرات ذلك على موثوقيَّة، وعموميَّة نتائج الدِّراسة الأصليَّة. بشكل عام، يشير التِّكرار المباشر إلى عملية جمع بيانات جديدة بهدف تكرار الطُّرق البحثيَّة للدِّراسات الأصليَّة بأكبر قدر ممكن، وهي محاولة \"تسعى إلى تكرار العناصر الضَّروريَّة التي أنتجت النَّتيجة الأصليَّة (Cruwell et al., 2019, p. 243). قد يكون الغرض من التِّكرار المباشر تحديد أخطاء النَّوع الأول، أو التَّأثيرات المتعلِّقة بالمُختبِر، أو تحديد إمكانيَّة تكرار نتيجة ما باستخدام نفس الممارسات البحثية، أو باستخدام ممارسات أفضل، أو إيجاد تقديرات أكثر دِقَّة لحجم التَّأثير (Hűffmeier et al., 2016). إنَّ مدى كون الدِّراسة التِّكراريَّة مباشرة تتراوح بين مجرَّد تكرار لملاحظات (بيانات) محدَّدة وبين ملاحظة التَّأثيرات المعمَّمة الظواهر. إنَّ مدى قرب الدِّراسة التِّكراريَّة للدِّراسة الأصليَّة هو غالبًا محل نقاش، وغالبًا ما يتم ذكر أنَّ الاختلافات تمثل عوامل وسيطة مخفيَّة. إضافة إلى ذلك، قد يكون هناك جدل حول الأهميَّة التَّقريبية للتَّكافؤ المنهجي (أي استخدام مواد متطابقة) للدِّراسة الأصليَّة مقابل التَّكافؤ النَّفسي (أي إدراك الظُّروف النَّفسيَّة المتطابقة) للدِّراسة الأصليَّة (Schwarz and Strack, 2014). على سبيل المثال، لننظر إلى دراسة حول الثِّقة في \\-رئيس الولايات المتَّحدة والتي أجريت في عام 2018\\. إنَّ التِّكرار المنهجي المتطابق سيقوم باستخدام ترامب كمحفز (كان رئيسًا في عام 2018)، أما التِّكرار النَّفسي المتطابق سيقوم باستخدام بايدن (وهو الرئيس الحالي). **المصطلحات ذات الصِّلة:** التِّكرار الوثيق، التِّكرار المفاهيمي، التِّكرار، العوامل الوسيطة الخفيَّة.",
                "related_terms": [
                    "close replication",
                    "Conceptual replication",
                    "exact replication",
                    "hidden moderators"
                ],
                "references": "Crüwell, S., van Doorn, J., Etz, A., Makel, M. C., Moshontz, H., Niebaum, J. C., Orben, A., Parsons, S., & Schulte-Mecklenbeck, M. (2019). Seven Easy Steps to Open Science: An Annotated Reading List. Zeitschrift Für Psychologie, 227(4), 237–248. https://doi.org/10.1027/2151-2604/a000387\n\nHüffmeier, J., Mazei, J., & Schultze, T. (2016). Reconceptualizing replication as a sequence of different studies: A replication typology. Journal of Experimental Social Psychology, 66, 81–92. https://doi.org/10.1016/j.jesp.2015.09.009\n\nLeBel, E. P., Vanpaemel, W., Cheung, I., & Campbell, L. (2017). A brief guide to evaluate replications. Meta-Psychology, 3. https://doi.org/10.15626/MP.2018.843\n\nSchwarz, N., & Strack, F. (2014). Does Merely Going Through the Same Moves Make for a “Direct” Replication?: Concepts, Contexts, and Operationalizations. Social Psychology, 45(4), 305–306.",
                "drafted_by": [
                    "Mahmoud Elsherif (original); Thomas Rhys Evans (alternative); Tina Lonsdorf (alternative)"
                ],
                "reviewed_by": [
                    "Beatrix Arendt",
                    "Adrien Fillon",
                    "Matt Jaquiery",
                    "Charlotte R. Pennington",
                    "Graham Reid",
                    "Lisa Spitzer",
                    "Tobias Wingen",
                    "Flávio Azevedo"
                ],
                "alt_related_terms": [
                    null
                ],
                "language": "arabic"
            },
            {
                "type": "glossary",
                "title": "التَّنوع (Diversity)",
                "definition": "يشير التَّنوع إلى الاختلاف بين الأشخاص (أي الأفراد من البشر)، على سبيل المثال من حيث القدرة، أو العمر، أو المعتقدات، أو الإدراك، أو البلد، أو الإعاقة، أو الثَّقافة، أو الجنس، أو اللُّغة، أو العِرق، أو الدِّين، أو الميول الجنسيَّة.  يمكن أنْ يشيرَ التَّنوع إلى تنوع الباحثين (الذين يقومون بالبحث)، وتنوع عيّنات المشاركين (الذين يتم تضمينهم في الدِّراسة)، وتنوع وجهات النَّظر (الآراء والمعتقدات التي يجلبها الباحثون في عملهم (Syed & Kathawalla, 2020).  **المصطلحات ذات الصِّلة:** بروبينساينس، بيزار، إنهاءمقاومة الاستعمار، الوعي المزدوج، العدالة، الشُّمول،  سترينج: الخلفية الاجتماعية والقابلية للتتبع والاختيار الذاتي وتاريخ التربية والتأقلم والتّعود، وِيرد",
                "related_terms": [
                    "Bropenscience",
                    "BIZARRE",
                    "Decolonisation",
                    "Double Consciousness",
                    "Equity",
                    "Inclusion",
                    "STRANGE",
                    "WEIRD"
                ],
                "references": "Syed, M., & Kathawalla, U. (2020). Cultural Psychology, Diversity, and Representation in Open Science. https://doi.org/10.31234/osf.io/t7hp2",
                "drafted_by": [
                    "Ryan Millager; Mariella Paul"
                ],
                "reviewed_by": [
                    "Nihan Albayrak-Aydemir",
                    "Mahmoud Elsherif",
                    "Helena Hartmann",
                    "Madeleine Ingham",
                    "Annalise A. LaPlume",
                    "Wanyin Li",
                    "Charlotte R. Pennington",
                    "Olly Robertson",
                    "Flávio Azevedo"
                ],
                "alt_related_terms": [
                    null
                ],
                "language": "arabic"
            },
            {
                "type": "glossary",
                "title": "ECRs الباحثون المبتدئون (Early career researchers (ECRs))",
                "definition": "هو تصنيف يُمنح للباحثين الذين هم من الطَّلبة في السِّنين الأخيرة من الدُّكتوراة إلى العاملين في مرحلة ما بعد الدُّكتوراة والذين قد تصل خبرتهم إلى 10 سنوات من التَّعليم بعد الدُّكتوراة؛ ولذلك قد تشمل المجموعة الأخيرة الأكاديميين في بداية حياتهم المهنيَّة، أو الأكاديميين المبتدئين\" (Eley et al., 2012, p. 3).  يختلف تحديد من ينطبق عليهم هذا التَّصنيف باختلاف هيئات التَّمويل، والمؤسسات الأكاديميَّة، والبلدان، مثلًا: فيما يتعلَّق بالعمر، وما إذا كانت الفترة منذ الحصول على درجة الدُّكتوراة شاملة فترات الإجازات، والتَّفرُّغ، أم لا، والمسمّى الوظيفي، والتَّمويل الممنوح. **المصطلحات ذات الصِّلة:** باحث في بداية حياته المهنيَّة.",
                "related_terms": [
                    "Early Career Investigator"
                ],
                "references": "Bazeley, P. (2003). Defining “Early Career” in Research. Higher Education, 45, 257–279. https://doi.org/10.1023/A:1022698529612\n\nEley, A. R. (2012). Becoming a successful early career researcher. Routledge. Retrieved from http://www.worldcat.org/oclc/934369360\n\nPownall, M., Talbot, C. V., Henschel, A., Lautarescu, A., Lloyd, K. E., Hartmann, H., Darda, K. M., Tang, K. T. Y., Carmichael-Murphy, P., & Siegel, J. A. (2021). Navigating Open Science as Early Career Feminist Researchers. Psychology of Women Quarterly, 45(4), 526–539. https://doi.org/10.1177/03616843211029255 Retrieved from https://journals.sagepub.com/doi/10.1177/03616843211029255",
                "drafted_by": [
                    "Micah Vandegrift"
                ],
                "reviewed_by": [
                    "Thomas Rhys Evans",
                    "Sam Parsons",
                    "Olly Robertson",
                    "Suzanne L. K. Stewart",
                    "Flávio Azevedo"
                ],
                "alt_related_terms": [
                    null
                ],
                "language": "arabic"
            },
            {
                "type": "glossary",
                "title": "الأثر الاقتصادي والاجتماعي (Economic and societal impact)",
                "definition": "المساهمة التي يقدمها بحث ما للاقتصاد، والمجتمع بشكل عام.  كما يُجسد هذا المصطلح فوائد البحث عن الأفراد والمؤسسات و الدُّول. **المصطلحات ذات الصِّلة**: التَّأثير الأكاديمي.",
                "related_terms": [
                    "Academic Impact"
                ],
                "references": "Economic, & Council, S. R. (n.d.). What is impact? Retrieved 8 July 2021. https://esrc.ukri.org/research/impact-toolkit/what-is-impact/",
                "drafted_by": [
                    "Adam Parker"
                ],
                "reviewed_by": [
                    "Helena Hartmann",
                    "Aoife O’Mahony",
                    "Charlotte R. Pennington"
                ],
                "alt_related_terms": [
                    null
                ],
                "language": "arabic"
            },
            {
                "type": "glossary",
                "title": "فترة الحظر (Embargo Period)",
                "definition": "في سياق العلم المفتوح في النَّشر الأكاديمي، تُعرّف فترة الحظر بالفترة الزَّمنيَّة التي تلي نشر المقالة وقبل أنْ تصبح متاحةً للوصول المفتوح.  إذا قرَّر المؤلِّف أرشفة مقالته ذاتيًا،على سبيل المثال: في مستودع للوصول المفتوح، فيتوجَّب عليه مراعاة أي فترة حظر يفرضها النَّاشر.  وتختلف فترات الحظر من فورية إلى 48 شهرًا، وتتراوح أغلبها بين 6 و12 شهرًا (Laakso & Björk, 2013). يمكن أن تنطبق فترات الحظر أيضًا على التَّسجيلات المسبقة، والمواد، والبيانات.  عندما يقرر المؤلِّفون إتاحتها للجمهور فقط بعد فترة زمنية معيَّنة، على سبيل المثال: عند النَّشر، أو حتى في وقت لاحق عندما يكون لدى المؤلِّف خطط نشر إضافية ويرغب في تجنب أن يتم سبقه (Klein et al., 2018).  **المصطلحات ذات الصِّلة**: الوصول المفتوح، حاجز مالي، الطّباعة الأوليَّة.",
                "related_terms": [
                    "Open access",
                    "Paywall",
                    "Preprint"
                ],
                "references": "Klein, O., Hardwicke, T. E., Aust, F., Breuer, J., Danielsson, H., Mohr, A. H., IJzerman, H., Nilsonne, G., Vanpaemel, W., & Frank, M. C. (2018). A practical guide for transparency in psychological science. Collabra: Psychology, 4(1), 20. https://doi.org/10.1525/collabra.158\n\nLaakso, M., & Björk, B. C. (2013). Delayed open access: An overlooked high‐impact category of openly available scientific literature. Journal of the American Society for Information Science and Technology, 64(7), 1323–1329.\n\nEmbargo (academic publishing). (2021). https://en.wikipedia.org/w/index.php?title=Embargo_(academic_publishing)&oldid=1016895567",
                "drafted_by": [
                    "Aleksandra Lazić"
                ],
                "reviewed_by": [
                    "Bradley Baker",
                    "Adam Parker",
                    "Sam Parsons",
                    "Steven Verheyen",
                    "Flávio Azevedo"
                ],
                "alt_related_terms": [
                    null
                ],
                "language": "arabic"
            },
            {
                "type": "glossary",
                "title": "اللايقين المعرفي (Epistemic uncertainty)",
                "definition": "عدم اليقين المنهجي ناتج عن محدوديَّة البيانات، أو دِقَّة القياس، أو مواصفات النَّموذج، أو العملية، أو نقص المعرفة. ويمكن نظريًا تقليل عدم اليقين النَّاتج بسبب نقص المعرفة من خلال إجراء بحوث إضافيَّة لزيادة الفهم. يذهب البعض إلى أنَّ عدم اليقين أمر شخصي؛ لأنَّ المعرفة تختلف بين العلماء، وكما أنَّه مؤقت؛ لأنَّه يمكن أنْ يتغيَّر مع توفُّر بيانات جديدة. **المصطلحات ذات الصِّلة:** الاحتمال العشوائي، عدم اليقين النَّايتي. **مصطلحات بديلة:** يعرف اللا يقين المعرفي أيضًا باسم اللا يقين العلمي، أو اللا يقين الذَّاتي، أو اللا يقين من النَّوع (ب)",
                "related_terms": [
                    "Aleatoric uncertainty",
                    "Knightian uncertainty"
                ],
                "references": "Der Kiureghian, A., & Ditlevsen, O. (2009). Aleatory or epistemic? Does it matter? Structural Safety, 31(2), 105–112. https://doi.org/10.1016/j.strusafe.2008.06.020\n\nFerson, S., Joslyn, C. A., Helton, J. C., Oberkampf, W. L., & Sentz, K. (2004). Summary from the epistemic uncertainty workshop: consensus amid diversity. Reliability Engineering & System Safety, 85(1–3), 355–369. https://doi.org/10.1016/j.ress.2004.03.023",
                "drafted_by": [
                    "Bradley Baker"
                ],
                "reviewed_by": [
                    "Jamie P. Cockcroft",
                    "Elizabeth Collins",
                    "Charlotte R. Pennington",
                    "Graham Reid"
                ],
                "alt_related_terms": [
                    null
                ],
                "language": "arabic"
            },
            {
                "type": "glossary",
                "title": "نظريَّة المعرفة (Epistemology)",
                "definition": "تعدُّ نظريَّة المعرفة أحد الفروع الأربعة الرَّئيسيَّة للفلسفة، إلى جانب أخلاقيات البحث والمنطق والميتافيزيا. تهتم نظرية المعرفة إلى حد كبير بطبيعة المعرفة، وأصلها، ونطاقها، إضافة إلى عقلانيَّة المعتقدات.  **المصطلحات ذات الصِّلة:** العلوم البعدية/التلوية أو البحث البعدي/التلوي، علم الوجود (الذَّكاء الاصطناعي)",
                "related_terms": [
                    "Meta-science or Meta-research ",
                    "Ontology (Artificial Intelligence)"
                ],
                "references": "Steup, M., & Neta, R. (2020). Epistemology. Stanford Encyclopedia of Philosophy. https://plato.stanford.edu/entries/epistemology/",
                "drafted_by": [
                    "Amélie Beffara Bret"
                ],
                "reviewed_by": [
                    "Emma Norris",
                    "Adam Parker",
                    "Robert M Ross",
                    "Steven Verheyen"
                ],
                "alt_related_terms": [
                    null
                ],
                "language": "arabic"
            },
            {
                "type": "glossary",
                "title": "العدالة (Equity)",
                "definition": "الأفراد المختلفون لديهم بدايات مختلفة (راجع \"فجوات الفرص\") واحتياجات مختلفة. ففي حين تركز المعاملة المتساوية على معاملة جميع الأفراد بنفس الدرجة، فإنَّ المعاملة المنصفة تهدف إلى تحقيق تكافؤ الفرص من خلال زيادة الفرص المتاحة للأقليات الممثَّلة تمثيلًا ناقصًا.  تهدف المعاملة المنصفة إلى تحقيق المساواة من خلال \"الإنصاف\"، مع مراعاة الاحتياجات المختلفة لدعم الأفراد المختلفين، بدلًا من التَّركيز فقط على احتياجات الأغلبيَّة. **المصطلحات ذات الصِّلة:** التَّنوع، العدالة، الإنصاف، الشُّمول، العدالة الاجتماعية.",
                "related_terms": [
                    "Diversity",
                    "Equality",
                    "Fairness",
                    "Inclusion",
                    "Social justice"
                ],
                "references": "Albayrak-Aydemir, N. (2020). The hidden costs of being a scholar from the global south. Retrieved from https://blogs.lse.ac.uk/highereducation/2020/02/20/the-hidden-costs-of-being-a-scholar-from-the-global-south/\n\nPosselt, J. R. (2020). Equity in Science: Representation, Culture, and the Dynamics of Change in Graduate Education. Stanford University Press. https://books.google.de/books?id=2CjwDwAAQBAJ",
                "drafted_by": [
                    "Gisela H. Govaart"
                ],
                "reviewed_by": [
                    "Nihan Albayrak-Aydemir",
                    "Mahmoud Elsherif",
                    "Ryan Millager",
                    "Charlotte R. Pennington",
                    "Beatrice Valentini",
                    "Flávio Azevedo"
                ],
                "alt_related_terms": [
                    null
                ],
                "language": "arabic"
            },
            {
                "type": "glossary",
                "title": "اختبار التَّكافؤ (Equivalence Testing)",
                "definition": "تقوم اختبارات التَّكافؤ بتقييم الفرضيَّة الصِّفريَّة إحصائيًا، وهل  لها تأثير معين يتجاوز الحد الأدنى من المعايير التي تعدُّ ذات معنى، وبالتَّالي فإنَّ رفض الفرضيَّة الصِّفرية يعدُّ دليلًا على عدم وجود تأثير (ذي معنى). بناءً على الإحصائيات المتكرِّرة.  تعمل اختبارات التَّكافؤ عبر تحديد حدود التكافؤ، قيمة دنيا وعليا تعكس أصغر حجم تأثير محل الاهتمام.  يتم بعد ذلك إجراء اختبارين \"تي\" أحادي الجانب مقابل كل من حدود التَّكافؤ هذه؛ لتقييم ما إذا كان من الممكن رفض التَّأثيرات التي تعدُّ ذات معنى (Schuirmann, 1972; Lakens et al., 2018; 2020).  **المصطلحات ذات الصِّلة:** حدود التَّكافؤ، التَّزوير، التَّحليلات المتكرِّرة، الاستدلال بفترات الثِّقة، اختبار دلالةالفرضيَّة الصِّفريَّة، أصغر حجم تأثير موضع اهتمام (SESOI)، (TOSTER)، إجراء TOST.",
                "related_terms": [
                    "Equivalence bounds",
                    "Falsification",
                    "Frequentist analyses",
                    "Inference by confidence intervals",
                    "Null Hypothesis Significance Testing (NHST)",
                    "Smallest effect size of interest (SESOI)",
                    "TOSTER",
                    "TOST procedure."
                ],
                "references": "Lakens, D., Scheel, A. M., & Isager, P. M. (2018). Equivalence testing for psychological research: A tutorial. Advances in Methods and Practices in Psychological Science, 1(2), 259–269. https://doi.org/10.1177/2515245918770963\n\nLakens, D., McLatchie, N., Isager, P. M., Scheel, A. M., & Dienes, Z. (2020). Improving inferences about null effects with Bayes factors and equivalence tests. The Journals of Gerontology: Series B, 75(1), 45–57. https://doi.org/10.1093/geronb/gby065\n\nSchuirmann, D. J. (1987). A comparison of the two one-sided tests procedure and the power approach for assessing the equivalence of average bioavailability. Journal of Pharmacokinetics and Biopharmaceutics, 15, 657–680. https://doi.org/10.1007/BF01068419",
                "drafted_by": [
                    "Charlotte R. Pennington"
                ],
                "reviewed_by": [
                    "Bradley Baker",
                    "James E. Bartlett",
                    "Jamie P. Cockcroft",
                    "Tobias Wingen",
                    "Flávio Azevedo"
                ],
                "alt_related_terms": [
                    null
                ],
                "language": "arabic"
            },
            {
                "type": "glossary",
                "title": "اكتشاف الاخطاء (Error detection)",
                "definition": "يشير هذا المصطلح بشكل عام إلى فحص البيانات، والنُّصوص البحثيَّة؛ لاكتشاف الأخطاء والتَّناقضات فيها.  من الأساليب الشَّائعة في هذا السِّياق فحص تناقضات الإحصاء الوصفي مثل أنْ يكون الموجز الإحصائي غير ممكنٍ؛ نظرًا لحجم العيّنة، أو خصائص القياس (Brown & Heathers, 2017; Heathers et al. 2018\\) ، أو فحص التَّناقضات في التَّقارير الإحصائيَّة مثل ألّا تتوافق القيمة الاحتمالية مع قيمة F ودرجات الحرّيّة المصاحبة (Epskamp, & Nuijten, 2016; Nuijten et al. 2016), أو التَّلاعب بالصُّور (Bik et al., 2016). يعد اكتشاف الأخطاء أحد الدَّوافع؛ لأنْ تكون البيانات ونص التَّحليل البرمجي متاحًا للجميع؛ لتتمكن عمليّة تحكيم الأقران من التَّأكد من نتائج البحث، أو تصحيح الأخطاء في حال كان البحث منشورًا.  كما أنَّه من الممكن أن تساهم الأخطاء المكتشفة في تصحيح، أو سحب البحوث المنشورة، بالرُّغم من أنَّ هذه الإجراءات عادة ما تتأخر، ولا تحدث إلا بعدما تكون هذة النَّتائج المغلوطة قد أثّرت في البحوث اللَّاحقة.  **المصطلحات ذات الصِّلة:** نزاهة البحث، التَّصحيح، سحب (البحث)",
                "related_terms": [
                    "Research integrity",
                    "correction",
                    "retraction"
                ],
                "references": "Bik, E. M., Casadevall, A., & Fang, F. C. (2016). The prevalence of inappropriate image duplication in biomedical research publications. MBio, 7(3), e00809-16.\n\nBrown, N. J., & Heathers, J. A. (2017). The grim test: A simple technique detects numerous anomalies in the reporting of results in psychology. Social Psychological and Personality Science, 8(4), 363–369.\n\nEpskamp, S., & Nuijten, M. B. (2016). statcheck: Extract statistics from articles and recompute p values. Retrieved from http://CRAN.R-project.org/package=statcheck\n\nHeathers, J. A., Anaya, J., van der Zee, T., & Brown, N. J. (2018). Recovering data from summary statistics: Sample Parameter Reconstruction via Iterative TEchniques (SPRITE). PeerJ Preprints, 6, e26968v1. https://doi.org/10.7287/peerj.preprints.26968v1\n\nNuijten, M. B., Hartgerink, C. H., van Assen, M. A., Epskamp, S., & Wicherts, J. M. (2016). The prevalence of statistical reporting errors in psychology (1985–2013). Behavior Research Methods, 48(4), 1205–1226.\n\nRetraction Watch. (n.d.). Retraction Watch. Retraction Watch. https://retractionwatch.com/",
                "drafted_by": [
                    "William Ngiam"
                ],
                "reviewed_by": [
                    "Ali H. Al-Hoorie",
                    "Jamie P. Cockcroft",
                    "Dominik Kiersz",
                    "Sam Parsons",
                    "Suzanne L. K. Stewart",
                    "Marta Topor"
                ],
                "alt_related_terms": [
                    null
                ],
                "language": "arabic"
            },
            {
                "type": "glossary",
                "title": "توليف الأدلة (Evidence Synthesis)",
                "definition": "التعريف**: هو نوع من مناهج   البحث و يهدف إلى استخلاص استنتاجات عامة لمعالجة سؤال بحثي حول موضوع أو ظاهرة أو تأثير معين من خلال مراجعة النتائج البحثية والمعلومات من مجموعة من المصادر المختلفة كما يمكن استخلاص المعلومات الخاضعة للتوليف من الدراسات النوعية والكمية. يمكن توليف هذه المعلومات بشكل كيفي (توليف سردي)، أو كمي (تحليل بعدي) أو بشكل مختلط (توليف بعدي، رسم خرائط منهجي). يحتوي توليف (تجميع) الأدلة على العديد من التطبيقات وغالبًا ما يستخدم في سياق الرعاية الصحية والسياسة العامة، ويُستخدم أيضًا لفهم وتطوير مجالات بحثية محددة. **المصطلحات ذات الصلة**: مراجعة الأدبيات،  التحليل البعدي،  التركيب البعدي،  العلوم البعدية/التلوية  أو البحث البعدي/التلوي؛  مراجعة السرد، مراجعة النطاق، خريطة منهجية، المراجعة المنهجية",
                "related_terms": [
                    "Literature Review",
                    "Meta-analysis",
                    "Meta-synthesis",
                    "Meta-science or Meta-research",
                    "Narrative review",
                    "Scoping review",
                    "Systematic map",
                    "Systematic review"
                ],
                "references": "for Evaluation, C. (n.d.). Evidence Synthesis. https://www.lshtm.ac.uk/research/centres/centre-evaluation/evidence-synthesis\n\nJames, K. L., Randall, N. P., & Haddaway, N. R. (2016). A methodology for systematic mapping in environmental sciences. Environmental Evidence, 5(1), 1–13. https://doi.org/10.1186/s13750-016-0059-6\n\nSiddaway, A. P., Wood, A. M., & Hedges, L. V. (2019). How to do a systematic review: a best practice guide for conducting and reporting narrative reviews, meta-analyses, and meta-syntheses. Annual Review of Psychology, 70, 747–770. https://doi.org/10.1146/annurev-psych-010418-102803",
                "drafted_by": [
                    "Marta Topor"
                ],
                "reviewed_by": [
                    "Aoife O’Mahony",
                    "Tamara Kalandadze",
                    "Adam Parker",
                    "Charlotte R. Pennington"
                ],
                "alt_related_terms": [
                    null
                ],
                "language": "arabic"
            },
            {
                "type": "glossary",
                "title": "تحليل البيانات الاستكشافيّ (Exploratory data analysis)",
                "definition": "هو تقليد إحصائي راسخ، يوفر أدوات مفاهيميَّة وحسابيَّة؛ لاستكشاف الأنماط في البيانات، وذلك لتعزيز تطوير الفرضيّات وتحسينها.  تُكمّل هذه الأدوات والتَّوجهات اختبار الفرضيَّات التي تستعمل في تحليل البيانات التّأكيدي، وحتى في سياق النَّظريات المحدَّدة والمعتبرة.  يساعد تحليل البيانات الاستكشافي في تفسير نتائج تحليل البيانات التَّأكيدي، و قد يكشف عن أنماط غير متوقّعة، أو مضّللة في البيانات.  **المصطلحات ذات الصِّلة:** التَّحليلات التَّوكيديَّة، البحث القائم على البيانات، البحث الاستكشافيّ.",
                "related_terms": [
                    "Confirmatory analyses",
                    "Data-driven research",
                    "Exploratory research"
                ],
                "references": "Behrens, J. T. (1997). Principles and procedures of exploratory data analysis. Psychological Methods, 2(2), 131–160. https://doi.org/10.1037/1082-989X.2.2.131\n\nBox, G. E. P. (1976). Science and statistics. Journal of the American Statistical Association, 71(356), 791–799.\n\nTukey, J. W. (1977). Exploratory data analysis. Addison-Wesley.\n\nWagenmakers, E. J., Wetzels, R., Borsboom, D., van der Maas, H. L., & Kievit, R. A. (2012). An agenda for purely confirmatory research. Perspectives on Psychological Science, 7(6), 632–638. https://doi.org/10.1177/1745691612463078",
                "drafted_by": [
                    "Jenny Terry"
                ],
                "reviewed_by": [
                    "Helena Hartmann",
                    "Timo Roettger",
                    "Charlotte R. Pennington",
                    "Flávio Azevedo"
                ],
                "alt_related_terms": [
                    null
                ],
                "language": "arabic"
            },
            {
                "type": "glossary",
                "title": "الصّدق الخارجي (External Validity)",
                "definition": "التعريف:** يشير إلى إمكانية تعميم نتائج دراسة علمية ما على سياقات أخرى خارج سياق الدراسة (مثلا مقاييس مختلفة، بيئات مختلفة، أشخاص مختلفين، بيئات مختلفة، أوقات مختلفة). من الناحية الإحصائية، قد يكون ما يهدد الصدق الخارجي تفاعلات معينة مثل أن يكون تأثير عامل معين (المتغير المستقل) معتمدا على عامل آخر (المتغير الدخيل). قد يكون الصدق الخارجي محدوداً بسبب تصميم الدراسة (ككونها في بيئة معملية، كون عينة الدراسة لا تمثل المجتمع).  **المصطلحات ذات العلاقة:**  قيود التعميم، الصدق الداخلي، القابلية للتعميم، التمثيل، الصدق **التعريف البديل:** في سياق علم القياس النفسي، فإن الصدق الخارجي هو درجة الأدلة التي تؤكد العلاقات بين بنية نفسية تم اختبارها والمتغيرات الخارجية. **المصطلحات المرتبطة بالتعريف البديل:** صدق المحك؛ الصلاحية التقاربية؛ الصلاحية الانفصالية",
                "related_terms": [
                    "Constraints on Generality (COG)",
                    "Internal validity",
                    "Generalizability",
                    "Representativity",
                    "Validity"
                ],
                "references": "Lynch, J. G., Jr. (1982). On the External Validity of Experiments in Consumer Research. Journal of Consumer Research, 9(3), 225. https://doi.org/10.1086/208919\n\nSteckler, A., & McLeroy, K. R. (2008). The Importance of External Validity. American Journal of Public Health, 98(1), 9–10. https://doi.org/10.2105/AJPH.2007.126847",
                "drafted_by": [
                    "Annalise A. LaPlume"
                ],
                "reviewed_by": [
                    "Mahmoud Elsherif",
                    "Helena Hartmann",
                    "Kai Krautter",
                    "Oscar Lecuona",
                    "Flávio Azevedo"
                ],
                "alt_related_terms": [
                    null
                ],
                "language": "arabic"
            },
            {
                "type": "glossary",
                "title": "الصِّدق الظاهري (Face validity)",
                "definition": "هو الحكم الشَّخصي على مدى ملاءمة مقياس معيّن بحسب ظاهره، وتحديداً، إلى أي مدى يمكن تقنينه. فعلى سبيل المثال، يشمل هذا الحكم على ما إذا كانت أسئلة الاستبيان في ظاهرها ترتبط بالظاهرة المراد دراستها.  يعدُّ الصِّدق الظَّاهري \\- وإن كان له علاقة بالصِّدق البنائي \\- شكلًا سهلًا وضعيفًا من أشكال الصِّدق لأنَّه حكم شخصي، وغير رسمي.  **المصطلحات ذات الصِّلة:** الصدق البنائي، صدق المحتوى، الصِّدق المنطقي، قابليَّة التَّنفيذ،  الصِّدق.",
                "related_terms": [
                    "Construct Validity",
                    "Content Validity",
                    "Logical Validity",
                    "Operationalization",
                    "Validity"
                ],
                "references": "Holden, R. B. (2010). Face Validity. In I. B. Weiner & W. E. Craighead (Eds.), The Corsini Encyclopedia of Psychology (4th ed.). Wiley. http://dx.doi.org/10.1002/9780470479216.corpsy0341",
                "drafted_by": [
                    "Annalise A. LaPlume"
                ],
                "reviewed_by": [
                    "Helena Hartmann",
                    "Kai Krautter",
                    "Adam Parker",
                    "Sam Parsons"
                ],
                "alt_related_terms": [
                    null
                ],
                "language": "arabic"
            },
            {
                "type": "glossary",
                "title": "مبادئ فير (FAIR principles)",
                "definition": "تعريف:** الاختصار \"فير\" مستمد من أربعة مبادئ تصف المواد العلميَّة على كونها يمكن العثور عليها، سهلة الوصول، قابلة للتَّكامل، ولإعادة الاستعمال.  يركز المبدآن الأوليان على أهميَّة المكان الذي تٌحفظ فيه المواد (مثل مستودعات البيانات)، بينما يركز المبدآن الأخيران على أهميَّة ترتيب وتنسيق البيانات، وكيفيَّة خضوع هذه التَّنسيقات للتَّغير مستقبلًا.  **المصطلحات ذات الصِّلة:** البيانات الوصفيَّة، الوصول المفتوح، النص البرمجي المفتوح، البيانات المفتوحة، المواد المفتوحة، مستودع البيانات.",
                "related_terms": [
                    "Metadata",
                    "Open Access",
                    "Open Code",
                    "Open Data",
                    "Open Material",
                    "Repository"
                ],
                "references": "Crüwell, S., van Doorn, J., Etz, A., Makel, M. C., Moshontz, H., Niebaum, J. C., Orben, A., Parsons, S., & Schulte-Mecklenbeck, M. (2019). Seven Easy Steps to Open Science: An Annotated Reading List. Zeitschrift Für Psychologie, 227(4), 237–248. https://doi.org/10.1027/2151-2604/a000387\n\nWilkinson, M. D., Dumontier, M., Aalbersberg, I. J., Appleton, G., Axton, M., Baak, A., & others. (2016). The FAIR Guiding Principles for scientific data management and stewardship. Scientific Data, 3(1), 1–9. https://doi.org/10.1038/sdata.2016.18",
                "drafted_by": [
                    "Sonia Rishi"
                ],
                "reviewed_by": [
                    "Mahmoud Elsherif",
                    "Sam Parsons",
                    "Charlotte R. Pennington"
                ],
                "alt_related_terms": [
                    null
                ],
                "language": "arabic"
            },
            {
                "type": "glossary",
                "title": "علم النَّفس النَّسوي (Feminist psychology)",
                "definition": "تعريف**: يهتم علم النَّفس النَّسوي بالتَّمثيل، والتَّنوع، والشُّموليَّة، والمساواة، ويركز على الجنسانيَّة والجندرية. نشأ علم النَّفس النَّسوي ليمثّل التَّجارب الحياتيَّة للفتيات والنِّساء، ولكنَّه تطوَّر منذ ذلك الحين إلى اهتمام أكثر دقَّة وتقاطعًا وشمولاً لجميع جوانب المساواة (Eagly & Riger, 2014). وقد دعا علماء النَّفس النَّسويون إلى النَّظر بشكل أكثر صرامة في المساواة، والتَّنوع، والشُّمول في مجالات العلوم المفتوحة (Pownall et al., 2021). **المصطلحات ذات الصِّلة**: الشُّمول، الموضعيَّة، الانعكاسيَّة، نقص التَّمثيل، العدالة.",
                "related_terms": [
                    "Inclusion",
                    "Positionality",
                    "Reflexivity",
                    "Under-representation",
                    "Equity"
                ],
                "references": "Eagly, A. H., & Riger, S. (2014). Feminism and psychology: Critiques of methods and epistemology. American Psychologist, 69(7), 685–702. https://doi.org/10.1037/a0037372\n\nGrzanka, P. R. (2020). From buzzword to critical psychology: An invitation to take intersectionality seriously. Women & Therapy, 43(3–4), 244–261.\n\nPownall, M., Talbot, C. V., Henschel, A., Lautarescu, A., Lloyd, K. E., Hartmann, H., Darda, K. M., Tang, K. T. Y., Carmichael-Murphy, P., & Siegel, J. A. (2021). Navigating Open Science as Early Career Feminist Researchers. Psychology of Women Quarterly, 45(4), 526–539. https://doi.org/10.1177/03616843211029255 Retrieved from https://journals.sagepub.com/doi/10.1177/03616843211029255",
                "drafted_by": [
                    "Madeleine Pownall"
                ],
                "reviewed_by": [
                    "Mahmoud Elsherif",
                    "Helena Hartmann",
                    "Kai Krautter",
                    "Charlotte R. Pennington"
                ],
                "alt_related_terms": [
                    null
                ],
                "language": "arabic"
            },
            {
                "type": "glossary",
                "title": "FLAE مبدأ التَّأكيد على المؤلفَين الأول والأخير (First-last-author-emphasis norm (FLAE))",
                "definition": "يحدِّد هذا المبدأ ترتيب المؤلِّفين بناءً على مساهمات كل مؤلِّف، ولكن مع تقدير أكبر للمؤلّفَين الأول والأخير. فحسب هذا المبدأ، يكون المؤلفان الرئيسيان هما الأول والأخير، بينما يُحدّد ترتيب باقي المؤلفِين الواقعين بينهما بحسب إسهاماتهم في التَّأليف.  **المصطلحات ذات اِّلصلة:** التأليف،  مساهمات المؤلِّف، تصنيف أدوار المؤلِّفين",
                "related_terms": [
                    "Authorship",
                    "Author contributions",
                    "CreDit taxonomy"
                ],
                "references": "Tscharntke, T., Hochberg, M. E., Rand, T. A., Resh, V. H., & Krauss, J. (2007). Author sequence and credit for contributions in multiauthored publications. PLoS Biology, 5(1), e18. https://doi.org/10.1371/journal.pbio.0050018",
                "drafted_by": [
                    "Myriam A. Baum"
                ],
                "reviewed_by": [
                    "Charlotte R. Pennington"
                ],
                "alt_related_terms": [
                    null
                ],
                "language": "arabic"
            },
            {
                "type": "glossary",
                "title": "فورت (FORRT)",
                "definition": "اختصار لإطار البحث، والتَّدريس المفتوح القابل للتِّكرار الذي يهدف إلى توفير بنية تحتية تربويَّة مصمَّمة للتَّعرُّف على البحوث المفتوحة، والقابلة للتِّكرار ودعم تدريسها وتوجيهها، جنبًا إلى جنب مع الموضوعات المتعارف عليها في التَّعليم العالي.  تسعى فورت إلى أن تكون منظَّمة فعَّالة، ومتطوِّرة ومجتمعيَّة تزيد الوعي بالآثار التَّربوية للعلوم المفتوحة والقابلة للتِّكرار والتَّحديّات المرتبطة بها (أي إصلاح المناهج الدِّراسيَّة، واللاليقين المعرفي، وأساليب التَّعليم).  تدعو فورت أيضًا إلى إتاحة مواد التَّدريس، والتّوجيه كوسيلة لتسهيل الوصول، والاكتشاف والتَّعلم للجميع بما فيهم من لا يستطيع  الوصول للمعرفة أو التَّعلم.  **مصطلحات ذات صلة:** دمج مبادئ العلوم المفتوحة والقابلة للتكرار في التعليم العالي",
                "related_terms": [
                    "Integrating open and reproducible science tenets into higher education"
                ],
                "references": "",
                "drafted_by": [
                    "Tamara Kalandadze"
                ],
                "reviewed_by": [
                    "Mahmoud Elsherif",
                    "Charlotte R. Pennington",
                    "Flávio Azevedo"
                ],
                "alt_related_terms": [
                    null
                ],
                "language": "arabic"
            },
            {
                "type": "glossary",
                "title": "منصَّة حرِّر معرفتنا (Free Our Knowledge Platform)",
                "definition": "منصَّة عمل جماعيَّة تهدف إلى دعم حركة العلوم المفتوحة من خلال الحصول على تعهُّدات من الباحثين بتنفيذ ممارسات بحثيَّة معيَّنة،مثل: التَّسجيل المسبق، والطِّباعة الأوليَّة).  تكون التَّعهدات في البداية مجهولة المصدر حتى يتعهَّد عدد كاف من الأشخاص، وعندها يتم إعلان أسماء المتعهدين.  وهذه المبادرة عبارة عن حركة بحثيَّة أنشأها مجموعة من الباحثين في بداية حياتهم المهنيَّة.  **المصطلحات ذات الصِّلة**: العلم المفتوح، تعهد التَّسجيل المسبق.",
                "related_terms": [
                    "Open Science",
                    "Preregistration Pledge"
                ],
                "references": "Free Our Knowledge. (n.d.). About. Retrieved from https://freeourknowledge.org/about/",
                "drafted_by": [
                    "Jamie P. Cockcroft"
                ],
                "reviewed_by": [
                    "Ashley Blake",
                    "Elizabeth Collins",
                    "Mahmoud Elsherif",
                    "Sam Parsons",
                    "Flávio Azevedo"
                ],
                "alt_related_terms": [
                    null
                ],
                "language": "arabic"
            },
            {
                "type": "glossary",
                "title": "برنامج قوة جي (GPower)",
                "definition": "برنامج إحصائي مجاني لإجراء تحليلات القوة. يحدد المستخدم الاختبار الإحصائي المطلوب (مثل اختبار تي ، الانحدار، تحليل التباين (أنوفا)، وثلاثة مما يلي: عدد المجموعات/الملاحظات، أو حجم التأثير، أو مستوى الأهمية، أو القوة، من أجل حساب الجانب غير المحدد.  **المصطلحات ذات الصِّلة:** تحليل القوة، تبرير حجم العينة، تخطيط حجم العينة، القوة الإحصائيَّة.",
                "related_terms": [
                    "Power analysis",
                    "Sample size justification",
                    "Sample size planning",
                    "Statistical power"
                ],
                "references": "Faul, F., Erdfelder, E., Lang, A.-G., & Buchner, A. (2007). G*Power 3: A flexible statistical power analysis program for the social, behavioral, and biomedical sciences. Behavior Research Methods, 39, 175–191. https://doi.org/10.3758/BF03193146\n\nFaul, F., Erdfelder, E., Buchner, A., & Lang, A.-G. (2009). Statistical power analyses using G*Power 3.1: Tests for correlation and regression analyses. Behavior Research Methods, 41, 1149–1160. https://doi.org/10.3758/BRM.41.4.1149",
                "drafted_by": [
                    "Filip Dechterenko"
                ],
                "reviewed_by": [
                    "Thomas Rhys Evans",
                    "Kai Krautter",
                    "Charlotte R. Pennington"
                ],
                "alt_related_terms": [
                    null
                ],
                "language": "arabic"
            },
            {
                "type": "glossary",
                "title": "the system التَّلاعب بالنِّظام (Gaming (the system))",
                "definition": "التَّعريف: اتباع ممارسات بحثيَّة مشبوهة كالتَّجزئة غير المبررة لورقة أكاديميَّة) والتي من شأنها أن تتماشى مع أنظمة الحوافز الأكاديميَّة التي تفيد الباحث، مثلًا: في المكانة، أو التَّوظيف، أو التَّرقية، بغض النَّظر عمَّا إذا كانت تدعم المعرفة. إذا كانت الأنظمة تعتمد على المقاييس لتحديد نتيجة ما (مثل السُّمعة الأكاديميَّة)، فقد تتعرَّض هذه المقاييس للتَّلاعب المتعمَّد (Naudet et al., 2018). وعندما تستند التَّرقيات، والتَّوظيف، والتَّثبيت الوظيفي إلى مقاييس معيبة، فقد لا يحبذ البعض الانفتاح، والصَّرامة، والعمل الشَّفاف (Naudet et al., 2018) - وذلك كتفضيل “البحوث الكميَّة على النوعية” - وتؤدي إلى تفاقم أوجه عدم المساواة القائمة. المصطلحات ذات الصِّلة: هيكل الحوافز، معامل تأثير المجلَّة، قرصنة القيمة الاحتماليَّة.",
                "related_terms": [
                    "Incentive structure",
                    "Journal Impact Factor",
                    "*P*\\-hacking"
                ],
                "references": "Moher, D., Naudet, F., Cristea, I. A., Miedema, F., Ioannidis, J. P. A., & Goodman, S. N. (2018). Assessing scientists for hiring, promotion, and tenure. PLOS Biology, 16(3), e2004089. https://doi.org/10.1371/journal.pbio.2004089\n\nNaudet, F., Ioannidis, J., Miedema, F., Cristea, I. A., Goodman, S. N., & Moher, D. (2018). Six principles for assessing scientists for hiring, promotion, and tenure. Impact of Social Sciences Blog.",
                "drafted_by": [
                    "Adrien Fillon"
                ],
                "reviewed_by": [
                    "Sarah Ashcroft-Jones",
                    "Helena Hartmann",
                    "Sam Parsons",
                    "Charlotte R. Pennington"
                ],
                "alt_related_terms": [
                    null
                ],
                "language": "arabic"
            },
            {
                "type": "glossary",
                "title": "حديقة المسارات المتشعبة (Garden of forking paths)",
                "definition": "شجرة اتخاذ القرار غير المرئيَّة التي يتم اجتيازها عادة أثناء التَّشغيل، والتَّحليل الإحصائي بالنَّظر إلى أن \"هناك طرق متعدِّدة للانتقال من الفرضيَّات العلميَّة إلى الفرضيَّات الإحصائيَّة\" (Gelman & Loken, 2013, p. 6). وبعبارة أخرى، حتى في حالة عدم وجود عمليَّات القرصنة للقيمة الاحتماليَّة، أو التَّصيُّد، وعندما يتم طرح فرضيَّة البحث مسبقًا، يمكن أن يكون هناك عدد كبير من النَّتائج الإحصائيَّة التي يمكن أن تبدو مدعومة بالبيانات النَّظريَّة المعطاة. \"تكمن المشكلة في أنَّه يمكن أن يكون هناك عدد كبير من المقارنات المحتملة عندما تكون تفاصيل التَّحليل معتمدة بشكل كبير على البيانات، دون أن يضطر الباحث إلى اتِّباع أي تصيُّد بشكل واع، أو فحص لقيم احتماليَّة متعدِّدة\" (Gelman & Loken, 2013, p. 1). يهدف المصطلح إلى تسليط الضَّوء على حالة عدم اليقين النَّاتجة عن الخيارات التَّحليليَّة والإحصائيَّة الشَّخصيَّة في الانتقال من النَّظريَّة لاختبارها، والتَّمييز بين الممارسات البحثيَّة المتعمدة (وغير الأخلاقيَّة) المشبوهة (مثل عمليات قرصنة القيمة الاحتماليَّة، والتَّصيُّد) وبين ممارسات البحث غير المقصودة التي من المحتمل أن يكون لها نفس التَّأثير على الرغم من عدم وجود نيّة لإفساد النَّتائج. تشير حديقة المسارات المتشعِّبة إلى القرارات أثناء العمليَّة العلميَّة التي تضخم معدَّل الإيجابيَّة الكاذبة كنتيجة للمسارات المحتملة التي كان من الممكن اتخاذها (في حالة اتخاذ قرارات أخرى). **المصطلحات ذات الصِّلة:** الإيجابيَّة الكاذبة، الخطأ العائلي، تحليل الأكوان المتعدِّدة، التَّسجيل المسبق، درجات حريَّة الباحث، تحليل منحنى المواصفات",
                "related_terms": [
                    "False-positive",
                    "Familywise error",
                    "Multiverse Analysis",
                    "Preregistration",
                    "Researcher degrees of freedom",
                    "Specification Curve Analysis"
                ],
                "references": "Gelman, A., & Loken, E. (n.d.). The garden of forking paths: Why multiple comparisons can be a problem, even when there is no “fishing expedition” or “p-hacking” and the research hypothesis was posited ahead of time. Retrieved from http://www.stat.columbia.edu/",
                "drafted_by": [
                    "Flávio Azevedo; Mahmoud Elsherif"
                ],
                "reviewed_by": [
                    "Gisela H. Govaart",
                    "Matt Jaquiery",
                    "Tamara Kalandadze",
                    "Charlotte R. Pennington"
                ],
                "alt_related_terms": [
                    null
                ],
                "language": "arabic"
            },
            {
                "type": "glossary",
                "title": "GDPR لائحة حماية البيانات العامَّة (General Data Protection Regulation (GDPR))",
                "definition": "إطار عمل قانوني مٌطبق في الاتحاد الأوروبيّ، ويتضمَّن سبعة مبادئ هدفها حماية بيانات الأفراد.  يسعى هذا الإطار إلى تمكين الأفراد من التحَّكم ببياناتهم الشَّخصيَّة وتنظيم عمل الجهات التي تعمل في مجال تحليل وحفظ هذه البيانات. كما تضبط هذه التَّشريعات حريَّة نقل البيانات الشَّخصيَّة للأفراد داخل وخارج الاتحاد الأوروبي والتي يجب على الباحثين الالتزام بها عند تصميم وإجراء الدِّراسات. **المصطلحات ذات الصِّلة:** التّعمية ، خِطة إدارة البيانات، مشاركة البيانات، التكرار، قابليَّة التِّكرار، ،قابلية إعادة الإنتاج.",
                "related_terms": [
                    "Anonymity",
                    "Data Management Plan (DMP)",
                    "Data sharing",
                    "Repeatability",
                    "Replicability",
                    "Reproducibility"
                ],
                "references": "Crutzen, R., Ygram Peters, G. J., & Mondschein, C. (2019). Why and how we should care about the General Data Protection Regulation. Psychology & Health, 34(11), 1347–1357. https://doi.org/10.1080/08870446.2019.1606222\n\nInformation Commissioner’s Office. (2021). Guide to the UK General Data Protection Regulation (UK GDPR). ICO. https://ico.org.uk/for-organisations/guide-to-data-protection/guide-to-the-general-data-protection-regulation-gdpr/",
                "drafted_by": [
                    "Graham Reid"
                ],
                "reviewed_by": [
                    "Elizabeth Collins",
                    "Mahmoud Elsherif",
                    "Christopher Graham",
                    "Sam Parsons"
                ],
                "alt_related_terms": [
                    null
                ],
                "language": "arabic"
            },
            {
                "type": "glossary",
                "title": "القابليَّة للتَّعميم (Generalizability)",
                "definition": "هي القدرة على تعميم نتائج دراسة معيّنة على عدد أكبر من المجموعات، أو الظروف، أو المواقف مقارنة بتلك التي شملتها الدِّراسة، وكذلك كيفيَّة ربط هذه النَّتائج بسياق أوسع (Frey, 2018; Kukull & Ganguli, 2012).  **المصطلحات ذات الصِّلة**: التِّكرار المفاهيمي، الصِّدق الخارجي، جمع العيّنات الانتهازي، تحيُّز اختيار العَينة، وِيرد (WEIRD).  **تعريف بديل:** استخدام المواد المعدَّلة، أو تطبيق سلسلة تحليلات لبيانات وعيّنات جديدة  لدراسة الفرضيَّة ذاتها (بمواد مختلفة، بيانات مختلفة) لاختبار مدى قابليَّة تعميم النَّتائج تحت الدِّراسة (The Turing Way Community & Scriberia, 2021).  المصطلحات ذات الصِّلة للتَّعريف البديل: التِّكرار المفاهيميّ.",
                "related_terms": [
                    "Conceptual replication",
                    "External Validity",
                    "Opportunistic sampling",
                    "Sampling bias",
                    "WEIRD **Alternative definition:** Applying modified materials and/or analysis pipelines to new data or samples to answer the same hypothesis (different materials, different data) to test how generalizable the effect under study is (The Turing Way Community & Scriberia, 2021). **Related terms to alternative definition:** (if applicable): Conceptual Replication"
                ],
                "references": "Esterling, K., Brady, D., & Schwitzgebel, E. (2021). The Necessity of Construct and External Validity for Generalized Causal Claims. Retrieved from https://doi.org/10.31219/osf.io/2s8w5.\n\nGeneralizability. (2018). Generalizability. In B. B. Frey (Ed.), The SAGE Encyclopedia of Educational Research, Measurement, and Evaluation. SAGE Publications, Inc. https://doi.org/10.4135/9781506326139.n284\n\nKukull, W. A., & Ganguli, M. (2012). Generalizability: The trees, the forest, and the low-hanging fruit. Neurology, 78(23), 1886–1891. https://doi.org/10.1212/WNL.0b013e318258f812\n\nLeBel, E. P., Vanpaemel, W., Cheung, I., & Campbell, L. (2017). A brief guide to evaluate replications. Meta-Psychology, 3. https://doi.org/10.15626/MP.2018.843\n\nNosek, B. A., & Errington, T. M. (2020). What is replication? PLOS Biology, 18(3), e3000691. https://doi.org/10.1371/journal.pbio.3000691\n\nYarkoni, T. (2020). The generalizability crisis. Behavioral and Brain Sciences, 1–37. https://doi.org/10.1017/S0140525X20001685",
                "drafted_by": [
                    "Aoife O’Mahony"
                ],
                "reviewed_by": [
                    "Adrien Fillon",
                    "Matt Jaquiery",
                    "Tina Lonsdorf",
                    "Sam Parsons",
                    "Julia Wolska"
                ],
                "alt_related_terms": [
                    null
                ],
                "language": "arabic"
            },
            {
                "type": "glossary",
                "title": "or Guest Authorship التَّأليف المُهدى أو المؤلف الضّيف (Gift (or Guest) Authorship)",
                "definition": "هي حزمة برامج لتتبع التَّغييرات في مجموعة محليَّة من الملفات (التَّحكُّم في الإصدار المحليّ)، طورها في البداية لينوس تورفالدس. وبشكل عام يستخدمه المبرمجون لتتبع وتطوير النُّصوص البرمجيَّة داخل دليل، أو مجلَّد، أو نظام ملفات.  يمكن للمتتبِّع الدُّولي العالميّ الوصول (Git) إلى خدمات استضافة المستودعات عن بُعد (مثل GitHub) للتّحكم في الإصدار عن بُعد الذي يتيح تطوير البرامج التَّعاونيَّة عن طريق تحميل المساهمات من نظام محلي. وجدت هذه العمليَّة طريقها إلى العمليَّة العلميَّة لتمكين البيانات المفتوحة، والنُّصوص البرمجيَّة المفتوحة، والتَّحليلات القابلة للتِّكرار. **المصطلحات ذات الصِّلة:**  Github،مستودع البيانات، التحكم في الإصدار.",
                "related_terms": [
                    "Authorship",
                    "CRediT"
                ],
                "references": "Bhopal, R., Rankin, J., McColl, E., Thomas, L., Kaner, E., Stacy, R., Pearson, P., Vernon, B., & Rodgers, H. (1997). The vexed question of authorship: views of researchers in a British medical faculty. BMJ, 314, 1009–1012. https://doi.org/10.1136/bmj.314.7086.1009\n\nof Medical Journal Editors, I. C. (2019). Recommendations for the conduct, reporting, eduting, and publication of scholarly work in medical journals. http://www.icmje.org/icmje-recommendations.pdf",
                "drafted_by": [
                    "Bradley Baker"
                ],
                "reviewed_by": [
                    "Helena Hartmann",
                    "Aoife O’Mahony",
                    "Sam Parsons",
                    "Charlotte R. Pennington",
                    "Flávio Azevedo"
                ],
                "alt_related_terms": [
                    null
                ],
                "language": "arabic"
            },
            {
                "type": "glossary",
                "title": "Git",
                "definition": "",
                "related_terms": [
                    "GitHub",
                    "Repository",
                    "Version control"
                ],
                "references": "Kalliamvakou, E., Gousios, G., Blincoe, K., Singer, L., German, D. M., & Damian, D. (2014). The promises and perils of mining github. Proceedings of the 11th Working Conference on Mining Software Repositories, 92–101.\n\nScopatz, A. M., & Huff, K. D. (2015). Effective Computation in Physics: Field Guide to Research with Python (1st ed.). O’Reilly Media. http://shop.oreilly.com/product/0636920033424.do\n\nVuorre, M., & Curley, J. P. (2018). Curating research assets: A tutorial on the Git version control system. Advances in Methods and Practices in Psychological Science, 1(2), 219–236. https://doi.org/10.1177/2515245918754826\n\ngit/git. (n.d.). Initial revision of ‘git’, the information manager from hell. GitHub. https://github.com/git/git/commit/e83c5163316f89bfbde7d9ab23ca2e25604af290",
                "drafted_by": [
                    "Emma Norris"
                ],
                "reviewed_by": [
                    "Adrien Fillon",
                    "Bettina M.J. Kern",
                    "Dominik Kiersz",
                    "Robert M. Ross"
                ],
                "alt_related_terms": [
                    null
                ],
                "language": "arabic"
            },
            {
                "type": "glossary",
                "title": "قانون قودهارت (Goodhart’s Law)",
                "definition": "مصطلح صاغه خبير الاقتصاد تشارلز قودهارت للإشارة إلى أنَّ قياس شيء ما يؤدي بطبيعته إلى تغيُّر سلوك المستخدِّم. فعندما يتعلَّق الأمر بتقييم الأداء \"عندما يصبح المقياس هو الهدف فإنه يتوقَّف عن كونه مقياسًا جيِّدًا\" (Strathern, 1997, p. 308). عند تطبيقه على العلم المفتوح وهيكل الحوافز في الأوساط الأكاديميَّة، يتنبأ قانون قودهارت بأنَّه من المرجَّح أن يتم إساءة استخدام مقاييس التَّقييم العلمي واستغلاله (Muller, 2019).  المصطلحات ذات الصِّلة: قانون كامبل، إعلان سان فرانسيسكو بشأن تقييم البحوث، التَّجسيد (المغالطة).",
                "related_terms": [
                    "Campbell's law",
                    "DORA",
                    "Reification (fallacy) **Reference (s):** \\[@Muller2018\\], \\[@Strathern1997\\]"
                ],
                "references": "",
                "drafted_by": [
                    "Adam Parker"
                ],
                "reviewed_by": [
                    "Sam Parsons",
                    "Flávio Azevedo"
                ],
                "alt_related_terms": [
                    null
                ],
                "language": "arabic"
            },
            {
                "type": "glossary",
                "title": "مؤشر إتش (H-index)",
                "definition": "يهدف مؤشر هيرش (ويختصر اسمه بمؤشر إتش) إلى قياس كل من الإنتاجيَّة وتأثير البحث من خلال الجمع بين عدد المنشورات، وعدد الاستشهادات لهذه المنشورات. يعرّف هذا المؤشر بأنَّه: عدد الأوراق التي تحمل عدد استشهادات ≥ إتش\" (Hirsch, 2005, p. 16569). وهذا يعني أكبر عدد من المنشورات لمؤلف، أو لمجلة تم الاستشهاد بها على الأقل نفس العدد من المرات.  يتفوَّق هذا المؤشر على المقاييس التي تقيم فقط، على سبيل المثال، عدد الاستشهادات وعدد المنشورات ولكن تم انتقاده عندما يستخدم في تقييم باحث ما (مثلًا Wendl, 2007).  **المصطلحات ذات الصِّلة:** الاستشهاد، إعلان سان فرانسيسكو بشأن تقييم البحوث، مؤشر i10، التَّأثير.",
                "related_terms": [
                    "Citation",
                    "DORA",
                    "I10-index",
                    "Impact"
                ],
                "references": "Hirsch, J. E. (2005). An index to quantify an individual’s scientific research output. Proceedings of the National Academy of Sciences, 102(46), 16569–16572. https://doi.org/10.1073/pnas.0507655102\n\nWendl, M. C. (2007). H-index: however ranked, citations need context. Nature, 449(7161), 403–403. https://doi.org/10.1038/449403b",
                "drafted_by": [
                    "Jacob Miranda"
                ],
                "reviewed_by": [
                    "Bradley J. Baker",
                    "Mahmoud M. Elsherif",
                    "Brett J. Gall",
                    "Charlotte R. Pennington"
                ],
                "alt_related_terms": [
                    null
                ],
                "language": "arabic"
            },
            {
                "type": "glossary",
                "title": "هاكثون (Hackathon)",
                "definition": "هو حدث منظَّم يتعاون فيه الخبراء، أو المصمِّمون، أو الباحثون لفترة زمنيَّة قصيرة نسبيًا للعمل بشكل مكثَّف على مشروع، أو مشكلة ما.  تم استعارة هذا المصطلح في الأصل من برمجة الحاسب ومسابقات تطوير البرامج التي تهدف إلى الوصول لمنتج كامل (مثل موارد، بحث، برنامج، جهاز) بحلول نهاية المسابقة، والتي يمكن أن تستمر من عدَّة ساعات إلى عدَّة أيام. **المصطلحات ذات الصِّلة:** التَّعاون؛ إديهاتون",
                "related_terms": [
                    "Collaboration",
                    "Edithaton"
                ],
                "references": "Kienzler, H., & Fontanesi, C. (2017). Learning through inquiry: A global health hackathon. Teaching in Higher Education, 22(2), 129–142. https://doi.org/10.1080/13562517.2016.1221805",
                "drafted_by": [
                    "Flávio Azevedo"
                ],
                "reviewed_by": [
                    "Tsvetomira Dumbalska",
                    "Brett J. Gall",
                    "Emma Norris"
                ],
                "alt_related_terms": [
                    null
                ],
                "language": "arabic"
            },
            {
                "type": "glossary",
                "title": "الافتراض بعد معرفة النَّتائج (HARKing)",
                "definition": "ممارسة بحثيَّة مشبوهة \"تعرّف بتقديم فرضيَّة لاحقة (أي فرضيَّة مبنيَّة على النَّتائج، أو مستنيرة بها) في تقرير بحثي كما لو كانت في الواقع بديهية\" (Kerr, 1998, p. 196). مثال ذلك إجراء تحليلات لمجموعات فرعيَّة، وإيجاد تأثير في مجموعة فرعيَّة واحدة، وكتابة المقدمة مع \"فرضيّة\" تطابق هذه النَّتائج. **المصطلح البديل:** الافتراض التكيفي **المصطلحات ذات الصِّلة:**  المرونة التحليليَّة، النَّقد بعد معرفة النَّتائج، التحليلات التوكيديَّة، تحليل البيانات الاستكشافي، التَّنصُّل، حديقة المسارات المتشعِّبة، قرصنة القيمة الاحتماليَّة، التَّسجيل بعد معرفة النَّتائج، ممارسات البحث المشكوك فيها أو ممارسات إعداد التقارير المشكوك فيها، تخطيط حجم العيِّنة بعد معرفة النَّتائج. **Alternative terms**: accommodational hypothesizing",
                "related_terms": [
                    "Analytic Flexibility",
                    "Confirmatory analyses",
                    "Exploratory data analysis",
                    "Fudging",
                    "Garden of forking paths",
                    "P-hacking",
                    "Questionable Research Practices or Questionable Reporting Practices (QRPs)"
                ],
                "references": "Kerr, N. L. (1998). HARKing: Hypothesizing after the results are known. Personality and Social Psychology Review, 2(3), 196–217. https://doi.org/10.1207/s15327957pspr0203_4\n\nNosek, B. A., & Lakens, D. (2014). Registered reports. Social Psychology, 45, 137–141. https://doi.org/10.1027/1864-9335/a000192",
                "drafted_by": [
                    "Beatrix Arendt"
                ],
                "reviewed_by": [
                    "Matt Jaquiery",
                    "Charlotte R. Pennington",
                    "Martin Vasilev",
                    "Flávio Azevedo"
                ],
                "alt_related_terms": [
                    null
                ],
                "language": "arabic"
            },
            {
                "type": "glossary",
                "title": "المتغيِّرات الوسيطة الخفيَّة (Hidden Moderators)",
                "definition": "الظُّروف السياقيَّة التي يمكن دون علم الباحثين أن تجعل نتائج محاولة التِّكرار تحيد عن نتائج الدِّراسة الأصليَّة. يتم أحيانًا اللُّجوء للمتغيِّرات الوسيطة الخفيَّة؛ لتبرير فشل تكرار النَّتائج. وتسمى أيضًا الافتراضات الخفيَّة.  **المصطلحات ذات الصِّلة:** الفرضيَّة المساعدة.",
                "related_terms": [
                    "Auxiliary Hypothesis"
                ],
                "references": "Zwaan, R., Etz, A., Lucas, R., & Donnellan, M. (2018). Making replication mainstream. Behavioral and Brain Sciences, 41, E120. https://doi.org/10.1017/S0140525X17001972",
                "drafted_by": [
                    "Ali H. Al-Hoorie"
                ],
                "reviewed_by": [
                    "Mahmoud Elsherif",
                    "Sam Parsons"
                ],
                "alt_related_terms": [
                    null
                ],
                "language": "arabic"
            },
            {
                "type": "glossary",
                "title": "الفرضيَّة (Hypothesis)",
                "definition": "الفرضيَّة عبارة عن ادِّعاء غير مثبت يتعلَّق بالعلاقة بين المتغيّرات (Glass & Hal, 2008\\) ويمكن أن تستند إلى تجارب سابقة، أو معرفة علميَّة، أو ملاحظات أوليَّة، أو نظريَّة أو منطق.  في الاختبارات العلميَّة، يمكن عادةً صياغة الفرضيَّة مع تحديد الاتجاه، مثلًا ارتباط إيجابي، أو بدون اتجاه، مثلًا سيكون هناك ارتباط ما. يذهب بعض الفلاسفة (Popper, 1959\\) إلى أن الفرضيَّات يجب أن تكون قابلة للدَّحض، أي أنَّه يجب أن يكون من الممكن إثبات خطأ الفرضيَّة. ومع ذلك، فقد قيل إن اختبار الفرضية القائم على التفنيد غامض؛ لأنَّه يعتمد على العديد من الافتراضات الأخرى غير المختبرة في الفرضيَّة (أي الفرضيَّات المساعدة). كما حاجج البعض (Longino, 1990, 1992\\) بأنَّ عدم التَّجانس الوجودي يجب أن يتم تقييمه أكثر من البساطة الوجوديَّة للعلوم البيولوجيَّة، والذي يرى أنَّه يجب علينا دراسة الاختلافات بين الكائنات البيولوجيَّة وداخلها. **المصطلحات ذات الصِّلة:** الفرضيَّة المساعدة، التحليلات التوكيديَّة، نتائج سلبيَّة كاذبة، نتائج إيجابيَّة كاذبة، النَّمذجة، التَّنبؤات، البحث الكميّ، النَّظريَّة، بناء النَّظريَّة، الخطأ من النَّوع الأوّل، الخطأ من النَّوع الثَّاني.",
                "related_terms": [
                    "Auxiliary Hypothesis",
                    "Confirmatory analyses",
                    "False negative result",
                    "False positive result",
                    "Modelling",
                    "Predictions",
                    "Quantitative research",
                    "Theory",
                    "Theory building",
                    "Type I error",
                    "Type II error"
                ],
                "references": "Beller, S., & Bender, A. (2017). Theory, the final frontier? A corpus-based analysis of the role of theory in psychological articles. Frontiers in Psychology, 8, 951. https://doi.org/10.3389/fpsyg.2017.00951\n\nGlass, D. J., & Hall, N. (2008). A brief history of the hypothesis. Cell, 134(3), 378–381. https://doi.org/10.1016/j.cell.2008.07.033\n\nLongino, H. E. (1990). Science as Social Knowledge: Values and Objectivity in Scientific Inquiry. Princeton University Press.\n\nLongino, H. E. (1992). Taking gender seriously in philosophy of science. PSA, 2, 333–340.\n\nPopper, K. (1959). The logic of scientific discovery. Routledge.",
                "drafted_by": [
                    "Ana Barbosa Mendes"
                ],
                "reviewed_by": [
                    "Ali H. Al-Hoorie",
                    "Mahmoud Elsherif",
                    "Helena Hartmann",
                    "Charlotte R. Pennington**;** Graham Reid",
                    "Olly Robertson"
                ],
                "alt_related_terms": [
                    null
                ],
                "language": "arabic"
            },
            {
                "type": "glossary",
                "title": "مؤشر i10 (i10-index)",
                "definition": "ا**لتَّعريف: مؤشر أنشأه الباحث العلمي من قوقل، ويمثل عدد المنشورات لباحث ما التي حصلت على 10 استشهادات على الأقل.  المصطلحات ذات الصِّلة: الاستشهاد، إعلان سان فرانسيسكو بشأن تقييم البحوث، مؤشر إتش، التَّأثير.",
                "related_terms": [
                    "Citation",
                    "DORA",
                    "H-index",
                    "Impact"
                ],
                "references": "University, C. (2020). Measuring your research impact: i10 index. Cornell University Library. https://guides.library.cornell.edu/impact/author-impact-10",
                "drafted_by": [
                    "Emma Norris"
                ],
                "reviewed_by": [
                    "Flávio Azevedo",
                    "Sam Parsons"
                ],
                "alt_related_terms": [
                    null
                ],
                "language": "arabic"
            },
            {
                "type": "glossary",
                "title": "التَّحيُّز الأيديولوجي (Ideological bias)",
                "definition": "يمكن أن تعتمد الآراء المسبقة حول جودة البحث على وجهات النَّظر الأيديولوجيَّة للمؤلف (المؤلفين) باعتبارها واحدة من التَّحيُّزات العديدة في عملية تحكيم الأقران.  يُتوقع أن تكون الآراء الإيجابيَّة تجاه البحث أكثر احتماليَّة إذا اتفق الأصدقاء، أو المتعاونون، أو العلماء مع وجهات النَّظر السِّياسيَّة للمحرِّر، أو المراجع (Tvina et al., 2019). وقد يؤدي ذلك إلى مجموعة متنوعة من تضارب المصالح التي تقلِّل من التَّنوع في وجهات النَّظر، على سبيل المثال تسريع أو تأخير تحكيم الأقران، أو التَّأثير على فرص دعوة الفرد لتقديم بحوثه، وبالتَّالي التَّرويج لعمله.  **المصطلحات ذات الصِّلة:** تحيُّز الشَّخصنة، تحكيم الأقران.",
                "related_terms": [
                    "Ad hominem bias",
                    "Peer review"
                ],
                "references": "Tvina, A., Spellecy, R., & Palatnik, A. (2019). Bias in the peer review process: can we do better? Obstetrics & Gynecology, 133(6), 1081–1083. https://doi.org/10.1097/AOG.0000000000003260",
                "drafted_by": [
                    "Mahmoud Elsherif"
                ],
                "reviewed_by": [
                    "Elizabeth Collins",
                    "Flávio Azevedo",
                    "Madeleine Ingham",
                    "Sam Parsons",
                    "Graham Reid"
                ],
                "alt_related_terms": [
                    null
                ],
                "language": "arabic"
            },
            {
                "type": "glossary",
                "title": "الشُّمول (Inclusion)",
                "definition": "يقصد بالشَّمول أو الشُّمولية: الشُّعور بالتَّرحاب، والاحترام داخل مشروع، أو بيئة تعاونيَّة معيَّنة (مثل الأوساط الأكاديميَّة) حيث يشير *التَّنوع* ببساطة إلى وجود مجموعة واسعة من الخلفيَّات، ووجهات النَّظر والخبرات.  كما تسعى الجهود المبذولة  لزيادة *الشُّموليَّة* إلى تعزيز المشاركة، والتَّقييم المتساوي بين مختلف الأفراد الذين قد يتعرضون للتَّهميش.  غالبًا ما تتضمَّن زيادة الشُّمولية تقليل تأثير، أو حتى إزالة الحواجز النِّظاميَّة التي تحول دون إمكانيَّة الوصول والمشاركة. **المصطلحات ذات الصِّلة:** التَّنوع، العدالة، العدالة الاجتماعيَّة.",
                "related_terms": [
                    "Diversity",
                    "Equity",
                    "Social Justice"
                ],
                "references": "Calvert, D. (2019). How to Make Inclusivity More Than Just an Office Buzzword. Retrieved from https://insight.kellogg.northwestern.edu/article/how-to-make-inclusivity-more-than-just-an-office-buzzword\n\nMartinez-Acosta, V. G., & Favero, C. B. (2018). A discussion of diversity and inclusivity at the institutional level: The need for a strategic plan. Journal of Undergraduate Neuroscience Education, 16(3), A252.",
                "drafted_by": [
                    "Ryan Millager"
                ],
                "reviewed_by": [
                    "Mahmoud Elsherif",
                    "Graham Reid",
                    "Kai Krautter",
                    "Suzanne L. K. Stewart",
                    "Flávio Azevedo"
                ],
                "alt_related_terms": [
                    null
                ],
                "language": "arabic"
            },
            {
                "type": "glossary",
                "title": "هيكل الحوافز (Incentive structure)",
                "definition": "الهياكل التَّحفيزيَّة هي مجموعة من آليات التَّقييم والمكافأة (الصَّريحة والضِّمنيَّة) للعلماء وأعمالهم. ضمن الهيكل الأوسع.  تتعلَّق المجالات المحفزَّة بممارسات التَّوظيف والتَّرقيات، ووجود سجل حافل في منح التَّمويل، ومؤشرات الاعتبار كالنَّشر في مجلات ذات معامل تأثير عالية، ودعوات العروض التَّقديميَّة، والتَّحرير، والجوائز. يذهب الكثير إلى أنَّ هذه المعايير غالبًا ما تكون غير متوافقة مع غاية العلم وبالتَّالي لا تشجع على الإنتاج العلمي الصَّارم. تهدف مبادرات مثل دورا إلى تقليل الاعتماد على معايير التَّقييم، مثل تقليل الاعتماد على معامل التَّأثير للمجلَّات، والتَّركيز على الجوهر العلمي لمخرجات البحوث. **المصطلحات ذات الصِّلة**: إعلان سان فرانسيسكو بشأن تقييم البحوث، المقاييس، الضَّغط، النشر أو الفناء، الكميَّة، هيكل المكافأة (نظام المكافآت)، المنشورات العلميَّة، العلم البطيء، العوامل الهيكليَّة (البنيويَّة).",
                "related_terms": [
                    "DORA",
                    "Metrics",
                    "Pressure",
                    "Publish or perish",
                    "Quantity",
                    "Reward structure",
                    "Scientific publications",
                    "Slow science",
                    "Structural factors"
                ],
                "references": "Koole, S. L., & Lakens, D. (2012). Rewarding replications: A sure and simple way to improve psychological science. Perspectives on Psychological Science, 7(6), 608–614. https://doi.org/10.1177/1745691612462586\n\nNosek, B. A., Spies, J. R., & Motyl, M. (2012). Scientific utopia: II. Restructuring incentives and practices to promote truth over publishability. Perspectives on Psychological Science, 7(6), 615–631. https://doi.org/10.1177/1745691612459058\n\nSchönbrodt, F. (2019). Training students for the Open Science future. Nature Human Behaviour, 3(10), 1031–1031. https://doi.org/10.1038/s41562-019-0726-z\n\nSmaldino, P. E., & McElreath, R. (2016). The natural selection of bad science. Royal Society Open Science, 3(9), 160384. https://doi.org/10.1098/rsos.160384",
                "drafted_by": [
                    "Charlotte R. Pennington; Olmo van den Akker"
                ],
                "reviewed_by": [
                    "Helena Hartmann",
                    "Flávio Azevedo",
                    "Robert M. Ross",
                    "Graham Reid",
                    "Suzanne L. K. Stewart"
                ],
                "alt_related_terms": [
                    null
                ],
                "language": "arabic"
            },
            {
                "type": "glossary",
                "title": "الاستقراء (Induction)",
                "definition": "هو الاستدلال باستخلاص نتائج لم تتضمَّنها المقدِّمات المنطقية، مثل استنباط قاعدة عامة من عدد محدود من الملاحظات.  ويعتقد بوبر بأن هذه العمليَّة غير ممكنة، فقد نخمِّن القواعد العامة، لكن مثل هذه التَّخمينات لا تصبح أكثر ترجيحًا عبر زيادة أي عدد من الملاحظات، وعلى نقيض ذلك، يقدّر علماء المنهج البايزي بشكل استقرائي مقدار الزِّيادة في ترجيح الفرضيَّة عبر تتبع الملاحظات\" (Dienes, 2008, p. 164).  **المصطلحات ذات الصِّلة:** الفرضيَّة.",
                "related_terms": [
                    "Hypothesis"
                ],
                "references": "Dienes, Z. (2008). Understanding psychology as a science: An introduction to scientific and statistical inference. Macmillan International Higher Education.",
                "drafted_by": [
                    "Alaa Aldoh"
                ],
                "reviewed_by": [],
                "alt_related_terms": [
                    null
                ],
                "language": "arabic"
            },
            {
                "type": "glossary",
                "title": "مغالطة التَّفاعل (Interaction Fallacy)",
                "definition": "هو خطأ إحصائي، حيث لا يتم إجراء اختبار منهجي لتقييم الفرق بين الارتباط ذي الدلالة الإحصائيَّة والارتباط الذي لم يحصل على لدلالة الإحصائيَّة، (أو مقاييس أخرى ، مثل نسبة الأرجحيَّة).  تحدث هذه المغالطة عندما يُفترض أنَّ معامل ارتباط دال إحصائياً وآخر غير دال يمثلان فرقًا ذا دلالة إحصائيَّة، ولكن بدون اختبار المقارنة نفسها بشكل صريح. **المصطلحات ذات الصِّلة:** مقارنة الارتباطات، اختبار دلالة الفرضيَّة الصِّفريَّة، الصِّدق الإحصائي، الخطأ من النَّوع الأول، الخطأ من النَّوع الثَّاني.",
                "related_terms": [
                    "Comparison of Correlations",
                    "Null Hypothesis Significance Testing (NHST)",
                    "Statistical Validity",
                    "Type I error",
                    "Type II error"
                ],
                "references": "Gelman, A., & Stern, H. (2006). The difference between “significant” and “not significant” is not itself statistically significant. The American Statistician, 60(4), 328–331. https://doi.org/10.1198/000313006X152649\n\nMorabia, A., Have, T. T., & Landis, J. R. (1997). Interaction Fallacy. Journal of Clinical Epidemiology, 50(7), 809–812. https://doi.org/10.1016/S0895-4356(97)00053-X\n\nNieuwenhuis, S., Forstmann, B. U., & Wagenmakers, E. J. (2011). Erroneous analyses of interactions in neuroscience: a problem of significance. Nature Neuroscience, 14(9), 1105–1107. https://doi.org/10.1038/nn.2886",
                "drafted_by": [
                    "Graham Reid"
                ],
                "reviewed_by": [
                    "Ali H. Al-Hoorie",
                    "Mahmoud Elsherif",
                    "Kai Krautter",
                    "Sam Parsons"
                ],
                "alt_related_terms": [
                    null
                ],
                "language": "arabic"
            },
            {
                "type": "glossary",
                "title": "التَّشابك (Interlocking)",
                "definition": "تحليل في صميم التَّقاطعية لتحليل السُّلطة، واللا مساواة، والإقصاء حيث لا يمكن إكمال الجهود المبذولة لإصلاح الثَّقافة الأكاديميَّة عبر دراسة وسيلة واحدة فقط بمعزل عن الآخر.  على سبيل المثال: العرق، أو الجنس، أو القدرة. ولكن من خلال النَّظر في جميع الأنظمة الإقصائية. وعلى النَّقيض من التَّقاطعيَّة، والتي تشير إلى أنَّ الفرد يمتلك هويات اجتماعيَّة متعدِّدة، عادةً ما يستخدم التَّشابك لوصف الأنظمة التي تتحد لتكون بمثابة تدابير قمعيَّة تجاه الفرد بناءً على هذه الهويات.  **المصطلحات ذات الصِّلة:** بروبينساينس، العدالة؛ التَّنوع، الشُّمول، التَّقاطعية، العلم المفتوح، العدالة الاجتماعيَّة.",
                "related_terms": [
                    "Bropenscience",
                    "Equity",
                    "Diversity",
                    "Inclusion",
                    "Intersectionality",
                    "Open Science",
                    "Social Justice"
                ],
                "references": "Ledgerwood, A., Hudson, S. T. J., Lewis, Jr., N. A., Maddox, K. B., Pickett, C., Remedios, J. D., & Wilkins, C. L. (2021). The Pandemic as a Portal: Reimagining Psychological Science as Truly Open and Inclusive. https://doi.org/10.31234/osf.io/gdzue",
                "drafted_by": [
                    "Christina Pomareda"
                ],
                "reviewed_by": [
                    "Ali H. Al-Hoorie",
                    "Flávio Azevedo",
                    "Mahmoud Elsherif",
                    "Eliza Woodward",
                    "Gerald Vineyard;"
                ],
                "alt_related_terms": [
                    null
                ],
                "language": "arabic"
            },
            {
                "type": "glossary",
                "title": "الصِّدق الدَّاخلي (Internal Validity)",
                "definition": "هو مؤشر على مدى كون نتائج دراسة ما تمثل التَّأثيرالحقيقي في مجتمع الدِّراسة المستهدف، وليست ناتجة عن متغيرات بحثيَّة دخيلة، مثل القصور في المنهجيَّة، وبعبارة أخرى: يعبّر الصِّدق الدَّاخلي عمَّا إذا كانت الأدلَّة المرصودة، أو التَّباين بين المتغيّرات المستقلة (المتنبِّئة) والمتغيِّرات التَّابعة (المحكية) ناتجة عن علاقة حقيقيَّة وليست نتيجة زائفة؛ بسبب الجوانب التي لا يمكن التَّحكُّم بها عند تصميم الدِّراسة.  وبما أنَّ الأمر يتعلَّق بجودة الدِّراسة نفسها، يعدُّ الصِّدق الدَّاخلي أولويَّة في البحث العلمي. **المصطلحات ذات الصِّلة:** الصِّدق الخارجي، الصِّدق. **التَّعريف البديل:** في القياسات النَّفسيَّة، هو درجة الأدلَّة التي تؤكد على التَّناسق بين الهيكل الدَّاخلي؛ لاختبار قياس نفسي، وبنية البناء النَّفسي. **المصطلحات ذات الصِّلة بالتَّعريف البديل:** الصِّدق البنائي.",
                "related_terms": [
                    "External validity",
                    "Validity"
                ],
                "references": "Campbell, D. T., & Stanley, J. C. (1966). Experimental and Quasi Experimental Designs. Rand McNally.",
                "drafted_by": [
                    "Annalise A. LaPlume"
                ],
                "reviewed_by": [
                    "Helena Hartmann",
                    "Oscar Lecuona",
                    "Meng Liu",
                    "Sam Parsons",
                    "Graham Reid",
                    "Flávio Azevedo"
                ],
                "alt_related_terms": [
                    null
                ],
                "language": "arabic"
            },
            {
                "type": "glossary",
                "title": "التَّقاطعيَّة (Intersectionality)",
                "definition": "مصطلح مستمد من الفكر النَّسوي الأسود، ويصف بشكل عام وجود الهويات الاجتماعيَّة داخل \"أنظمة الاضطهاد المتشابكة\" وهياكل (عدم) المساواة (Crenshaw, 1989). تقدِّم التَّقاطعيَّة منظورًا عن الطَّريقة التي تعمل بها أشكال متعدِّدة من عدم المساواة معًا؛ لمضاعفة بعضها البعض.  يمكن أن يكون لأشكال الهويَّة المتعدِّدة والمتزامنة تأثير مضاعف أكبر من مجرَّد مجموع العناصر المكونة لها.  أحد الآثار المترتِّبة على ذلك هو أنَّه لا يمكن فهم الهوية بشكل كاف من خلال فحص محور واحد (كالعرق، أو الجنس، أو الميول الجنسيَّة، أو الطَّبقية) في وقت ما بمعزل عن غيره، بل يتطلَّب دراسة متزامنة لأشكال الهوية المتداخلة. **المصطلحات ذات الصِّلة**: بروبنساينس، التَّنوع، الشمول، التشابك، العلم المفتوح.",
                "related_terms": [
                    "Bropenscience",
                    "Diversity",
                    "Inclusion",
                    "Interlocking",
                    "Open Science"
                ],
                "references": "Crenshaw, K. W. (1989). Demarginalizing the Intersection of Race and Sex: A Black Feminist Critique of Antidiscrimination Doctrine. University of Chicago Legal Forum, 1989(8), 139–168.\n\nGrzanka, P. R. (2020). From buzzword to critical psychology: An invitation to take intersectionality seriously. Women & Therapy, 43(3–4), 244–261.\n\nLedgerwood, A., Hudson, S. T. J., Lewis, Jr., N. A., Maddox, K. B., Pickett, C., Remedios, J. D., & Wilkins, C. L. (2021). The Pandemic as a Portal: Reimagining Psychological Science as Truly Open and Inclusive. https://doi.org/10.31234/osf.io/gdzue",
                "drafted_by": [
                    "Madeleine Pownall"
                ],
                "reviewed_by": [
                    "Ali H. Al-Hoorie",
                    "Bradley Baker",
                    "Mahmoud Elsherif",
                    "Wanyin Li",
                    "Ryan Millager",
                    "Charlotte R. Pennington",
                    "Flávio Azevedo"
                ],
                "alt_related_terms": [
                    null
                ],
                "language": "arabic"
            },
            {
                "type": "glossary",
                "title": "برنامج جاب ريف (JabRef)",
                "definition": "أداة لإدارة المراجع، والاستشهادات، مفتوحة المصدر، متعدِّدة المنصَّات، متاحة مجانًا.  تسمح بتحرير ملفات بيب تيكس، واستيراد البيانات من قواعد البيانات العلميَّة عبر الإنترنت، وإدارة ملفات بيب تيكس والبحث فيه**ا.**  **المصطلحات ذات الصِّلة:** برنامج مفتوح المصدر.",
                "related_terms": [
                    "Open source software"
                ],
                "references": "Team, J. D. (2021). JabRef - An open-source, cross-platform citation and reference management software. https://www.jabref.org",
                "drafted_by": [
                    "Aleksandra Lazić"
                ],
                "reviewed_by": [
                    "Christopher Graham",
                    "Michele C. Lim",
                    "Sam Parsons",
                    "Steven Verheyen"
                ],
                "alt_related_terms": [
                    null
                ],
                "language": "arabic"
            },
            {
                "type": "glossary",
                "title": "جاموفي (Jamovi)",
                "definition": "هو برنامج  مجاني، ومفتوح المصد؛ لتحليل البيانات، مبني على لغة آر.  يحتوي البرنامج على واجهة مستخدم رسوميَّة، ويوفِّر نصوص برمجيَّة للتَّحليلات بلغة الآر.  يدعم جاموفي إمكانيَّة إعادة الإنتاج الإحصائي من خلال حفظ البيانات، والتَّعليمات البرمجيَّة، والَّتحليلات، والنَّتائج في ملف واحد. **المصطلحات ذات الصِّلة:** برنامج جاسب، المصدر المفتوح، لغة الآر، قابلية إعادة الإنتاج.",
                "related_terms": [
                    "JASP",
                    "Open source",
                    "R",
                    "Reproducibility"
                ],
                "references": "",
                "drafted_by": [
                    "Amélie Beffara Bret"
                ],
                "reviewed_by": [
                    "Adrien Fillon",
                    "Alexander Hart",
                    "Charlotte R. Pennington"
                ],
                "alt_related_terms": [
                    null
                ],
                "language": "arabic"
            },
            {
                "type": "glossary",
                "title": "برنامج جاسب (JASP)",
                "definition": "سُمّيَ البرنامج على اسم السير هارولد جيفريز، وهو اختصار لبرنامج \" جيفري المذهل للإحصائيَّات\". وهو برنامج مجاني، ومصدر مفتوح لتحليل البيانات.  يعتمد برنامج جاسب على واجهة مستخدم ويوفِّر كلًا من اختبارات الفرضيَّة الصِّفريَّة، وكذلك نظيرتها من الاختبارات البيزية. يدعم برنامج جاسب إمكانيَّة إعادة الإنتاج الإحصائي من خلال حفظ البيانات، والتَّعليمات البرمجيَّة، والتَّحليلات، والنَّتائج في ملف واحد.  **المصطلحات ذات الصِّلة:** جاموفي، المصدر المفتوح.",
                "related_terms": [
                    "Jamovi",
                    "Open source"
                ],
                "references": "Team, J. (2020). JASP (Version 0.14.1) [Computer software].",
                "drafted_by": [
                    "Amélie Beffara Bret"
                ],
                "reviewed_by": [
                    "Adrien Fillon, Adam Parker",
                    "Sam Parsons"
                ],
                "alt_related_terms": [
                    null
                ],
                "language": "arabic"
            },
            {
                "type": "glossary",
                "title": "معامل تأثير المجَّلة (Journal Impact Factor™)",
                "definition": "متوسط عدد الاستشهادات للمقالات البحثيَّة في مجلة خلال العامين السَّابقين، وهو عبارة عن حساب خاص ومبهم تسوقه شركة كلاريفات. لا ترتبط عوامل تأثير المجلَّة بجودة المحتوى، أو بعمليَّة تحكيم الأقران.  **المصطلحات ذات العلاقة:** إعلان سان فرانسيسكو بشأن تقييم البحوث، مؤشر إتش.",
                "related_terms": [
                    "DORA",
                    "H-index"
                ],
                "references": "Brembs, B., Button, K., & Munafò, M. (2013). Deep impact: unintended consequences of journal rank. Frontiers in Human Neuroscience, 7, 291. https://doi.org/10.3389/fnhum.2013.00291\n\nCurry, S. (2012). Sick of impact factors. http://occamstypewriter.org/scurry/2012/08/13/sick-of-impact-factors/\n\nNaudet, F., Ioannidis, J., Miedema, F., Cristea, I. A., Goodman, S. N., & Moher, D. (2018). Six principles for assessing scientists for hiring, promotion, and tenure. Impact of Social Sciences Blog.\n\nRossner, M., Van Epps, H., & Hill, E. (2008). Show me the data. https://doi.org/10.1083/jcb.200711140\n\nSharma, M., Sarin, A., Gupta, P., Sachdeva, S., & Desai, A. (2014). Journal impact factor: its use, significance and limitations. World Journal of Nuclear Medicine, 13(2), 146. https://doi.org/10.4103/1450-1147.139151",
                "drafted_by": [
                    "Jacob Miranda"
                ],
                "reviewed_by": [
                    "Tsvetomira Dumbalska",
                    "Adam Parker"
                ],
                "alt_related_terms": [
                    null
                ],
                "language": "arabic"
            },
            {
                "type": "glossary",
                "title": "ملفات جي سون (JSON file)",
                "definition": "تدوين كائن جافا النصّي هو تنسيق للبيانات المنظَّمة التي يمكن استخدامها لتمثيل أزواج القيم والسِّمات. وبالتَّالي يمكن أن تحتوي القيم على مزيد من تدوينات جافا النَّصيَّة (مثل المعلومات المتداخلة). يمكن تشفير هذه الملفات رسميًا كسلاسل نصيَّة، وبالتَّالي يمكن للإنسان قراءتها. بالإضافة إلى تخزين المعلومات، فإن هذه الميزة تجعلها مناسبة للتَّعليق على محتوى آخر. فمثلًا تُستخدم هذه الملفات في بنية بيانات تصوير الدِّماغ لوصف مجموعة البيانات الوصفيَّة باتباع تنسيق موحَّد. (dataset\\_description.json).  **المصطلحات ذات الصِّلة:** بنية بيانات تصوير الدّماغ، البيانات الوصفيَّة.",
                "related_terms": [
                    "BIDS data structure",
                    "Metadata"
                ],
                "references": "BIDS. (n.d.). Modality agnostic files. Retrieved from https://bids-specification.readthedocs.io/en/stable/03-modality-agnostic-files.html",
                "drafted_by": [
                    "Tina Lonsdorf"
                ],
                "reviewed_by": [
                    "Alexander Hart",
                    "Matt Jaquiery",
                    "Emma Norris",
                    "Charlotte R. Pennington"
                ],
                "alt_related_terms": [
                    null
                ],
                "language": "arabic"
            },
            {
                "type": "glossary",
                "title": "اكتساب المعرفة (Knowledge acquisition)",
                "definition": "هي عملية ذهنيَّة يتم من خلالها تفسير، أو استخلاص وتخزين وربط المعلومات الجديدة بالمعلومات المخزَّنة في الذَّاكرة طويلة المدى. ونظرًا لصعوبة وتعقيد المعرفة، وتركيبها فإنَّ هذه العلميَّة الذِّهنيَّة تتم دراستها في المجال الفلسفي لنظرية المعرفة، وكذلك المجال النَّفسي للتعلُّم و الذَّاكرة.  **المصطلحات ذات الصِّلة:** نظرية المعرفة، المعلومات، التَّعلمُّ",
                "related_terms": [
                    "Epistemology",
                    "Information",
                    "Learning"
                ],
                "references": "Brule, J., & Blount, A. (1989). Knowledge acquisition. McGraw-Hill.",
                "drafted_by": [
                    "Oscar Lecuona"
                ],
                "reviewed_by": [
                    "Bradley Baker",
                    "Helena Hartmann",
                    "Kai Krautter",
                    "Graham Reid"
                ],
                "alt_related_terms": [
                    null
                ],
                "language": "arabic"
            },
            {
                "type": "glossary",
                "title": "دالة الاحتماليَّة (Likelihood function)",
                "definition": "نموذج إحصائي للبيانات المستخدمة في التَّحليلات التِّكراريَّة والبايزية محدد حتى معامِل ثابت التَّناسب.  تمثل دالة الاحتماليَّة مدى احتمال المؤشرات  المختلفة للتَّوزيع في ضوء البيانات. بالنَّظر إلى أنَّ التَّوزيعات الاحتماليَّة لها مؤشرات غير معروفة تتعلَّق بمجتمع الدِّراسة. تشير دالة الاحتماليَّة إلى جودة بيانات العيِّنة في تلخيص هذه المؤشرات. وعلى هذا النَّحو، تعطي دالة الاحتماليَّة فكرة عن جودة ملاءمة نموذج بيانات العيِّنة لمجموعة معيَّنة من القيم لمؤشرات المجتمع غير المعروفة. **تعريف بديل:** من النَّاحية الإحصائيَّة، بالنَّظر إلى نموذج حدودي محدَّد بواسطة دالة الاحتماليَّة، أو الكثافة (س | ثيتا)، يتم تعريف احتماليَّة النّموذج الإحصائي بنفس صيغة الكثافة باستثناء أن أدوار البيانات ***س*** والمؤشر ***ثيتا*** متبادلة، وبالتَّالي يمكن اعتبار الاحتماليَّة دالة ثيتا للبيانات الثابتة س. في هذه الحالة ستصف دالة الاحتمال منحنى أو سطحا فائقا تمثل ذروته، إن وجدت، مزيجًا من قيم معلمات النّموذج التي تزيد من احتماليَّة رسم العيِّنة التي تم الحصول عليها. **المصطلحات ذات الصِّلة**: معامل بايز، الاستدلال البايزي، تقدير المعاملات باستخدام النهج البايزي، التَّوزيع اللَّاحق، التَّوزيع المسبق.",
                "related_terms": [
                    "Bayes factor",
                    "Bayesian inference",
                    "Bayesian parameter estimation",
                    "Posterior distribution",
                    "Prior distribution **Alternative definition:** For a more statistically-informed definition, given a parametric model specified by a probability (densidity) function f(x|theta), a likelihood *for* a statistical model is defined by the same formula as the density except that the roles of the data *x* and the parameter *theta* are interchanged, and thus the likelihood can be considered a function of *theta* for fixed data *x*. Here, then, the likelihood function would describe a curve or hypersurface whose peak, if it exists, represents the combination of model parameter values that maximize the probability of drawing the sample obtained."
                ],
                "references": "Dienes, Z. (2008). Understanding psychology as a science: An introduction to scientific and statistical inference. Macmillan International Higher Education.\n\nHogg, D., Bovy, J., & Lang, D. (2010). Data analysis recipes: Fitting a model to data. arXiv:1008.4686 [astro-ph.IM].\n\nGeyer, C. J. (2003). Maximum Likelihood in R (pp. 1–9). Open Science Framework.\n\nGeyer, C. J. (2007). Stat 5102 Notes: Maximum Likelihood (pp. 1–8). Open Science Framework.\n\nHuber, C. (2016). The Stata Blog: Introduction to Bayesian statistics, part 1: The basic concepts. In The Stata Blog. https://blog.stata.com/2016/11/01/introduction-to-bayesian-statistics-part-1-the-basic-concepts/",
                "drafted_by": [
                    "Alaa AlDoh"
                ],
                "reviewed_by": [
                    "Dominik Kiersz",
                    "Graham Reid",
                    "Sam Parsons",
                    "Flávio Azevedo"
                ],
                "alt_related_terms": [
                    null
                ],
                "language": "arabic"
            },
            {
                "type": "glossary",
                "title": "مبدأ الاحتماليَّة (Likelihood Principle)",
                "definition": "فكرة أن جميع المعلومات ذات الصِّلة بالاستدلال الواردة في البيانات يتم توفيرها من خلال الاحتماليَّة. يشير المبدأ إلى أنَّه يمكن استخدام دالة الاحتماليَّة لمقارنة مدى معقوليَّة قيم المعلمات المختلفة. في حين يؤيد البايزيون ومنظرو الاحتماليَّة مبدأَ الاحتماليَّة، فإن منظري نيمان-بيرسون لا يؤيدونه؛ لأن الاختبارات الدّلالية تنتهك مبدأ الاحتماليّة لأنَّهم يأخذون في الاعتبار معلومات ليست في الاحتمال نفسه.  **المصطلحات ذات الصلة:** الاستدلال البايزي؛ دالة الاحتماليَّة",
                "related_terms": [
                    "Bayesian inference",
                    "Likelihood Function"
                ],
                "references": "Dienes, Z. (2008). Understanding psychology as a science: An introduction to scientific and statistical inference. Macmillan International Higher Education.\n\nGeyer, C. J. (2003). Maximum Likelihood in R (pp. 1–9). Open Science Framework.\n\nGeyer, C. J. (2007). Stat 5102 Notes: Maximum Likelihood (pp. 1–8). Open Science Framework.",
                "drafted_by": [
                    "Alaa Aldoh"
                ],
                "reviewed_by": [
                    "Sam Parsons",
                    "Flávio Azevedo"
                ],
                "alt_related_terms": [
                    null
                ],
                "language": "arabic"
            },
            {
                "type": "glossary",
                "title": "مراجعة الأدبيَّات (Literature Review)",
                "definition": "غالبًا ما يقوم الباحثون بمراجعة سجلات البحث عن موضوع معين؛ لفهم التَّأثيرات، والظَّواهر محل الاهتمام بشكل أفضل قبل االبدء في مشروع بحثي جديد؛ لفهم كيفية ارتباط النَّظرية بالأدلَّة، أو للتَّحقيق في الموضوعات والاتِّجاهات المشتركة لنتائج وادعاءات الدِّراسة الحالية. يمكن إجراء أنواع مختلفة من المراجعات اعتمادًا على سؤال البحث ونطاق الأدبيَّات.  يمكن للباحثين في حقل معرفي ما أن يقوموا بإجراء مراجعة أدبية استطلاعية لغرض تحديد النِّطاق، والمفاهيم الأساسيَّة في حقلهم المعرفي. تهدف المراجعات المنهجيَّة إلى الوصول إلى جميع السجلَّات المتاحة، ومراجعتها للحصول على تمثيل أكثر دقّة وغير متحيِّز للأدبيَّات الموجودة، بينما تهدف المراجعات غير المنهجيَّة، أو المركّزة على تجميع المعلومات من مجموعة مختارة من الدِّراسات ذات الصِّلة بسؤال البحث وإن كانت غير شائعة بسبب القابليَّة للتَّحيُّزات (مثل تحيُّز الباحث؛ Siddaway et al., 2019). **المصطلحات ذات الصِّلة:** توليف الأدلَّة، العلوم البعدية/التلوية أو البحث البعديَّ/التلوي، المراجعات السَّرديَّة، المراجعة المنهجيَّة.",
                "related_terms": [
                    "Evidence synthesis",
                    "Meta-research",
                    "Narrative reviews",
                    "Systematic reviews"
                ],
                "references": "Huelin, R., Iheanacho, I., Payne, K., & Sandman, K. (2015). What’s in a name? Systematic and non-systematic literature reviews, and why the distinction matters. The Evidence Forum, 34–37. Retrieved from https://www.evidera.com/wp-content/uploads/2015/06/Whats-in-a-Name-Systematic-and-Non-Systematic-Literature-Reviews-and-Why-the-Distinction-Matters.pdf\n\nMunn, Z., Peters, M. D., Stern, C., Tufanaru, C., McArthur, A., & Aromataris, E. (2018). Systematic review or scoping review? Guidance for authors when choosing between a systematic or scoping review approach. BMC Medical Research Methodology, 18(1), 1–7. https://doi.org/10.1186/s12874-018-0611-x\n\nPautasso, M. (2013). Ten Simple Rules for Writing a Literature Review. PLoS Computational Biology, 9(7), e1003149. https://doi.org/10.1371/journal.pcbi.1003149\n\nSiddaway, A. P., Wood, A. M., & Hedges, L. V. (2019). How to do a systematic review: a best practice guide for conducting and reporting narrative reviews, meta-analyses, and meta-syntheses. Annual Review of Psychology, 70, 747–770. https://doi.org/10.1146/annurev-psych-010418-102803",
                "drafted_by": [
                    "Marta Topor"
                ],
                "reviewed_by": [],
                "alt_related_terms": [
                    null
                ],
                "language": "arabic"
            },
            {
                "type": "glossary",
                "title": "اللَّجنة الذُّكوريَّة (Manel)",
                "definition": "يستخدم هذا التَّعبير- المأخوذ من كلمتي رجل ولجنة من الخبراء في اللُّغة الإنجليزيَّة \\- عادةً للإشارة إلى مجموعات المتحدثين في المؤتمرات المكوَّنة بالكامل من الذُّكور (وهي عادة مجالس مغلقة). وعادة ما يناقش في سياق الفوارق بين الجنسين في الأوساط الأكاديميَّة (على سبيل المثال، تقلُّ احتماليَّة الاعتراف بالنِّساء كخبيرات من قبل أقرانهن، وبالتَّالي تقلُّ فرصهنَّ في التَّطور الوظيفي).  **المصطلحات ذات الصِّلة:** بروبنساينس، التَّنوع، العدالة،علم النَّفس النَّسوي، الشُّمول، نقص التَّمثيل",
                "related_terms": [
                    "Bropenscience",
                    "Diversity",
                    "Equity",
                    "Feminist psychology",
                    "Inclusion",
                    "Under-representation"
                ],
                "references": "Bouvy, J. C., & Mujoomdar, M. (2019). All-Male Panels and Gender Diversity of Issue Panels and Plenary Sessions at ISPOR Europe. PharmacoEconomics-Open, 3(3), 419–422. https://doi.org/10.1007/s41669-019-0153-0\n\nGoodman, S. W., & Pepinsky, T. B. (2019). Gender Representation and Strategies for Panel Diversity: Lessons from the APSA Annual Meeting. PS: Political Science & Politics, 52(4), 669–676. https://doi.org/10.1017/S1049096519000908\n\nNittrouer, C., Hebl, M., Ashburn-Nardo, L., Trump-Steele, R., Lane, D., & Valian, V. (2018). Gender disparities in colloquium speakers. Proceedings of the National Academy of Sciences, 115(1), 104–108. https://doi.org/10.1073/pnas.1708414115\n\nRodriguez, J. K., & Günther, E. A. (2020). What’s wrong with manels and what can we do about it. The Conversation. https://theconversation.com/whats-wrong-with-manels-and-what-can-we-do-about-them-148068",
                "drafted_by": [
                    "Sam Parsons"
                ],
                "reviewed_by": [
                    "Mahmoud Elsherif",
                    "Thomas Rhys Evans",
                    "Beatrice Valentini",
                    "Christopher Graham",
                    "Flávio Azevedo"
                ],
                "alt_related_terms": [
                    null
                ],
                "language": "arabic"
            },
            {
                "type": "glossary",
                "title": "العديد من المؤلِّفين (Many authors)",
                "definition": "هي مشاريع تعاونيَّة واسعة النِّطاق يشترك فيها العشرات، أو المئات من الباحثين من مؤسّسات مختلفة، أصبحت هذه المنهجيَّة شائعة جدا في علم النَّفس، وبعض العلوم الأخرى خلال السَّنوات الماضية مقارنة بالبحوث التي تجريها فِرق صغيرة من المؤلِّفين، اتباعًا لاتجاهات في بحوث الطَّاقة الفيزيائيَّة العالية، أو البحوث الطبيَّة الحيويَّة في أوائل التِّسعينيات. تعمل هذه الائتلافات العلميَّة الدُّوليَّة على جمع مجموعة واسعة من الخبرات، والعمل بشكل تعاونيّ لإنتاج البحوث.  **المصطلحات ذات الصِّلة:** تعاون، الائتلافات، التَّحالف التَّأليفي ، عدد الكتاب مرتفع (فرط الباحثين)، حشد المصادر، تعدُّد الباحثين، فريق بحث.",
                "related_terms": [
                    "Collaboration",
                    "Consortia",
                    "Consortium authorship",
                    "Crowdsourcing",
                    "Hyperauthorship",
                    "Multiple-authors",
                    "Team science"
                ],
                "references": "Cronin, B. (2001). Hyperauthorship: A postmodern perversion or evidence of a structural shift in scholarly communication practices? Journal of the American Society for Information Science and Technology, 52(7), 558–569. https://doi.org/10.1002/asi.1097\n\nMoshontz, H., Ebersole, C. R., Weston, S. J., & Klein, R. A. (2021). A guide for many authors: Writing manuscripts in large collaborations. Social and Personality Psychology Compass, 15(4). https://doi.org/10.1111/spc3.12590\n\nWuchty, S., Jones, B. F., & Uzzi, B. (2007). The increasing dominance of teams in production of knowledge. Science, 316(5827), 1036–1039. https://doi.org/10.1126/science.1136099",
                "drafted_by": [
                    "Yu-Fang Yang"
                ],
                "reviewed_by": [
                    "Christopher Graham",
                    "Adam Parker",
                    "Charlotte R. Pennington",
                    "Birgit Schmidt",
                    "Beatrice Valentini"
                ],
                "alt_related_terms": [
                    null
                ],
                "language": "arabic"
            },
            {
                "type": "glossary",
                "title": "المعامل المتعدِّدة (Many Labs)",
                "definition": "التّعريف:** مبادرة بقيادة فريق التَّعاون العلميّ المفتوح عام 2015 حيث إنَّ مئات المجموعات البحثيَّة من مختلف الجامعات كرَّرت عددًا من الدِّراسات المنشورة ذات نسبة تأثير عاليَة.  عرفت هذه المبادرة بـ \"المعامل المتعدِّدة رقم 1\" وتلاها مشروع \"المعامل المتعدِّدة رقم 2\" الذي قام بتقييم التَّباين في نتائج الدِّراسات المكرَّرة عبر العيّنات، والسِّياقات المختلفة.  وهناك مشاريع مشابهة مثل \"العديد من الأطفال\" و \"العديد من تخطيطات الدِّماغ الكهربائيَّة\" ومسرع العلوم النَّفسيَّة. **المصطلحات ذات الصِّلة: التَّعاون، العديد من المحلِّلين، المعامل المتعدِّدة رقم 1، المعامل المتعدِّدة رقم 2، التَّعاون العلميّ المفتوح، التِّكرار.",
                "related_terms": [
                    "Collaboration",
                    "Many analysts",
                    "Many Labs I",
                    "Many Labs II",
                    "Open Science Collaboration",
                    "Replication"
                ],
                "references": "Ebersole, C. R., Atherton, O. E., Belanger, A. L., Skulborstad, H. M., Allen, J. M., Banks, J. B., & Nosek, B. A. (2016). Many Labs 3: Evaluating participant pool quality across the academic semester via replication. Journal of Experimental Social Psychology, 67, 68–82. https://doi.org/10.1016/j.jesp.2015.10.012\n\nFrank, M. C., Bergelson, E., Bergmann, C., Cristia, A., Floccia, C., Gervain, J., Hamlin, J. K., Hannon, E. E., Kline, M., Levelt, C., Lew-Williams, C., Nazzi, T., Panneton, R., Rabagliati, H., Soderstrom, M., Sullivan, J., Waxman, S., & Yurovsky, D. (2017). A Collaborative Approach to Infant Research: Promoting Reproducibility, Best Practices, and Theory-Building. Infancy, 22, 421–435. https://doi.org/10.1111/infa.12182\n\nKlein, R. A., Ratliff, K. A., Vianello, M., Adams, R. B., Bahník, Š., Bernstein, M. J., & et al. (2014). Investigating variation in replicability: A “many labs” replication project. Social Psychology, 45, 142–152. https://doi.org/10.1027/1864-9335/a000178\n\nKlein, R. A., Vianello, M., Hasselman, F., Adams, B. G., Adams, R. B., Alper, S., & … Nosek, B. A. (2018). Many Labs 2: Investigating Variation in Replicability Across Samples and Settings. Advances in Methods and Practices in Psychological Science, 1(4), 443–490. https://doi.org/10.1177/2515245918810225\n\nMoshontz, H., Campbell, L., Ebersole, C. R., IJzerman, H., Urry, H. L., Forscher, P. S., & Chartier, C. R. (2018). The Psychological Science Accelerator: Advancing psychology through a distributed collaborative network. Advances in Methods and Practices in Psychological Science, 1(4), 501–515. https://doi.org/10.1177/2515245918797607\n\nPavlov, Y. G., Adamian, N., Appelhoff, S., Arvaneh, M., Benwell, C., Beste, C., & Mushtaq, F. (2020). #EEGManyLabs: Investigating the Replicability of Influential EEG Experiments. PsyArXiv Preprint. https://doi.org/10.31234/osf.io/528nr",
                "drafted_by": [
                    "Sam Parsons"
                ],
                "reviewed_by": [
                    "Helena Hartmann",
                    "Charlotte R. Pennington",
                    "Mirela Zaneva"
                ],
                "alt_related_terms": [
                    null
                ],
                "language": "arabic"
            },
            {
                "type": "glossary",
                "title": "MOOCs مقرَّرات التَّعلُّم الضَّخمة المفتوحة عن بعد (Massive Open Online Courses (MOOCs))",
                "definition": "هي الدَّورات التَّدريبيَّة التي تنفذ عبر الإنترنت حصريًا، والتي يمكن الوصول إليها من قبل أي متعلِّم في أي وقت، عادة ما يكون الوصول إليها مجانًا (على الرُّغم من أنَّها ليست بالضَّرورة ذات رخصة استعمال علني)، وتوفِّر إرشادات قائمة على الفيديو، ومجموعات وتمارين بيانات قابلة للتَّنزيل.  يصف الجانب \"الضَّخم\" منها الحجم الكبير من الطُّلاب الذين يمكنهم الوصول إلى الدُّورة التَّدريبيَّة في أي وقت بسبب مرونتها، وتكلفتها المنخفضة، أو كونها مجانيَّة، وطبيعة المواد عبر الإنترنت.  **المصطلحات ذات الصِّلة:** إمكانيَّة الوصول،  التَّعليم عن بعد، الشُّمول، التَّعلم المفتوح",
                "related_terms": [
                    "Accessibility",
                    "Distance education",
                    "Inclusion",
                    "Open learning"
                ],
                "references": "Baturay, M. H. (2015). An overview of the world of MOOCs. Procedia-Social and Behavioral Sciences, 174, 427–433. https://doi.org/10.1016/j.sbspro.2015.01.685",
                "drafted_by": [
                    "Elizabeth Collins"
                ],
                "reviewed_by": [
                    "Tsvetomira Dumbalska",
                    "Mahmoud Elsherif",
                    "Helena Hartmann",
                    "Sam Parsons",
                    "Charlotte R. Pennington"
                ],
                "alt_related_terms": [
                    null
                ],
                "language": "arabic"
            },
            {
                "type": "glossary",
                "title": "MOOPs الأوراق الضخمة والمفتوحة على الإنترنت (Massively Open Online Papers (MOOPs))",
                "definition": "تتبع الأوراق الضَّخمة المفتوحة على الإنترنت نموذجًا مفتوحًا تشاركيًا يمتاز بالمرونة، ولا تحكمه قائمة محدَّدة من المشاركين، على عكس المقال التَّقليدي المشترك.  **المصطلحات ذات الصِّلة:** علم المواطن، البحث الجماعي، العديد من المؤلِّفين.",
                "related_terms": [
                    "Citizen science",
                    "Collaboration",
                    "Crowdsourced Research",
                    "Many authors",
                    "Team science"
                ],
                "references": "Himmelstein, D. S., Rubinetti, V., Slochower, D. R., Hu, D., Malladi, V. S., Greene, C. S., & Gitter, A. (2019). Open collaborative writing with Manubot. PLOS Computational Biology, 15(6), e1007128. https://doi.org/10.1371/journal.pcbi.1007128\n\nTennant, J., Bielczyk, N. Z., Greshake Tzovaras, B., Masuzzo, P., & Steiner, T. (2019). Introducing Massively Open Online Papers (MOOPs). MetaArXiv. https://doi.org/10.31222/osf.io/et8ak",
                "drafted_by": [
                    "Ali H. Al-Hoorie"
                ],
                "reviewed_by": [],
                "alt_related_terms": [
                    null
                ],
                "language": "arabic"
            },
            {
                "type": "glossary",
                "title": "in science تأثير ماثيو في العلوم (Matthew effect (in science))",
                "definition": "هو إعادة صياغة لمبدأ ماثيو الإنجيلي (الأثرياء يصبحون أكثر ثراءً؛ الفقراء يصبحون أكثر فقرًا)، حيث يُنسب إلى العلماء البارزين، والباحثين في بداية حياتهم المهنيَّة الحاصلين على زمالة مرموقة مستوياتٍ أعلى من السُّمعة، والتَّمويل بشكل غير متناسب مع مساهماتهم العلميَّة، في حين أنَّ الباحثين غير المعروفين نسبيًا أو الباحثين في بداية حياتهم المهنيَّة دون زمالة مرموقة لا يحصلون على سمعة تتناسب مع مثل هذه المساهمات. ويمثل هذا التَّأثير ميزة تراكميَّة كبيرة تنتج عن تراكم مزايا أوليَّة متواضعة (والعكس صحيح). . **المصطلحات ذات الصِّلة:** تأثبر ماثيو في (العلوم) ؛ قانون ستيجلر للتَّسمية .",
                "related_terms": [
                    "Matthew effect in education",
                    "Stigler’s law of eponymy"
                ],
                "references": "Bol, T., de Vaan, M., & van de Rijt, A. (2018). The Matthew effect in science funding. Proceedings of the National Academy of Sciences, 115(19), 4887–4890. https://doi.org/10.1073/pnas.1719557115\n\nBornmann, L., Ganser, C., Tekles, A., & Leydesdorff, L. (2019). Does the hα index reinforce the Matthew effect in science? Agent-based simulations using Stata and R. arXiv preprint https://arxiv.org/abs/1905.11052.\n\nMerton, R. K. (1968). The Matthew Effect in Science. Science, 159(3810), 56–63. https://doi.org/10.1126/science.159.3810.56",
                "drafted_by": [
                    "Tamara Kalandadze"
                ],
                "reviewed_by": [
                    "Bradley Baker",
                    "Tsvetomira Dumbalska",
                    "Mahmoud Elsherif",
                    "Matt Jaquiery",
                    "Charlotte R. Pennington"
                ],
                "alt_related_terms": [
                    null
                ],
                "language": "arabic"
            },
            {
                "type": "glossary",
                "title": "التَّحليل البعدي (Meta-analysis)",
                "definition": "التَّحليل البعدي (أو التَّلوي) هو تجميع إحصائي لنتائج عدد من الدِّراسات التي تبحث في نفس الظَّاهرة.  توجد طرق متنوعة من أساليب التَّحليل البعدي، بما في ذلك نماذج التَّأثيرات العشوائيَّة، أو الثَّابتة، أو الانحدارات البعديَّة، والتي تسمح بفحص التَّأثيرات الوسيطة. ومن خلال تجميع  البيانات من دراسات متعدِّدة، يمكن أن يوفِّر التَّحليل البعدي تقديرًا أكثر دِقّة لظاهرة ما، مثل: معاملة معيّنة من الدِّراسات الفرديَّة. عادة ما يتم تصوير النَّتائج في مخطط غابي. ويمكن أن تساعد التَّحليلات البعديَّة أيضًا في فحص غياب التَّجانس في نتائج الدِّراسات. وغالبًا ما يتم إجراء التَّحليلات البعديَّة بالاقتران مع المراجعات المنهجيَّة التي تتطلَّب كذلك بحثًا، وفحصًا منهجيًا للدِّراسات. ومن الشَّائع أيضًا فحص تحيُّز النَّشر في سياق التَّحليل البعدي، وعادة ما يتم تقديمه بشكل مرئي عبر مخطط قمعي. **المصطلحات ذات الصِّلة:** كونسرتCONSORT ، التَّحليل التَّلوي الارتباطي ، حجم الأثر ، توليف الأدلَّة ، المراجعات المنهجيَّة غير التدخلية المفتوحة والقابلة للتِّكرار؛ بريزما  PRISMA ، تحيُّز النَّشر (مشكلة درج الملفات) ؛ ستروب STROBE ؛ المراجعة المنهجيَّة",
                "related_terms": [
                    "CONSORT",
                    "Correlational Meta-Analysis",
                    "Effect size",
                    "Evidence synthesis",
                    "Non-Intervention, Reproducible, and Open Systematic Reviews (NIRO-SR)",
                    "PRISMA",
                    "Publication bias (File Drawer Problem)",
                    "STROBE",
                    "Systematic Review"
                ],
                "references": "Borenstein, M., Hedges, L. V., Higgins, J. P., & Rothstein, H. R. (2011). Introduction to meta-analysis. John Wiley & Sons.\n\nYeung, S. K., Feldman, G., Fillon, A., Protzko, J., Elsherif, M. M., Xiao, Q., & Pickering, J. (2020). Experimental Studies Meta-Analysis Registered Report Templates. https://osf.io/ytgrp/",
                "drafted_by": [
                    "Martin Vasilev; Siu Kit Yeung"
                ],
                "reviewed_by": [
                    "Thomas Rhys Evans",
                    "Tamara Kalandadze",
                    "Charlotte R. Pennington",
                    "Mirela Zaneva"
                ],
                "alt_related_terms": [
                    null
                ],
                "language": "arabic"
            },
            {
                "type": "glossary",
                "title": "البيانات الوصفيَّة (Metadata)",
                "definition": "بيانات مرتَّبة تجمع وتصف بيانات أخرى.  تساعد البيانات الوصفيَّة في العثور على البيانات، وترتيبها وفهمها. تتضمَّن أمثلة البيانات الوصفيَّة على معلومات المنتج،  والعنوان، والمساهمين، والكلمات المفتاحية والعلامات، بالإضافة إلى أي نوع من المعلومات اللَّازمة للتَّحقق وفهم  نتائج الدِّراسات واستنتاجاتها مثل كتاب الرُّموز حول تسميات البيانات، والأوصاف، ونوع العيِّنة وطريقة جمع البيانات. **المصطلحات ذات الصِّلة**: بيانات، البيانات المفتوحة. تعريف بديل: (إن أمكن) بيانات حول البيانات.",
                "related_terms": [
                    "Data",
                    "Open Data **Alternative definition:** (if applicable) Data about data"
                ],
                "references": "Gollwitzer, M., Abele-Brehm, A., Fiebach, C., Ramthun, R., Scheel, A. M., Schönbrodt, F. D., & Steinberg, U. (2020). Data Management and Data Sharing in Psychological Science: Revision of the DGPs Recommendations.",
                "drafted_by": [
                    "Matt Jaquiery"
                ],
                "reviewed_by": [
                    "Helena Hartmann",
                    "Tina Lonsdorf",
                    "Charlotte R. Pennington",
                    "Mirela Zaneva"
                ],
                "alt_related_terms": [
                    null
                ],
                "language": "arabic"
            },
            {
                "type": "glossary",
                "title": "العلوم البعديَّة/التَّلوية أو البحث البعدي/التَّلوي (Meta-science or Meta-research)",
                "definition": "يشير المصطلح إلى الدِّراسة العلميَّة للعلم نفسه بهدف وصف وشرح وتقييم، وتطوير الممارسات العلميَّة. يبحث علم \"ماوراء العلوم \" عادة في الأساليب العلميَّة والتَّحليل وإعداد التَّقارير عن البيانات وتقييمها وإعادة إنتاج، أو تكرار نتائج البحث، وكذلك في حوافز البحث. المصطلحات ذات الصِّلة:",
                "related_terms": [],
                "references": "Ioannidis, J. P., Fanelli, D., Dunne, D. D., & Goodman, S. N. (2015). Meta-research: Eevaluation and improvement of research methods and practices. PLoS Biology, 13(10), e1002264. https://doi.org/10.1371/journal.pbio.1002264\n\nPeterson, D., & Panofsky, A. (2020). Metascience as a scientific social movement. https://doi.org/10.31235/osf.io/4dsqa",
                "drafted_by": [
                    "Elizabeth Collins"
                ],
                "reviewed_by": [
                    "Tamara Kalandadze",
                    "Lisa Spitzer",
                    "Olmo van den Akker"
                ],
                "alt_related_terms": [
                    null
                ],
                "language": "arabic"
            },
            {
                "type": "glossary",
                "title": "computational النُّموذج الحاسوبيّ (Model (computational))",
                "definition": "تهدف النَّماذج الحاسوبيَّة إلى ترجمة الظَّواهر قيد  الدِّراسة رياضيًاً بشكل حسابي؛ لفهم السُّلوكيَّات المعقَّدة وتفسيرها، والتَّنبؤ بها بشكلٍ أفضل.  **المصطلحات ذات الصِّلة:** خوارزميّات، محاكاة البيانات، الفرضيَّة، النَّظريَّة، بناء النَّظريَّة",
                "related_terms": [
                    "algorithms",
                    "data simulation",
                    "hypothesis",
                    "theory",
                    "theory building"
                ],
                "references": "Guest, O., & Martin, A. E. (2020). How computational modeling can force theory building in psychological science. Perspectives on Psychological Science. https://doi.org/10.1177/1745691620970585\n\nWilson, R. C., & Collins, A. G. (2019). Ten simple rules for the computational modeling of behavioral data. eLife, 8, e49547. https://doi.org/10.7554/eLife.49547",
                "drafted_by": [
                    "Charlotte R. Pennington"
                ],
                "reviewed_by": [
                    "Jamie P. Cockcroft",
                    "Mahmoud Elsherif",
                    "Meng Liu",
                    "Yu-Fang Yang",
                    "Michele C. Lim"
                ],
                "alt_related_terms": [
                    null
                ],
                "language": "arabic"
            },
            {
                "type": "glossary",
                "title": "statistical النَّموذج الإحصائيّ (Model (statistical))",
                "definition": "تمثيل رياضي للبيانات المرصودة؛ بهدف عكس المجتمع البحثي قيد الدِّراسة، ممَّا يسمح بفهم أفضل للظَّاهرة محل الاهتمام، وكذلك تحديد العلاقات بين المتغيِّرات، أو التَّنبؤ بالحالات المستقبليَّة.  مثال شائع هو استخدام اختبار مربع كاي (Chi square) لفهم العلاقة بين التَّدخين والسَّرطان (Doll & Hill, 1954\\)**.**  **المصطلحات ذات الصِّة:** الاستدلال البايزي، النَّموذج الحاسوبي  ، النَّموذج الفلسفي ، اختبار دلالة الفرضيَّة الصِّفريَّة. **التَّعريف البديل:** نموذج رياضيّ يجسِّد مجموعة من الافتراضات الإحصائيَّة المتعلِّقة بتوليد بيانات العيِّنة ويستخدم في تطبيق التَّحليل الإحصائيّ",
                "related_terms": [
                    "Bayesian Inference",
                    "Model (computational)",
                    "Model (philosophy)",
                    "Null Hypothesis Significance Testing (NHST) **Alternative definition:** A mathematical model that embodies a set of statistical assumptions concerning the generation of sample data and is used to apply statistical analysis."
                ],
                "references": "Doll, R., & Hill, A. B. (1954). The mortality of doctors in relation to their smoking habits; a preliminary report. British Medical Journal, 1(4877), 1451–1455. https://doi.org/10.1136/bmj.1.4877.1451",
                "drafted_by": [
                    "Jamie P. Cockcroft"
                ],
                "reviewed_by": [
                    "Alaa AlDoh",
                    "Mahmoud Elsherif",
                    "Meng Liu",
                    "Catia M. Oliveira",
                    "Charlotte R. Pennington"
                ],
                "alt_related_terms": [
                    null
                ],
                "language": "arabic"
            },
            {
                "type": "glossary",
                "title": "philosophy النَّموذج الفلسفيّ (Model (philosophy))",
                "definition": "هو العمليَّة التي يتم من خلالها إضفاء الطَّابع الرَّسمي على الوصف اللَّفظي؛ لإزالة الغموض، مع تقييد الأبعاد التي يمكن أنْ تمتدَّ إليها النَّظريَّة؛ مما ينتج عنه اشتقاق النَّموذج من البيانات.  \"إنَّ العديد من النَّماذج العلميَّة هي نماذج تمثيليَّة: فهي تمثل جزءًا أو جانبًا مختارًا من العالم، وهو النِّظام المستهدف للنَّموذج\" (Frigg & Hartman, 2020). **المصطلحات ذات الصِّلة:** الفرضيَّة، النَّظريَّة،  بناء النَّظريَّة.",
                "related_terms": [
                    "Hypothesis",
                    "Theory",
                    "Theory building"
                ],
                "references": "Guest, O., & Martin, A. E. (2020). How computational modeling can force theory building in psychological science. Perspectives on Psychological Science. https://doi.org/10.1177/1745691620970585",
                "drafted_by": [
                    "Mahmoud Elsherif"
                ],
                "reviewed_by": [
                    "Charlotte R. Pennington",
                    "Michele C. Lim"
                ],
                "alt_related_terms": [
                    null
                ],
                "language": "arabic"
            },
            {
                "type": "glossary",
                "title": "الدِّراسات متعدِّدة المحلّلين (Multi-Analyst Studies)",
                "definition": "يجري التَّحليل في الدِّراسات الإحصائيَّة النَّموذجيَّة باحث واحد أو مجموعة بحثيَّة، وهذا يشكك في مدى تأثير اختيار نوع التَّحليل على النَّتائج.  في الدِّراسات متعدِّدة المحلّلين يقوم باحثان، أو أكثر بتحليل نفس سؤال، أو فرضيَّة البحث اعتمادًا على نفس مجموعة البيانات بشكل مستقل، ويري أكزل وزملاؤه (2021) أنَّ أسلوب تعدُّد محللي البيانات قد يفيد في زيادة ثقتنا بنتيجة معيّنة، وكشف تأثير التَّفضيلات التَّحليليَّة بين المجموعات البحثيَّة، وتسليط الضَّوء على الاختلافات بين أساليب التَّحليل. **المصطلحات ذات الصِّلة:** المرونة التَّحليليَّة، العلوم الجماعية، تحليل البيانات، حديقة المسارات المتشعِّبة، تحليل الأكوان المتعدِّدة، درجات حريَّة الباحث، الشَّفافيَّة العلمية.",
                "related_terms": [
                    "Analytic flexibility",
                    "Crowdsourcing science",
                    "Data Analysis",
                    "Garden of Forking Paths",
                    "Multiverse Analysis",
                    "Researcher Degrees of Freedom",
                    "Scientific Transparency"
                ],
                "references": "Aczel, B., Szaszi, B., Nilsonne, G., Van den Akker, O., Albers, C. J., van Assen, M. A. L. M., ..., & Wagenmakers, E. (2021). Guidance for Multi-Analyst Studies. https://doi.org/10.31222/osf.io/5ecnh\n\nSilberzahn, R., Uhlmann, E. L., Martin, D. P., Anselmi, P., Aust, F., Awtrey, E., Bahník, Š., Bai, F., Bannard, C., Bonnier, E., & others. (2018). Many Analysts, One Data Set: Making Transparent How Variations in Analytic Choices Affect Results. Advances in Methods and Practices in Psychological Science, 337–356. https://doi.org/10.1177/2515245917747646",
                "drafted_by": [
                    "Sam Parsons"
                ],
                "reviewed_by": [
                    "Tsvetomira Dumbalska",
                    "Mahmoud Elsherif",
                    "William Ngiam",
                    "Charlotte R. Pennington",
                    "Graham Reid",
                    "Barnabas Szaszi",
                    "Flávio Azevedo"
                ],
                "alt_related_terms": [
                    null
                ],
                "language": "arabic"
            },
            {
                "type": "glossary",
                "title": "التَّعدديَّة (Multiplicity)",
                "definition": "التَّعريف: ه**و التَّضخُّم المحتمل لمعدلات الخطأ من النُّوع الأوّل (رفض الفرضيَّة الصِّفريَّة بطريقة غير صحيحة) بسبب الاختبارات الإحصائيَّة المتعدِّدة، مثلًا حين استعمال النَّتائج المتعدِّدة، أو نقاط المتابعة المتعدِّدة، أو تحليلات المجموعات الفرعيَّة المتعدِّدة.  وللتَّغلب على مشكلات التَّعدديَّة، غالبًا ما يطبِّق الباحثون إجراءات التَّحكم (مثل  Bonferroni ، Holm-Bonferroni ، Tukey) التي تصحِّح قيمة ألفا للتَّحكم في أخطاء النُّوع الأول المتضخِّمة. ومع ذلك، ومن خلال التَّحكم في أخطاء النَّوع الأول ، قد تزداد احتمالية حدوث أخطاء من النَّوع الثَّاني للباحث (أي قبول الفرضيَّة الصِّفريَّة بطريقة غير صحيحة).  **المصطلحات ذات الصِّلة:** ألفا، معدل الاكتشاف الخاطئ، مشكلة المقارنات المتعدِّدة، اختبار متعدِّد، اختبار دلالة الفرضيَّة الصِّفريَّة",
                "related_terms": [
                    "Alpha",
                    "False Discovery Rate",
                    "Multiple comparisons problem",
                    "Multiple testing",
                    "Null Hypothesis Significance Testing (NHST)"
                ],
                "references": "Sato, T. (1996). Type I and Type II error in multiple comparisons. The Journal of Psychology, 130(3), 293–302. https://doi.org/10.1080/00223980.1996.9915010\n\nSchulz, K. F., & Grimes, D. A. (2005). Multiplicity in randomised trials I: endpoints and treatments. The Lancet, 365(9470), 1591–1595. https://doi.org/10.1016/S0140-6736(05)66461-6",
                "drafted_by": [
                    "Aidan Cashin"
                ],
                "reviewed_by": [
                    "Jamie P. Cockcroft",
                    "Mahmoud Elsherif",
                    "Meng Liu",
                    "Charlotte R. Pennington"
                ],
                "alt_related_terms": [
                    null
                ],
                "language": "arabic"
            },
            {
                "type": "glossary",
                "title": "تحليل الأكوان المتعدِّدة (Multiverse analysis)",
                "definition": "تستند التَّحليلات متعدِّدة الأكوان إلى عمليَّات معالجة البيانات، والتَّحليل الإحصائي التي يمكن تبريرها بشكل متساوٍ، والتي يمكن استخدامها لاختبار فرضيَّة واحدة في تحليل البيانات متعدِّدة الأكوان. تتم معالجة مجموعة واحدة من البيانات الأوليَّة (الخام) في أكوان متعدِّدة من مجموعات البيانات من خلال تطبيق جميع الطُّرق الممكنة من خيارات المعالجة المسبقة المبرَّرة. تُطبّق تحليلات نموذج الأكوان المتعدِّدة نماذج إحصائيَّة مبرّرة بنفس القدر على نفس البيانات للإجابة على نفس الفرضيَّة. ثم يتم إجراء التَّحليل الإحصائي على جميع مجموعات البيانات في الكون المتعدِّد، ويعد تقريراً عن  جميع النَّتائج ممَّا يعزِّز الشَّفافيَّة، ويوضح متانة النَّتائج مقابل معالجة البيانات المختلِفة (البيانات متعدِّدة الأكوان) أو المعالجة الإحصائيَّة (النَّماذج متعدِّدة الأكوان). يختلف تحليل الأكوان المتعدِّدة عن تحليل منحنى المواصفات فيما يتعلَّق بالعروض الرُّسوميَّة (الرَّسم البياني، أو المدرج التِّكراري، ومخطَّط التَّجانب بدلًا عن مخطَّط منحنى المواصفات). **المصطلحات ذات الصِّلة:** حديقة المسارات المتشعِّبة، المتانة (في التَّحليلات)،  تحليل منحنى المواصفات،  اهتزاز التَّأثيرات.",
                "related_terms": [
                    "Garden of forking paths",
                    "Robustness (analyses)",
                    "Specification curve analysis",
                    "Vibration of effects"
                ],
                "references": "Del Giudice, M., & Gangestad, S. W. (2021). A traveler’s guide to the multiverse: Promises, pitfalls, and a framework for the evaluation of analytic decisions. Advances in Methods and Practices in Psychological Science, 4(1), 2515245920954925. https://doi.org/10.1177/2515245920954925\n\nSteegen, S., Tuerlinckx, F., Gelman, A., & Vanpaemel, W. (2016). Increasing Transparency through a Multiverse Analysis. Perspectives on Psychological Science, 11, 702–712. https://doi.org/10.1177/1745691616658637",
                "drafted_by": [
                    "Tina Lonsdorf; Flávio Azevedo"
                ],
                "reviewed_by": [
                    "Mahmoud Elsherif",
                    "Adrien Fillon",
                    "William Ngiam",
                    "Sam Parsons"
                ],
                "alt_related_terms": [
                    null
                ],
                "language": "arabic"
            },
            {
                "type": "glossary",
                "title": "مشكلة ازدواجية الاسم (Name Ambiguity Problem)",
                "definition": "هي مشكلة  الإسناد والتي تنشأ من مشكلتين ذات صلة بالموضوع: وهي أنَّ المؤلفين ربَّما يستخدمون أسماء، أو ألقاب متعدِّدة، لنشر أعمالهم، وربَّما يشترك عدَّة مؤلفين في مجال واحد في الاسم كاملًا. وهذا يجعل إثبات شخصيَّة المؤلِّفين بالأسماء والتَّخصُّصات أمرًا صعبًا. ويمكن تجاوز هذه المشكلة بإنشاء معرِّفات رقميَّة متميزة مطابقة للبصمة الرَّقمية مثل أوركيد (ORCID).  **المصطلحات ذات الصِّلة:** التَّأليف، معرّف الكائن الرَّقمي، الأوركيد",
                "related_terms": [
                    "Authorship",
                    "DOI (digital object identifier)",
                    "ORCID (Open Researcher and Contributor ID)"
                ],
                "references": "Wilson, B., & Fenner, M. (2012). Open Researcher & Contributor ID (ORCID): Solving the Name Ambiguity Problem. Educause Review - E-Content, 47(3), 54–55. https://er.educause.edu/articles/2012/5/open-researcher--contributor-id-orcid-solving-the-name-ambiguity-problem",
                "drafted_by": [
                    "Shannon Francis"
                ],
                "reviewed_by": [
                    "Tsvetomira Dumbalska",
                    "Mahmoud Elsherif",
                    "Helena Hartmann",
                    "Wanyin Li",
                    "Charlotte R. Pennington"
                ],
                "alt_related_terms": [
                    null
                ],
                "language": "arabic"
            },
            {
                "type": "glossary",
                "title": "NETANOS إخفاء هُويَّة النَّص المعتمد على الكيان للعلم المفتوح (Named entity-based Text Anonymization for Open Science (NETANOS))",
                "definition": "هو برنامج مجاني مفتوح المصدر؛ لإخفاء الهويَّة ويعمل على تحديد وتعديل الكيانات المذكورة (مثل الأشخاص، والمواقع، والأوقات، والتَّواريخ). وتتمثَّل ميزته الرَّئيسيَّة في أنَّه يحافظ على السِّياق الدَّقيق اللَّازم للتَّحليلات الثَّانويَّة.  والهدف هو مساعدة الباحثين في مشاركة بياناتهم النَّصيَّة الأوليَّة ، مع الالتزام بأخلاقيات البحث.  **المصطلحات ذات الصِّلة:** التّعمية، السِّريَّة، مشاركة البيانات، أخلاقيَّات البحث",
                "related_terms": [
                    "Anonymity",
                    "Confidentiality",
                    "Data sharing",
                    "Research ethics"
                ],
                "references": "Kleinberg, B., Mozes, M., van der Toolen, Y., & Verschuere, B. (2017). NETANOS - Named entity-based Text Anonymization for Open Science. https://osf.io/w9nhb/",
                "drafted_by": [
                    "Norbert Vanek"
                ],
                "reviewed_by": [
                    "Jamie P. Cockcroft",
                    "Aleksandra Lazić",
                    "Charlotte R. Pennington",
                    "Sam Parsons"
                ],
                "alt_related_terms": [
                    null
                ],
                "language": "arabic"
            },
            {
                "type": "glossary",
                "title": "NIRO-SR المراجعات المنهجيَّة غير التَّدخليَّة المفتوحة، والقابلة للتِّكرار (Non-Intervention, Reproducible, and Open Systematic Reviews (NIRO-SR))",
                "definition": "مجموعة شاملة من الأدوات؛ لتسهيل التَّطوير، والتَّسجيل المسبق، ونشر مراجعات الأدبيَّات المنهجيَّة للبحوث غير التَّدخليَّة. يمثل الجزء (أ) مبادئ توجيهيَّة (إرشادات) مفصَّلة لإنشاء إجراءات مراجعة منهجيَّة وتسجيله مسبقًا في سياق بحث غيرتدخلي أثناء التَّحضير للشَّفافيَّة، ويمثِّل الجزء (ب) مبادئ توجيهيَّة (إرشادات) لكتابة المراجعة المنهجيَّة المنجزة، مع التَّركيز على تعزيز إمكانيَّة التِّكرار.  **المصطلحات ذات الصِّلة:** تراكم المعرفة، المراجعة المنهجيَّة، بروتوكول المراجعة المنهجيَّة.",
                "related_terms": [
                    "Knowledge accumulation",
                    "Systematic review",
                    "Systematic Review Protocol"
                ],
                "references": "Topor, M., Pickering, J. S., Barbosa Mendes, A., Bishop, D. V. M., Büttner, F. C., Elsherif, M. M., & others. (2021). An integrative framework for planning and conducting Non-Intervention, Reproducible, and Open Systematic Reviews (NIRO-SR). https://doi.org/10.31222/osf.io/8gu5z",
                "drafted_by": [
                    "Asma Assaneea"
                ],
                "reviewed_by": [
                    "Tsvetomira Dumbalska",
                    "Thomas Rhys Evans",
                    "Tamara Kalandadze",
                    "Jade Pickering",
                    "Mirela Zaneva"
                ],
                "alt_related_terms": [
                    null
                ],
                "language": "arabic"
            },
            {
                "type": "glossary",
                "title": "NHST اختبار دلالة الفرضيَّة الصِّفريَّة (Null Hypothesis Significance Testing (NHST))",
                "definition": "يستخدم المنهج التِّكراري للاستنتاج لفحص احتماليَّة أن يكون هناك تأثير مرصود  ضد الفرضيَّة الصِّفرية التي تفترض عدم وجود علاقة أو تأثير (Pernet, 2015). ويتم الوصول لهذا الاستنتاج من خلال رقم استدلالي يسمى قيمة ألفا. وعلى وجه التَّحديد، يستنتج الباحثون وجود تأثير ما إذا تحقَّقت شروط القيمة الحدية لألفا التي وضعت مسبقا من قبلهم، وهذا ما يحدِّد المستوى المقبول من الاحتمالية وكذلك يكون قريبًا جدًا من الخطأ من النَّوع الأوّل.  ال**مصطلحات ذات الصِّلة:** الاستدلال، قيمة ألفا (p)، الدِّلالة الإحصائية، الخطأ من النَّوع الأوّل.",
                "related_terms": [
                    "Inference",
                    "P-value",
                    "Statistical significance",
                    "Type I error"
                ],
                "references": "Lakens, D., Scheel, A. M., & Isager, P. M. (2018). Equivalence testing for psychological research: A tutorial. Advances in Methods and Practices in Psychological Science, 1(2), 259–269. https://doi.org/10.1177/2515245918770963\n\nPernet, C. R. (2015). Null hypothesis significance testing: a short tutorial. F1000Research, 4, 621. https://doi.org/10.12688/f1000research.6963.3\n\nSpence, J. R., & Stanley, D. J. (2018). Concise, simple, and not wrong: In search of a short-hand interpretation of statistical significance. Frontiers in Psychology, 9, 2185. https://doi.org/10.3389/fpsyg.2018.02185",
                "drafted_by": [
                    "Alaa AlDoh"
                ],
                "reviewed_by": [
                    "Jamie P. Cockcroft",
                    "Annalise A. LaPlume",
                    "Charlotte R. Pennington",
                    "Sonia Rishi"
                ],
                "alt_related_terms": [
                    null
                ],
                "language": "arabic"
            },
            {
                "type": "glossary",
                "title": "الموضوعيَّة (Objectivity)",
                "definition": "تشير الموضوعيَّة إلى أنَّه يجب أنْ تكون الادِّعاءات، والطُّرق، والنَّتائج العلميَّة غير منحازة، وذلك يشمل عدم انحياز العلماء أنفسهم، وبالتَّالي عدم تأثرهم بالتَّحيُّز الثَّقافي، أو السِّياسي، أو العرقي، أو الدِّيني وكذلك أي مصالح شخصيَّة (ميرتون ، 1942).  **المصطلحات ذات الصِّة:** التَّشاركيَّة، قواعد ميرتون، الحياديَّة",
                "related_terms": [
                    "Communality",
                    "Mertonian norms",
                    "Neutrality"
                ],
                "references": "Macfarlane, B., & Cheng, M. (2008). Communism, Universalism and Disinterestedness: Re-examining Contemporary Support among Academics for Merton’s Scientific Norms. Journal of Academic Ethics, 6(1), 67–78. https://doi.org/10.1007/s10805-008-9055-y\n\nMerton, R. K. (1942). A note on science and democracy. Journal of Legal and Political Sociology, 1, 115–126. https://doi.org/10.1515/9783110375008-013",
                "drafted_by": [
                    "Ryan Millager"
                ],
                "reviewed_by": [
                    "Mahmoud Elsherif",
                    "Madeleine Ingham",
                    "Kai Krautter",
                    "Sam Parsons",
                    "Charlotte R. Pennington"
                ],
                "alt_related_terms": [
                    null
                ],
                "language": "arabic"
            },
            {
                "type": "glossary",
                "title": "Artificial Intelligence علم الوجود الذَّكاء الاصطناعي (Ontology (Artificial Intelligence))",
                "definition": "مجموعة من البديهيات في أي مجال، والتي  تساعد على تصنيف طبيعة العناصر تحت الدِّراسة، وشرح العلاقة بينها.  **المصطلحات ذات الصِّلة**: علم وظائف العناصر، نظريَّة المعرفة، علم التَّصنيف.",
                "related_terms": [
                    "Axiology",
                    "Epistemology",
                    "Taxonomy"
                ],
                "references": "Noy, N. F., & McGuinness, D. L. (2001). Ontology development 101: A guide to creating your first ontology. https://corais.org/sites/default/files/ontology_development_101_aguide_to_creating_your_first_ontology.pdf",
                "drafted_by": [
                    "Emma Norris"
                ],
                "reviewed_by": [
                    "Charlotte R. Pennington",
                    "Graham Reid"
                ],
                "alt_related_terms": [
                    null
                ],
                "language": "arabic"
            },
            {
                "type": "glossary",
                "title": "الوصول المفتوح (Open access)",
                "definition": "هو \"التَّوفر المجّاني للمعرفة على الإنترنت، ممَّا يسمح لأي مستخدم بقراءة النُّصوص الكاملة للمقالات البحثيَّة، أو تنزيلها، أو نسخها، أو توزيعها، أو طباعتها، أو البحث عنها، أو الارتباط بها، أو فهرستها، أو استخدامها كبيانات في البرامج، أو استخدامها لأي غرض قانوني آخر بدون عوائق ماليَّة، أو قانونيَّة، أو تقنيَّة، بخلاف من لا يمكنه الوصول إلى الإنترنت بحد ذاته\" (Boai, 2002). وغالبًا ما تستخدم الألوان للإشارة للطُّرق المختلفة للوصول المفتوح، بما في ذلك: الوصول المفتوح الأخضر (وذلك عندما يكون العمل متاحًا بشكل مفتوح من مستودع عام)، والوصول المفتوح الذَّهبي (وذلك عندما يكون العمل متاحًا مباشرة عند النَّشر عبر موقع المجلة على الويب)، والوصول المفتوح البلاتينيوم (أو الألماسي) (وهو فرع من الوصول المفتوح الذَّهبي حيث يمكن الوصول إلى جميع الأعمال في المجلَّة فورًا بعد النَّشر من موقع المجلَّة على الويب دون حاجة المؤلِّفين إلى دفع رسوم نشر المقالة).  **المصطلحات ذات الصِّلة:** رسوم النَّشر، مبادئ فير، حاجز مالي، الطّّباعة الأوليَّة، مستودع البيانات.",
                "related_terms": [
                    "Article Processing Charge",
                    "FAIR principles",
                    "Paywall",
                    "Preprint",
                    "Repository"
                ],
                "references": "Budapest Open Access Initiative. (2002). Read the Budapest open access initiative. Retrieved from https://www.budapestopenaccessinitiative.org/read\n\nSuber, P. (2015). Open Access Overview. http://legacy.earlham.edu/~peters/fos/overview.htm",
                "drafted_by": [
                    "Mahmoud Elsherif"
                ],
                "reviewed_by": [
                    "Nick Ballou",
                    "Helena Hartmann",
                    "Aoife O’Mahony",
                    "Ross Mounce",
                    "Mariella Paul",
                    "Charlotte R. Pennington"
                ],
                "alt_related_terms": [
                    null
                ],
                "language": "arabic"
            },
            {
                "type": "glossary",
                "title": "النَّص البرمجي المفتوح (Open Code)",
                "definition": "إتاحة النَّص البرمجي للحاسوب (مثل البرمجة، والنص البرمجي للتَّحليل، وتوليد المحفِّزات) مجانًا للجمهور من أجل جعل منهجيَّة البحث، والتَّحليل واضحة، والسَّماح بإعادة الإنتاج والتشارك البحثي.  ويمكن إتاحة  الرمز (الكود) عبر مواقع الويب ذات البرمجيات المفتوحة، مثل GitHub و Open Science Framework و Codeshare (على سبيل المثال لا الحصر) ، مما يتيح للآخرين تقييم الأخطاء وتصحيحها وإعادة استخدام وتعديل الكود للبحث اللاحق.  المصطلحات ذات الصِّلة: الاستنساخ الحسابي، الوصول المفتوح، التَّراخيص المفتوحة، المواد المفتوحة، المصدر المفتوح، برنامج مفتوح المصدر، قابليَّة إعادة الإنتاج، بناء الجملة.",
                "related_terms": [
                    "Computational Reproducibility",
                    "Open Access",
                    "Open Licensing",
                    "Open Material",
                    "Open Source",
                    "Open Source Software",
                    "Reproducibility",
                    "Syntax"
                ],
                "references": "Easterbrook, S. M. (2014). Open code for open science? Nature Geoscience, 7(11), 779–781. https://doi.org/10.1038/ngeo2283",
                "drafted_by": [
                    "Charlotte R. Pennington"
                ],
                "reviewed_by": [
                    "Elizabeth Collins",
                    "Mahmoud Elsherif",
                    "Christopher Graham",
                    "Emma Henderson"
                ],
                "alt_related_terms": [
                    null
                ],
                "language": "arabic"
            },
            {
                "type": "glossary",
                "title": "البيانات المفتوحة (Open Data)",
                "definition": "تشير البيانات المفتوحة إلى البيانات المتاحة مجانًا، والتي تمكّن الآخرون من الوصول إليها بسهولة للاستخدام دون قيود.  \"يمكن استخدام البيانات والمحتوى المفتوح وتعديله ومشاركته مجانًا من قبل أي شخص لأي غرض\" ([https://opendefinition.org/](https://opendefinition.org/)). وتخضع البيانات المفتوحة لمتطلَّبات السِّمة، والمشاركة على حد سواء. ولذلك فإنَّه من المهم النَّظر في التَّراخيص المفتوحة المناسبة. ويمكن حظر مجموعة البيانات الحساسة، أو الحساسة للوقت، أو مشاركتها مع خيارات وصول أكثر انتقائيَّة لضمان سلامة البيانات. **المصطلحات ذات الصِّلة:** الشَّارات (العلم المفتوح)، توافر البيانات، مبادئ فير، البيانات الوصفيَّة، التَّراخيص المفتوحة، المواد المفتوحة، قابليَّة إعادة النَّتائج، تحليل البيانات الثَّانويَّة",
                "related_terms": [
                    "Badges (Open Science)",
                    "Data availability",
                    "FAIR principles",
                    "Metadata",
                    "Open Licenses",
                    "Open Material",
                    "Reproducibility",
                    "Secondary data analysis"
                ],
                "references": "Definition, T. O. (n.d.). The Open Definition—Open Definition—Defining Open in Open Data, Open Content and Open Knowledge. Open Knowledge Foundation. https://opendefinition.org/\n\nHandbook, O. D. (n.d.). What is Open Data? Retrieved 9 July 2021. https://opendatahandbook.org/guide/en/what-is-open-data/",
                "drafted_by": [
                    "Lisa Spitzer"
                ],
                "reviewed_by": [
                    "Sarah Ashcroft-Jones",
                    "Mahmoud Elsherif",
                    "Helena Hartmann",
                    "Matt Jaquiery",
                    "Flávio Azevedo",
                    "Ross Mounce",
                    "Charlotte R. Pennington",
                    "Steven Verheyen"
                ],
                "alt_related_terms": [
                    null
                ],
                "language": "arabic"
            },
            {
                "type": "glossary",
                "title": "OERs المصادر التَّعليميَّة المفتوحة (Open Educational Resources (OERs))",
                "definition": "هي المواد التَّعليميَّة التي يمكن تعديلها، وتحسينها؛ لأنَّ منشئيها منحوا الإذن للآخرين للقيام بذلك. يتنازل الأفراد، أو المنظّمات التي تنشئ موارد تعليميَّة مفتوحة، والتي يمكن أن تتضمَّن مواد مثل شرائح العرض التَّقديمي، والبودكاست، والمناهج الدِّراسيَّة، والصُّور، وخطط الدُّروس، ومقاطع فيديوهات المحاضرات، والخرائط، وأوراق العمل، وحتى الكتب المدرسيَّة بأكملها \\- عن بعض (إن لم يكن كل) حقوق النشر المرتبطة بأعمالهم- عادةً عن طريق الأدوات القانونية مثل تراخيص المشاع الإبداعي، بحيث يمكن للآخرين الوصول إليها وإعادة استخدامها وترجمتها وتعديلها بحُرية.  **المصطلحات ذات الصِّلة:** إمكانية الوصول، فورت، الوصول المفتوح، التَّراخيص المفتوحة، المواد المفتوحة",
                "related_terms": [
                    "Accessibility",
                    "FORRT",
                    "Open access",
                    "Open Licenses",
                    "Open Material"
                ],
                "references": "Opensource.Com. (n.d.). What is open education? Retrieved 9 July 2021. https://opensource.com/resources/what-open-education",
                "drafted_by": [
                    "Aleksandra Lazić"
                ],
                "reviewed_by": [
                    "Sam Parsons",
                    "Charlotte R. Pennington",
                    "Steven Verheyen",
                    "Elizabeth Collins"
                ],
                "alt_related_terms": [
                    null
                ],
                "language": "arabic"
            },
            {
                "type": "glossary",
                "title": "OER Commons مصادر التعَّلُّم المفتوحة العامّة (Open Educational Resources (OER) Commons)",
                "definition": "مصادر التَّعلُّم المفتوحة هي مكتبة مجانيَّة على الإنترنت تُمكِّن المعلِّمين من إنشاء، ومشاركة ودمج المصادر التَّعليميَّة.  تهدف حركة مصادر التَّعلُّم المفتوح إلى تحفيز\"التعليم والتَّعلُّم التَّعاوني\" ([https://www.oercommons.org/about](https://www.oercommons.org/about)) كما تقدِّم موارد تعليميَّة ذات جودة عالية متاحة للجميع. **المصطلحات ذات الصِّلة:** العدالة، فورت، الشمول، قاعدة المعرفة  للمنح الدِّراسية المفتوحة، إطار العلوم المفتوحة.",
                "related_terms": [
                    "Equity",
                    "FORRT",
                    "Inclusion",
                    "Open Scholarship Knowledge Base",
                    "Open Science Framework"
                ],
                "references": "OER Commons. (n.d.). OER Commons. https://www.oercommons.org/",
                "drafted_by": [
                    "Ali H. Al-Hoorie"
                ],
                "reviewed_by": [
                    "Mahmoud Elsherif, Gisela H. Govaart"
                ],
                "alt_related_terms": [
                    null
                ],
                "language": "arabic"
            },
            {
                "type": "glossary",
                "title": "التَّراخيص المفتوحة (Open Licenses)",
                "definition": "تُلحق التَّراخيص المفتوحة بالبيانات، والبرامج المفتوحة (مثل النَّص البرمجي للتَّحليل)؛ لتحديد كيف يمكن للآخرين (إعادة) استخدام المواد المرخّصة. فعند تحديد الأذونات والقيود، غالبًا ما تسمح التَّراخيص المفتوحة بالوصول غير المقيد إلى العمل الأصلي للمؤلف وإعادة استخدامه. عادةً ما يتم ترخيص مجموعات البيانات بموجب نوع من التَّرخيص المفتوح المعروف باسم ترخيص المشاع الإبداعي، على سبيل المثال: معهد ماساتشوستس للتكنولوجيا، وأباتشي، والتَّرخيص العام). يمكن أنْ تختلف هذه التَّراخيص في بعض التَّفاصيل، فمثلًا تلزم التَّراخيص العاملة (وأشكالها المختلفة) كونها تراخيص الحقوق المتروكة التي تتطلَّب أن يتم ترخيص أي عمل مشتق بموجب نفس شروط العمل الأصلي. **المصطلحات ذات الصِّلة:** رخصة المشاع الإبداعي،  الحقوق المتروكة  حقوق النَّشر،  رخصة،  البيانات المفتوحة،  المصدر المفتوح.",
                "related_terms": [
                    "Creative Commons (CC) License",
                    "Copyleft",
                    "Copyright",
                    "Licence",
                    "Open Data",
                    "Open Source"
                ],
                "references": "",
                "drafted_by": [
                    "Andrew J. Stewart"
                ],
                "reviewed_by": [
                    "Elizabeth Collins",
                    "Sam Parsons",
                    "Graham Reid",
                    "Steven Verheyen"
                ],
                "alt_related_terms": [
                    null
                ],
                "language": "arabic"
            },
            {
                "type": "glossary",
                "title": "المواد المفتوحة (Open Material)",
                "definition": "مشاركة المؤلِّف للمواد التي تم استخدامها في الدِّراسة، مثل أسئلة الاستبيان، والمواد التَّحفيزيَّة وبرامج التَّجارب. (Kidwell et al., 2016, p. 3). يتم نشر المواد القابلة للمشاركة رقميًا في مستودعات الوصول المفتوح، ممَّا يجعلها متاحة للجمهور، ويمكن الوصول إليها. وبحسب التَّرخيص، فإنَّه يمكن إعادة استخدام المواد من قبل مؤلِّفين آخرين في دراساتهم. يجب وصف المكونات غير القابلة للمشاركة رقميًا (مثل المواد البيولوجيَّة والمعدَّات) بتفاصيل كافية للسَّماح بإمكانيَّة التِّكرار. **المصطلحات ذات الصِّلة:** الشَّارات (العلم المفتوح)، مصداقيَّة الادِّعاءات العلميَّة، مبادئ فير،  الوصول المفتوح، النَّص البرمجي المفتوح، البيانات المفتوحة، قابليَّة إعادة الإنتاج ، الشَّفافيَّة.",
                "related_terms": [
                    "Badges (Open Science)",
                    "Credibility of scientific claims",
                    "FAIR principles",
                    "Open Access",
                    "Open Code",
                    "Open Data",
                    "Reproducibility",
                    "Transparency"
                ],
                "references": "Blohowiak, B. B., Cohoon, J., de Wit, L., Eich, E., Farach, F. J., Hasselman, F., & others. (2020). Badges to Acknowledge Open Practices. Retrieved from https://osf.io/tvyxz\n\nKidwell, M. C., Lazarević, L. B., Baranski, E., Hardwicke, T. E., Piechowski, S., Falkenberg, L. S., & Nosek, B. A. (2016). Badges to acknowledge open practices: A simple, low-cost, effective method for increasing transparency. PLoS Biology, 14(5), e1002456. https://doi.org/10.1371/journal.pbio.1002456",
                "drafted_by": [
                    "Lisa Spitzer"
                ],
                "reviewed_by": [
                    "Sam Parsons",
                    "Charlotte R. Pennington",
                    "Olly Robertson",
                    "Emily A. Williams",
                    "Flávio Azevedo"
                ],
                "alt_related_terms": [
                    null
                ],
                "language": "arabic"
            },
            {
                "type": "glossary",
                "title": "المنصَّة العصبيَّة المفتوحة (OpenNeuro)",
                "definition": "هذه منصَّة مجانيَّة تمكِّن الباحثين من مشاركة بيانات تصوير الدِّماغ وتصفُّحها وتنزيلها وإعادة استخدامها بحريَّة وبشكل مفتوح، مثل: بيانات التَّصوير بالرَّنين المغناطيسي و MEG و EEG و iEEG و ECoG و ASL و PET).  **المصطلحات ذات الصِّلة:** بنية بيانات تصوير الدِّماغ، البيانات المفتوحة، التَّصوير بالرَّنين المغناطيسي المفتوح.",
                "related_terms": [
                    "BIDS data structure",
                    "Open data",
                    "OpenfMRI"
                ],
                "references": "Poldrack, R. A., Barch, D. M., Mitchell, J. P., Wager, T. D., Wagner, A. D., Devlin, J. T., Cumba, C., Koyejo, O., & Milham, M. P. (2013). Toward open sharing of task-based fMRI data: The OpenfMRI project. Frontiers in Neuroinformatics, 7, 1–12. https://doi.org/10.3389/fninf.2013.00012\n\nPoldrack, R. A., & Gorgolewski, K. J. (2014). Making big data open: Data sharing in neuroimaging. Nature Neuroscience, 17(11), 1510–1517. https://doi.org/10.1038/nn.3818\n\nOpenNeuro. (n.d.). A free and open platform for sharing MRI, MEG, EEG, iEEG, ECoG, ASL, and PET data—OpenNeuro. OpenNeuro. https://openneuro.org/",
                "drafted_by": [
                    "Ali H. Al-Hoorie"
                ],
                "reviewed_by": [
                    "Leticia Micheli, Gisela H. Govaart"
                ],
                "alt_related_terms": [
                    null
                ],
                "language": "arabic"
            },
            {
                "type": "glossary",
                "title": "التّحكيم المفتوح (Open Peer Review)",
                "definition": "آليَّة تحكيم علميَّة يُكشف فيها عن هويات المؤلِّف، أو المحكّم، أو كليهما، بالإضافة إلى تقارير المحكّمين، وخطابات قرار المحررين لبعضهم البعض، علنًا في أي وقت أثناء، أو بعد عمليَّة التحكيم أو النَّشر.  وقد يشير أيضًا إلى إزالة القيود المفروضة على من يمكنه المشاركة في التحكيم والمنصَّات اللَّازمة للقيام بذلك. وتجدر الإشارة أنَّ التحكيم المفتوح  قد استخدم بالتَّبادل للإشارة إلى أي من الممارسات المذكورة أعلاه أو جميعها. **المصطلحات ذات الصِّلة:** التحكيم من دون تعمية، العلم المفتوح، مبادرة انفتاح تحكيم الأقران؛ التحكيم الشَّفاف.",
                "related_terms": [
                    "Non-anonymised peer review",
                    "Open science",
                    "PRO (peer review openness) initiative",
                    "Transparent peer review"
                ],
                "references": "",
                "drafted_by": [
                    "Sonia Rishi"
                ],
                "reviewed_by": [
                    "Mahmoud Elsherif",
                    "Sam Parsons",
                    "Charlotte R. Pennington",
                    "Yuki Yamada",
                    "Flávio Azevedo"
                ],
                "alt_related_terms": [
                    null
                ],
                "language": "arabic"
            },
            {
                "type": "glossary",
                "title": "المعرفة المفتوحة (Open Scholarship)",
                "definition": "غالبًا ما يتم استخدام المعرفة المفتوحة بشكل مرادف لـ \"العلوم المفتوحة\" ، ولكنَّها تمتدُّ إلى جميع التَّخصُّصات ، مع الأخذ في الاعتبار تلك التي قد لا يتم تحديدها تقليديًا على أنها من ضمن العلوم. وتعكس مفهوم المعرفة المفتوحة أنَّ المعرفة بجميع أنواعها يجب أن تكون مشتركة بشكل مفتوح، وشفاف، وصارم، وقابل لإعادة الإنتاج، والتِّكرار، وتراكميَّة، وشاملة (تسمح لجميع أنظمة المعرفة). تشمل المعرفة المفتوحة جميع الأنشطة العلميَّة التي لا تقتصر فقط على البحث مثل التَّدريس، والبيداغوجيا. **المصطلحات ذات الصِّلة:** بروبنساينس، مقاومة الاستعمار، المعرفة، البحث المفتوح، العلم المفتوح",
                "related_terms": [
                    "Bropenscience",
                    "Decolonisation",
                    "Knowledge",
                    "Open Research",
                    "Open Science"
                ],
                "references": "Tennant, J., Beamer, J. E., Bosman, J., Brembs, B., Chung, N. C., Clement, G., Crick, T., Dugan, J., Dunning, A., Eccles, D., Enkhbayar, A., Graziotin, D., Harding, R., Havemann, J., Katz, D. S., Khanal, K., Kjaer, J. N., Koder, T., Macklin, P., & Turner, A. (2019). Foundations for Open Scholarship Strategy Development. MetaArXiv. https://doi.org/10.31222/osf.io/b4v8p",
                "drafted_by": [
                    "Gerald Vineyard"
                ],
                "reviewed_by": [
                    "Mahmoud Elsherif",
                    "Zoe Flack",
                    "Sam Parsons",
                    "Charlotte R. Pennington"
                ],
                "alt_related_terms": [
                    null
                ],
                "language": "arabic"
            },
            {
                "type": "glossary",
                "title": "قاعدة المعرفة للمنح الدِّراسيَّة المفتوحة (Open Scholarship Knowledge Base)",
                "definition": "قاعدة المعرفة المفتوحة (OSKB) هي مبادرة تعاونيَّة لمشاركة المعرفة بماذا، ولماذا، وكيف يكون العلم المفتوح؛ وذلك لتسهيل المعرفة بالعلم المفتوح وبتطبيقه، كما يتم اختيار، وإنشاء المعلومات وتنظيمها بدقَّة من قِبل المجتمع.  وتعدُّ قاعدة المعرفة المفتوحة مجتمعًا تابعًا لمركز العلم المفتوح. **المصطلحات ذات الصِّلة:** مركز العلم المفتوح ، المصادر التَّعليميَّة  المفتوحة، المعرفة المفتوحة،  العلم المفتوح.",
                "related_terms": [
                    "Center for Open Science (COS), Open Educational Resources (OERs)",
                    "Open scholarship",
                    "Open Science"
                ],
                "references": "",
                "drafted_by": [
                    "Ali H. Al-Hoorie"
                ],
                "reviewed_by": [
                    "Mahmoud Elsherif",
                    "Samuel Guay",
                    "Tamara Kalandadze"
                ],
                "alt_related_terms": [
                    null
                ],
                "language": "arabic"
            },
            {
                "type": "glossary",
                "title": "العلم المفتوح (Open Science)",
                "definition": "مصطلح شامل يعكس فكرة أنَّ المعرفة العلميَّة بجميع أنواعها، وحيثما كان ذلك مناسبًا، لابد أن تكون متاحة بشكل مفتوح، وشفاف، ودقيق، وقابل للتِّكرار، وإعادة الإنتاج، ومتراكم، وشامل.  وتعدُّ جميع هذه السِّمات أساسيَّة في المسعى العلمي. ويتكون العلم المفتوح من مبادئ وسلوكيات تعزِّز شفافية العلوم، والمصداقيَّة، والقابليَّة  للتِّكرار، وسهولة الوصول. يشتمل العلم المفتوح على ستة جوانب رئيسيَّة: البيانات المفتوحة، والمنهجيَّة المفتوحة، والمصدر المفتوح، والوصول المفتوح، والتّحكيم المفتوح، والموارد التَّعليميَّة المفتوحة. **المصطلحات ذات الصِّلة:** إمكانيَّة الوصول، المصداقيَّة، البيانات المفتوحة، المواد المفتوحة، تحكيم الأقران المفتوحة، البحث المفتوح، الممارسات العلميَّة  المفتوحة، المعرفة المفتوحة، أزمة إعادة الإنتاج (أو أزمة التِّكرار)، قابليَّة إعادة الإنتاج ، الشَّفافيَّة",
                "related_terms": [
                    "Accessibility",
                    "Credibility",
                    "Open Data",
                    "Open Material",
                    "Open Peer Review",
                    "Open Research",
                    "Open Science Practices",
                    "Open Scholarship",
                    "Reproducibility crisis (aka Replicability or replication crisis)",
                    "Reproducibility",
                    "Transparency"
                ],
                "references": "Abele-Brehm, A. E., Gollwitzer, M., Steinberg, U., & Schönbrodt, F. D. (2019). Attitudes toward open science and public data sharing. Social Psychology, 50, 252–260. https://doi.org/10.1027/1864-9335/a000384\n\nCrüwell, S., van Doorn, J., Etz, A., Makel, M. C., Moshontz, H., Niebaum, J. C., Orben, A., Parsons, S., & Schulte-Mecklenbeck, M. (2019). Seven Easy Steps to Open Science: An Annotated Reading List. Zeitschrift Für Psychologie, 227(4), 237–248. https://doi.org/10.1027/2151-2604/a000387\n\nKathawalla, U., Silverstein, P., & Syed, M. (2020). Easing into Open Science: A Guide for Graduate Students and Their Advisors. Collabra: Psychology. https://doi.org/10.31234/osf.io/vzjdp Retrieved from https://psyarxiv.com/vzjdp\n\nSyed, M. (2019). The Open Science Movement is for all of us. PsyArXiv.\n\nWoelfle, M., Olliaro, P., & Todd, M. H. (2011). Open science is a research accelerator. Nature Chemistry, 3(10), 745–748. https://doi.org/10.1038/nchem.1149",
                "drafted_by": [
                    "Mahmoud Elsherif"
                ],
                "reviewed_by": [
                    "Zoe Flack",
                    "Tamara Kalandadze",
                    "Charlotte R. Pennington",
                    "Qinyu Xiao"
                ],
                "alt_related_terms": [
                    null
                ],
                "language": "arabic"
            },
            {
                "type": "glossary",
                "title": "إطار العلوم المفتوحة (Open Science Framework)",
                "definition": "منصَّة مجانيَّة مفتوحة المصدر للباحثين؛ لتنظيم ومشاركة مشاريعهم البحثيَّة، ولتشجيع التَّعاون بينهم. وغالبًا ما تستخدم كمستودع مفتوح للنُّصوص البرمجيَّة للبحوث والبيانات، والمواد، والمطبوعات، والتَّسجيلات المسبقة مع إدارة أكثر كفاءة لسير العمل. وقد أنشأها وطوَّرها مركز العلوم المفتوحة.  **المصطلحات ذات الصِّلة:** أرشيف، مركز العلم المفتوح، النَّص البرمجي المفتوح، البيانات المفتوحة، الطّباعة الأوليَّة، التَّسجيل المسبق.",
                "related_terms": [
                    "Archive",
                    "Center for Open Science (COS)",
                    "Open Code",
                    "Open Data",
                    "Preprint",
                    "Preregistration"
                ],
                "references": "Foster, E. D., & Deardorff, A. (2017). Open science framework (OSF). Journal of the Medical Library Association, 105(2), 203. https://doi.org/10.5195/jmla.2017.88\n\nfor Open Science, C. (2011–2021). Open Science Framework. https://osf.io/",
                "drafted_by": [
                    "William Ngiam"
                ],
                "reviewed_by": [
                    "Mahmoud Elsherif",
                    "Charlotte R. Pennington",
                    "Lisa Spitzer"
                ],
                "alt_related_terms": [
                    null
                ],
                "language": "arabic"
            },
            {
                "type": "glossary",
                "title": "برنامج مفتوح المصدر (Open Source software)",
                "definition": "نوع من البرامج الحاسوبيَّة يتم فيه إصدار تعليمات برمجيَّة بموجب ترخيص يسمح للآخرين باستخدام البرنامج، وتغييره، وتوزيعه لأي شخص، ولأي غرض.  ما يميّز هذه البرامج ليس الوصول المفتوح فحسب، بل يجب أيضًا أن تتوافق شروط توزيع البرامج مفتوحة المصدر مع 10 معايير محدَّدة (انظر: https://opensource.org/osd).  **المصطلحات ذات الصِّلة:** GITHUB. الوصول المفتوح، النَّص البرمجي مفتوح، البيانات المفتوحة، التَّراخيص المفتوحة، بايثون، لغة الآر، مستودع البيانات",
                "related_terms": [
                    "Github",
                    "Open Access",
                    "Open Code",
                    "Open Data",
                    "Open Licenses",
                    "Python",
                    "R",
                    "Repository"
                ],
                "references": "Anon. (n.d.). Open Source in Open Science | FOSTER. Retrieved from https://www.fosteropenscience.eu/foster-taxonomy/open-source-open-science",
                "drafted_by": [
                    "Connor Keating"
                ],
                "reviewed_by": [
                    "Jamie P. Cockcroft",
                    "Helena Hartmann",
                    "Charlotte R. Pennington",
                    "Andrew J. Stewart"
                ],
                "alt_related_terms": [
                    null
                ],
                "language": "arabic"
            },
            {
                "type": "glossary",
                "title": "الغسل المفتوح (Open washing)",
                "definition": "يشير الغسل المفتوح إلى ادِّعاء الانفتاح لتقمّص الصَّرامة، والهيبة المرتبطة بالممارسات المفتوحة، وتمَّ استخدام هذا التَّعبير لتوصيف استراتيجيَّة التَّسويق لدى شركات البرمجيَّات التي تبدو مفتوحة المصدر، وذات تراخيص مفتوحة، ولكنَّها متورِّطة في ممارسات الملكيَّة.  ويعدُّ الغسيل المفتوح مصدر قلقٍ متزايدٍ لأولئك الذين يتبنَّون ممارسات علميَّة مفتوحة حيث يتم تقويض جهودهم من خلال هذه الاستخدامات المضلِّلة، بينما يتم اختزال الإجراءات المصمَّمة؛ لتسهيل التَّطورات التَّقدميَّة إلى \"وضع علامة على المربع\" دون مراقبة واضحة للجودة.  **المصطلحات ذات الصِّلة:** الوصول المفتوح ،البيانات المفتوحة، المصدر المفتوح",
                "related_terms": [
                    "Open Access",
                    "Open Data",
                    "Open Source"
                ],
                "references": "Farrow, R. (2017). Open Education and Critical Pedagogy. Learning, Media and Technology, 42(2), 130–146. https://doi.org/10.1080/17439884.2016.1113991\n\nMoretti, M. (2020). Beyond Open-washing: Are Narratives the Future of Open Data Portals? Medium Blog. https://medium.com/nightingale/beyond-open-washing-are-stories-and-narratives-the-future-of-open-data-portals-93228d8882f3\n\nVillum, C. (2016). “Open-washing” – The difference between opening your data and simply making them available. https://blog.okfn.org/2014/03/10/open-washing-the-difference-between-opening-your-data-and-simply-making-them-available/\n\nVlaeminck, S., & Podkrajac, F. (2017). Journals in Economic Sciences: Paying Lip Service to Reproducible Research? IASSIST Quarterly, 41(1–4), 16. https://doi.org/10.29173/iq6",
                "drafted_by": [
                    "Meng Liu"
                ],
                "reviewed_by": [
                    "Thomas Rhys Evans",
                    "Sam Guay",
                    "Sam Parsons",
                    "Charlotte R. Pennington",
                    "Beatrice Valentini"
                ],
                "alt_related_terms": [
                    null
                ],
                "language": "arabic"
            },
            {
                "type": "glossary",
                "title": "الإيقاف الاختياري (Optional Stopping)",
                "definition": "ممارسة تحليل البيانات بشكلٍ متكرِّر أثناء عمليَّة جمع البيانات، واتِّخاذ قرار بإيقاف جمع البيانات إذا وصل معيار إحصائي (مثل قيمة الاحتماليَّة، أو عامل بايز) إلى حدٍّ معيّن.  وإذا تم اتِّخاذ الاحتياطات المنهجيَّة المناسبة للتَّحكُّم في معدَّل الخطأ من النُّوع الأوَّل، فقد يكون هذا إجراءً فعالًا (مثل Lakens ، 2014). ومع ذلك ، بدون الإبلاغ الشَّفاف، أو التَّحكُّم المناسب في الخطأ ، يمكن أن يزيد الخطأ من النُّوع الأوَّل بشكل كبير، ويمكن اعتبار التوقف الاختياري ممارسة بحث مشكوك فيها، أو شكلًا من أشكال قرصنة القيمة الاحتماليَّة.  **المصطلحات ذات الصِّلة:** قرصنة القيمة الاحتماليَّة، ممارسات البحث المشكوك فيها، أو ممارسات إعداد التَّقارير المشكوك فيها، الاختبار المتسلسل",
                "related_terms": [
                    "*P*\\-hacking",
                    "Questionable Research Practices or Questionable Reporting Practices (QRPs)",
                    "Sequential testing"
                ],
                "references": "Beffara Bret, B., Beffara Bret, A., & Nalborczyk, L. (2021). A fully automated, transparent, reproducible, and blind protocol for sequential analyses. Meta-Psychology, 5. https://doi.org/10.15626/MP.2018.869\n\nLakens, D. (2014). Performing high-powered studies efficiently with sequential analyses. European Journal of Social Psychology, 44(7), 701–710. https://doi.org/10.1002/ejsp.2023\n\nSagarin, B. J., Ambler, J. K., & Lee, E. M. (2014). An ethical approach to peeking at data. Perspectives on Psychological Science, 9(3), 293–304. https://doi.org/10.1177/1745691614528214\n\nSchönbrodt, F. D., Wagenmakers, E.-J., Zehetleitner, M., & Perugini, M. (2017). Sequential hypothesis testing with Bayes factors: Efficiently testing mean differences. Psychological Methods, 22(2), 322–339. https://doi.org/10.1037/met0000061",
                "drafted_by": [
                    "Brice Beffara Bret; Bettina M. J. Kern"
                ],
                "reviewed_by": [
                    "Ali H. Al-Hoorie",
                    "Helena Hartmann",
                    "Catia M. Oliveira",
                    "Sam Parsons"
                ],
                "alt_related_terms": [
                    null
                ],
                "language": "arabic"
            },
            {
                "type": "glossary",
                "title": "Open Researcher and Contributor ID الأوركيد (ORCID (Open Researcher and Contributor ID))",
                "definition": "هي منظمة توفِّر سجل ثابت لمعرِّفات فريدة للباحثين، والعلماء بما يسمح لهم بربط وثائق بحوثهم الرَّقميَّة، ومساهماتهم الأخرى بسجل الأوركيد.  يساعد هذا الإجراء في تجنُّب التباس الأسماء في مجال الاتِّصال العلمي. مُعرِّفات الأوركيد توفِّر معرِّفًا فريدًا ثابتًا دائمًا؛ لربط الباحثين بأعمالهم. يمكن الاشتراك مجانًا عن طريق : [https://orcid.org/register](https://orcid.org/register) **المصطلحات ذات الصِّلة:**  التَّأليف، معرِّف الكائن الرَّقميَّ، مشكلة ازدواجيَّة الاسم.",
                "related_terms": [
                    "Authorship",
                    "DOI (digital object identifier)",
                    "Name Ambiguity Problem"
                ],
                "references": "Haak, L. L., Fenner, M., Paglione, L., Pentz, E., & Ratner, H. (2012). ORCID: A system to uniquely identify researchers. Learned Publishing, 25(4), 259–264. https://doi.org/10.1087/20120404\n\nORCID. (n.d.). ORCID. https://orcid.org/",
                "drafted_by": [
                    "Martin Vasilev"
                ],
                "reviewed_by": [
                    "Bradley Baker",
                    "Mahmoud Elsherif",
                    "Shannon Francis",
                    "Charlotte R. Pennington",
                    "Emily A. Williams",
                    "Flávio Azevedo"
                ],
                "alt_related_terms": [
                    null
                ],
                "language": "arabic"
            },
            {
                "type": "glossary",
                "title": "المجلَّات التَّركيبيَّة (Overlay Journal)",
                "definition": "هي مجلَّات علميَّة إلكترونيَّة ذات الوصول المفتوح، تقوم بتجميع، وتنظيم المقالات المتاحة من مصادر أخرى (عادةً خوادم ما قبل الطِّباعة مثل arXiv). قد تتضمَّن معالجة  المقالات تحكيم الأقران بعد النَّشر، أو لجان التَّحكيم.  لا تهدف المجلَّات التَّركيبيَّة المفتوحة إلى نشر مواد جديدة، ولكنَّها تهدف إلى تنظيم، وتجميع المقالات المتاحة الموجودة في المستودعات الرَّقميَّة. **المصطلحات ذات الصِّلة:** الوصول المفتوح ، الطّباعة الأوليَّة",
                "related_terms": [
                    "Open access",
                    "Preprint"
                ],
                "references": "Ginsparg, P. (1997). Winners and losers in the global research village. The Serials Librarian, 30(3–4), 83–95. https://doi.org/10.1300/J123v30n03_13\n\nGinsparg, P. (2001). Creating a global knowledge network. In Second Joint ICSU Press-UNESCO Expert Conference on Electronic Publishing in Science (pp. 19–23).\n\nBrown, J. (2010). An Introduction to Overlay Journals [Techreport]. Repositories Support Project: UK. https://discovery.ucl.ac.uk/id/eprint/19081/",
                "drafted_by": [
                    "Bradley Baker"
                ],
                "reviewed_by": [
                    "Christopher Graham",
                    "Helena Hartmann",
                    "Sam Parsons",
                    "Charlotte R. Pennington"
                ],
                "alt_related_terms": [
                    null
                ],
                "language": "arabic"
            },
            {
                "type": "glossary",
                "title": "منحنى القيم الاحتماليَّة (P-curve)",
                "definition": "أداة لتحديد تحيُّز النَّشر المحتمل، والتي تستعمل توزيع القيم الاحتماليَّة ذات الدِّلالة الإحصائيَّة في سلسلة من النَّتائج المستقلَّة.  يمكن استعمال الانحراف عن التَّوزيع المتوقَّع لتقييم وجود ودرجة تحيُّز النَّشر، فإذا كان المنحنى منحرفًا لليمين، فهناك قيم احتماليَّة أكثر انخفاضًا ذات دلالة إحصائيَّة، ممَّا يعكس وجود تأثير حقيقي، أمَّا إذا كان المنحنى منحرفًا إلى اليسار، فهناك العديد من النَّتائج ذات الدِّلالة الإحصائيَّة التي تقل عن مستوى 0.05 بقليل فقط.  يشير هذا إلى أنَّ الدِّراسات تفتقر إلى القيمة الإثباتيَّة وقد تكون مدعومة بممارسات بحثيَّة مشكوك فيها، مثل: قرصنة القيمة الاحتماليَّة  في حالة عدم وجود تأثير حقيقي (الفرضيَّة الصِّفريَّة حقيقيَّة) وعندما يكون الإبلاغ عن القيمة الاحتماليَّة غير متحيِّز، فيجب أن يكون منحنى القيم الاحتماليَّة خطًا أفقيًا مسطَّحًا، يمثل التَّوزيع النَّموذجي للقيم الاحتماليَّة.  **المصطلحات ذات الصِّلة:**  درج حفظ الملفَّات، الفرضيَّة، قرصنة القيمة الاحتماليَّة، القيمة الاحتماليَّة، تَّحيُّز النَّشر (مشكلة درج البيانات)، ممارسات البحث المشكوك فيها أو ممارسات إعداد التَّقارير المشكوك فيها، تقارير انتقائيَّة، منحني زد",
                "related_terms": [
                    "File-drawer",
                    "Hypothesis",
                    "*P*\\-hacking",
                    "*p*\\-value",
                    "Publication bias (File Drawer Problem)",
                    "Questionable Research Practices or Questionable Reporting Practices (QRPs)",
                    "Selective reporting",
                    "Z-curve"
                ],
                "references": "Bruns, S. B., & Ioannidis, J. P. (2016). P-curve and p-hacking in observational research. PLoS ONE, 11(2), e0149144. https://doi.org/10.1371/journal.pone.0149144\n\nSimonsohn, U., Nelson, L. D., & Simmons, J. P. (2014). P-curve: a key to the file-drawer. Journal of Experimental Psychology: General, 143(2), 534. https://doi.org/10.1037/a0030850\n\nSimonsohn, U., Nelson, L. D., & Simmons, J. P. (2014). P-curve and effect size: Correcting for publication bias using only significant results. Perspectives on Psychological Science, 9(6), 666–681. https://doi.org/10.1177/1745691614553988\n\nSimonsohn, U., Nelson, L. D., & Simmons, J. P. (2019). P-curve won’t do your laundry, but it will distinguish replicable from non-replicable findings in observational research: Comment on Bruns & Ioannidis (2016). PLoS ONE, 14(3), e0213454. https://doi.org/10.1371/journal.pone.0213454",
                "drafted_by": [
                    "Bettina M. J. Kern"
                ],
                "reviewed_by": [
                    "Sam Guay",
                    "Kamil Izydorczak",
                    "Charlotte R. Pennington",
                    "Robert M. Ross",
                    "Olmo van den Akker"
                ],
                "alt_related_terms": [
                    null
                ],
                "language": "arabic"
            },
            {
                "type": "glossary",
                "title": "قرصنة القيمة الاحتماليَّة (p*****-hacking *)",
                "definition": "هو أسلوب استغلالي قد يزيد بنحو غير طبيعي من احتماليَّة  الحصول على نتيجة ذات دلالة إحصائيَّة من خلال تحقيق معيار درجة الدلالة الإحصائيَّة اللَّازمة (عادة α \\= .05). مثال ذلك، القيام بعدة تحليلات، ونشر تلك التي تكون فيها القيمة الاحتماليَّة أقل من 0.05 فقط، أو إزالة بعض البيانات بشكل انتقائي حتى تكون القيمة الاحتماليَّة أقل من 0.05، أو اختيار بعض العوامل للتَّحليل بناء على ما إذا كانت المؤشِّرات ذات دلالة إحصائيَّة. **المصطلحات ذات الصِّلة:**  المرونة التَّحليلية ، التَّصيد ، حديقة المسارات المتشعِّبة ، الافتراض بعد معرفة النَّتائج، ممارسات البحث المشكوك فيها أو ممارسات إعداد التَّقارير المشكوك فيها ، تقرير انتقائي",
                "related_terms": [
                    "Analytic flexibility",
                    "Fishing",
                    "Garden of forking paths",
                    "HARKing",
                    "Questionable Research Practices or Questionable Reporting Practices (QRPs)",
                    "Selective reporting"
                ],
                "references": "Hardwicke, T. E., Jameel, L., Jones, M., Walczak, E. J., & Weinberg, L. M. (2014). Only human: Scientists, systems, and suspect statistics. Opticon1826, 16, 25. https://doi.org/10.5334/OPT.CH\n\nNeuroskeptic. (2012). The nine circles of scientific hell. Perspectives on Psychological Science, 7(6), 643–644. https://doi.org/10.1177/1745691612459519",
                "drafted_by": [
                    "Mahmoud Elsherif"
                ],
                "reviewed_by": [
                    "Tamara Kalandadze",
                    "William Ngiam",
                    "Sam Parsons",
                    "Martin Vasilev"
                ],
                "alt_related_terms": [
                    null
                ],
                "language": "arabic"
            },
            {
                "type": "glossary",
                "title": "القيمة الاحتماليّة (p*****-value *)",
                "definition": "التعريف:** رقم إحصائي يستخدم لتقييم نتائج الفرضيات في اختبار دلالة الفرضية الصفرية يشير إلى احتمال ملاحظة تأثير ما أو تأثير أكثر تطرفاً بافتراض صحة الفرضية الصفرية (Lakens, 2021b). ويشير بيان الجمعية الإحصائية الأمريكية (Wasserstein & Lazar, 2016) إلى أن القيم الاحتمالية ليست مؤشرًا على حقيقة الفرضية الصفرية؛ بل تعرف القيم الاحتمالية بهذه الطريقة: \"بشكل غير رسمي، القيمة الاحتمالية هي الاحتمالية بموجب نموذج إحصائي محدد ويكون الملخص الإحصائي للبيانات (على سبيل المثال، متوسط الفرق في ​​العينة بين مجموعتين مقارنتين) مساوياً أو أكثر تطرفاً من قيمته المرصودة\" (ص 131).  **المصطلحات ذات الصِّلة:** اختبار دلالة الفرضيَّة الصِّفريَّة، الدِّلالة الإحصائيَّة",
                "related_terms": [
                    "Null Hypothesis Statistical Testing (NHST)",
                    "statistical significance"
                ],
                "references": "psyTeachR Team. (n.d.). P | Glossary. psyTeachR. https://psyteachr.github.io/glossary\n\nLakens, D. (2021). The Practical Alternative to the p Value Is the Correctly Used p Value. Perspectives on Psychological Science, 16(3), 639–648. https://doi.org/10.1177/1745691620958012\n\nWasserstein, R. L., & Lazar, N. A. (2016). The ASA Statement on p-Values: Context, Process, and Purpose. The American Statistician, 70, 129–133. https://doi.org/10.1080/00031305.2016.1154108",
                "drafted_by": [
                    "Alaa AlDoh; Flávio Azevedo"
                ],
                "reviewed_by": [
                    "Jamie P. Cockcroft",
                    "Charlotte R. Pennington",
                    "Suzanne L. K. Stewart",
                    "Robbie C.M. van Aert",
                    "Marcel A.L.M. van Assen",
                    "Martin Vasilev"
                ],
                "alt_related_terms": [
                    null
                ],
                "language": "arabic"
            },
            {
                "type": "glossary",
                "title": "مصانع  الورق (Papermill)",
                "definition": "منظمة متورِّطة في سوء السُّلوك العلمي حيث يتم إنتاج بحوث متعدِّدة عن طريق تزوير، أو تلفيق البيانات، على سبيل المثال: عن طريق تعديل الأرقام، أو البيانات الرَّقميَّة، أو سرقة النُّصوص المكتوبة.  ومصانع الورق \"تُتهم بأنَّها تقدِّم منتجات تتراوح من بيانات البحث إلى المخطوطات المكتوبة الاحتياليَّة، أو المزِّيفة وخدمات التَّقديم\" (Byrne & Christopher, 2020, p. 583). تقوم مصانع الورق بالإنتاج السَّريع ونشر العديد من الأوراق التي تدَّعي بأنَّها جديدة.  غالبًا لا يتم اكتشافها في عملية النَّشر العلمي، وبالتَّالي لا يتم العثور عليها مطلقًا، أو يتم سحبها إذا تم اكتشافها، مثلًا: من خلال برامج الانتحال.  **المصطلحات ذات الصِّلة:** تزييف البيانات، تزوير البيانات، احتيال، إنتحال ، ممارسات البحث المشكوك فيها أو ممارسات إعداد التَّقارير المشكوك فيها، سوء السُّلوك العلمي، النَّشر العلمي.",
                "related_terms": [
                    "Data fabrication",
                    "Data falsification",
                    "Fraud",
                    "Plagiarism",
                    "Questionable Research Practices or Questionable Reporting Practices (QRPs)",
                    "Scientific misconduct",
                    "Scientific publishing"
                ],
                "references": "Byrne, J. A., & Christopher, J. (2020). Digital magic, or the dark arts of the 21st century—how can journals and peer reviewers detect manuscripts and publications from paper mills? FEBS Letters, 594(4), 583–589. https://doi.org/10.1002/1873-3468.13747\n\nHackett, R., & Kelly, S. (2020). Publishing ethics in the era of paper mills. Biology Open, 9(10), bio056556. https://doi.org/10.1242/bio.056556",
                "drafted_by": [
                    "Helena Hartmann"
                ],
                "reviewed_by": [
                    "Sarah Ashcroft-Jones",
                    "Elizabeth Collins",
                    "Mahmoud Elsherif",
                    "Charlotte R. Pennington"
                ],
                "alt_related_terms": [
                    null
                ],
                "language": "arabic"
            },
            {
                "type": "glossary",
                "title": "أشباه البيانات (Paradata)",
                "definition": "البيانات التي يتم التقاطها حول خصائص وسياق البيانات الأوليَّة التي تم جمعها من شخص ما،  وتختلف عن البيانات الوصفيَّة.  ويمكن استخدامها للتَّحقيق في تفاعل المجيب مع استطلاع ما، أو تجربة على المستوى الجزئي.  ويمكن جمعها بسهولة أكبر أثناء الاستطلاعات التي يتم إجراؤها بواسطة الحاسوب، ولكنَّها لا تقتصر عليها. وتشمل الأمثلة أوقات الاستجابة لأسئلة الاستطلاع، والأنماط المتكرِّرة للاستجابات، مثل اختيار نفس الإجابة لجميع الأسئلة، والخصائص السِّياقيَّة للمشارك، مثل: الإصابات التي تمنع الأداء الجيِّد في المهام، وعدد الاستجابات المبكِّرة للمحفِّزات في التَّجربة.  وتم استخدام أشباه البيانات للتَّحقيق، وتصحيح أخطاء القياس، وأخذ العيِّنات.  **المصطلحات ذات الصِّلة:** البيانات المساعدة، جمع البيانات، جودة البيانات، البيانات الوصفيَّة، قبول المعلومات.",
                "related_terms": [
                    "Auxiliary data",
                    "Data collection",
                    "Data quality",
                    "Metadata",
                    "Process information"
                ],
                "references": "Kreuter, F. (Ed.). (2013). Improving Surveys with Paradata. https://doi.org/10.1002/9781118596869",
                "drafted_by": [
                    "Alexander Hart; Graham Reid"
                ],
                "reviewed_by": [
                    "Helena Hartmann",
                    "Charlotte R. Pennington",
                    "Marta Topor",
                    "Flávio Azevedo"
                ],
                "alt_related_terms": [
                    null
                ],
                "language": "arabic"
            },
            {
                "type": "glossary",
                "title": "التَّسجيل بعد معرفة النَّتائج (PARKing)",
                "definition": "ممارسة يكمل فيها الباحثون تجربة، وربَّما عدد غير محدود من التَّجارب قبل التَّسجيل المسبق.  وهذه الممارسة تبطل الغرض من التَّسجيل المسبق، وهي واحدة من ممارسات إعداد التَّقارير المشكوك (أو حتى سوء السُّلوك العلمي) الذي يحاول اكتساب مصداقيَّة التَّسجيل المسبق.  **المصطلحات ذات الصِّلة:** النَّقد بعد معرفة النَّتائج، ممارسات البحث المشكوك فيها، أو ممارسات إعداد التَّقارير المشكوك فيها ، سباركنغ",
                "related_terms": [
                    "HARKing",
                    "Preregistration",
                    "Questionable Research Practices or Questionable Reporting Practices (QRPs)"
                ],
                "references": "Ikeda, A., Xu, H., Fuji, N., Zhu, S., & Yamada, Y. (2019). Questionable research practices following pre-registration. Japanese Psychological Review, 62, 281–295.\n\nYamada, Y. (2018). How to crack pre-registration: Toward transparent and open science. Frontiers in Psychology, 9, 1831. https://doi.org/10.3389/fpsyg.2018.01831",
                "drafted_by": [
                    "Qinyu Xiao"
                ],
                "reviewed_by": [
                    "Helena Hartmann",
                    "Sam Parsons",
                    "Yuki Yamada"
                ],
                "alt_related_terms": [
                    null
                ],
                "language": "arabic"
            },
            {
                "type": "glossary",
                "title": "البحوث التَّشاركيَّة (Participatory Research)",
                "definition": "يشير البحث التَّشاركي إلى دمج آراء الأشخاص من المجتمعات ذات الصِّلة في عملية البحث بأكملها؛ لتحقيق أهداف مشتركة بين الباحثين والمجتمعات.  يتَّخذ هذا النَّهج موقفًا تعاونيًا يسعى إلى تقليل اختلال توازن القوى بين الباحث وأولئك الذين يتم بحثهم من خلال \"الإبداع المشترك المنهجي للمعرفة الجديدة\" (Andersson, 2018). **المصطلحات ذات الصِّلة**: البحث التَّعاوني؛ الشُّمول؛  التَّنوع العصبيّ،  عيِّنة ومجتمع الدِّراسة المساهمين؛  النَّموذج التَّحويليّ.",
                "related_terms": [
                    "Collaborative research",
                    "Inclusion",
                    "Neurodiversity",
                    "Patient and Public Involvement (PPI)",
                    "Transformative paradigm"
                ],
                "references": "Cornwall, A., & Jewkes, R. (1995). What is participatory research? Social Science & Medicine, 41(12), 1667–1676. https://doi.org/10.1016/0277-9536(95)00127-S\n\nFletcher-Watson, S., Adams, J., Brook, K., Charman, T., Crane, L., Cusack, J., Leekam, S., Milton, D., Parr, J. R., & Pellicano, E. (2019). Making the Future Together: Shaping Autism Research Through Meaningful Participation. Autism, 23(4), 943–953.\n\nKiernan, C. (1999). Participation in research by people with learning disability: Origins and issues. British Journal of Learning Disabilities, 27(2), 43–47. https://doi.org/10.1111/j.1468-3156.1999.tb00084.x\n\nLeavy, P. (2017). Research Design: Quantitative, Qualitative, Mixed Methods, Arts-Based, and Community-Based Participatory Research Approaches. The Guilford Press.\n\nOttmann, G., Laragy, C., Allen, J., & Feldman, P. (2011). Coproduction in practice: Participatory action research to develop a model of community aged care. Systemic Practice and Action Research, 24, 413–427. https://doi.org/10.1007/s11213-011-9192-x\n\nRose, D. (2018). Participatory research: Real or imagined. Social Psychiatry and Psychiatric Epidemiology, 53, 765–771. https://doi.org/10.1007/s00127-018-1549-3",
                "drafted_by": [
                    "Tamara Kalandadze"
                ],
                "reviewed_by": [
                    "Jamie P. Cockcroft",
                    "Bethan Iley",
                    "Halil E. Kocalar",
                    "Michele C. Lim"
                ],
                "alt_related_terms": [
                    null
                ],
                "language": "arabic"
            },
            {
                "type": "glossary",
                "title": "PPI عيِّنة ومجتمع الدِّراسة المساهمين (Patient and Public Involvement (PPI))",
                "definition": "التَّعاون البحثي النَّشط مع مجتمع البحث الذي لديه اهتمام بحثي ما فضلا عن تطبيق البحث عليهم، يمكن للباحثين دمج الخبرة الحياتية وخبرة العينة ومجتمع البحث في جميع مراحل عملية البحث.  على سبيل المثال، يمكن لعيّنة الدِّراسة المساعدة في تطوير مجموعة من الأسئلة البحثيَّة، ومراجعة تصميم الدِّراسة، واعتماد الملخَّصات الإنجليزيَّة للتَّقديم على المنح، ونماذج الأخلاقياَّت البحثيَّة ونشرها، وجمع البيانات وتحليلها، والمساعدة في كتابة مشروع بحثي للنَّشر.  يوصى بتطبيق هذه الممارسات والتي تعد واحدة من متطلبات جهات تمويل البحوث (Boivin et al., 2018).  **المصطلحات ذات الصِّلة**: الإنتاج المشترك، البحوث التَّشاركيَّة.",
                "related_terms": [
                    "Co-production",
                    "Participatory research"
                ],
                "references": "Boivin, A., Richards, T., Forsythe, L., Gregoire, A., L’Esperance, A., Abelson, J., & Carman, K. L. (2018). Evaluating the patient and public involvement in research. British Medical Journal, 363, k5147. https://doi.org/10.1136/bmj.k5147\n\nINVOLVE. (n.d.). INVOLVE – Supporting public involvement in NHS, public health and social care research. https://www.invo.org.uk/",
                "drafted_by": [
                    "Jade Pickering"
                ],
                "reviewed_by": [
                    "Mahmoud Elsherif",
                    "Sam Parsons",
                    "Catia M. Oliveira"
                ],
                "alt_related_terms": [
                    null
                ],
                "language": "arabic"
            },
            {
                "type": "glossary",
                "title": "حاجز مالي (Paywall)",
                "definition": "حاجز تقني يتيح الوصول إلى المعلومات للأشخاص الذين دفعوا رسوم الاشتراك فقط إما شخصيًا، أو عن طريق منظَّمة.  ال**مصطلحات ذات الصِّلة:** إمكانيَّة الوصول، الوصول المفتوح.",
                "related_terms": [
                    "Accessibility",
                    "Open Access"
                ],
                "references": "Day, S., Rennie, S., Luo, D., & Tucker, J. D. (2020). Open to the public: Paywalls and the public rationale for open access medical research publishing. Research Involvement and Engagement, 6(1), 8. https://doi.org/10.1186/s40900-020-0182-y",
                "drafted_by": [
                    "Bradley Baker"
                ],
                "reviewed_by": [
                    "Mahmoud Elsherif",
                    "Sam Parsons",
                    "Charlotte R. Pennington",
                    "Julia Wolska"
                ],
                "alt_related_terms": [
                    null
                ],
                "language": "arabic"
            },
            {
                "type": "glossary",
                "title": "Peer Community In منظَّمة الأقران (PCI (Peer Community In))",
                "definition": "منظّمة غير ربحيَّة تنشئ مجتمعات من الباحثين الذين يراجعون المطبوعات الأوليَّة، ويوصون بنشرها، بحيث تُبنى التَّوصية على مراجعة عالية الجودة للأقران.  يقوم بها باحثَين متخصِّصين على الأقل، ويتم بعد ذلك تخصيص معرّف كائن رقمي لهذه المطبوعات الأوليَّة كأنَّه مقال في مجلة، وقد تم تطوير منظمة الأقران؛ لإنشاء نظام نشر علمي مجاني، وشفَّاف، وعام يعتمد على مراجعة المطبوعات الأوليَّة والتَّوصية بها.  **المصطلحات ذات الصِّلة:** الوصول المفتوح، المحفوظات المفتوحة، التّحكيم المفتوح، مجتمع الأقران للتَّقارير المسجَّلة، تحكيم الأقران، الطّباعة الأوليَّة",
                "related_terms": [
                    "Open Access",
                    "Open Archives",
                    "Open Peer Review",
                    "PCI Registered Reports",
                    "Peer review",
                    "Preprints"
                ],
                "references": "",
                "drafted_by": [
                    "Emma Henderson"
                ],
                "reviewed_by": [
                    "Jamie P. Cockcroft",
                    "Christopher Graham",
                    "Bethan Iley",
                    "Aleksandra Lazić",
                    "Charlotte R. Pennington"
                ],
                "alt_related_terms": [
                    null
                ],
                "language": "arabic"
            },
            {
                "type": "glossary",
                "title": "مجتمع الأقران للتَّقارير المسجَّلة (PCI Registered Reports)",
                "definition": "مبادرة تم إطلاقها في عام 2021 مخصَّصة لتلقي التقارير المسجَّلة، ومراجعتها، والتَّوصية بها لمختلف التَّخصُّصات من العلوم والتّكنولوجيا، والهندسة، والرِّياضيَّات، والطِّب، والعلوم الاجتماعيَّة، والعلوم الإنسانيَّة.  ويشرف على هذه العمليَّة صاحب التَّزكية (أي ما يعادل المحرّر) ويتم مراجعة البحث من قبل خبيرَين على الأقل في مجال التَّخصُّص. وتوفِّر هذه العمليَّة مراجعات مجانيَّة، وشفَّافة لما قبل (المرحلة الأولى) وما بعد إجراء الدِّراسة (المرحلة الثَّانية). يعتمد عدد من المجلَّات الصَّديقة لمجموعة الأقران للتَّقارير المسجّلة معايير المراجعة هذه، وتلتزم بقبول الأبحاث التي تنال توصية نهائيَّة إيجابيَّة منها.  **المصطلحات ذات الصِّلة:** القبول المبدئي (IPA) ، الوصول المفتوح، منظَّمة الأقران، تحيُّز النَّشر (مشكلة درج الملفَّات)، التَّقرير المسجَّل، تعمية النَّتائج ، مراجعة دراسة المرحلة الأولى، مراجعة دراسة المرحلة الثَّانية، الشَّفافيَّة.",
                "related_terms": [
                    "In Principle Acceptance (IPA)",
                    "Open Access",
                    "PCI (Peer Community In)",
                    "Publication bias (File Drawer Problem)",
                    "Registered Report",
                    "Results blind",
                    "Stage 1 study review",
                    "Stage 2 study review",
                    "Transparency"
                ],
                "references": "",
                "drafted_by": [
                    "Charlotte R. Pennington"
                ],
                "reviewed_by": [
                    "Sarah Ashcroft-Jones",
                    "Jamie P. Cockcroft",
                    "Mahmoud Elsherif",
                    "Helena Hartmann"
                ],
                "alt_related_terms": [
                    null
                ],
                "language": "arabic"
            },
            {
                "type": "glossary",
                "title": "خطة الصَّدمة (Plan S)",
                "definition": "مبادرة تم إطلاقها في سبتمبر 2018 من قبل ائتلاف الصَّدمة (cOAlition S)، وهو اتِّحاد من منظَّمات تمويل البحوث يهدف إلى تسريع الانتقال إلى الوصول المفتوح الكامل والفوري.  ويطالب المموّلون المشاركون من متلقي المنح البحثيَّة نشر بحوثهم في مجلَّات، أو منصَّات ذات وصول مفتوح متوافق، أو جعل أعمالهم متاحة بشكل مفتوح وفوري في مستودع الوصول المفتوح اعتبارًا من عام 2021 فصاعدًا، ويلتزم ممولو ائتلاف الصَّدمة بعدم تقديم الدَّعم الماليّ لرسوم الَّنشر ذات الوصول المفتوح المختلط، أو الهجين.  ومع ذلك، يمكن للمؤلفين الامتثال للخطّة من خلال نشر الوصول المفتوح في مجلة اشتراك بموجب \"ترتيب تحويلي\" كما هو موضح بمزيد من التَّفصيل في إرشادات التَّنفيذ. يشير الحرف \"S\" في الخطة S إلى الصَّدمة.  **المصطلحات ذات الصِّلة:** الوصول المفتوح ، إعلان سان فرانسيسكو بشأن تقييم البحوث، مستودع البيانات.",
                "related_terms": [
                    "Open Access",
                    "DORA",
                    "Repository"
                ],
                "references": "",
                "drafted_by": [
                    "Olmo van den Akker"
                ],
                "reviewed_by": [
                    "Jamie P. Cockcroft",
                    "Helena Hartmann",
                    "Halil E. Kocalar",
                    "Birgit Schmidt"
                ],
                "alt_related_terms": [
                    null
                ],
                "language": "arabic"
            },
            {
                "type": "glossary",
                "title": "الموضعيَّة (Positionality)",
                "definition": "تحديد سياق كلٌّ من بيئة البحث والباحث، لتحديد الحدود التي أُنتِج البحث داخلها (Jaraf, 2018). عادة ما يتم التَّركيز على الموضعيَّة والاحتفاء بها في البحث النُّوعي، ولكن هناك دعوات حديثة لاستخدامها في البحث الكمي أيضًا. تم اقتراح صياغات لبيان الموضعيَّة، حيث يحدِّد الباحث خلفيته و\"موقعه\" داخل البحث واتجاهه كطريقة للتَّعرُّف على تحيُّز الباحث وتركيزه.  **المصطلحات ذات الصِّلة:**  التَّحيز، الانعكاسيَّة، وجهات النَّظر.",
                "related_terms": [
                    "Bias",
                    "Reflexivity",
                    "Perspective"
                ],
                "references": "Jafar, A. J. N. (2018). What is positionality and should it be expressed in quantitative studies? Emergency Medicine Journal, 35(5), 323–324. https://doi.org/10.1136/emermed-2017-207158",
                "drafted_by": [
                    "Joanne McCuaig"
                ],
                "reviewed_by": [
                    "Helena Hartmann",
                    "Aoife O’Mahony",
                    "Madeleine Pownall",
                    "Graham Reid"
                ],
                "alt_related_terms": [
                    null
                ],
                "language": "arabic"
            },
            {
                "type": "glossary",
                "title": "خريطة الموضعية (Positionality Map)",
                "definition": "أداة انعكاسيَّة لممارسة الموضعيَّة بشفافيَّة في البحوث النُّوعية النَّقديَّة. يجب استخدام الخريطة \"كنقطة انطلاق مرنة لتوجيه الباحثين للتَّفكير والتَّأمل حول موقعهم الاجتماعي. تتضمَّن الخريطة ثلاثة مستويات: تحديد الهُويَّات الاجتماعيَّة (المستوى 1\\) ، وكيف تؤثر هذه المواقف على حياتنا (المستوى 2)، والتَّفاصيل التي قد تكون مرتبطة بخصائص هويتنا الاجتماعيَّة (المستوى 3)\" (Jacobson & Mustafa 2019, p. 1). الهدف من الخريطة هو \"أن يتمكَّن الباحثون من تحديد وفهم مواقعهم الاجتماعيَّة بشكل أفضل، وكيف يمكن أن يطرحوا التَّحديَّات، وجوانب السُّهولة في عمليَّة البحث النَّوعي.\" **المصطلحات ذات الصِّلة:** الموضعيَّة،  البحث النَّوعي، خريطة الهُويَّة الاجتماعيَّة،  الشَّفافيَّة.",
                "related_terms": [
                    "Positionality",
                    "Qualitative research",
                    "Social identity map",
                    "Transparency"
                ],
                "references": "Jacobson, D., & Mustafa, N. (2019). Social Identity Map: A Reflexivity Tool for Practicing Explicit Positionality in Critical Qualitative Research. International Journal of Qualitative Methods, 18, 1609406919870075. https://doi.org/10.1177/1609406919870075",
                "drafted_by": [
                    "Joanne McCuaig"
                ],
                "reviewed_by": [
                    "Helena Hartmann",
                    "Michele C. Lim",
                    "Charlotte R. Pennington",
                    "Graham Reid"
                ],
                "alt_related_terms": [
                    null
                ],
                "language": "arabic"
            },
            {
                "type": "glossary",
                "title": "التَّحليل البعدي (Post Hoc)",
                "definition": "تم استعارة المصطلح الإنجليزي من اللاتينيَّة، ويعني \"بعد هذا\". وفي الإحصاء يشير التَّحليل البعدي لاختبار الفرضيَّات التي لم يتم تحديدها بشكل مسبق. ففي الإحصاء التِّكراري، تختلف طريقة التَّحليل بناءً على ما إذا كان مخططًا له أم بعديًا، مثلًا: من خلال تطبيق تحكُّم أكثر صرامة بالخطأ، وفي المقابل، لا تختلف طريقة التَّحليل في المنهجيَّة الاحتماليَّة والبايزيَّة عندما يتم تحديد الفرضيَّة مسبقًا أو لا.  **المصطلحات ذات الصِّلة:** قبليَّة ، بعديَّة، الافتراض بعد معرفة النَّتائج، قرصنة القيمة الاحتماليَّة",
                "related_terms": [
                    "A priori, Ad hoc",
                    "HARKing",
                    "P-hacking"
                ],
                "references": "Dienes, Z. (2008). Understanding psychology as a science: An introduction to scientific and statistical inference. Macmillan International Higher Education.",
                "drafted_by": [
                    "Alaa Aldoh"
                ],
                "reviewed_by": [
                    "Sam Parsons",
                    "Jamie P. Cockcroft",
                    "Bethan Iley",
                    "Halil E. Kocalar",
                    "Graham Reid",
                    "Flávio Azevedo"
                ],
                "alt_related_terms": [
                    null
                ],
                "language": "arabic"
            },
            {
                "type": "glossary",
                "title": "مراجعة الأقران بعد النَّشر (Post Publication Peer Review)",
                "definition": "مراجعة تتم بعد نشر البحث وعادةً ما يتم نشرها على منصَّة مخصَّصة (مثل منصة PubPeer). وهو يختلف عن التَّعليق التَّقليدي الذي ينشر في نفس المجلَّة، والذي عادة ما يخضع لتحكيم الأقران.  **المصطلحات ذات الصِّلة:** التّحكيم المفتوح، منصة بابّير (PubPeer)، تحكيم الأقران",
                "related_terms": [
                    "Open Peer Review",
                    "PeerPub",
                    "Peer review"
                ],
                "references": "",
                "drafted_by": [
                    "Ali H. Al-Hoorie"
                ],
                "reviewed_by": [
                    "Mahmoud Elsherif",
                    "Sam Parsons"
                ],
                "alt_related_terms": [
                    null
                ],
                "language": "arabic"
            },
            {
                "type": "glossary",
                "title": "التَّوزيع اللَّاحق (Posterior distribution)",
                "definition": "طريقة لتلخيص المعرفة المحدَّثة في الاستدلال البايزي، تحقِّق التَّوازن بين المعرفة السَّابقة والبيانات المرصودة من النَّاحية الإحصائيَّة، توازن التَّوزيعات اللَّاحقة بين ناتج دالة الاحتمال، والافتراضات السَّابقة. يوضِّح التَّوزيع الاحتمالي اللَّاحق وجود اليقين (أو عدمه) حول قيمة مؤشِّر معيَّن.  **المصطلحات ذات الصِّلة:** معامل بايز، الاستدلال البايزي، تقدير المعاملات باستخدام النهج البايزي، دالة الاحتماليَّة، التَّوزيع المسبق.ِ",
                "related_terms": [
                    "Bayes Factor",
                    "Bayesian inference",
                    "Bayesian parameter estimation",
                    "Likelihood function",
                    "Prior distribution"
                ],
                "references": "Dienes, Z. (2014). Using Bayes to get the most out of non-significant results. Frontiers in Psychology, 5, 781. https://doi.org/10.3389/fpsyg.2014.00781\n\nLüdtke, O., Ulitzsch, E., & Robitzsch, A. (2020). A Comparison of Penalized Maximum Likelihood Estimation and Markov Chain Monte Carlo Techniques for Estimating Confirmatory Factor Analysis Models with Small Sample Sizes . https://doi.org/10.31234/osf.io/u3qag",
                "drafted_by": [
                    "Alaa AlDoh"
                ],
                "reviewed_by": [
                    "Adam Parker",
                    "Jamie P. Cockcroft",
                    "Julia Wolska",
                    "Yu-Fang Yang",
                    "Charlotte R. Pennington"
                ],
                "alt_related_terms": [
                    null
                ],
                "language": "arabic"
            },
            {
                "type": "glossary",
                "title": "النَّشر المفترس الاستغلاليّ (Predatory Publishing)",
                "definition": "يصف النَّشر الاستغلالي مجموعة من الممارسات التِّجاريَّة التي يسعى النَّاشرون من خلالها إلى الرِّبح، غالبًا من خلال تحصيل رسوم نشر المقالات، عبر نشر الأعمال العلميَّة بدون فحص معتبر للجودة، مثل تحكيم الأقران، أو خدمات التَّحرير. في أكثر أشكاله تطرفًا، ينشر النَّاشرون الاستغلاليون أي عمل طالما تم دفع رسومه. ومن الاستراتيجيات الأخرى الأقل تطرفًا التي يعدَّها البعض بأنَّها أيضًا استغلاليَّة هي إرسال أعداد كبيرة من الطَّلبات للتَّحرير، أو النَّشر في أعداد خاصّة مدفوعة الأجر. (Crosetto ، 2021).  **المصطلحات ذات الصِّلة:** رسوم النَّشر، التَّلاعب بالنِّظام",
                "related_terms": [
                    "Article Processing Charge (APC)",
                    "Gaming (the system)"
                ],
                "references": "Crosetto, P. (2021). Is MDPI a predatory publisher? https://paolocrosetto.wordpress.com/2021/04/12/is-mdpi-a-predatory-publisher/\n\nXia, J., Harmon, J. L., Connolly, K. G., Donnelly, R. M., Anderson, M. R., & Howard, H. A. (2015). Who publishes in “predatory” journals? Journal of the Association for Information Science and Technology, 66(7), 1406–1417. https://doi.org/10.1002/asi.23265",
                "drafted_by": [
                    "Nick Ballou"
                ],
                "reviewed_by": [
                    "Olmo van den Akker",
                    "Helena Hartmann",
                    "Aleksandra Lazić",
                    "Graham Reid",
                    "Flávio Azevedo"
                ],
                "alt_related_terms": [
                    null
                ],
                "language": "arabic"
            },
            {
                "type": "glossary",
                "title": "إرشادات الإعداد (PREPARE Guidelines)",
                "definition": "تهدف وثيقة \"تخطيط البحوث، والإجراءات التَّجريبيَّة على الحيوانات: توصيات للتَّميز\" إلى إعداد إرشادات وقائمة للمراجعة تساعد في التَّخطيط لأبحاث الحيوانات، ودعم الالتزام بالاستبدال، والتَّقليل، والصَّقل (المعروفة بالرَّاءات الثَّلاث)، وتسهيل إعادة إنتاج البحوث الحيوانيَّة.  **المصطلحات ذات الصِّلة**: إرشادات أرايف، دليل الإبلاغ، سترينج: الخلفيَّة الاجتماعيَّة والقابليَّة للتَّتبع والاختيار الذَّاتي، وتاريخ التَّربية، والتَّأقلم، والتَّعود.",
                "related_terms": [
                    "ARRIVE Guidelines",
                    "Reporting Guideline",
                    "STRANGE"
                ],
                "references": "Smith, A. J., Clutton, R. E., Lilley, E., Hansen, K. E. A., & Brattelid, T. (2018). PREPARE: Guidelines for planning animal research and testing. Laboratory Animals, 52(2), 135–141. https://doi.org/10.1177/0023677217724823",
                "drafted_by": [
                    "Ben Farrar"
                ],
                "reviewed_by": [
                    "Mahmoud Elsherif",
                    "Gilad Feldman",
                    "Elias Garcia-Pelegrin"
                ],
                "alt_related_terms": [
                    null
                ],
                "language": "arabic"
            },
            {
                "type": "glossary",
                "title": "الطّباعة الأوليّة (Preprint)",
                "definition": "نسخة متاحة للجمهور من أي نوع من المخطوطات العلميَّة، أو مخرجات البحث التي تسبق النَّشر الرَّسمي، وتعدُّ شكلًا من أشكال الوصول المفتوح الأخضر.  وعادةً ما يتم استضافة هذه النُّسخ في مستودع (مثل arXiv) والذي يسهل النَّشر من خلال مشاركة نتائج البحث بشكل أسرع من النَّشر التَّقليدي.  عادةً ما توفر  مستودعات الطباعة الأولية معرِّفات ثابتة (مثل معرِّفات الكائن الرَّقمي) للمطبوعات الأوليَّة. يمكن نشر النُّسخ الأوليَّة في أي وقت خلال دورة البحث، ولكن يتم نشرها بشكل شائع عند تقديمها للنَّشر (أي قبل تحكيم الأقران). وغالبًا ما يتم \\_أيضًا\\_ تحميل إصدارات المقالات المقبولة والتي تمت مراجعتها من قِبل الأقران إلى مستودعات ما قبل الطِّباعة، وتسمى هذه نسخ ما بعد الطِّباعة.  **المصطلحات ذات الصِّلة:** الوصول المفتوح ،معرِّف الكائن الرَّقمي، ما بعد الطِّباعة، ورقة عمل",
                "related_terms": [
                    "Open Access",
                    "DOI (digital object identifier)",
                    "Postprint",
                    "Working Paper"
                ],
                "references": "Bourne, P. E., Polka, J. K., Vale, R. D., & Kiley, R. (2017). Ten simple rules to consider regarding preprint submission. PLoS Computational Biology, 13(5), e1005473. https://doi.org/10.1371/journal.pcbi.1005473\n\nElmore, S. A. (2018). Preprints: What Role Do These Have in Communicating Scientific Results? Toxicologic Pathology, 46(4), 364–365. https://doi.org/10.1177/0192623318767322",
                "drafted_by": [
                    "Mariella Paul"
                ],
                "reviewed_by": [
                    "Gisela H. Govaart",
                    "Helena Hartmann",
                    "Sam Parsons",
                    "Tobias Wingen",
                    "Flávio Azevedo"
                ],
                "alt_related_terms": [
                    null
                ],
                "language": "arabic"
            },
            {
                "type": "glossary",
                "title": "التَّسجيل المسبق (Preregistration)",
                "definition": "ممارسة نشر خطّة الدِّراسة بما في ذلك أسئلة، وفرضيَّات البحث، وتصميم البحث، وطريقة تحليل البيانات قبل جمع البيانات، أو فحصها، ومن الممكن أيضًا التَّسجيل المسبق لتحليلات البيانات الثَّانويَّة (Merten & Krypotos, 2019). يتم ختم وثيقة التَّسجيل المسبق بختم زمني، وعادة ما يتم تسجيلها لدى طرف مستقل (مثل مستودع بيانات) بحيث يمكن مشاركتها علنًا مع الآخرين (أحيانًا بعد انتهاء فترة حظر اختياريَّة). ويوفِّر التَّسجيل المسبق توثيقًا شفافًا لما تم التَّخطيط له في وقت معين، ويسمح لأطراف ثالثة بتقييم التَّغييرات التي قد تكون حدثت بعد ذلك. وكلَّما كان التَّسجيل المسبق أكثر تفصيلًا، كان بإمكان الطَّرف الثَّالث تقييم هذه التَّغييرات بشكل أفضل، وبالتَّالي صحة التَّحليلات التي تم إجراؤها، ويهدف التَّسجيل المسبق إلى التَّمييز بوضوح بين البحث التَّوكيدي والاستكشافي.  **المصطلحات ذات الصِّلة:** الانحياز التَّوكيدي، التَّحليلات التَّوكيدية، تحليل البيانات الاستكشافيَّ، الافتراض بعد معرفة النَّتائج، ترقيم خطة التَّحليل المسبق، ممارسات البحث المشكوك فيها، أو ممارسات إعداد التَّقارير المشكوك فيها، التَّقرير المسجَّل، مراسم البحث، الشَّفافيَّة.",
                "related_terms": [
                    "Confirmation bias",
                    "Confirmatory analyses",
                    "Exploratory Data Analysis",
                    "HARKing",
                    "Pre-analysis plan",
                    "Questionable Research Practices or Questionable Reporting Practices (QRPs)",
                    "Registered Report",
                    "Research Protocol",
                    "Transparency"
                ],
                "references": "Lewandowsky, S., & Bishop, D. (2016). Research integrity: Don’t let transparency damage science. Nature News, 529(7587), 459. https://doi.org/10.1038/529459a\n\nMertens, G., & Krypotos, A. M. (2019). Preregistration of analyses of preexisting data. Psychologica Belgica, 59(1), 338.\n\nNavarro, D. (2020). Paths in strange spaces: A comment on preregistration.\n\nNosek, B. A., Ebersole, C. R., DeHaven, A. C., & Mellor, D. T. (2018). The preregistration revolution. Proceedings of the National Academy of Sciences, 115(11), 2600–2606. https://doi.org/10.1073/pnas.1708274114\n\nSimmons, J., Nelson, L., & Simonsohn, U. (2021). Pre‐registration: Why and how. Journal of Consumer Psychology, 31(1), 151–162. https://doi.org/10.1002/jcpy.1208",
                "drafted_by": [
                    "Mahmoud Elsherif"
                ],
                "reviewed_by": [
                    "Gisela H. Govaart",
                    "Helena Hartmann",
                    "Tina Lonsdorf",
                    "William Ngiam",
                    "Eike Mark Rinke",
                    "Lisa Spitzer",
                    "Olmo van den Akker",
                    "Flávio Azevedo"
                ],
                "alt_related_terms": [
                    null
                ],
                "language": "arabic"
            },
            {
                "type": "glossary",
                "title": "تّعهد التَّسجيل المسبق (Preregistration Pledge)",
                "definition": "يعد تعهُّد التَّسجيل المسبق \"إجراءً جماعيًا؛ لدعم ممارسات البحث المفتوحة، والقابلة للتِّكرار\" وهو حملة تابعة لمنصَّة (حرِّر معرفتنا) الذي يطلب من الباحث الالتزام بالتَّسجيل المسبق لدراسة واحدة على الأقل في العامين المقبلين ([https://freeourknowledge.org/2020-12-03-preregistration-pledge/](https://freeourknowledge.org/2020-12-03-preregistration-pledge/)) . وهذا المشروع عبارة عن حركة شعبيَّة بدأها مجموعة من الباحثين في بداية حياتهم المهنيَّة.  **المصطلحات ذات الصلة:** التَّسجيل المسبق",
                "related_terms": [
                    "Preregistration"
                ],
                "references": "Knowledge, F. O. (2020). Preregistration Pledge. https://freeourknowledge.org/2020-12-03-preregistration-pledge/",
                "drafted_by": [
                    "Helena Hartmann"
                ],
                "reviewed_by": [
                    "Jamie P. Cockcroft",
                    "Mahmoud Elsherif",
                    "Aleksandra Lazić, Steven Verheyen"
                ],
                "alt_related_terms": [
                    null
                ],
                "language": "arabic"
            },
            {
                "type": "glossary",
                "title": "peer review openness initiative مبادرة انفتاح تحكيم الأقران (PRO (peer review openness) initiative)",
                "definition": "هي اتفاقيّة تم التَّوصل إليها من قبل عدد من الأكاديميين بأنَّهم لن يقوموا بمراجعة لأي مخطوطة ما لم تستوفِ شروط معيّنة، وعلى وجه التَّحديد يجب على مؤلفي المخطوطات التَّأكد من إتاحة البيانات، والمواد للجمهور (أو تقديم مبرِّر لعدم توفّرها، أو مشاركتها مجانًا)، وتقديم وثائق توضِّح بالتَّفصيل كيفيَّة تفسير، وتشغيل أي ملفات، أو نصوص برمجيَّة، وتفاصيل مكان هذه الملفَّات، وتحديد موقعها داخل المخطوطة نفسها.  **المصطلحات ذات الصِّلة:** تحكيم الأقران غير مجهولة المصدر، العلم المفتوح، التحكيم المفتوح، تحكيم الأقران الشفاف",
                "related_terms": [
                    "Non-anonymised peer review",
                    "Open Science",
                    "Open Peer Review",
                    "Transparent peer review"
                ],
                "references": "Morey, R. D., Chambers, C. D., Etchells, P. J., Harris, C. R., Hoekstra, R., Lakens, D., Lewandowsky, S., Morey, C. C., Newman, D. P., Schönbrodt, F. D., Vanpaemel, W., Wagenmakers, E.-J., & Zwaan, R. A. (2016). The Peer Reviewers’ Openness Initiative: incentivizing open research practices through peer review. Royal Society Open Science, 3(1). https://doi.org/10.1098/rsos.150547",
                "drafted_by": [
                    "Jamie P. Cockcroft"
                ],
                "reviewed_by": [
                    "Sarah Ashcroft-Jones",
                    "Mahmoud Elsherif",
                    "Helena Hartmann",
                    "Steven Verheyen"
                ],
                "alt_related_terms": [
                    null
                ],
                "language": "arabic"
            },
            {
                "type": "glossary",
                "title": "التَّوزيع المسبق (Prior distribution)",
                "definition": "معتقدات الباحثين حول مؤشِّرات النّموذج الإحصائي قبل جمع البيانات، ويتم التَّعبير عنها على شكل توزيع احتمالي، ويمكن تحديده بعدَّة طرق (مثل البحث السَّابق، أو التَّقييم الذَّاتي، أو بعض المبادئ مثل \"ازدياد الإنتروبية بسبب القيود\")، وعادة ما يتم دمجها مع دالة الاحتماليَّة باستخدام نظرية بايز للحصول على التَّوزيع اللَّاحق.  **المصطلحات ذات الصِّلة:** معامل بايز، الاستدلال البايزي، تقدير المعاملات باستخدام النَّهج البايزي، دالة الاحتماليَّة، التَّوزيع اللَّاحق",
                "related_terms": [
                    "Bayes Factor",
                    "Bayesian inference",
                    "Bayesian Parameter Estimation",
                    "Likelihood function",
                    "Posterior distribution"
                ],
                "references": "",
                "drafted_by": [
                    "Alaa AlDoh"
                ],
                "reviewed_by": [
                    "Charlotte R. Pennington",
                    "Martin Vasilev"
                ],
                "alt_related_terms": [
                    null
                ],
                "language": "arabic"
            },
            {
                "type": "glossary",
                "title": "التَّسمية المستعارة (Pseudonymisation)",
                "definition": "تشير إلى تقنيَّة تتضمَّن استبدال، أو إزالة أي معلومات يمكن أن تؤدِّي إلى تحديد هويّة الأشخاص الخاضعين للبحث مع الاستمرار في جعل التَّعرُّف عليهم ممكنًا من خلال استخدام مجموعة من الأرقام، والمعرِّفات الرَّمزيَّة، وتتضمَّن هذه العمليَّة الخطوات الآتية:  إزالة جميع ما يمكن أن يُعرف عن الهوية من بيانات البحث، إسناد معرّف اسم مستعار لكلِّ مشارك واستخدامه؛ لتسميته في كل سجل بحث، وحفظ هذه الأسماء المستعارة مع ما ترمز إليه في مستند منفصل عن البيانات البحثيَّة.  وعادةً ما يكون استخدام الأسماء المستعارة هو الحد الأدنى من  متطلبات اللِّجان الأخلاقيَّة عند إجراء البحوث، خاصةً فيما يتعلَّق بالمشاركين البشر والمعلومات السِّريَّة، من أجل ضمان الحفاظ على خصوصيَّة البيانات.  **المصطلحات ذات الصِّلة:** التّعمية، السِّريَّة، خصوصيَّة البيانات، إزالة الهوية، التَّسمية المستعارة، أخلاقيَّات البحث",
                "related_terms": [
                    "Anonymity",
                    "Confidentiality",
                    "Data privacy",
                    "De-identification",
                    "Pseudonymisation",
                    "Research ethics"
                ],
                "references": "Mourby, M., Mackey, E., Elliot, M., Gowans, H., Wallace, S. E., Bell, J., Smith, H., Aidinlis, S., & Kaye, J. (2018). Are ‘pseudonymised’ data always personal data? Implications of the GDPR for administrative data research in the UK. Computer Law & Security Review, 34(2), 222–233. https://doi.org/10.1016/j.clsr.2018.01.002\n\nMedical Research Council. (2019). Identifiability, anonymisation and pseudonymisation. Medical Research Council. https://mrc.ukri.org/documents/pdf/gdpr-guidance-note-5-identifiability-anonymisation-and-pseudonymisation/",
                "drafted_by": [
                    "Catia M. Oliveira"
                ],
                "reviewed_by": [
                    "Helena Hartmann",
                    "Sam Parsons",
                    "Charlotte R. Pennington",
                    "Birgit Schmidt"
                ],
                "alt_related_terms": [
                    null
                ],
                "language": "arabic"
            },
            {
                "type": "glossary",
                "title": "التِّكرار الكاذب (Pseudoreplication)",
                "definition": "يقع التِّكرار الكاذب عندما لا يكون هناك استقلاليَّة إحصائيَّة في البيانات، والذي يؤدي إلى تضخيم عدد العيِّنات بشكل مصطنع، مثلًا عند جمع أكثر من نقطة بيانات واحدة من نفس الوحدة التَّجريبيَّة (مثلًا مأخوذة من مشارك في الدِّراسة أو من محصول؟).  هناك طرق للتَّغلُّب على هذه المشكلة مثل: استخراج المتوسِّط من هذه التِّكرارات (كأخذ متوسِّط وقت الاستجابة للمشارك)، أو اللُّجوء لنماذج التَّأثيرات المختلطة مع حساب التَّأثيرات العشوائيَّة للتِّكرار الكاذب (كتحديد أنَّ أوقات الاستجابة لكلِّ فرد تنتمي إلى ذلك الفرد).  ونلاحظ أنَّ الخيار الأوّل سيكون مرتبطًا بفقدان المعلومات، والقوة الإحصائيَّة.  **المصطلحات ذات الصِّلة:** مربك ،القابليَّة للتَّعميم، التكرار، الصدق",
                "related_terms": [
                    "Confounding",
                    "Generalizability",
                    "Replication",
                    "Validity"
                ],
                "references": "Davies, G. M., & Gray, A. (2015). Don’t let spurious accusations of pseudoreplication limit our ability to learn from natural experiments (and other messy kinds of ecological monitoring). Ecology and Evolution, 5(22), 5295–5304. https://doi.org/10.1002/ece3.1782\n\nHurlbert, S. H. (1984). Pseudoreplication and the Design of Ecological Field Experiments. Ecological Monographs, 54(2), 187–211. https://doi.org/10.2307/1942661\n\nLazic, S. E. (2019). Genuine replication and pseudoreplication: What’s the difference? In BMJ Open Science. https://blogs.bmj.com/openscience/2019/09/16/genuine-replication-and-pseudoreplication-whats-the-difference/",
                "drafted_by": [
                    "Ben Farrar"
                ],
                "reviewed_by": [
                    "Jamie P. Cockcroft",
                    "Mahmoud Elsherif",
                    "Elias Garcia-Pelegrin",
                    "Annalise A. LaPlume"
                ],
                "alt_related_terms": [
                    null
                ],
                "language": "arabic"
            },
            {
                "type": "glossary",
                "title": "التَّحليل البعديّ للمقاييس النَّفسيّة (Psychometric meta-analysis)",
                "definition": "تهدف التَّحليلات البعديَّة للمقاييس النَّفسيَّة إلى تصحيح حجم تأثير معيّن، والحاصل بسبب خطأ في القياس وغيره من الأخطاء؛ وذلك باستخدام إجراءات تعتمد على مبادئ القياس النَّفسي، كثبات المقاييس.  ينبغي تنفيذ هذه الإجراءات قبل تحليل حجم الأثر في التَّحليل البعدي الارتباطي، أو التَّجريبي، حيث إن إجراء هذه التَّصحيحات قد يؤدِّي إلى أحجام تأثير أكبر، وأكثر ثباتًا.  **المصطلحات ذات الصِّلة:** التَّحليل التَّالي التَّرابطي، التَّحليل التَّالي لهنتر شميدت، التَّحليل البعدي، المراجعات المنهجيَّة غير التَّدخليَّة المفتوحة، وقابلة للتِّكرار، تحيُّز النَّشر (مشكلة درج الملفَّات)، تعميم الصَّلاحيَّة",
                "related_terms": [
                    "Correlational meta-analysis",
                    "Hunter-Schmidt meta-analysis",
                    "Meta-analysis",
                    "Non-Intervention, Reproducible, and Open Systematic Reviews (NIRO-SR)",
                    "Publication bias (File Drawer Problem)",
                    "Validity generalization"
                ],
                "references": "Borenstein, M., Hedges, L. V., Higgins, J. P., & Rothstein, H. R. (2011). Introduction to meta-analysis. John Wiley & Sons.\n\nHunter, J. E., & Schmidt, F. L. (2015). Methods of Meta-Analysis: Correcting Error and Bias in Research Findings (Third). SAGE.",
                "drafted_by": [
                    "Adrien Fillon"
                ],
                "reviewed_by": [
                    "Mahmoud Elsherif",
                    "Eduardo Garcia-Garzon",
                    "Helena Hartmann",
                    "Catia M. Oliveira",
                    "Flávio Azevedo"
                ],
                "alt_related_terms": [
                    null
                ],
                "language": "arabic"
            },
            {
                "type": "glossary",
                "title": "File Drawer Problem تحيُّز النَّشر مشكلة درج الملفَّات (Publication bias (File Drawer Problem))",
                "definition": "هو عدم نشر النَّتائج بناءً على \"اتِّجاه أو قوّة نتائج الدِّراسة\" (Dickersin & Min, 1993, p. 135). ينشأ التَّحيُّز عندما يتوقَّف تقييم قابليَّة نشر الدِّراسة بشكل كبير على نتائج الدِّراسة، وغالبًا ما يكون ذلك مع الميل إلى أنَّ النَّتائج الجديدة، وذات الدّلالة الإحصائيَّة تستحق النَّشر أكثر من الدِّراسات التِّكراريَّة، والنَّتائج التي لا تظهر دلالة إحصائيَّة.  يتضح هذا التَّحيُّز عادةً من خلال وجود عدد كبير من الدِّراسات ذات الدِّلالة الإحصائيَّة، وأحجام التَّأثير المتضخِّمة فيها.  تؤدي هذه العمليَّة إلى كون الأدبياَّت العلميَّة المنشورة لا تمثِّل المدى الكامل لجميع الأبحاث، خصوصًا الدِّراسات التي لا تظهر نتائجها دلالة إحصائيَّة، يؤدِّي هذا بدوره إلى ما يسمَّى \"درج الملفَّات\" للأبحاث التي لا يتم نشرها أبدًا، وليس لها وثائق يمكن من خلالها العثور عليها.  **المصطلحات ذات الصِّلة:** تحيُّز النَّشر؛  منحنى القيم الاحتماليَّة،  قرصنة القيمة الاحتماليَّة،  تقارير انتقائيَّة؛  الدِّلالة الإحصائيَّة؛  طريقة التَّقليم والتَّعبئة **تعريف بديل:** في سياق التَّحليل التَّلوي، يحدث تحيُّز النَّشر \"... عندما يكون البحث الذي يظهر في الأدبيَّات المنشورة غير مُمثل بشكل منهجي لمجموعة الدِّراسات المكتملة. ببساطة، عندما يختلف البحث المتاح بسهولة في نتائجه عن نتائج جميع البحوث التي تم إجراؤها في مجال ما، فإنَّ القراء والمراجعين لهذا البحث معرَّضون لخطر التَّوصل إلى استنتاج خاطئ حول ما تظهره مجموعة الأبحاث هذه  \".  (روثستين وآخرون، 2005، ص 1\\) **المصطلحات ذات الصِّلة بالتَّعريف البديل:** التَّحليل البعدي",
                "related_terms": [
                    "Dissemination bias",
                    "P-curve",
                    "P-hacking",
                    "Selective reporting",
                    "Statistical significance",
                    "Trim and fill **Alternative definition:** In the context of meta-analysis, publication bias “...occurs whenever the research that appears in the published literature is systematically unrepresentative of the population of completed studies. Simply put, when the research that is readily available differs in its results from the results of all the research that has been done in an area, readers and reviewers of that research are in danger of drawing the wrong conclusion about what that body of research shows.” (Rothstein et al., 2005, p. 1\\) **Related terms to alternative definition:** meta-analysis"
                ],
                "references": "Dickersin, K., & Min, Y. (1993). Publication Bias: The Problem That Won’t Go Away. Annals of the New York Academy of Sciences, 703(1), 135–148. https://doi.org/10.1111/j.1749-6632.1993.tb26343.x\n\nDevito, N., & Goldacre, B. (2019). Publication Bias. Catalogue of Bias. https://catalogofbias.org/biases/publication-bias/\n\nDuval, S., & Tweedie, R. (2000). A nonparametric “trim and fill” method of accounting for publication bias in meta-analysis. Journal of the American Statistical Association, 95, 89–98. https://doi.org/10.2307/2669529\n\nDuval, S., & Tweedie, R. (2000). Trim and fill: A simple funnel-plot–based method of testing and adjusting for publication bias in meta-analysis. Biometrics, 56, 455–463. https://doi.org/10.1111/j.0006-341x.2000.00455.x\n\nFranco, A., Malhotra, N., & Simonovits, G. (2014). Publication bias in the social sciences: Unlocking the file drawer. Science, 345(6203), 1502–1505. https://doi.org/10.1126/science.1255484\n\nLindsay, D. S. (2020). Seven steps toward transparency and replicability in psychological science. Canadian Psychology/Psychologie Canadienne, 61(4), 310–317. https://doi.org/10.1037/cap0000222\n\nRothstein, H. R., Sutton, A. J., & Borenstein, M. (2005). Publication bias in meta-analysis. In Publication bias in meta-analysis: Prevention, assessment and adjustments (pp. 1–7). John Wiley & Sons, Ltd. https://doi.org/10.1002/0470870168.ch1",
                "drafted_by": [
                    "Mahmoud Elsherif"
                ],
                "reviewed_by": [
                    "Jamie P. Cockcroft",
                    "Gilad Feldman",
                    "Adrien Fillon",
                    "Helena Hartmann",
                    "Tamara Kalandadze",
                    "William Ngiam",
                    "Martin Vasilev",
                    "Olmo van den Akker",
                    "Flávio Azevedo"
                ],
                "alt_related_terms": [
                    null
                ],
                "language": "arabic"
            },
            {
                "type": "glossary",
                "title": "ثقة الجمهور في العلوم (Public Trust in Science)",
                "definition": "الثِّقة في المعرفة والمبادئ التَّوجيهيَّة، والتَّوصيات التي أنتجها العلماء، أو قدَّموها للمجتمع المدني (Hendriks et al., 2016). وقد تشير أيضًا إلى الثِّقة في التَّوصيات القائمة على أساس علمي  بشأن الصِّحة العامّة (كالرِّعاية الصِّحيَّة الشَّاملة، وبحوث الخلايا الجذعيَّة، والصَّناديق الفيدراليَّة المخصَّصة للحقوق الإنجابيَّة للمرأة، والتَّدابير الوقائيَّة للأمراض المعدية، والتَّطعيم)، وتغيُّر المناخ، والسِّياسات الاقتصاديَّة ( مثل الرَّفاهيَّة، وعدم المساواة، ومكافحة الفقر) وتقاطعاتها.  ولقد تبيّن أن ثقة الجمهور في العلم تتأثَّر بعدد كبير من العوامل مثل العمر (Anderson et al., 2012\\) ، والجنس (Von Roten, 2004\\) ، ورفض المعايير العلميَّة (Lewandowsky & Oberauer, 2021\\) ، والأيديولوجيَّة السِّياسيَّة (Azevedo & Jost, 2021; Brewer & Ley, 2012; Leiserowitz et al., 2010\\) ، والاستبداد اليميني والهيمنة الاجتماعيَّة (Kerr & Wilson, 2021\\) ، والتَّعليم (Bak, 2001; Hayes & Tariq, 2000\\) والدَّخل (Anderson et al., 2012\\) والمعرفة العلميَّة (Evans & Durant, 1995; Nisbet et al., 2002\\) واستخدام وسائل التَّواصل الاجتماعي (Huber et al., 2019\\) والتدين (Azevedo, 2021; Brewer & Ley, 2013; Liu & Priest, 2009).  **المصطلحات ذات الصِّلة:** مصداقية الادِّعاءات العلميَّة، الثِّقة المعرفيَّة",
                "related_terms": [
                    "Credibility of scientific claims",
                    "Epistemic Trust"
                ],
                "references": "Anderson, M. S., Ronning, E. A., Devries, R., & Martinson, B. C. (2010). Extending the Mertonian norms: Scientists’ subscription to norms of research. Journal of Higher Education, 81(3), 366–393. https://doi.org/10.1353/jhe.0.0095\n\nAzevedo, F., & Jost, J. T. (2021). The ideological basis of antiscientific attitudes: Effects of authoritarianism, conservatism, religiosity, social dominance, and system justification. Group Processes & Intergroup Relations, 24(4), 518–549. https://doi.org/10.1177/1368430221990104\n\nBak, H.-J. (2001). Education and Public Attitudes toward Science: Implications for the ‘Deficit Model’ of Education and Support for Science and Technology. Social Science Quarterly, 82(4), 779–795. https://www.jstor.org/stable/42955760\n\nBrewer, P. R., & Ley, B. L. (2013). Whose Science Do You Believe? Explaining Trust in Sources of Scientific Information About the Environment. Science Communication, 35(1), 115–137. https://doi.org/10.1177/1075547012441691\n\nEvans, G., & Durant, J. (1995). The relationship between knowledge and attitudes in the public understanding of science in Britain. Public Understanding of Science, 4(1), 57–74. https://doi.org/10.1088/0963-6625/4/1/004\n\nHayes, B. C., & Tariq, V. N. (2000). Gender differences in scientific knowledge and attitudes toward science: A comparative study of four Anglo-American nations. Public Understanding of Science, 9(4), 433–447. https://doi.org/10.1088/0963-6625/9/4/306\n\nHendriks, F., Kienhues, D., & Bromme, R. (2016). Trust in science and the science of trust. Trust and Communication in a Digitized World, 143–159.\n\nHuber, B., Barnidge, M., Gil de Zúñiga, H., & Liu, J. (2019). Fostering public trust in science: The role of social media. Public Understanding of Science, 28(7), 759–777. https://doi.org/10.1177/0963662519869097\n\nKerr, J. R., & Wilson, M. S. (2021). Right-wing authoritarianism and social dominance orientation predict rejection of science and scientists. Group Processes & Intergroup Relations, 24(4), 550–567. https://doi.org/10.1177/1368430221992126\n\nLewandowsky, S., & Oberauer, K. (2021). Worldview-motivated rejection of science and the norms of science. Cognition, 215, 104820. https://doi.org/10.1016/j.cognition.2021.104820\n\nLiu, H., & Priest, S. (2009). Understanding public support for stem cell research: Media communication, interpersonal communication and trust in key actors. Public Understanding of Science, 18(6), 704–718. https://doi.org/10.1177/0963662508097625\n\nNisbet, M. C., Scheufele, D. A., Shanahan, J., Moy, P., Brossard, D., & Lewenstein, B. V. (2002). Knowledge, Reservations, or Promise?: A Media Effects Model for Public Perceptions of Science and Technology. Communication Research, 29(5), 584–608. https://doi.org/10.1177/009365002236196\n\nSchneider, J., Merk, S., & Rosman, T. (2019). (Re)Building Trust? Investigating the effects of open science badges on perceived trustworthiness in journal articles. https://doi.org/10.17605/OSF.IO/VGBRS\n\nWingen, T., Berkessel, J. B., & Englich, B. (2020). No Replication, No Trust? How Low Replicability Influences Trust in Psychology. Social Psychological and Personality Science, 11(4). https://doi.org/10.1177/1948550619877412",
                "drafted_by": [
                    "Tobias Wingen; Flávio Azevedo"
                ],
                "reviewed_by": [
                    "Elias Garcia-Pelegrin",
                    "Helena Hartmann",
                    "Catia M. Oliveira",
                    "Olmo van den Akker"
                ],
                "alt_related_terms": [
                    null
                ],
                "language": "arabic"
            },
            {
                "type": "glossary",
                "title": "النَّشر أو  الفناء (Publish or Perish)",
                "definition": "قول مأثور يصف الضَّغط الذي يتعرَّض له الباحثون لنشر عدد كبير من الأبحاث في مجلات أكاديميَّة مرموقة بغرض النَّجاح في مهنهم الأكاديميَّة، يؤثِّر هذا الضَّغط على جودة البحوث لتركيزه على كمية عوضاً عن جودتها، كما يتفاقم هذا الضَّغط المؤسَّسي باعتماد إجراءات التَّوظيف، وقرارات التَّمويل المؤسَّسيَّة على عدد البحوث المنشورة ومعامل تأثيرها.  **المصطلحات ذات الصِّلة:** هيكل الحوافز، معامل تأثير المجلَّة، أزمة إعادة الإنتاج (أو أزمة التِّكرار)، التَّقطيع غير المبرَّر، العلم البطيء.",
                "related_terms": [
                    "Incentive structure",
                    "Journal Impact Factor",
                    "Reproducibility crisis (aka Replicability or replication crisis)",
                    "Salami slicing",
                    "Slow Science"
                ],
                "references": "Case, C. M. (1928). Scholarship in Sociology. Sociology and Social Research, 12, 323–340. http://www.sudoc.fr/036493414\n\nFanelli, D. (2010). Do Pressures to Publish Increase Scientists’ Bias? An Empirical Support from US States Data. PLOS ONE, 5(4), e10271. https://doi.org/10.1371/journal.pone.0010271",
                "drafted_by": [
                    "Eliza Woodward"
                ],
                "reviewed_by": [
                    "Nick Ballou",
                    "Mahmoud Elsherif",
                    "Helena Hartmann",
                    "Annalise A. LaPlume",
                    "Sam Parsons",
                    "Timo Roettger",
                    "Olmo van den Akker"
                ],
                "alt_related_terms": [
                    null
                ],
                "language": "arabic"
            },
            {
                "type": "glossary",
                "title": "منصة بابّير PubPeer (PubPeer)",
                "definition": "موقع ويب يسمح للمستخدمين بنشر مراجعات أقران مجهولة المصدر للبحوث ا المنشورة (أي مراجعة الأقران بعد النَّشر).  **المصطلحات ذات الصِّلة:** التّحكيم المفتوح.",
                "related_terms": [
                    "Open Peer Review"
                ],
                "references": "PubPeer. (n.d.). PubPeer—Search publications and join the conversation. Pubpeer. https://www.pubpeer.com/",
                "drafted_by": [
                    "Ali H. Al-Hoorie"
                ],
                "reviewed_by": [
                    "Mahmoud ELsherif"
                ],
                "alt_related_terms": [
                    null
                ],
                "language": "arabic"
            },
            {
                "type": "glossary",
                "title": "بايثون (Python)",
                "definition": "لغة برمجة عامَّة الغرض، تهدف إلى أن تكون سهلة الاستخدام وسهلة القراءة.  تم إنشاؤها في الأصل بواسطة غويدو فإنَّ روسوم في عام 1991\\. و تمتلك بايثون مكتبة واسعة من المميّزات الإضافيَّة مع وثائق يمكن الوصول إليها للمهام التي تتراوح من تحليل البيانات إلى إنشاء التَّجارب. تعد بايثون لغة برمجة شائعة في علوم البيانات، والتَّعلُّم الآلي وتطوير الشبكة. على غرار موقع آر مارك داون (R Markdown)، يمكن تقديم بايثون بتنسيق تفاعلي عبر الإنترنت يسمى مشروع جوبتر نوت بوك (Jupyter notebook)، والذي يجمع بين التَّعليمات البرمجيَّة، والبيانات والنُّصوص.  **المصطلحات ذات الصِّلة:** جوبترJupyter، مات بلوت ليب (Matplotlib)،  نمباي (NumPy، اوبن سيسمي (OpenSesame)، بيسكو بي (PsychoPy)، لغة الآر",
                "related_terms": [
                    "Jupyter",
                    "Matplotlib",
                    "NumPy",
                    "OpenSesame",
                    "PsychoPy",
                    "R"
                ],
                "references": "Lutz, M. (2001). Programming Python. O’Reilly Media, Inc.",
                "drafted_by": [
                    "Shannon Francis"
                ],
                "reviewed_by": [
                    "James E. Bartlett",
                    "Alexander Hart",
                    "Helena Hartmann",
                    "Dominik Kiersz",
                    "Graham Reid",
                    "Andrew J. Stewart"
                ],
                "alt_related_terms": [
                    null
                ],
                "language": "arabic"
            },
            {
                "type": "glossary",
                "title": "البحث النَّوعي (Qualitative research)",
                "definition": "البحث الذي يستخدم بيانات غير رقميَّة، مثل الإجابات النَّصيَة، أو الصُّور، أو مقاطع الفيديو وما شابه، لاستكشاف المفاهيم، أو النَّظريَّات، أو التَّجارب بعمق. هناك مجموعة واسعة من الأساليب النَّوعيَّة بدءًا من الاستكشاف التَّفصيلي الدَّقيق للغة، أو التَّركيز على التَّجارب الشَّخصيَّة إلى تلك التي تستكشف التَّجارب، والآراء الاجتماعيَّة على المستوى الأكبر. **المصطلحات ذات الصِّلة:** تأطير المقابلات؛  الموضعيَّة؛ البحث الكمي؛ الانعكاسيَّة **تعريف بديل:** (إن وُجِد) في علم النَّفس، عادةً ما تهتم نظرية المعرفة للبحث النوعي بفهم وجهات نظر النَّاس. تقترح نظريَّة المعرفة هذه افتراض المساواة بين الباحثين، والمشاركين كبشر، وبالتَّالي الحاجة إلى فهم إنساني متعاطف بدلًا من الاستنتاجات المستندة إلى البيانات.",
                "related_terms": [
                    "Bracketing Interviews",
                    "Positionality",
                    "Quantitative research",
                    "Reflexivity **Alternative definition:** (if applicable) In Psychology, the **epistemology** of qualitative research is typically concerned with understanding people’s perspectives. Such epistemology proposes assuming the equity of researchers and participants as human beings, and in consequence, the need of sympathetic human understanding instead of data-driven conclusions"
                ],
                "references": "Aspers, P., & Corte, U. (2019). What is qualitative in qualitative research. Qualitative Sociology, 42(2), 139–160. https://doi.org/10.1007/s11133-019-9413-7\n\nLevitt, H. M., Motulsky, S. L., Wertz, F. J., Morrow, S. L., & Ponterotto, J. G. (2017). Recommendations for designing and reviewing qualitative research in psychology: Promoting methodological integrity. Qualitative Psychology, 4(1), 2. https://doi.org/10.1037/qup0000082",
                "drafted_by": [
                    "Madeleine Pownall"
                ],
                "reviewed_by": [
                    "Mahmoud Elsherif",
                    "Helena Hartmann",
                    "Oscar Lecuona",
                    "Claire Melia",
                    "Flávio Azevedo"
                ],
                "alt_related_terms": [
                    null
                ],
                "language": "arabic"
            },
            {
                "type": "glossary",
                "title": "البحث الكميّ (Quantitative research)",
                "definition": "يشمل البحث الكمي مجموعة متنوِّعة من الأساليب للبحث بشكل منهجي في مجموعة من الظَّواهر من خلال استخدام البيانات الرَّقمية التي يمكن تحليلها إحصائيًا.  **المصطلحات ذات الصِّلة:** القياس، البحث النَّوعي، حجم العيّنة، القوة الإحصائيَّة، إحصائيّات.",
                "related_terms": [
                    "Measuring",
                    "Qualitative research",
                    "Sample size",
                    "Statistical power",
                    "Statistics"
                ],
                "references": "Goertzen, M. J. (2017). Introduction to Quantitative Research and Data. Library Technology Reports, 53(4), 12–18.",
                "drafted_by": [
                    "Aoife O’Mahony"
                ],
                "reviewed_by": [
                    "Valeria Agostini",
                    "Tamara Kalandadze",
                    "Adam Parker"
                ],
                "alt_related_terms": [
                    null
                ],
                "language": "arabic"
            },
            {
                "type": "glossary",
                "title": "QRPs ممارسات البحث المشكوك فيها أو ممارسات إعداد التَّقارير المشكوك فيها (Questionable Research Practices or Questionable Reporting Practices (QRPs))",
                "definition": "مجموعة من الأنشطة التي تشوه البيانات عن قصد، أو عن غير قصد لصالح فرضيَّات الباحث نفسه \\- أو إغفال الإبلاغ عن مثل هذه الممارسات \\- بما في ذلك التَّضمين الانتقائي للبيانات، والافتراض بعد معرفة النَّتائج، وقرصنة القيمة الاحتماليَّة، وشاعت هذه التَّسمية بعد بحث جون وزملاؤه (John et al., 2012).  **المصطلحات ذات الصِّلة:** الاستخدام الإبداعي للقيم المتطرِّفة. التَّزوير ، درج الملفَّات، حديقة المسارات المتشعِّبة، الافتراض بعد معرفة النَّتائج، ترقيم عدم نشر البيانات، قرصنة القيمة الاحتماليَّة  ، صيد القيمة الاحتماليَّة P ، النَّشر الجزئي للبيانات، رواية القصص اللَّاحقة، التَّسجيل المسبق، ممارسات القياس المشكوك فيها، درجات حرية الباحث، القرصنة العكسيَّة للقيمة الاحتماليَّة، التَّقطيع غير المبرَّر",
                "related_terms": [
                    "Creative use of outliers",
                    "Fabrication",
                    "File-drawer",
                    "Garden of forking paths",
                    "HARKing",
                    "Nonpublication of data",
                    "*P*\\-hacking",
                    "*P*\\-value fishing",
                    "Partial publication of data",
                    "Post-hoc storytelling",
                    "Preregistration",
                    "Questionable Measurement Practices (QMP)",
                    "Researcher degrees of freedom",
                    "Reverse *p*\\-hacking",
                    "Salami slicing"
                ],
                "references": "Banks, G. C., Rogelberg, S. G., Woznyj, H. M., Landis, R. S., & Rupp, D. E. (2016). Editorial: Evidence on questionable research practices: The good, the bad, and the ugly. Journal of Business and Psychology, 31(3), 323–338. https://doi.org/10.1007/s10869-016-9456-7\n\nFiedler, K., & Schwarz, N. (2016). Questionable research practices revisited. Social Psychological and Personality Science, 7(1), 45–52. https://doi.org/10.1177/1948550615612150\n\nHardwicke, T. E., Jameel, L., Jones, M., Walczak, E. J., & Weinberg, L. M. (2014). Only human: Scientists, systems, and suspect statistics. Opticon1826, 16, 25. https://doi.org/10.5334/OPT.CH\n\nJohn, L. K., Loewenstein, G., & Prelec, D. (2012). Measuring the prevalence of questionable research practices with incentives for truth telling. Psychological Science, 23(5), 524–532. https://doi.org/10.1177/0956797611430953\n\nNeuroskeptic. (2012). The nine circles of scientific hell. Perspectives on Psychological Science, 7(6), 643–644. https://doi.org/10.1177/1745691612459519\n\nSijtsma, K. (2016). Playing with data—Or how to discourage questionable research practices and stimulate researchers to do things right. Psychometrika, 81(1), 1–15. https://doi.org/10.1007/s11336-015-9446-0",
                "drafted_by": [
                    "Mahmoud Elsherif"
                ],
                "reviewed_by": [
                    "Tamara Kalandadze",
                    "William Ngiam",
                    "Sam Parsons",
                    "Mariella Paul",
                    "Eike Mark Rinke",
                    "Timo Roettger",
                    "Flávio Azevedo"
                ],
                "alt_related_terms": [
                    null
                ],
                "language": "arabic"
            },
            {
                "type": "glossary",
                "title": "QMP ممارسات القياس المشكوك فيها (Questionable Measurement Practices (QMP))",
                "definition": "القرارات التي يتَّخذها الباحثون، والتي تثير الشُّكوك حول صحة المقاييس المستخدمة في الدِّراسة، وبالتَّالي التَّشكيك في صحة نتائج هذه الدِّراسة (Flake & Fried, 2020). وتنشأ المشكلات من انعدام الشَّفافيَّة في الإبلاغ عن ممارسات القياس، أو الفشل في معالجة الصِّدق البنائي، أو الإهمال، أو الجهل، أو التَّحريف المتعمَّد للمعلومات.  **المصطلحات ذات الصِّلة:** الصِّدق البنائي، القياس،  قرصنة القيمة الاحتماليَّة ، القياس النَّفسي، ممارسات البحث المشكوك فيها، أو ممارسات إعداد التَّقارير المشكوك فيها، الصدق",
                "related_terms": [
                    "Construct validity",
                    "Measurement schmeasurement",
                    "*P*\\-hacking",
                    "Psychometrics",
                    "Questionable Research Practices or Questionable Reporting Practices (QRPs)",
                    "Validity"
                ],
                "references": "Flake, J. K., & Fried, E. I. (2020). Measurement schmeasurement: Questionable measurement practices and how to avoid them. Advances in Methods and Practices in Psychological Science, 3(4), 456–465. https://doi.org/10.1177/2515245920952393",
                "drafted_by": [
                    "Halil Emre Kocalar"
                ],
                "reviewed_by": [
                    "Jamie P. Cockcroft",
                    "Annalise A. LaPlume",
                    "Sam Parsons",
                    "Mirela Zaneva",
                    "Flávio Azevedo"
                ],
                "alt_related_terms": [
                    null
                ],
                "language": "arabic"
            },
            {
                "type": "glossary",
                "title": "لغة الآر (R)",
                "definition": "هي لغة برمجة مجانيَّة ومفتوحة المصدر وبيئة برمجيَّة يمكن استخدامها لإجراء التَّحليلات الإحصائيَّة ورسم البيانات.  أنشأ هذه اللُّغة روس إسحاق، وروبرت جنتلمان في جامعة أوكلاند. يساعد برنامج الآر المؤلفين من مشاركة البرامج النَّصية للتَّحليل القابلة للتِّكرار، مما يزيد من شفافيَّة الدِّراسة. غالبًا ما يتم استخدام الار جنبًا إلى جنب مع بيئة التَّطوير المتكاملة التي تبسط العمل مع اللُّغة، مثل RStudio  أو Visual Studio Code، أو Tinn-R. **مصطلحات ذات العلاقة:** مفتوح المصدر, التحليلات الإحصائية.",
                "related_terms": [
                    "Open-source",
                    "Statistical analysis"
                ],
                "references": "R Project for Statistical Computing. (n.d.). R: The R Project for Statistical Computing. R Project. https://www.r-project.org/",
                "drafted_by": [
                    "Lisa Spitzer"
                ],
                "reviewed_by": [
                    "Bradley Baker",
                    "Alexander Hart",
                    "Joanne McCuaig",
                    "Andrew J. Stewart"
                ],
                "alt_related_terms": [
                    null
                ],
                "language": "arabic"
            },
            {
                "type": "glossary",
                "title": "الفرق الحمراء (Red Teams)",
                "definition": "نهج يدمج بين النَّقد الخارجي، ونقد الأقران في عمليَّة البحث. تعتمد الفرق الحمراء على فكرة أنَّ البحث الذي يتم تقييمه بشكل نقدي، وواسع يكون أكثر موثوقيَّة.  نشأ هذا المصطلح من التَّدريبات العسكريَّة: مجموعة (الفريق الأحمر) تهاجم شيئًا ما، ومجموعة أخرى (الفريق الأزرق) تدافع عنه. تم تطبيق هذه الممارسة على العلم المفتوح، من خلال إعطاء الفريق الأحمر (الأفراد الناقدين) حوافز ماليَّة للعثور على أخطاء في البحث، أو لتحديد تحسينات على البحث ومحتواه، مثلًا في المواد، أو النُّصوص البرمجيَّة، أو الكتابة، إلى آخره (Coles et al., 2020).  **المصطلحات ذات الصِّلة:** التَّشارك العدائي",
                "related_terms": [
                    "Adversarial collaboration"
                ],
                "references": "Coles, N. A., Tiokhin, L., Arslan, R., Forscher, P., Scheel, A., & Lakens, D. (2020). Red Team Challenge. http://daniellakens.blogspot.com/2020/05/red-team-challenge.html\n\nLakens, D. (2020). The 20% Statistician: Red Team Challenge. The 20% Statistician. http://daniellakens.blogspot.com/2020/05/red-team-challenge.html",
                "drafted_by": [
                    "Annalise A. LaPlume"
                ],
                "reviewed_by": [
                    "Nick Ballou",
                    "Mahmoud Elsherif**;** Thomas Rhys Evans",
                    "Helena Hartmann",
                    "Timo Roettger"
                ],
                "alt_related_terms": [
                    null
                ],
                "language": "arabic"
            },
            {
                "type": "glossary",
                "title": "الانعكاسيَّة (Reflexivity)",
                "definition": "تشير عمليَّة الانعكاسيَّة إلى النَّظر النَّقدي في المعرفة التي ننتجها من خلال البحث، وكيفيَّة إنتاجها، ودورنا كباحثين في إنتاج هذه المعرفة.  هناك أشكال مختلفة من الانعكاسيَّة. ففي الانعكاسيَّة الشَّخصية، ينظر الباحثون في تأثير تجاربهم الشَّخصيَّة على الإنتاج المعرفي، وفي الانعكاسيَّة الوظيفية ينظر الباحثون في الطَّريقة التي قد تؤثر بها أدوات وأساليب البحث لدينا على إنتاج المعرفة.  تهدف الانعكاسيَّة إلى لفت الانتباه إلى العوامل الأساسيَّة التي قد تؤثر على عمليَّة البحث، بما في ذلك تطوير أسئلة البحث وجمع البيانات والتَّحليل. ا**لمصطلحات ذات الصِّلة:** تأطير المقابلات،  البحث النَّوعي",
                "related_terms": [
                    "Bracketing Interviews",
                    "Qualitative Research"
                ],
                "references": "Braun, V., & Clarke, V. (2013). Successful Qualitative Research. SAGE Publications.\n\nFinlay, L., & Gough, B. (2008). Reflexivity: A practical guide for researchers in health and social sciences. John Wiley & Sons.",
                "drafted_by": [
                    "Claire Melia"
                ],
                "reviewed_by": [
                    "Gilad Feldman",
                    "Annalise A. LaPlume"
                ],
                "alt_related_terms": [
                    null
                ],
                "language": "arabic"
            },
            {
                "type": "glossary",
                "title": "التَّقرير المسجَّل (Registered Report)",
                "definition": "شكل من أشكال النَّشر العلمي يتضمَّن جولة أوليَّة من تحكيم الأقران للخلفيَّة المعرفية، وطرق البحث (تصميم الدِّراسة والقياس وخطة التّحليل)، يتم بعدها قبول المخطوطات عالية الجودة مبدئيًا. عادةً ما تحدث هذه المراجعة الأولى قبل جمع البيانات، ولكن من الممكن اقتراح تحليل بيانات تم جمعها بهذه الطَّريقة. وبعد الانتهاء من تحليل البيانات، وكتابة النَّتائج والمناقشة، تركِّز المرحلة الثَّانية من المراجعة على ما إذا كان المؤلفون قد اتبعوا خطة مقترحة مسبقًا بشكلٍ كافٍ، وأبلغوا عن الانحرافات عنها دون التَّركيز على النَّتائج.  يؤدِّي هذا إلى تحويل تركيز المراجعة إلى سؤال ومنهجيَّة البحث المقترحة للدِّراسة، وبعيدًا عن الاهتمام المتصوَّر بنتائج الدِّراسة.  **المصطلحات ذات الصِّلة:** التَّسجيل المسبق، تحيُّز النَّشر (مشكلة درج الملفَّات)، مراجعة خالية من النّتائج ، منظمةالأقران، مراسم البحث.",
                "related_terms": [
                    "Preregistration",
                    "Publication bias (File Drawer Problem)",
                    "Results-free review",
                    "PCI (Peer Community In)",
                    "Research Protocol"
                ],
                "references": "Chambers, C. D. (2013). Registered reports: a new publishing initiative at Cortex. Cortex, 49(3), 609–610. https://doi.org/10.1016/j.cortex.2012.12.016\n\nChambers, C. D., Dienes, Z., McIntosh, R. D., Rotshtein, P., & Willmes, K. (2015). Registered reports: realigning incentives in scientific publishing. Cortex, 66, A1–A2. https://doi.org/10.1016/j.cortex.2015.03.022\n\nChambers, C. D., & Tzavella, L. (2020). Registered Reports: Past, Present and Future. https://doi.org/10.31222/osf.io/43298\n\nFindley, M. G., Jensen, N. M., Malesky, E. J., & Pepinsky, T. B. (2016). Can results-free review reduce publication bias? The results and implications of a pilot study. Comparative Political Studies, 49(13), 1667–1703. https://doi.org/10.1177/0010414016655539",
                "drafted_by": [
                    "Madeleine Pownall"
                ],
                "reviewed_by": [
                    "Gilad Feldman",
                    "Emma Henderson",
                    "Aoife O’Mahony",
                    "Sam Parsons",
                    "Mariella Paul",
                    "Charlotte R. Pennington",
                    "Eike Mark Rinke",
                    "Timo Roettger",
                    "Olmo van den Akker",
                    "Yuki Yamada",
                    "Flávio Azevedo"
                ],
                "alt_related_terms": [
                    null
                ],
                "language": "arabic"
            },
            {
                "type": "glossary",
                "title": "سجّل مستودعات بيانات البحث (Registry of Research Data Repositories)",
                "definition": "سجل عالمي لمستودعات البيانات البحثيَّة من مختلف التَّخصُّصات الأكاديميَّة، ويشمل المستودعات التي تمكِّن من التَّخزين الدَّائم، والتَّوصيف عبر البيانات الوصفيَّة، والوصول إلى مجموعات البيانات من قبل الباحثين، وهيئات التَّمويل، والنَّاشرين، والمؤسسات العلميَّة.  **المصطلحات ذات الصِّلة:** البيانات الوصفيَّة، الوصول المفتوح، البيانات المفتوحة، المواد المفتوحة، مستودع البيانات",
                "related_terms": [
                    "Metadata",
                    "Open Access",
                    "Open Data",
                    "Open Material",
                    "Repository"
                ],
                "references": "Anon. (n.d.). Home | re3data.org. Retrieved from https://www.re3data.org/",
                "drafted_by": [
                    "Aleksandra Lazić"
                ],
                "reviewed_by": [
                    "Mahmoud Elsherif",
                    "Sam Parsons",
                    "Charlotte R. Pennington",
                    "Helena Hartmann"
                ],
                "alt_related_terms": [
                    null
                ],
                "language": "arabic"
            },
            {
                "type": "glossary",
                "title": "الثَّبات (Reliability)",
                "definition": "يشير الثَّبات إلى مدى تكرار القياس، والحصول على نفس النَّتائج، وفي القياس النَّفسي يشير الثَّبات إلى مدى حصول المستجيبين على درجات مماثلة عندما يقومون بتعبئة الاستبانة في أوقات مختلفة، ومن الجدير بالذِّكر أنَّ الثَّبات لا يعني الصِّدق بالإضافة إلى ذلك، توجد أنواع أخرى من الثَّبات إلى جانب الاتسِّاق الدَّاخلي، وتشمل: ثبات إعادة الاختبار، وثبات النَّماذج المتكافئة، وثبات المقيمين/المحكمين.  **المصطلحات ذات الصِّلة:** الاتِّساق، الاتِّساق الدَّاخلي، معايير الجودة، قابلية التِّكرار، قابليَّة إعادة الإنتاج، الصِّدق.",
                "related_terms": [
                    "Consistency",
                    "Internal consistency",
                    "Quality Criteria",
                    "Replicability",
                    "Reproducibility",
                    "Validity"
                ],
                "references": "Bollen, K. A. (1989). Structural Equations with Latent Variables (pp. 179–225). John Wiley & Sons.\n\nDrost, E. A. (2011). Validity and reliability in social science research. Education Research and Perspectives, 38(1), 105–123.",
                "drafted_by": [
                    "Annalise A. LaPlume"
                ],
                "reviewed_by": [
                    "Mahmoud Elsherif**;** Eduardo Garcia-Garzon",
                    "Kai Krautter",
                    "Olmo van den Akker"
                ],
                "alt_related_terms": [
                    null
                ],
                "language": "arabic"
            },
            {
                "type": "glossary",
                "title": "التِّكرار (Repeatability)",
                "definition": "مرادف لثبات إعادة الاختبار، ويشير إلى الاتِّفاق بين نتائج القياسات المتتالية لنفس المقياس، يتطلّب التِّكرار نفس الأدوات التَّجريبيَّة، ونفس المراقب، ونفس أداة القياس التي تدار في ظل نفس الظُّروف، ونفس الموقع، والتِّكرار على مدى فترة زمنيَّة قصيرة، ونفس الأهداف (Joint Committee for Guidelines in Metrology, 2008).  **المصطلحات ذات الصِّلة:** الثَّبات",
                "related_terms": [
                    "Reliability"
                ],
                "references": "ISO. (1993). Guide to the Expression of Uncertainty in Measurement (1st ed.). International Organization for Standardization.\n\nStodden, V. C. (2011). Trust your science? Open your data and code.",
                "drafted_by": [
                    "Mahmoud Elsherif, Adam Parker"
                ],
                "reviewed_by": [
                    "Gilad Feldman",
                    "Helena Hartmann",
                    "Joanne McCuaig",
                    "Sam Parsons"
                ],
                "alt_related_terms": [
                    null
                ],
                "language": "arabic"
            },
            {
                "type": "glossary",
                "title": "قابليَّة التِّكرار (Replicability)",
                "definition": "مصطلح شامل، يستخدم بشكل مختلف عبر المجالات، ويغطي مفاهيم: التِّكرار المباشر والمفاهيمي، إعادة الإنتاج الحسابي، وتحليل قابليَّة التَّعميم، وتحليلات القوّة، وتتضمَّن بعض التَّعريفات المستخدمة سابقًا: وصول فريق مختلف إلى نفس النَّتائج باستخدام أدوات  المؤلِّف الأصليَّة (Barba, 2018)، وصول دراسة إلى نفس النَّتيجة بعد جمع بيانات جديدة (Claerbout & Karrenbach, 1992)، بالإضافة إلى الدِّراسات التي يمكن اعتبار نتائجها دليلًا تشخيصيًا حول مصداقيَّة نتائج بحوث سابقة (Nosek & Errington, 2020).  **المصطلحات ذات الصِّلة:** التِّكرار المفاهيمي، التِّكرار المباشر ، الابليَّة للتَّعميم، قابليَّة إعادة الإنتاج، الثَّبات، المتانة (في التَّحليلات)",
                "related_terms": [
                    "Conceptual replication",
                    "Direct Replication",
                    "Generalizability",
                    "Reproducibility",
                    "Reliability",
                    "Robustness (analyses)"
                ],
                "references": "Barba, L. A. (2018). Terminologies for reproducible research. arXiv Preprint arXiv:1802.03311.\n\nCrüwell, S., van Doorn, J., Etz, A., Makel, M. C., Moshontz, H., Niebaum, J. C., Orben, A., Parsons, S., & Schulte-Mecklenbeck, M. (2019). Seven Easy Steps to Open Science: An Annotated Reading List. Zeitschrift Für Psychologie, 227(4), 237–248. https://doi.org/10.1027/2151-2604/a000387\n\nKing, G. (1995). Replication, replication. PS: Political Science & Politics, 28(3), 444–452. https://doi.org/10.2307/420301\n\nNosek, B. A., & Errington, T. M. (2020). What is replication? PLOS Biology, 18(3), e3000691. https://doi.org/10.1371/journal.pbio.3000691",
                "drafted_by": [
                    "Mahmoud Elsherif"
                ],
                "reviewed_by": [
                    "Jamie P. Cockcroft",
                    "Adrien Fillon",
                    "Gilad Feldman",
                    "Annalise A. LaPlume",
                    "Tina B. Lonsdorf",
                    "Sam Parsons",
                    "Eike Mark Rinke",
                    "Tobias Wingen"
                ],
                "alt_related_terms": [
                    null
                ],
                "language": "arabic"
            },
            {
                "type": "glossary",
                "title": "أسواق التِّكرار (Replication Markets)",
                "definition": "سوق التِّكرار هو بيئة يراهن فيها المستخدمون على إمكانيَّة تكرار تأثيرات معيَّنة. يحفّز المتنبؤن لإجراء تنبؤات دقيقة بتلقي تعويضًا ماليًا أو مساهمة معيّنة مقابل رهاناتهم في حال نجاح توقعاتهم.  المنطق وراء سوق التِّكرار هو الاعتماد على الحكمة الجماعيَّة للمجتمع العلمي للتنبؤ بالتأثير الذي من المرجح أن يتكرِّر، وبالتَّالي تشجيع الباحثين على توجيه مواردهم المحدودة لتكرار هذه التَّأثيرات.  **المصطلحات ذات الصِّلة:** علم المواطن. التَّعهيد الجماعي، قابليَّة التِّكرار، قابليَّة إعادة الإنتاج",
                "related_terms": [
                    "Citizen science",
                    "Crowdsourcing",
                    "Replicability",
                    "Reproducibility"
                ],
                "references": "Liu, Y., Gordon, M., Wang, J., Bishop, M., Chen, Y., Pfeiffer, T., Twardy, C., & Viganola, D. (2020). Replication Markets: Results, Lessons, Challenges and Opportunities in AI Replication. ArXiv:2005.04543 . http://arxiv.org/abs/2005.04543\n\nTierney, W., Hardy III, J. H., Ebersole, C. R., Leavitt, K., Viganola, D., Clemente, E. G., & others. (2020). Creative destruction in science. Organizational Behavior and Human Decision Processes, 161, 291–309. https://doi.org/10.1016/j.obhdp.2020.07.002\n\nTierney, W., Hardy III, J., Ebersole, C. R., Viganola, D., Clemente, E. G., Gordon, M., & others. (2021). A creative destruction approach to replication: Implicit work and sex morality across cultures. Journal of Experimental Social Psychology, 93, 104060. https://doi.org/10.1016/j.jesp.2020.104060\n\nReplication Markets. (n.d.). Replication Markets – Reliable research replicates…you can bet on it. Replication Markets. https://www.replicationmarkets.com/",
                "drafted_by": [
                    "Ali H. Al-Hoorie"
                ],
                "reviewed_by": [
                    "Mahmoud Elsherif",
                    "Leticia Micheli",
                    "Sam Parsons"
                ],
                "alt_related_terms": [
                    null
                ],
                "language": "arabic"
            },
            {
                "type": "glossary",
                "title": "دليل إعداد التَّقارير (Reporting Guideline)",
                "definition": "الدَّليل الإرشادي لإعداد التَّقارير هو \"قائمة مراجعة، أو مخطَّط، أو نص منظَّم لتوجيه المؤلِّفين في طريقة كتابة التقرير عن نوع معيَّن من البحث. تم تطويره باستخدام منهجيَّة واضحة\" (EQUATOR Network, n.d.). وتوفِّر إرشادات إعداد التَّقارير الحد الأدنى من التَّوجيه المطلوب لضمان إمكانيَّة تفسير نتائج البحث وتقييمها وتوليفها وتكرارها بشكل مناسب. وغالبَا ما يختلف استخدامها باختلاف المجلَّة العلميَّة، أو النَّاشر.  **المصطلحات ذات الصِّلة:** موقع CONSORT ، المراجعات المنهجيَّة غير التَّدخليَّة المفتوحة والقابلة للتِّكرار ،PRISMA ،STROBE",
                "related_terms": [
                    "CONSORT",
                    "Non-Intervention, Reproducible, and Open Systematic Reviews (NIRO-SR)",
                    "PRISMA",
                    "STROBE"
                ],
                "references": "Moher, D., Liberati, A., Tetzlaff, J., & Altman, D. (2009). Preferred Reporting Items for Systematic Reviews and Meta-Analyses: The PRISMA Statement. PLoS Medicine, 6(7), e1000097. https://doi.org/10.1371/journal.pmed.1000097\n\nSchulz, K. F., Altman, D. G., & Moher, D. (2010). CONSORT 2010 statement: updated guidelines for reporting parallel group randomised trials. Trials, 11(1), 32. https://doi.org/10.1186/1745-6215-11-32\n\nNetwork, T. E. (n.d.). What is a reporting guideline? Retrieved 10 July 2021. https://www.equator-network.org/about-us/what-is-a-reporting-guideline/",
                "drafted_by": [
                    "Aidan Cashin"
                ],
                "reviewed_by": [
                    "Gilad Feldman",
                    "Helena Hartmann",
                    "Joanne McCuaig"
                ],
                "alt_related_terms": [
                    null
                ],
                "language": "arabic"
            },
            {
                "type": "glossary",
                "title": "مستودع البيانات (Repository)",
                "definition": "أرشيف على الإنترنت لتخزين المواد الرَّقميَّة بما في ذلك مخرجات البحث والُّنصوص،  والنَّص البرمجي للتَّحليل والبيانات.  وتتضمَّن الأمثلة خوادم ما قبل الطِّباعة مثل bioRxiv و MetaArXiv و PsyArXiv ومستودعات البحث المؤسسيَّة، بالإضافة إلى مستودعات البيانات التي تجمع، وتخزِّن مجموعات البيانات بما في ذلك zenodo.org و PsychData ومستودعات النُّصوص البرمجيَّة  مثل Github أو مستودعات أكثر عموميَّة لجميع أنواع بيانات البحث، مثل إطار العلوم المفتوحة، ويتم عادةً وصف الكائنات الرَّقميَّة المخزَّنة في المستودعات من خلال البيانات التَّعريفيَّة التي يتاح اكتشافها عبر مواقع التَّخزين المختلفة.  **المصطلحات ذات الصِّلة:** مشاركة البيانات، Github البيانات الوصفيَّة، الوصول المفتوح، البيانات المفتوحة، المواد المفتوحة ، إطار العلوم المفتوحة، المصدر المفتوح، نسخة أوليَّة",
                "related_terms": [
                    "Data sharing",
                    "Github",
                    "Metadata",
                    "Open Access",
                    "Open data",
                    "Open Material",
                    "Open Science Framework",
                    "Open Source",
                    "Preprint"
                ],
                "references": "",
                "drafted_by": [
                    "Tina Lonsdorf"
                ],
                "reviewed_by": [
                    "Gilad Feldman",
                    "Connor Keating",
                    "Mariella Paul",
                    "Charlotte R. Pennington",
                    "Flávio Azevedo"
                ],
                "alt_related_terms": [
                    null
                ],
                "language": "arabic"
            },
            {
                "type": "glossary",
                "title": "نوادي الأبحاث التِّكراريَّة (ReproducibiliTea)",
                "definition": "هي مبادرة شعبيَّة تساعد الباحثين على إنشاء نوادي محليَّة في الجامعات؛ لمناقشة مجموعة من الموضوعات المتعلِّقة بالبحوث، والمعرفة المفتوحة.  عادة ما يتمحور كل اجتماع حول مناقشة ورقة علميَّة محدَّدة، مثل: إعادة الإنتاج، وممارسة البحث،وجودته، والعدالة الاجتماعيَّة، والشُّموليَّة، والأفكار المتعلِّقة بتحسين العلوم. **المصطلحات ذات الصِّلة:** مبادرة شعبيَّة، نادي المجلَّة، العلم المفتوح، قابليَّة إعادة الإنتاج",
                "related_terms": [
                    "Grassroots initiative",
                    "Journal club",
                    "Open science",
                    "Reproducibility"
                ],
                "references": "Orben, A. (2019). A journal club to fix science. Nature, 573(7775), 465–466. https://doi.org/10.1038/d41586-019-02842-8",
                "drafted_by": [
                    "Emma Norris"
                ],
                "reviewed_by": [
                    "Mahmoud Elsherif",
                    "Gilad Feldman",
                    "Connor Keating",
                    "Charlotte R. Pennington",
                    "Sam Parsons",
                    "Flávio Azevedo"
                ],
                "alt_related_terms": [
                    null
                ],
                "language": "arabic"
            },
            {
                "type": "glossary",
                "title": "قابليَّة إعادة الإنتاج (Reproducibility)",
                "definition": "يشير إلى الحد الأدنى من نطاق قابليَّة إعادة الإنتاج؛ لتقييم قيمة، أو دقّة الادِّعاءات العلميَّة بناءً على طرق البحث، والبيانات، والتَّعليمات البرمجيَّة الأصليَّة.  يشتمل ذلك على سبيل المثال حالات استخدام البيانات، والتَّعليمات البرمجيَّة للباحث الأصلي؛ لغرض إعادة إنتاج النَّتائج، والتي يشار إليها غالبًا بقابليَّة إعادة الإنتاج الحسابي.  لا تضمن قابليَّة إعادة الإنتاج جودة، أو دقّة، أو صدق النَّتائج المنشورة (Peng, 2011). و في بعض المجالات، يتم ربط هذه الفكرة بمصطلح \"قابليَّة التِّكرار\" أو \"قابليَّة الإعادة\".  **المصطلحات ذات الصِّلة:** الاستنساخ الحسابي، قابليّة التِّكرار، التِّكرار.",
                "related_terms": [
                    "Computational reproducibility",
                    "Replicability",
                    "repeatability"
                ],
                "references": "Barba, L. A. (2018). Terminologies for reproducible research. arXiv Preprint arXiv:1802.03311.\n\nCrüwell, S., van Doorn, J., Etz, A., Makel, M. C., Moshontz, H., Niebaum, J. C., Orben, A., Parsons, S., & Schulte-Mecklenbeck, M. (2019). Seven Easy Steps to Open Science: An Annotated Reading List. Zeitschrift Für Psychologie, 227(4), 237–248. https://doi.org/10.1027/2151-2604/a000387\n\nPeng, R. D. (2011). Reproducible Research in Computational Science. Science, 334(6060), 1226–1227. https://doi.org/10.1126/science.1213847\n\nStodden, V. C. (2011). Trust your science? Open your data and code.\n\nSyed, M. (2019). The Open Science Movement is for all of us. PsyArXiv.",
                "drafted_by": [
                    "Mahmoud Elsherif"
                ],
                "reviewed_by": [
                    "Helena Hartmann",
                    "Annalise A. LaPlume",
                    "Tina B. Lonsdorf",
                    "Sam Parsons",
                    "Charlotte R. Pennington",
                    "Suzanne L. K. Stewart"
                ],
                "alt_related_terms": [
                    null
                ],
                "language": "arabic"
            },
            {
                "type": "glossary",
                "title": "aka Replicability or replication crisis أزمة إعادة الإنتاج أو أزمة التِّكرار (Reproducibility crisis (aka Replicability or replication crisis))",
                "definition": "هي اكتشاف، وما تتعلق بها من تحوُّل في التَّفكير، والثَّقافة الأكاديميَّة، إنَّ نسبة كبيرة من الدِّراسات العلميَّة المنشورة في مختلف التَّخصُّصات غير قابلة للتِّكرار (Open Science Collaboration, 2015). ويعزى ذلك للافتقار إلى النَّزاهة، ونقص جودة وسلامة ممارسات البحث والنَّشر- مثل تحيُّز النَّشر- وممارسات البحث المشكوك فيها، والافتقار إلى الشَّفافيَّة؛ ممَّا يؤدِّي إلى تضخم معدَّل النّتائج الإيجابيَّة الخاطئة. ووصف آخرون هذه العمليَّة بأنَّها \"ثورة المصداقيَّة\" نحو تحسين هذه الممارسات.  **المصطلحات ذات الصِّلة**: أزمة المصداقيَّة، تحيُّز النَّشر (مشكلة درج الملفَّات) ، ممارسات البحث المشكوك فيها،أو ممارسات إعداد التَّقارير المشكوك فيها، قابليَّة التِّكرار، قابليَّة إعادة الإنتاج.",
                "related_terms": [
                    "Credibility crisis",
                    "Publication bias (File Drawer Problem)",
                    "Questionable Research Practices or Questionable Reporting Practices (QRPs)",
                    "Replicability",
                    "Reproducibility"
                ],
                "references": "Fanelli, D. (2018). Opinion: Is science really facing a reproducibility crisis, and do we need it to? Proceedings of the National Academy of Sciences, 115(11), 2628–2631. https://doi.org/10.1073/pnas.1708272114",
                "drafted_by": [
                    "Mahmoud Elsherif"
                ],
                "reviewed_by": [
                    "Helena Hartmann",
                    "Annalise A. LaPlume",
                    "Mariella Paul",
                    "Sonia Rishi",
                    "Lisa Spitzer"
                ],
                "alt_related_terms": [
                    null
                ],
                "language": "arabic"
            },
            {
                "type": "glossary",
                "title": "شبكة التكرار (Reproducibility Network)",
                "definition": "التعريف:** شبكة التِّكرار عبارة عن اتِّحاد يتكَّون من مجموعات تهتم بالبحوث المفتوحة، غالبًا ما يقودها الأقران. وتتبع هذه المجموعات نموذج العجلة، والشُّعاع في بلد معين، حيث تقوم الشَّبكة بربط الباحثين، والمجموعات والمؤسَّسات المحليَّة في تخصُّصات مختلفة بمجموعة توجيهيَّة مركزيَّة، والتي تتواصل أيضًا مع أصحاب المصلحة الخارجيين.  وتشمل أهداف شبكات التِّكرار الدَّعوة إلى مزيد من الوعي، وتعزيز أنشطة التَّدريب، ونشر أفضل الممارسات على المستوى الشَّعبي، والمؤسّسي، والبحثي، وتوجد مثل هذه الشَّبكات في المملكة المتَّحدة وألمانيا وسويسرا وسلوفاكيا وأستراليا (اعتبارًا من مارس 2021).",
                "related_terms": [],
                "references": "Network, U. R. (n.d.). UK Reproducibility Network. Retrieved 10 July 2021. https://www.ukrn.org/\n\nGRN · German Reproducibility Network. (n.d.). A German Reproducibility Network. Retrieved from https://reproducibilitynetwork.de/\n\nAnon. (n.d.). Domov | SKRN (Slovak Reproducibility network). Retrieved from https://slovakrn.wixsite.com/skrn\n\nAusRN. (n.d.). Australian Reproducibility Network. Retrieved from https://www.aus-rn.org/",
                "drafted_by": [
                    "Suzanne L. K. Stewart"
                ],
                "reviewed_by": [
                    "Annalise A. LaPlume",
                    "Sam Parsons",
                    "Flávio Azevedo"
                ],
                "alt_related_terms": [
                    null
                ],
                "language": "arabic"
            },
            {
                "type": "glossary",
                "title": "p مقياس المساهمة البحثيَّة نشر (Research Contribution Metric (*p*))",
                "definition": "طريقة للقياس الدّلالي الذي يقيّم تشابه المنشورات المرتبطة  بشبكة الاستشهاد، وتستخدم هذه الطَّريقة صيغة  بسيطة لتقييم مساهمات المؤلفين، فيمكن تقييمها على أساس المسافة الدّلالية من المنشورات التي استشهد بها إلى المنشورات المستشهَد بِها.  **المصطلحات ذات الصِّلة:** القياسات الدّلاليَّة",
                "related_terms": [
                    "Semantometrics"
                ],
                "references": "Knoth, P., & Herrmannova, D. (2014). Towards semantometrics: A new semantic similarity based measure for assessing a research publication’s contribution. D-Lib Magazine, 20(11), 8. https://doi.org/10.1045/november2014-knoth\n\nHolcombe, A. O. (2019). Contributorship, not authorship: Use CRediT to indicate who did what. Publications, 7(3), 48. https://doi.org/10.3390/publications7030048\n\nLarivière, V., Desrochers, N., Macaluso, B., Mongeon, P., Paul-Hus, A., & Sugimoto, C. R. (2016). Contributorship and division of labor in knowledge production. Social Studies of Science, 46(3), 417–435. https://doi.org/10.1177/0306312716650046",
                "drafted_by": [
                    "Alaa AlDoh"
                ],
                "reviewed_by": [
                    "Michele C. Lim",
                    "Jamie P. Cockcroft",
                    "Micah Vandegrift",
                    "Dominik Kiersz    ####"
                ],
                "alt_related_terms": [
                    null
                ],
                "language": "arabic"
            },
            {
                "type": "glossary",
                "title": "دورة البحث (Research Cycle)",
                "definition": "هي العمليَّة الدَّائريَّة لإجراء البحث العلمي حيث يعمل \"الباحثون في مراحل مختلفة في البحث، ابتداءً بالبحوث الأوليَّة، والاستكشافيَّة إلى اختبار الادِّعاءات الأكثر تحديدًا والمدعومة جيدًا\" (Lieberman, 2020, p. 42). وتشمل الدَّورة: البحث في الأدبيَّات، وتوليد الفرضيَّات، وجمع البيانات وتحليلها، بالإضافة إلى نشر النَّتائج (على سبيل المثال من خلال النَّشر في المجلَّات المحكّمة)، والتي بدورها تغذي النَّظريَّة والفرضيَّات والبحوث الجديدة.  **المصطلحات ذات الصِّلة:** عمليَّة البحث",
                "related_terms": [
                    "Research process"
                ],
                "references": "Bramoullé, Y., & Saint-Paul, G. (2010). Research cycles. In Journal of Economic Theory (Vol. 145, pp. 1890–1920). https://doi.org/10.2139/ssrn.965816\n\nLieberman, E. (2020). Research Cycles. In C. Elman, J. Gerring, & J. Mahoney (Eds.), The Production of Knowledge: Enhancing Progress in Social Science (pp. 42–70). Cambridge University Press. https://doi.org/10.1017/9781108762519.003",
                "drafted_by": [
                    "Helena Hartmann"
                ],
                "reviewed_by": [
                    "Jamie P. Cockcroft",
                    "Aleksandra Lazić",
                    "Graham Reid",
                    "Beatrice Valentini"
                ],
                "alt_related_terms": [
                    null
                ],
                "language": "arabic"
            },
            {
                "type": "glossary",
                "title": "إدارة بيانات البحث (Research Data Management)",
                "definition": "إدارة بيانات البحث هو مفهوم واسع يتضمَّن العمليَّات التي يتم إجراؤها لإنشاء بيانات بحثيَّة عالية الجودة، ومنظَّمة، وموثّقة ويمكن الوصول إليها، وقابلة لإعادة الاستخدام. توفِّر الإدارة الملائمة لبيانات البحث عدَّة مزايا بما في ذلك، على سبيل المثال لا الحصر: تقليل احتماليَّة فقدان البيانات، زيادة الشَّفافيَّة، والتَّعاون بسبب مشاركة البيانات، وإظهار النَّزاهة والمسؤوليَّة. **المصطلحات ذات الصِّلة:** تنظيم  البيانات، توثيق البيانات، خطة إدارة البيانات، مشاركة البيانات، البيانات الوصفيَّة، إدارة بيانات البحث",
                "related_terms": [
                    "Data curation",
                    "Data documentation",
                    "Data management plan (DMP)",
                    "Data sharing",
                    "Metadata",
                    "Research data management"
                ],
                "references": "Corti, L., Van den Eynden, V., Bishop, L., & Woollard, M. (2019). Managing and sharing research data: a guide to good practice. Sage.",
                "drafted_by": [
                    "Micah Vandegrift"
                ],
                "reviewed_by": [
                    "Helena Hartmann",
                    "Tina B. Lonsdorf",
                    "Catia M. Oliveira",
                    "Julia Wolska"
                ],
                "alt_related_terms": [
                    null
                ],
                "language": "arabic"
            },
            {
                "type": "glossary",
                "title": "نزاهة البحث (Research integrity)",
                "definition": "يتم تعريف نزاهة البحث من خلال مجموعة من الممارسات البحثيَّة الجيِّدة القائمة على المبادئ الأساسيَّة: الصِّدق، والموثوقيَّة، والاحترام، ، والمساءلة (ALLEA, 2017).  تشير ممارسات البحث الجيدة  \\_التي تعتمد على المبادئ الأساسيَّة لنزاهة البحث، والتي يجب أن توجه الباحثين في عملهم، وكذلك في تفاعلهم مع التَّحديّات العمليَّة، والأخلاقيَّة، والفكريَّة الكامنة  في البحث\\_  إلى مجالات مثل: بيئة البحث، على سبيل المثال: تعمل المؤسَّسات والمنظمات البحثيَّة على تعزيز الوعي، وضمان وجود ثقافة سائدة لنزاهة البحث، والتَّدريب والإشراف والتَّوجيه، مثلًا عندما تقوم المؤسَّسات، والمنظَّمات البحثيَّة بتطوير التَّدريب المناسب، والكافي في مجال الأخلاقيَّات، ونزاهة البحث؛ لضمان توعية جميع المعنيين بالقوانين والقواعد واللّوائح ذات الصِّلة، وإجراءات البحث، ومثال ذلك عندما يقوم الباحثون بالإبلاغ عن نتائجهم بطريقة متوافقة مع معايير التَّخصُّص، وعند الحاجة يمكن التَّحقُّق منها وإعادة إنتاجها، والضَّمانات (كأن يولي الباحثون الاعتبار الواجب للصحة والسلامة والبيئة) ورفاهية المجتمع والمتعاونين وغيرهم ممن لهم صلة ببحوثهم )، وممارسات البيانات وإدارتها  (كأن يوفر الباحثون والمؤسسات البحثيَّة والمنظمات الشَّفافيَّة حول كيفيَّة الوصول إلى البيانات، والمواد البحثيَّة الخاصّة بهم، أو الاستفادة منها)، والعمل التَّعاوني والنَّشر (على سبيل المثال: يعد المؤلفون والناشرون النَّتائج السَّلبيَّة صالحة للنَّشر، والتَّوزيع مثل النَّتائج الإيجابيَّة) والمراجعة والتَّقييم والتَّحرير (كأن يقوم الباحثون بمراجعة وتقييم الطَّلبات المقدَّمة للنَّشر، أو التَّمويل، أو التَّعيين، أو التَّرقية، أو المكافأة بطريقة شفافّة ومبررة).  **المصطلحات ذات الصِّلة:** مصداقيَّة الادِّعاءات العلميَّة، اكتشاف الأخطاء، أخلاق مهنيَّة، البحث المفتوح، ممارسات البحث المشكوك فيها، أو ممارسات إعداد التَّقارير المشكوك فيها (QRPs) ، ممارسات البحث المسؤولة ، الدِّقة، الشَّفافيَّة ، بحث جدير بالثِّقة.",
                "related_terms": [
                    "Credibility of scientific claims",
                    "Error detection",
                    "Ethics",
                    "Open research",
                    "Questionable Research Practices or Questionable Reporting Practices (QRPs)",
                    "Responsible Research Practices",
                    "Rigour",
                    "Transparency",
                    "Trustworthy research"
                ],
                "references": "Academies, A. A. E. (2017). The European Code of Conduct for Research Integrity. Revised Edition. Retrieved from https://allea.org/code-of-conduct/\n\nMedin, D. L. (2012). Rigor without rigor mortis: The APS Board discusses research integrity. APS Observer, 25(5–9), 27–28. https://www.psychologicalscience.org/observer/scientific-rigor\n\nMoher, D., Bouter, L., Kleinert, S., Glasziou, P., Sham, M. H., Barbour, V., & Dirnagl, U. (2020). The Hong Kong Principles for assessing researchers: Fostering research integrity. PLoS Biology, 18(7), e3000737. https://doi.org/10.1371/journal.pbio.3000737",
                "drafted_by": [
                    "Ana Barbosa Mendes; Flávio Azevedo"
                ],
                "reviewed_by": [
                    "Valeria Agostini",
                    "Bradley Baker",
                    "Gilad Feldman",
                    "Tamara Kalandadze",
                    "Charlotte R. Pennington"
                ],
                "alt_related_terms": [
                    null
                ],
                "language": "arabic"
            },
            {
                "type": "glossary",
                "title": "مراسم البحث (Research Protocol)",
                "definition": "وثيقة مفصَّلة يتم إعدادها قبل إجراء الدِّراسة، وغالبًا ما تتم كتابتها كجزء من التَّقديم على الأخلاقيَّات وطلبات التَّمويل البحثي.  يجب أن تتضمَّن المراسم معلومات تتعلق بخلفية الدِّراسة، ومبرِّراتها، وأهدافها، بالإضافة إلى الفرضيَّات التي تعكس توقُّعات الباحثين.  ويجب أن تقدِّم المراسم أيضًا \"وصفة\" لإجراء الدِّراسة، بما في ذلك التَّفاصيل المنهجيَّة، وخطط واضحة للتَّحليل، ويجب  استخدام أدلة أفضل الممارسات لإنشاء مراسم الدِّراسة لمنهجيَّات، ومجالات محدَّدة. ومن الممكن مشاركة مراسم البحث علنًا لجذب متعاونين جدد أو تسهيل التَّعاون الفعّال عبر المختبرات (مثل https://www.protocols.io/). وفي المجالات الطبيَّة والتَّعليميَّة، غالبًا ما تكون المراسم نوعًا منفصلًا من المقالات المناسبة للنَّشر في المجلّات.  ويمكن للباحثين اختيار التَّسجيل المسبق عندما لا تكون مشاركة المراسم، أو النَّشر ممارسة شائعة.  **المصطلحات ذات الصِّلة**: المعامل المتعدِّدة ، التَّسجيل المسبق.",
                "related_terms": [
                    "Many Labs",
                    "Preregistration"
                ],
                "references": "BMJ. (2015). Introducing ‘How to write and publish a Study Protocol’ using BMJ’s new eLearning programme: Research to Publication. Retrieved from https://blogs.bmj.com/bmjopen/2015/09/22/introducing-how-to-write-and-publish-a-study-protocol-using-bmjs-new-elearning-programme-research-to-publication/\n\nNosek, B. A., Ebersole, C. R., DeHaven, A. C., & Mellor, D. T. (2018). The preregistration revolution. Proceedings of the National Academy of Sciences, 115(11), 2600–2606. https://doi.org/10.1073/pnas.1708274114",
                "drafted_by": [
                    "Marta Topor"
                ],
                "reviewed_by": [
                    "Helena Hartmann",
                    "Bethan Iley",
                    "Annalise A. LaPlume",
                    "Charlotte Pennington"
                ],
                "alt_related_terms": [
                    null
                ],
                "language": "arabic"
            },
            {
                "type": "glossary",
                "title": "مسار البحث العلميّ (Research workflow)",
                "definition": "يشير هذا المصطلح إلى العمليَّة التي يبنى عليها البحث العلمي بدءًا من صياغة فكرة البحث إلى نشره، وعادة ما يكون سير العمل النّموذجي على النَّحو التَّالي: البدء بتصُّور المفاهيم لتحديد سؤال البحث، وتصميم الدِّراسة، وعند الانتهاء من تصميم البحث فإنَّه يتوجَّب على الباحث الحصول على الموافقة الأخلاقيَّة (إذا لزم الأمر) ويمكن أن يتم تسجيل نسخة نهائيَّة من الخطة البحثيَّة قبل البدء بالبحث. يقوم الباحث بعد ذلك بجمع البيانات وتحليلها وصولًا لآخر مرحلة وهي مرحلة نشر البحث، والتي ينتقل فيها الباحث بين مرحلتي ما قبل وما بعد الطِّباعة عند تسليمه البحث لمجلة علميَّة.  **المصطلحات ذات الصِّلة:** مسار البحث العلمي المفتوح، دورة البحث، خط الإنتاج البحثي.",
                "related_terms": [
                    "Open Research Workflow",
                    "Research cycle",
                    "Research pipeline"
                ],
                "references": "Kathawalla, U., Silverstein, P., & Syed, M. (2020). Easing into Open Science: A Guide for Graduate Students and Their Advisors. Collabra: Psychology. https://doi.org/10.31234/osf.io/vzjdp Retrieved from https://psyarxiv.com/vzjdp\n\nStodden, V. C. (2011). Trust your science? Open your data and code.",
                "drafted_by": [
                    "James E Bartlett"
                ],
                "reviewed_by": [
                    "Gilad Feldman",
                    "Helena Hartmann",
                    "Aleksandra Lazić",
                    "Joanne McCuaig",
                    "Timo Roettger",
                    "Sam Parsons",
                    "Steven Verheyen"
                ],
                "alt_related_terms": [
                    null
                ],
                "language": "arabic"
            },
            {
                "type": "glossary",
                "title": "درجات حرِّيَّة الباحث (Researcher degrees of freedom)",
                "definition": "يشير إلى المرونة الكامنة في كثير من الأحيان في العمليَّة العلميَّة، بدءً من إنشاء الفرضيَّات، وتصميم وإجراء دراسة بحثيَّة إلى معالجة البيانات وتحليلها وكذلك تفسير النَّتائج والإبلاغ عنها. ونظرًا لعدم وجود نظريَّات محدَّدة بدقّة، أو أدلَّة تجريبيَّة، غالبًا ما تكون القرارات المتعدِّدة مبرَّرة بنفس القدر، وبشكل متساوٍ. ويستخدم المصطلح أحيانًا للإشارة إلى الاستخدام الانتهازي، أو السّيء لهذه المرونة بهدف تحقيق النَّتائج المرجوّة \\-على سبيل المثال عند إدراج بيانات معيّنة أو استبعادها \\- على الرُّغم من حقيقة أنَّ المصطلح من النَّاحية الفنيَّة ليس محملًا بهذا المعنى بطبيعته.  **المصطلحات ذات الصِّلة:** المرونة التَّحليليَّة، حديقة المسارات المتشعبِّة، نموذج عدم اليقين، تحليل الأكاون المتعدِّدة، قرصنة القيمة الاحتماليَّة،  المتانة (في التحليلات)، تحليل منحنى المواصفات",
                "related_terms": [
                    "Analytic Flexibility",
                    "Garden of forking paths",
                    "Model uncertainty",
                    "Multiverse analysis",
                    "*P*\\-hacking",
                    "Robustness (analyses)",
                    "Specification curve analysis"
                ],
                "references": "Gelman, A., & Loken, E. (n.d.). The garden of forking paths: Why multiple comparisons can be a problem, even when there is no “fishing expedition” or “p-hacking” and the research hypothesis was posited ahead of time. Retrieved from http://www.stat.columbia.edu/\n\nSimmons, J. P., Nelson, L. D., & Simonsohn, U. (2011). False-positive psychology: Undisclosed flexibility in data collection and analysis allows presenting anything as significant. Psychological Science, 22(11), 1359–1366. https://doi.org/10.1177/0956797611417632\n\nWicherts, J. M., Veldkamp, C. L., Augusteijn, H. E., Bakker, M., Van Aert, R., & Van Assen, M. A. (2016). Degrees of freedom in planning, running, analyzing, and reporting psychological studies: A checklist to avoid p-hacking. Frontiers in Psychology, 7, 1832. https://doi.org/10.3389/fpsyg.2016.01832",
                "drafted_by": [
                    "Tina Lonsdorf"
                ],
                "reviewed_by": [
                    "Gilad Feldman",
                    "Helena Hartmann",
                    "Timo Roettger",
                    "Robbie C.M. van Aert",
                    "Flávio Azevedo"
                ],
                "alt_related_terms": [
                    null
                ],
                "language": "arabic"
            },
            {
                "type": "glossary",
                "title": "مشروع التِّكرار (RepliCATs project)",
                "definition": "يشير إلى مشروع \"التَّقييم التَّعاوني للعلوم الجديرة بالثقة\". ويهدف المشروع إلى جمع التنبؤات حول موثوقيَّة البحوث المنشورة، وإمكانيَّة تكرارها في ثمانية مجالات من مجالات العلوم الاجتماعيَّة وهي: بحوث الأعمال، وعلم الجريمة، والاقتصاد، والتَّعليم، والعلوم السِّياسيَّة، وعلم النَّفس، والإدارة العامّة، وعلم الاجتماع.  **المصطلحات ذات الصِّلة:** قابليَّة التِّكرار ، الجدارة بالثِّقة.",
                "related_terms": [
                    "Replicability",
                    "Trustworthiness"
                ],
                "references": "Fraser, H., Bush, M., Wintle, B., Mody, F., Smith, E., Hanea, A., & others. (2021). Predicting reliability through structured expert elicitation with repliCATS (Collaborative Assessments for Trustworthy Science).",
                "drafted_by": [
                    "Tamara Kalandadze"
                ],
                "reviewed_by": [
                    "Sarah Ashcroft-Jones",
                    "Mahmoud Elsherif",
                    "Gilad Feldman",
                    "Helena Hartmann",
                    "Charlotte R. Pennington"
                ],
                "alt_related_terms": [
                    null
                ],
                "language": "arabic"
            },
            {
                "type": "glossary",
                "title": "البحث والابتكار المسؤول (Responsible Research and Innovation)",
                "definition": "هو نهج يأخذ في الاعتبار الآثار، والتَّوقّعات المجتمعيَّة المتعلِّقة بالبحث والابتكار، بهدف تعزيز الشُّموليَّة والاستدامة.  وهو يفسر حقيقة أنَّ المساعي العلميَّة ليست معزولة عن آثارها الأوسع نطاقًا، وأنَّ البحث يحفّزه عوامل تتجاوز السَّعي وراء المعرفة، وعليه فإنَّ العديد من الأطراف مهمَّة في تعزيز البحوث المسؤولة، بما في ذلك هيئات التَّمويل، وفرق البحث، وأصحاب المصلحة، والنَّاشطين، والجمهور.  **المصطلحات ذات الصِّلة:**  علم المواطن، المشاركة العامة،  البحوث متعدِّدة التَّخصُّصات.",
                "related_terms": [
                    "Citizen Science",
                    "Public Engagement",
                    "Transdisciplinary Research"
                ],
                "references": "",
                "drafted_by": [
                    "Ana Barbosa Mendes"
                ],
                "reviewed_by": [
                    "Helena Hartmann",
                    "Joanne McCuaig",
                    "Sam Parsons",
                    "Graham Reid"
                ],
                "alt_related_terms": [
                    null
                ],
                "language": "arabic"
            },
            {
                "type": "glossary",
                "title": "القرصنة العكسيَّة للقيمة الاحتماليَّة (Reverse p-hacking)",
                "definition": "استغلال درجات حريَّة الباحث أثناء التَّحليل الإحصائي من أجل زيادة احتماليَّة قبول الفرضيَّة الصِّفريَّة (ككون القيمة الاحتماليَّة أكبر من 0.05).  **المصطلحات ذات الصِّلة**: المرونة التَّحليليَّة، الافتراض بعد معرفة النَّتائج، ممارسات البحث المشكوك فيهاأو ممارسات إعداد التَّقارير المشكوك فيها، درجات حريَّة الباحث، التَّقارير الانتقائيَّة.",
                "related_terms": [
                    "Analytic flexibility",
                    "HARKing",
                    "P-hacking",
                    "Questionable Research Practices or Questionable Reporting Practices (QRPs)",
                    "Researcher degrees of freedom",
                    "Selective reporting"
                ],
                "references": "Chuard, P. J. C., Vrtilek, M., Head, M. L., & Jennions, M. D. (2019). Evidence that non-significant results are sometimes preferred: Reverse P-hacking or selective reporting? PLoS Biol, 17(1), e3000127. https://doi.org/10.1371/journal.pbio.3000127",
                "drafted_by": [
                    "Robert M. Ross"
                ],
                "reviewed_by": [
                    "Mahmoud Elsherif",
                    "Alexander Hart",
                    "Sam Parsons",
                    "Timo Roettger"
                ],
                "alt_related_terms": [
                    null
                ],
                "language": "arabic"
            },
            {
                "type": "glossary",
                "title": "منتدى العلوم المتعدِّد القابلة للتِّكرار، والقابلة للتَّفسير، والمفتوحة، والشَّفافة نادي رايوت للعلوم (RIOT Science Club)",
                "definition": "هو عبارة عن سلسلة ندوات متعدِّدة المواقع تعمل على زيادة الوعي، وتوفير التَّدريب على الممارسات العلميَّة القابلة للتِّكرار، والتَّفسير، والمفتوحة، والشَّفافة، وهو يوفِّر محاضرات وورش عمل ومؤتمرات منتظمة، وكلّها متاحة بشكل مفتوح، ويمكن إعادة مشاهدتها على مواقع الويب الخاصّة بالموقع المعني وعلى اليوتيوب.  **المصطلحات ذات الصِّلة:** الباحثون المبتدئون، القابليَّة للتَّفسير، الانفتاح، قابلية إعادة الإنتاج، الشفافية.",
                "related_terms": [
                    "Early career researchers (ECRs)",
                    "Interpretability",
                    "Openness",
                    "Reproducibility",
                    "Transparency"
                ],
                "references": "",
                "drafted_by": [
                    "Tamara Kalandadze"
                ],
                "reviewed_by": [
                    "Helena Hartmann",
                    "Emma Henderson",
                    "Joanne McCuaig",
                    "Flávio Azevedo"
                ],
                "alt_related_terms": [
                    null
                ],
                "language": "arabic"
            },
            {
                "type": "glossary",
                "title": "analyses المتانة في التحليلات (Robustness (analyses))",
                "definition": "استمرار دعم فرضيّة ما في ظل تغيُّر المسار المنهجي، أو التَّحليلي. وبعبارة أخرى، تطبيق طرق ومسارات تحليل مختلفة لفحص ما إذا كان نفس الاستنتاج مدعومًا في ظل ظروف تحليليَّة مختلفة.  **المصطلحات ذات الصِّلة**: المعامل المتعدِّدة، تحليل الأكوان المتعدِّدة، تحليلات الحساسيَّة، تحليل منحنى المواصفات **التَّعريف البديل:** \"تشير المتانة إلى استقرار الاستنتاجات التَّجريبيَّة للتَّغيرات في الافتراضات الأساسيَّة، أو الإجراءات التَّجريبيَّة وهذا يرتبط إلى حدٍّ ما بمفهوم التَّعميم (المعروف أيضًا باسم قابليّة النَّقل)، والذي يشير إلى استمرار التَّأثير في الإعدادات المختلفة عن إطار العمل التَّجريبي وخارجه \\[...\\] ما إذا كان تصميم الدِّراسة مشابهًا بدرجة كافية للتَّصميم الأصلي أم لا؛ ليتم اعتباره تكرارًا، أو \"اختبار المتانة\"، أو بعض الاختلافات العديدة التي تم تحديدها في التِّكرار الخالص، لا سيما في العلوم الاجتماعيَّة (كالتِّكرار المفاهيمي، والتِّكرار الكاذب)، وهو سؤال غير محسوم\" (Goodman et al., 2016).",
                "related_terms": [
                    "Many Labs",
                    "Multiverse analysis",
                    "Sensitivity analyses",
                    "Specification Curve Analysis **Alternative definition:** “Robustness refers to the stability of experimental conclusions to variations in either baseline assumptions or experimental procedures. It is somewhat related to the concept of generalizability (also known as transportability), which refers to the persistence of an effect in settings different from and outside of an experimental framework \\[...\\] Whether a study design is similar enough to the original to be considered a replication, a “robustness test,” or some of many variations of pure replication that have been identified, particularly in the social sciences (for example, conceptual replication, pseudoreplication), is an unsettled question” (Goodman et al., 2016)."
                ],
                "references": "Goodman, S. N., Fanelli, D., & Ioannidis, J. P. A. (2016). What does research reproducibility mean? Science Translational Medicine, 8(341), 341ps12-341ps12. https://doi.org/10.1126/scitranslmed.aaf5027\n\nNosek, B. A., & Errington, T. M. (2020). What is replication? PLOS Biology, 18(3), e3000691. https://doi.org/10.1371/journal.pbio.3000691",
                "drafted_by": [
                    "Tina Lonsdorf; Flávio Azevedo"
                ],
                "reviewed_by": [
                    "Gilad Feldman",
                    "Adrien Fillon",
                    "Helena Hartmann",
                    "Timo Roettger"
                ],
                "alt_related_terms": [
                    null
                ],
                "language": "arabic"
            },
            {
                "type": "glossary",
                "title": "التَّقطيع غير المبرَّر (Salami slicing)",
                "definition": "استراتيجيَّة مشبوهة في إجراء البحوث أو إعداد تقارير تهدف لزيادة عدد النُّصوص القابلة للنَّشر عن طريق \"تقسيم\" البيانات من دراسة واحدة. غالبًا ما يتم اتِّخاذ قرار التَّقسيم في وقت متأخِّر من العمليَّة البحثيَّة، وهو أحد الأمثلة على التَّلاعب بنظام الحوافِّز الأكاديميَّة. فمثلًا قد يتضمَّن ذلك نشر دراسات متعدِّدة تستند إلى مجموعة بيانات واحدة، أو نشر دراسات متعدِّدة من مواقع مختلفة لجمع البيانات دون الإشارة بشفافيَّة إلى مصدر البيانات في الأصل. مثل هذه الممارسات تشوِّه الأدبيَّات العلميَّة، وخاصّة التَّحليلات البعديَّة؛ لأنَّه ليس واضحًا ما إذا كانت  النَّتائج مستخلصة من نفس مجموعة البيانات، وبالتَّالي تخفي تبعيَّة هذه الأوراق ببعض كونها منشورة بشكل منفصل.  **المصطلحات ذات الصِّلة:** التَّلاعب بالنِّظام، ممارسات البحث المشكوك فيها، أو ممارسات إعداد التَّقارير المشكوك فيها ، نشر جزئي.",
                "related_terms": [
                    "Gaming (the system)",
                    "Questionable Research Practices or Questionable Reporting Practices (QRPs)",
                    "Partial publication"
                ],
                "references": "Fanelli, D. (2018). Opinion: Is science really facing a reproducibility crisis, and do we need it to? Proceedings of the National Academy of Sciences, 115(11), 2628–2631. https://doi.org/10.1073/pnas.1708272114",
                "drafted_by": [
                    "Mahmoud Elsherif"
                ],
                "reviewed_by": [
                    "Sarah Ashcroft-Jones",
                    "Adrien Fillon",
                    "Helena Hartmann",
                    "Matt Jaquiery",
                    "Tamara Kalandadze",
                    "Charlotte R. Pennington",
                    "Graham Reid",
                    "Suzanne L. K. Stewart"
                ],
                "alt_related_terms": [
                    null
                ],
                "language": "arabic"
            },
            {
                "type": "glossary",
                "title": "السَّبق (Scooping)",
                "definition": "الإبلاغ عن اكتشاف نتيجة جديدة، أو نشرها قبل باحث أو فريق آخر. وتشير الأبحاث المسحيَّة إلى أنَّ الخوف من السَّبق هو عائق مهم مرتبط بالخوف أمام مشاركة البيانات في علم النَّفس، وكما تشير البحوث التي تتبع منهجيَّة النمذجة القائمة على الوكلاء إلا أنَّ التَّنافس على الأولويَّة يضرُّ بالموثوقيَّة العلميَّة (Tiokhin et al., 2021).  **المصطلحات ذات الصِّلة:** الحداثة، البيانات المفتوحة، التَّسجيل المسبق.",
                "related_terms": [
                    "Novelty",
                    "Open data",
                    "Preregistration"
                ],
                "references": "Houtkoop, B. L., Chambers, C., Macleod, M., Bishop, D. V. M., Nichols, T. E., & Wagenmekers, E.-J. (2018). Data sharing in psychology: A survey on barriers and preconditions. Advances in Methods and Practices in Psychological Science, 1(1), 70.85. https://doi.org/10.1177/2515245917751886\n\nLaine, H. (2017). Afraid of scooping – Case study on researcher strategies against fear of scooping in the context of open science. In Data Science Journal (Vol. 16, pp. 1–14). https://doi.org/10.5334/dsj-2017-029\n\nTiokhin, L., Yan, M., & Horgan, T. J. H. (2021). Competition for priority harms the reliability of science, but reforms can help. Nature Human Behaviour. https://doi.org/10.1038/s41562-020-01040-1",
                "drafted_by": [
                    "William Ngiam"
                ],
                "reviewed_by": [
                    "Ashley Blake",
                    "Thomas Rhys Evans",
                    "Connor Keating",
                    "Graham Reid",
                    "Timo Roettger",
                    "Robert M. Ross",
                    "Flávio Azevedo"
                ],
                "alt_related_terms": [
                    null
                ],
                "language": "arabic"
            },
            {
                "type": "glossary",
                "title": "القياسات الدِّلاليَّة (Semantometrics)",
                "definition": "نوع من المقاييس لتقييم البحوث العلميَّة باستخدام نص النَّشر الكامل؛ لقياس التَّشابه الدِّلالي بين البحوث وتسليط الضَّوء على مساهمة البحث في التَّقدُّم العلمي. وهو امتداد لأدوات شبيهة مثل القياسات الببليومتريَّة وقياسات الويب، والمقاييس البديلة.  **المصطلحات ذات الصِّلة:** القياسات الببليومتريَّة، الإسهام",
                "related_terms": [
                    "Bibliometrics",
                    "Contribution(p)"
                ],
                "references": "Herrmannova, D., & Knoth, P. (n.d.). Semantometrics Towards Full text-based Research Evaluation. Retrieved from https://arxiv.org/pdf/1605.04180.pdf\n\nKnoth, P., & Herrmannova, D. (2014). Towards semantometrics: A new semantic similarity based measure for assessing a research publication’s contribution. D-Lib Magazine, 20(11), 8. https://doi.org/10.1045/november2014-knoth",
                "drafted_by": [
                    "Alaa AlDoh"
                ],
                "reviewed_by": [
                    "Jamie P. Cockcroft",
                    "Mahmoud Elsherif",
                    "Christopher Graham",
                    "Charlotte R. Pennington"
                ],
                "alt_related_terms": [
                    null
                ],
                "language": "arabic"
            },
            {
                "type": "glossary",
                "title": "البحوث الحسَّاسة (Sensitive research)",
                "definition": "البحوث التي تشكل تهديدًا لأولئك الذين شاركوا فيها، بما في ذلك الباحثين، والمشاركين والمجتمع بشكل واسع. يمكن أن يكون هذا التَّهديد خطرًا جسديًا (مثل الانتحار) أو استجابة عاطفيَّة سلبيَّة (مثل الاكتئاب) للمشاركين في عملية البحث. على سبيل المثال، في البحوث التي أجريت على ضحايا الانتحار، قد يتعرَّض الباحث لصدمة عاطفيَّة بسبب وصف السُّلوكيَّات الانتحاريَّة، كما أنَّ التَّواصل مع الضَّحايا قد يجعلهم أيضًا يعيدون تجربة الذِّكريات المؤلمة، ممَّا يؤدِّي إلى استجابات نفسيَّة سلبيَّة.  **المصطلحات ذات الصِّلة:** التّعمية.",
                "related_terms": [
                    "Anonymity"
                ],
                "references": "Lee, R. M. (1993). Doing research on sensitive topics. Sage.\n\nAlbayrak-Aydemir, N. (2020). The hidden costs of being a scholar from the global south. Retrieved from https://blogs.lse.ac.uk/highereducation/2020/02/20/the-hidden-costs-of-being-a-scholar-from-the-global-south/",
                "drafted_by": [
                    "Nihan Albayrak-Aydemir"
                ],
                "reviewed_by": [
                    "Valeria Agostini",
                    "Mahmoud Elsherif",
                    "Helena Hartmann",
                    "Graham Reid"
                ],
                "alt_related_terms": [
                    null
                ],
                "language": "arabic"
            },
            {
                "type": "glossary",
                "title": "SDC نهج التَّسلسل الذي يحدِّد المساهمة (Sequence-determines-credit approach (SDC))",
                "definition": "نظام تأليف يحدِّد ترتيب التَّأليف بناءً على مساهمة كلِّ مؤلِّف. ويتمّ سرد أسماء المؤلِّفين وفقًا لمساهمتهم بترتيب تنازلي  بدءًا بالمؤلِّف الأكثر مساهمةً وانتهاءً بالمؤلِّف الأقل مساهمة.  **المصطلحات ذات الصِّلة:** التَّأليف، مبدأ التَّأكيد على  المؤلِّفين الأول والأخير",
                "related_terms": [
                    "Authorship",
                    "First-last-author-emphasis norm (FLAE)"
                ],
                "references": "Schmidt, R. H. (1987). A worksheet for authorship of scientific articles. The Bulletin of the Ecological Society of America, 68, 8–10. http://www.jstor.org/stable/20166549\n\nTscharntke, T., Hochberg, M. E., Rand, T. A., Resh, V. H., & Krauss, J. (2007). Author sequence and credit for contributions in multiauthored publications. PLoS Biology, 5(1), e18. https://doi.org/10.1371/journal.pbio.0050018",
                "drafted_by": [
                    "Myriam A. Baum"
                ],
                "reviewed_by": [
                    "Sam Parsons",
                    "Charlotte R. Pennington"
                ],
                "alt_related_terms": [
                    null
                ],
                "language": "arabic"
            },
            {
                "type": "glossary",
                "title": "شيربا روميو (Sherpa Romeo)",
                "definition": "مصدر عبر الإنترنت يجمع، ويعرض سياسات الوصول المفتوح للنَّاشرين من جميع أنحاء العالم، ويقدِّم ملخَّصات لحقوق الطَّبع، والنَّشر، والأرشفة الخاصّة بكلِّ المجلَّة.  **المصطلحات ذات الصِّلة**: فترة الحظر، الوصول المفتوح، حاجز مالي، الطّباعة الأوليَّة، مستودع  البيانات.",
                "related_terms": [
                    "Embargo period",
                    "Open access",
                    "Paywall",
                    "Preprint",
                    "Repository"
                ],
                "references": "Anon. (n.d.). Welcome to Sherpa Romeo - v2.sherpa. Retrieved from https://v2.sherpa.ac.uk/romeo/",
                "drafted_by": [
                    "Aleksandra Lazić"
                ],
                "reviewed_by": [
                    "Mahmoud Elsherif",
                    "Christopher Graham",
                    "Sam Parsons",
                    "Martin Vasilev"
                ],
                "alt_related_terms": [
                    null
                ],
                "language": "arabic"
            },
            {
                "type": "glossary",
                "title": "التحكيم أحاديَّ التَّعمية (Single-blind peer review)",
                "definition": "تقييم الأعمال البحثيَّة من قبل خبراء مؤهلين حيث يعرف المحكّم (أو المحكّمون) هويَّة المؤلِّف (أو المؤلفين)، لكن تبقى هوية المحكّم مجهولة بالنَّسبة للمؤلِّف.  **المصطلحات ذات الصِّلة:** تحكيم مجهول ، التّحكيم مزدوج التَّعمية، التحكيم المخفي ، التّحكيم المفتوح، تحكيم الأقران ، التّحكيم ثَّلاثيَّ التَّعمية.",
                "related_terms": [
                    "Anonymous review",
                    "Double-blind peer review",
                    "Masked review",
                    "Open Peer Review",
                    "Peer review",
                    "Triple-blind peer review"
                ],
                "references": "Largent, E. A., & Snodgrass, R. T. (2016). Blind peer review by academic journals. In C. T. Robertson & A. S. Kesselheim (Eds.), Blinding as a solution to bias: Strengthening biomedical science, forensic science, and law (pp. 75–95). Academic Press. https://doi.org/10.1016/B978-0-12-802460-7.00005-X",
                "drafted_by": [
                    "Bradley Baker"
                ],
                "reviewed_by": [
                    "Ashley Blake",
                    "Christopher Graham",
                    "Helena Hartmann",
                    "Graham Reid"
                ],
                "alt_related_terms": [
                    null
                ],
                "language": "arabic"
            },
            {
                "type": "glossary",
                "title": "العلم البطيء (Slow science)",
                "definition": "يؤدِّي تبني ممارسات المعرفة المفتوحة إلى عمليَّة بحث أطول مدةً في المجمل، مع تركيز أكبر  على الشَّفافيَّة، وقابليَّة التِّكرار، وإعادة الإنتاج، والجودة، بدلًا من كميَّة المخرجات، وتعارض عمليَّة العلم البطيء ثقافة \"النَّشر أو الفناء \" وتصف نظامًا أكاديميًا يوفِّر الوقت، والموارد لإنتاج عدد أقل من المخرجات، ولكنَّها عالية الجودة والشَّفافية. مثلًا تعطى الأولويَّة لوقت الباحث لجمع المزيد من البيانات، والمزيد من الوقت لقراءة الأدبيَّات، والتَّفكير في كيفيَّة تناسب النَّتائج التي توصَّلوا إليها مع الأدبيَّات، وتوثيق ومشاركة المواد البحثيَّة، بدلًا من إجراء دراسات إضافيَّة.  **المصطلحات ذات الصِّلة:** التَّعاون، هيكل الحوافز، النَّشر أو الفناء، ثقافة البحث، جودة البحث.",
                "related_terms": [
                    "collaboration",
                    "Incentive structure",
                    "Publish or Perish",
                    "research culture",
                    "research quality"
                ],
                "references": "Academy, S. S. (2010). The Slow Science Manifesto. Slow Science. http://slow-science.org/\n\nNelson, L. D., Simmons, J. P., & Simonsohn, U. (2012). Let’s Publish Fewer Papers. Psychological Inquiry, 23(3), 291–293. https://doi.org/10.1080/1047840X.2012.705245\n\nFrith, U. (2020). Fast lane to slow science. Trends in Cognitive Sciences, 24(1), 1–2. https://doi.org/10.1016/j.tics.2019.10.007",
                "drafted_by": [
                    "Sonia Rishi"
                ],
                "reviewed_by": [
                    "Adrien Fillon",
                    "Tamara Kalandadze",
                    "Sam Parsons Charlotte R. Pennington",
                    "Robert M Ross",
                    "Timo Roettger"
                ],
                "alt_related_terms": [
                    null
                ],
                "language": "arabic"
            },
            {
                "type": "glossary",
                "title": "SORTEE جمعيَّة علم البيئة والبيولوجيا التَّطوريَّة المنفتحة والموثوقة والشَّفافة (Society for Open, Reliable, and Transparent Ecology and Evolutionary biology (SORTEE))",
                "definition": "مجتمع دولي يهدف إلى تحسين شفافيَّة وموثوقيَّة نتائج البحوث في مجالات البيئة والتَّطور والتَّخصُّصات ذات الصِّلة من خلال التَّغييرات الثَّقافية والمؤسّسيَّة ([https://www.sortee.org/](https://www.sortee.org/)). تم إطلاق الجمعية في ديسمبر 2020 وهي متاحة لأي شخص مهتم بتحسين البحث في هذه التَّخصُّصات، بغض النَّظر عن الخبرة، فالجمعيَّة دوليَّة في نطاقها وعضويتها وأهدافها. تضم الجمعيَّة أكثر من 600 عضوا اعتبارًاًمن مايو 2021\\.  **المصطلحات ذات الصِّلة:** جمعيَّة تطوير العلوم النَّفسيَّة",
                "related_terms": [
                    "Society for the Improvement of Psychological Science (SIPS)"
                ],
                "references": "",
                "drafted_by": [
                    "Brice Beffara Bret; Dominique Roche"
                ],
                "reviewed_by": [
                    "Sarah Ashcroft-Jones",
                    "Mahmoud Elsherif",
                    "Charlotte R. Pennington",
                    "Graham Reid"
                ],
                "alt_related_terms": [
                    null
                ],
                "language": "arabic"
            },
            {
                "type": "glossary",
                "title": "SIPS جمعية تطويرالعلوم النَّفسيَّة (Society for the Improvement of Psychological Science (SIPS))",
                "definition": "جمعيَّة تأسَّست لتعزيز الأساليب، والممارسات التي تحسِّن البحوث النَّفسيَّة. تهدف الجمعيَّة إلى تحقيق رسالتها من خلال تدريب الباحثين في علم النَّفس، وتعزيز الثَّقافة البحثيَّة التي تساعد على تحسين جودة البحوث، وتقييم وقياس تأثير هذه الإصلاحات تجريبيًا، وتنظيم أنشطة توعية داخل وخارج علم النَّفس؛ لتحسين معايير البحث العلمي الحاليَّة.  **المصطلحات ذات الصِّلة:** جمعيَّة علم البيئة والبيولوجيا التَّطوريَّة المنفتحة والموثوقة والشَّفافة",
                "related_terms": [
                    "Society for Open, Reliable, and Transparent Ecology and Evolutionary biology (SORTEE)"
                ],
                "references": "Improving Psychology. (n.d.). Improving Psychology. https://improvingpsych.org/",
                "drafted_by": [
                    "Mahmoud Elsherif"
                ],
                "reviewed_by": [
                    "Ashley Blake",
                    "Jade Pickering",
                    "Graham Reid",
                    "Flávio Azevedo"
                ],
                "alt_related_terms": [
                    null
                ],
                "language": "arabic"
            },
            {
                "type": "glossary",
                "title": "الطَّبقة الاجتماعيّة (Social class)",
                "definition": "عادةً ما يتم قياس الطبقة الاجتماعيَّة باستخدام القياسات الموضوعيَّة والذَّاتيَّة، على النَّحو الذي توصي به جمعيَّة علم النَّفس الأمريكيَّة (American Psychological Association,Task Force on Socioeconomic Status, 2007). وعلى عكس المفهوم التَّقليدي، الذي يأخذ بعين الاعتبار عاملًا واحدًا فقط، إما التَّعليم أو الدَّخل (مثلا المتغيرات الاقتصادية)، تعتبر الطبقة الاجتماعية للفرد مزيجًا من التعليم والدخل والمكانة المِهنية والحالة الاجتماعية الذاتية والوضع الاجتماعي المحدد ذاتيًا. وتعد الطَّبقة الاجتماعيَّة، جزئيًا، متغيرًا ثقافيًا؛ حيث إنَّها متغير مستقر قد يتغيَّر ببطء على مرِّ السِّنين. ويمكن أن يكون للطَّبقة الاجتماعيَّة آثارًا مهمَّة على النَّتائج الأكاديميَّة، فقد يتمتَّع الفرد بمكانة اجتماعيَّة واقتصاديَّة عالية مع تعريفه كفرد من الطَّبقة العاملة. ويميل طلاب الطَّبقة العاملة إلى أن تكون لديهم ظروف حياتيَّة مختلفة، وغالبًا ما تكون التزاماتهم أكثر تقييدًا من طلاب الطَّبقة المتوسِّطة، مما يجعل اندماجهم مع الطُّلاب الآخرين أكثر صعوبة (Rubin, 2021). كما إن ضيق الوقت والمال قد يعيق تجربتهم الاجتماعيَّة في الجامعة. وعادةً يعمل طلاب الطَّبقة العاملة؛ لأجل دعم أنفسهم، ممَّا يؤدي إلى تقليل وقت الأنشطة الأكاديميَّة، والتَّواصل الاجتماعي مع الطُّلاب الآخرين وكذلك امتلاك مال أقل لشراء العناصر المرتبطة بالتَّجارب الاجتماعيَّة (مثل الطَّعام).  **المصطلحات ذات الصِّلة:** الاندماج الاجتماعي",
                "related_terms": [
                    "Social integration"
                ],
                "references": "Evans, O., & Rubin, M. (2021). In a Class on Their Own: Investigating the Role of Social Integration in the Association Between Social Class and Mental Well-Being. Personality and Social Psychology Bulletin, 014616722110211. https://doi.org/10.1177/01461672211021190\n\nRubin, M., Evans, O., & McGuffog, R. (2019). Social class differences in social integration at university: Implications for academic outcomes and mental health. In J. Jetten & K. Peters (Eds.), The social psychology of inequality (pp. 87–102). Springer. https://doi.org/10.1007/978-3-030-28856-3_6\n\nRubin, M. (2021). Explaining the association between subjective social status and mental health among university students using an impact ratings approach. SN Social Sciences, 1(1), 1–21. https://doi.org/10.1007/s43545-020-00031-3",
                "drafted_by": [
                    "Mahmoud Elsherif"
                ],
                "reviewed_by": [
                    "Leticia Micheli",
                    "Eliza Woodward",
                    "Julika Wolska",
                    "Gerald Vineyard**;** Yu-Fang Yang"
                ],
                "alt_related_terms": [
                    null
                ],
                "language": "arabic"
            },
            {
                "type": "glossary",
                "title": "الاندماج الاجتماعيّ (Social integration)",
                "definition": "الاندماج الاجتماعي هو مفهوم متعدِّد الأبعاد. في السِّياق الأكاديمي، يرتبط الاندماج الاجتماعي بكميَّة ونوعيَّة التَّفاعلات الاجتماعيَّة مع الموظفين والطُّلاب، إضافة إلى الشُّعور بالارتباط والانتماء إلى الجامعة والأفراد فيها. ولكي نكون أكثر تحديدًا ، فإنَّ الدَّعم الاجتماعي والثِّقة والتَّرابط كلها متغيرات تساهم في الاندماج الاجتماعي. للاندماج الاجتماعي آثار مهمة على النَّتائج الأكاديميَّة والصِّحة العقليَّة (Evans & Rubin, 2021)؛ فيقل احتمال اندماج طلاب الطَّبقة العاملة مع الطُّلاب الآخرين؛ نظرًا لأنَّ لديهم خلفيَّات اجتماعيَّة واقتصاديَّة مختلفة ودخلًا أقل، وبالتَّالي فإنَّهم غير قادرين على تجربة عدد مماثل من الفرص التَّعليميَّة والماليَّة. وهذا بدوره يمكن أن يؤدي إلى تدهور الصِّحة العقليَّة والشُّعور بالنبذ ​​(Rubin, 2021).  **المصطلحات ذات الصِّلة:** الطَّبقة الاجتماعيَّة",
                "related_terms": [
                    "Social class"
                ],
                "references": "Evans, O., & Rubin, M. (2021). In a Class on Their Own: Investigating the Role of Social Integration in the Association Between Social Class and Mental Well-Being. Personality and Social Psychology Bulletin, 014616722110211. https://doi.org/10.1177/01461672211021190\n\nRubin, M., Evans, O., & McGuffog, R. (2019). Social class differences in social integration at university: Implications for academic outcomes and mental health. In J. Jetten & K. Peters (Eds.), The social psychology of inequality (pp. 87–102). Springer. https://doi.org/10.1007/978-3-030-28856-3_6\n\nRubin, M. (2021). Explaining the association between subjective social status and mental health among university students using an impact ratings approach. SN Social Sciences, 1(1), 1–21. https://doi.org/10.1007/s43545-020-00031-3",
                "drafted_by": [
                    "Mahmoud Elsherif"
                ],
                "reviewed_by": [
                    "Leticia Micheli",
                    "Eliza Woodward",
                    "Julika Wolska**;** Gerald Vineyard",
                    "Yu-Fang Yang",
                    "Flávio Azevedo"
                ],
                "alt_related_terms": [
                    null
                ],
                "language": "arabic"
            },
            {
                "type": "glossary",
                "title": "تحليل منحنى المواصفات (Specification Curve Analysis)",
                "definition": "هو نهج تحليلي يتكوَّن من تحديد النَّتائج وحسابها وتصوّرها وتفسيرها (من خلال الإحصائيَّات الاستدلاليَّة) *لجميع* المواصفات المعقولة لسؤال بحث معين (see Simonsohn et al., 2015). ويساعد تحليل منحنى المواصفات على جعل تأثير القرارات العشوائيَّة المفترضة شفافة أثناء التَّقدُّم العلمي (كالتَّصميم التَّجريبي، وتوظيف المفاهيم، والنَّماذج الإحصائيَّة ،أو خليط من هذا كله) والتي يقوم بها الباحث من خلال الإبلاغ الشَّامل عن جميع الاختبارات المعقولة، وغير الزَّائدة عن الحاجة لسؤال البحث.  يشير بعض الباحثين (Voracek et al., 2019\\) إلى أن تحليل منحنى المواصفات يختلف عن تحليل الأكوان المتعدِّدة فيما يتعلَّق بالعروض الرُّسوميَّة (مخطَّط منحنى المواصفات بدلًا من الرَّسم البياني، ومخطَّط توزيع البيانات) واستخدام الإحصائيَّات الاستدلاليَّة لتفسير النَّتائج.  **المصطلحات ذات الصِّلة:** تحليل الأكوان المتعدِّدة، توليف البحث، المتانة (في التَّحليلات)، التَّقارير الانتقائيَّة، اهتزاز التَّأثيرات.",
                "related_terms": [
                    "Multiverse analysis",
                    "Research synthesis",
                    "Robustness (analyses)",
                    "Selective reporting",
                    "Vibration of effects"
                ],
                "references": "Simonsohn, U., Simmons, J. P., & Nelson, L. D. (2015). Specification curve: Descriptive and inferential statistics on all reasonable specifications. http://sticerd.lse.ac.uk/seminarpapers/psyc16022016.pdf\n\nSimonsohn, U., Simmons, J. P., & Nelson, L. D. (2020). Specification curve analysis. Nature Human Behaviour, 4(11), 1208–1214. https://doi.org/10.1038/s41562-020-0912-z\n\nVoracek, M., Kossmeier, M., & Tran, U. S. (2019). Which Data to Meta-Analyze, and How? Zeitschrift Für Psychologie. https://doi.org/10.1027/2151-2604/a000357",
                "drafted_by": [
                    "Bradley Baker"
                ],
                "reviewed_by": [
                    "Tina B. Lonsdorf",
                    "Sam Parsons",
                    "Charlotte R. Pennington"
                ],
                "alt_related_terms": [
                    null
                ],
                "language": "arabic"
            },
            {
                "type": "glossary",
                "title": "الافتراضات الإحصائيَّة (Statistical Assumptions)",
                "definition": "تفترض الأساليب والنَّماذج التَّحليليَّة خصائص معيَّنة للبيانات (كالاستقلال الإحصائي، وعشوائيَّة العيّنات، والانحناء الطبيعي، والتَّباين المتساوي وغيرها). قبل إجراء تحليل، فإنَّه يجب التَّحقُّق من هذه الافتراضات؛ لأنَّ انتهاكها يمكن أن يغيِّر نتائج الدِّراسة واستنتاجها. وتتمثَّل الممارسة الجيِّدة في العلوم المفتوحة والقابلة للتِّكرار في الإبلاغ عن اختبار الافتراضات، بذكر الافتراضات التي تم فحصها ونتائج هذه الفحوصات، أو التَّصحيحات المطبقة.  **المصطلحات ذات الصِّلة:** اختبار دلالة الفرضيَّة الصِّفرية ، الدِّلالة الإحصائيَّة، الصِّدق الإحصائي، الشَّفافيَّة ، الخطأ من النُّوع الأوّل، الخطأ من النَّوع الثَّاني ، الخطأ من نوع إم، الخطأ  من نوع إس.",
                "related_terms": [
                    "Null Hypothesis Significance Testing (NHST)",
                    "Statistical Significance",
                    "Statistical Validity",
                    "Transparency",
                    "Type I error",
                    "Type II error",
                    "Type M error",
                    "Type S error"
                ],
                "references": "Garson, G. D. (2012). Testing Statistical Assumptions (2012th ed.). North Carolina State University.\n\nHahn, G. J., & Meeker, W. Q. (1993). Assumptions for Statistical Inference. The American Statistician, 47(1), 1–11. https://doi.org/10.1080/00031305.1993.10475924\n\nHoekstra, R., Kiers, H., & Johnson, A. (2012). Are assumptions of well-known statistical techniques checked, and why (not)? Frontiers in Psychology, 3(137), 1–9. https://doi.org/10.3389/fpsyg.2012.00137",
                "drafted_by": [
                    "Graham Reid"
                ],
                "reviewed_by": [
                    "Jamie P. Cockcroft",
                    "Sam Parsons",
                    "Martin Vasilev",
                    "Julia Wolska"
                ],
                "alt_related_terms": [
                    null
                ],
                "language": "arabic"
            },
            {
                "type": "glossary",
                "title": "القوّة الإحصائيّة (Statistical power)",
                "definition": "القوة الإحصائيَّة هي الاحتماليَّة على المدى الطَّويل بأنَّ الاختبار الإحصائي يرفض بشكل صحيح الفرضيَّة الصِّفريَّة إذا كانت الفرضيَّة البديلة صحيحة.  يتراوح من 0 إلى 1، ولكن غالبًا ما يتم التَّعبير عنه كنسبة مئويَّة. يمكن تقدير القدرة باستخدام معيار الأهميَّة (ألفا)، وحجم التَّأثير، وحجم العيِّنة المستخدمة في تقنيَّة تحليل محدَّدة. هناك نوعان من التَّطبيقات الرَّئيسيَّة للقوّة الإحصائيَّة. قوة مسبقة حيث يطرح الباحث السُّؤال \"بالنَّظر إلى حجم التَّأثير، كم عدد المشاركين الذين سأحتاجهم للحصول على قوة معينة؟\". أما قوّة الحساسيَّة فتطرح السُّؤال التَّالي: \"نظرًا لحجم عيّنة معروف، ما هو حجم التَّأثير الذي يمكنني اكتشافه بقوّة معيّنة؟\".  **المصطلحات ذات الصِّلة:** حجم التَّأثير ، التَّحليل البعدي، اختبار دلالة الفرضيَّة الصِّفريَّة ، تحليل القوّة، القيمة التَّنبؤيَّة الإيجابيَّة، البحث الكمي، حجم العيِّنة، معيار الأهميَّة (ألفا) ، الخطأ من النَّوع الأوّل ،الخطأ من النُّوع الثَّاني.  **المصطلحات ذات الصِّلة بالتَّعريف البديل:** الخطأ من النُّوع الثَّاني.",
                "related_terms": [
                    "Effect Size",
                    "Meta-analysis",
                    "Null Hypothesis Significance Testing (NHST)",
                    "Power Analysis",
                    "Positive Predictive Value",
                    "Quantitative research",
                    "Sample size",
                    "Significance criterion (alpha)",
                    "Type I error",
                    "Type II error **Related terms to alternative definition:** Type II Error"
                ],
                "references": "Carter, A., Tilling, K., & Munafo, M. R. (2021). Considerations of sample size and power calculations given a range of analytical scenarios. https://doi.org/10.31234/osf.io/tcqrn\n\nCohen, J. (1962). The statistical power of abnormal-social psychological research: A review. The Journal of Abnormal and Social Psychology, 65(3), 145–153. https://doi.org/10.1037/h0045186\n\nCohen, J. (1969). Statistical power analysis for the behavioral sciences. Academic Press.\n\nDienes, Z. (2008). Understanding psychology as a science: An introduction to scientific and statistical inference. Macmillan International Higher Education.\n\nGiner-Sorolla, R., Aberson, C. L., Bostyn, D. H., Carpenter, T., Conrique, B. G., Lewis, N. A., & Soderberg, C. (2019). Power to detect what? Considerations for planning and evaluating sample size. Retrieved from https://osf.io/jnmya/\n\nIoannidis, J. P. (2005). Why most published research findings are false. PLoS Medicine, 2(8), e124. https://doi.org/10.1371/journal.pmed.0020124\n\nLakens, D. (2021). Sample Size Justification. https://doi.org/10.31234/osf.io/9d3yf",
                "drafted_by": [
                    "Thomas Rhys Evans"
                ],
                "reviewed_by": [
                    "James E. Bartlett",
                    "Jamie P. Cockcroft",
                    "Adrien Fillon",
                    "Emma Henderson",
                    "Tamara Kalandadze",
                    "William Ngiam",
                    "Catia M. Oliveira",
                    "Charlotte R. Pennington",
                    "Graham Reid",
                    "Martin Vasilev",
                    "Qinyu Xiao",
                    "Flávio Azevedo"
                ],
                "alt_related_terms": [
                    null
                ],
                "language": "arabic"
            },
            {
                "type": "glossary",
                "title": "الدَّلالة الإحصائيَّة (Statistical significance)",
                "definition": "هي وصف لنتيجة ما باستخدام اختبار دلالة الفرضيَّة الصِّفريَّة التي لا يرجَّح الحصول عليها باعتبار مستوى معين من الدَّلالة الإحصائيَّة في النَّظرية الصِّفريَّة. وقد عرفها بعض الباحثين بأنَّها \"مقياس لاحتماليَّة الحصول على بياناتك، أو بيانات أكثر تطرفًا على افتراض أنَّ الفرضيَّة الصِّفرية صحيحة، مقارنة بمستوى مقبول محدَّد مسبقًا من عدم اليقين فيما يتعلَّق بالإجابة الحقيقية\" (Tenny & Abdelgawad, 2017, p. 1). وتختلف الأعراف في تحديد بدايتها بين التَّطبيقات، والتَّخصُّصات، ولكنَّها تعتمد في نهاية الأمر على اعتبارات الباحث حول هامش الخطأ المناسب.  ويشير بيان الجمعيَّة الإحصائيَّة الأمريكيَّة (Wasserstein & Lazar, 2016\\) إلى أنَّ \"الباحثون غالبًا ما يرغبون في تحويل القيمة الاحتماليَّة إلى دليل صحة الفرضيَّة الصِّفريَّة، أو حول احتمال أن تكون الصُّدفة العشوائيَّة قد سبَّبت البيانات المرصودة. لكن القيمة الاحتماليَّة ليست كذلك، حيث إنَّها إشارة حول البيانات المتعلِّقة بتفسير افتراضيَّة معيّنة، وليست إشارة عن التَّفسير نفسه\" (ص. 131). **المصطلحات ذات الصِّلة:** خطأ ألفا، الإحصاءات المتكرِّرة؛ فرضيَّة الصِّفر، اختبار دلالة الفرضيَّة الصِّفريَّة؛ القيمة الاحتماليَّة؛ الخطأ من النُّوع الأوَّل.",
                "related_terms": [
                    "Alpha error",
                    "Frequentist statistics",
                    "Null hypothesis",
                    "Null Hypothesis Significance Testing (NHST)",
                    "*P*\\-value",
                    "Type I error **Incorrect definition:** Statistical significance describes the likelihood of the observed result against chance (regardless of the null hypotheses)"
                ],
                "references": "Cassidy, S. A., Dimova, R., Giguère, B., Spence, J. R., & Stanley, D. J. (2019). Failing grade: 89% of introduction-to-psychology textbooks that define or explain statistical significance do so incorrectly. Advances in Methods and Practices in Psychological Science, 2(3), 233–239. https://doi.org/10.1177/2515245919858072\n\nWasserstein, R. L., & Lazar, N. A. (2016). The ASA Statement on p-Values: Context, Process, and Purpose. The American Statistician, 70, 129–133. https://doi.org/10.1080/00031305.2016.1154108",
                "drafted_by": [
                    "Alaa AlDoh; Flávio Azevedo"
                ],
                "reviewed_by": [
                    "James E. Bartlett",
                    "Alexander Hart**;** Annalise A. LaPlume",
                    "Charlotte R. Pennington",
                    "Graham Reid",
                    "Timo Roettger",
                    "Suzanne L. K. Stewart"
                ],
                "alt_related_terms": [
                    null
                ],
                "language": "arabic"
            },
            {
                "type": "glossary",
                "title": "الصِّدق الإحصائي (Statistical validity)",
                "definition": "مدى دقّة استنتاجات الاختبار الإحصائي وعكاسها للتَّأثير الحقيقي الموجود في الواقع. بمعنى آخر، هو معرفة ما إذا كان هناك علاقة بين متغيرين أم لا، ويمكن اكتشافها بدقَّة من خلال التَّحليلات التي تم إجراؤها. وتشمل مهدِّدات الصِّدق الإحصائي الآتي: القوة المنخفضة، انتهاك الفرضيَّات، ثبات المقاييس، وغير ذلك ممَّا يؤثِّر على موثوقيَّة النَّتائج وعموميّتها.  **المصطلحات ذات الصِّلة:** القوّة (الإحصائيَّة)، الصِّدق، الافتراضات الإحصائيَّة",
                "related_terms": [
                    "Power",
                    "Validity",
                    "Statistical assumptions"
                ],
                "references": "Cook, T. D., & Campbell, D. T. (1979). Quasi-Experimentation. Rand McNally.\n\nDrost, E. A. (2011). Validity and reliability in social science research. Education Research and Perspectives, 38(1), 105–123.",
                "drafted_by": [
                    "Annalise A. LaPlume"
                ],
                "reviewed_by": [
                    "Jamie P. Cockcroft, Zoltan Kekecs",
                    "Graham Reid"
                ],
                "alt_related_terms": [
                    null
                ],
                "language": "arabic"
            },
            {
                "type": "glossary",
                "title": "سترينج: الخلفيَّة الاجتماعيَّة والقابليَّة للتَّتبع والاختيار الذَّاتي وتاريخ التَّربية والتَّأقلم والتَّعود (STRANGE)",
                "definition": "إطار يقدم مقترحًا وسلسلة من الأسئلة لمساعدة الباحثين في مجال سلوك الحيوان على النَّظر في تحيُّزات أخذ العيِّنات عند التَّخطيط، وإجراء وتفسير البحوث على الحيوانات، وهو اختصار يسلِّط الضَّوء على العديد من المصادر المحتملة لتحيُّز أخذ العيِّنات في بحوث الحيوانات، مثل: الخلفيَّة الاجتماعيَّة للحيوانات، والقابليَّة للتَّتبع والاختيار الذَّاتي، وتاريخ التَّربية، و التَّأقلم والتَّعوُّد، والتَّغيُّرات الطَّبيعيَّة في الاستجابة، والتَّكوين الجيني والخبرة.  **المصطلحات ذات الصِّلة:** التَّحيُّز، قيود التَّعميم، العيِّنة، تحيّز أخذ العيِّنات، ويرد",
                "related_terms": [
                    "Bias",
                    "Constraints on Generality (COG)",
                    "Populations",
                    "Sampling bias",
                    "WEIRD"
                ],
                "references": "Webster, M. M., & Rutz, C. (2020). How STRANGE are your study animals? Nature, 582, 337–340. https://doi.org/10.1038/d41586-020-01751-5",
                "drafted_by": [
                    "Mahmoud Elsherif"
                ],
                "reviewed_by": [
                    "Ben Farrar",
                    "Zoe Flack",
                    "Elias Garcia-Pelegrin",
                    "Charlotte R. Pennington",
                    "Graham Reid"
                ],
                "alt_related_terms": [
                    null
                ],
                "language": "arabic"
            },
            {
                "type": "glossary",
                "title": "مبادلة الدِّراسات (StudySwap)",
                "definition": "منصَّة مجانيَّة على الإنترنت ينشر من خلالها الباحثون وصفًا موجزًا للمشاريع البحثيَّة، أو الموارد المتاحة للاستخدام (الممتلكات) أو التي يحتاجون  إليها وقد يكون لدى باحث آخر (الاحتياجات). مبادلة الدِّراسات هو نهج للتَّعهيد الجماعي للبحث والذي يمكن أن يضمن بقاء موارد بحثيَّة غير مستخدمة وحصول المزيد من الباحثين على الموارد التي يحتاجون إليها.  **المصطلحات ذات الصِّلة:** التَّعاون ،التَّعهيد الجماعي، علم الفريق",
                "related_terms": [
                    "Collaboration",
                    "Crowdsourcing",
                    "Team science"
                ],
                "references": "Chartier, C. R., Riegelman, A., & McCarthy, R. J. (2018). StudySwap: A platform for interlab replication, collaboration, and resource exchange. Advances in Methods and Practices in Psychological Science, 1(4), 574–579. https://doi.org/10.1177/2515245918808767",
                "drafted_by": [
                    "Charlotte R. Pennington"
                ],
                "reviewed_by": [
                    "Mahmoud Elsherif",
                    "Helena Hartmann",
                    "Emma Henderson",
                    "Graham Reid"
                ],
                "alt_related_terms": [
                    null
                ],
                "language": "arabic"
            },
            {
                "type": "glossary",
                "title": "المراجعة المنهجيّة (Systematic Review)",
                "definition": "تعدُّ المراجعة المنهجيَّة شكلًا من أشكال مراجعة الأدبيَّات وتجميع الأدلَّة. عادة ما تتضمَّن المراجعة المنهجيَّة استراتيجيَّة بحث شاملة وقابلة للتِّكرار، وتشمل: المصطلحات الرَّئيسيَّة، وقواعد البيانات المستخدمة للوصول إلى الأدبيَّات ذات العلاقة بموضوع ، أو سؤال بحثي معين. يقوم المراجعون المنهجيُّون بعمليَّة فحص الدِّراسات التي تم العثور عليها من خلال بحثهم، وتصفيتها حتى الوصول إلى مجموعة من الدِّراسات التي تتطابق مع معايير التَّضمين المحدَّدة مسبقًا. يمكن بعد ذلك جمع هذه الدِّراسات في مراجعة مكتوبة والتي قد تحتوي بدورها- ولكن ليس بالضَّرورة- على تجميع احصائي باستخدام التَّحليل البعدي. يجب أن تتبع المراجعة المنهجيَّة مجموعة محدَّدة من الإرشادات؛ لضمان إبقاء التَّحيُّز عند الحدِّ الأدنى مثل إرشادات بريزما (Moher et al., 2009; Page et al., 2021) ومراجعات كوكرين المنهجيَّة (Higgins et al. 2019\\) أو قائمة نيرو للمراجعات المنهجيَّة (Topor et al., 2021).  **المصطلحات ذات الصِّلة:** التَّحليل البعدي ، كونسورت CONSORT، المراجعات المنهجيَّة غير التَّدخليَّة  المفتوحة والقابلة للتِّكرار",
                "related_terms": [
                    "Meta-analysis",
                    "CONSORT",
                    "Non-Intervention, Reproducible, and Open Systematic Reviews (NIRO-SR)",
                    "PRISMA"
                ],
                "references": "Higgins, J. P. T., Thomas, J., Chandler, J., Cumpston, M., Li, T., Page, M. J., & Welch, V. A. (Eds.). (2019). Cochrane Handbook for Systematic Reviews of Interventions. 2nd Edition. John Wiley & Sons.\n\nMoher, D., Liberati, A., Tetzlaff, J., & Altman, D. (2009). Preferred Reporting Items for Systematic Reviews and Meta-Analyses: The PRISMA Statement. PLoS Medicine, 6(7), e1000097. https://doi.org/10.1371/journal.pmed.1000097\n\nPage, M. J., Moher, D., Bossuyt, P. M., Boutron, I., Hoffmann, T. C., Mulrow, C. D., & McKenzie, J. E. (2021). PRISMA 2020 explanation and elaboration: updated guidance and exemplars for reporting systematic reviews. British Medical Journal, 372. https://doi.org/10.1136/bmj.n160\n\nTopor, M., Pickering, J. S., Barbosa Mendes, A., Bishop, D. V. M., Büttner, F. C., Elsherif, M. M., & others. (2021). An integrative framework for planning and conducting Non-Intervention, Reproducible, and Open Systematic Reviews (NIRO-SR). https://doi.org/10.31222/osf.io/8gu5z",
                "drafted_by": [
                    "Jade Pickering"
                ],
                "reviewed_by": [
                    "Mahmoud Elsherif",
                    "Adam Parker",
                    "Charlotte R. Pennington",
                    "Timo Roettger",
                    "Marta Topor",
                    "Emily A. Williams",
                    "Flávio Azevedo    ###"
                ],
                "alt_related_terms": [
                    null
                ],
                "language": "arabic"
            },
            {
                "type": "glossary",
                "title": "تينزينج (Tenzing)",
                "definition": "حزمة تطبيقات ويب وحزمة آر على الإنترنت تساعد الباحثين على تتبُّع مساهمات كلِّ عضوٍ في الفريق، والإبلاغ عنها باستخدام تصنيف أدوار المؤلِّفين بطريقة فعَّالة.  ويمكن لأعضاء فريق المشروع البحثي الإشارة إلى مساهماتهم في كلِّ دورٍ من تصنيف أدوار المؤلِّفين باستخدام قالب جدول بيانات عبر الإنترنت، وتقديم أي معلوماتٍ إضافيَّة للمؤلِّفين (كالاسم، والانتماء، وترتيب النَّشر، وعنوان البريد الإلكتروني، ورقم الأوركيد). وعند كتابة المخطوطة، يمكن أنْ ينشئ تينزينج تلقائيًا قائمة بالمساهمين المنتمين إلى كلِّ دورٍ بحسب تصنيف أدوار المؤلِّفين؛ ليتم تضمينهم في قسم المساهمات وإنشاء صفحة عنوان المخطوطة.  **المصطلحات ذات الصِّلة:** التَّأليف، التَّحالف التَّأليفي، الإسهام، تصنيف أدوار المؤلِّفين.",
                "related_terms": [
                    "Authorship",
                    "Consortium authorship",
                    "Contributions",
                    "CRediT"
                ],
                "references": "Holcombe, A. O., Kovacs, M., Aust, F., & Aczel, B. (2020). Documenting contributions to scholarly articles using CRediT and tenzing. Plos One, 15(12), e0244611.",
                "drafted_by": [
                    "Marton Kovacs"
                ],
                "reviewed_by": [
                    "Balazs Aczel",
                    "Mahmoud Elsherif",
                    "Helena Hartmann",
                    "Charlotte R. Pennington",
                    "Graham Reid",
                    "Flávio Azevedo"
                ],
                "alt_related_terms": [
                    null
                ],
                "language": "arabic"
            },
            {
                "type": "glossary",
                "title": "النَّظريَّة (Theory)",
                "definition": "هي تفسير، أو وصف موحّد لعملية أو ظاهرة ما، تكون قابلة للاختبارات المتكرِّرة، ويمكن التَّحقُّق منها من خلال البحث العلمي باستعمال تجارب مختلفة بقيادة العديد من الباحثين المستقلين.  من الممكن رفض النَّظريات أو اعتبارها تفسيرًا غير مرضٍ لظاهرة ما بعد الاختبار الصَّارم لفرضيَّة جديدة.  تفسِّر الظَّاهرة بشكل أفضل أو تبدو متعارضة معها، ولكنَّها مع ذلك أكثر قابليَّة للتَّعميم على مجموعة أوسع من النَّتائج. **المصطلحات ذات الصِّلة:** الفرضيَّة، النّموذج (الفلسفي)، بناء النَّظريَّة.",
                "related_terms": [
                    "Hypothesis",
                    "Model (philosophy)",
                    "Theory building"
                ],
                "references": "Schafersman, S. D. (1997). An Introduction to Science. https://www.geo.sunysb.edu/esp/files/scientific-method.html\n\nWacker, J. (1998). A definition of theory: research guidelines for different theory-building research methods in operations management. Journal of Operations Management, 16(4), 361–385. https://doi.org/10.1016/s0272-6963(98)00019-9",
                "drafted_by": [
                    "Aoife O’Mahony"
                ],
                "reviewed_by": [
                    "Mahmoud Elsherif",
                    "Helena Hartmann",
                    "Charlotte R. Pennington",
                    "Graham Reid"
                ],
                "alt_related_terms": [
                    null
                ],
                "language": "arabic"
            },
            {
                "type": "glossary",
                "title": "بناء النَّظريَّة (Theory building)",
                "definition": "هي عمليَّة إنشاء وتطوير بيان للمفاهيم وعلاقاتها المتبادلة؛ لإظهار كيف أو لماذا تحدث ظاهرةٌ ما، فبناء الَّنظريَّة يقودنا إلى اختبارها.  **المصطلحات ذات الصِّلة:** الفرضيَّة، النَّموذج الفلسفي، النَّظريَّة، المساهمة النَّظريَّة، النَّموذج النَّظري.",
                "related_terms": [
                    "Hypothesis",
                    "Model (philosophy)",
                    "Theory",
                    "Theoretical contribution",
                    "Theoretical model"
                ],
                "references": "Borsboom, D., van der Maas, H., Dalege, J., Kievit, R., & Haig, B. (2020). Theory Construction Methodology: A practical framework for theory formation in psychology. https://doi.org/10.31234/osf.io/w5tp8\n\nCorley, K. G., & Gioia, D. A. (2011). Building theory about theory building: what constitutes a theoretical contribution? Academy of Management Review, 36(1), 12–32. https://doi.org/10.5465/amr.2009.0486\n\nGioia, D. A., & Pitre, E. (1990). Multiparadigm perspectives on theory building. Academy of Management Review, 15(4), 584–602. https://doi.org/10.5465/amr.1990.4310758\n\nWacker, J. (1998). A definition of theory: research guidelines for different theory-building research methods in operations management. Journal of Operations Management, 16(4), 361–385. https://doi.org/10.1016/s0272-6963(98)00019-9",
                "drafted_by": [
                    "Filip Dechterenko"
                ],
                "reviewed_by": [
                    "Mahmoud Elsherif",
                    "Helena Hartmann",
                    "Charlotte R. Pennington"
                ],
                "alt_related_terms": [
                    null
                ],
                "language": "arabic"
            },
            {
                "type": "glossary",
                "title": "الثّلاثي المقلق (The Troubling Trio)",
                "definition": "يوصف بأنَّه مزيج من القوة الإحصائيَّة المنخفضة، والنَّتيجة المفاجئة، والقيمة الاحتماليَّة التي تقل عن 0.05 بقليل فقط.  **المصطلحات ذات الصِّلة**: التِّكرار، قابليَّة إعادة الإنتاج، اختبار دلالة الفرضيَّة الصِّفرية، قرصنة القيمة الاحتماليَّة، ممارسات البحث المشكوك فيها أو ممارسات إعداد التَّقارير المشكوك فيها.",
                "related_terms": [
                    "Replication",
                    "Reproducibility",
                    "Null Hypothesis Significance Testing (NHST)",
                    "*P*\\-hacking",
                    "Questionable Research Practices or Questionable Reporting Practices (QRPs)"
                ],
                "references": "Lindsay, D. S. (2015). Replication in Psychological Science . Psychological Science, 26(12), 1827–1832. https://doi.org/10.1177/0956797615616374",
                "drafted_by": [
                    "Halil Emre Kocalar"
                ],
                "reviewed_by": [
                    "",
                    "Catia M. Oliveira",
                    "Adam Parker",
                    "Sam Parsons;Charlotte R. Pennington"
                ],
                "alt_related_terms": [
                    null
                ],
                "language": "arabic"
            },
            {
                "type": "glossary",
                "title": "الشَّفافيَّة (Transparency)",
                "definition": "تشير إلى جعل أعمال الفرد متاحة، وقابلة للإطِّلاع ويمكن الوصول إليها؛ لغرض التَّقييم الخارجي. تتعلَّق الشَّفافيَّة بأن يكون الباحثون صادقين بشأن القرارات النَّظريَّة، و المنهجيَّة، والتَّحليلية التي يتم اتخاذها طوال دورة البحث. ويمكن الفصل بين نوعين من الشَّفافيَّة: الأول يتعلَّق \"بالشَّفافيَّة العلميَّة\" والتي تركز على النِّقاشات المتعلِّقة بالعلم المفتوح، بينما النَّوع الثَّاني يتعلَّق \"بالشَّفافيَّة المجتمعيَّة\" والتي تركِّز على ضرورة توفير المعلومات العلميَّة بطرق لها علاقة بصنّاع القرار والمجتمع العام (Elliott & Resnik, 2019).  **المصطلحات ذات الصِّلة:** المصداقيَّة في الادعِّاءات العلميَّة، العلم المفتوح، التَّسجيل المسبق، قابليَّة إعادة الإنتاج، استحقاق الثِّقة.",
                "related_terms": [
                    "Credibility of scientific claims",
                    "Open science",
                    "Preregistration",
                    "Reproducibility",
                    "Trustworthiness"
                ],
                "references": "Elliott, K. C., & Resnik, D. B. (2019). Making open science work for science and society. Environmental Health Perspectives, 127(7). https://doi.org/10.1289/EHP4808\n\nLyon, L. (2016). Transparency: The Emerging Third Dimension of Open Science and Open Data. LIBER Quarterly, 25(4), 153–171. http://doi.org/10.18352/lq.10113\n\nSyed, M. (2019). The Open Science Movement is for all of us. PsyArXiv.",
                "drafted_by": [
                    "William Ngiam"
                ],
                "reviewed_by": [
                    "Tamara Kalandadze",
                    "Aoife O’Mahony",
                    "Eike Mark Rinke",
                    "Flávio Azevedo"
                ],
                "alt_related_terms": [
                    null
                ],
                "language": "arabic"
            },
            {
                "type": "glossary",
                "title": "قائمة مراجعة الشَّفافيَّة (Transparency Checklist)",
                "definition": "قائمة مراجعة الشَّفافيَّة عبارة عن قائمة مراجعة شاملة مبنية على الإجماع تحتوي على 36 عنصرًا و تغطِّي التَّسجيل المسبق، والطُّرق، والنَّتائج، والمناقشة، والبيانات، والنَّص البرمجي، وتوافر المواد. تتوفَّر أيضًا نسخة مختصرة من 12 عنصرًا من قائمة المراجعة. يمكن تقديم الرُّدود على قائمة المراجعة جنبًا إلى جنب مع مخطوطة للمراجعة. و على الرَّغم من أنَّه يمكن أن تُستعمل هذه القائمة للأغراض التَّعليميَّة، إلا أنَّها تهدف بشكل أساسي إلى دعم الباحثين لتحديد الإجراءات الملموسة التي يمكن أن تزيد من شفافيَّة أبحاثهم بينما يمكن لقائمة المراجعة التي تم الكشف عنها أن تساعد القراء والمراجعين في الحصول على معلومات مهمَّة حول الجوانب المختلفة لشفافيَّة البحث المُقدم.  **المصطلحات ذات الصِّلة:** مصداقيَّة الادِّعاءات العلميَّة ،العلم المفتوح، التَّسجيل المسبق، قابليَّة إعادة الإنتاج، الجدارة بالثِّقة",
                "related_terms": [
                    "Credibility of scientific claims",
                    "Open science",
                    "Preregistration",
                    "Reproducibility",
                    "Trustworthiness"
                ],
                "references": "",
                "drafted_by": [
                    "Barnabas Szaszi"
                ],
                "reviewed_by": [
                    "Sarah Ashcroft-Jones",
                    "Mahmoud Elsherif",
                    "Helena Hartmann",
                    "Graham Reid",
                    "Flávio Azevedo"
                ],
                "alt_related_terms": [
                    null
                ],
                "language": "arabic"
            },
            {
                "type": "glossary",
                "title": "التّحكيم ثلاثيّ التَّعمية (Triple-blind peer review)",
                "definition": "تقييم المنتجات البحثيَّة بعرضها على خبراء مؤهلين بحيث يكون المؤلِّف (أو المؤلِّفون) مجهول الهوية لكلِّ من المحكّمين والمحرِّرين. \"إخفاء المؤلِّفين وانتمائهم لكلٍّ من المحرِّرين والمراجعين. هذه المنهجيَّة تهدف الى استبعاد التَّحيُّزات الشَّخصيَّة، والمؤسَّسيَّة والجندريَّة\" (Tvina et al., 2019, p. 1082).  **المصطلحات ذات الصِّلة:** التَّحكيم مزدوج التَّعمية، التَّحكيم المفتوح، التَّحكيم أحاديَّ التَّعمية.",
                "related_terms": [
                    "Double-blind peer review",
                    "Open Peer Review",
                    "Single-blind peer review"
                ],
                "references": "Largent, E. A., & Snodgrass, R. T. (2016). Blind peer review by academic journals. In C. T. Robertson & A. S. Kesselheim (Eds.), Blinding as a solution to bias: Strengthening biomedical science, forensic science, and law (pp. 75–95). Academic Press. https://doi.org/10.1016/B978-0-12-802460-7.00005-X\n\nTvina, A., Spellecy, R., & Palatnik, A. (2019). Bias in the peer review process: can we do better? Obstetrics & Gynecology, 133(6), 1081–1083. https://doi.org/10.1097/AOG.0000000000003260",
                "drafted_by": [
                    "Mahmoud Elsherif"
                ],
                "reviewed_by": [
                    "Bradley Baker",
                    "Helena Hartmann",
                    "Charlotte R. Pennington",
                    "Christopher Graham"
                ],
                "alt_related_terms": [
                    null
                ],
                "language": "arabic"
            },
            {
                "type": "glossary",
                "title": "مبادئ الثِّقة (TRUST Principles)",
                "definition": "مجموعة من المبادئ التَّوجيهيَّة التي تَعتبر الشَّفافيَّة، والمسؤوليَّة، والتَّركيز على المستخدم والاستدامة والتّكنولوجيا مكونات أساسيَّة لتقييم، وتطوير، واستدامة موثوقيَّة مستودعات البيانات الرَّقميَّة (خاصّة تلك التي تخزِّن بيانات البحث). وهي مُكمّلة لمبادئ بيانات فير.  **المصطلحات ذات الصِّلة:** مبادئ فير ،البيانات الوصفيَّة، الوصول المفتوح، البيانات المفتوحة، المواد المفتوحة، مستودع  البيانات.",
                "related_terms": [
                    "FAIR principles",
                    "Metadata",
                    "Open Access",
                    "Open Data",
                    "Open Material",
                    "Repository"
                ],
                "references": "Lin, D., Crabtree, J., Dillo, I., Downs, R. R., Edmunds, R., Giaretta, D., De Giusti, M., L’Hours, H., Hugo, W., Jenkyns, R., Khodiyar, V., Martone, M. E., Mokrane, M., Navale, V., Petters, J., Sierman, B., Sokolova, D. V., Stockhause, M., & Westbrook, J. (2020). The TRUST Principles for digital repositories. Scientific Data, 7(1), 144. https://doi.org/10.1038/s41597-020-0486-7",
                "drafted_by": [
                    "Aleksandra Lazić"
                ],
                "reviewed_by": [
                    "Jamie P. Cockcroft",
                    "Mahmoud Elsherif",
                    "Helena Hartmann",
                    "Sam Parsons"
                ],
                "alt_related_terms": [
                    null
                ],
                "language": "arabic"
            },
            {
                "type": "glossary",
                "title": "الخطأ من النَّوع الأوّل (Type I error)",
                "definition": "\"الرَّفض غير الصَّحيح للفرضيَّة الصِّفريّة \" (Simmons et al., 2011, p. 1359)، هو الوصول إلى دليل لرفض الفرضيَّة الصِّفريَّة  (القائلة بأنَّه لا يوجد أي تأثير) عندما يكون الدَّليل في الواقع لصالح الإبقاء على فرضيَّة العدم الذي لا أثر له  (مثلًا، قاض يسجن شخصًا بريئًا). وهو استنتاج أنَّ هناك تأثيرًا حقيقيًا، ورفض الفرضية الصِّفرية  حينما تكون النَّتائج قد حدثت بالفعل عن طريق الصُّدفة.  **المصطلحات ذات الصِّلة:** الإحصاءات التِّكراريَّة، اختبار دلالة الفرضيَّة الصِّفريَّة، نتيجة عدميَّة، القيمة الاحتماليَّة، ممارسات البحث المشكوك فيها، أو ممارسات إعداد التَّقارير المشكوك فيها، أزمة إعادة الإنتاج  (أو أزمة التِّكرار)؛ النَّزاهة العلميَّة، القوة الإحصائيَّة،  نتيجة إيجابيَّة حقيقيَّة، الخطأ من النُّوع الثَّاني.",
                "related_terms": [
                    "Frequentist statistics",
                    "Null Hypothesis Significance Testing (NHST)",
                    "Null Result",
                    "*P* value",
                    "Questionable Research Practices or Questionable Reporting Practices (QRPs)",
                    "Reproducibility crisis (aka Replicability or replication crisis)",
                    "Scientific integrity",
                    "Statistical power",
                    "True positive result",
                    "Type II error"
                ],
                "references": "Simmons, J. P., Nelson, L. D., & Simonsohn, U. (2011). False-positive psychology: Undisclosed flexibility in data collection and analysis allows presenting anything as significant. Psychological Science, 22(11), 1359–1366. https://doi.org/10.1177/0956797611417632",
                "drafted_by": [
                    "Lisa Spitzer"
                ],
                "reviewed_by": [
                    "Mahmoud Elsherif",
                    "Adrien Fillon",
                    "Helena Hartmann",
                    "Matt Jaquiery",
                    "Mariella Paul",
                    "Charlotte R. Pennington",
                    "Graham Reid",
                    "Olly Robertson",
                    "Mirela Zaneva"
                ],
                "alt_related_terms": [
                    null
                ],
                "language": "arabic"
            },
            {
                "type": "glossary",
                "title": "الخطأ من النَّوع الثَّاني (Type II error)",
                "definition": "تحدث النَّتيجة السَّلبيَّة الخاطئة عندما تكون الفرضيَّة البديلة صحيحة في مجتمع الدِّراسة، ولكن يتم قبول الفرضيَّة الصِّفريَّة كجزء من التَّحليل (Hartgerink et al.، 2017). أي العثور على نتيجة إحصائيَّة ليست ذات دلالة إحصائيَّة عندما يكون التَّأثير حقيقيًا (على سبيل المثال، قاض يصدر حكمًا بريئًا على شخص مذنب). عادةً ما تجذب النَّتائج السَّلبيَّة الخاطئة اهتمامًا أقل من الأبحاث التِّكرارية مقارنةً بالنَّتائج الإيجابيَّة (Fiedler et al.، 2012\\) ، وتظل هذه مشكلة لم يتم إيجاد حل لها في البحث العلمي (Hartgerink et al.، 2017).  **المصطلحات ذات الصِّلة:** حجم الأثر، اختبار دلالة الفرضيَّة الصِّفريَّة،  ممارسات البحث المشكوك فيها، أو ممارسات إعداد التَّقارير المشكوك فيها، أزمة إعادة الإنتاج  (أو أزمة التَّكرار،)، النَّزاهة العلميَّة، القوة الإحصائيَّة، نتيجة إيجابيَّة حقيقيَّة، الخطأ من النَّوع الأوّل.",
                "related_terms": [
                    "Effect size",
                    "Null Hypothesis Significance Testing (NHST)",
                    "Questionable Research Practices or Questionable Reporting Practices (QRPs)",
                    "Reproducibility crisis (aka Replicability or replication crisis)",
                    "Scientific integrity",
                    "Statistical power",
                    "True positive result",
                    "Type I error"
                ],
                "references": "Fiedler, K., Kutzner, F., & Krueger, J. I. (2012). The long way from α-error control to validity proper: Problems with a short-sighted false-positive debate. Perspectives on Psychological Science, 7(6), 661–669. https://doi.org/10.1177/1745691612462587\n\nHartgerink, C. H., Wicherts, J. M., & Van Assen, M. A. L. M. (2017). Too good to be false: Nonsignificant results revisited. Collabra: Psychology, 3(1). https://doi.org/10.1525/collabra.71",
                "drafted_by": [
                    "Olly Robertson"
                ],
                "reviewed_by": [
                    "Mahmoud Elsherif",
                    "Charlotte R. Pennington"
                ],
                "alt_related_terms": [
                    null
                ],
                "language": "arabic"
            },
            {
                "type": "glossary",
                "title": "الخطأ من النَّوع إم (Type M error)",
                "definition": "يحدث خطأ من النَّوع (إم) عندما يستنتج الباحث أنَّه تم ملاحظة تأثير بحجم أقل، أو أعلى من التَّأثير الحقيقي. على سبيل المثال: يحدث خطأ من النَّوع (إم) عندما يدَّعي الباحث أنَّه تم ملاحظة تأثير صغير الحجم عندما يكون كبيرًا في الحقيقة أو العكس.  **المصطلحات ذات الصِّلة:** القوة الإحصائيَّة، الخطأ من النَّوع إس، الخطأ من النَّوع الأوّل، الخطأ من النَّوع الثَّاني.",
                "related_terms": [
                    "Statistical power",
                    "Type S error",
                    "Type I error",
                    "Type II error"
                ],
                "references": "Gelman, A., & Carlin, J. (2014). Beyond Power Calculations: Assessing Type S (Sign) and Type M (Magnitude) Errors. Perspectives on Psychological Science, 9(6), 641–651. https://doi.org/10.1177/1745691614551642\n\nLu, J., Qiu, Y., & Deng, A. (2018). A note on Type S/M errors in hypothesis testing. British Journal of Mathematical and Statistical Psychology, 72(1), 1–17. https://doi.org/10.1111/bmsp.12132",
                "drafted_by": [
                    "Eduardo Garcia-Garzon"
                ],
                "reviewed_by": [
                    "Mahmoud Elsherif",
                    "Helena Hartmann",
                    "Graham Reid",
                    "Mirela Zaneva"
                ],
                "alt_related_terms": [
                    null
                ],
                "language": "arabic"
            },
            {
                "type": "glossary",
                "title": "الخطأ من النَّوع إس (Type S error)",
                "definition": "يحدث خطأ من النَّوع (إس) عندما يستنتج الباحث أنَّه تم ملاحظة تأثير بعلامة معاكسة للعلامة الحقيقيَّة. على سبيل المثال: يحدث خطأ من النَّوع إس عندما يدَّعي الباحث أنَّه تم ملاحظة تأثير إيجابي عندما يكون سلبيًا في الواقع أو العكس.  **المصطلحات ذات الصِّلة:** القوة الإحصائيَّة، الخطأ من النَّوع إم، الخطأ من النَّوع الأوّل، الخطأ من النَّوع الثَّاني.",
                "related_terms": [
                    "Statistical power",
                    "Type M error",
                    "Type I error",
                    "Type II error"
                ],
                "references": "Gelman, A., & Carlin, J. (2014). Beyond Power Calculations: Assessing Type S (Sign) and Type M (Magnitude) Errors. Perspectives on Psychological Science, 9(6), 641–651. https://doi.org/10.1177/1745691614551642\n\nLu, J., Qiu, Y., & Deng, A. (2018). A note on Type S/M errors in hypothesis testing. British Journal of Mathematical and Statistical Psychology, 72(1), 1–17. https://doi.org/10.1111/bmsp.12132",
                "drafted_by": [
                    "Eduardo Garcia-Garzon"
                ],
                "reviewed_by": [
                    "Helena Hartmann",
                    "Sam Parsons",
                    "Graham Reid",
                    "Mirela Zaneva"
                ],
                "alt_related_terms": [
                    null
                ],
                "language": "arabic"
            },
            {
                "type": "glossary",
                "title": "نقص التَّمثيل (Under-representation)",
                "definition": "يصف هذا المصطلح النَّقص أو القصور في تمثيل كافّة الأصوات، ووجهات النَّظر، والفئات الموجودة في المجتمع بشكل كافٍ.  ويحدث نقص التَّمثيل عادةً عندما يسيطر صوت ووجهات نظر مجموعة ما على المجتمع ممّا يؤدِّي إلى تهميش للفئات الأخرى، ويؤثِّر هذا غالبًا على الأقليّات التي تمتلك خصائص فرديّة معيّنة.  **المصطلحات ذات الصِّلة:** العدالة، عدالة، عدم مساواة، ويرد",
                "related_terms": [
                    "Equity",
                    "Fairness",
                    "Inequality",
                    "WEIRD"
                ],
                "references": "",
                "drafted_by": [
                    "Madeleine Pownall"
                ],
                "reviewed_by": [
                    "Mahmoud Elsherif",
                    "Helena Hartmann",
                    "Bethan Iley",
                    "Adam Parker",
                    "Charlotte R. Pennington, Mirela Zaneva"
                ],
                "alt_related_terms": [
                    null
                ],
                "language": "arabic"
            },
            {
                "type": "glossary",
                "title": "UDL التَّصميم الشَّامل للتَّعلُّم (Universal design for learning (UDL))",
                "definition": "هو إطار عمل لتحسين التَّعلُّم وتحسين التَّدريس بناءً على الرُّؤى العلميَّة لكيفيَّة تعلُّم البشر.  يهدف إلى جعل التَّعلُّم شاملًا وتحويليًا لجميع الأشخاص حيث ينصبُّ التَّركيز على تلبية الاحتياجات المختلفة للطُّلاب المختلفين. وغالبًا ما يُنظر إليه على أنَّه إطار عمل قائم على الأدلَّة و صحيح علميًا لتوجيه الممارسة التَّعليميَّة، حيث يتألَّف من ثلاثة مبادئ رئيسيَّة وهي: المشاركة، والتَّمثيل، والعمل والتعبير. بالإضافة إلى ذلك، تم تضمين التَّصميم الشَّامل للتَّعلُّم في قانون فرص التَّعليم العالي لعام 2008 (Edyburn, 2010). **المصطلحات ذات الصِّلة:** تكافؤ الفرص، الشُّمولية، أصول التَّربية، ممارسة التَّدريس.",
                "related_terms": [
                    "Equal opportunities",
                    "Inclusivity",
                    "Pedagogy",
                    "Teaching practice"
                ],
                "references": "Hitchcock, C., Meyer, A., Rose, D., & Jackson, R. (2002). Providing new access to the general curriculum: Universal design for learning. Teaching Exceptional Children, 35(2), 8–17. https://www.proquest.com/scholarly-journals/providing-new-access-general-curriculum/docview/201139970/se-2?accountid=8630\n\nRose, D. (2000). Universal design for learning. Journal of Special Education Technology, 15(3), 45–49. https://doi.org/10.1177/016264340001500307\n\nRose, D. H., & Meyer, A. (2002). Teaching every student in the digital age: Universal design for learning. In The Corsini Encyclopedia of Psychology. Association for Supervision.",
                "drafted_by": [
                    "Charlotte R. Pennington"
                ],
                "reviewed_by": [
                    "Valeria Agostini",
                    "Mahmoud Elsherif",
                    "Graham Reid",
                    "Mirela Zaneva",
                    "Flávio Azevedo"
                ],
                "alt_related_terms": [
                    null
                ],
                "language": "arabic"
            },
            {
                "type": "glossary",
                "title": "الصِّدق (Validity)",
                "definition": "يشير الصِّدق إلى تطبيق المبادئ الإحصائيَّة للوصول إلى مفاهيم، أو استنتاجات، أو مقاييس ذات أساس جيد؛ أي من المحتمل أن تتطابق بدقَّة مع الواقع.  وفي القياس النَّفسي، يشير الصِّدق إلى المدى الذي يقيس فيه شيء ما، و يهدف أو يدّّعي قياسه. يشمل هذا المصطلح العام عدَّة أنواع مختلفة من الصِّدق، منها على سبيل المثال: الصِّدق الدَّاخلي، صدق البناء، الصِّدق الظَّاهري، صدق المحك، الصِّدق التَّشخيصي، الصِّدق التَّمييزي، الصِّدق التَّلازمي، الصِّدق التَّقاربي، الصِّدق التَّنبؤي، الصِّدق الخارجي.  **المصطلحات ذات الصِّلة:** السَّببية، الصِّدق البنائي، صدق المحتوى، صدق المحك، الصِّدق الخارجي، الصِّدق الظَّاهري، الصِّدق الدَّاخلي، القياس، ممارسات القياس المشكوك فيها، القياس النَّفسي، الثَّبات، القوة الإحصائيَّة، الصِّدق الإحصائي، الاختبار.",
                "related_terms": [
                    "Causality",
                    "Construct validity",
                    "Content validity",
                    "Criterion validity",
                    "External validity",
                    "Face validity",
                    "Internal validity",
                    "Measurement",
                    "Questionable Measurement Practices (QMP)",
                    "Psychometry",
                    "Reliability",
                    "Statistical power",
                    "Statistical validity",
                    "Test"
                ],
                "references": "Campbell, D. T. (1957). Factors relevant to the validity of experiments in social settings. Psychological Bulletin, 54(4), 297–312. https://doi.org/10.1037/h0040950\n\nKelley, T. L. (1927). Interpretation of educational measurements. Macmillan.",
                "drafted_by": [
                    "Tamara Kalandadze; Madeleine Pownall; Flávio Azevedo"
                ],
                "reviewed_by": [
                    "Eduardo Garcia-Garzon",
                    "Halil E. Kocalar",
                    "Annalise A. LaPlume",
                    "Joanne McCuaig",
                    "Adam Parker",
                    "Charlotte R. Pennington"
                ],
                "alt_related_terms": [
                    null
                ],
                "language": "arabic"
            },
            {
                "type": "glossary",
                "title": "التَّحكم في الإصدار (Version control)",
                "definition": "ممارسة إدارة وتسجيل التَّغيُّرات التي تطرأ على الموارد الرَّقميَّة،مثل: الملفَّات، والمواقع الإلكترونيَّة، والبرامج وما إلى ذلك…  بمرور الوقت بحيث يمكن مراجعة إصدارات معيَّنة لاحقًا. تم تصميم أنظمة التَّحكم في الإصدار لتسجيل محفوظات التَّغييرات (من، وماذا، ومتى)، والمساعدة على تجنُّب الأخطاء البشريَّة، مثل العمل على إصدار خاطئ، على سبيل المثال: يعد نظام التَّحكم في إصدار Git أداة برمجيَّة مستخدمة على نطاق واسع حيث ساعدت مطوري البرامج في الأصل على التَّحكُّم في إصدار التَّعليمات البرمجيَّة المشتركة، ويستخدم الآن في العديد من التَّخصُّصات العلميَّة لإدارة الملفات ومشاركتها.  **المصطلحات ذات الصِّلة: ا**لمتتبِّع الدُّولي العالمي، قابليَّة إعادة الإنتاج، إدارة تكوين البرامج، إدارة شفرة المصدر، التَّحكم في المصدر.",
                "related_terms": [
                    "Git",
                    "Reproducibility",
                    "Software configuration management",
                    "Source code management",
                    "Source control"
                ],
                "references": "Git. (n.d.). Git—About Version Control. https://git-scm.com/book/en/v2/Getting-Started-About-Version-Control",
                "drafted_by": [
                    "Mahmoud Elsherif"
                ],
                "reviewed_by": [
                    "Sarah Ashcroft-Jones",
                    "Thomas Rhys Evans",
                    "Helena Hartmann",
                    "Matt Jaquiery",
                    "Adam Parker",
                    "Charlotte R. Pennington",
                    "Robert M. Ross",
                    "Timo Roettger",
                    "Andrew J. Stewart"
                ],
                "alt_related_terms": [
                    null
                ],
                "language": "arabic"
            },
            {
                "type": "glossary",
                "title": "قياسات الويب (Webometrics)",
                "definition": "تتضمَّن قياسات الويب دراسة المحتوى عبر الإنترنت، وتركِّز على أعداد، وأنواع الارتباطات التَّشعبيَّة بين مواقع الإنترنت المختلفة. وتعدُّ هذه الأساليب كنوع من المقاييس البديلة. \"دراسة الجوانب الكميَّة لبناء واستخدام موارد المعلومات، والهياكل، والتِّقنيَّات على الويب بالاعتماد على المناهج الببليومتريَّة والمعلوماتيَّة\" (Björneborn & Ingwersen, 2004).  **المصطلحات ذات الصِّلة:** المقاييس البديلة، القياسات الببليومتريَّة.",
                "related_terms": [
                    "Altmetrics",
                    "Bibliometrics"
                ],
                "references": "Bjørneborn, L., & Ingwersen, P. (2004). Toward a basic framework for webometrics. Journal of the American Society for Information Science and Technology, 55(14), 1216–1227. https://doi.org/10.1002/asi.20077",
                "drafted_by": [
                    "Charlotte R. Pennington"
                ],
                "reviewed_by": [
                    "Christopher Graham",
                    "Mirela Zaneva"
                ],
                "alt_related_terms": [
                    null
                ],
                "language": "arabic"
            },
            {
                "type": "glossary",
                "title": "وِيرد (WEIRD)",
                "definition": "التَّعريف: يشير هذا الاختصار إلى المجتمعات الغربيَّة، والمتعلِّمة، والصِّناعيَّة، والغنيَّة، والدِّيمقراطية. يتم إجراء معظم البحوث على، وبواسطة، عيِّنات متجانسة نسبيًا من مجتمعات ويرد، وهذا ما يحد من قابليَّة تعميم عدد كبير من نتائج البحوث، لا سيما بالنَّظر إلى أنَّ الأشخاص الغريبين غالبًا ما يكونون متطرِّفين نفسيًا. لقد قيل أنَّ \"علم نفس ويرد\" بدأ في التَّطور ثقافيًا نتيجة للتَّغيُّرات المجتمعيَّة، والمعتقدات الدِّينيَّة في العصور الوسطى في أوروبا. يقترح نقّاد هذا المصطلح أنَّه يقدِّم وجهة نظر ثنائيَّة لسكان العالم، ويمحو التَّباين الموجود بين وداخل المجتمعات .وأنَّ الجوانب الأخرى للتَّنوُّع لا يتم التقاطها.\nالمصطلحات ذات الصِّلة: التَّحيُّز، بيزار، التَّنوع، القابليَّة للتَّعميم، السُّكان، تحيُّز أخذ العيِّنات، سترينج: الخلفية الاجتماعيَّة والقابليَّة للتَّتبُّع والاختيار الذَّاتي وتاريخ التَّربية، والتَّأقلم، والتَّعود.",
                "related_terms": [
                    "**Alternative definition:** (if applicable) **Related terms to alternative definition:** (if applicable)"
                ],
                "references": "",
                "drafted_by": [],
                "reviewed_by": [],
                "alt_related_terms": [
                    null
                ],
                "language": "arabic"
            },
            {
                "type": "glossary",
                "title": "منحنى زد (Z-Curve)",
                "definition": "حساب درجة زد هو نهج إحصائي يستخدم بشكل أساسي للحصول على \"معدل النَّسخ المقدر\" و \"معدل الاكتشاف المتوقَّع\" لمجموعة من الدِّراسات التي تم إعداد تقارير نتائجها. يتضمَّن حساب منحنى زد لمجموعة من الدِّراسات ذات الدِّلالة الإحصائيَّة تحويل القيم الاحتماليَّة التي أعدت سلفاً إلى درجات زد، وتثبيت نموذج خليط محدود لتوزيع درجات زد، وتقدير متوسط ​​القوة بناءً على نموذج الخليط. يمكن إجراء تحليل منحنى زد في لغة الآر من خلال حزمة مخصَّصة \\- [https://cran.r-project.org/web/packages/zcurve/index.html](https://cran.r-project.org/web/packages/zcurve/index.html).  **المصطلحات ذات الصِّلة:** المقاييس البديلة، نسبة درج الملفَّات، منحنى القيمة الاحتماليَّة، قرصنة القيمة الاحتماليَّة ، تكرار أو نسخ متماثل، القوة الإحصائيَّة.",
                "related_terms": [
                    "Altmetrics",
                    "File drawer ratio",
                    "P-curve",
                    "P-hacking",
                    "Replication",
                    "Statistical power"
                ],
                "references": "Bartoš, F., & Schimmack, U. (2020). Z-Curve 2.0: Estimating replication rates and discovery rates. https://doi.org/10.31234/osf.io/urgtn\n\nBrunner, J., & Schimmack, U. (2020). Estimating population mean power under conditions of heterogeneity and selection for significance. Meta-Psychology, 4, MP.2018.874. https://doi.org/10.15626/MP.2018.874",
                "drafted_by": [
                    "Bradley J. Baker"
                ],
                "reviewed_by": [
                    "Kamil Izydorczak",
                    "Sam Parsons",
                    "Charlotte R. Pennington",
                    "Mirela Zaneva"
                ],
                "alt_related_terms": [
                    null
                ],
                "language": "arabic"
            },
            {
                "type": "glossary",
                "title": "زينودو (Zenodo)",
                "definition": "مستودع علمي مفتوح حيث يمكن للباحثين إيداع الأوراق البحثيَّة، والتَّقارير، ومجموعات البيانات، وبرامج البحث، وأي أدوات رقميَّة أخرى متعلِّقة بالبحوث، وينشئ زينودو معرفًا رقميًا ثابتًا لكلِّ عمليَّة إرسال لجعلها قابلة للاستشهاد. وقد تم تطوير هذه المنصَّة في إطار برنامج OpenAIRE أوبن إيرالأوروبي وتشغيلها بواسطة منظمة سيرن.  **المصطلحات ذات الصِّلة:** DOI (معرف الكائن الرقمي)، مشاركة، البيانات المفتوحة، إطار العلوم المفتوحة، الطّباعة الأولية",
                "related_terms": [
                    "DOI (digital object identifier)",
                    "figshare",
                    "Open data",
                    "Open Science Framework",
                    "Preprint"
                ],
                "references": "Zenodo. (n.d.). Zenodo—Research. Shared. https://www.zenodo.org/",
                "drafted_by": [
                    "Ali H. Al-Hoorie"
                ],
                "reviewed_by": [],
                "alt_related_terms": [
                    null
                ],
                "language": "arabic"
            }
        ]
    },
    {
        "german": [
            {
                "type": "glossary",
                "title": "Abstract-Verzerrung (Abstract Bias)",
                "definition": "Die Tendenz, nur signifikante Ergebnisse in der Zusammenfassung (Abstract) zu berichten, obwohl nicht-signifikante Ergebnisse im Hauptteil des Manuskripts berichtet werden (die nicht-signifikanten Ergebnisse gar nicht zu berichten wäre selektives Berichten (selective reporting)). Die Folge von Abstract Verzerrung ist, dass Studien mit nicht-signifikanten Ergebnissen möglicherweise durch übliche Suchstrategien für Metaanalysen (die auf Informationen in Titel, Abstract und Keywords beruhen) nicht gefunden werden und damit die Ergebnisse von Metaanalysen verfälscht (biased) werden.",
                "related_terms": [
                    "Cherry-picking",
                    "Publication bias (File Drawer Problem)",
                    "Selective reporting"
                ],
                "references": "Duyx, B., Swaen, G. M., Urlings, M. J., Bouter, L. M., & Zeegers, M. P. (2019). The strong focus on positive results in abstracts may cause bias in systematic reviews: A case study on abstract reporting bias. Systematic Reviews, 8(1), 1–8.",
                "drafted_by": [
                    "Ali H. Al-Hoorie"
                ],
                "reviewed_by": [
                    "Mahmoud Elsherif",
                    "Bethan Iley",
                    "Sam Parsons",
                    "Gerald Vineyard",
                    "Eliza Woodward",
                    "Flávio Azevedo"
                ],
                "alt_related_terms": [
                    null
                ],
                "language": "german"
            },
            {
                "type": "glossary",
                "title": "Akademischer Einfluss (Academic Impact)",
                "definition": "Den Einfluss, den ein Forschungsergebnis (z. B. ein publizierter Artikel) auf unser Verstehen von wissenschaftliche Theorien, Methoden und Anwendungen hat, und wie weit es dieses Verstehen vorantreibt, sowohl innerhalb von Disziplinen als auch disziplinübergreifend. Einfluss kann auch den Grad meinen, zu dem ein Forschungsergebnis oder ein Forschungsprogramm Veränderungen außerhalb der Wissenschaft beeinflusst, z. B. soziale oder wirtschaftliche Auswirkung (cf. ESRC: [https://esrc.ukri.org/research/impact-toolkit/what-is-impact/](https://esrc.ukri.org/research/impact-toolkit/what-is-impact/)).",
                "related_terms": [
                    "Beneficiaries",
                    "DORA",
                    "Reach",
                    "REF"
                ],
                "references": "Anon. (2021). What is impact?. Retrieved from https://esrc.ukri.org/research/impact-toolkit/what-is-impact/",
                "drafted_by": [
                    "Connor Keating"
                ],
                "reviewed_by": [
                    "Myriam A. Baum",
                    "Adam Parker",
                    "Charlotte R. Pennington",
                    "Suzanne L. K. Stewart",
                    "Flávio Azevedo"
                ],
                "alt_related_terms": [
                    null
                ],
                "language": "german"
            },
            {
                "type": "glossary",
                "title": "Barrierefreiheit (Accessibility)",
                "definition": "Barrierefreiheit (Accessibility) bezieht sich auf den einfachen Zugriff auf und die Wiederverwendung von Materialien (z. B. Daten, Code, Ergebnisse, Veröffentlichungen) für akademische Zwecke, wobei insbesondere Menschen mit einer chronischen Krankheit, einer Behinderung und/oder Neurodivergenz der Zugang erleichtert wird. Diese Gruppen sind mit zahlreichen finanziellen, rechtlichen und/oder technischen Hindernissen in der Forschung konfrontiert, einschließlich (aber nicht beschränkt auf) dem Erwerb von angemessen formatierten Materialien und dem physischen Zugang zu Räumen. Accessibility umfasst auch strukturelle Anliegen wie Diversität, Gleichberechtigung, Inklusion und Repräsentation (Pownall et al., 2021). Bei der Gestaltung von Benutzer:innenoberflächen, Veranstaltungen und Räumen sollte auf Barrierefreiheit geachtet werden, um eine uneingeschränkte Teilnahme zu gewährleisten. Beispielsweise kann sichergestellt werden, dass webbasierte Bilder erkennbar für farbenblinde Menschen sind und über Bildbeschreibungen verfügen, oder indem bei Veranstaltungen Live-Untertitel eingeblendet werden (Brown et al., 2018; Pollet & Bond, 2021; World Wide Web Consortium, 2021).",
                "related_terms": [
                    "Availability",
                    "Data availability statements",
                    "Inclusion",
                    "Open Access",
                    "Under-representation",
                    "Universal design for learning (UDL)"
                ],
                "references": "Brown, N., Thompson, P., & Leigh, J. S. (2018). Making academia more accessible. Journal of Perspectives in Applied Academic Practice, 6(2), 82–90. https://doi.org/10.14297/jpaap.v6i2.348\n\nPollet, I. L., & Bond, A. L. (2021). Evaluation and recommendations for greater accessibility of colour figures in ornithology. Ibis, 163, 292–295. https://doi.org/10.1111/ibi.12887\n\nSuber, P. (2004). The primacy of authors in achieving Open Access. In Nature. Retrieved from http://dash.harvard.edu/handle/1/4391161)\n\nPownall, M., Talbot, C. V., Henschel, A., Lautarescu, A., Lloyd, K. E., Hartmann, H., Darda, K. M., Tang, K. T. Y., Carmichael-Murphy, P., & Siegel, J. A. (2021). Navigating Open Science as Early Career Feminist Researchers. Psychology of Women Quarterly, 45(4), 526–539. https://doi.org/10.1177/03616843211029255 Retrieved from https://journals.sagepub.com/doi/10.1177/03616843211029255",
                "drafted_by": [
                    "Kai Krautter"
                ],
                "reviewed_by": [
                    "Valeria Agostini",
                    "Myriam A. Baum",
                    "Mahmoud Elsherif",
                    "Bethan Iley",
                    "Tamara Kalandadze",
                    "Ryan Millager",
                    "Sara Middleton",
                    "Charlotte R. Pennington",
                    "Madeleine Pownall",
                    "Robert M. Ross",
                    "Flávio Azevedo"
                ],
                "alt_related_terms": [
                    null
                ],
                "language": "german"
            },
            {
                "type": "glossary",
                "title": "Ad hominem Verzerrung (Ad hominem bias)",
                "definition": "Aus dem Lateinischen “zum Menschen hin”; Bewertung eines Arguments oder einer Arbeit in Abhängigkeit von Merkmalen der Person, die diese hervorgebracht hat, und nicht von Merkmalen des Arguments selbst. Ad hominem Verzerrung (bias) kann negativ sein, z. B. wenn Arbeit eine:r Konkurrent:in oder einer Zielperson persönlicher Anfeindungen kritischer betrachtet wird als die Qualität der Arbeit dieser Person verdient, aber auch positiv, z. B. wenn Arbeit eine:r Freund:in von übertrieben positiver Bewertung profitiert.",
                "related_terms": [
                    "Peer review"
                ],
                "references": "Barnes, R. M., Johnston, H. M., MacKenzie, N., Tobin, S. J., & Taglang, C. M. (2018). The effect of ad hominem attacks on the evaluation of claims promoted by scientists. PloS One, 13(1), e0192025. https://doi.org/10.1371/journal.pone.0192025\n\nTvina, A., Spellecy, R., & Palatnik, A. (2019). Bias in the peer review process: can we do better? Obstetrics & Gynecology, 133(6), 1081–1083. https://doi.org/10.1097/AOG.0000000000003260",
                "drafted_by": [
                    "Mahmoud Elsherif"
                ],
                "reviewed_by": [
                    "Bradley Baker",
                    "Filip Dechterenko",
                    "Bethan Iley",
                    "Madeleine Ingham",
                    "Graham Reid"
                ],
                "alt_related_terms": [
                    null
                ],
                "language": "german"
            },
            {
                "type": "glossary",
                "title": "Gegnerische Kollaboration (Adversarial collaboration)",
                "definition": "Eine Kollaboration, bei der zwei oder mehr Forschende mit gegensätzlichen oder widersprüchlichen theoretischen Ansichten \\- und wahrscheinlich unterschiedlichen Vorhersagen über die Ergebnisse einer Studie \\- an einem Projekt zusammenarbeiten. Das Ziel ist, Verzerrung (bias) und methodische Schwächen zu minimieren und eine gemeinsame Basis an Fakten zu erarbeiten, die von gegensätzlichen Theorien berücksichtigt werden muss.",
                "related_terms": [
                    "Collaboration",
                    "Many Analysts",
                    "Many Labs",
                    "Preregistration",
                    "Publication bias (File Drawer Problem)"
                ],
                "references": "Bateman, I., Kahneman, D., Munro, A., Starmer, C., & Sugden, R. (2005). Testing competing models of loss aversion: An adversarial collaboration. Journal of Public Economics, 89(8), 1561–1580. https://doi.org/10.1016/j.jpubeco.2004.06.013\n\nCowan, N., Belletier, C., Doherty, J. M., Jaroslawska, A. J., Rhodes, S., Forsberg, A., & Logie, R. H. (2020). How do scientific views change? Notes from an extended adversarial collaboration. Perspectives on Psychological Science, 15(4), 1011–1025. https://doi.org/10.1177/1745691620906415\n\nKerr, N. L., Ao, X., Hogg, M. A., & Zhang, J. (2018). Addressing replicability concerns via adversarial collaboration: Discovering hidden moderators of the minimal intergroup discrimination effect. Journal of Experimental Social Psychology, 78, 66–76. https://doi.org/10.1016/j.jesp.2018.05.001\n\nMellers, B., Hertwig, R., & Kahneman, D. (2001). Do frequency representations eliminate conjunction effects? An exercise in adversarial collaboration. Psychological Science, 12(4), 269–275. https://doi.org/10.1111/1467-9280.00350\n\nRakow, T., Thompson, V., Ball, L., & Markovits, H. (2014). Rationale and guidelines for empirical adversarial collaboration: A Thinking & Reasoning initiative. Thinking & Reasoning, 21(2), 167–175. https://doi.org/10.1080/13546783.2015.975405",
                "drafted_by": [
                    "Siu Kit Yeung"
                ],
                "reviewed_by": [
                    "Matt Jaquiery",
                    "Aoife O’Mahony",
                    "Charlotte R. Pennington",
                    "Flávio Azevedo",
                    "Madeleine Pownall**;** Martin Vasilev"
                ],
                "alt_related_terms": [
                    null
                ],
                "language": "german"
            },
            {
                "type": "glossary",
                "title": "collaborative commentary Gegnerischer kollaborativer Kommentar (Adversarial (collaborative) commentary)",
                "definition": "Ein Kommentar, bei dem die ursprünglichen Autor:innen eines Werks und die Kritiker:innen dieses Werks zusammenarbeiten, um eine Konsenserklärung zu verfassen. Ziel ist es, einen Kommentar zu schreiben, der frei von persönlichen (ad hominem) Angriffen ist und ein gemeinsames Verständnis beschreibt oder zumindest aufzeigt, wo beide Parteien übereinstimmen und wo nicht. Auf diese Weise wird ein klares Fazit vermittelt und ein zukünftiger Weg aufgezeigt, anstatt die Lesenden zwischen gegensätzlichen Ansichten entscheiden zu lassen, die in separaten Kommentaren dargelegt werden.",
                "related_terms": [
                    "Adversarial collaboration",
                    "Collaborative commentary"
                ],
                "references": "Heyman, T., Moors, P., & Rabagliati, H. (2020). The benefits of adversarial collaboration for commentaries. Nature Human Behavior, 4, 1217. https://doi.org/10.1038/s41562-020-00978-6\n\nRabagliati, H., Moors, P., & Heyman, T. (2019). Can item effects explain away the evidence for unconscious sound symbolism? An adversarial commentary on Heyman, Maerten, Vankrunkelsven, Voorspoels, and Moors (2019). Psychological Science, 31(9), 1200–1204. https://doi.org/10.1177/0956797620949461\n\nSilberzahn, R., Simonsohn, U., & Ulhmann, E. L. (2014). Matched-names analysis reveals no evidence of name-meaning effects: A collaborative commentary on Silberzahn and Uhlmann (2013). Psychological Science, 25(7), 1504–1505. https://doi.org/10.1177/0956797614533802",
                "drafted_by": [
                    "Steven Verheyen"
                ],
                "reviewed_by": [
                    "Mahmoud Elsherif",
                    "Sam Parsons",
                    "Emma Henderson",
                    "Michele C. Lim",
                    "Flávio Azevedo"
                ],
                "alt_related_terms": [
                    null
                ],
                "language": "german"
            },
            {
                "type": "glossary",
                "title": "Affiliations-Verzerrung (Affiliation bias)",
                "definition": "Diese Art Verzerrung (bias) tritt auf, wenn Meinungen über die Qualität einer Arbeit oder deren Bewertung durch die Affiliation der Autor:innen beeinflusst werden. Ein mögliches Beispiel für Affiliations-Verzerrung bei der Publikation von Manuskripten wäre, wenn Herausgeber:innen beim Publizieren die Arbeiten von Autor:innen besonders angesehener Institutionen bevorzugen würden.",
                "related_terms": [
                    "Peer review"
                ],
                "references": "Tvina, A., Spellecy, R., & Palatnik, A. (2019). Bias in the peer review process: can we do better? Obstetrics & Gynecology, 133(6), 1081–1083. https://doi.org/10.1097/AOG.0000000000003260",
                "drafted_by": [
                    "Mahmoud Elsherif"
                ],
                "reviewed_by": [
                    "Christopher Graham",
                    "Madeleine Ingham",
                    "Adam Parker",
                    "Graham Reid"
                ],
                "alt_related_terms": [
                    null
                ],
                "language": "german"
            },
            {
                "type": "glossary",
                "title": "Aleatorische Unsicherheit (Aleatoric uncertainty)",
                "definition": "Die Variabilität von Ergebnissen aufgrund von unbekannten oder inhärent zufälligen Faktoren. Die stochastische Komponente der Ergebnisunsicherheit, die nicht durch zusätzliche Informationsquellen reduziert werden kann. Ein Beispiel: Beim Werfen einer Münze ist ungewiss, ob sie auf Kopf oder Zahl fällt.",
                "related_terms": [
                    "Epistemic uncertainty",
                    "Knightian uncertainty"
                ],
                "references": "Der Kiureghian, A., & Ditlevsen, O. (2009). Aleatory or epistemic? Does it matter? Structural Safety, 31(2), 105–112. https://doi.org/10.1016/j.strusafe.2008.06.020",
                "drafted_by": [
                    "Bradley Baker"
                ],
                "reviewed_by": [
                    "Nihan Albayrak-Aydemir**;** Brett Gall",
                    "Magdalena Grose-Hodge",
                    "Bethan Iley",
                    "Charlotte R. Pennington"
                ],
                "alt_related_terms": [
                    null
                ],
                "language": "german"
            },
            {
                "type": "glossary",
                "title": "Altmetrics",
                "definition": "Ausgehend von traditionellen Zitationsmetriken bieten Altmetrics (kurz für den englischen Ausdruck \"alternative metrics\") eine Bewertung der Aufmerksamkeit und der breiteren Bedeutung von Forschungsarbeiten auf der Grundlage verschiedener Quellen. Diese Quellen können soziale Medien umfassen (z. B. Twitter/X), digitale Nachrichtenkanäle, die Anzahl der Downloads von Preprints usw. Altmetrics sind dafür kritisiert worden, dass sensationelle Behauptungen in der Regel mehr Aufmerksamkeit erhalten als seriöse Forschung (Ali, 2021).",
                "related_terms": [
                    "Academic impact",
                    "Alternative metrics",
                    "Bibliometrics",
                    "H-index",
                    "Impact assessment",
                    "Journal impact factor"
                ],
                "references": "Ali, M. J. (2021). Understanding the Altmetrics. Seminars in Ophthalmology. https://doi.org/10.1080/08820538.2021.1930806\n\nGalligan, F., & Dyas-Correia, S. (2013). Altmetrics: rethinking the way we measure. Serials Review, 39(1), 56–61. https://doi.org/10.1016/j.serrev.2013.01.003",
                "drafted_by": [
                    "Mirela Zaneva"
                ],
                "reviewed_by": [
                    "Ali H. Al-Hoorie",
                    "Charlotte R. Pennington",
                    "Birgit Schmidt",
                    "Flávio Azevedo"
                ],
                "alt_related_terms": [
                    null
                ],
                "language": "german"
            },
            {
                "type": "glossary",
                "title": "AMNESIA",
                "definition": "AMNESIA ist ein kostenloses Anonymisierungswerkzeug zum Entfernen von persönlich identifizierenden Informationen aus Daten. Nach dem Hochladen eines Datensatzes, der personenbezogene Daten enthält, wird der ursprüngliche Datensatz mithilfe von AMNESIA umgewandelt. Dies führt zu einem Datensatz, der hinsichtlich personenbezogener und sensibler Daten anonymisiert ist.",
                "related_terms": [
                    "Anonymity",
                    "Confidentiality",
                    "Research ethics"
                ],
                "references": "",
                "drafted_by": [
                    "Norbert Vanek"
                ],
                "reviewed_by": [
                    "Ali H. Al-Hoorie",
                    "Myriam A. Baum",
                    "Charlotte R. Pennington"
                ],
                "alt_related_terms": [
                    null
                ],
                "language": "german"
            },
            {
                "type": "glossary",
                "title": "Analytische Flexibilität (Analytic Flexibility)",
                "definition": "Analytische Flexibilität beschreibt eine Art der Freiheitsgrade von Forschenden (Simmons, Nelson, & Simonsohn, 2011), die sich speziell auf die große Anzahl von Entscheidungen bezieht, die während der Datenvorverarbeitung und Datenauswertung getroffen werden. “\\[T\\]he range of analysis outcomes across different acceptable analysis methods” (dt. \\[D\\]ie Bandbreite an Analyseergebnissen über verschiedene akzeptable Analysemethoden; Carp, 2012, p. 1). Analytische Flexibilität kann problematisch sein, da diese Variabilität bei Datenanalyse-Strategien zu einer Variabilität der Forschungsergebnisse führen kann, insbesondere wenn mehrere Strategien angewandt, aber nicht transparent veröffentlicht werden (Masur, 2021).",
                "related_terms": [
                    "Garden of forking paths",
                    "Multiverse analysis",
                    "Researcher degrees of freedom"
                ],
                "references": "Breznau, N. (2021). I saw you in the crowd: Credibility, reproducibility, and meta-utility. PS: Political Science & Politics, 54(2), 309–313. https://doi.org/10.1017/S1049096520000980\n\nJones, A., Dr, J., Duckworth, & Christiansen, P. (2020). May I have your attention, please? Methodological and Analytical Flexibility in the Addiction Stroop. https://doi.org/10.31234/osf.io/ws8xp\n\nMasur, P. K. (2020). Understanding the Effects of Analytical Choices on Finding the Privacy Paradox: A Specification Curve Analysis of Large-Scale Survey Data. Preprint. https://osf.io/m72gb/\n\nSimmons, J. P., Nelson, L. D., & Simonsohn, U. (2011). False-positive psychology: Undisclosed flexibility in data collection and analysis allows presenting anything as significant. Psychological Science, 22(11), 1359–1366. https://doi.org/10.1177/0956797611417632",
                "drafted_by": [
                    "Mariella Paul"
                ],
                "reviewed_by": [
                    "Adrien Fillon",
                    "Bettina M. J . Kern",
                    "Adam Parker",
                    "Charlotte R. Pennington",
                    "Flávio Azevedo"
                ],
                "alt_related_terms": [
                    null
                ],
                "language": "german"
            },
            {
                "type": "glossary",
                "title": "Anonymität (Anonymity)",
                "definition": "Das Anonymisieren von Daten meint das Entfernen, Generalisieren, Zusammenfassen oder Verzerren von jeglicher Information, die möglicherweise Versuchspersonen, Gutachter:innen, Autor:innen oder andere Personen identifizieren kann. Daten sollten so anonymisiert sein, dass Versuchspersonen nicht identifizierbar sind. Die grundlegendste Art zu anonymisieren ist es, die Namen der Versuchspersonen mit Pseudonymen (künstlichen Namen) zu ersetzen und Bezüge zu konkreten Orten zu entfernen. Anonymität ist besonders wichtig für offene Daten (Datenveröffentlichungen) und Daten können möglicherweise nicht veröffentlicht werden, sollte es Bedenken bezüglich ihrer Anonymität geben. Anonymität und offene Daten wurden sowohl für qualitative Forschung diskutiert, die oft persönliche Erfahrungen und Meinungen untersucht, als auch für quantitative Forschung mit Versuchspersonen aus klinischen Stichproben.",
                "related_terms": [
                    "Anonymising",
                    "Clinical populations",
                    "Confidentiality",
                    "Research ethics",
                    "Research participants",
                    "Vulnerable population"
                ],
                "references": "Braun, V., & Clarke, V. (2013). Successful Qualitative Research. SAGE Publications.",
                "drafted_by": [
                    "Claire Melia"
                ],
                "reviewed_by": [
                    "Tsvetomira Dumbalska",
                    "Bethan Iley",
                    "Tamara Kalandadze",
                    "Bettina M.J. Kern",
                    "Sam Parsons",
                    "Charlotte R. Pennington",
                    "Flávio Azevedo",
                    "Madeleine Pownall",
                    "Birgit Schmidt"
                ],
                "alt_related_terms": [
                    null
                ],
                "language": "german"
            },
            {
                "type": "glossary",
                "title": "ARRIVE Leitlinien (ARRIVE Guidelines)",
                "definition": "Die ARRIVE-Leitlinien bzw. \\-Guidelines (Animal Research: Reporting of In Vivo Experiments) sind ein auf Checklisten basierender Leitfaden für die Berichterstattung, der entwickelt wurde, um Veröffentlichungsstandards zu verbessern und die Reproduzierbarkeit in der Forschung an lebenden Tieren (d. h. in vivo) zu erhöhen. Die zweite Generation der ARRIVE-Leitlinien, ARRIVE 2.0, wurde im Jahr 2020 veröffentlicht. In diesen neuen Leitlinien wurde die Verständlichkeit verbessert, die Punkte wurden nach Prioritäten sortiert, und es wurden neue Informationen mit einem begleitenden \"Erklärungs-\" (Explanation)  und \"Elaborations-\" (Elaboration) Dokument hinzugefügt, um eine Begründung für jeden einzelnen Punkt zu liefern, sowie ein Vorschlag, um den Kontext der beschriebenen Studie ergänzen.",
                "related_terms": [
                    "PREPARE Guidelines",
                    "Reporting Guideline",
                    "STRANGE"
                ],
                "references": "",
                "drafted_by": [
                    "Ben Farrar"
                ],
                "reviewed_by": [
                    "Jamie P. Cockcroft",
                    "Mahmoud Elsherif",
                    "Elias Garcia-Pelegrin",
                    "Helena Hartmann",
                    "Wanyin Li",
                    "Charlotte R. Pennington"
                ],
                "alt_related_terms": [
                    null
                ],
                "language": "german"
            },
            {
                "type": "glossary",
                "title": "Artikelverarbeitungsgebühr (Article Processing Charge (APC))",
                "definition": "Eine Artikel- oder Autor:innen-Verarbeitungsgebühr (article processing charge, APC) ist eine Gebühr, die ein Verlag von Autor:innen für die Veröffentlichung und Bereitstellung eines frei verfügbaren (open access) Artikels verlangt. APCs sind oft dazu gedacht, mögliche Einnahmeverluste auszugleichen, die der Zeitschrift entstehen, wenn sie von traditionellen Publikationsmodellen wie Abonnements oder Pay-per-View (das Bezahlen einzelner Artikel durch Lesende) auf Open Access umstellt. Während einige Zeitschriften nur etwa 300 US-Dollar fordern, variieren die APCs stark und reichen von 1000 US-Dollar (Advances in Methods and Practice in Psychological Science; checked Dec 2023\\) oder weniger bis zu über 10.000 US-Dollar (Nature; checked Dec 2023). Während einige Verlage Forschenden aus bestimmten Weltregionen oder mit geringen finanziellen Mitteln eine Gebührenbefreiung gewähren, wurden einige APCs dafür kritisiert, dass sie im Vergleich zu den tatsächlichen Bearbeitungs- und Bereitstellungs-Kosten unverhältnismäßig hoch sind (Grossmann & Brembs, 2021). Damit schaffen sie möglicherweise Ungleichheiten in Bezug darauf, welche Forschenden es sich leisten können, ihre Werke offen zugänglich zu machen (Smith et al. 2020).",
                "related_terms": [
                    "Open Access",
                    "Under-representation"
                ],
                "references": "Grossmann, A., & Brembs, B. (2021). Current market rates for scholarly publishing services. F1000Research, 10(20), 20. https://doi.org/10.12688/f1000research.27468.1\n\nSmith, A. C., Merz, L., Borden, J. B., Gulick, C., Kshirsagar, A. R., & Bruna, E. M. (2020). Assessing the effect of article processing charges on the geographic diversity of authors using Elsevier’s ‘Mirror Journal’ system. https://doi.org/10.31222/osf.io/s7cx4",
                "drafted_by": [
                    "Nick Ballou"
                ],
                "reviewed_by": [
                    "Ali H. Al-Hoorie",
                    "Bethan Iley",
                    "Flávio Azevedo",
                    "Robert Ross",
                    "Tobias Wingen \\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_"
                ],
                "alt_related_terms": [
                    null
                ],
                "language": "german"
            },
            {
                "type": "glossary",
                "title": "Autor:innenschaft (Authorship)",
                "definition": "Mit der Autor:innenschaft (Authorship) werden Forschungsergebnisse (z. B. Manuskripte, Daten und Software) anerkannt und die Verantwortlichkeit für den Inhalt geregelt (McNutt et al. 2018; Patience et al. 2019). Die Konventionen unterscheiden sich zwischen verschiedenen Disziplinen, Kulturen und sogar Forschungsgruppen. Es gibt unterschiedliche Ansichten welche Beiträge zur Forschung Autor:innenschaft verdienen, was die Reihenfolge von Autor:innen bedeutet (wenn überhaupt etwas), wie viel Verantwortung für die Forschung der/die Korrespondenzautor:in (Corresponding Author) übernimmt und inwieweit die Autor:innen für Aspekte der Arbeit verantwortlich sind, die sie selbst nicht persönlich durchgeführt haben.",
                "related_terms": [
                    "Co-authorship",
                    "Consortium authorship",
                    "Contributorship",
                    "CRediT",
                    "First-last-author-emphasis norm (FLAE)",
                    "Gift (or Guest) Authorship",
                    "Sequence-determines-credit approach (SDC)"
                ],
                "references": "Academies, A. A. E. (2017). The European Code of Conduct for Research Integrity. Revised Edition. Retrieved from https://allea.org/code-of-conduct/\n\nGerman Research Foundation. (2019). Guidelines for Safeguarding Good Research Practice. Code of Conduct.\n\nMcNutt, M. K., Bradford, M., Drazen, J. M., Hanson, B., Howard, B., Jamieson, K. H., Kiermer, V., Marcus, E., Pope, B. K., Schekman, R., Swaminathan, S., Stang, P. J., & Verma, I. M. (2018). Transparency in authors’ contributions and responsibilities to promote integrity in scientific publication. Proceedings of the National Academy of Sciences of the United States of America, 115(11), 2557–2560. https://doi.org/10.1073/pnas.1715374115\n\nPatience, G. S., Galli, F., Patience, P. A., & Boffito, D. C. (2019). Intellectual contributions meriting authorship: Survey results from the top cited authors across all science categories. PLoS One, 14(1), e0198117. https://doi.org/10.1371/journal.pone.0198117",
                "drafted_by": [
                    "Jacob Miranda"
                ],
                "reviewed_by": [
                    "Bradley Baker",
                    "Brett J. Gall",
                    "Matt Jaquiery",
                    "Charlotte R. Pennington",
                    "Flávio Azevedo",
                    "Birgit Schmidt",
                    "Yuki Yamada"
                ],
                "alt_related_terms": [
                    null
                ],
                "language": "german"
            },
            {
                "type": "glossary",
                "title": "Hilfshypothese (Auxiliary Hypothesis)",
                "definition": "Alle Theorien enthalten Annahmen über die Art von Konstrukten und wie sie gemessen werden können. Allerdings werden nicht alle Vorhersagen aus Theorien abgeleitet, und Annahmen können manchmal aus anderen Prämissen abgeleitet werden. Zusätzliche Annahmen, die getroffen werden, um eine Vorhersage abzuleiten, werden durch Verknüpfung mit beobachtbaren Daten geprüft. Diese Hilfshypothesen werden manchmal herangezogen, um zu erklären, warum ein Replikationsversuch fehlgeschlagen ist.",
                "related_terms": [
                    "Epistemic uncertainty",
                    "Hypothesis",
                    "Statistical assumptions",
                    "Hidden moderators"
                ],
                "references": "Dienes, Z. (2008). Understanding psychology as a science: An introduction to scientific and statistical inference. Macmillan International Higher Education.\n\nLakatos, I. (1978). The Methodology of Scientific Research Programs: Vol. I. Cambridge University Press.",
                "drafted_by": [
                    "Alaa Aldoh"
                ],
                "reviewed_by": [
                    "Ali H. Al-Hoorie",
                    "Nihan Albayrak-Aydemir",
                    "Mahmoud Elsherif",
                    "Bethan Iley",
                    "Sam Parsons",
                    "Flávio Azevedo"
                ],
                "alt_related_terms": [
                    null
                ],
                "language": "german"
            },
            {
                "type": "glossary",
                "title": "Open Science Open Science Abzeichen (Badges (Open Science))",
                "definition": "Anzeichen (badges) sind Symbole, die Herausgeber:innen veröffentlichten Manuskripten hinzufügen, um Open-Science-Praktiken anzuerkennen und die als Anreiz für Forschende dienen, Daten und Materialien zu teilen oder ihre Studien zu präregistrieren. Als deutlich sichtbare Abzeichen sollen sie den Lesenden signalisieren, dass der Inhalt die Open Science-Anforderungen (in der Regel von dieser Zeitschrift) erfüllt, die für die Verleihung des Abzeichens erforderlich sind. Für unterschiedliche Open Science-Praktiken können verschiedene Abzeichen vergeben werden, z. B. für die Bereitstellung und Zugänglichkeit von Forschungsmaterialien und \\-ergebnissen an einem permanenten Ort (\"Open Material Badge\" und \"Open Data Badge\") oder für die Präregistrierung von Studien (\"Preregistration Badge\").",
                "related_terms": [
                    "Incentives",
                    "Open Data badge",
                    "Preregistration",
                    "Triple badge"
                ],
                "references": "Hardwicke, T. E., Bohn, M., MacDonald, K., Hembacher, E., Nuijten, M. B., Peloquin, B. N., & others. (2020). Analytic reproducibility in articles receiving open data badges at the journal Psychological Science: an observational study. Royal Society Open Science, 8(1), 201494. https://doi.org/10.1098/rsos.201494\n\nKidwell, M. C., Lazarević, L. B., Baranski, E., Hardwicke, T. E., Piechowski, S., Falkenberg, L. S., & Nosek, B. A. (2016). Badges to acknowledge open practices: A simple, low-cost, effective method for increasing transparency. PLoS Biology, 14(5), e1002456. https://doi.org/10.1371/journal.pbio.1002456",
                "drafted_by": [
                    "Jacob Miranda"
                ],
                "reviewed_by": [
                    "Brett Gall",
                    "Helena Hartmann",
                    "Mariella Paul",
                    "Charlotte R. Pennington",
                    "Lisa Spitzer",
                    "Suzanne L. K. Stewart"
                ],
                "alt_related_terms": [
                    null
                ],
                "language": "german"
            },
            {
                "type": "glossary",
                "title": "Bayes-Faktor (Bayes Factor)",
                "definition": "Ein kontinuierliches statistisches Maß für die Modellauswahl bei der Bayes'schen Inferenz. Es beschreibt die *relative* Evidenz für ein Modell im Vergleich zu einem anderen, unabhängig davon, ob die Modelle korrekt sind. Bayes-Faktoren (BF) reichen von 0 bis unendlich und geben die relative Stärke der Evidenz an, wobei 1 ein neutraler Punkt ohne jegliche Evidenz ist. Im Gegensatz zu p-Werten lassen Bayes-Faktoren drei Arten von Schlussfolgerungen zu: a) Belege für die Alternativhypothese, b) Belege für die Nullhypothese und c) keine ausreichenden Belege für beide Hypothesen. Daher werden BFs in der Regel als BF10 bei Evidenz für die Alternativhypothese im Vergleich zur Nullhypothese und als BF01 bei Evidenz für die Nullhypothese im Vergleich zur Alternativhypothese ausgedrückt.",
                "related_terms": [
                    "Bayesian inference",
                    "Bayesian statistics",
                    "Likelihood function",
                    "Null Hypothesis Significance Testing (NHST)",
                    "*p*\\-value"
                ],
                "references": "Hoijtink, H., Mulder, J., van Lissa, C., & Gu, X. (2019). A tutorial on testing hypotheses using the Bayes factor. Psychological Methods, 24(5), 539–556. https://doi.org/10.1037/met0000201\n\nMakowski, D., Ben-Shachar, M. S., Chen, S. H. A., & Lüdecke, D. (2019). Indices of Effect Existence and Significance in the Bayesian Framework. https://doi.org/10.3389/fpsyg.2019.02767",
                "drafted_by": [
                    "Meng Liu"
                ],
                "reviewed_by": [
                    "Alaa AlDoh",
                    "Helena Hartmann",
                    "Connor Keating",
                    "Kai Krautter",
                    "Michele C. Lim",
                    "Suzanne L. K. Stewart",
                    "Ana Todorovic"
                ],
                "alt_related_terms": [
                    null
                ],
                "language": "german"
            },
            {
                "type": "glossary",
                "title": "Bayessche Inferenz (Bayesian Inference)",
                "definition": "Eine Methode der statistischen Inferenz oder Schlussfolgerung auf der Grundlage des Bayes'schen Theorems, welche die wissenschaftstheoretische (Un-)Sicherheit unter Zuhilfenahme der Wahrscheinlichkeit nutzt. Die Bayes'sche Schlussfolgerung basiert auf der Zuweisung (und Neuzuweisung, basierend auf neu beobachteten Daten oder gesammelter Evidenz) der Glaubhaftigkeit zu verschiedenen Möglichkeiten. Zwei bestehende Ansätze für Bayes'sche Schlussfolgerungen sind die \"Bayes-Faktoren\" (BF) und die Bayes'sche Parameterschätzung.",
                "related_terms": [
                    "Bayes Factor",
                    "Bayesian statistics",
                    "Bayesian Parameter Estimation"
                ],
                "references": "Dienes, Z. (2011). Bayesian versus orthodox statistics: Which side are you on? Perspectives on Psychological Science, 6(3), 274–290. https://doi.org/10.1177/1745691611406920\n\nDienes, Z. (2014). Using Bayes to get the most out of non-significant results. Frontiers in Psychology, 5, 781. https://doi.org/10.3389/fpsyg.2014.00781\n\nDienes, Z. (2016). How Bayes factors change scientific practice. Journal of Mathematical Psychology, 72, 78–89. https://doi.org/10.1016/j.jmp.2015.10.003\n\nEtz, A., Gronau, Q. F., Dablander, F., & others. (2018). How to become a Bayesian in eight easy steps: An annotated reading list. Psychonomic Bulletin & Review, 25, 219–234. https://doi.org/10.3758/s13423-017-1317-5\n\nKruschke, J. K. (2015). Doing Bayesian data analysis: A tutorial with R, JAGS, and Stan (2nd ed.). Academic Press.\n\nMcElreath, R. (2020). Statistical rethinking: A Bayesian course with examples in R and Stan (2nd ed.). Taylor.\n\nWagenmakers, E.-J., Marsman, M., Jamil, T., Ly, A., Verhagen, J., Love, J., Selker, R., Gronau, Q. F., Šmíra, M., Epskamp, S., Matzke, D., Rouder, J. N., & Morey, R. D. (2018). Bayesian inference for psychology. Part I: Theoretical advantages and practical ramifications. Psychonomic Bulletin & Review, 25(1), 35–57. https://doi.org/10.3758/s13423-017-1343-3",
                "drafted_by": [
                    "Charlotte R. Pennington"
                ],
                "reviewed_by": [
                    "Alaa AlDoh",
                    "Bradley Baker",
                    "Robert Ross",
                    "Markus Weinmann",
                    "Tobias Wingen",
                    "Steven Verheyen"
                ],
                "alt_related_terms": [
                    null
                ],
                "language": "german"
            },
            {
                "type": "glossary",
                "title": "Bayes’sche Parameter-Schätzung (Bayesian Parameter Estimation)",
                "definition": "Ein Bayes’scher Ansatz zur Schätzung von Parameterwerten (Bayesian Parameter Estimation), bei dem eine vorherige Annahme über Modellparameter (d. h. die vorherige \"Prior\" Verteilung) mit neuen Belegen (d. h. beobachteten Daten) über eine Wahrscheinlichkeitsfunktion aktualisiert wird. Dies führt zu einer posterioren Verteilung. Die resultierende (\"Posterior\") Verteilung kann auf verschiedene Weise zusammengefasst werden: Punktschätzungen (Mittelwert/Modus/Median einer posterioren Wahrscheinlichkeitsverteilung), Intervalle mit definierten Grenzen und Intervalle mit definierter Masse (typischerweise als credible interval, d. h. glaubwürdiges Intervall bezeichnet). Eine Posterior-Verteilung kann wiederum in einer nachfolgenden Schätzung zu einer neuen Prior-Verteilung zusammengefasst werden. Eine Posterior-Verteilung kann auch mit Hilfe von Monte-Carlo-Markov-Ketten-Methoden untersucht werden, die zur Bestimmung komplexer Modellunsicherheiten verwendet werden können (z. B. Foreman-Mackey et al., 2013).",
                "related_terms": [
                    "Bayes Factor",
                    "Bayesian inference",
                    "Bayesian statistics",
                    "Null Hypothesis Significance Testing (NHST)"
                ],
                "references": "Foreman-Mackey, D., Hogg, D. W., Lang, D., & Goodman, J. (2013). emcee: The MCMC Hammer. Publications of the Astronomical Society of the Pacific, 125(925), 306–312. https://doi.org/10.1086/670067\n\nMcElreath, R. (2020). Statistical rethinking: A Bayesian course with examples in R and Stan (2nd ed.). Taylor.\n\nPress, W. (2007). Numerical recipes: the art of scientific computing, 3rd edition.\n\nHuber, C. (2016). Introduction to Bayesian statistics, part 2: MCMC and the Metropolis–Hastings algorithm. In The Stata Blog. https://blog.stata.com/2016/11/15/introduction-to-bayesian-statistics-part-2-mcmc-and-the-metropolis-hastings-algorithm/",
                "drafted_by": [
                    "Alaa AlDoh"
                ],
                "reviewed_by": [
                    "Mahmoud Elsherif",
                    "Helena Hartmann",
                    "Dominik Kiersz",
                    "Meng Liu",
                    "Ana Todorovic",
                    "Markus Weinmann"
                ],
                "alt_related_terms": [
                    null
                ],
                "language": "german"
            },
            {
                "type": "glossary",
                "title": "BIDS Datenstruktur (BIDS data structure)",
                "definition": "Die Brain Imaging Data Structure (BIDS) beschreibt eine einfache und leicht zu implementierende Methode zur Organisation von neuronalen, elektrophysiologischen und Verhaltensdaten (d. h. Dateiformate, Ordnerstrukturen). BIDS ist ein Gemeinschaftsprojekt, das *von* der Gemeinschaft *für* die Gemeinschaft entwickelt wurde. Es wurde von dem Format inspiriert, das intern im OpenfMRI-Repositorium (auch bekannt als [OpenNeuro](https://openneuro.org)) verwendet wird. Obwohl ursprünglich für fMRT-Daten entwickelt, wurde die BIDS-Datenstruktur bereits für viele andere Maße, wie z. B. EEG, erweitert (Pernet et al., 2019).",
                "related_terms": [
                    "Open Data"
                ],
                "references": "Gorgolewski, K., Auer, T., Calhoun, V., & others. (2016). The brain imaging data structure, a format for organizing and describing outputs of neuroimaging experiments. Scientific Data, 3, 160044. https://doi.org/10.1038/sdata.2016.44\n\nBIDS. (2020). About BIDS. Retrieved from https://bids.neuroimaging.io",
                "drafted_by": [
                    "Tina Lonsdorf"
                ],
                "reviewed_by": [
                    "Ali H. Al-Hoorie",
                    "David Moreau",
                    "Mariella Paul",
                    "Charlotte R. Pennington"
                ],
                "alt_related_terms": [
                    null
                ],
                "language": "german"
            },
            {
                "type": "glossary",
                "title": "BIZARRE",
                "definition": "Dieses Akronym steht für “Barren, Institutional, Zoo, and other Rare Rearing Environments” (BIZARRE, dt.: karg, institutionell, Zoo und andere ungewöhnliche Umgebungen der Aufzucht). Die meisten Forschungsarbeiten über Schimpansen werden an dieser speziellen Stichprobe durchgeführt. Dies schränkt die Generalisierbarkeit vieler Forschungsergebnisse an der Schimpansenpopulation ein. Es wurde diskutiert, ob BIZARRE das universelle Konzept des Schimpansen widerspiegelt (siehe auch WEIRD, wo ebenfalls diskutiert wurde, ob dieses als universelles Konzept für den Menschen gilt).",
                "related_terms": [
                    "Populations",
                    "STRANGE",
                    "WEIRD"
                ],
                "references": "Clark, H., Elsherif, M. M., & Leavens, D. A. (2019). Ontogeny vs. phylogeny in primate/canid comparisons: a meta-analysis of the object choice task. Neuroscience & Biobehavioral Reviews, 105, 178–189. https://doi.org/10.1016/j.neubiorev.2019.06.001\n\nLeavens, D. A., Bard, K. A., & Hopkins, W. D. (2010). BIZARRE chimpanzees do not represent “the chimpanzee.” Behavioral and Brain Sciences, 33(2–3), 100–101. https://doi.org/10.1017/S0140525X10000166",
                "drafted_by": [
                    "Mahmoud Elsherif"
                ],
                "reviewed_by": [
                    "Zoe Flack",
                    "Charlotte R. Pennington"
                ],
                "alt_related_terms": [
                    null
                ],
                "language": "german"
            },
            {
                "type": "glossary",
                "title": "to Open Scholarship Bottom-Up Ansatz (Bottom-up approach (to Open Scholarship))",
                "definition": "In der akademischen Welt ein Ansatz, der sich auf das intrinsische Interesse von Akademiker:innen konzentriert, die Qualität der Forschung und der Forschungskultur zu verbessern, z. B. indem man sie unterstützend, kooperativ, kreativ und inklusiv gestaltet. In der Regel geht die Führung von Nachwuchswissenschaftler:innen aus, die durch Enthusiasmus und Innovation Veränderungen in wissenschaftlicher Methodik vorantreiben, im Gegensatz zu einem \"Top-down\"-Ansatz, der von erfahrenen Forscher:innen initiiert wird. \"Bottom-up approaches take into account the specific local circumstances of the case itself, often using empirical data, lived experience, personal accounts, and circumstances as the starting point for developing policy solutions.\" (dt. Bottom-up-Ansätze berücksichtigen die spezifischen lokalen Umstände des jeweiligen Falles und verwenden oft empirische Daten, direkte Erfahrungen, persönliche Berichte und Umstände als Ausgangspunkt für die Entwicklung politischer Lösungen; Meslin, 2010, p. 208\\)",
                "related_terms": [
                    "Early Career Researchers (ECRs)",
                    "Grassroot initiatives"
                ],
                "references": "Button, K. S., Lawrence, N. S., Chambers, C. D., & Munafò, M. R. (2016). Instilling scientific rigour at the grassroots. Psychologist, 29(3), 158–159.\n\nButton, K. S., Chambers, C. D., Lawrence, N., & Munafò, M. R. (2020). Grassroots training for reproducible science: a consortium-based approach to the empirical dissertation. Psychology Learning & Teaching, 19(1), 77–90. https://doi.org/10.1177/1475725719857659\n\nHart, D. D., & Silka, L. (2020). Rebuilding the Ivory Tower: A Bottom-Up Experiment in Aligning Research with Societal Needs. Issues in Science and Technology, 79–85. Retrieved from https://issues.org/aligning-research-with-societal-needs/\n\nMeslin, E. M. (2009). Achieving global justice in health through global research ethics: supplementing Macklin’s ‘top-down’ approach with one from the ‘ground up’. In R. M. Green, A. Donovan, & S. A. Jauss (Eds.), Global Bioethics: Issues of Conscience for the Twenty-First Century (pp. 163–177). University Press.\n\nMoran, H., Karlin, L., Lauchlan, E., Rappaport, S. J., Bleasdale, B., Wild, L., & Dorr, J. (2020). Understanding Research Culture: What researchers think about the culture they work in. Wellcome Open Research, 5, 201. https://doi.org/10.12688/wellcomeopenres.15832.1\n\nNosek, B. A. (2019). Strategy for Culture Change. Center for Open Science. https://www.cos.io/blog/strategy-for-culture-change",
                "drafted_by": [
                    "Catherine Laverty"
                ],
                "reviewed_by": [
                    "Helena Hartmann",
                    "Michele C. Lim",
                    "Adam Parker",
                    "Charlotte R. Pennington",
                    "Birgit Schmidt",
                    "Marta Topor",
                    "Flávio Azevedo"
                ],
                "alt_related_terms": [
                    null
                ],
                "language": "german"
            },
            {
                "type": "glossary",
                "title": "Klammer-Interviews (Bracketing Interviews)",
                "definition": "Bracketing Interviews (Klammer-Interviews) werden üblicherweise im Rahmen qualitativer Ansätze verwendet. Während dieser Interviews erkunden Forschende ihre persönlichen Subjektivitäten und Annahmen im Zusammenhang mit ihrer laufenden Forschung. Dies ermöglicht es den Forschenden, sich ihrer eigenen Interessen bewusst zu werden, hilft ihnen, ihre Forschung zu reflektieren und kritisch zu betrachten, und zu überlegen, wie ihre eigenen Erfahrungen den Forschungsprozess beeinflussen können. Klammer-Interviews können auch einer qualitativen Analyse unterzogen werden.",
                "related_terms": [
                    "Qualitative research",
                    "Reflexivity",
                    "Researcher bias **Reference (s)**:  \\[@RollsRelf2006\\], \\[@Sorsa2015\\]"
                ],
                "references": "",
                "drafted_by": [
                    "Claire Melia"
                ],
                "reviewed_by": [
                    "Tamara Kalandadze",
                    "Charlotte R. Pennington",
                    "Graham Reid",
                    "Marta Topor"
                ],
                "alt_related_terms": [
                    null
                ],
                "language": "german"
            },
            {
                "type": "glossary",
                "title": "Bropenscience",
                "definition": "Ein ironisch gemeinter Ausdruck, der für den Mangel an diversen Stimmen in der Open Science Bewegung sensibilisieren soll (Bahlai, Bartlett, Burgio et al. 2019; Onie, 2020), sowie für das Vorhandensein von Verhaltensweisen und Kommunikationsarten, die toxisch oder ausgrenzend sein können. Wichtig ist, dass nicht alle “Bros” Männer sind; vielmehr handelt es sich um Personen, die ein starres Denken an den Tag legen, denen es an Selbsterkenntnis fehlt und die zu Feindseligkeit, Unfreundlichkeit und Ausgrenzung neigen (Pownall et al., 2021; Whitaker & Guest, 2020). Sie gehören im Allgemeinen zu dominanten Gruppen, die von strukturellen Privilegien profitieren. Um \\#bropenscience anzugehen, sollten Forschende strukturelle Ungleichheiten innerhalb akademischer Systeme und Institutionen untersuchen und angehen.",
                "related_terms": [
                    "Diversity",
                    "Inclusion",
                    "Intersectionality",
                    "Open Science **Reference (s)**: \\[@GuestTweet2017\\], \\[@Whitaker2020\\], \\[@Pownall20210\\]"
                ],
                "references": "",
                "drafted_by": [
                    "Zoe Flack"
                ],
                "reviewed_by": [
                    "Magdalena Grose-Hodge",
                    "Helena Hartmann",
                    "Bethan Iley",
                    "Tamara Kalandadze",
                    "Adam Parker",
                    "Charlotte R. Pennington",
                    "Flávio Azevedo",
                    "Bradley Baker",
                    "Mahmoud Elsherif"
                ],
                "alt_related_terms": [
                    null
                ],
                "language": "german"
            },
            {
                "type": "glossary",
                "title": "CARKing",
                "definition": "Critiquing After the Results are Known (CARKing; Kritik nach Wissen der Ergebnisse) bedeutet, eine Kritik an einem Studiendesign so zu formulieren, als hätte man sie vor dem Vorliegen der Ergebnisse geäußert. Dies ist in der Regel eine Reaktion auf oder Kritik bezüglich unerwünschter oder ungünstiger Ergebnisse, unabhängig davon, ob sich der/die Kritiker:in dieser Tatsache bewusst ist oder nicht.",
                "related_terms": [
                    "HARKing",
                    "Preregistration",
                    "Registered Report"
                ],
                "references": "Bardsley, N. (2018). What lessons does the “replication crisis” in psychology hold for experimental economics? In Handbook of Psychology and Economic Behaviour, 2nd edition. Cambridge University Press. Retrieved from http://centaur.reading.ac.uk/69874/\n\nNosek, B. A., & Lakens, D. (2014). Registered reports. Social Psychology, 45, 137–141. https://doi.org/10.1027/1864-9335/a000192",
                "drafted_by": [
                    "Mahmoud Elsherif"
                ],
                "reviewed_by": [
                    "Ali H. Al-Hoorie",
                    "Ashley Blake",
                    "Adrien Fillon",
                    "Charlotte R. Pennington"
                ],
                "alt_related_terms": [
                    null
                ],
                "language": "german"
            },
            {
                "type": "glossary",
                "title": "COS (Center for Open Science (COS))",
                "definition": "Eine gemeinnützige Organisation im Technologiesektor mit Sitz in Charlottesville, Virginia, die sich zum Ziel gesetzt hat, \"to increase openness, integrity, and reproducibility of research” (cos.io; dt.: die Offenheit, Integrität und Reproduzierbarkeit von Forschung zu verbessern). Neben anderen Ressourcen betreibt das COS das Open Science Framework (OSF) und die Open Scholarship Knowledge Base.",
                "related_terms": [
                    "Open Science badges",
                    "Open Science Framework",
                    "OSF collections",
                    "OSF institutions",
                    "OSF meetings",
                    "OSF preprints",
                    "OSF registries",
                    "Registrations (Preregistrations & Registered Reports)",
                    "Transparency and Openness Promotion Guidelines (TOP)"
                ],
                "references": "for Open Science, C. (n.d.). Show Your Work. Share Your Work. Advance Science. That’s Open Science. https://www.cos.io/",
                "drafted_by": [
                    "Beatrix Arendt"
                ],
                "reviewed_by": [
                    "Ali H. Al-Hoorie",
                    "Mariella Paul",
                    "Charlotte R. Pennington",
                    "Lisa Spitzer"
                ],
                "alt_related_terms": [
                    null
                ],
                "language": "german"
            },
            {
                "type": "glossary",
                "title": "Zitations-Verzerrung (Citation bias)",
                "definition": "Eine verzerrte (“biased”) Auswahl von Arbeiten oder Autor:innen, die im Text zitiert und in die Referenzen mit aufgenommen werden. Wenn eine Verzerrung von Zitationen vorliegt, geschieht dies häufig in einer Weise, die der/dem/den Autor:innen oder Gutachter:innen zugute kommt, statistisch signifikante Studien überrepräsentiert oder weitreichende geschlechts- oder herkunftsspezifische Vorurteile widerspiegelt (Brooks, 1985; Jannot et al., 2013; Zurn et al., 2020). Eine vorgeschlagene Lösung ist die Verwendung von Citation Diversity Statements (Zitations-Vielfalt-Angaben), in denen Autor:innen ihre Zitierpraxis reflektieren und Verzerrungen aufzeigen, die möglicherweise aufgetreten sind (Zurn et al., 2020).",
                "related_terms": [
                    "Citation diversity statement",
                    "Reporting bias"
                ],
                "references": "Brooks, T. A. (1985). Private acts and public objects: An investigation of citer motivations. Journal of the American Society for Information Science, 36(4), 223–229. https://doi.org/10.1002/asi.4630360402\n\nJannot, A. S., Agoritsas, T., Gayet-Ageron, A., & Perneger, T. V. (2013). Citation bias favoring statistically significant studies was present in medical research. Journal of Clinical Epidemiology, 66(3), 296–301. https://doi.org/10.1016/j.jclinepi.2012.09.015\n\nThombs, B. D., Levis, A. W., Razykov, I., Syamchandra, A., Leentjens, A. F., Levenson, J. L., & Lumley, M. A. (2015). Potentially coercive self-citation by peer reviewers: a cross-sectional study. Journal of Psychosomatic Research, 78(1), 1–6. https://doi.org/10.1016/j.jpsychores.2014.09.015\n\nZurn, P., Bassett, D. S., & Rust, N. C. (2020). The Citation Diversity Statement: A Practice of Transparency, A Way of Life. Trends in Cognitive Sciences, 24(9), 669–672. https://doi.org/10.1016/j.tics.2020.06.009",
                "drafted_by": [
                    "Bettina M. J. Kern"
                ],
                "reviewed_by": [
                    "Mahmoud Elsherif",
                    "Annalise A. LaPlume",
                    "Helena Hartmann",
                    "Bethan Iley",
                    "Charlotte R. Pennington",
                    "Timo Roettger",
                    "Tobias Wingen"
                ],
                "alt_related_terms": [
                    null
                ],
                "language": "german"
            },
            {
                "type": "glossary",
                "title": "Zitations-Diversitäts-Erklärung (Citation Diversity Statement)",
                "definition": "Die Zitations-Diversitäts-Erklärung (Citation Diversity Statement) ist eine aktuelle Bemühung, das Bewusstsein für Verzerrungen beim Zitieren in Bezug auf Geschlecht und Herkunft zu schärfen und diese abzuschwächen. Es ist ein kurzer Absatz, in dem \"the authors consider their own bias and quantify the equitability of their reference lists. It states: (i) the importance of citation diversity, (ii) the percentage breakdown (or other diversity indicators) of citations in the paper, (iii) the method by which percentages were assessed and its limitations, and (iv) a commitment to improving equitable practices in science” (Zurn et al., 2020, p. 669, dt. \\[indem\\] die Autor:innen ihre eigene Verzerrung betrachten und die Ausgewogenheit ihrer Quellenangaben quantifizieren. Dabei wird Folgendes angegeben: (i) die Bedeutung der Diversität von zitierten Quellen, (ii) die prozentuale Aufschlüsselung (oder andere Diversitätsindikatoren) der zitierten Quellen in der Arbeit, (iii) die Methode, mit der die Prozentsätze berechnet wurden, und ihre Grenzen, und (iv) der Einsatz zur Verbesserung gerechter Praktiken in der Wissenschaft).",
                "related_terms": [
                    "Citation bias",
                    "Diversity",
                    "Under-representation"
                ],
                "references": "Zurn, P., Bassett, D. S., & Rust, N. C. (2020). The Citation Diversity Statement: A Practice of Transparency, A Way of Life. Trends in Cognitive Sciences, 24(9), 669–672. https://doi.org/10.1016/j.tics.2020.06.009",
                "drafted_by": [
                    "Helena Hartmann"
                ],
                "reviewed_by": [
                    "Mahmoud Elsherif",
                    "Magdalena Grose-Hodge",
                    "Sam Parsons",
                    "Timo Roettger"
                ],
                "alt_related_terms": [
                    null
                ],
                "language": "german"
            },
            {
                "type": "glossary",
                "title": "Bürger:innenwissenschaft (Citizen Science)",
                "definition": "Citizen Science (Bürger:innenwissenschaft) bezieht sich auf Projekte, die die breite Öffentlichkeit aktiv in den wissenschaftlichen Prozess mit einbeziehen, mit dem Ziel, Wissenschaft zu demokratisieren. Bürgerwissenschaftler:innen können in alle Phasen der Forschung eingebunden werden und als Mitarbeiter:innen, Mitwirkende oder Projektleiter:innen fungieren. Ein Beispiel für ein großes Citizen Science Projekt war die Identifizierung von astronomischen Körpern durch Einzelpersonen (Lintott, 2008).",
                "related_terms": [
                    "Crowd science",
                    "Crowdsourcing **Alternative definition:** (if applicable) In the past, citizen science mostly referred to volunteers who participate as field assistants in scientific studies (Cohn, 2008, p. 193)."
                ],
                "references": "Cohn, J. P. (2008). Citizen science: Can volunteers do real research? BioScience, 58(3), 192–197. https://doi.org/10.1641/B580303\n\nLintott, C. J., Schawinski, K., Slosar, A., Land, K., Bamford, S., Thomas, D., & Vandenberg, J. (2008). Galaxy Zoo: morphologies derived from visual inspection of galaxies from the Sloan Digital Sky Survey. Monthly Notices of the Royal Astronomical Society, 389(3), 1179–1189. https://doi.org/10.1111/j.1365-2966.2008.13689.x",
                "drafted_by": [
                    "Mahmoud Elsherif; Ana Barbosa Mendes"
                ],
                "reviewed_by": [
                    "Gisela H. Govaart",
                    "Tamara Kalandadze",
                    "Dominik Kiersz",
                    "Charlotte R. Pennington",
                    "Robert M. Ross"
                ],
                "alt_related_terms": [
                    null
                ],
                "language": "german"
            },
            {
                "type": "glossary",
                "title": "CKAN",
                "definition": "Das Comprehensive Knowledge Archive Network (CKAN, dt. ) ist eine Open-Source-Datenplattform und freie Software, die darauf abzielt, Werkzeuge für optimierte Veröffentlichung und Datenaustausch zur Verfügung zu stellen. CKAN unterstützt Regierungen, Forschungseinrichtungen und andere Organisationen bei der Verwaltung und Veröffentlichung großer Datenmengen.",
                "related_terms": [
                    "Data platforms",
                    "Data sharing"
                ],
                "references": "Anon. (n.d.). Ckan. Retrieved from https://ckan.org/",
                "drafted_by": [
                    "Tsvetomira Dumbalska"
                ],
                "reviewed_by": [
                    "Ali H. Al-Hoorie",
                    "Kai Krautter",
                    "Charlotte R. Pennington"
                ],
                "alt_related_terms": [
                    null
                ],
                "language": "german"
            },
            {
                "type": "glossary",
                "title": "COAR Community Framework for Good Practices in Repositories",
                "definition": "Ein Leitfaden, der die besten Praktiken für wissenschaftliche Repositorien und Bewertungskriterien für diese Praktiken identifiziert. Sein flexibler und mehrdimensionaler Ansatz bedeutet, dass er auf verschiedene Typen von Repositorien angewendet werden kann, einschließlich solcher, die Publikationen oder Daten bereitstellen, über geografische und thematische Kontexte hinweg.",
                "related_terms": [
                    "Metadata",
                    "Open Access",
                    "Open Data",
                    "Open Material",
                    "Repository",
                    "TRUST principles"
                ],
                "references": "of Open Access Repositories, C. (2020). COAR Community Framework for Best Practices in Repositories (Version 1). Zenodo. https://doi.org/10.5281/zenodo.4110829",
                "drafted_by": [
                    "Aleksandra Lazić"
                ],
                "reviewed_by": [
                    "Ashley Blake",
                    "Jamie P. Cockcroft",
                    "Bethan Iley",
                    "Sam Parsons"
                ],
                "alt_related_terms": [
                    null
                ],
                "language": "german"
            },
            {
                "type": "glossary",
                "title": "Codebuch (Codebook)",
                "definition": "Ein Codebook (Codebuch) ist eine Zusammenfassung, die den Inhalt, die Struktur, die Art und das Layout eines Datensatzes beschreibt. Ein gut dokumentiertes Codebuch enthält Informationen, die für jede Variable in einer Datendatei vollständig selbsterklärend sind, z. B. den Wortlaut und die Kodierung des Items und das zugrunde liegende Konstrukt. Ein Codebook schafft Transparenz für Forschende, die mit den Daten nicht vertraut sind, aber Analysen reproduzieren oder die Daten wiederverwenden möchten.",
                "related_terms": [
                    "Data dictionary",
                    "Metadata"
                ],
                "references": "Arslan, R. C. (2019). How to Automatically Document Data With the codebook Package to Facilitate Data Reuse. Advances in Methods and Practices in Psychological Science, 2(2), 169–187. https://doi.org/10.1177/2515245919838783",
                "drafted_by": [
                    "Tina Lonsdorf"
                ],
                "reviewed_by": [
                    "Ali H. Al-Hoorie",
                    "Ashley Blake, Kai Krautter",
                    "Charlotte R. Pennington",
                    "Flávio Azevedo"
                ],
                "alt_related_terms": [
                    null
                ],
                "language": "german"
            },
            {
                "type": "glossary",
                "title": "Code-Überprüfung (Code review)",
                "definition": "Der Prozess der Überprüfung der Codes eines anderen Forschenden (insbesondere des Computer-Quellcodes), einschließlich (aber nicht beschränkt auf), statistischen Code und Datenmodellierung. Dieser Prozess dient dazu, Fehler zu erkennen und zu beheben und so die Codequalität zu verbessern. In der Praxis kann ein modernes Begutachtungsverfahren (Peer-Review) über ein gehostetes Online-Repository wie GitHub, GitLab oder SourceForge stattfinden.",
                "related_terms": [
                    "Reproducibility",
                    "Version control"
                ],
                "references": "Petre, M., & Wilson, G. (2014). Code review for and by scientists. arXiv Preprint arXiv:1407.5648. https://arxiv.org/abs/1407.5648\n\nScopatz, A. M., & Huff, K. D. (2015). Effective Computation in Physics: Field Guide to Research with Python (1st ed.). O’Reilly Media. http://shop.oreilly.com/product/0636920033424.do",
                "drafted_by": [
                    "Filip Dechterenko"
                ],
                "reviewed_by": [
                    "Ali H. Al-Hoorie",
                    "Dominik Kiersz",
                    "Sam Parsons",
                    "Charlotte R. Pennington"
                ],
                "alt_related_terms": [
                    null
                ],
                "language": "german"
            },
            {
                "type": "glossary",
                "title": "CREP (Collaborative Replication and Education Project (CREP))",
                "definition": "Das Collaborative Replication and Education Project (CREP; Kollaboratives Replikations- und Bildungssprojekt) ist eine Initiative, die darauf abzielt, die Replikationen viel zitierter empirischer Studien in der Psychologie zu organisieren und zu strukturieren. So soll der zweifache Bedarf nach mehr qualitativ hochwertigen direkten Replikationen und mehr Ausbildung in empirischen Forschungstechniken für Psychologiestudierende gedeckt werden. CREP zielt darauf ab, den Bedarf an Replikationen viel zitierter Studien zu decken und Wissenschaftler:innen, die Replikationsprojekte durchführen, Schulungen, Unterstützung und berufliche Entwicklungsmöglichkeiten zu bieten.",
                "related_terms": [
                    "Direct replication",
                    "Exact replication"
                ],
                "references": "Wagge, J. R., Baciu, C., Banas, K., Nadler, J. T., Schwarz, S., Weisberg, Y., & others. (2019). A demonstration of the collaborative replication and education project: Replication attempts of the red-romance effect. Collabra: Psychology, 5(1). https://doi.org/10.1525/collabra.177",
                "drafted_by": [
                    "Connor Keating"
                ],
                "reviewed_by": [
                    "Bradley Baker",
                    "Mahmoud Elsherif",
                    "Zoe Flack",
                    "Charlotte R. Pennington"
                ],
                "alt_related_terms": [
                    null
                ],
                "language": "german"
            },
            {
                "type": "glossary",
                "title": "COBIDAS (Committee on Best Practices in Data Analysis and Sharing (COBIDAS))",
                "definition": "Die Organization for Human Brain Mapping (OHBM; dt. Organisation für menschliche Hirnbildgebung) hat einen Leitfaden für optimale Verfahren (best practices) bei der Erhebung und Analyse von Bildgebungsdaten, der Veröffentlichung und der gemeinsamen Nutzung von Daten und Analysecodes entwickelt. Er enthält acht Elemente, die beim Verfassen oder Einreichen eines Manuskripts berücksichtigt werden sollten, um die Veröffentlichung der Methodik und die daraus resultierenden Bildgebungsdaten zu verbessern und so die Transparenz und Reproduzierbarkeit zu optimieren. **Alternative definition:** (if applicable) Checklist for data analysis and sharing",
                "related_terms": [],
                "references": "Nichols, T. E., Das, S., Eickhoff, S. B., Evans, A. C., Glatard, T., Hanke, M., & others. (2017). Best practices in data analysis and sharing in neuroimaging using MRI. Nature Neuroscience, 20(3), 299–303. https://doi.org/10.1038/nn.4500\n\nPernet, C., Garrido, M. I., Gramfort, A., Maurits, N., Michel, C. M., Pang, E., & others. (2020). Issues and recommendations from the OHBM COBIDAS MEEG committee for reproducible EEG and MEG research. Nature Neuroscience, 23(12), 1473–1483. https://doi.org/10.1038/s41593-020-00709-0",
                "drafted_by": [
                    "Yu-Fang Yang"
                ],
                "reviewed_by": [
                    "Jamie P. Cockcroft",
                    "Helena Hartmann",
                    "Adam Parker",
                    "Sam Parsons"
                ],
                "alt_related_terms": [
                    null
                ],
                "language": "german"
            },
            {
                "type": "glossary",
                "title": "Kommunalität (Communality)",
                "definition": "Das geteilte öffentliche Eigentum wissenschaftlicher Ergebnisse und Methoden und das daraus folgende Gebot, beides frei zu teilen. Die Kommunalität beruht auf der Tatsache, dass jede wissenschaftliche Erkenntnis das Ergebnis der Bemühungen einer Reihe von Akteuren ist. Um diese Norm zu befolgen, teilen Forschende ihre neuen Erkenntnisse offen mit Kolleg:innen.",
                "related_terms": [
                    "Mertonian norms",
                    "Objectivity **Alternative definition:** Communism (in Merton, 1942\\) **Related terms to alternative definition** (if applicable)"
                ],
                "references": "Anderson, M. S., Ronning, E. A., Devries, R., & Martinson, B. C. (2010). Extending the Mertonian norms: Scientists’ subscription to norms of research. Journal of Higher Education, 81(3), 366–393. https://doi.org/10.1353/jhe.0.0095\n\nHardwicke, T. E., Jameel, L., Jones, M., Walczak, E. J., & Weinberg, L. M. (2014). Only human: Scientists, systems, and suspect statistics. Opticon1826, 16, 25. https://doi.org/10.5334/OPT.CH\n\nMerton, R. K. (1938). Science and the social order. Philosophy of Science, 5(3), 321–337. https://doi.org/10.1086/286513\n\nMerton, R. K. (1942). A note on science and democracy. Journal of Legal and Political Sociology, 1, 115–126. https://doi.org/10.1515/9783110375008-013",
                "drafted_by": [
                    "David Moreau"
                ],
                "reviewed_by": [
                    "Ashley Blake",
                    "Mahmoud Elsherif",
                    "Charlotte R. Pennington",
                    "Beatrice Valentini"
                ],
                "alt_related_terms": [
                    null
                ],
                "language": "german"
            },
            {
                "type": "glossary",
                "title": "Gemeinschaftsprojekte (Community Projects)",
                "definition": "Kooperationsprojekte, an denen Forschende aus verschiedenen Karrierestufen, Disziplinen, Einrichtungen oder Ländern beteiligt sind. Die Projekte können unterschiedliche Ziele verfolgen, darunter gegenseitige Unterstützung und Lernen voneinander, Durchführen von Forschung, Lehre und Ausbildung. Sie können kurzfristig (z. B. Konferenzveranstaltungen oder Hackathons) oder langfristig (z. B. Journal Clubs oder von einem Konsortium geleitete Forschungsprojekte) sein. Eine Kultur der Zusammenarbeit und der Aufbau einer Gemeinschaft sind der Schlüssel zum Erreichen der Projektziele.",
                "related_terms": [
                    "Bottom-up approach (to Open Scholarship)",
                    "Crowdsourced research",
                    "Hackathon",
                    "Many Labs",
                    "ReproducibiliTea"
                ],
                "references": "Ellemers, N. (2021). Science as collaborative knowledge generation. British Journal of Social Psychology, 60(1), 1–28. https://doi.org/10.1111/bjso.12430\n\nOrben, A. (2019). A journal club to fix science. Nature, 573(7775), 465–466. https://doi.org/10.1038/d41586-019-02842-8\n\nShepard, B. (2015). Community projects as social activism. SAGE.",
                "drafted_by": [
                    "Marta Topor"
                ],
                "reviewed_by": [
                    "Ali H. Al-Hoorie",
                    "Jamie P. Cockcroft",
                    "Mahmoud Elsherif",
                    "Kai Krautter",
                    "Gerald Vineyard"
                ],
                "alt_related_terms": [
                    null
                ],
                "language": "german"
            },
            {
                "type": "glossary",
                "title": "Handbuch (Compendium)",
                "definition": "Eine Sammlung von Dateien, die von Forschenden zur Unterstützung eines Berichts oder einer Veröffentlichung erstellt wurden und die Daten, Metadaten, Programmiercodes, Softwareabhängigkeiten, Lizenzen und andere Anweisungen enthalten, die andere Forschende benötigen, um die in dem Bericht oder der Veröffentlichung dargestellten Ergebnisse unabhängig zu reproduzieren.",
                "related_terms": [
                    "Compendia",
                    "Replication",
                    "Reproducibility",
                    "Research compendium",
                    "**References:** \\[@Claerbout1992\\], \\[@Gentleman2005\\], \\[@Marwick2018\\], \\[@Nust2018\\]"
                ],
                "references": "",
                "drafted_by": [
                    "Ben Marwick"
                ],
                "reviewed_by": [
                    "Ali H. Al-Hoorie",
                    "Charlotte R. Pennington"
                ],
                "alt_related_terms": [
                    null
                ],
                "language": "german"
            },
            {
                "type": "glossary",
                "title": "rechnerische Reproduzierbarkeit (Computational reproducibility)",
                "definition": "Die Fähigkeit, dieselben Ergebnisse wie in der Originalstudie (einschließlich Tabellen, Abbildungen und quantitativer Ergebnisse) unter Verwendung derselben Eingangsdaten, Berechnungsmethoden und Analysebedingungen zu reproduzieren. Die Verfügbarkeit von Code und Daten erleichtert die rechnerische Reproduzierbarkeit, ebenso wie die Aufbereitung dieser Materialien (Datenbeschriftung, Angabe der verwendeten Softwareversionen, gemeinsame Nutzung von Rechenumgebungen usw.). Im Idealfall sollte die rechnerische Reproduzierbarkeit von einer/m zweiten Forschenden (oder der/m ursprünglichen Forschenden zu einem späteren Zeitpunkt) erreicht werden können, wofür nur eine Reihe von Dateien und schriftliche Anweisungen benötigt werden. Dies wird auch als analytische Reproduzierbarkeit bezeichnet (LeBel et al., 2018).",
                "related_terms": [
                    "FAIR principles",
                    "Replicability",
                    "Reproducibility"
                ],
                "references": "on Reproducibility, C., & in Science et al., R. (2019). Reproducibility and Replicability in Science (p. 25303). National Academies Press. https://doi.org/10.17226/25303\n\nKitzes, J., Turek, D., & Deniz, F. (2017). The practice of reproducible research: Case studies and lessons from the data-intensive sciences. University of California Press.\n\nLeBel, E. P., McCarthy, R. J., Earp, B. D., Elson, M., & Vanpaemel, W. (2018). A unified framework to quantify the credibility of scientific findings. Advances in Methods and Practices in Psychological Science, 1(3), 389–402. https://doi.org/10.1177/2515245918787489\n\nNosek, B. A., & Errington, T. M. (2020). What is replication? PLOS Biology, 18(3), e3000691. https://doi.org/10.1371/journal.pbio.3000691\n\nObels, P., Lakens, D., Coles, N. A., Gottfried, J., & Green, S. A. (2020). Analysis of open data and computational reproducibility in registered reports in psychology. Advances in Methods and Practices in Psychological Science, 3(2), 229–237. https://doi.org/10.1177/2515245920917961",
                "drafted_by": [
                    "Tina Lonsdorf"
                ],
                "reviewed_by": [
                    "Sarah Ashcroft-Jones",
                    "Helena Hartmann",
                    "Annalise A. LaPlume",
                    "Adam Parker",
                    "Charlotte R. Pennington",
                    "Eike Mark Rinke"
                ],
                "alt_related_terms": [
                    null
                ],
                "language": "german"
            },
            {
                "type": "glossary",
                "title": "Konzeptionelle Replikation (Conceptual replication)",
                "definition": "Ein Replikationsversuch, bei dem der primäre Effekt, der von Interesse ist, derselbe ist, aber in einer anderen Stichprobe getestet und auf eine andere Weise erfasst wird als ursprünglich berichtet (d. h. durch Verwendung anderer Operationalisierungen, Datenverarbeitungs- und statistischer Ansätze und/oder anderer Konstrukte; LeBel et al., 2018). Der Zweck einer konzeptionellen Replikation besteht häufig darin, zu erforschen, welche Rahmenbedingungen das Ausmaß begrenzen, in dem ein Effekt beobachtet und verallgemeinert werden kann (z. B. nur in bestimmten Kontexten, mit bestimmten Stichproben, unter Verwendung bestimmter Messansätze), um die Theorie zu evaluieren und weiterzuentwickeln (Hüffmeier et al., 2016).",
                "related_terms": [
                    "Direct replication",
                    "Generalizability"
                ],
                "references": "Crüwell, S., van Doorn, J., Etz, A., Makel, M. C., Moshontz, H., Niebaum, J. C., Orben, A., Parsons, S., & Schulte-Mecklenbeck, M. (2019). Seven Easy Steps to Open Science: An Annotated Reading List. Zeitschrift Für Psychologie, 227(4), 237–248. https://doi.org/10.1027/2151-2604/a000387\n\nHüffmeier, J., Mazei, J., & Schultze, T. (2016). Reconceptualizing replication as a sequence of different studies: A replication typology. Journal of Experimental Social Psychology, 66, 81–92. https://doi.org/10.1016/j.jesp.2015.09.009\n\nLeBel, E. P., McCarthy, R. J., Earp, B. D., Elson, M., & Vanpaemel, W. (2018). A unified framework to quantify the credibility of scientific findings. Advances in Methods and Practices in Psychological Science, 1(3), 389–402. https://doi.org/10.1177/2515245918787489",
                "drafted_by": [
                    "Mahmoud Elsherif; Thomas Rhys Evans"
                ],
                "reviewed_by": [
                    "Adrien Fillon",
                    "Helena Hartmann",
                    "Matt Jaquiery",
                    "Tina B. Lonsdorf",
                    "Catia M. Oliveira",
                    "Charlotte R. Pennington",
                    "Graham Reid",
                    "Timo Roettger",
                    "Lisa Spitzer",
                    "Suzanne L. K. Stewart",
                    "Flávio Azevedo"
                ],
                "alt_related_terms": [
                    null
                ],
                "language": "german"
            },
            {
                "type": "glossary",
                "title": "Bestätigungsverzerrung (Confirmation bias)",
                "definition": "Die Tendenz, Informationen in einer Weise zu suchen, zu interpretieren, zu bevorzugen und abzurufen, die die eigenen früheren Werte, Überzeugungen, Erwartungen oder Hypothesen beschäftigt und unterstützt.",
                "related_terms": [
                    "Confirmatory bias",
                    "Congeniality bias",
                    "Myside bias"
                ],
                "references": "Bishop, D. V. (2020). The psychology of experimental psychologists: Overcoming cognitive constraints to improve research: The 47th Sir Frederic Bartlett Lecture. Quarterly Journal of Experimental Psychology, 73(1), 1–19. https://doi.org/10.1177/1747021819886519\n\nNickerson, R. S. (1998). Confirmation bias: A ubiquitous phenomenon in many guises. Review of General Psychology, 2(2), 175–220. https://doi.org/10.1037/1089-2680.2.2.175\n\nSpencer, E. A., & Heneghan, C. (2018). Confirmation bias. Catalogue Of Bias. https://catalogofbias.org/biases/confirmation-bias/\n\nWason, P. C. (1960). On the failure to eliminate hypotheses in a conceptual task. Quarterly Journal of Experimental Psychology, 12(3), 129–140. https://doi.org/10.1080/17470216008416717",
                "drafted_by": [
                    "Barnabas Szaszi; Jenny Terry"
                ],
                "reviewed_by": [
                    "Mahmoud Elsherif",
                    "Tamara Kalandadze",
                    "Sam Parsons",
                    "Charlotte R. Pennington"
                ],
                "alt_related_terms": [
                    null
                ],
                "language": "german"
            },
            {
                "type": "glossary",
                "title": "konfirmatorische Analysen (Confirmatory analyses)",
                "definition": "Teil der Unterscheidung zwischen konfirmatorisch und explorativ (Wagenmakers et al., 2012), wobei sich konfirmatorische Analysen auf Analysen beziehen, die im Vorfeld festgelegt wurden und bestehende Hypothesen testen. Die fehlende Abgrenzung dieser beiden Analyse-Arten in veröffentlichten Forschungsergebnissen wird als Erklärung für Replikationsprobleme vermutet. Es wurde vorgeschlagen, dieses Problem durch Präregistrierungen (Preregistrations) zu überwinden, da diese klar konfirmatorische und explorative Analysen trennen. Andere Forschende haben diese Begriffe in Frage gestellt und eine Umbenennung in \"entdeckungsorientierte\" und \"theorieprüfende Forschung\" empfohlen (Oberauer & Lewandowsky, 2019; siehe auch Szollosi & Donkin, 2019).",
                "related_terms": [
                    "Exploratory data analysis",
                    "Preregistration"
                ],
                "references": "Box, G. E. P. (1976). Science and statistics. Journal of the American Statistical Association, 71(356), 791–799.\n\nOberauer, K., & Lewandowsky, S. (2019). Addressing the theory crisis in psychology. Psychonomic Bulletin & Review, 26(5), 1596–1618. https://doi.org/10.3758/s13423-019-01645-2\n\nSzollosi, A., & Donkin, C. (2019). Arrested theory development: The misguided distinction between exploratory and confirmatory research. PsyArXiv. https://doi.org/10.31234/osf.io/your_doi_placeholder\n\nTukey, J. W. (1977). Exploratory data analysis. Addison-Wesley.\n\nWagenmakers, E. J., Wetzels, R., Borsboom, D., van der Maas, H. L., & Kievit, R. A. (2012). An agenda for purely confirmatory research. Perspectives on Psychological Science, 7(6), 632–638. https://doi.org/10.1177/1745691612463078",
                "drafted_by": [
                    "Jenny Terry"
                ],
                "reviewed_by": [
                    "Mahmoud Elsherif",
                    "Eduardo Garcia-Garzon",
                    "Helena Hartmann",
                    "Mariella Paul",
                    "Charlotte R. Pennington",
                    "Timo Roettger",
                    "Lisa Spitzer"
                ],
                "alt_related_terms": [
                    null
                ],
                "language": "german"
            },
            {
                "type": "glossary",
                "title": "Interessenkonflikt (Conflict of interest)",
                "definition": "Ein Interessenkonflikt (COI, Conflict of Interest) ist eine finanzielle oder nicht-finanzielle Beziehung, Aktivität oder ein anderes Interesse, das die Objektivität oder das professionelle Urteilsvermögen von Autor:innen, Gutachter:innen, Redakteur:innen oder Redaktionsmitgliedern beeinträchtigen könnte. In den “Principles of Transparency and Best Practice in Scholarly Publishing” des Committee on Publication Ethics (COPE), des Directory of Open Access Journals (DOAJ), der Open Access Scholarly Publishers Association (OASPA) und der World Association of Medical Editors (WAME) heißt es, dass Zeitschriften über Richtlinien zur Publikationsethik verfügen sollten, einschließlich Richtlinien zu COI (DOAJ, 2018). COIs sollten transparent offengelegt werden, damit die Lesende die Forschung richtig bewerten und auf potenzielle oder tatsächliche Voreingenommenheit(en) prüfen können. Jenseits von Veröffentlichungen sollten akademische Vortragende, Gremiumsmitglieder und Lehrende ebenfalls COIs angeben. Die absichtliche Nichtoffenlegung eines Interessenkonflikts kann als eine Form des Fehlverhaltens angesehen werden.",
                "related_terms": [
                    "Objectivity",
                    "Peer review",
                    "Public Trust in Science",
                    "Publication ethics",
                    "Transparency"
                ],
                "references": "Directory of Open Access Journals. (n.d.). https://doaj.org/apply/transparency/",
                "drafted_by": [
                    "Christopher Graham"
                ],
                "reviewed_by": [
                    "Flávio Azevedo"
                ],
                "alt_related_terms": [
                    null
                ],
                "language": "german"
            },
            {
                "type": "glossary",
                "title": "Konsortium Autor:innenschaft (Consortium authorship)",
                "definition": "In der Autor:innenspalte erscheint nur der Name des Konsortiums oder der Organisation; die Namen der einzelnen Personen werden nicht genannt: Zum Beispiel \"FORRT\" als Autor. Dies ist bei den Publikationen der Ergebnisse von Gemeinschaftsprojekten mit einer sehr großen Anzahl von Mitwirkenden zu beobachten. Je nach den Richtlinien der Zeitschrift können einzelne Forschende in Literaturdatenbanken wie ORCID und Scopus als einer der Autoren aufgeführt werden. Die Autor:innenschaft eines Konsortiums kann auch als Gruppen-, Unternehmens-, Organisations- oder kollektive Autorenschaft (z. B. [https://www.bmj.com/about-bmj/resources-authors/article-submission/authorship-contributorship](https://www.bmj.com/about-bmj/resources-authors/article-submission/authorship-contributorship)) oder als kollaborative Autorenschaft (z. B. [https://support.jmir.org/hc/en-us/articles/115001449591-What-is-a-group-author-collaborative-author-and-does-it-need-an-ORCID](https://support.jmir.org/hc/en-us/articles/115001449591-What-is-a-group-author-collaborative-author-and-does-it-need-an-ORCID)) bezeichnet werden.",
                "related_terms": [
                    "Authorship",
                    "CRediT"
                ],
                "references": "Tierney, W., Hardy III, J. H., Ebersole, C. R., Leavitt, K., Viganola, D., Clemente, E. G., & others. (2020). Creative destruction in science. Organizational Behavior and Human Decision Processes, 161, 291–309. https://doi.org/10.1016/j.obhdp.2020.07.002\n\nTierney, W., Hardy III, J., Ebersole, C. R., Viganola, D., Clemente, E. G., Gordon, M., & others. (2021). A creative destruction approach to replication: Implicit work and sex morality across cultures. Journal of Experimental Social Psychology, 93, 104060. https://doi.org/10.1016/j.jesp.2020.104060",
                "drafted_by": [
                    "Yuki Yamada"
                ],
                "reviewed_by": [
                    "Adam Parker",
                    "Charlotte R. Pennington",
                    "Beatrice Valentini",
                    "Qinyu Xiao",
                    "Flávio Azevedo"
                ],
                "alt_related_terms": [
                    null
                ],
                "language": "german"
            },
            {
                "type": "glossary",
                "title": "COG, Beschränkungen der Generalisierbarkeit (Constraints on Generality (COG))",
                "definition": "Eine Erklärung, in der die Zielpopulation und die Bedingungen für die berichteten Ergebnisse ausdrücklich genannt und begründet werden. Forschende sollten explizit auf mögliche Randbedingungen für ihre Verallgemeinerungen eingehen (Simons et al., 2017). Forschende sollten detaillierte Beschreibungen der Population aus der die Stichprobe stammte und/oder kontextbezogene Faktoren liefern, die die Ergebnisse beeinflusst haben könnten, damit künftige Replikationsversuche diese Faktoren berücksichtigen können (Brandt et al., 2014). Bei nicht explizit aufgeführten Bedingungen wird davon ausgegangen, dass sie keine theoretische Relevanz für die Replizierbarkeit des Effekts haben.",
                "related_terms": [
                    "BIZARRE",
                    "Diversity",
                    "Equity",
                    "Generalizability",
                    "Inclusion",
                    "Reproducibility",
                    "Replication",
                    "STRANGE",
                    "WEIRD"
                ],
                "references": "Busse, C., Kach, A. P., & Wagner, S. M. (2017). Boundary Conditions: What They Are, How to Explore Them, Why We Need Them, and When to Consider Them. Organizational Research Methods, 20(4), 574–609. https://doi.org/10.1177/1094428116641191\n\nBrandt, M. J., IJzerman, H., Dijksterhuis, A., Farach, F. J., Geller, J., Giner-Sorolla, R., & others. (2014). The replication recipe: What makes for a convincing replication? Journal of Experimental Social Psychology, 50, 217–224. https://doi.org/10.1016/j.jesp.2013.10.005\n\nSimons, D. J., Shoda, Y., & Lindsay, D. S. (2017). Constraints on generality (COG): A proposed addition to all empirical papers. Perspectives on Psychological Science, 12(6), 1123–1128. https://doi.org/10.1177/1745691617708630\n\nYarkoni, T. (2020). The generalizability crisis. Behavioral and Brain Sciences, 1–37. https://doi.org/10.1017/S0140525X20001685",
                "drafted_by": [
                    "Mahmoud Elsherif"
                ],
                "reviewed_by": [
                    "Ali H. Al-Hoorie",
                    "Jamie P. Cockcroft",
                    "Sam Parsons",
                    "Charlotte R. Pennington",
                    "Timo Roettger"
                ],
                "alt_related_terms": [
                    null
                ],
                "language": "german"
            },
            {
                "type": "glossary",
                "title": "Konstruktvalidität (Construct validity)",
                "definition": "Im Zusammenhang mit Messungen und Tests bezieht sich die Konstruktvalidität auf den Grad, in dem ein Test das misst, was er zu messen vorgibt. In Bereichen, in denen hypothetische, nicht beobachtbare Entitäten untersucht werden, ist die Konstruktvalidierung im Wesentlichen ein Theorietest, da es darum geht, festzustellen, ob eine Messung (ein Fragebogen, eine Laboraufgabe usw.) eine gültige Repräsentation eines hypothetischen Konstrukts ist (d. h. einer Theorie entspricht). Wird die Konstruktvalidität im weiteren Sinne auf eine Studie oder eine Behauptung, Schlussfolgerung oder beobachtete Wirkung in einer Forschungsarbeit angewandt, so bezieht sie sich auf das Ausmaß, in dem die in der Studie verwendeten Stichprobenmerkmale und Charakteristika (Teilnehmende, Setting, Interventionen und abhängige Variablen) den Konstrukten höherer Ordnung entsprechen, um die es in der Studie, der Behauptung oder der Schlussfolgerung geht. Nach Shadish et al. (2002) kann Konstruktvalidität definiert werden als \"the degree to which inferences are warranted from the observed persons, settings, and cause and effect operations included in a study to the constructs that these instances might represent\" (S. 38,dt. das Ausmaß, in dem Rückschlüsse von den beobachteten Personen, Settings und Ursache-Wirkungs-Beziehungen in einer Studie auf die Konstrukte, die diese Instanzen repräsentieren könnten, gerechtfertigt sind).",
                "related_terms": [
                    "Measurement crisis",
                    "Measurement validity",
                    "Questionable Measurement Practices (QMP)",
                    "Theory",
                    "Validity",
                    "Validation"
                ],
                "references": "Cronbach, L. J., & Meehl, P. E. (1955). Construct validity in psychological tests. Psychological Bulletin, 52(4), 281–302. https://doi.org/10.1037/h0040957\n\nShadish, W. R., Cook, T. D., & Campbell, D. T. (2002). Experimental and quasi-experimental designs for generalized causal inference. Houghton Mifflin.\n\nSmith, G. T. (2005). On Construct Validity: Issues of Method and Measurement. Psychological Assessment, 17(4), 396–408. https://doi.org/10.1037/1040-3590.17.4.396",
                "drafted_by": [
                    "Annalise A. LaPlume"
                ],
                "reviewed_by": [
                    "Ali H. Al-Hoorie",
                    "Mahmoud Elsherif",
                    "Zoltan Kekecs",
                    "Charlotte R. Pennington"
                ],
                "alt_related_terms": [
                    null
                ],
                "language": "german"
            },
            {
                "type": "glossary",
                "title": "Inhaltsvalidität (Content validity)",
                "definition": "Das Ausmaß, zu dem eine Messung alle Aspekte des Konzepts umfasst, das die/der Forschende zu messen vorgibt; “A qualitative type of validity where the domain of the concept is made clear and the analyst judges whether the measures fully represent the domain” (dt. eine qualitative Art der Validität, bei der der Bereich des Konzepts deutlich gemacht wird und die/der Analytiker:in beurteilt, ob die Maßnahmen den Bereich vollständig repräsentieren; Bollen, 1989, S. 185). Sie ist eine Komponente der *Konstruktvalidität* und kann sowohl mit quantitativen als auch mit qualitativen Methoden ermittelt werden, die häufig eine Expert:innenbewertung beinhalten.",
                "related_terms": [
                    "Construct validity",
                    "Validity"
                ],
                "references": "Bollen, K. A. (1989). Structural Equations with Latent Variables (pp. 179–225). John Wiley & Sons.\n\nBrod, M., Tesler, L., & Christensen, T. (2009). Qualitative research and content validity: Developing best practices based on science and experience. Quality of Life Research, 18(9), 1263–1278. https://doi.org/10.1007/s11136-009-9540-9\n\nDrost, E. A. (2011). Validity and reliability in social science research. Education Research and Perspectives, 38(1), 105–123.\n\nHaynes, S. N., Richard, D. C. S., & Kubany, E. S. (1995). Content validity in psychological assessment: A functional approach to concepts and methods. Psychological Assessment, 7(3), 238–247. https://doi.org/10.1037/1040-3590.7.3.238",
                "drafted_by": [
                    "Annalise A. LaPlume"
                ],
                "reviewed_by": [
                    "Mahmoud Elsherif",
                    "Wanyin Li",
                    "Aoife O’Mahony",
                    "Eike Mark Rinke",
                    "Sam Parsons",
                    "Graham Reid"
                ],
                "alt_related_terms": [
                    null
                ],
                "language": "german"
            },
            {
                "type": "glossary",
                "title": "Beitrag (Contribution)",
                "definition": "Eine formale Ergänzung oder Aktivität in einem Forschungskontext. Erklärungen zu Beiträgen und Mitwirkenden, einschließlich der Danksagungen in Zeitschriftenartikeln, werden an Forschungsprodukte angehängt, um die Vielfalt der Arbeit, die über \"Autor:innenschaft\" hinausgeht und die jede intellektuelle Tätigkeit erfordert, besser zu klassifizieren und anzuerkennen. Contributions sind eine sich entwickelnde “source of data for understanding the relationship between authorship and knowledge production.” (dt. Datenquelle für das Verständnis der Beziehung zwischen Autor:innenschaft und Wissensproduktion; Lariviere et al., S.430). Bei der Entwicklung von Open-Source-Software kann ein solcher Beitrag eine Änderung sein, die nach Begutachtung in das Software-Repositorium eines Projekts aufgenommen wird (technisch als Pull-Request bezeichnet). Ein Beispiel für ein Open-Source-Projekt, das Beiträge akzeptiert, ist NumPy (Harris et al., 2020).",
                "related_terms": [
                    "authorship",
                    "CRediT",
                    "Semantometrics"
                ],
                "references": "Knoth, P., & Herrmannova, D. (2014). Towards semantometrics: A new semantic similarity based measure for assessing a research publication’s contribution. D-Lib Magazine, 20(11), 8. https://doi.org/10.1045/november2014-knoth\n\nLarivière, V., Desrochers, N., Macaluso, B., Mongeon, P., Paul-Hus, A., & Sugimoto, C. R. (2016). Contributorship and division of labor in knowledge production. Social Studies of Science, 46(3), 417–435. https://doi.org/10.1177/0306312716650046\n\nHolcombe, A. O. (2019). Contributorship, not authorship: Use CRediT to indicate who did what. Publications, 7(3), 48. https://doi.org/10.3390/publications7030048",
                "drafted_by": [
                    "Micah Vandegrift"
                ],
                "reviewed_by": [
                    "Jamie P. Cockcroft",
                    "Dominik Kiersz",
                    "Michele C. Lim",
                    "Leticia Micheli",
                    "Sam Parsons",
                    "Gerald Vineyard"
                ],
                "alt_related_terms": [
                    null
                ],
                "language": "german"
            },
            {
                "type": "glossary",
                "title": "Korrigendum (Corrigendum)",
                "definition": "Ein Korrigendum (pl. Korrigenda, lat.: \"berichtigen\") dokumentiert einen oder mehrere Fehler in einer veröffentlichten Arbeit, die die zentrale Aussage oder die Schlussfolgerungen nicht verändern und somit nicht den Standard erreichen, der ein Zurückziehen der Arbeit erfordert. Korrigenda werden in der Regel neben dem Originalwerk veröffentlicht, um die Transparenz zu erhöhen. Einige Verlage bezeichnen dieses Dokument als Erratum (pl. errata, lateinisch: \"Fehler\"), während andere eine Unterscheidung zwischen beiden treffen (Corrigenda als Fehler der Autor:innen und Errata als Verlagsfehler).",
                "related_terms": [
                    "Correction",
                    "Errata",
                    "Retraction"
                ],
                "references": "Anon. (2006). Correction or retraction? In Nature (Vol. 444, pp. 123–124). https://doi.org/10.1038/444123b",
                "drafted_by": [
                    "Charlotte R. Pennington"
                ],
                "reviewed_by": [
                    "Bradley Baker",
                    "Nick Ballou",
                    "Wanyin Li",
                    "Adam Parker",
                    "Emily A. Williams"
                ],
                "alt_related_terms": [
                    null
                ],
                "language": "german"
            },
            {
                "type": "glossary",
                "title": "Ko-Produktion (Co-production)",
                "definition": "Bei diesem Forschungsansatz werden Beteiligte, die traditionell nicht in den Forschungsprozess eingebunden sind, zu Beginn des Projekts oder während des gesamten Forschungsprozesses in die Zusammenarbeit eingebunden. So können beispielsweise an der ko-produzierten Gesundheitsforschung Angehörige von Gesundheitsberufen und Patient:innen beteiligt sein. An der ko-produzierten Bildungsforschung beispielsweise können Lehrkräfte und Schüler:innen/Studierende beteiligt sein. Dahinter stehen Grundsätze wie die Achtung und Wertschätzung der Erfahrungen von Nicht-Forschenden, der bewusste Umgang mit Machtdynamiken und der Aufbau von Beziehungen..",
                "related_terms": [
                    "Citizen science",
                    "Collaboration",
                    "Collaborative research",
                    "Crowd science",
                    "Engaged scholarship",
                    "Integrated Knowledge Translation (IKT)",
                    "Mode 2 of knowledge production",
                    "Participatory research",
                    "Patient and Public Involvement (PPI)"
                ],
                "references": "Filipe, A., Renedo, A., & Marston, C. (2017). The co-production of what? Knowledge, values, and social relations in health care. PLoS Biology, 15(5), e2001403. https://doi.org/10.1371/journal.pbio.2001403\n\nGraham, I. D., McCutcheon, C., & Kothari, A. (2019). Exploring the frontiers of research co-production: the Integrated Knowledge Translation Research Network concept papers. Health Research Policy and Systems, 17, 88. https://doi.org/10.1186/s12961-019-0501-7\n\nNIHR Guidance on Co-Producing a Research Project. (2021). https://www.learningforinvolvement.org.uk/?opportunity=nihr-guidance-on-co-producing-a-research-project\n\nCo-Production Collective. (n.d.). Our Approach. Co-Production Collective. https://www.coproductioncollective.co.uk/what-is-co-production/our-approach",
                "drafted_by": [
                    "Emma Norris"
                ],
                "reviewed_by": [
                    "Gisela H. Govaart",
                    "Magdalena Grose-Hodge",
                    "Helena Hartmann;Charlotte R. Pennington",
                    "Sonia Rishi",
                    "Emily A. Williams"
                ],
                "alt_related_terms": [
                    null
                ],
                "language": "german"
            },
            {
                "type": "glossary",
                "title": "CC (Creative Commons (CC) license)",
                "definition": "Eine Reihe kostenloser und benutzerfreundlicher Urheberrechtslizenzen, die die Rechte von Autor:innen und Nutzer:innen frei verfügbarer Daten und Materialien auf standardisierte Weise definieren. CC-Lizenzen ermöglichen es Autor:innen oder Schöpfer:innen, urheberrechtlich geschützte Werke mit der Öffentlichkeit zu teilen. Es gibt sie in verschiedenen Varianten mit mehr oder weniger Bestimmungen. Die Attribution-NonCommercial-ShareAlike 4.0 International (CC BY-NC-SA 4.0) erlaubt beispielsweise, das Material zu teilen und anzupassen, unter den Bedingungen, dass die ursprünglichen Urheber:innen genannt werden, angegeben wird, ob Änderungen vorgenommen wurden, es unter derselben Lizenz wie das Original geteilt wird und das Material nicht für kommerzielle Zwecke verwendet wird.",
                "related_terms": [
                    "Copyright",
                    "Licence **Alternative definition:** (if applicable) Creative Commons is an international nonprofit organization that provides Creative Commons licences, with the goal to minimize legal obstacles to the sharing of knowledge and creativity."
                ],
                "references": "Anon. (n.d.). About CC Licenses. Retrieved from https://creativecommons.org/about/cclicenses/",
                "drafted_by": [
                    "Tina Lonsdorf"
                ],
                "reviewed_by": [
                    "Adrien Fillon",
                    "Gisela H. Govaart",
                    "Annalise A. LaPlume",
                    "Sam Parsons",
                    "Charlotte R. Pennington"
                ],
                "alt_related_terms": [
                    null
                ],
                "language": "german"
            },
            {
                "type": "glossary",
                "title": "Glaubhaftigkeitsrevolution (Credibility revolution)",
                "definition": "Der Begriff wurde als Alternative zum Begriff “Replikationskrise” vorgeschlagen und umfasst die vielen Lösungen zur Verbesserung der Glaubwürdigkeit der Forschung, wie Präregistrierung, Transparenz und Replikation.",
                "related_terms": [
                    "Credibility of scientific claims",
                    "High standards of evidence",
                    "Openness",
                    "Open Science;Reproducibility crisis (aka Replicability or replication crisis)",
                    "Transparency"
                ],
                "references": "Angrist, J. D., & Pischke, J. S. (2010). The credibility revolution in empirical economics: How better research design is taking the con out of econometrics. Journal of Economic Perspectives, 24, 3–30. https://doi.org/10.1257/jep.24.2.3\n\nVazire, S. (2018). Implications of the Credibility Revolution for Productivity, Creativity, and Progress. Perspectives on Psychological Science, 13(4), 411–417. https://doi.org/10.1177/1745691617751884\n\nVazire, S., Schiavone, S. R., & Bottesini, J. G. (2020). Credibility Beyond Replicability: Improving the Four Validities in Psychological Science. https://doi.org/10.31234/osf.io/bu4d3",
                "drafted_by": [
                    "Tamara Kalandadze"
                ],
                "reviewed_by": [
                    "Bradley Baker",
                    "Mahmoud Elsherif",
                    "Helena Hartmann",
                    "Kai Krautter",
                    "Annalise A. LaPlume",
                    "Oscar Lecuona",
                    "Charlotte R. Pennington",
                    "Robert Ross",
                    "Tobias Wingen",
                    "Flávio Azevedo"
                ],
                "alt_related_terms": [
                    null
                ],
                "language": "german"
            },
            {
                "type": "glossary",
                "title": "Schöpferische Zerstörung als Herangehensweise an Replikationen (Creative destruction approach to replication)",
                "definition": "Replikationsbemühungen sollten nicht nur darauf abzielen, die ursprünglichen Ergebnisse zu stützen oder in Frage zu stellen, sondern sie auch durch überarbeitete, stärkere Theorien mit größerer Erklärungskraft zu ersetzen. Dieser Ansatz beinhaltet daher das \"Stutzen\" bestehender Theorien, den Vergleich aller alternativen Theorien und eine generativere und theoriebildende Ausrichtung von Replikationsbemühungen (Tierney et al. 2020, 2021).",
                "related_terms": [
                    "Crowdsourced research",
                    "Falsification",
                    "Replication",
                    "Theory"
                ],
                "references": "Tierney, W., Hardy III, J. H., Ebersole, C. R., Leavitt, K., Viganola, D., Clemente, E. G., & others. (2020). Creative destruction in science. Organizational Behavior and Human Decision Processes, 161, 291–309. https://doi.org/10.1016/j.obhdp.2020.07.002\n\nTierney, W., Hardy III, J., Ebersole, C. R., Viganola, D., Clemente, E. G., Gordon, M., & others. (2021). A creative destruction approach to replication: Implicit work and sex morality across cultures. Journal of Experimental Social Psychology, 93, 104060. https://doi.org/10.1016/j.jesp.2020.104060",
                "drafted_by": [
                    "Mahmoud Elsherif"
                ],
                "reviewed_by": [
                    "Magdalena Grose-Hodge",
                    "Aoife O’Mahony",
                    "Adam Parker",
                    "Charlotte R. Pennington",
                    "Sonia Rishi",
                    "Beatrice Valentini"
                ],
                "alt_related_terms": [
                    null
                ],
                "language": "german"
            },
            {
                "type": "glossary",
                "title": "CRediT",
                "definition": "Die Contributor Roles Taxonomy (CRediT; dt. Klassifikation der Rollen von Mitwirkenden, https://credit.niso.org/) ist eine übergeordnete Taxonomie, die dazu dient, die Rollen zu beschreiben, die typischerweise von Mitwirkenden an wissenschaftlichen Arbeiten eingenommen werden. Gegenwärtig gibt es 14 Rollen, die den spezifischen Beitrag eines jeden Mitwirkenden zur Veröffentlichung beschreiben. Sie können verschiedenen Autor:innen mehrfach zugewiesen werden, und einer:m Autor:in können auch mehrere Rollen zugewiesen werden. CRediT umfasst die folgenden Rollen: Konzeptualisierung, Datenkuratierung, formale Analyse, Akquise von Drittmitteln, Untersuchung, Methodik, Projektverwaltung, Ressourcen, Software, Betreuung, Validierung, Visualisierung, Schreiben \\- ursprünglicher Entwurf, Schreiben \\- Überprüfung und Bearbeitung. Eine Beschreibung der verschiedenen Rollen findet sich in der Arbeit von Brand et al. (2015).",
                "related_terms": [
                    "Authorship",
                    "Contributions"
                ],
                "references": "Brand, A., Allen, L., Altman, M., Hlava, M., & Scott, J. (2015). Beyond authorship: attribution, contribution, collaboration, and credit. Learned Publishing, 28(2), 151–155. https://doi.org/10.1087/20150211\n\nHolcombe, A. O. (2019). Contributorship, not authorship: Use CRediT to indicate who did what. Publications, 7(3), 48. https://doi.org/10.3390/publications7030048",
                "drafted_by": [
                    "Sam Parsons"
                ],
                "reviewed_by": [
                    "Myriam A. Baum",
                    "Matt Jaquiery",
                    "Tamara Kalandadze",
                    "Connor Keating",
                    "Charlotte R. Pennington",
                    "Yuki Yamada",
                    "Flávio Azevedo"
                ],
                "alt_related_terms": [
                    null
                ],
                "language": "german"
            },
            {
                "type": "glossary",
                "title": "Kriteriumsvalidität (Criterion validity)",
                "definition": "Das Ausmaß, in dem eine Messung mit anderen gültigen Messungen desselben Konzepts übereinstimmt. Die Kriteriumsvalidität wird in der Regel durch die Berechnung von Regressionskoeffizienten oder bivariaten Korrelationen ermittelt, mit denen die Richtung und Stärke der Beziehung zwischen dem Testmaß und dem Kriteriumsmaß geschätzt wird. Sie wird oft mit der *Konstruktvalidität* verwechselt, obwohl sie sich von dieser in der Absicht (lediglich prädiktiv und nicht theoretisch) und im Interesse (Vorhersage eines beobachtbaren Ergebnisses und nicht eines latenten Konstrukts) unterscheidet. Fehlende Reliabilität (Zuverlässigkeit) bei Test- oder Kriteriumsergebnissen beeinträchtigt in der Regel die Kriteriumsvalidität. Sie wird auch kriterienbezogene oder konkrete Validität genannt.",
                "related_terms": [
                    "Construct validity",
                    "Validity"
                ],
                "references": "DeVellis, R. F. (2017). Scale development: Theory and applications (4th ed.). Sage.\n\nDrost, E. A. (2011). Validity and reliability in social science research. Education Research and Perspectives, 38(1), 105–123.",
                "drafted_by": [
                    "Annalise A. LaPlume"
                ],
                "reviewed_by": [
                    "Helena Hartmann",
                    "Kai Krautter",
                    "Sam Parsons",
                    "Eike Mark Rinke"
                ],
                "alt_related_terms": [
                    null
                ],
                "language": "german"
            },
            {
                "type": "glossary",
                "title": "Crowdsourcing-Forschung (Crowdsourced Research)",
                "definition": "Die Crowdsourcing-Forschung ist ein Modell für die soziale Organisation der Forschung als groß angelegte Zusammenarbeit, bei der ein oder mehrere Forschungsprojekte von mehreren Teams auf unabhängige und koordinierte Weise durchgeführt werden. Crowdsourcing-Forschung zielt darauf ab, durch die Bündelung von Ressourcen, die Förderung von Transparenz und sozialer Inklusion Effizienz und Skalierbarkeit zu erreichen und die Genauigkeit, Zuverlässigkeit (Reliabilität) und Vertrauenswürdigkeit durch die Verbesserung der statistischen Teststärke (Power) und gegenseitiger sozialer Abstimmung zu erhöhen. Sie steht im Gegensatz zum traditionellen Modell der akademischen Forschungsproduktion, das von der unabhängigen Arbeit einzelner Forschenden oder kleiner Forschendengruppen (\"small science\") dominiert wird. Beispiele für Crowdsourcing-Forschung sind sogenannte \"Many Labs\"-Replikations-Studien (Klein et al., 2018), \"Many Analysts, One Dataset\"-Studien (Silberzahn et al., 2018), verteilte kollaborative Netzwerke (Moshontz et al., 2018\\) und offene kollaborative Schreibprojekte wie Massively Open Online Papers (MOOPs) (Himmelstein et al., 2019; Tennant et al., 2019). Alternativ kann sich Crowdsourcing-Forschung auf den Einsatz einer großen Anzahl von \"Crowdforschenden\" bei der Datenerhebung beziehen, die über Online-Arbeitsmärkte wie Amazon Mechanical Turk oder Prolific angeheuert werden, zum Beispiel bei Inhaltsanalyse (Benoit et al., 2016; Lind et al., 2017\\) oder experimenteller Forschung (Peer et al., 2017). Crowdsourcing-Forschung, die sowohl offen für die Teilnahme als auch offen für gemeinsame Zwischenergebnisse ist, wird als Crowd Science bezeichnet (Franzoni & Sauermann, 2014).",
                "related_terms": [
                    "Citizen science",
                    "Collaboration",
                    "Crowdsourcing",
                    "Team science"
                ],
                "references": "Benoit, K., Conway, D., Lauderdale, B. E., Laver, M., & Mikhaylov, S. (2016). Crowd-sourced text analysis: Reproducible and agile production of political data. American Political Science Review, 110(2), 278–295. https://doi.org/10.1017/S0003055416000058\n\nBreznau, N. (2021). I saw you in the crowd: Credibility, reproducibility, and meta-utility. PS: Political Science & Politics, 54(2), 309–313. https://doi.org/10.1017/S1049096520000980\n\nFranzoni, C., & Sauermann, H. (2014). Crowd science: The organization of scientific research in open collaborative projects. Research Policy, 43(1), 1–20. https://doi.org/10.1016/j.respol.2013.07.005\n\nHimmelstein, D. S., Rubinetti, V., Slochower, D. R., Hu, D., Malladi, V. S., Greene, C. S., & Gitter, A. (2019). Open collaborative writing with Manubot. PLOS Computational Biology, 15(6), e1007128. https://doi.org/10.1371/journal.pcbi.1007128\n\nKlein, R. A., Vianello, M., Hasselman, F., Adams, B. G., Adams, R. B., Alper, S., & … Nosek, B. A. (2018). Many Labs 2: Investigating Variation in Replicability Across Samples and Settings. Advances in Methods and Practices in Psychological Science, 1(4), 443–490. https://doi.org/10.1177/2515245918810225\n\nLind, F., Gruber, M., & Boomgaarden, H. G. (2017). Content analysis by the crowd: Assessing the usability of crowdsourcing for coding latent constructs. Communication Methods and Measures, 11(3), 191–209. https://doi.org/10.1080/19312458.2017.1317338\n\nMoshontz, H., Campbell, L., Ebersole, C. R., IJzerman, H., Urry, H. L., Forscher, P. S., & Chartier, C. R. (2018). The Psychological Science Accelerator: Advancing psychology through a distributed collaborative network. Advances in Methods and Practices in Psychological Science, 1(4), 501–515. https://doi.org/10.1177/2515245918797607\n\nPeer, E., Brandimarte, L., Samat, S., & Acquisti, A. (2017). Beyond the Turk: Alternative platforms for crowdsourcing behavioral research. Journal of Experimental Social Psychology, 70, 153–163. https://doi.org/10.1016/j.jesp.2017.01.006\n\nSilberzahn, R., Uhlmann, E. L., Martin, D. P., Anselmi, P., Aust, F., Awtrey, E., Bahník, Š., Bai, F., Bannard, C., Bonnier, E., & others. (2018). Many Analysts, One Data Set: Making Transparent How Variations in Analytic Choices Affect Results. Advances in Methods and Practices in Psychological Science, 337–356. https://doi.org/10.1177/2515245917747646\n\nStewart, N., Chandler, J., & Paolacci, G. (2017). Crowdsourcing samples in cognitive science. Trends in Cognitive Sciences, 21(10), 736–748. https://doi.org/10.1016/j.tics.2017.06.007\n\nTennant, J., Bielczyk, N. Z., Cheplygina, V., Greshake Tzovaras, B., Hartgerink, C. H. J., Havemann, J., Masuzzo, P., & Steiner, T. (2019). Ten simple rules for researchers collaborating on Massively Open Online Papers (MOOPs). MetaArXiv. https://doi.org/10.31222/osf.io/et8ak\n\nUhlmann, E. L., Ebersole, C. R., Chartier, C. R., Errington, T. M., Kidwell, M. C., Lai, C. K., McCarthy, R. J., Riegelman, A., Silberzahn, R., & Nosek, B. A. (2019). Scientific utopia III: Crowdsourcing science. Perspectives on Psychological Science, 14(5), 711–733. https://doi.org/10.1177/1745691619850561\n\nWeek, C. (2021). What is Crowdsourcing? https://crowdsourcingweek.com/what-is-crowdsourcing/",
                "drafted_by": [
                    "Eike Mark Rinke"
                ],
                "reviewed_by": [
                    "Ali H. Al-Hoorie",
                    "Sam Parsons",
                    "Charlotte R. Pennington",
                    "Suzanne L. K. Stewart",
                    "Flávio Azevedo"
                ],
                "alt_related_terms": [
                    null
                ],
                "language": "german"
            },
            {
                "type": "glossary",
                "title": "kulturelle Taxierung (Cultural taxation)",
                "definition": "Die zusätzliche Arbeit, die von Mitgliedern unterrepräsentierter oder marginalisierter Minderheitsgruppen, insbesondere von Wissenschaftler:innen of color, erwartet oder verlangt wird. Diese Arbeit entsteht oft durch die Übernahme von Rollen für die herkunftsspezifische, kulturelle oder geschlechtsspezifische Repräsentativität und Diversität. Diese Aufgaben können formell oder informell sein und werden in der Regel nicht vergütet oder belohnt. Dazu gehören die Bereitstellung von Fachwissen zu Fragen der Diversität, die Schulung der Mitglieder von Mehrheitsgruppen, die Tätigkeit als Verbindungsperson zu Minderheitsgemeinschaften sowie formelle und informelle Rollen als Mentor:in und Unterstützungssystem für Studierende aus Minderheiten.",
                "related_terms": [
                    "Invisible labor",
                    "Power imbalances",
                    "Power relations"
                ],
                "references": "Joseph, T. D., & Hirshfield, L. E. (2011). `Why don’t you get somebody new to do it?’ Race and cultural taxation in the academy. Ethnic and Racial Studies, 34(1), 121–141. https://doi.org/10.1080/01419870.2010.496489\n\nLedgerwood, A., Hudson, S. T. J., Lewis, Jr., N. A., Maddox, K. B., Pickett, C., Remedios, J. D., & Wilkins, C. L. (2021). The Pandemic as a Portal: Reimagining Psychological Science as Truly Open and Inclusive. https://doi.org/10.31234/osf.io/gdzue\n\nPadilla, A. M. (1994). Research news and comment: Ethnic minority scholars; research, and mentoring: Current and future issues. Educational Researcher, 23(4), 24–27. https://doi.org/10.3102/0013189X023004024",
                "drafted_by": [
                    "Bradley Baker"
                ],
                "reviewed_by": [
                    "Helena Hartmann",
                    "Bethan Iley",
                    "Aoife O’Mahony",
                    "Charlotte R. Pennington",
                    "Flávio Azevedo"
                ],
                "alt_related_terms": [
                    null
                ],
                "language": "german"
            },
            {
                "type": "glossary",
                "title": "kumulative Wissenschaft (Cumulative science)",
                "definition": "Ziel jeder empirischen Wissenschaft ist \"the construction of a cumulative base of knowledge upon which the future of the science may be built” (dt. das Erschaffen einer kumulativen Wissensbasis, auf der die Zukunft der Wissenschaft aufgebaut werden kann, Curran, 2009, S. 1). Die Idee, dass Wissenschaft in Abhängigkeit von der Menge der gesammelten Evidenz und Daten immer vollständigere und genauere Theorien entwickelt. Die kumulative Wissenschaft entwickelt sich in graduellen und aufeinander aufbauenden Schritten, im Gegensatz zu einer plötzlichen Entdeckung. Während revolutionäre Wissenschaft selten vorkommt, ist die kumulative Wissenschaft die häufigste Form der Wissenschaft.",
                "related_terms": [
                    "Slow Science"
                ],
                "references": "Curran, P. J. (2009). The seemingly quixotic pursuit of a cumulative psychological science: Introduction to the special issue. Psychological Methods, 14(2), 77–80. https://doi.org/10.1037/a0015972\n\nd’Espagnat, B. (2008). Is science cumulative? A physicist viewpoint. In Rethinking Scientific Change and Theory Comparison (pp. 145–151). Springer. https://doi.org/10.1007/978-1-4020-6279-7_10\n\nKuhn, T. (1962). The Structure of Scientific Revolutions. University of Chicago Press.\n\nMischel, W. (2009). Becoming a Cumulative Science. Association for Psychological Science. https://www.psychologicalscience.org/observer/becoming-a-cumulative-science",
                "drafted_by": [
                    "Beatrice Valentini"
                ],
                "reviewed_by": [
                    "Sarah Ashcroft-Jones",
                    "Mahmoud Elsherif",
                    "Helena Hartmann",
                    "Oscar Lecuona",
                    "Wanyin Li",
                    "Sonia Rishi",
                    "Flávio Azevedo"
                ],
                "alt_related_terms": [
                    null
                ],
                "language": "german"
            },
            {
                "type": "glossary",
                "title": "DA-RT (Data Access and Research Transparency (DA-RT))",
                "definition": "Data Access and Research Transparency\" ([DA-RT](https://www.dartstatement.org/); dt. Datenzugang und Forschungstransparenz) ist eine Initiative, die den Datenzugang und die Forschungstransparenz in den Sozialwissenschaften verbessern soll. Es handelt sich um eine multi-erkenntnistheoretische und multimethodische Initiative, die 2014 vom Rat der American Political Science Association (APSA) ins Leben gerufen wurde, um die Genauigkeit (rigor) der empirischen Sozialforschung zu stärken. Neben anderen Aktivitäten hat DA-RT die Transparenzerklärung der Herausgeber:innen von Zeitschriften (Journal Editors' Transparency Statement, JETS) entwickelt. Diese verlangt von den unterzeichnenden Zeitschriften (a) relevante Daten öffentlich zugänglich zu machen, wenn die Studie veröffentlicht wird, (b) strengen Datenzitierregeln zu folgen, (c) die Analyseverfahren transparent zu beschreiben und, wenn möglich, öffentlichen Zugang zum Analysecode zu gewähren und (d) ihre Zeitschriftenleitfäden und Ethikerklärungen zu aktualisieren, um verbesserte Anforderungen an den Datenzugang und die Forschungstransparenz einzubeziehen.",
                "related_terms": [
                    "Accessibility",
                    "Data sharing",
                    "Replicability",
                    "Reproducibility"
                ],
                "references": "Carsey, T. M. (2014). Making DA-RT a reality. PS: Political Science & Politics, 47(1), 72–77. https://doi.org/10.1017/S1049096513001753\n\nMonroe, K. R. (2018). The rush to transparency: DA-RT and the potential dangers for qualitative research. Perspectives on Politics, 16(1), 141–148. https://doi.org/10.1017/S153759271700336X",
                "drafted_by": [
                    "Eike Mark Rinke"
                ],
                "reviewed_by": [
                    "Filip Dechterenko",
                    "Kai Krautter",
                    "Flávio Azevedo"
                ],
                "alt_related_terms": [
                    null
                ],
                "language": "german"
            },
            {
                "type": "glossary",
                "title": "DMP; Datenmanagementplan (Data management plan (DMP))",
                "definition": "Ein strukturiertes Dokument, das den Prozess der Datenerhebung, \\-analyse, \\-verwaltung und \\-speicherung während eines Forschungsprojekts beschreibt. Es beschreibt auch die Eigentumsrechte an den Daten und wie die Daten während und nach Abschluss eines Projekts aufbewahrt und geteilt werden. Vorlagen für das Datenmanagement bieten auch Anhaltspunkte dafür, wie Forschungsdaten FAIR und, wenn möglich, offen zugänglich gemacht werden können.",
                "related_terms": [
                    "Data archiving",
                    "Data sharing",
                    "Data storage",
                    "FAIR principles",
                    "Open data"
                ],
                "references": "Burnette, M., Williams, S., & Imker, H. (2016). From Plan to Action: Successful Data Management Plan Implementation in a Multidisciplinary Project. Journal of eScience Librarianship, 5(1), e1101. https://doi.org/10.7191/jeslib.2016.1101\n\nMichener, W. K. (2015). Ten simple rules for creating a good data management plan. PLoS Computational Biology, 11(10), e1004525. https://doi.org/10.1371/journal.pcbi.1004525",
                "drafted_by": [
                    "Dominique Roche"
                ],
                "reviewed_by": [
                    "Charlotte R. Pennington",
                    "Sam Parsons",
                    "Birgit Schmidt",
                    "Flávio Azevedo"
                ],
                "alt_related_terms": [
                    null
                ],
                "language": "german"
            },
            {
                "type": "glossary",
                "title": "Data sharing",
                "definition": "Sammlung von Praktiken, Technologien, kulturellen Elementen und rechtlichen Rahmenbedingungen, die für die Bereitstellung von Daten, die für wissenschaftliche Forschung verwendet wurden, an andere Forschende relevant sind. Gollwitzer et al. (2020) beschreiben zwei Arten der gemeinsamen Nutzung von Daten: Typ 1: Daten, die notwendig sind, um die Ergebnisse eines veröffentlichten Forschungsartikels zu reproduzieren. Typ 2: Daten, die im Rahmen eines Forschungsprojekts erhoben wurden, aber nach Abschluss des Projekts nicht (oder nur teilweise) analysiert oder veröffentlicht wurden und daher in der Regel unter einer bestimmten Sperrfrist freigegeben werden.",
                "related_terms": [
                    "FAIR principles",
                    "Open data"
                ],
                "references": "Abele-Brehm, A. E., Gollwitzer, M., Steinberg, U., & Schönbrodt, F. D. (2019). Attitudes toward open science and public data sharing. Social Psychology, 50, 252–260. https://doi.org/10.1027/1864-9335/a000384\n\nGollwitzer, M., Abele-Brehm, A., Fiebach, C., Ramthun, R., Scheel, A. M., Schönbrodt, F. D., & Steinberg, U. (2020). Data Management and Data Sharing in Psychological Science: Revision of the DGPs Recommendations.\n\nfor Data Sharing, S. C. (n.d.). What is data sharing? Retrieved 11 July 2021. https://eudatasharing.eu/what-data-sharing",
                "drafted_by": [
                    "Mahmoud Elsherif"
                ],
                "reviewed_by": [
                    "Helena Hartmann",
                    "Tina Lonsdorf",
                    "Charlotte R. Pennington",
                    "Timo Roettger",
                    "Flávio Azevedo"
                ],
                "alt_related_terms": [
                    null
                ],
                "language": "german"
            },
            {
                "type": "glossary",
                "title": "Datenvisualisierung (Data visualisation)",
                "definition": "Grafische Darstellung von Daten oder Informationen. Die Datenvisualisierung nutzt die gut entwickelte visuelle Verarbeitungskapazität des Menschen, um Erkenntnisse und wichtige Informationen zu vermitteln. Datenvisualisierungen zeigen oft die Rohdaten, deskriptive Statistiken und/oder Inferenzstatistiken.",
                "related_terms": [
                    "Figure",
                    "Graph",
                    "Plot"
                ],
                "references": "Healy, K. (2018). Data visualization: A practical introduction. Princeton University Press.\n\nTufte, E. R. (1983). The visual display of quantitative information. Graphics Press.",
                "drafted_by": [
                    "Bradley Baker"
                ],
                "reviewed_by": [
                    "Mahmoud Elsherif",
                    "Charlotte R. Pennington",
                    "Suzanne L. K. Stewart;"
                ],
                "alt_related_terms": [
                    null
                ],
                "language": "german"
            },
            {
                "type": "glossary",
                "title": "Dekolonialisierung (Decolonisation)",
                "definition": "Kolonialität kann als die Etablierung / Einbürgerung von Konzepten wie Imperialismus, Kapitalismus und Nationalismus beschrieben werden. Zusammen können diese Konzepte als eine Matrix der Macht (und der Machtbeziehungen) betrachtet werden, die sich auf die Kolonialzeit zurückführen lässt. Die Dekolonialisierung zielt darauf ab, diese Machtverhältnisse aufzubrechen und zu dezentralisieren, um ihr Fortbestehen zu verstehen und die Normen und Werte eines bestimmten Bereichs zu rekonstruieren. Im akademischen Umfeld bedeutet Dekolonisierung, dass wir die Sichtweise, durch die wir lehren, forschen und zusammenleben, überdenken, so dass sie über westlich orientierte und koloniale Perspektiven hinausgeht. Die Entkolonialisierung der Wissenschaft beinhaltet die Rekonstruktion der verwendeten historischen und kulturellen Rahmenbedingungen, die Neuverteilung des Zugehörigkeitsgefühls an den Universitäten und die Ermächtigung und Einbeziehung von Stimmen und Wissensarten, die in der Vergangenheit von der Wissenschaft ausgeschlossen waren. Dies geschieht, wenn Menschen sich mit ihrer Vergangenheit, Gegenwart und Zukunft auseinandersetzen und dabei eine Perspektive einnehmen, die sich von der gesellschaftlich dominanten Perspektive unterscheidet. Dies geschieht auch dadurch, dass die verinnerlichten Normen und Tabus der jeweiligen Kolonie berücksichtigt und nicht abgelehnt werden.",
                "related_terms": [
                    "Diversity",
                    "Equity",
                    "Inclusion"
                ],
                "references": "Albayrak, N. (2018). Diversity helps but decolonisation is the key to equality in higher education. Retrieved from https://lsepgcertcitl.wordpress.com/2018/04/16/diversity-helps-but-decolonisation-is-the-key-to-equality-in-higher-education/",
                "drafted_by": [
                    "Nihan Albayrak-Aydemir"
                ],
                "reviewed_by": [
                    "Sarah Ashcroft-Jones",
                    "Mahmoud Elsherif",
                    "Michele C. Lim",
                    "Emma Norris",
                    "Flávio Azevedo"
                ],
                "alt_related_terms": [
                    null
                ],
                "language": "german"
            },
            {
                "type": "glossary",
                "title": "Demarkationskriterium (Demarcation criterion)",
                "definition": "Ein Kriterium zur Unterscheidung zwischen Wissenschaft und Nicht-Wissenschaft, das einen optimalen Weg für die Ansammlung des Wissens der Welt aufzeigen soll. In einem Popper’schen Ansatz war das Demarkationskriterium oder Abgrenzungskriterium die Falsifizierbarkeit und die Anwendung einer falsifizierenden Haltung. Zu den alternativen Ansätzen gehören der von Kuhn, der das Kriterium in der Lösung von Rätseln mit dem Ziel des Verständnisses der Natur sah, und der von Lakatos, der argumentierte, dass Wissenschaft durch die Arbeit innerhalb eines fortschreitenden Forschungsprogramms gekennzeichnet ist.",
                "related_terms": [
                    "Hypothesis",
                    "Falsification"
                ],
                "references": "Dienes, Z. (2008). Understanding psychology as a science: An introduction to scientific and statistical inference. Macmillan International Higher Education.",
                "drafted_by": [
                    "Alaa AlDoh"
                ],
                "reviewed_by": [
                    "Bethan Iley",
                    "Sara Middleton"
                ],
                "alt_related_terms": [
                    null
                ],
                "language": "german"
            },
            {
                "type": "glossary",
                "title": "digital object identifier (DOI (digital object identifier))",
                "definition": "Digital Object Identifiers (DOI) sind alphanumerische Zeichenfolgen, die jeder beliebigen Einheit zugewiesen werden können, z. B. Veröffentlichungen (einschließlich Preprints), Materialien, Datensätze und Spielfilme \\- die Verwendung von DOIs ist nicht nur auf wissenschaftliches oder akademisches Material beschränkt. DOIs “provides a system for persistent and actionable identification and interoperable exchange of managed information on digital networks.” (dt. bieten ein System für die dauerhafte und umsetzbare Identifizierung und den interoperablen Austausch von verwalteten Informationen in digitalen Netzwerken; https://www.doi.org/doi-handbook/HTML/index.html). Es gibt viele verschiedene DOI-Registrierungsstellen, die DOIs betreiben, aber die beiden, auf die Forschende am ehesten stoßen, sind [Crossref](https://www.crossref.org/) und [Datacite](https://datacite.org/).",
                "related_terms": [
                    "arXiv and BibTex",
                    "Crossref, Datacite, ISBN, ISO, ORCID",
                    "Permalink"
                ],
                "references": "Bilder, G. (2013). DOIs unambiguously and persistently identify published, trustworthy, citable online scholarly literature. Right? https://www.crossref.org/blog/dois-unambiguously-and-persistently-identify-published-trustworthy-citable-online-scholarly-literature-right/\n\nMorgan, C. (1998). The DOI (Digital Object Identifier). Serials, 11(1), 47–51. http://doi.org/10.1629/1147\n\nAnon. (2019). The DOI Handbook.",
                "drafted_by": [
                    "Tina Lonsdorf"
                ],
                "reviewed_by": [
                    "Ashley Blake",
                    "Helena Hartmann",
                    "Sam Parsons",
                    "Charlotte R. Pennington",
                    "Flávio Azevedo"
                ],
                "alt_related_terms": [
                    null
                ],
                "language": "german"
            },
            {
                "type": "glossary",
                "title": "Doppelblinde Begutachtung (Double-blind peer review)",
                "definition": "Bewertung von Forschungsprodukten durch qualifizierte Expert:innen, wobei sowohl der/die Autor:in(nen) als auch der/die Gutachter:in(nen) anonym bleiben. \"This approach conceals the identity of the authors and their affiliations from reviewers and would, in theory, remove biases of professional reputation, gender, race, and institutional affiliation, allowing the reviewer to avoid bias and to focus on the manuscript’s merit alone” (dt. Dieser Ansatz verbirgt die Identität der Autor:innen und ihre  Affiliation vor den Begutachtenden und würde, theoretisch, Vorurteile aufgrund von beruflicher Reputation, Geschlecht, Herkunft und institutioneller Zugehörigkeit beseitigen, so dass die Begutachtenden Voreingenommenheit vermeiden und sich allein auf die Leistung des Manuskripts konzentrieren können, Tvina et al., 2019, 1082). Wie alle Arten von Peer-Review ist auch die doppelblinde Begutachtung nicht ohne Schwierigkeiten. Anonymität kann für bestimmte Forschende, die in einem Nischenbereich arbeiten, schwierig, wenn nicht gar unmöglich zu erreichen sein.",
                "related_terms": [
                    "Ad hominem bias",
                    "Affiliation bias",
                    "Anonymous review",
                    "Masked review",
                    "Open peer review",
                    "Peer review",
                    "Single-blind peer review",
                    "Traditional peer review",
                    "Triple-Blind peer review"
                ],
                "references": "Largent, E. A., & Snodgrass, R. T. (2016). Blind peer review by academic journals. In C. T. Robertson & A. S. Kesselheim (Eds.), Blinding as a solution to bias: Strengthening biomedical science, forensic science, and law (pp. 75–95). Academic Press. https://doi.org/10.1016/B978-0-12-802460-7.00005-X\n\nTvina, A., Spellecy, R., & Palatnik, A. (2019). Bias in the peer review process: can we do better? Obstetrics & Gynecology, 133(6), 1081–1083. https://doi.org/10.1097/AOG.0000000000003260",
                "drafted_by": [
                    "Mahmoud Elsherif"
                ],
                "reviewed_by": [
                    "Bradley Baker",
                    "Helena Hartmann",
                    "Meng Liu",
                    "Emma Norris"
                ],
                "alt_related_terms": [
                    null
                ],
                "language": "german"
            },
            {
                "type": "glossary",
                "title": "Doppeltes Bewusstsein (Double consciousness)",
                "definition": "Eine Identitätsverwirrung, da der Einzelne das Gefühl hat, zwei verschiedene Identitäten zu haben. Die eine ist die Anpassung an die dominante Kultur an der Universität, wenn die/der Einzelne mit Kolleg:innen und Professor:innen zusammen ist, die andere, wenn sie/er mit der Familie zusammen ist. Dieser ständige Wechsel kann dazu führen, dass man sich seiner Identität nicht sicher ist und glaubt, nirgendwo richtig dazuzugehören. Dieser Mangel an Zugehörigkeit kann zu einer schlechten sozialen Integration innerhalb der akademischen Kultur führen, die sich in geringeren Chancen und mehr psychischen Problemen bei der Person äußern kann (Rubin, 2021; Rubin et al., 2019).",
                "related_terms": [
                    "Social class",
                    "Social integration"
                ],
                "references": "Albayrak, N., & Okoroji, C. (2019). Facing the challenges of postgraduate study as a minority student. A Guide for Psychology Postgraduates, 63.\n\nDu Bois, W. E. B. (1968). The souls of black folk; essays and sketches. Johnson Reprint Corp.\n\nGilroy, P. (1993). The black Atlantic: Modernity and double consciousness. Harvard University Press.",
                "drafted_by": [
                    "Nihan Albayrak-Aydemir"
                ],
                "reviewed_by": [
                    "Mahmoud Elsherif",
                    "Wanyin Li",
                    "Michele C. Lim",
                    "Adam Parker"
                ],
                "alt_related_terms": [
                    null
                ],
                "language": "german"
            },
            {
                "type": "glossary",
                "title": "DORA",
                "definition": "Die San Francisco Declaration on Research Assessment (DORA) ist eine globale Initiative, die darauf abzielt, die Abhängigkeit von Zeitschriften-basierten Metriken (z. B. Impact Faktor und Zitationszahlen) zu verringern und stattdessen eine Kultur zu fördern, die den intrinsischen Wert der Forschung betont. Die DORA-Erklärung richtet sich an Forschungsfördende, Verlage, Forschungsinstitute und Forschende, die sich mit ihrer Unterschrift verpflichten, ihre Forschungspraktiken und \\-verfahren an die Grundsätze der Erklärung anzupassen.",
                "related_terms": [
                    "Generalizability",
                    "Journal Impact Factor",
                    "Open Science"
                ],
                "references": "Health Research Board. (n.d.). Declaration on Research Assessment. Retrieved from https://www.hrb.ie/funding/funding-schemes/before-you-apply/how-we-assess-applications/declaration-on-research-assessment/",
                "drafted_by": [
                    "Aoife O’Mahony"
                ],
                "reviewed_by": [
                    "Connor Keating",
                    "Charlotte R. Pennington"
                ],
                "alt_related_terms": [
                    null
                ],
                "language": "german"
            },
            {
                "type": "glossary",
                "title": "direkte Replikation (Direct replication)",
                "definition": "Da der Begriff \"direkte Replikation\" keine allgemein anerkannte technische Bedeutung hat und es keine eindeutige Unterscheidung zwischen einer direkten und einer konzeptionellen Replikation gibt, werden im Folgenden einige Beiträge zur Erzielung eines Konsenses aufgeführt. Anstatt über die \"Genauigkeit\" einer Replikation zu debattieren, ist es hilfreicher, die relevanten Unterschiede zwischen einer Replikation und der zu replizierenden Arbeit, sowie deren Auswirkungen auf die Zuverlässigkeit und Generalisierbarkeit der Ergebnisse der ursprünglichen Arbeit zu diskutieren. Im Allgemeinen bezieht sich die direkte Replikation auf eine neue Datenerhebung, die versucht, die Methoden der ursprünglichen Studie so genau wie möglich zu replizieren. Ein Replikationsversuch, der \"seek(s) to duplicate the necessary elements that produced the original finding.” (dt. versucht, die notwendigen Elemente zu duplizieren, die zu den ursprünglichen Ergebnissen geführt haben, Cruwell et al., 2019; p. 243). Der Zweck einer direkten Replikation kann darin bestehen, Fehler vom Typ 1 und/oder Versuchsleiter:innen-Effekte zu identifizieren, die Replizierbarkeit eines Effekts unter Verwendung derselben oder verbesserter Verfahren zu bestimmen oder spezifischere Schätzungen der Effektgröße zu erstellen (Hüffmeier et al., 2016). Die Direktheit der Replikation ist ein Kontinuum zwischen der Wiederholung spezifischer Beobachtungen (Daten) und der Beobachtung verallgemeinerter Effekte (Phänomene). Wie genau eine Replikation eine ursprüngliche Studie nachbildet, ist oft umstritten, wobei Unterschiede oft als versteckte Moderatoren von Effekten angeführt werden. Darüber hinaus kann eine Debatte über die Bedeutung der technischen Äquivalenz (d. h. die Verwendung identischer Materialien) gegenüber der psychologischen Äquivalenz (d. h. die Realisierung identischer psychologischer Bedingungen) der Originalstudie geführt werden (Schwarz and Strack, 2014). Nehmen wir zum Beispiel eine Studie zum Vertrauen in den US-Präsidenten aus dem Jahr 2018\\. Eine technisch gleichwertige Replikation würde Trump als Stimulus verwenden (er war 2018 Präsident), eine psychologisch gleichwertige Studie im Jahr 2023 würde Biden als aktuellen Präsidenten verwenden.",
                "related_terms": [
                    "close replication",
                    "Conceptual replication",
                    "exact replication",
                    "hidden moderators"
                ],
                "references": "Crüwell, S., van Doorn, J., Etz, A., Makel, M. C., Moshontz, H., Niebaum, J. C., Orben, A., Parsons, S., & Schulte-Mecklenbeck, M. (2019). Seven Easy Steps to Open Science: An Annotated Reading List. Zeitschrift Für Psychologie, 227(4), 237–248. https://doi.org/10.1027/2151-2604/a000387\n\nHüffmeier, J., Mazei, J., & Schultze, T. (2016). Reconceptualizing replication as a sequence of different studies: A replication typology. Journal of Experimental Social Psychology, 66, 81–92. https://doi.org/10.1016/j.jesp.2015.09.009\n\nLeBel, E. P., Vanpaemel, W., Cheung, I., & Campbell, L. (2017). A brief guide to evaluate replications. Meta-Psychology, 3. https://doi.org/10.15626/MP.2018.843\n\nSchwarz, N., & Strack, F. (2014). Does Merely Going Through the Same Moves Make for a “Direct” Replication?: Concepts, Contexts, and Operationalizations. Social Psychology, 45(4), 305–306.",
                "drafted_by": [
                    "Mahmoud Elsherif (original); Thomas Rhys Evans (alternative); Tina Lonsdorf (alternative)"
                ],
                "reviewed_by": [
                    "Beatrix Arendt",
                    "Adrien Fillon",
                    "Matt Jaquiery",
                    "Charlotte R. Pennington",
                    "Graham Reid",
                    "Lisa Spitzer",
                    "Tobias Wingen",
                    "Flávio Azevedo"
                ],
                "alt_related_terms": [
                    null
                ],
                "language": "german"
            },
            {
                "type": "glossary",
                "title": "Diversity",
                "definition": "Diversity (Diversität) bezieht sich auf die zwischenmenschliche (d. h. interindividuelle) Variation bei Menschen, z. B. in Bezug auf Fähigkeiten, Alter, Überzeugungen, Kognition, Land, Behinderung, ethnische Zugehörigkeit, Geschlecht, Sprache, Herkunft, Religion oder sexuelle Orientierung. Diversität kann sich auf die Vielfalt der Forschenden (die die Forschung durchführen), die Vielfalt der Teilnehmendenstichproben (die in die Studie einbezogen werden) und die Vielfalt der Perspektiven (die Ansichten und Überzeugungen der Forschenden) beziehen.",
                "related_terms": [
                    "Bropenscience",
                    "BIZARRE",
                    "Decolonisation",
                    "Double Consciousness",
                    "Equity",
                    "Inclusion",
                    "STRANGE",
                    "WEIRD"
                ],
                "references": "Syed, M., & Kathawalla, U. (2020). Cultural Psychology, Diversity, and Representation in Open Science. https://doi.org/10.31234/osf.io/t7hp2",
                "drafted_by": [
                    "Ryan Millager; Mariella Paul"
                ],
                "reviewed_by": [
                    "Nihan Albayrak-Aydemir",
                    "Mahmoud Elsherif",
                    "Helena Hartmann",
                    "Madeleine Ingham",
                    "Annalise A. LaPlume",
                    "Wanyin Li",
                    "Charlotte R. Pennington",
                    "Olly Robertson",
                    "Flávio Azevedo"
                ],
                "alt_related_terms": [
                    null
                ],
                "language": "german"
            },
            {
                "type": "glossary",
                "title": "ECRs, Nachwuchswissenschaftler:innen (Early career researchers (ECRs))",
                "definition": "Eine Bezeichnung für Forschende, die von erfahrenen Doktorand:innen bis hin zu Postdoktorand:innen reichen, die bis zu zehn Jahre nach der Promotion ausgebildet wurden; die letztgenannte Gruppe kann daher auch Juniorprofessor:innen umfassen (Eley et al., 2012, S. 3). Was genau (z. B. Alter, Zeit seit der Promotion einschließlich oder ausschließlich von Karriere-Unterbrechungen und Urlaub / Freistellungen, Titel, bewilligte Fördermittel) eine/n ECR ausmacht, kann je nach Fördereinrichtung, akademischer Organisation und Land variieren.",
                "related_terms": [
                    "Early Career Investigator"
                ],
                "references": "Bazeley, P. (2003). Defining “Early Career” in Research. Higher Education, 45, 257–279. https://doi.org/10.1023/A:1022698529612\n\nEley, A. R. (2012). Becoming a successful early career researcher. Routledge. Retrieved from http://www.worldcat.org/oclc/934369360\n\nPownall, M., Talbot, C. V., Henschel, A., Lautarescu, A., Lloyd, K. E., Hartmann, H., Darda, K. M., Tang, K. T. Y., Carmichael-Murphy, P., & Siegel, J. A. (2021). Navigating Open Science as Early Career Feminist Researchers. Psychology of Women Quarterly, 45(4), 526–539. https://doi.org/10.1177/03616843211029255 Retrieved from https://journals.sagepub.com/doi/10.1177/03616843211029255",
                "drafted_by": [
                    "Micah Vandegrift"
                ],
                "reviewed_by": [
                    "Thomas Rhys Evans",
                    "Sam Parsons",
                    "Olly Robertson",
                    "Suzanne L. K. Stewart",
                    "Flávio Azevedo"
                ],
                "alt_related_terms": [
                    null
                ],
                "language": "german"
            },
            {
                "type": "glossary",
                "title": "Economic and societal impact",
                "definition": "Der Beitrag, den ein Forschungsgegenstand für die Wirtschaft und Gesellschaft im weiteren Sinne leistet. Erfasst wird auch der Nutzen der Forschung für Einzelpersonen, Organisationen und/oder Nationen.",
                "related_terms": [
                    "Academic Impact"
                ],
                "references": "Economic, & Council, S. R. (n.d.). What is impact? Retrieved 8 July 2021. https://esrc.ukri.org/research/impact-toolkit/what-is-impact/",
                "drafted_by": [
                    "Adam Parker"
                ],
                "reviewed_by": [
                    "Helena Hartmann",
                    "Aoife O’Mahony",
                    "Charlotte R. Pennington"
                ],
                "alt_related_terms": [
                    null
                ],
                "language": "german"
            },
            {
                "type": "glossary",
                "title": "Sperrfrist (Embargo Period)",
                "definition": "Angewandt auf Open Scholarship (dt. Offene Forschung) beschreibt es im akademischen Verlagswesen die Zeitspanne, nachdem ein Artikel veröffentlicht wurde und bevor er offen (\"open-access\") zugänglich gemacht werden kann. Wenn Autor:innen beschließen, ihren Artikel selbst zu archivieren (z. B. in einem Open-Access-Repositorium), müssen sie alle Sperrfristen beachten. Sperrfristen variieren von sofort bis zu 48 Monaten, wobei 6 und 12 Monate üblich sind (Laakso & Björk, 2013). Sperrfristen können auch für Präregistrierungen, Materialien, und Daten gelten, wenn Autor:innen beschließen, diese erst nach einer bestimmten Zeit der Öffentlichkeit zur Verfügung zu stellen, beispielsweise bei der Veröffentlichung der Publikation oder sogar später, wenn sie zusätzliche Veröffentlichungspläne haben und vermeiden wollen, dass man ihnen zuvorkommt (Klein et al., 2018).",
                "related_terms": [
                    "Open access",
                    "Paywall",
                    "Preprint"
                ],
                "references": "Klein, O., Hardwicke, T. E., Aust, F., Breuer, J., Danielsson, H., Mohr, A. H., IJzerman, H., Nilsonne, G., Vanpaemel, W., & Frank, M. C. (2018). A practical guide for transparency in psychological science. Collabra: Psychology, 4(1), 20. https://doi.org/10.1525/collabra.158\n\nLaakso, M., & Björk, B. C. (2013). Delayed open access: An overlooked high‐impact category of openly available scientific literature. Journal of the American Society for Information Science and Technology, 64(7), 1323–1329.\n\nEmbargo (academic publishing). (2021). https://en.wikipedia.org/w/index.php?title=Embargo_(academic_publishing)&oldid=1016895567",
                "drafted_by": [
                    "Aleksandra Lazić"
                ],
                "reviewed_by": [
                    "Bradley Baker",
                    "Adam Parker",
                    "Sam Parsons",
                    "Steven Verheyen",
                    "Flávio Azevedo"
                ],
                "alt_related_terms": [
                    null
                ],
                "language": "german"
            },
            {
                "type": "glossary",
                "title": "epistemische Unsicherheit (Epistemic uncertainty)",
                "definition": "Systematische Unsicherheit aufgrund begrenzter Daten, Messgenauigkeit, Modell- oder Prozessspezifikation oder mangelnden Wissens. Das heißt, Unsicherheit aufgrund mangelnden Kenntnisstands, die theoretisch durch zusätzliche Forschung zur Verbesserung des Verständnisses verringert werden könnte. Eine solche Unsicherheit wird als persönlich bezeichnet, da sich das Wissen der Forschenden unterscheidet, und als vorübergehend, da sie sich ändern kann, wenn neue Daten verfügbar werden.",
                "related_terms": [
                    "Aleatoric uncertainty",
                    "Knightian uncertainty"
                ],
                "references": "Der Kiureghian, A., & Ditlevsen, O. (2009). Aleatory or epistemic? Does it matter? Structural Safety, 31(2), 105–112. https://doi.org/10.1016/j.strusafe.2008.06.020\n\nFerson, S., Joslyn, C. A., Helton, J. C., Oberkampf, W. L., & Sentz, K. (2004). Summary from the epistemic uncertainty workshop: consensus amid diversity. Reliability Engineering & System Safety, 85(1–3), 355–369. https://doi.org/10.1016/j.ress.2004.03.023",
                "drafted_by": [
                    "Bradley Baker"
                ],
                "reviewed_by": [
                    "Jamie P. Cockcroft",
                    "Elizabeth Collins",
                    "Charlotte R. Pennington",
                    "Graham Reid"
                ],
                "alt_related_terms": [
                    null
                ],
                "language": "german"
            },
            {
                "type": "glossary",
                "title": "Epistemiologie / Erkenntnistheorie (Epistemology)",
                "definition": "Neben der Ethik, der Logik und der Metaphysik ist die Erkenntnistheorie einer der vier Hauptzweige der Philosophie. Die Erkenntnistheorie befasst sich weitgehend mit der Natur, dem Ursprung und dem Umfang des Wissens sowie mit der Rationalität von Überzeugungen.",
                "related_terms": [
                    "Meta-science or Meta-research ",
                    "Ontology (Artificial Intelligence)"
                ],
                "references": "Steup, M., & Neta, R. (2020). Epistemology. Stanford Encyclopedia of Philosophy. https://plato.stanford.edu/entries/epistemology/",
                "drafted_by": [
                    "Amélie Beffara Bret"
                ],
                "reviewed_by": [
                    "Emma Norris",
                    "Adam Parker",
                    "Robert M Ross",
                    "Steven Verheyen"
                ],
                "alt_related_terms": [
                    null
                ],
                "language": "german"
            },
            {
                "type": "glossary",
                "title": "Gleichstellung (Equity)",
                "definition": "Verschiedene Personen haben unterschiedliche Ausgangspositionen (vgl. \"Chancenlücken\") und Bedürfnisse. Während sich die Gleichbehandlung (\"equal treatment\") darauf konzentriert, alle Personen *gleich* zu behandeln, zielt die Gleichstellung darauf ab, die Chancengleichheit zu erhöhen, indem mehr Chancen für unterrepräsentierte Minderheiten hergestellt werden. Die Gleichstellung soll durch Fairness erreicht werden, d. h. durch die Berücksichtigung der unterschiedlichen Unterstützungsbedürfnisse der einzelnen Personen, anstatt sich nur auf die Bedürfnisse der Mehrheit zu konzentrieren.",
                "related_terms": [
                    "Diversity",
                    "Equality",
                    "Fairness",
                    "Inclusion",
                    "Social justice"
                ],
                "references": "Albayrak-Aydemir, N. (2020). The hidden costs of being a scholar from the global south. Retrieved from https://blogs.lse.ac.uk/highereducation/2020/02/20/the-hidden-costs-of-being-a-scholar-from-the-global-south/\n\nPosselt, J. R. (2020). Equity in Science: Representation, Culture, and the Dynamics of Change in Graduate Education. Stanford University Press. https://books.google.de/books?id=2CjwDwAAQBAJ",
                "drafted_by": [
                    "Gisela H. Govaart"
                ],
                "reviewed_by": [
                    "Nihan Albayrak-Aydemir",
                    "Mahmoud Elsherif",
                    "Ryan Millager",
                    "Charlotte R. Pennington",
                    "Beatrice Valentini",
                    "Flávio Azevedo"
                ],
                "alt_related_terms": [
                    null
                ],
                "language": "german"
            },
            {
                "type": "glossary",
                "title": "Äquivalenztesten (Equivalence Testing)",
                "definition": "Äquivalenztests bewerten statistisch die Nullhypothese, dass ein bestimmter Effekt ein Minimalkriterium überschreitet, um als bedeutsam eingestuft zu werden. Die Ablehnung der Nullhypothese ist somit ein Beweis für das Fehlen eines (bedeutsamen) Effekts. Äquivalenztests basieren auf der frequentistischen Statistik und funktionieren durch die Festlegung von Äquivalenzgrenzen: einer unteren und einer oberen Grenze, die die kleinste interessierende Effektgröße widerspiegeln. Anschließend werden zwei einseitige *t*\\-Tests gegen jede dieser Äquivalenzgrenzen durchgeführt, um festzustellen, ob als bedeutsam erachteten Effekte *abgelehnt* werden können (siehe Schuirmann, 1972; Lakens et al., 2018; 2020).",
                "related_terms": [
                    "Equivalence bounds",
                    "Falsification",
                    "Frequentist analyses",
                    "Inference by confidence intervals",
                    "Null Hypothesis Significance Testing (NHST)",
                    "Smallest effect size of interest (SESOI)",
                    "TOSTER",
                    "TOST procedure."
                ],
                "references": "Lakens, D., Scheel, A. M., & Isager, P. M. (2018). Equivalence testing for psychological research: A tutorial. Advances in Methods and Practices in Psychological Science, 1(2), 259–269. https://doi.org/10.1177/2515245918770963\n\nLakens, D., McLatchie, N., Isager, P. M., Scheel, A. M., & Dienes, Z. (2020). Improving inferences about null effects with Bayes factors and equivalence tests. The Journals of Gerontology: Series B, 75(1), 45–57. https://doi.org/10.1093/geronb/gby065\n\nSchuirmann, D. J. (1987). A comparison of the two one-sided tests procedure and the power approach for assessing the equivalence of average bioavailability. Journal of Pharmacokinetics and Biopharmaceutics, 15, 657–680. https://doi.org/10.1007/BF01068419",
                "drafted_by": [
                    "Charlotte R. Pennington"
                ],
                "reviewed_by": [
                    "Bradley Baker",
                    "James E. Bartlett",
                    "Jamie P. Cockcroft",
                    "Tobias Wingen",
                    "Flávio Azevedo"
                ],
                "alt_related_terms": [
                    null
                ],
                "language": "german"
            },
            {
                "type": "glossary",
                "title": "Fehlererfassung (Error detection)",
                "definition": "Bezieht sich allgemein auf die Prüfung von Forschungsdaten und Manuskripten auf Fehler oder Unstimmigkeiten in der Berichterstattung. Zu den häufig diskutierten Ansätzen gehören: die Überprüfung von Unstimmigkeiten in der deskriptiven Statistik (z. B. zusammenfassende Statistiken, die angesichts des Stichprobenumfangs und der Messmerkmale nicht möglich sind; Brown & Heathers, 2017; Heathers et al. 2018), Unstimmigkeiten in den berichteten Statistiken (z. B. p-Werte, die nicht mit den berichteten F-Statistiken und den dazugehörigen Freiheitsgraden übereinstimmen; Epskamp, & Nuijten, 2016; Nuijten et al. 2016\\) und Bildmanipulationen (Bik et al., 2016). Die Erfassung von  Fehlern ist eine Motivation dafür, dass Daten und Analysecodes offen zugänglich sein sollen, so dass die Ergebnisse eines Manuskripts durch Gutachten bestätigt werden können oder, falls sie bereits veröffentlicht wurden, die Aufzeichnungen korrigiert werden können. Entdeckte Fehler können zu Korrekturen oder zum Rückzug veröffentlichter Artikel führen, auch wenn sich diese Maßnahmen oft verzögern, lange nachdem die fehlerhaften Ergebnisse die weitere Forschung beeinflusst und beeinträchtigt haben.",
                "related_terms": [
                    "Research integrity",
                    "correction",
                    "retraction"
                ],
                "references": "Bik, E. M., Casadevall, A., & Fang, F. C. (2016). The prevalence of inappropriate image duplication in biomedical research publications. MBio, 7(3), e00809-16.\n\nBrown, N. J., & Heathers, J. A. (2017). The grim test: A simple technique detects numerous anomalies in the reporting of results in psychology. Social Psychological and Personality Science, 8(4), 363–369.\n\nEpskamp, S., & Nuijten, M. B. (2016). statcheck: Extract statistics from articles and recompute p values. Retrieved from http://CRAN.R-project.org/package=statcheck\n\nHeathers, J. A., Anaya, J., van der Zee, T., & Brown, N. J. (2018). Recovering data from summary statistics: Sample Parameter Reconstruction via Iterative TEchniques (SPRITE). PeerJ Preprints, 6, e26968v1. https://doi.org/10.7287/peerj.preprints.26968v1\n\nNuijten, M. B., Hartgerink, C. H., van Assen, M. A., Epskamp, S., & Wicherts, J. M. (2016). The prevalence of statistical reporting errors in psychology (1985–2013). Behavior Research Methods, 48(4), 1205–1226.\n\nRetraction Watch. (n.d.). Retraction Watch. Retraction Watch. https://retractionwatch.com/",
                "drafted_by": [
                    "William Ngiam"
                ],
                "reviewed_by": [
                    "Ali H. Al-Hoorie",
                    "Jamie P. Cockcroft",
                    "Dominik Kiersz",
                    "Sam Parsons",
                    "Suzanne L. K. Stewart",
                    "Marta Topor"
                ],
                "alt_related_terms": [
                    null
                ],
                "language": "german"
            },
            {
                "type": "glossary",
                "title": "Evidenzsynthese (Evidence Synthesis)",
                "definition": "Hierbei handelt es sich um eine Forschungsmethode, die darauf abzielt, allgemeine Schlussfolgerungen zur Beantwortung einer Forschungsfrage zu einem bestimmten Thema, Phänomen oder Effekt zu ziehen, indem Forschungsergebnisse und Informationen aus vielen unterschiedlichen Quellen betrachtet werden. Informationen, die einer Synthese unterzogen werden, können sowohl aus qualitativen als auch aus quantitativen Studien gewonnen werden. Die zur Synthese der gesammelten Informationen verwendete Methode kann qualitativ (narrative Synthese), quantitativ (Metaanalyse) oder gemischt (Metasynthese, systematisches Mapping) sein. Evidenzsynthese ist vielseitig anwendbar und wird häufig im Kontext der Gesundheitsversorgung, der Öffentlichkeitspolitik sowie des Verständnisses und der Förderung spezifischer Forschungsbereiche eingesetzt.",
                "related_terms": [
                    "Literature Review",
                    "Meta-analysis",
                    "Meta-synthesis",
                    "Meta-science or Meta-research",
                    "Narrative review",
                    "Scoping review",
                    "Systematic map",
                    "Systematic review"
                ],
                "references": "for Evaluation, C. (n.d.). Evidence Synthesis. https://www.lshtm.ac.uk/research/centres/centre-evaluation/evidence-synthesis\n\nJames, K. L., Randall, N. P., & Haddaway, N. R. (2016). A methodology for systematic mapping in environmental sciences. Environmental Evidence, 5(1), 1–13. https://doi.org/10.1186/s13750-016-0059-6\n\nSiddaway, A. P., Wood, A. M., & Hedges, L. V. (2019). How to do a systematic review: a best practice guide for conducting and reporting narrative reviews, meta-analyses, and meta-syntheses. Annual Review of Psychology, 70, 747–770. https://doi.org/10.1146/annurev-psych-010418-102803",
                "drafted_by": [
                    "Marta Topor"
                ],
                "reviewed_by": [
                    "Aoife O’Mahony",
                    "Tamara Kalandadze",
                    "Adam Parker",
                    "Charlotte R. Pennington"
                ],
                "alt_related_terms": [
                    null
                ],
                "language": "german"
            },
            {
                "type": "glossary",
                "title": "Explorative Datenanalyse (Exploratory data analysis)",
                "definition": "Die explorative Datenanalyse (EDA) ist eine etablierte statistische Tradition, die konzeptionelle und computergestützte Instrumente zur Entdeckung von Mustern in Daten bereitstellt, um die Entwicklung und Verfeinerung von Hypothesen zu fördern. Diese Werkzeuge und Einstellungen ergänzen den Einsatz von Hypothesentests, die in der konfirmatorischen Datenanalyse (CDA) verwendet werden. Selbst wenn gut begründete Theorien vorliegen, kann EDA helfen, die Ergebnisse einer CDA zu interpretieren und unerwartete oder irreführende Muster in den Daten aufzudecken.",
                "related_terms": [
                    "Confirmatory analyses",
                    "Data-driven research",
                    "Exploratory research"
                ],
                "references": "Behrens, J. T. (1997). Principles and procedures of exploratory data analysis. Psychological Methods, 2(2), 131–160. https://doi.org/10.1037/1082-989X.2.2.131\n\nBox, G. E. P. (1976). Science and statistics. Journal of the American Statistical Association, 71(356), 791–799.\n\nTukey, J. W. (1977). Exploratory data analysis. Addison-Wesley.\n\nWagenmakers, E. J., Wetzels, R., Borsboom, D., van der Maas, H. L., & Kievit, R. A. (2012). An agenda for purely confirmatory research. Perspectives on Psychological Science, 7(6), 632–638. https://doi.org/10.1177/1745691612463078",
                "drafted_by": [
                    "Jenny Terry"
                ],
                "reviewed_by": [
                    "Helena Hartmann",
                    "Timo Roettger",
                    "Charlotte R. Pennington",
                    "Flávio Azevedo"
                ],
                "alt_related_terms": [
                    null
                ],
                "language": "german"
            },
            {
                "type": "glossary",
                "title": "externe Validität (External Validity)",
                "definition": "Ob die Ergebnisse einer wissenschaftlichen Studie auf andere Kontexte außerhalb des Studienkontextes verallgemeinert werden können (andere Maße, Rahmenbedingungen, Personen, Orte und Zeiten). Statistisch gesehen können Bedrohungen der externen Validität Wechselwirkungen widerspiegeln, bei denen die Wirkung eines Faktors (der unabhängigen Variable) von einem anderen Faktor (einer Störvariable) abhängt. Die externe Validität kann auch durch das Studiendesign eingeschränkt werden (z. B. durch eine künstliche Laborumgebung oder eine nicht repräsentative Stichprobe).",
                "related_terms": [
                    "Constraints on Generality (COG)",
                    "Internal validity",
                    "Generalizability",
                    "Representativity",
                    "Validity"
                ],
                "references": "Lynch, J. G., Jr. (1982). On the External Validity of Experiments in Consumer Research. Journal of Consumer Research, 9(3), 225. https://doi.org/10.1086/208919\n\nSteckler, A., & McLeroy, K. R. (2008). The Importance of External Validity. American Journal of Public Health, 98(1), 9–10. https://doi.org/10.2105/AJPH.2007.126847",
                "drafted_by": [
                    "Annalise A. LaPlume"
                ],
                "reviewed_by": [
                    "Mahmoud Elsherif",
                    "Helena Hartmann",
                    "Kai Krautter",
                    "Oscar Lecuona",
                    "Flávio Azevedo"
                ],
                "alt_related_terms": [
                    null
                ],
                "language": "german"
            },
            {
                "type": "glossary",
                "title": "Augenscheinvalidität (Face validity)",
                "definition": "Ein subjektives Urteil darüber, wie geeignet ein Maß auf den ersten Blick zu sein scheint, d. h. wie gut ein Maß operationalisiert ist. Zum Beispiel die Beurteilung, ob sich die Items eines Fragebogens auf ein interessierendes Konstrukt beziehen. Die Augenscheinvalidität ist mit der Konstruktvalidität verwandt, aber da sie subjektiv/informell ist, wird sie als eine einfache, aber schwache Form der Validität angesehen.",
                "related_terms": [
                    "Construct Validity",
                    "Content Validity",
                    "Logical Validity",
                    "Operationalization",
                    "Validity"
                ],
                "references": "Holden, R. B. (2010). Face Validity. In I. B. Weiner & W. E. Craighead (Eds.), The Corsini Encyclopedia of Psychology (4th ed.). Wiley. http://dx.doi.org/10.1002/9780470479216.corpsy0341",
                "drafted_by": [
                    "Annalise A. LaPlume"
                ],
                "reviewed_by": [
                    "Helena Hartmann",
                    "Kai Krautter",
                    "Adam Parker",
                    "Sam Parsons"
                ],
                "alt_related_terms": [
                    null
                ],
                "language": "german"
            },
            {
                "type": "glossary",
                "title": "FAIR Prinzipien (FAIR principles)",
                "definition": "Beschreibt das Prinzip, wissenschaftliche Materialien Findable (auffindbar), Accessible (zugänglich), Interoperable (interoperabel) und Reusable (wiederverwendbar) zu machen (FAIR). Auffindbar und Zugänglich beziehen sich darauf, wo die Materialien gespeichert sind (z. B. in Datenarchiven), während Interoperabel und Wiederverwendbar sich auf die Bedeutung von Datenformaten und deren mögliche künftige Veränderung konzentrieren.",
                "related_terms": [
                    "Metadata",
                    "Open Access",
                    "Open Code",
                    "Open Data",
                    "Open Material",
                    "Repository"
                ],
                "references": "Crüwell, S., van Doorn, J., Etz, A., Makel, M. C., Moshontz, H., Niebaum, J. C., Orben, A., Parsons, S., & Schulte-Mecklenbeck, M. (2019). Seven Easy Steps to Open Science: An Annotated Reading List. Zeitschrift Für Psychologie, 227(4), 237–248. https://doi.org/10.1027/2151-2604/a000387\n\nWilkinson, M. D., Dumontier, M., Aalbersberg, I. J., Appleton, G., Axton, M., Baak, A., & others. (2016). The FAIR Guiding Principles for scientific data management and stewardship. Scientific Data, 3(1), 1–9. https://doi.org/10.1038/sdata.2016.18",
                "drafted_by": [
                    "Sonia Rishi"
                ],
                "reviewed_by": [
                    "Mahmoud Elsherif",
                    "Sam Parsons",
                    "Charlotte R. Pennington"
                ],
                "alt_related_terms": [
                    null
                ],
                "language": "german"
            },
            {
                "type": "glossary",
                "title": "Feministische Psychologie (Feminist psychology)",
                "definition": "Mit einem besonderen Schwerpunkt auf Geschlecht und Sexualität befasst sich die feministische Psychologie von Natur aus mit Repräsentation, Diversität, Inklusion, Zugänglichkeit und Gleichberechtigung. Feministische Psychologie entstand ursprünglich aus dem Anliegen, die gelebten Erfahrungen von Mädchen und Frauen zu repräsentieren, hat sich aber seither zu einem differenzierteren, intersektionalen und umfassenden Anliegen für alle Aspekte der Gleichberechtigung entwickelt (z. B. Eagly & Riger, 2014). Feministische Psycholog:innen haben sich für eine strengere Berücksichtigung von Gleichheit, Vielfalt und Inklusion in Open-Science-Bereichen eingesetzt (Pownall et al., 2021).",
                "related_terms": [
                    "Inclusion",
                    "Positionality",
                    "Reflexivity",
                    "Under-representation",
                    "Equity"
                ],
                "references": "Eagly, A. H., & Riger, S. (2014). Feminism and psychology: Critiques of methods and epistemology. American Psychologist, 69(7), 685–702. https://doi.org/10.1037/a0037372\n\nGrzanka, P. R. (2020). From buzzword to critical psychology: An invitation to take intersectionality seriously. Women & Therapy, 43(3–4), 244–261.\n\nPownall, M., Talbot, C. V., Henschel, A., Lautarescu, A., Lloyd, K. E., Hartmann, H., Darda, K. M., Tang, K. T. Y., Carmichael-Murphy, P., & Siegel, J. A. (2021). Navigating Open Science as Early Career Feminist Researchers. Psychology of Women Quarterly, 45(4), 526–539. https://doi.org/10.1177/03616843211029255 Retrieved from https://journals.sagepub.com/doi/10.1177/03616843211029255",
                "drafted_by": [
                    "Madeleine Pownall"
                ],
                "reviewed_by": [
                    "Mahmoud Elsherif",
                    "Helena Hartmann",
                    "Kai Krautter",
                    "Charlotte R. Pennington"
                ],
                "alt_related_terms": [
                    null
                ],
                "language": "german"
            },
            {
                "type": "glossary",
                "title": "FLAE, Norm der Betonung der Erst- und Letztautor:innenschaft (First-last-author-emphasis norm (FLAE))",
                "definition": "Ein System der Autor:innenschaft, das die Reihenfolge der Autor:innen in Abhängigkeit von den Leistungen einer/eines bestimmten Autorin/Autors festlegt und gleichzeitig die erste und letzte Position der Autor:innenreihenfolge am höchsten bewertet. Nach diesem System werden die beiden Hauptautoren als erster und letzter Autor angegeben \\- die Reihenfolge der Autor:innen zwischen der ersten und letzten Position wird durch den Leistungsumfang in absteigender Reihenfolge bestimmt.",
                "related_terms": [
                    "Authorship",
                    "Author contributions",
                    "CreDit taxonomy"
                ],
                "references": "Tscharntke, T., Hochberg, M. E., Rand, T. A., Resh, V. H., & Krauss, J. (2007). Author sequence and credit for contributions in multiauthored publications. PLoS Biology, 5(1), e18. https://doi.org/10.1371/journal.pbio.0050018",
                "drafted_by": [
                    "Myriam A. Baum"
                ],
                "reviewed_by": [
                    "Charlotte R. Pennington"
                ],
                "alt_related_terms": [
                    null
                ],
                "language": "german"
            },
            {
                "type": "glossary",
                "title": "FORRT",
                "definition": "Framework for Open and Reproducible Research Training (dt. Konzept für offene und reproduzierbare Forschung und Lehre). Es zielt darauf ab, eine pädagogische Infrastruktur bereitzustellen, die die Lehre und Betreuung von offener und reproduzierbarer Forschung in Verbindung mit prototypischen Fächern in der Hochschulbildung anerkennt und unterstützt. FORRT ist bestrebt, eine effektive, sich entwickelnde und von der Gemeinschaft getragene Organisation zu sein, die für die pädagogischen Implikationen offener und reproduzierbarer Wissenschaft und die damit verbundenen Herausforderungen (d. h. Lehrplanreform, epistemologische Unsicherheit, Lehrmethoden) sensibilisiert. FORRT setzt sich auch für die Öffnung von Lehr- und Betreuungsmaterialien ein, um den Zugang, das Entdecken und das Lernen für diejenigen zu erleichtern, die sonst von der Bildung ausgeschlossen wären.",
                "related_terms": [
                    "Integrating open and reproducible science tenets into higher education"
                ],
                "references": "",
                "drafted_by": [
                    "Tamara Kalandadze"
                ],
                "reviewed_by": [
                    "Mahmoud Elsherif",
                    "Charlotte R. Pennington",
                    "Flávio Azevedo"
                ],
                "alt_related_terms": [
                    null
                ],
                "language": "german"
            },
            {
                "type": "glossary",
                "title": "Free Our Knowledge Platform",
                "definition": "Eine kollektive Handlungsplattform, die darauf abzielt, die Open-Science-Bewegung zu unterstützen, indem sie Zusagen von Forschenden einholt, dass diese bestimmte Forschungspraktiken (z. B. Präregistrierung, Preprint) anwenden werden. Bei der Initiative handelt es sich um eine Initiative, die von Nachwuchswissenschaftler:innen ins Leben gerufen wurde.",
                "related_terms": [
                    "Open Science",
                    "Preregistration Pledge"
                ],
                "references": "Free Our Knowledge. (n.d.). About. Retrieved from https://freeourknowledge.org/about/",
                "drafted_by": [
                    "Jamie P. Cockcroft"
                ],
                "reviewed_by": [
                    "Ashley Blake",
                    "Elizabeth Collins",
                    "Mahmoud Elsherif",
                    "Sam Parsons",
                    "Flávio Azevedo"
                ],
                "alt_related_terms": [
                    null
                ],
                "language": "german"
            },
            {
                "type": "glossary",
                "title": "GPower",
                "definition": "Kostenlose Statistiksoftware zur Durchführung von Power-Analysen. Man gibt den gewünschten statistischen Test (z. B. *t*\\-Test, Regression, ANOVA) und drei der folgenden Punkte an: die Anzahl der Gruppen/Beobachtungen, die Effektgröße, das Signifikanzniveau oder die Power. Aus den drei ausgewählten Parametern wird dann der vierte Parameter, der nicht ausgewählt wurde, bestimmt.",
                "related_terms": [
                    "Power analysis",
                    "Sample size justification",
                    "Sample size planning",
                    "Statistical power"
                ],
                "references": "Faul, F., Erdfelder, E., Lang, A.-G., & Buchner, A. (2007). G*Power 3: A flexible statistical power analysis program for the social, behavioral, and biomedical sciences. Behavior Research Methods, 39, 175–191. https://doi.org/10.3758/BF03193146\n\nFaul, F., Erdfelder, E., Buchner, A., & Lang, A.-G. (2009). Statistical power analyses using G*Power 3.1: Tests for correlation and regression analyses. Behavior Research Methods, 41, 1149–1160. https://doi.org/10.3758/BRM.41.4.1149",
                "drafted_by": [
                    "Filip Dechterenko"
                ],
                "reviewed_by": [
                    "Thomas Rhys Evans",
                    "Kai Krautter",
                    "Charlotte R. Pennington"
                ],
                "alt_related_terms": [
                    null
                ],
                "language": "german"
            },
            {
                "type": "glossary",
                "title": "the system das System überlisten (Gaming (the system))",
                "definition": "Bezeichnet den Einsatz fragwürdiger Forschungspraktiken (*Questionable Research Practices*; QRPs, wie beispielsweise Salami-Slicing eines akademischen Papers), die aufgrund der akademischen Anreizstrukturen dem Forschenden zugutekommen (z. B. in Bezug auf Prestige, Einstellung oder Beförderung), unabhängig davon, ob diese Praxis den wissenschaftlichen Prozess als solchen unterstützt. Wann immer ein System zur Bestimmung eines Ergebnisses (z. B. akademische Leistungen) auf quantitative Kennzahlen angewiesen ist, können diese Kennzahlen absichtlich und gezielt manipuliert oder “überlistet” werden (Naudet et al., 2018). Wenn Beförderungen, Einstellungen und Entfristungen auf solchen manipulierbaren Kennzahlen beruhen, können sie offene, genaue und transparente Forschungsleistungen benachteiligen (Naudet et al., 2018\\) \\- z. B. durch die Bevorzugung von Quantität gegenüber Qualität \\- und bestehende Ungleichheiten verschärfen.",
                "related_terms": [
                    "Incentive structure",
                    "Journal Impact Factor",
                    "*P*\\-hacking"
                ],
                "references": "Moher, D., Naudet, F., Cristea, I. A., Miedema, F., Ioannidis, J. P. A., & Goodman, S. N. (2018). Assessing scientists for hiring, promotion, and tenure. PLOS Biology, 16(3), e2004089. https://doi.org/10.1371/journal.pbio.2004089\n\nNaudet, F., Ioannidis, J., Miedema, F., Cristea, I. A., Goodman, S. N., & Moher, D. (2018). Six principles for assessing scientists for hiring, promotion, and tenure. Impact of Social Sciences Blog.",
                "drafted_by": [
                    "Adrien Fillon"
                ],
                "reviewed_by": [
                    "Sarah Ashcroft-Jones",
                    "Helena Hartmann",
                    "Sam Parsons",
                    "Charlotte R. Pennington"
                ],
                "alt_related_terms": [
                    null
                ],
                "language": "german"
            },
            {
                "type": "glossary",
                "title": "Garten der Weggabelungen (Garden of forking paths)",
                "definition": "Der in der Regel unsichtbare Entscheidungsbaum, der während der Operationalisierung und der statistischen Analyse entsteht, da ‘there is a one-to-many mapping from scientific to statistical hypotheses' (dt. es eine Eins-zu-Viele-Zuordnung von wissenschaftlichen zu statistischen Hypothesen gibt; Gelman und Loken, 2013, S. 6). Auch ohne *p*\\-hacking oder fishing expeditions und HARKing, kann es eine Vielzahl von teils sehr unterschiedlichen statistischen Ergebnissen geben, die alle durch die Theorie und die Daten gestützt zu sein scheinen. “The problem is there can be a large number of potential comparisons when the details of data analysis are highly contingent on data, without the researcher having to perform any conscious procedure of fishing or examining multiple p-values” (dt. Das Problem ist, dass es eine große Anzahl potenzieller Vergleiche geben kann, wenn die Details der Datenanalyse in hohem Maße von den Daten abhängen, ohne dass der Forschende ein bewusstes Verfahren des \"fishing\" oder die Berechnung mehrerer *p*\\-Werte durchführen muss; Gelman und Loken, 2013, S. 1). Der Begriff des Gartens der Weggabelungen zielt darauf ab, die Unsicherheit zu unterstreichen, die sich aus analytischen und statistischen Entscheidungen bei der Zuordnung von Theorien zu Tests ergibt. Zugleich soll das Konzept absichtliche (und unethische) fragwürdige Forschungspraktiken (z. B. *p*\\-hacking und fishing expeditions) mit “normalen”, nicht-fragwürdigen Forschungspraktiken kontrastieren, die potenziell denselben Effekt haben können, ohne dass die Absicht besteht, ihre Ergebnisse zu verfälschen. Der Garten der Weggabelungen (garden of forking paths) bezieht sich auf alle Entscheidungen während des wissenschaftlichen Prozesses, die die Falsch-Positiv-Rate als Folge der möglichen Wege, die man hätte einschlagen können (wenn andere Entscheidungen getroffen worden wären), in die Höhe treiben.",
                "related_terms": [
                    "False-positive",
                    "Familywise error",
                    "Multiverse Analysis",
                    "Preregistration",
                    "Researcher degrees of freedom",
                    "Specification Curve Analysis"
                ],
                "references": "Gelman, A., & Loken, E. (n.d.). The garden of forking paths: Why multiple comparisons can be a problem, even when there is no “fishing expedition” or “p-hacking” and the research hypothesis was posited ahead of time. Retrieved from http://www.stat.columbia.edu/",
                "drafted_by": [
                    "Flávio Azevedo; Mahmoud Elsherif"
                ],
                "reviewed_by": [
                    "Gisela H. Govaart",
                    "Matt Jaquiery",
                    "Tamara Kalandadze",
                    "Charlotte R. Pennington"
                ],
                "alt_related_terms": [
                    null
                ],
                "language": "german"
            },
            {
                "type": "glossary",
                "title": "GDPR, dt. Datenschutzgrundverordnung DSGVO (General Data Protection Regulation (GDPR))",
                "definition": "Ein rechtlicher Rahmen mit sieben Grundsätzen, der in der gesamten Europäischen Union (EU) umgesetzt wurde und den Schutz persönlicher Daten zum Ziel hat. Dieser Rahmen soll den Bürger:innen Kontrolle über ihre personenbezogenen Daten geben und gleichzeitig die an der Speicherung und Verarbeitung dieser Daten beteiligten Parteien regulieren. Dieser Rechtsrahmen bezieht sich auf den freien Verkehr personenbezogener Daten innerhalb und außerhalb der EU und muss von Forschenden bei der Konzeption und Durchführung von Studien berücksichtigt werden.",
                "related_terms": [
                    "Anonymity",
                    "Data Management Plan (DMP)",
                    "Data sharing",
                    "Repeatability",
                    "Replicability",
                    "Reproducibility"
                ],
                "references": "Crutzen, R., Ygram Peters, G. J., & Mondschein, C. (2019). Why and how we should care about the General Data Protection Regulation. Psychology & Health, 34(11), 1347–1357. https://doi.org/10.1080/08870446.2019.1606222\n\nInformation Commissioner’s Office. (2021). Guide to the UK General Data Protection Regulation (UK GDPR). ICO. https://ico.org.uk/for-organisations/guide-to-data-protection/guide-to-the-general-data-protection-regulation-gdpr/",
                "drafted_by": [
                    "Graham Reid"
                ],
                "reviewed_by": [
                    "Elizabeth Collins",
                    "Mahmoud Elsherif",
                    "Christopher Graham",
                    "Sam Parsons"
                ],
                "alt_related_terms": [
                    null
                ],
                "language": "german"
            },
            {
                "type": "glossary",
                "title": "Generalisierbarkeit (Generalizability)",
                "definition": "Generalisierbarkeit bezieht sich darauf, inwieweit die Ergebnisse einer Studie auf breitere Personengruppen, Rahmenbedingungen oder Situationen übertragen werden können und wie sich die Ergebnisse auf diesen breiteren Kontext beziehen (Frey, 2018; Kukull & Ganguli, 2012).",
                "related_terms": [
                    "Conceptual replication",
                    "External Validity",
                    "Opportunistic sampling",
                    "Sampling bias",
                    "WEIRD **Alternative definition:** Applying modified materials and/or analysis pipelines to new data or samples to answer the same hypothesis (different materials, different data) to test how generalizable the effect under study is (The Turing Way Community & Scriberia, 2021). **Related terms to alternative definition:** (if applicable): Conceptual Replication"
                ],
                "references": "Esterling, K., Brady, D., & Schwitzgebel, E. (2021). The Necessity of Construct and External Validity for Generalized Causal Claims. Retrieved from https://doi.org/10.31219/osf.io/2s8w5.\n\nGeneralizability. (2018). Generalizability. In B. B. Frey (Ed.), The SAGE Encyclopedia of Educational Research, Measurement, and Evaluation. SAGE Publications, Inc. https://doi.org/10.4135/9781506326139.n284\n\nKukull, W. A., & Ganguli, M. (2012). Generalizability: The trees, the forest, and the low-hanging fruit. Neurology, 78(23), 1886–1891. https://doi.org/10.1212/WNL.0b013e318258f812\n\nLeBel, E. P., Vanpaemel, W., Cheung, I., & Campbell, L. (2017). A brief guide to evaluate replications. Meta-Psychology, 3. https://doi.org/10.15626/MP.2018.843\n\nNosek, B. A., & Errington, T. M. (2020). What is replication? PLOS Biology, 18(3), e3000691. https://doi.org/10.1371/journal.pbio.3000691\n\nYarkoni, T. (2020). The generalizability crisis. Behavioral and Brain Sciences, 1–37. https://doi.org/10.1017/S0140525X20001685",
                "drafted_by": [
                    "Aoife O’Mahony"
                ],
                "reviewed_by": [
                    "Adrien Fillon",
                    "Matt Jaquiery",
                    "Tina Lonsdorf",
                    "Sam Parsons",
                    "Julia Wolska"
                ],
                "alt_related_terms": [
                    null
                ],
                "language": "german"
            },
            {
                "type": "glossary",
                "title": "or Guest Authorship geschenkte Autor:innenschaft / Gastautor:innenschaft (Gift (or Guest) Authorship)",
                "definition": "Der Begriff bezeichnet die Aufnahme von Personen in die Autor:innenliste eines Forschungsartikels, welche die Kriterien für eine Mitautor:innenschaft nicht erfüllen. Da die Autor:innenschaft mit Vorteilen wie Anerkennung durch Fachkolleg:innen und finanziellen Prämien verbunden ist, gibt es Anreize, auf der Autor:innenliste von publizierten Forschungsarbeiten zu stehen. Das Verschenken von Autor:innenschaft oder das Gewähren von Autor:innenschaft an eine Person, die eine solche Anerkennung nicht verdient, kann somit dazu dienen, einen Gefallen zu erwidern (einschließlich gegenseitiger geschenkter Autor:innenschaft), persönliche und berufliche Beziehungen zu pflegen und die Chancen auf eine Veröffentlichung zu erhöhen. Geschenkte Autorenschaft wird oft als unethische Forschungspraxis angesehen.",
                "related_terms": [
                    "Authorship",
                    "CRediT"
                ],
                "references": "Bhopal, R., Rankin, J., McColl, E., Thomas, L., Kaner, E., Stacy, R., Pearson, P., Vernon, B., & Rodgers, H. (1997). The vexed question of authorship: views of researchers in a British medical faculty. BMJ, 314, 1009–1012. https://doi.org/10.1136/bmj.314.7086.1009\n\nof Medical Journal Editors, I. C. (2019). Recommendations for the conduct, reporting, eduting, and publication of scholarly work in medical journals. http://www.icmje.org/icmje-recommendations.pdf",
                "drafted_by": [
                    "Bradley Baker"
                ],
                "reviewed_by": [
                    "Helena Hartmann",
                    "Aoife O’Mahony",
                    "Sam Parsons",
                    "Charlotte R. Pennington",
                    "Flávio Azevedo"
                ],
                "alt_related_terms": [
                    null
                ],
                "language": "german"
            },
            {
                "type": "glossary",
                "title": "Git",
                "definition": "Ein Softwarepaket zur Nachverfolgung von Änderungen in einer lokalen Gruppe von Dateien (lokale Versionskontrolle), ursprünglich entwickelt von Linus Torvalds. Im Allgemeinen wird es von Programmierenden bei der Entwicklung von Computer-Quellcode innerhalb eines bestimmten Verzeichnisses, Ordners oder Dateisystems verwendet. Git kann auf Remote-Repository-Hosting-Dienste (z. B. GitHub) zugreifen, um eine Remote-Versionskontrolle zu ermöglichen, die eine kollaborative Softwareentwicklung durch Hochladen von Änderungen von einem lokalen System ermöglicht. Dieser Prozess hat seinen Weg in den wissenschaftlichen Prozess gefunden, um offene Daten, offenen Code und reproduzierbare Analysen zu ermöglichen.",
                "related_terms": [
                    "GitHub",
                    "Repository",
                    "Version control"
                ],
                "references": "Kalliamvakou, E., Gousios, G., Blincoe, K., Singer, L., German, D. M., & Damian, D. (2014). The promises and perils of mining github. Proceedings of the 11th Working Conference on Mining Software Repositories, 92–101.\n\nScopatz, A. M., & Huff, K. D. (2015). Effective Computation in Physics: Field Guide to Research with Python (1st ed.). O’Reilly Media. http://shop.oreilly.com/product/0636920033424.do\n\nVuorre, M., & Curley, J. P. (2018). Curating research assets: A tutorial on the Git version control system. Advances in Methods and Practices in Psychological Science, 1(2), 219–236. https://doi.org/10.1177/2515245918754826\n\ngit/git. (n.d.). Initial revision of ‘git’, the information manager from hell. GitHub. https://github.com/git/git/commit/e83c5163316f89bfbde7d9ab23ca2e25604af290",
                "drafted_by": [
                    "Emma Norris"
                ],
                "reviewed_by": [
                    "Adrien Fillon",
                    "Bettina M.J. Kern",
                    "Dominik Kiersz",
                    "Robert M. Ross"
                ],
                "alt_related_terms": [
                    null
                ],
                "language": "german"
            },
            {
                "type": "glossary",
                "title": "Goodharts Gesetz (Goodhart’s Law)",
                "definition": "Der Begriff geht auf den Wirtschaftswissenschaftler Charles Goodhart zurück und bezeichnet die Beobachtung, dass die Messung eines Verhaltens oder einer Leistung das Verhalten der Nutzer:innen verändert. Strathern (1997) stellte in Bezug auf Prüfungsleistungen fest, “when a measure becomes a target, it ceases to be a good measure” (dt. dass eine Kennzahl aufhört nützlich zu sein, wenn das Optimieren der Kennzahl zum Hauptziel wird, S. 308\\) und nicht mehr die Optimierung dessen im Vordergrund steht, was die Kennzahl eigentlich misst oder ursprünglich messen sollte. Übertragen auf Open Scholarship und die Anreizstruktur in der Forschung geht Goodharts Gesetz davon aus, dass Kennzahlen für Bewertung wissenschaftlicher Leistung, wie beispielsweise die Zahl der Zitationen einer forschenden Person, wahrscheinlich manipuliert und gezielt optimiert werden, was verschiedene Studien, wie etwa von Muller (2019) auch so bestätigen.",
                "related_terms": [
                    "Campbell's law",
                    "DORA",
                    "Reification (fallacy) **Reference (s):** \\[@Muller2018\\], \\[@Strathern1997\\]"
                ],
                "references": "",
                "drafted_by": [
                    "Adam Parker"
                ],
                "reviewed_by": [
                    "Sam Parsons",
                    "Flávio Azevedo"
                ],
                "alt_related_terms": [
                    null
                ],
                "language": "german"
            },
            {
                "type": "glossary",
                "title": "H-Index, Hirsch Index (H-index)",
                "definition": "Der Hirsch-Index, abgekürzt als H-Index, soll die Produktivität sowie den Einfluss einer Forschungsarbeit messen, indem er die Anzahl der Veröffentlichungen und die Anzahl der Zitierungen dieser Veröffentlichungen in Relation setzt. Hirsch (2005) definiert den Index als “the number of papers with citation number ≥ *h*” (dt. die Anzahl der Veröffentlichungen mit einer Zitationszahl ≥ h; S. 16569). Eine forschende Person hat einen Hirsch-Index h, wenn h von ihren insgesamt N Publikationen mindestens h-mal und die restlichen (N-h) Publikationen höchstens h-mal zitiert wurden. Der Index gilt zwar als besseres Kriterium als Maßzahlen, die nur die Anzahl der Zitierungen und die Anzahl der Veröffentlichungen bewerten, wird aber dennoch für die Bewertung von Forschenden kritisch gesehen (z. B. Wendl, 2007).",
                "related_terms": [
                    "Citation",
                    "DORA",
                    "I10-index",
                    "Impact"
                ],
                "references": "Hirsch, J. E. (2005). An index to quantify an individual’s scientific research output. Proceedings of the National Academy of Sciences, 102(46), 16569–16572. https://doi.org/10.1073/pnas.0507655102\n\nWendl, M. C. (2007). H-index: however ranked, citations need context. Nature, 449(7161), 403–403. https://doi.org/10.1038/449403b",
                "drafted_by": [
                    "Jacob Miranda"
                ],
                "reviewed_by": [
                    "Bradley J. Baker",
                    "Mahmoud M. Elsherif",
                    "Brett J. Gall",
                    "Charlotte R. Pennington"
                ],
                "alt_related_terms": [
                    null
                ],
                "language": "german"
            },
            {
                "type": "glossary",
                "title": "Hackathon",
                "definition": "Eine Veranstaltung, bei der Expert:innen, Designer:innen oder Forschende für einen begrenzten Zeitraum zusammenarbeiten, um intensiv ein Projekt oder Problem zu bearbeiten. Der Begriff kommt ursprünglich von Veranstaltungen für Programmierenden und Softwareentwicklenden mit dem Ziel, am Ende der Veranstaltung, die einige Stunden bis mehrere Tage dauern kann, ein vollständiges Produkt (Ressourcen, Forschung, Software, Hardware) zu schaffen.",
                "related_terms": [
                    "Collaboration",
                    "Edithaton"
                ],
                "references": "Kienzler, H., & Fontanesi, C. (2017). Learning through inquiry: A global health hackathon. Teaching in Higher Education, 22(2), 129–142. https://doi.org/10.1080/13562517.2016.1221805",
                "drafted_by": [
                    "Flávio Azevedo"
                ],
                "reviewed_by": [
                    "Tsvetomira Dumbalska",
                    "Brett J. Gall",
                    "Emma Norris"
                ],
                "alt_related_terms": [
                    null
                ],
                "language": "german"
            },
            {
                "type": "glossary",
                "title": "HARKing",
                "definition": "Eine fragwürdige Forschungspraxis, die als \"*Hypothesizing After the Results are Known*\" (HARKing, dt. Hypothesenaufstellung nach Wissen der Ergebnisse) bezeichnet wird. “HARKing is defined as presenting a post hoc hypothesis (i.e., one based on or informed by one's results) in a research report as if it was, in fact, *a priori*” (dt. Bei HARKing wird eine Post-hoc-Hypothese (d. h. einer Hypothese, die erst nach der Datenauswertung entwickelt wurde und auf ihren Ergebnissen beruht) in einer Forschungsarbeit so dargestellt, als wäre sie a priori geplant und hypothesengeleitet gewesen; Kerr, 1998, S. 196). Ein Beispiel ist die Durchführung von Subgruppenanalysen, bei der ein Effekt in einer Untergruppe entdeckt wird und daraufhin im Nachhinein eine Hypothese in die Forschungsarbeit mit aufgenommen wird, die diese Ergebnisse vorhersagt. Dabei wird so getan, als wäre die Subgruppenanalyse hypothesengeleitet und a priori geplant gewesen. **Alternative terms**: accommodational hypothesizing",
                "related_terms": [
                    "Analytic Flexibility",
                    "Confirmatory analyses",
                    "Exploratory data analysis",
                    "Fudging",
                    "Garden of forking paths",
                    "P-hacking",
                    "Questionable Research Practices or Questionable Reporting Practices (QRPs)"
                ],
                "references": "Kerr, N. L. (1998). HARKing: Hypothesizing after the results are known. Personality and Social Psychology Review, 2(3), 196–217. https://doi.org/10.1207/s15327957pspr0203_4\n\nNosek, B. A., & Lakens, D. (2014). Registered reports. Social Psychology, 45, 137–141. https://doi.org/10.1027/1864-9335/a000192",
                "drafted_by": [
                    "Beatrix Arendt"
                ],
                "reviewed_by": [
                    "Matt Jaquiery",
                    "Charlotte R. Pennington",
                    "Martin Vasilev",
                    "Flávio Azevedo"
                ],
                "alt_related_terms": [
                    null
                ],
                "language": "german"
            },
            {
                "type": "glossary",
                "title": "Versteckte Moderatoren (Hidden Moderators)",
                "definition": "Kontextuelle Bedingungen, die ohne Wissen der Forschenden dazu führen können, dass die Ergebnisse eines Replikationsversuchs von denen der ursprünglichen Studie abweichen. Versteckte (hidden) Moderatoren werden manchmal angeführt, um fehlgeschlagene Replikationen (weg) zu erklären. Auch versteckte Annahmen (hidden assumptions) genannt.",
                "related_terms": [
                    "Auxiliary Hypothesis"
                ],
                "references": "Zwaan, R., Etz, A., Lucas, R., & Donnellan, M. (2018). Making replication mainstream. Behavioral and Brain Sciences, 41, E120. https://doi.org/10.1017/S0140525X17001972",
                "drafted_by": [
                    "Ali H. Al-Hoorie"
                ],
                "reviewed_by": [
                    "Mahmoud Elsherif",
                    "Sam Parsons"
                ],
                "alt_related_terms": [
                    null
                ],
                "language": "german"
            },
            {
                "type": "glossary",
                "title": "Hypothese (Hypothesis)",
                "definition": "Eine Hypothese ist eine unbewiesene Aussage über den Zusammenhang zwischen Variablen (Glass & Hall, 2008\\) und kann auf früheren Erfahrungen, wissenschaftlichen Erkenntnissen, vorläufigen Beobachtungen, Theorie und/oder Logik beruhen. Bei wissenschaftlichen Tests kann eine Hypothese gerichtet (z. B. eine positive Korrelation) oder ungerichtet (z. B. es wird eine Korrelation geben, aber unklar, ob positiv oder negativ) formuliert werden. Nach Popper (1959) müssen Hypothesen falsifizierbar sein, d. h. es muss möglich sein, die Hypothese zu widerlegen. Es wurde jedoch argumentiert, dass Hypothesentests, die auf Falsifikation beruhen, vage sind, da sie von vielen anderen ungeprüften Annahmen (d. h. Hilfshypothesen, \\[auxiliary hypotheses\\]) abhängig sind. Longino (1990, 1992\\) vertrat die Auffassung, dass in den Biowissenschaften ontologische Heterogenität höher bewertet werden sollte als ontologische Einfachheit, d. h. wir sollten Unterschiede zwischen und innerhalb biologischer Organismen untersuchen.",
                "related_terms": [
                    "Auxiliary Hypothesis",
                    "Confirmatory analyses",
                    "False negative result",
                    "False positive result",
                    "Modelling",
                    "Predictions",
                    "Quantitative research",
                    "Theory",
                    "Theory building",
                    "Type I error",
                    "Type II error"
                ],
                "references": "Beller, S., & Bender, A. (2017). Theory, the final frontier? A corpus-based analysis of the role of theory in psychological articles. Frontiers in Psychology, 8, 951. https://doi.org/10.3389/fpsyg.2017.00951\n\nGlass, D. J., & Hall, N. (2008). A brief history of the hypothesis. Cell, 134(3), 378–381. https://doi.org/10.1016/j.cell.2008.07.033\n\nLongino, H. E. (1990). Science as Social Knowledge: Values and Objectivity in Scientific Inquiry. Princeton University Press.\n\nLongino, H. E. (1992). Taking gender seriously in philosophy of science. PSA, 2, 333–340.\n\nPopper, K. (1959). The logic of scientific discovery. Routledge.",
                "drafted_by": [
                    "Ana Barbosa Mendes"
                ],
                "reviewed_by": [
                    "Ali H. Al-Hoorie",
                    "Mahmoud Elsherif",
                    "Helena Hartmann",
                    "Charlotte R. Pennington**;** Graham Reid",
                    "Olly Robertson"
                ],
                "alt_related_terms": [
                    null
                ],
                "language": "german"
            },
            {
                "type": "glossary",
                "title": "i10-Index (i10-index)",
                "definition": "Eine von Google Scholar erstellte Forschungsmetrik. Sie gibt für eine/n Forschenden die Anzahl der Publikationen mit mindestens 10 Zitationen an.",
                "related_terms": [
                    "Citation",
                    "DORA",
                    "H-index",
                    "Impact"
                ],
                "references": "University, C. (2020). Measuring your research impact: i10 index. Cornell University Library. https://guides.library.cornell.edu/impact/author-impact-10",
                "drafted_by": [
                    "Emma Norris"
                ],
                "reviewed_by": [
                    "Flávio Azevedo",
                    "Sam Parsons"
                ],
                "alt_related_terms": [
                    null
                ],
                "language": "german"
            },
            {
                "type": "glossary",
                "title": "Ideologische Verzerrung (Ideological bias)",
                "definition": "Der Gedanke, dass Meinungen über die Qualität von Forschungsarbeiten von den ideologischen Ansichten der Forschenden abhängen können. Ideologische Verzerrung ist eine der vielen möglichen Verzerrungen (Bias), die im Peer-Review-Verfahren auftreten können, und geht davon aus, dass eine positive Bewertung der Forschungsarbeit wahrscheinlicher ist, wenn die politischen Ansichten der begutachtenden Person und/oder ihres Umfeldes mit den Ansichten der Verfasser:innen der Forschungsarbeit übereinstimmen (Tvina et al. 2019). Dies kann zu einer Reihe von Interessenkonflikten führen und unterschiedliche Perspektiven untergraben, so kann es zum Beispiel zu einer Beschleunigung oder Verzögerung des Peer-Review-Verfahrens führen oder die Chance des Forschenden, zu einer Präsentation ihrer Forschung eingeladen zu werden, negativ beeinflussen.",
                "related_terms": [
                    "Ad hominem bias",
                    "Peer review"
                ],
                "references": "Tvina, A., Spellecy, R., & Palatnik, A. (2019). Bias in the peer review process: can we do better? Obstetrics & Gynecology, 133(6), 1081–1083. https://doi.org/10.1097/AOG.0000000000003260",
                "drafted_by": [
                    "Mahmoud Elsherif"
                ],
                "reviewed_by": [
                    "Elizabeth Collins",
                    "Flávio Azevedo",
                    "Madeleine Ingham",
                    "Sam Parsons",
                    "Graham Reid"
                ],
                "alt_related_terms": [
                    null
                ],
                "language": "german"
            },
            {
                "type": "glossary",
                "title": "Inklusion, Inklusivität (Inclusion)",
                "definition": "Inklusion oder Inklusivität bezieht sich auf eine Kultur des Willkommens und des Respekts in der Zusammenarbeit an einem bestimmten Projekt oder in einem Umfeld (z. B. im akademischen Bereich), wobei Vielfalt (Diversität) hier ein breites Spektrum an unterschiedlichen Hintergründen, Perspektiven und Erfahrungen bedeutet. Bemühungen zur Erhöhung der Inklusivität beinhalten die Verminderung der Folgen oder die komplette Beseitigung systemischer Barrieren für benachteiligte Gruppen und Minderheiten.",
                "related_terms": [
                    "Diversity",
                    "Equity",
                    "Social Justice"
                ],
                "references": "Calvert, D. (2019). How to Make Inclusivity More Than Just an Office Buzzword. Retrieved from https://insight.kellogg.northwestern.edu/article/how-to-make-inclusivity-more-than-just-an-office-buzzword\n\nMartinez-Acosta, V. G., & Favero, C. B. (2018). A discussion of diversity and inclusivity at the institutional level: The need for a strategic plan. Journal of Undergraduate Neuroscience Education, 16(3), A252.",
                "drafted_by": [
                    "Ryan Millager"
                ],
                "reviewed_by": [
                    "Mahmoud Elsherif",
                    "Graham Reid",
                    "Kai Krautter",
                    "Suzanne L. K. Stewart",
                    "Flávio Azevedo"
                ],
                "alt_related_terms": [
                    null
                ],
                "language": "german"
            },
            {
                "type": "glossary",
                "title": "Anreizsystem (Incentive structure)",
                "definition": "Anreizsystem (incentive structure) bezeichnet die Gesamtheit der (expliziten und impliziten) Bewertungs- und Belohnungsmechanismen für Forschende und ihre Forschung. Zu den Bereichen, in denen Anreize geschaffen werden, gehören Einstellungs- und Beförderungspraktiken, die Erfolgsbilanz bei der Vergabe von Fördermitteln und Prestigeindikatoren wie Veröffentlichungen in Zeitschriften mit hohem Impact-Faktor, eingeladene Vorträge, Herausgeber:innenschaften und Auszeichnungen. Es wird allgemein angenommen, dass diese Kriterien oft nicht mit dem Sinn und Zweck von Wissenschaft übereinstimmen und daher nicht geeignet sind, anspruchsvolle wissenschaftliche Leistungen zu fördern. Initiativen wie DORA zielen darauf ab, die Abhängigkeit des Fachgebiets von rein quantitativen Bewertungskriterien wie dem Impact-Faktor von Fachzeitschriften zugunsten von Bewertungen zu verringern, die auf Qualität der Forschungsarbeiten basieren, und nicht auf ihrer Anzahl.",
                "related_terms": [
                    "DORA",
                    "Metrics",
                    "Pressure",
                    "Publish or perish",
                    "Quantity",
                    "Reward structure",
                    "Scientific publications",
                    "Slow science",
                    "Structural factors"
                ],
                "references": "Koole, S. L., & Lakens, D. (2012). Rewarding replications: A sure and simple way to improve psychological science. Perspectives on Psychological Science, 7(6), 608–614. https://doi.org/10.1177/1745691612462586\n\nNosek, B. A., Spies, J. R., & Motyl, M. (2012). Scientific utopia: II. Restructuring incentives and practices to promote truth over publishability. Perspectives on Psychological Science, 7(6), 615–631. https://doi.org/10.1177/1745691612459058\n\nSchönbrodt, F. (2019). Training students for the Open Science future. Nature Human Behaviour, 3(10), 1031–1031. https://doi.org/10.1038/s41562-019-0726-z\n\nSmaldino, P. E., & McElreath, R. (2016). The natural selection of bad science. Royal Society Open Science, 3(9), 160384. https://doi.org/10.1098/rsos.160384",
                "drafted_by": [
                    "Charlotte R. Pennington; Olmo van den Akker"
                ],
                "reviewed_by": [
                    "Helena Hartmann",
                    "Flávio Azevedo",
                    "Robert M. Ross",
                    "Graham Reid",
                    "Suzanne L. K. Stewart"
                ],
                "alt_related_terms": [
                    null
                ],
                "language": "german"
            },
            {
                "type": "glossary",
                "title": "Induktion (Induction)",
                "definition": "Reasoning by drawing a conclusion not guaranteed by the premises; for example, by inferring a general rule from a limited number of observations. Popper believed that there was no such logical process; we may guess general rules but such guesses are not rendered even more probable by any number of observations. By contrast, Bayesians inductively work out the increase in probability of a hypothesis that follows from the observations.” (dt. Argumentieren, indem man eine Schlussfolgerung zieht, die nicht durch die Prämissen garantiert ist, z. B. indem man eine allgemeine Regel aus einer begrenzten Anzahl von Beobachtungen ableitet. Popper glaubte, dass es einen solchen logischen Prozess nicht gibt; wir können allgemeine Regeln erraten, aber solche Vermutungen werden nicht durch eine beliebige Anzahl von Beobachtungen noch wahrscheinlicher. Im Gegensatz dazu erarbeiten Bayesianer:innen induktiv die Zunahme der Wahrscheinlichkeit einer Hypothese, die aus den Beobachtungen folgt; Dienes, S. 164, 2008).",
                "related_terms": [
                    "Hypothesis"
                ],
                "references": "Dienes, Z. (2008). Understanding psychology as a science: An introduction to scientific and statistical inference. Macmillan International Higher Education.",
                "drafted_by": [
                    "Alaa Aldoh"
                ],
                "reviewed_by": [],
                "alt_related_terms": [
                    null
                ],
                "language": "german"
            },
            {
                "type": "glossary",
                "title": "Interaktionsfehlschluss (Interaction Fallacy)",
                "definition": "Ein statistischer Fehlschluss, der passiert, wenn kein formaler Test durchgeführt wird, um den Unterschied zwischen einer signifikanten und einer nicht signifikanten Korrelation (oder anderen Maßzahlen wie Odds Ratio) zu bewerten. Wenn angenommen wird, dass ein signifikanter und ein nicht signifikanter Korrelationskoeffizient einen statistisch signifikanten Unterschied darstellen, ohne dass der Unterschied ausdrücklich getestet wird, spricht man vom Interaktionsfehler (interaction fallacy)..",
                "related_terms": [
                    "Comparison of Correlations",
                    "Null Hypothesis Significance Testing (NHST)",
                    "Statistical Validity",
                    "Type I error",
                    "Type II error"
                ],
                "references": "Gelman, A., & Stern, H. (2006). The difference between “significant” and “not significant” is not itself statistically significant. The American Statistician, 60(4), 328–331. https://doi.org/10.1198/000313006X152649\n\nMorabia, A., Have, T. T., & Landis, J. R. (1997). Interaction Fallacy. Journal of Clinical Epidemiology, 50(7), 809–812. https://doi.org/10.1016/S0895-4356(97)00053-X\n\nNieuwenhuis, S., Forstmann, B. U., & Wagenmakers, E. J. (2011). Erroneous analyses of interactions in neuroscience: a problem of significance. Nature Neuroscience, 14(9), 1105–1107. https://doi.org/10.1038/nn.2886",
                "drafted_by": [
                    "Graham Reid"
                ],
                "reviewed_by": [
                    "Ali H. Al-Hoorie",
                    "Mahmoud Elsherif",
                    "Kai Krautter",
                    "Sam Parsons"
                ],
                "alt_related_terms": [
                    null
                ],
                "language": "german"
            },
            {
                "type": "glossary",
                "title": "Verzahnung (Interlocking)",
                "definition": "Im Gegensatz zur Intersektionalität (die sich darauf bezieht, dass das Individuum mehrere soziale Identitäten hat) wird Interlocking verwendet, um die Systeme zu beschreiben, die zusammenwirken, um als Unterdrückungsmaßnahmen gegen ein Individuum auf der Grundlage dieser Identitäten zu dienen. Interlocking ist ein wichtiger Bestandteil von Intersektionalität und hilft dabei, Mechanismen von Macht, Ungleichheit und Ausgrenzung zu analysieren. So können beispielsweise Reformen der akademischen Kultur nicht funktionieren, wenn nur ein Problem isoliert untersucht wird (z. B. Herkunft, Geschlecht oder Fähigkeit), sondern nur, indem alle Systeme der Ausgrenzung und ihr Zusammenwirken berücksichtigt werden.",
                "related_terms": [
                    "Bropenscience",
                    "Equity",
                    "Diversity",
                    "Inclusion",
                    "Intersectionality",
                    "Open Science",
                    "Social Justice"
                ],
                "references": "Ledgerwood, A., Hudson, S. T. J., Lewis, Jr., N. A., Maddox, K. B., Pickett, C., Remedios, J. D., & Wilkins, C. L. (2021). The Pandemic as a Portal: Reimagining Psychological Science as Truly Open and Inclusive. https://doi.org/10.31234/osf.io/gdzue",
                "drafted_by": [
                    "Christina Pomareda"
                ],
                "reviewed_by": [
                    "Ali H. Al-Hoorie",
                    "Flávio Azevedo",
                    "Mahmoud Elsherif",
                    "Eliza Woodward",
                    "Gerald Vineyard;"
                ],
                "alt_related_terms": [
                    null
                ],
                "language": "german"
            },
            {
                "type": "glossary",
                "title": "interne Validität (Internal Validity)",
                "definition": "Ein Indikator dafür, inwieweit die Ergebnisse einer Studie für den tatsächlichen Effekt in der jeweiligen Grundgesamtheit repräsentativ sind und nicht auf Störfaktoren, wie z. B. methodische Mängel zurückzuführen sind. Interne Validität gibt an, ob die beobachtete Evidenz oder Kovariation zwischen der unabhängigen (Prädiktor-) und der abhängigen (Kriteriums-) Variable als eine echte Beziehung und nicht als ein falsch-positiver Effekt aufgrund unkontrollierter Aspekte des Studiendesigns angesehen werden kann. Da es dabei um die Qualität der Studie selbst geht, hat die interne Validität einen hohen Stellenwert für die wissenschaftliche Forschung.",
                "related_terms": [
                    "External validity",
                    "Validity"
                ],
                "references": "Campbell, D. T., & Stanley, J. C. (1966). Experimental and Quasi Experimental Designs. Rand McNally.",
                "drafted_by": [
                    "Annalise A. LaPlume"
                ],
                "reviewed_by": [
                    "Helena Hartmann",
                    "Oscar Lecuona",
                    "Meng Liu",
                    "Sam Parsons",
                    "Graham Reid",
                    "Flávio Azevedo"
                ],
                "alt_related_terms": [
                    null
                ],
                "language": "german"
            },
            {
                "type": "glossary",
                "title": "Intersektionalität (Intersectionality)",
                "definition": "Der Begriff stammt aus dem schwarzen feministischen Denken und beschreibt, wie soziale Identitäten innerhalb von ineinandergreifenden Unterdrückungssystemen und Strukturen der (Un-)Gleichheit existieren (Crenshaw, 1989). Intersektionalität bietet einen Blickwinkel darauf, wie mehrere Formen der Ungleichheit zusammenwirken und sich gegenseitig verstärken oder verschlimmern können. Mehrere gleichzeitig auftretende soziale Identitäten können einen Multiplikatoreffekt haben und sind mehr als die Summe der einzelnen Elemente. Eine zentrale Annahme von Intersektionalität ist, dass Identität nicht angemessen verstanden werden kann, wenn eine einzelne Achse (z. B. Herkunft, Geschlecht, sexuelle Orientierung, Klasse) isoliert untersucht wird, und dass stattdessen die gleichzeitige Betrachtung sich überschneidender Identitäten erforderlich ist.",
                "related_terms": [
                    "Bropenscience",
                    "Diversity",
                    "Inclusion",
                    "Interlocking",
                    "Open Science"
                ],
                "references": "Crenshaw, K. W. (1989). Demarginalizing the Intersection of Race and Sex: A Black Feminist Critique of Antidiscrimination Doctrine. University of Chicago Legal Forum, 1989(8), 139–168.\n\nGrzanka, P. R. (2020). From buzzword to critical psychology: An invitation to take intersectionality seriously. Women & Therapy, 43(3–4), 244–261.\n\nLedgerwood, A., Hudson, S. T. J., Lewis, Jr., N. A., Maddox, K. B., Pickett, C., Remedios, J. D., & Wilkins, C. L. (2021). The Pandemic as a Portal: Reimagining Psychological Science as Truly Open and Inclusive. https://doi.org/10.31234/osf.io/gdzue",
                "drafted_by": [
                    "Madeleine Pownall"
                ],
                "reviewed_by": [
                    "Ali H. Al-Hoorie",
                    "Bradley Baker",
                    "Mahmoud Elsherif",
                    "Wanyin Li",
                    "Ryan Millager",
                    "Charlotte R. Pennington",
                    "Flávio Azevedo"
                ],
                "alt_related_terms": [
                    null
                ],
                "language": "german"
            },
            {
                "type": "glossary",
                "title": "JabRef",
                "definition": "Ein kostenloses, quell-offenes, plattformübergreifendes Tool zur Verwaltung von Zitationen und Referenzen. Es ermöglicht die Bearbeitung von BibTeX-Dateien, den Import von Daten aus wissenschaftlichen Online-Datenbanken sowie die Verwaltung und Suche von BibTeX-Dateien.",
                "related_terms": [
                    "Open source software"
                ],
                "references": "Team, J. D. (2021). JabRef - An open-source, cross-platform citation and reference management software. https://www.jabref.org",
                "drafted_by": [
                    "Aleksandra Lazić"
                ],
                "reviewed_by": [
                    "Christopher Graham",
                    "Michele C. Lim",
                    "Sam Parsons",
                    "Steven Verheyen"
                ],
                "alt_related_terms": [
                    null
                ],
                "language": "german"
            },
            {
                "type": "glossary",
                "title": "Jamovi",
                "definition": "Freie und quelloffene Software für Datenanalyse, auf der Grundlage der statistischen Programmiersprache R. Jamovi verfügt über eine grafische Benutzeroberfläche und stellt den verwendeten R-Code bereit. Damit unterstützt Jamovi die Reproduzierbarkeit von Forschungsergebnissen, indem Daten, Code, Analysen und Ergebnisse in einer einzigen Datei gespeichert werden.",
                "related_terms": [
                    "JASP",
                    "Open source",
                    "R",
                    "Reproducibility"
                ],
                "references": "",
                "drafted_by": [
                    "Amélie Beffara Bret"
                ],
                "reviewed_by": [
                    "Adrien Fillon",
                    "Alexander Hart",
                    "Charlotte R. Pennington"
                ],
                "alt_related_terms": [
                    null
                ],
                "language": "german"
            },
            {
                "type": "glossary",
                "title": "JASP",
                "definition": "JASP steht für “Jeffrey's Amazing Statistics Program” und ist nach Sir Harold Jeffreys benannt. Es handelt sich um eine freie und quelloffene Software für Datenanalyse. JASP stützt sich auf eine graphische Benutzeroberfläche und bietet sowohl Nullhypothesentests als auch deren Bayes’sche Gegenstücke. JASP unterstützt die Reproduzierbarkeit von Forschungsergebnissen, indem es die Daten, den Code, die Analysen und die Ergebnisse in einer einzigen Datei speichert.",
                "related_terms": [
                    "Jamovi",
                    "Open source"
                ],
                "references": "Team, J. (2020). JASP (Version 0.14.1) [Computer software].",
                "drafted_by": [
                    "Amélie Beffara Bret"
                ],
                "reviewed_by": [
                    "Adrien Fillon, Adam Parker",
                    "Sam Parsons"
                ],
                "alt_related_terms": [
                    null
                ],
                "language": "german"
            },
            {
                "type": "glossary",
                "title": "Zeitschriften-Impact-Faktor (Journal Impact Factor™)",
                "definition": "Die durchschnittliche Anzahl der Zitierungen von Forschungsartikeln in einer bestimmten Zeitschrift in den vorangegangenen zwei Jahren. Es handelt sich um eine urheberrechtlich geschützte und undurchsichtige Berechnung, die von Clarivate™ vermarktet wird. Die Impact-Faktoren von Zeitschriften stehen nicht in Zusammenhang mit der Qualität des Inhalts oder dem Peer-Review-Verfahren.",
                "related_terms": [
                    "DORA",
                    "H-index"
                ],
                "references": "Brembs, B., Button, K., & Munafò, M. (2013). Deep impact: unintended consequences of journal rank. Frontiers in Human Neuroscience, 7, 291. https://doi.org/10.3389/fnhum.2013.00291\n\nCurry, S. (2012). Sick of impact factors. http://occamstypewriter.org/scurry/2012/08/13/sick-of-impact-factors/\n\nNaudet, F., Ioannidis, J., Miedema, F., Cristea, I. A., Goodman, S. N., & Moher, D. (2018). Six principles for assessing scientists for hiring, promotion, and tenure. Impact of Social Sciences Blog.\n\nRossner, M., Van Epps, H., & Hill, E. (2008). Show me the data. https://doi.org/10.1083/jcb.200711140\n\nSharma, M., Sarin, A., Gupta, P., Sachdeva, S., & Desai, A. (2014). Journal impact factor: its use, significance and limitations. World Journal of Nuclear Medicine, 13(2), 146. https://doi.org/10.4103/1450-1147.139151",
                "drafted_by": [
                    "Jacob Miranda"
                ],
                "reviewed_by": [
                    "Tsvetomira Dumbalska",
                    "Adam Parker"
                ],
                "alt_related_terms": [
                    null
                ],
                "language": "german"
            },
            {
                "type": "glossary",
                "title": "JSON file",
                "definition": "JavaScript Object Notation (JSON) ist ein Datenformat für strukturierte Daten, das zur Darstellung von Attribut-Wert-Paaren verwendet werden kann. Die Werte können dabei weitere JSON-Notationen (d. h. verschachtelte Informationen) enthalten. JSON-Dateien können formal als Textketten kodiert werden und sind daher für Menschen lesbar. Neben der Speicherung von Informationen eignet sich diese Eigenschaft auch für die Kommentierung anderer Inhalte. Beispielsweise werden JSON-Dateien in der Brain Imaging Data Structure (BIDS) zur Beschreibung des Metadatensatzes verwendet, wobei ein standardisiertes Format (dataset\\_description.json) verwendet wird.",
                "related_terms": [
                    "BIDS data structure",
                    "Metadata"
                ],
                "references": "BIDS. (n.d.). Modality agnostic files. Retrieved from https://bids-specification.readthedocs.io/en/stable/03-modality-agnostic-files.html",
                "drafted_by": [
                    "Tina Lonsdorf"
                ],
                "reviewed_by": [
                    "Alexander Hart",
                    "Matt Jaquiery",
                    "Emma Norris",
                    "Charlotte R. Pennington"
                ],
                "alt_related_terms": [
                    null
                ],
                "language": "german"
            },
            {
                "type": "glossary",
                "title": "Knowledge acquisition",
                "definition": "Der Prozess, durch den der Verstand neue Informationen entschlüsselt oder extrahiert, speichert und mit bereits vorhandenen Informationen im Langzeitgedächtnis verknüpft. Angesichts der komplexen Struktur und Natur von Wissen wird dieser Prozess im philosophischen Bereich der Erkenntnistheorie sowie im psychologischen Bereich des Lernens und des Gedächtnisses untersucht.",
                "related_terms": [
                    "Epistemology",
                    "Information",
                    "Learning"
                ],
                "references": "Brule, J., & Blount, A. (1989). Knowledge acquisition. McGraw-Hill.",
                "drafted_by": [
                    "Oscar Lecuona"
                ],
                "reviewed_by": [
                    "Bradley Baker",
                    "Helena Hartmann",
                    "Kai Krautter",
                    "Graham Reid"
                ],
                "alt_related_terms": [
                    null
                ],
                "language": "german"
            },
            {
                "type": "glossary",
                "title": "Likelihood-Funktion (Likelihood function)",
                "definition": "Eine Likelihood-Funktion ist ein statistisches Modell von Daten in frequentistischen und Bayes’schen Analysen und stellt die Wahrscheinlichkeit verschiedener Parameter für ihre Verteilung gegeben den Daten dar. Da Wahrscheinlichkeitsverteilungen unbekannte Populationsparameter haben, gibt die Likelihood-Funktion an, wie gut die Stichprobendaten diese Parameter zusammenfassen. Die Likelihood-Funktion gibt also Aufschluss darüber, wie gut ein Modell an die Stichprobendaten für einen bestimmten Satz von Werten der unbekannten Populationsparameter angepasst ist.",
                "related_terms": [
                    "Bayes factor",
                    "Bayesian inference",
                    "Bayesian parameter estimation",
                    "Posterior distribution",
                    "Prior distribution **Alternative definition:** For a more statistically-informed definition, given a parametric model specified by a probability (densidity) function f(x|theta), a likelihood *for* a statistical model is defined by the same formula as the density except that the roles of the data *x* and the parameter *theta* are interchanged, and thus the likelihood can be considered a function of *theta* for fixed data *x*. Here, then, the likelihood function would describe a curve or hypersurface whose peak, if it exists, represents the combination of model parameter values that maximize the probability of drawing the sample obtained."
                ],
                "references": "Dienes, Z. (2008). Understanding psychology as a science: An introduction to scientific and statistical inference. Macmillan International Higher Education.\n\nHogg, D., Bovy, J., & Lang, D. (2010). Data analysis recipes: Fitting a model to data. arXiv:1008.4686 [astro-ph.IM].\n\nGeyer, C. J. (2003). Maximum Likelihood in R (pp. 1–9). Open Science Framework.\n\nGeyer, C. J. (2007). Stat 5102 Notes: Maximum Likelihood (pp. 1–8). Open Science Framework.\n\nHuber, C. (2016). The Stata Blog: Introduction to Bayesian statistics, part 1: The basic concepts. In The Stata Blog. https://blog.stata.com/2016/11/01/introduction-to-bayesian-statistics-part-1-the-basic-concepts/",
                "drafted_by": [
                    "Alaa AlDoh"
                ],
                "reviewed_by": [
                    "Dominik Kiersz",
                    "Graham Reid",
                    "Sam Parsons",
                    "Flávio Azevedo"
                ],
                "alt_related_terms": [
                    null
                ],
                "language": "german"
            },
            {
                "type": "glossary",
                "title": "Likelihood Prinzip (Likelihood Principle)",
                "definition": "Die Vorstellung, dass alle in den Daten enthaltenen schlussfolgerelevanten Informationen von der Likelihood geliefert werden. Das Prinzip besagt, dass die Likelihood Funktion verwendet werden kann, um die Plausibilität verschiedener Parameterwerte zu vergleichen. Während Bayesianer:innen und Likelihood-Theoretiker:innen dem Likelihood Prinzip zustimmen, tun dies Neyman-Pearson-Theoretiker:innen nicht, da Signifikanztests das Likelihood Prinzip verletzen, weil sie Informationen berücksichtigen, die nicht in der Likelihood enthalten sind.",
                "related_terms": [
                    "Bayesian inference",
                    "Likelihood Function"
                ],
                "references": "Dienes, Z. (2008). Understanding psychology as a science: An introduction to scientific and statistical inference. Macmillan International Higher Education.\n\nGeyer, C. J. (2003). Maximum Likelihood in R (pp. 1–9). Open Science Framework.\n\nGeyer, C. J. (2007). Stat 5102 Notes: Maximum Likelihood (pp. 1–8). Open Science Framework.",
                "drafted_by": [
                    "Alaa Aldoh"
                ],
                "reviewed_by": [
                    "Sam Parsons",
                    "Flávio Azevedo"
                ],
                "alt_related_terms": [
                    null
                ],
                "language": "german"
            },
            {
                "type": "glossary",
                "title": "Literaturzusammenfassung (Literature Review)",
                "definition": "Wissenschaftler:innen sichten häufig Forschungsarbeiten zu einem bestimmten Thema, um die Auswirkungen und Phänomene, die von Interesse sind, besser zu verstehen, bevor sie ein neues Forschungsprojekt in Angriff nehmen. Dies unterstützt dabei, zu verstehen, wie die Theorie mit den Beweisen zusammenhängt, oder dabei, gemeinsame Themen und Richtungen bestehender Studienergebnisse und Aussagen zu untersuchen. Abhängig von der Forschungsfrage und dem Umfang der Literatur können verschiedene Arten von Literaturzusammenfassungen (Literature Reviews) durchgeführt werden. Um den Umfang und die wichtigsten Konzepte in einem bestimmten Bereich zu bestimmen, können Forscher ein Scoping Literature Review durchführen. Systematische Literaturzusammenfassungen zielen darauf ab, alle verfügbaren Aufzeichnungen aufzurufen und zu überprüfen, um eine möglichst genaue und unvoreingenommene Darstellung der vorhandenen Literatur zu erhalten. Nicht systematische oder fokussierte Literature Reviews fassen Informationen aus einer Auswahl von Studien zusammen, die für die Forschungsfrage relevant sind, obwohl sie aufgrund der Anfälligkeit für Voreingenommenheit (z. B. Voreingenommenheit des Forschenden; Siddaway et al., 2019\\) selten sind.",
                "related_terms": [
                    "Evidence synthesis",
                    "Meta-research",
                    "Narrative reviews",
                    "Systematic reviews"
                ],
                "references": "Huelin, R., Iheanacho, I., Payne, K., & Sandman, K. (2015). What’s in a name? Systematic and non-systematic literature reviews, and why the distinction matters. The Evidence Forum, 34–37. Retrieved from https://www.evidera.com/wp-content/uploads/2015/06/Whats-in-a-Name-Systematic-and-Non-Systematic-Literature-Reviews-and-Why-the-Distinction-Matters.pdf\n\nMunn, Z., Peters, M. D., Stern, C., Tufanaru, C., McArthur, A., & Aromataris, E. (2018). Systematic review or scoping review? Guidance for authors when choosing between a systematic or scoping review approach. BMC Medical Research Methodology, 18(1), 1–7. https://doi.org/10.1186/s12874-018-0611-x\n\nPautasso, M. (2013). Ten Simple Rules for Writing a Literature Review. PLoS Computational Biology, 9(7), e1003149. https://doi.org/10.1371/journal.pcbi.1003149\n\nSiddaway, A. P., Wood, A. M., & Hedges, L. V. (2019). How to do a systematic review: a best practice guide for conducting and reporting narrative reviews, meta-analyses, and meta-syntheses. Annual Review of Psychology, 70, 747–770. https://doi.org/10.1146/annurev-psych-010418-102803",
                "drafted_by": [
                    "Marta Topor"
                ],
                "reviewed_by": [],
                "alt_related_terms": [
                    null
                ],
                "language": "german"
            },
            {
                "type": "glossary",
                "title": "Manel",
                "definition": "Kofferwort für “male panel” (männliches Gremium), das sich in der Regel auf die Zusammensetzung von Vortragenden auf Konferenzen (panels) bezieht, die ausschließlich aus (meist weißen) Männern bestehen. Wird in der Regel im Zusammenhang mit Geschlechter-Ungleichheiten im akademischen Bereich diskutiert (z. B. werden Frauen von ihren Kolleg:innen seltener als Expertinnen anerkannt und haben infolgedessen weniger Möglichkeiten zur Karriereentwicklung).",
                "related_terms": [
                    "Bropenscience",
                    "Diversity",
                    "Equity",
                    "Feminist psychology",
                    "Inclusion",
                    "Under-representation"
                ],
                "references": "Bouvy, J. C., & Mujoomdar, M. (2019). All-Male Panels and Gender Diversity of Issue Panels and Plenary Sessions at ISPOR Europe. PharmacoEconomics-Open, 3(3), 419–422. https://doi.org/10.1007/s41669-019-0153-0\n\nGoodman, S. W., & Pepinsky, T. B. (2019). Gender Representation and Strategies for Panel Diversity: Lessons from the APSA Annual Meeting. PS: Political Science & Politics, 52(4), 669–676. https://doi.org/10.1017/S1049096519000908\n\nNittrouer, C., Hebl, M., Ashburn-Nardo, L., Trump-Steele, R., Lane, D., & Valian, V. (2018). Gender disparities in colloquium speakers. Proceedings of the National Academy of Sciences, 115(1), 104–108. https://doi.org/10.1073/pnas.1708414115\n\nRodriguez, J. K., & Günther, E. A. (2020). What’s wrong with manels and what can we do about it. The Conversation. https://theconversation.com/whats-wrong-with-manels-and-what-can-we-do-about-them-148068",
                "drafted_by": [
                    "Sam Parsons"
                ],
                "reviewed_by": [
                    "Mahmoud Elsherif",
                    "Thomas Rhys Evans",
                    "Beatrice Valentini",
                    "Christopher Graham",
                    "Flávio Azevedo"
                ],
                "alt_related_terms": [
                    null
                ],
                "language": "german"
            },
            {
                "type": "glossary",
                "title": "Many authors",
                "definition": "Groß angelegte kollaborative Projekte, an denen Dutzende oder Hunderte von Autor:innen aus verschiedenen Einrichtungen beteiligt sind. Dieser Ansatz ist in den letzten Jahren in der Psychologie und anderen Wissenschaften immer häufiger anzutreffen, im Gegensatz zu Forschungsarbeiten, die von kleinen Autor:innenteams durchgeführt werden. Damit folgt er früheren Trends, die z. B. in der Hochenergiephysik oder der biomedizinischen Forschung in den 1990er Jahren zu beobachten waren. Diese großen internationalen wissenschaftlichen Konsortien arbeiten an einem Forschungsprojekt, um ein breiteres Spektrum an Expertise zusammenzubringen und kollaborativ Manuskripte zu erstellen.",
                "related_terms": [
                    "Collaboration",
                    "Consortia",
                    "Consortium authorship",
                    "Crowdsourcing",
                    "Hyperauthorship",
                    "Multiple-authors",
                    "Team science"
                ],
                "references": "Cronin, B. (2001). Hyperauthorship: A postmodern perversion or evidence of a structural shift in scholarly communication practices? Journal of the American Society for Information Science and Technology, 52(7), 558–569. https://doi.org/10.1002/asi.1097\n\nMoshontz, H., Ebersole, C. R., Weston, S. J., & Klein, R. A. (2021). A guide for many authors: Writing manuscripts in large collaborations. Social and Personality Psychology Compass, 15(4). https://doi.org/10.1111/spc3.12590\n\nWuchty, S., Jones, B. F., & Uzzi, B. (2007). The increasing dominance of teams in production of knowledge. Science, 316(5827), 1036–1039. https://doi.org/10.1126/science.1136099",
                "drafted_by": [
                    "Yu-Fang Yang"
                ],
                "reviewed_by": [
                    "Christopher Graham",
                    "Adam Parker",
                    "Charlotte R. Pennington",
                    "Birgit Schmidt",
                    "Beatrice Valentini"
                ],
                "alt_related_terms": [
                    null
                ],
                "language": "german"
            },
            {
                "type": "glossary",
                "title": "Many Labs",
                "definition": "Eine von der Open Science Collaboration (2015) geleitete Crowdsourcing-Initiative, bei der mehrere hundert Forschungsgruppen aus verschiedenen Universitäten Replikationsstudien zu veröffentlichten Befunden durchführen. Diese Initiative ist auch unter dem Namen \"Many Labs I\" bekannt und wurde anschließend durch das Projekt \"Many Labs II\" ergänzt, das die Unterschiede zwischen Replikationsergebnissen in verschiedenen Stichproben und Kontexten bewertet. Zu ähnlichen Projekten gehören ManyBabies, EEGManyLabs und der Psychological Science Accelerator.",
                "related_terms": [
                    "Collaboration",
                    "Many analysts",
                    "Many Labs I",
                    "Many Labs II",
                    "Open Science Collaboration",
                    "Replication"
                ],
                "references": "Ebersole, C. R., Atherton, O. E., Belanger, A. L., Skulborstad, H. M., Allen, J. M., Banks, J. B., & Nosek, B. A. (2016). Many Labs 3: Evaluating participant pool quality across the academic semester via replication. Journal of Experimental Social Psychology, 67, 68–82. https://doi.org/10.1016/j.jesp.2015.10.012\n\nFrank, M. C., Bergelson, E., Bergmann, C., Cristia, A., Floccia, C., Gervain, J., Hamlin, J. K., Hannon, E. E., Kline, M., Levelt, C., Lew-Williams, C., Nazzi, T., Panneton, R., Rabagliati, H., Soderstrom, M., Sullivan, J., Waxman, S., & Yurovsky, D. (2017). A Collaborative Approach to Infant Research: Promoting Reproducibility, Best Practices, and Theory-Building. Infancy, 22, 421–435. https://doi.org/10.1111/infa.12182\n\nKlein, R. A., Ratliff, K. A., Vianello, M., Adams, R. B., Bahník, Š., Bernstein, M. J., & et al. (2014). Investigating variation in replicability: A “many labs” replication project. Social Psychology, 45, 142–152. https://doi.org/10.1027/1864-9335/a000178\n\nKlein, R. A., Vianello, M., Hasselman, F., Adams, B. G., Adams, R. B., Alper, S., & … Nosek, B. A. (2018). Many Labs 2: Investigating Variation in Replicability Across Samples and Settings. Advances in Methods and Practices in Psychological Science, 1(4), 443–490. https://doi.org/10.1177/2515245918810225\n\nMoshontz, H., Campbell, L., Ebersole, C. R., IJzerman, H., Urry, H. L., Forscher, P. S., & Chartier, C. R. (2018). The Psychological Science Accelerator: Advancing psychology through a distributed collaborative network. Advances in Methods and Practices in Psychological Science, 1(4), 501–515. https://doi.org/10.1177/2515245918797607\n\nPavlov, Y. G., Adamian, N., Appelhoff, S., Arvaneh, M., Benwell, C., Beste, C., & Mushtaq, F. (2020). #EEGManyLabs: Investigating the Replicability of Influential EEG Experiments. PsyArXiv Preprint. https://doi.org/10.31234/osf.io/528nr",
                "drafted_by": [
                    "Sam Parsons"
                ],
                "reviewed_by": [
                    "Helena Hartmann",
                    "Charlotte R. Pennington",
                    "Mirela Zaneva"
                ],
                "alt_related_terms": [
                    null
                ],
                "language": "german"
            },
            {
                "type": "glossary",
                "title": "MOOCs (Massive Open Online Courses (MOOCs))",
                "definition": "Kurse, die ausschließlich online stattfinden, für jeden Lernenden jederzeit zugänglich sind, in der Regel kostenlos sind (wenn auch nicht unbedingt offen lizenziert) und videobasierte Anleitungen sowie herunterladbare Datensätze und Übungen bieten. Der Aspekt \"massive\" beschreibt die große Anzahl von Lernenden, die aufgrund der Flexibilität, der geringen oder gar nicht anfallenden Kosten und der Online-Natur der Materialien gleichzeitig auf den Kurs zugreifen können.",
                "related_terms": [
                    "Accessibility",
                    "Distance education",
                    "Inclusion",
                    "Open learning"
                ],
                "references": "Baturay, M. H. (2015). An overview of the world of MOOCs. Procedia-Social and Behavioral Sciences, 174, 427–433. https://doi.org/10.1016/j.sbspro.2015.01.685",
                "drafted_by": [
                    "Elizabeth Collins"
                ],
                "reviewed_by": [
                    "Tsvetomira Dumbalska",
                    "Mahmoud Elsherif",
                    "Helena Hartmann",
                    "Sam Parsons",
                    "Charlotte R. Pennington"
                ],
                "alt_related_terms": [
                    null
                ],
                "language": "german"
            },
            {
                "type": "glossary",
                "title": "MOOPs (Massively Open Online Papers (MOOPs))",
                "definition": "Im Gegensatz zum traditionellen kollaborativen Artikel folgt ein MOOP einem offenen, partizipativen und dynamischen Modell, das nicht durch eine vorgegebene Liste von Mitwirkenden eingeschränkt ist.",
                "related_terms": [
                    "Citizen science",
                    "Collaboration",
                    "Crowdsourced Research",
                    "Many authors",
                    "Team science"
                ],
                "references": "Himmelstein, D. S., Rubinetti, V., Slochower, D. R., Hu, D., Malladi, V. S., Greene, C. S., & Gitter, A. (2019). Open collaborative writing with Manubot. PLOS Computational Biology, 15(6), e1007128. https://doi.org/10.1371/journal.pcbi.1007128\n\nTennant, J., Bielczyk, N. Z., Greshake Tzovaras, B., Masuzzo, P., & Steiner, T. (2019). Introducing Massively Open Online Papers (MOOPs). MetaArXiv. https://doi.org/10.31222/osf.io/et8ak",
                "drafted_by": [
                    "Ali H. Al-Hoorie"
                ],
                "reviewed_by": [],
                "alt_related_terms": [
                    null
                ],
                "language": "german"
            },
            {
                "type": "glossary",
                "title": "in science Matthew-Effekt in der Wissenschaft (Matthew effect (in science))",
                "definition": "Benannt nach dem Sprichwort ‘rich get richer; poor get poorer’ (dt. Die Reichen werden reicher, die Armen werden ärmer) aus dem Matthäus-Evangelium. Herausragende Forschende und Nachwuchsforschende mit einem angesehenen Stipendium erhalten unverhältnismäßig viel Anerkennung und Finanzierung für ihre Beiträge zur Wissenschaft, während relativ unbekannte oder Nachwuchsforschende ohne ein angesehenes Stipendium unverhältnismäßig wenig Anerkennung für vergleichbare Beiträge erhalten. Die Folge ist ein erheblicher kumulativer Vorteil, der sich aus bescheidenen anfänglichen vergleichbaren Vorteilen ergibt (und umgekehrt).",
                "related_terms": [
                    "Matthew effect in education",
                    "Stigler’s law of eponymy"
                ],
                "references": "Bol, T., de Vaan, M., & van de Rijt, A. (2018). The Matthew effect in science funding. Proceedings of the National Academy of Sciences, 115(19), 4887–4890. https://doi.org/10.1073/pnas.1719557115\n\nBornmann, L., Ganser, C., Tekles, A., & Leydesdorff, L. (2019). Does the hα index reinforce the Matthew effect in science? Agent-based simulations using Stata and R. arXiv preprint https://arxiv.org/abs/1905.11052.\n\nMerton, R. K. (1968). The Matthew Effect in Science. Science, 159(3810), 56–63. https://doi.org/10.1126/science.159.3810.56",
                "drafted_by": [
                    "Tamara Kalandadze"
                ],
                "reviewed_by": [
                    "Bradley Baker",
                    "Tsvetomira Dumbalska",
                    "Mahmoud Elsherif",
                    "Matt Jaquiery",
                    "Charlotte R. Pennington"
                ],
                "alt_related_terms": [
                    null
                ],
                "language": "german"
            },
            {
                "type": "glossary",
                "title": "Meta-Analyse (Meta-analysis)",
                "definition": "Eine Meta-Analyse ist eine statistische Zusammenfassung von Ergebnissen aus einer Reihe von Studien, die dasselbe Phänomen untersuchen. Es gibt eine Vielzahl von meta-analytischen Ansätzen, darunter Modelle mit zufälligen oder festen Effekten oder Meta-Regressionen, die eine Untersuchung von Moderationseffekten ermöglichen. Durch die Aggregation von Daten aus mehreren Studien könnte eine Meta-Analyse eine genauere Schätzung für ein Phänomen (z. B. die Art der Behandlung) liefern als einzelne Studien. Die Ergebnisse werden in der Regel in einem Forest Plot dargestellt. Meta-Analysen können auch dazu beitragen, die Heterogenität zwischen Studienergebnissen zu untersuchen. Meta-Analysen werden häufig in Verbindung mit systematischen Literaturzusammenfassungen (Reviews) durchgeführt und erfordern ebenfalls eine systematische Suche und ein Screening von Studien. Publikationsverzerrung (Publication bias) wird ebenfalls häufig im Rahmen einer Meta-Analyse untersucht und in der Regel anhand eines Trichterdiagramms (Funnel Plots) visuell dargestellt.",
                "related_terms": [
                    "CONSORT",
                    "Correlational Meta-Analysis",
                    "Effect size",
                    "Evidence synthesis",
                    "Non-Intervention, Reproducible, and Open Systematic Reviews (NIRO-SR)",
                    "PRISMA",
                    "Publication bias (File Drawer Problem)",
                    "STROBE",
                    "Systematic Review"
                ],
                "references": "Borenstein, M., Hedges, L. V., Higgins, J. P., & Rothstein, H. R. (2011). Introduction to meta-analysis. John Wiley & Sons.\n\nYeung, S. K., Feldman, G., Fillon, A., Protzko, J., Elsherif, M. M., Xiao, Q., & Pickering, J. (2020). Experimental Studies Meta-Analysis Registered Report Templates. https://osf.io/ytgrp/",
                "drafted_by": [
                    "Martin Vasilev; Siu Kit Yeung"
                ],
                "reviewed_by": [
                    "Thomas Rhys Evans",
                    "Tamara Kalandadze",
                    "Charlotte R. Pennington",
                    "Mirela Zaneva"
                ],
                "alt_related_terms": [
                    null
                ],
                "language": "german"
            },
            {
                "type": "glossary",
                "title": "Metadaten (Metadata)",
                "definition": "Strukturierte Daten, die andere Daten beschreiben und zusammenfassen. Metadaten können helfen, Daten zu finden, zu organisieren und zu verstehen. Beispiele für Metadaten sind Urheber:in, Titel, Mitwirkende, Schlüsselwörter (keywords), Tags sowie jede Art von Information, die zur Überprüfung und zum Verständnis der Ergebnisse und Schlussfolgerungen einer Studie erforderlich ist, z. B. ein Codebuch für Datenbeschriftungen, Beschreibungen, die Stichprobe und den Prozess der Datenerhebung.",
                "related_terms": [
                    "Data",
                    "Open Data **Alternative definition:** (if applicable) Data about data"
                ],
                "references": "Gollwitzer, M., Abele-Brehm, A., Fiebach, C., Ramthun, R., Scheel, A. M., Schönbrodt, F. D., & Steinberg, U. (2020). Data Management and Data Sharing in Psychological Science: Revision of the DGPs Recommendations.",
                "drafted_by": [
                    "Matt Jaquiery"
                ],
                "reviewed_by": [
                    "Helena Hartmann",
                    "Tina Lonsdorf",
                    "Charlotte R. Pennington",
                    "Mirela Zaneva"
                ],
                "alt_related_terms": [
                    null
                ],
                "language": "german"
            },
            {
                "type": "glossary",
                "title": "Meta-Wissenschaft oder Meta-Forschung (Meta-science or Meta-research)",
                "definition": "Die wissenschaftliche Untersuchung der Wissenschaft selbst mit dem Ziel, wissenschaftliche Praktiken zu beschreiben, zu erklären, zu bewerten und/oder zu verbessern. Meta-Wissenschaft untersucht typischerweise wissenschaftliche Methoden, Analysen, die Berichterstattung und Auswertung von Daten, die Reproduzierbarkeit und Replizierbarkeit von Forschungsergebnissen sowie Anreize in der Wissenschaft.",
                "related_terms": [],
                "references": "Ioannidis, J. P., Fanelli, D., Dunne, D. D., & Goodman, S. N. (2015). Meta-research: Eevaluation and improvement of research methods and practices. PLoS Biology, 13(10), e1002264. https://doi.org/10.1371/journal.pbio.1002264\n\nPeterson, D., & Panofsky, A. (2020). Metascience as a scientific social movement. https://doi.org/10.31235/osf.io/4dsqa",
                "drafted_by": [
                    "Elizabeth Collins"
                ],
                "reviewed_by": [
                    "Tamara Kalandadze",
                    "Lisa Spitzer",
                    "Olmo van den Akker"
                ],
                "alt_related_terms": [
                    null
                ],
                "language": "german"
            },
            {
                "type": "glossary",
                "title": "computational Rechenmodell (Model (computational))",
                "definition": "Rechenmodelle zielen darauf ab, untersuchte Phänomene mathematisch zu übersetzen, um komplexe Verhaltensweisen besser zu verstehen, zu vermitteln und vorherzusagen.",
                "related_terms": [
                    "algorithms",
                    "data simulation",
                    "hypothesis",
                    "theory",
                    "theory building"
                ],
                "references": "Guest, O., & Martin, A. E. (2020). How computational modeling can force theory building in psychological science. Perspectives on Psychological Science. https://doi.org/10.1177/1745691620970585\n\nWilson, R. C., & Collins, A. G. (2019). Ten simple rules for the computational modeling of behavioral data. eLife, 8, e49547. https://doi.org/10.7554/eLife.49547",
                "drafted_by": [
                    "Charlotte R. Pennington"
                ],
                "reviewed_by": [
                    "Jamie P. Cockcroft",
                    "Mahmoud Elsherif",
                    "Meng Liu",
                    "Yu-Fang Yang",
                    "Michele C. Lim"
                ],
                "alt_related_terms": [
                    null
                ],
                "language": "german"
            },
            {
                "type": "glossary",
                "title": "statistical statistisches Modell (Model (statistical))",
                "definition": "Eine mathematische Repräsentation beobachteter Daten, die darauf abzielt, die untersuchte Population widerzuspiegeln, und die ein besseres Verständnis des interessierenden Phänomens, die Identifizierung von Beziehungen zwischen den Variablen und Vorhersagen über zukünftige Fälle ermöglicht. Ein klassisches Beispiel ist die Anwendung von Chi-Quadrat, um den Zusammenhang zwischen Rauchen und Krebs zu verstehen (Doll & Hill, 1954).",
                "related_terms": [
                    "Bayesian Inference",
                    "Model (computational)",
                    "Model (philosophy)",
                    "Null Hypothesis Significance Testing (NHST) **Alternative definition:** A mathematical model that embodies a set of statistical assumptions concerning the generation of sample data and is used to apply statistical analysis."
                ],
                "references": "Doll, R., & Hill, A. B. (1954). The mortality of doctors in relation to their smoking habits; a preliminary report. British Medical Journal, 1(4877), 1451–1455. https://doi.org/10.1136/bmj.1.4877.1451",
                "drafted_by": [
                    "Jamie P. Cockcroft"
                ],
                "reviewed_by": [
                    "Alaa AlDoh",
                    "Mahmoud Elsherif",
                    "Meng Liu",
                    "Catia M. Oliveira",
                    "Charlotte R. Pennington"
                ],
                "alt_related_terms": [
                    null
                ],
                "language": "german"
            },
            {
                "type": "glossary",
                "title": "philosophy (Model (philosophy))",
                "definition": "Der Prozess, bei dem eine verbale Beschreibung formalisiert wird, um Mehrdeutigkeiten zu beseitigen und gleichzeitig die Dimensionen einzuschränken, die eine Theorie umfassen kann. Das Modell wird also aus Daten abgeleitet. “Many scientific models are representational models: they represent a selected part or aspect of the world, which is the model’s target system” (übersetzt \"Viele wissenschaftliche Modelle sind Repräsentationsmodelle: Sie stellen einen ausgewählten Teil oder Aspekt der Welt dar, der das Zielsystem des Modells ist\"; Frigg & Hartman, 2020).",
                "related_terms": [
                    "Hypothesis",
                    "Theory",
                    "Theory building"
                ],
                "references": "Guest, O., & Martin, A. E. (2020). How computational modeling can force theory building in psychological science. Perspectives on Psychological Science. https://doi.org/10.1177/1745691620970585",
                "drafted_by": [
                    "Mahmoud Elsherif"
                ],
                "reviewed_by": [
                    "Charlotte R. Pennington",
                    "Michele C. Lim"
                ],
                "alt_related_terms": [
                    null
                ],
                "language": "german"
            },
            {
                "type": "glossary",
                "title": "Multi-Analytiker:innen-Studien (Multi-Analyst Studies)",
                "definition": "In typischen empirischen Studien führt ein:e einzige:r Forschende:r oder ein Forschungsteam die Analyse durch, was zu Unsicherheit darüber führt, inwieweit die Wahl der Analyse die Ergebnisse beeinflusst. Bei Studien mit mehreren Analytiker:innen analysieren zwei oder mehr Forschende unabhängig voneinander dieselbe Forschungsfrage oder Hypothese anhand desselben Datensatzes. Laut Aczel und Kollegen (2021) kann ein Multi-Analytiker:innen-Ansatz vorteilhaft sein, um das Vertrauen in ein bestimmtes Ergebnis zu erhöhen, die Auswirkungen der analytischen Präferenzen verschiedener Forschungsteams aufzudecken und die Variabilität solcher analytischen Ansätze hervorzuheben.",
                "related_terms": [
                    "Analytic flexibility",
                    "Crowdsourcing science",
                    "Data Analysis",
                    "Garden of Forking Paths",
                    "Multiverse Analysis",
                    "Researcher Degrees of Freedom",
                    "Scientific Transparency"
                ],
                "references": "Aczel, B., Szaszi, B., Nilsonne, G., Van den Akker, O., Albers, C. J., van Assen, M. A. L. M., ..., & Wagenmakers, E. (2021). Guidance for Multi-Analyst Studies. https://doi.org/10.31222/osf.io/5ecnh\n\nSilberzahn, R., Uhlmann, E. L., Martin, D. P., Anselmi, P., Aust, F., Awtrey, E., Bahník, Š., Bai, F., Bannard, C., Bonnier, E., & others. (2018). Many Analysts, One Data Set: Making Transparent How Variations in Analytic Choices Affect Results. Advances in Methods and Practices in Psychological Science, 337–356. https://doi.org/10.1177/2515245917747646",
                "drafted_by": [
                    "Sam Parsons"
                ],
                "reviewed_by": [
                    "Tsvetomira Dumbalska",
                    "Mahmoud Elsherif",
                    "William Ngiam",
                    "Charlotte R. Pennington",
                    "Graham Reid",
                    "Barnabas Szaszi",
                    "Flávio Azevedo"
                ],
                "alt_related_terms": [
                    null
                ],
                "language": "german"
            },
            {
                "type": "glossary",
                "title": "Multiplizität (Multiplicity)",
                "definition": "Potenzielle Inflation der Typ I \\- Fehlerraten (fälschliche Zurückweisung der Nullhypothese) aufgrund mehrfacher statistischer Tests, z. B. mehrere Ergebnisse, mehrere Follow-up-Zeitpunkte oder mehrere Teilgruppenanalysen. Um Probleme mit der Multiplizität zu überwinden, wenden Forschende häufig Kontrollverfahren an (z. B. Bonferroni, Holm-Bonferroni; Tukey), die den Alpha-Wert korrigieren, um die Inflation von Typ-I-Fehlern zu kontrollieren. Durch die Kontrolle von Fehlern des Typs I kann jedoch die Möglichkeit von Fehlern des Typs II (d. h. das falsche Beibehalten der Nullhypothese) erhöht werden.",
                "related_terms": [
                    "Alpha",
                    "False Discovery Rate",
                    "Multiple comparisons problem",
                    "Multiple testing",
                    "Null Hypothesis Significance Testing (NHST)"
                ],
                "references": "Sato, T. (1996). Type I and Type II error in multiple comparisons. The Journal of Psychology, 130(3), 293–302. https://doi.org/10.1080/00223980.1996.9915010\n\nSchulz, K. F., & Grimes, D. A. (2005). Multiplicity in randomised trials I: endpoints and treatments. The Lancet, 365(9470), 1591–1595. https://doi.org/10.1016/S0140-6736(05)66461-6",
                "drafted_by": [
                    "Aidan Cashin"
                ],
                "reviewed_by": [
                    "Jamie P. Cockcroft",
                    "Mahmoud Elsherif",
                    "Meng Liu",
                    "Charlotte R. Pennington"
                ],
                "alt_related_terms": [
                    null
                ],
                "language": "german"
            },
            {
                "type": "glossary",
                "title": "Multiversumsanalyse (Multiverse analysis)",
                "definition": "Multiversums-Analysen basieren auf allen potenziell gleichermaßen vertretbaren Datenverarbeitungs- und statistischen Analysemöglichkeiten, die zur Prüfung einer einzigen Hypothese eingesetzt werden können. Bei einer Daten-Multiversums-Analyse wird ein einzelner Rohdatensatz zu einem Multiversum von Datensätzen verarbeitet, indem alle möglichen Kombinationen von vertretbaren Vorverarbeitungsentscheidungen angewendet werden. Bei Modell-Multiversums-Analysen werden gleichermaßen vertretbare statistische Modelle auf dieselben Daten angewendet, um dieselbe Hypothese zu beantworten. Die statistische Analyse wird dann für alle Datensätze im Multiversum durchgeführt, und alle Ergebnisse werden berichtet, was die Transparenz fördert und die Robustheit der Ergebnisse gegenüber verschiedenen Datenverarbeitungs- (Daten-Multiversum) oder statistischen (Modell-Multiversum) Pipelines verdeutlicht. Die Multiversum-Analyse unterscheidet sich von der Spezifikationskurven-Analyse durch die grafische Darstellung (Histogramm und Kacheldiagramm anstelle eines Spezifikationskurvendiagramms).",
                "related_terms": [
                    "Garden of forking paths",
                    "Robustness (analyses)",
                    "Specification curve analysis",
                    "Vibration of effects"
                ],
                "references": "Del Giudice, M., & Gangestad, S. W. (2021). A traveler’s guide to the multiverse: Promises, pitfalls, and a framework for the evaluation of analytic decisions. Advances in Methods and Practices in Psychological Science, 4(1), 2515245920954925. https://doi.org/10.1177/2515245920954925\n\nSteegen, S., Tuerlinckx, F., Gelman, A., & Vanpaemel, W. (2016). Increasing Transparency through a Multiverse Analysis. Perspectives on Psychological Science, 11, 702–712. https://doi.org/10.1177/1745691616658637",
                "drafted_by": [
                    "Tina Lonsdorf; Flávio Azevedo"
                ],
                "reviewed_by": [
                    "Mahmoud Elsherif",
                    "Adrien Fillon",
                    "William Ngiam",
                    "Sam Parsons"
                ],
                "alt_related_terms": [
                    null
                ],
                "language": "german"
            },
            {
                "type": "glossary",
                "title": "Problem mehrdeutiger Namen (Name Ambiguity Problem)",
                "definition": "Ein Zuordnungsproblem, das sich aus zwei miteinander zusammenhängenden Problemen ergibt: Autor:innen können mehrere Namen oder Pseudonyme verwenden, um ihre Arbeiten zu veröffentlichen, und mehrere Autor:innen eines Fachgebiets können denselben vollständigen Namen haben. Dies macht die genaue Identifizierung von Autor:innen allein anhand von Namen und Fachgebieten zu einer schwierigen Aufgabe. Dieses Problem kann durch die Schaffung und Verwendung eindeutiger digitaler Identifikatoren gelöst werden, die wie digitale Fingerabdrücke funktionieren, wie z. B. ORCID.",
                "related_terms": [
                    "Authorship",
                    "DOI (digital object identifier)",
                    "ORCID (Open Researcher and Contributor ID)"
                ],
                "references": "Wilson, B., & Fenner, M. (2012). Open Researcher & Contributor ID (ORCID): Solving the Name Ambiguity Problem. Educause Review - E-Content, 47(3), 54–55. https://er.educause.edu/articles/2012/5/open-researcher--contributor-id-orcid-solving-the-name-ambiguity-problem",
                "drafted_by": [
                    "Shannon Francis"
                ],
                "reviewed_by": [
                    "Tsvetomira Dumbalska",
                    "Mahmoud Elsherif",
                    "Helena Hartmann",
                    "Wanyin Li",
                    "Charlotte R. Pennington"
                ],
                "alt_related_terms": [
                    null
                ],
                "language": "german"
            },
            {
                "type": "glossary",
                "title": "NETANOS (Named entity-based Text Anonymization for Open Science (NETANOS))",
                "definition": "Eine kostenlose, quelloffene Anonymisierungssoftware, die benannte Entitäten (z. B. Personen, Orte, Zeiten, Daten) identifiziert und verändert. Ihr Hauptmerkmal besteht darin, dass sie den für Sekundäranalysen benötigten kritischen Kontext bewahrt. Ziel ist es, Forscher:innen bei der gemeinsamen Nutzung ihrer ursprünglichen Textdaten zu unterstützen und sich dabei an Forschungsethik zu halten.",
                "related_terms": [
                    "Anonymity",
                    "Confidentiality",
                    "Data sharing",
                    "Research ethics"
                ],
                "references": "Kleinberg, B., Mozes, M., van der Toolen, Y., & Verschuere, B. (2017). NETANOS - Named entity-based Text Anonymization for Open Science. https://osf.io/w9nhb/",
                "drafted_by": [
                    "Norbert Vanek"
                ],
                "reviewed_by": [
                    "Jamie P. Cockcroft",
                    "Aleksandra Lazić",
                    "Charlotte R. Pennington",
                    "Sam Parsons"
                ],
                "alt_related_terms": [
                    null
                ],
                "language": "german"
            },
            {
                "type": "glossary",
                "title": "NIRO-SR (Non-Intervention, Reproducible, and Open Systematic Reviews (NIRO-SR))",
                "definition": "Ein umfassender Satz von Instrumenten zur Erleichterung der Entwicklung, Präregistrierung und Veröffentlichung von systematischen Literaturarbeiten für nicht-interventionelle Forschung. Teil A enthält detaillierte Leitlinien für die Erstellung und Präregistrierung eines systematischen Reviewprotokolls im Kontext der nicht-interventionellen Forschung und kreiert dabei gleichzeitig die Rahmenbedingungen für spätere Transparenz. Teil B enthält Leitlinien für das Verfassen der fertigen systematischen Literaturarbeit, wobei der Schwerpunkt auf der Verbesserung der Reproduzierbarkeit liegt.",
                "related_terms": [
                    "Knowledge accumulation",
                    "Systematic review",
                    "Systematic Review Protocol"
                ],
                "references": "Topor, M., Pickering, J. S., Barbosa Mendes, A., Bishop, D. V. M., Büttner, F. C., Elsherif, M. M., & others. (2021). An integrative framework for planning and conducting Non-Intervention, Reproducible, and Open Systematic Reviews (NIRO-SR). https://doi.org/10.31222/osf.io/8gu5z",
                "drafted_by": [
                    "Asma Assaneea"
                ],
                "reviewed_by": [
                    "Tsvetomira Dumbalska",
                    "Thomas Rhys Evans",
                    "Tamara Kalandadze",
                    "Jade Pickering",
                    "Mirela Zaneva"
                ],
                "alt_related_terms": [
                    null
                ],
                "language": "german"
            },
            {
                "type": "glossary",
                "title": "NHST, Nullhypothesen-Signifikanztestung (Null Hypothesis Significance Testing (NHST))",
                "definition": "Ein frequentistischer Ansatz für Schlussfolgerungen, der verwendet wird, um die Wahrscheinlichkeit eines beobachteten Effekts angesichts der Nullhypothese eines fehlenden Effekts/Zusammenhangs zu testen (Pernet, 2015). Eine solche Schlussfolgerung wird durch die Verwendung eines Indexes, des sogenannten p-Wertes, erreicht. Insbesondere schließen Forscher:innen auf das Vorhandensein eines Effekts, wenn ein von den Forscher:innen vorher festgelegter Alpha-Schwellenwert erfüllt ist; dieser bestimmt das akzeptable Maß an Unsicherheit und ist eng mit dem Typ-I-Fehler verbunden.",
                "related_terms": [
                    "Inference",
                    "P-value",
                    "Statistical significance",
                    "Type I error"
                ],
                "references": "Lakens, D., Scheel, A. M., & Isager, P. M. (2018). Equivalence testing for psychological research: A tutorial. Advances in Methods and Practices in Psychological Science, 1(2), 259–269. https://doi.org/10.1177/2515245918770963\n\nPernet, C. R. (2015). Null hypothesis significance testing: a short tutorial. F1000Research, 4, 621. https://doi.org/10.12688/f1000research.6963.3\n\nSpence, J. R., & Stanley, D. J. (2018). Concise, simple, and not wrong: In search of a short-hand interpretation of statistical significance. Frontiers in Psychology, 9, 2185. https://doi.org/10.3389/fpsyg.2018.02185",
                "drafted_by": [
                    "Alaa AlDoh"
                ],
                "reviewed_by": [
                    "Jamie P. Cockcroft",
                    "Annalise A. LaPlume",
                    "Charlotte R. Pennington",
                    "Sonia Rishi"
                ],
                "alt_related_terms": [
                    null
                ],
                "language": "german"
            },
            {
                "type": "glossary",
                "title": "Objektivität (Objectivity)",
                "definition": "Die Idee, dass wissenschaftliche Behauptungen, Methoden, Ergebnisse und Wissenschaftler:innen selbst wertfrei und unvoreingenommen bleiben sollten und somit nicht von kulturellen, politischen, herkunftsbezogenen oder religiösen Vorurteilen sowie persönlichen Interessen beeinflusst werden sollten (Merton, 1942).",
                "related_terms": [
                    "Communality",
                    "Mertonian norms",
                    "Neutrality"
                ],
                "references": "Macfarlane, B., & Cheng, M. (2008). Communism, Universalism and Disinterestedness: Re-examining Contemporary Support among Academics for Merton’s Scientific Norms. Journal of Academic Ethics, 6(1), 67–78. https://doi.org/10.1007/s10805-008-9055-y\n\nMerton, R. K. (1942). A note on science and democracy. Journal of Legal and Political Sociology, 1, 115–126. https://doi.org/10.1515/9783110375008-013",
                "drafted_by": [
                    "Ryan Millager"
                ],
                "reviewed_by": [
                    "Mahmoud Elsherif",
                    "Madeleine Ingham",
                    "Kai Krautter",
                    "Sam Parsons",
                    "Charlotte R. Pennington"
                ],
                "alt_related_terms": [
                    null
                ],
                "language": "german"
            },
            {
                "type": "glossary",
                "title": "Artificial Intelligence Ontologie in der Künstlichen Intelligenz (Ontology (Artificial Intelligence))",
                "definition": "Eine Sammlung von Grundregeln (axioms) in einem Fachgebiet, die zur Klassifizierung und Erklärung der Art der untersuchten Einheiten und der Beziehungen zwischen ihnen beitragen.",
                "related_terms": [
                    "Axiology",
                    "Epistemology",
                    "Taxonomy"
                ],
                "references": "Noy, N. F., & McGuinness, D. L. (2001). Ontology development 101: A guide to creating your first ontology. https://corais.org/sites/default/files/ontology_development_101_aguide_to_creating_your_first_ontology.pdf",
                "drafted_by": [
                    "Emma Norris"
                ],
                "reviewed_by": [
                    "Charlotte R. Pennington",
                    "Graham Reid"
                ],
                "alt_related_terms": [
                    null
                ],
                "language": "german"
            },
            {
                "type": "glossary",
                "title": "Open access",
                "definition": "Free availability of scholarship on the public internet, permitting any users to read, download, copy, distribute, print, search, or link to the full texts of these research articles, crawl them for indexing, pass them as data to software, or use them for any other lawful purpose, without financial, legal, or technical barriers other than those inseparable from gaining access to the internet itself” (Boai, 2002; dt.: Freie Verfügbarkeit von wissenschaftlichen Arbeiten im öffentlichen Internet, die es allen Nutzer:innen erlaubt, die Volltexte dieser Forschungsartikel zu lesen, herunterzuladen, zu kopieren, zu verbreiten, zu drucken, zu durchsuchen oder mit ihnen zu verlinken, sie für die Indexierung zu durchsuchen, sie als Daten an Software weiterzugeben oder sie für jeden anderen rechtmäßigen Zweck zu nutzen, ohne andere finanzielle, rechtliche oder technische Hindernisse als die, die untrennbar mit dem Zugang zum Internet selbst verbunden sind). Verschiedene Methoden zur Erreichung von Open Access werden oft farblich gekennzeichnet, darunter Green Open Access (wenn das Werk über ein öffentliches Repositorium zugänglich ist), Gold Open Access (wenn das Werk unmittelbar nach der Veröffentlichung über eine Zeitschriften-Website zugänglich ist) und Platin (oder Diamant) Open Access (eine Untergruppe von Gold Open Access, bei der alle Werke in der Zeitschrift unmittelbar nach der Veröffentlichung über die Zeitschriften-Website zugänglich sind, ohne dass die Autor:innen eine Gebühr für die Bearbeitung von Artikeln \\[article processing fee; APC\\] zahlen müssen).",
                "related_terms": [
                    "Article Processing Charge",
                    "FAIR principles",
                    "Paywall",
                    "Preprint",
                    "Repository"
                ],
                "references": "Budapest Open Access Initiative. (2002). Read the Budapest open access initiative. Retrieved from https://www.budapestopenaccessinitiative.org/read\n\nSuber, P. (2015). Open Access Overview. http://legacy.earlham.edu/~peters/fos/overview.htm",
                "drafted_by": [
                    "Mahmoud Elsherif"
                ],
                "reviewed_by": [
                    "Nick Ballou",
                    "Helena Hartmann",
                    "Aoife O’Mahony",
                    "Ross Mounce",
                    "Mariella Paul",
                    "Charlotte R. Pennington"
                ],
                "alt_related_terms": [
                    null
                ],
                "language": "german"
            },
            {
                "type": "glossary",
                "title": "Offener Code (Open Code)",
                "definition": "Freier und öffentlicher Zugang zu Computercode (z. B. Programmierung, Analysecode, Stimulusgenerierung), um die Forschungsmethodik und \\-analyse transparent zu machen und Reproduzierbarkeit und Zusammenarbeit zu ermöglichen. Der Code kann z. B. über Open-Code-Websites wie GitHub, das Open Science Framework und Codeshare zur Verfügung gestellt werden, so dass andere den Code bewerten, Fehler korrigieren und ihn für spätere Forschungsarbeiten wiederverwenden und verändern können.",
                "related_terms": [
                    "Computational Reproducibility",
                    "Open Access",
                    "Open Licensing",
                    "Open Material",
                    "Open Source",
                    "Open Source Software",
                    "Reproducibility",
                    "Syntax"
                ],
                "references": "Easterbrook, S. M. (2014). Open code for open science? Nature Geoscience, 7(11), 779–781. https://doi.org/10.1038/ngeo2283",
                "drafted_by": [
                    "Charlotte R. Pennington"
                ],
                "reviewed_by": [
                    "Elizabeth Collins",
                    "Mahmoud Elsherif",
                    "Christopher Graham",
                    "Emma Henderson"
                ],
                "alt_related_terms": [
                    null
                ],
                "language": "german"
            },
            {
                "type": "glossary",
                "title": "Offene Daten (Open Data)",
                "definition": "Offene Daten sind frei verfügbare und direkt zugängliche Daten, die von anderen uneingeschränkt genutzt werden können. \"Open data and content can be freely used, modified, and shared by anyone for any purpose” (dt. Offene Daten und Inhalte können von jeder:m zu jedem Zweck frei verwendet, verändert und weitergegeben werden\" ) ([https://opendefinition.org/](https://opendefinition.org/)). Offene Daten unterliegen sowohl dem Erfordernis der Kennzeichnung als auch des Teilens, daher ist es wichtig, geeignete Lizenzen (Open Licenses) in Betracht zu ziehen. Sensible oder zeitkritische Datensätze können mit einer Sperrfrist (embargo) belegt oder mit selektiveren Zugriffsoptionen freigegeben werden, um die Integrität der Daten zu gewährleisten.",
                "related_terms": [
                    "Badges (Open Science)",
                    "Data availability",
                    "FAIR principles",
                    "Metadata",
                    "Open Licenses",
                    "Open Material",
                    "Reproducibility",
                    "Secondary data analysis"
                ],
                "references": "Definition, T. O. (n.d.). The Open Definition—Open Definition—Defining Open in Open Data, Open Content and Open Knowledge. Open Knowledge Foundation. https://opendefinition.org/\n\nHandbook, O. D. (n.d.). What is Open Data? Retrieved 9 July 2021. https://opendatahandbook.org/guide/en/what-is-open-data/",
                "drafted_by": [
                    "Lisa Spitzer"
                ],
                "reviewed_by": [
                    "Sarah Ashcroft-Jones",
                    "Mahmoud Elsherif",
                    "Helena Hartmann",
                    "Matt Jaquiery",
                    "Flávio Azevedo",
                    "Ross Mounce",
                    "Charlotte R. Pennington",
                    "Steven Verheyen"
                ],
                "alt_related_terms": [
                    null
                ],
                "language": "german"
            },
            {
                "type": "glossary",
                "title": "OERs, Offene Bildungsressourcen (Open Educational Resources (OERs))",
                "definition": "Lernmaterialien, die verändert und verbessert werden können, weil ihre Urheber:innen anderen die Erlaubnis dazu erteilt haben. Die Personen oder Organisationen, die OER erstellen \\- zu denen Materialien wie Präsentationsfolien, Podcasts, Lehrpläne, Bilder, Unterrichtspläne, Vorlesungsvideos, Karten, Arbeitsblätter und sogar ganze Lehrbücher gehören können \\- verzichten auf einige (oder alle) der mit ihren Werken verbundenen Urheberrechte, in der Regel über rechtliche Instrumente wie Creative-Commons-Lizenzen, so dass andere frei darauf zugreifen, sie wiederverwenden, übersetzen und verändern können.",
                "related_terms": [
                    "Accessibility",
                    "FORRT",
                    "Open access",
                    "Open Licenses",
                    "Open Material"
                ],
                "references": "Opensource.Com. (n.d.). What is open education? Retrieved 9 July 2021. https://opensource.com/resources/what-open-education",
                "drafted_by": [
                    "Aleksandra Lazić"
                ],
                "reviewed_by": [
                    "Sam Parsons",
                    "Charlotte R. Pennington",
                    "Steven Verheyen",
                    "Elizabeth Collins"
                ],
                "alt_related_terms": [
                    null
                ],
                "language": "german"
            },
            {
                "type": "glossary",
                "title": "OER (Open Educational Resources (OER) Commons)",
                "definition": "OER Commons (OER steht für Open Educational Resources, aus dem Engl. offene Bildungsressourcen) ist eine frei zugängliche Online-Bibliothek, die es Lehrkräften ermöglicht, Bildungsressourcen zu erstellen, zu teilen und neu zu kombinieren. Das Ziel der OER-Bewegung ist es, kollaboratives Lehren und Lernen (https://www.oercommons.org/about) zu fördern und qualitativ hochwertige Bildungsressourcen bereitzustellen, die für jeden zugänglich sind.",
                "related_terms": [
                    "Equity",
                    "FORRT",
                    "Inclusion",
                    "Open Scholarship Knowledge Base",
                    "Open Science Framework"
                ],
                "references": "OER Commons. (n.d.). OER Commons. https://www.oercommons.org/",
                "drafted_by": [
                    "Ali H. Al-Hoorie"
                ],
                "reviewed_by": [
                    "Mahmoud Elsherif, Gisela H. Govaart"
                ],
                "alt_related_terms": [
                    null
                ],
                "language": "german"
            },
            {
                "type": "glossary",
                "title": "Offene Lizenzen (Open Licenses)",
                "definition": "Offene Lizenzen werden mit offenen Daten und offener Software (z. B. Analysecode) bereitgestellt, um festzulegen, wie andere das lizenzierte Material (wieder-)verwenden können. Indem sie die Rechte und Einschränkungen festlegen, erlauben offene Lizenzen oft den uneingeschränkten Zugang, die Wiederverwendung und die Weitergabe der Originalarbeit einer:s Autor:in. Datensätze werden in der Regel unter einer Art von offener Lizenz lizenziert, die als Creative-Commons-Lizenz bekannt ist (z. B. MIT, Apache und GPL). Diese können sich auf relativ subtile Weise unterscheiden, wobei die GPL-Lizenzen (und ihre Varianten) Copyleft-Lizenzen sind, die verlangen, dass jede abgeleitete Arbeit unter denselben Bedingungen wie das Original lizenziert wird.",
                "related_terms": [
                    "Creative Commons (CC) License",
                    "Copyleft",
                    "Copyright",
                    "Licence",
                    "Open Data",
                    "Open Source"
                ],
                "references": "",
                "drafted_by": [
                    "Andrew J. Stewart"
                ],
                "reviewed_by": [
                    "Elizabeth Collins",
                    "Sam Parsons",
                    "Graham Reid",
                    "Steven Verheyen"
                ],
                "alt_related_terms": [
                    null
                ],
                "language": "german"
            },
            {
                "type": "glossary",
                "title": "offene Materialien (Open Material)",
                "definition": "Die öffentliche Freigabe von Materialien, die in einer Studie verwendet wurden, “such as survey items, stimulus materials, and experiment programs” (Kidwell et al., 2016, S. 3, aus dem Engl. \"wie z. B. Umfrageelemente, Stimulusmaterialien und Experimental-Programme\") durch den/die Autor:in. Digital teilbare Materialien werden auf Open-Access-Repositorien veröffentlicht, wodurch sie öffentlich verfügbar und zugänglich werden. Je nach Lizenzierung kann das Material von anderen Autor:innen für ihre eigenen Studien wiederverwendet werden. Komponenten, die nicht digital weitergegeben werden können (z. B. biologische Materialien, Geräte), müssen ausreichend detailliert beschrieben werden, um eine Reproduzierbarkeit zu ermöglichen.",
                "related_terms": [
                    "Badges (Open Science)",
                    "Credibility of scientific claims",
                    "FAIR principles",
                    "Open Access",
                    "Open Code",
                    "Open Data",
                    "Reproducibility",
                    "Transparency"
                ],
                "references": "Blohowiak, B. B., Cohoon, J., de Wit, L., Eich, E., Farach, F. J., Hasselman, F., & others. (2020). Badges to Acknowledge Open Practices. Retrieved from https://osf.io/tvyxz\n\nKidwell, M. C., Lazarević, L. B., Baranski, E., Hardwicke, T. E., Piechowski, S., Falkenberg, L. S., & Nosek, B. A. (2016). Badges to acknowledge open practices: A simple, low-cost, effective method for increasing transparency. PLoS Biology, 14(5), e1002456. https://doi.org/10.1371/journal.pbio.1002456",
                "drafted_by": [
                    "Lisa Spitzer"
                ],
                "reviewed_by": [
                    "Sam Parsons",
                    "Charlotte R. Pennington",
                    "Olly Robertson",
                    "Emily A. Williams",
                    "Flávio Azevedo"
                ],
                "alt_related_terms": [
                    null
                ],
                "language": "german"
            },
            {
                "type": "glossary",
                "title": "OpenNeuro",
                "definition": "Eine kostenlose Plattform, auf der Forscher:innen Daten aus Bildgebungsverfahren des Gehirns (z. B. MRI-, MEG-, EEG-, iEEG-, EKoG-, ASL- und PET-Daten) frei und offen austauschen, durchsuchen, herunterladen und wiederverwenden können.",
                "related_terms": [
                    "BIDS data structure",
                    "Open data",
                    "OpenfMRI"
                ],
                "references": "Poldrack, R. A., Barch, D. M., Mitchell, J. P., Wager, T. D., Wagner, A. D., Devlin, J. T., Cumba, C., Koyejo, O., & Milham, M. P. (2013). Toward open sharing of task-based fMRI data: The OpenfMRI project. Frontiers in Neuroinformatics, 7, 1–12. https://doi.org/10.3389/fninf.2013.00012\n\nPoldrack, R. A., & Gorgolewski, K. J. (2014). Making big data open: Data sharing in neuroimaging. Nature Neuroscience, 17(11), 1510–1517. https://doi.org/10.1038/nn.3818\n\nOpenNeuro. (n.d.). A free and open platform for sharing MRI, MEG, EEG, iEEG, ECoG, ASL, and PET data—OpenNeuro. OpenNeuro. https://openneuro.org/",
                "drafted_by": [
                    "Ali H. Al-Hoorie"
                ],
                "reviewed_by": [
                    "Leticia Micheli, Gisela H. Govaart"
                ],
                "alt_related_terms": [
                    null
                ],
                "language": "german"
            },
            {
                "type": "glossary",
                "title": "Offene Begutachtung (Open Peer Review)",
                "definition": "Ein wissenschaftlicher Begutachtungsmechanismus, der die Offenlegung der Identität von Autor:innen und Gutachter:innen sowie von Peer-Review-Gutachten und editorischen Entscheidungsschreiben untereinander oder gegenüber der Öffentlichkeit zu jedem Zeitpunkt während oder nach dem Peer-Review- oder Veröffentlichungsprozess ermöglicht. Der Begriff kann sich auch auf die Aufhebung von Beschränkungen für die Teilnahme am Peer-Review-Verfahren und die entsprechenden Plattformen dafür beziehen. Beachten Sie, dass der Begriff \"offene Begutachtung\" austauschbar verwendet wird, um sich auf jede der oben genannten Praktiken oder auf alle zu beziehen.",
                "related_terms": [
                    "Non-anonymised peer review",
                    "Open science",
                    "PRO (peer review openness) initiative",
                    "Transparent peer review"
                ],
                "references": "",
                "drafted_by": [
                    "Sonia Rishi"
                ],
                "reviewed_by": [
                    "Mahmoud Elsherif",
                    "Sam Parsons",
                    "Charlotte R. Pennington",
                    "Yuki Yamada",
                    "Flávio Azevedo"
                ],
                "alt_related_terms": [
                    null
                ],
                "language": "german"
            },
            {
                "type": "glossary",
                "title": "Open Scholarship",
                "definition": "Der Begriff \"Open Scholarship\" wird oft synonym mit \"Open Science\" verwendet, erstreckt sich aber auf alle Disziplinen und schließt auch solche ein, die sich selbst traditionell nicht als wissenschaftlich bezeichnen. Er spiegelt die Idee wider, dass Wissen aller Art offen, transparent, rigoros (genau), reproduzierbar, replizierbar, akkumulierbar und inklusiv sein sollte (unter Berücksichtigung aller Wissenssysteme). Open Scholarship umfasst alle wissenschaftlichen Aktivitäten, die sich nicht nur auf die Forschung beschränken, wie Lehre und Pädagogik.",
                "related_terms": [
                    "Bropenscience",
                    "Decolonisation",
                    "Knowledge",
                    "Open Research",
                    "Open Science"
                ],
                "references": "Tennant, J., Beamer, J. E., Bosman, J., Brembs, B., Chung, N. C., Clement, G., Crick, T., Dugan, J., Dunning, A., Eccles, D., Enkhbayar, A., Graziotin, D., Harding, R., Havemann, J., Katz, D. S., Khanal, K., Kjaer, J. N., Koder, T., Macklin, P., & Turner, A. (2019). Foundations for Open Scholarship Strategy Development. MetaArXiv. https://doi.org/10.31222/osf.io/b4v8p",
                "drafted_by": [
                    "Gerald Vineyard"
                ],
                "reviewed_by": [
                    "Mahmoud Elsherif",
                    "Zoe Flack",
                    "Sam Parsons",
                    "Charlotte R. Pennington"
                ],
                "alt_related_terms": [
                    null
                ],
                "language": "german"
            },
            {
                "type": "glossary",
                "title": "Open Scholarship Knowledge Base",
                "definition": "Die Open Scholarship Knowledge Base (OSKB) ist eine gemeinschaftliche Initiative zum Austausch von Wissen über das Was, Warum und Wie von Open Scholarship, damit dieses Wissen leicht zu finden und anzuwenden ist. Die Informationen werden von der Gemeinschaft kuratiert und erstellt. Die OSKB ist eine Gemeinschaft im Rahmen des Center for Open Science (COS).",
                "related_terms": [
                    "Center for Open Science (COS), Open Educational Resources (OERs)",
                    "Open scholarship",
                    "Open Science"
                ],
                "references": "",
                "drafted_by": [
                    "Ali H. Al-Hoorie"
                ],
                "reviewed_by": [
                    "Mahmoud Elsherif",
                    "Samuel Guay",
                    "Tamara Kalandadze"
                ],
                "alt_related_terms": [
                    null
                ],
                "language": "german"
            },
            {
                "type": "glossary",
                "title": "Offene Forschung (Open Science)",
                "definition": "Ein Oberbegriff, der die Idee widerspiegelt, dass wissenschaftliche Erkenntnisse aller Art, wenn möglich, offen zugänglich, transparent, rigoros (genau), reproduzierbar, replizierbar, akkumulierbar und inklusiv sein sollten. Diese Eigenschaften werden als  grundlegende Merkmale wissenschaftlicher Bestrebungen angesehen. Offene Wissenschaft besteht aus Prinzipien und Verhaltensweisen, die eine transparente, glaubwürdige, reproduzierbare und zugängliche Wissenschaft fördern. Offene Wissenschaft hat sechs Hauptaspekte: offene Daten, offene Methodik, offene Quellcodes, offener Zugang (Open Access), offenes Peer Review und offene Bildungsressourcen.",
                "related_terms": [
                    "Accessibility",
                    "Credibility",
                    "Open Data",
                    "Open Material",
                    "Open Peer Review",
                    "Open Research",
                    "Open Science Practices",
                    "Open Scholarship",
                    "Reproducibility crisis (aka Replicability or replication crisis)",
                    "Reproducibility",
                    "Transparency"
                ],
                "references": "Abele-Brehm, A. E., Gollwitzer, M., Steinberg, U., & Schönbrodt, F. D. (2019). Attitudes toward open science and public data sharing. Social Psychology, 50, 252–260. https://doi.org/10.1027/1864-9335/a000384\n\nCrüwell, S., van Doorn, J., Etz, A., Makel, M. C., Moshontz, H., Niebaum, J. C., Orben, A., Parsons, S., & Schulte-Mecklenbeck, M. (2019). Seven Easy Steps to Open Science: An Annotated Reading List. Zeitschrift Für Psychologie, 227(4), 237–248. https://doi.org/10.1027/2151-2604/a000387\n\nKathawalla, U., Silverstein, P., & Syed, M. (2020). Easing into Open Science: A Guide for Graduate Students and Their Advisors. Collabra: Psychology. https://doi.org/10.31234/osf.io/vzjdp Retrieved from https://psyarxiv.com/vzjdp\n\nSyed, M. (2019). The Open Science Movement is for all of us. PsyArXiv.\n\nWoelfle, M., Olliaro, P., & Todd, M. H. (2011). Open science is a research accelerator. Nature Chemistry, 3(10), 745–748. https://doi.org/10.1038/nchem.1149",
                "drafted_by": [
                    "Mahmoud Elsherif"
                ],
                "reviewed_by": [
                    "Zoe Flack",
                    "Tamara Kalandadze",
                    "Charlotte R. Pennington",
                    "Qinyu Xiao"
                ],
                "alt_related_terms": [
                    null
                ],
                "language": "german"
            },
            {
                "type": "glossary",
                "title": "Open Science Framework",
                "definition": "Eine kostenlose open-source Plattform für Forschende zur Organisation und gemeinsamen Nutzung ihrer Forschungsprojekte und zur Förderung der Zusammenarbeit. Sie wird häufig als offenes Repositorium für Forschungscode, Daten und Materialien, Preprints und Präregistrierungen verwendet und ermöglicht gleichzeitig einen effizienteren Arbeitsablauf. Erstellt und gepflegt durch das Center for Open Science.",
                "related_terms": [
                    "Archive",
                    "Center for Open Science (COS)",
                    "Open Code",
                    "Open Data",
                    "Preprint",
                    "Preregistration"
                ],
                "references": "Foster, E. D., & Deardorff, A. (2017). Open science framework (OSF). Journal of the Medical Library Association, 105(2), 203. https://doi.org/10.5195/jmla.2017.88\n\nfor Open Science, C. (2011–2021). Open Science Framework. https://osf.io/",
                "drafted_by": [
                    "William Ngiam"
                ],
                "reviewed_by": [
                    "Mahmoud Elsherif",
                    "Charlotte R. Pennington",
                    "Lisa Spitzer"
                ],
                "alt_related_terms": [
                    null
                ],
                "language": "german"
            },
            {
                "type": "glossary",
                "title": "Software mit offenem Quellcode (Open Source software)",
                "definition": "Eine Art von Computersoftware, bei der der Quellcode unter einer Lizenz veröffentlicht wird, die es anderen erlaubt, die Software zu nutzen, zu verändern und an jede Person und für jeglichen Zweck weiterzugeben. Open Source ist mehr als nur offen zugänglich: Die Vertriebsbedingungen von Open-Source-Software müssen 10 spezifische Kriterien erfüllen (siehe: https://opensource.org/osd).",
                "related_terms": [
                    "Github",
                    "Open Access",
                    "Open Code",
                    "Open Data",
                    "Open Licenses",
                    "Python",
                    "R",
                    "Repository"
                ],
                "references": "Anon. (n.d.). Open Source in Open Science | FOSTER. Retrieved from https://www.fosteropenscience.eu/foster-taxonomy/open-source-open-science",
                "drafted_by": [
                    "Connor Keating"
                ],
                "reviewed_by": [
                    "Jamie P. Cockcroft",
                    "Helena Hartmann",
                    "Charlotte R. Pennington",
                    "Andrew J. Stewart"
                ],
                "alt_related_terms": [
                    null
                ],
                "language": "german"
            },
            {
                "type": "glossary",
                "title": "Open washing",
                "definition": "Open Washing, benannt nach dem Ausdruck \"Greenwashing\", bezieht sich auf die Behauptung von Offenheit, um den Eindruck von Rigorosität (Sorgfalt) oder Prestige zu erwecken, die mit offenen Praktiken verbunden sind. Der Begriff wurde verwendet, um die Marketingstrategie von Softwareunternehmen zu charakterisieren, die den Anschein von Open-Source und Open-Licensing erwecken, während sie proprietäre Praktiken anwenden. Open Washing ist ein wachsendes Problem für diejenigen, die offene wissenschaftliche Praktiken anwenden, da ihre Maßnahmen durch eine irreführende Verwendung der Praktiken untergraben werden und Maßnahmen zur Erleichterung fortschrittlicher Entwicklungen auf ein \"Abhaken von Checklisten\" ohne klare Qualitätskontrolle reduziert werden.",
                "related_terms": [
                    "Open Access",
                    "Open Data",
                    "Open Source"
                ],
                "references": "Farrow, R. (2017). Open Education and Critical Pedagogy. Learning, Media and Technology, 42(2), 130–146. https://doi.org/10.1080/17439884.2016.1113991\n\nMoretti, M. (2020). Beyond Open-washing: Are Narratives the Future of Open Data Portals? Medium Blog. https://medium.com/nightingale/beyond-open-washing-are-stories-and-narratives-the-future-of-open-data-portals-93228d8882f3\n\nVillum, C. (2016). “Open-washing” – The difference between opening your data and simply making them available. https://blog.okfn.org/2014/03/10/open-washing-the-difference-between-opening-your-data-and-simply-making-them-available/\n\nVlaeminck, S., & Podkrajac, F. (2017). Journals in Economic Sciences: Paying Lip Service to Reproducible Research? IASSIST Quarterly, 41(1–4), 16. https://doi.org/10.29173/iq6",
                "drafted_by": [
                    "Meng Liu"
                ],
                "reviewed_by": [
                    "Thomas Rhys Evans",
                    "Sam Guay",
                    "Sam Parsons",
                    "Charlotte R. Pennington",
                    "Beatrice Valentini"
                ],
                "alt_related_terms": [
                    null
                ],
                "language": "german"
            },
            {
                "type": "glossary",
                "title": "Optionales Stoppen (Optional Stopping)",
                "definition": "Die Praxis der (wiederholten) Analyse von Daten während der Datenerhebung und die Entscheidung, die Datenerhebung zu beenden, wenn ein statistisches Kriterium (z. B. der *p*\\-Wert oder der Bayes-Faktor) einen bestimmten Schwellenwert erreicht. Wenn geeignete methodische Vorkehrungen zur Kontrolle der Fehlerquote vom Typ 1 getroffen werden, kann dies ein effizientes Analyseverfahren sein (z. B. Lakens, 2014). Ohne eine transparente Berichterstattung oder eine angemessene Fehlerkontrolle kann der Typ-1-Fehler jedoch stark ansteigen, und Optionales Stoppen könnte als fragwürdige Forschungspraxis (questionable research practice; QRP) oder als eine Form des *p*\\-hacking betrachtet werden.",
                "related_terms": [
                    "*P*\\-hacking",
                    "Questionable Research Practices or Questionable Reporting Practices (QRPs)",
                    "Sequential testing"
                ],
                "references": "Beffara Bret, B., Beffara Bret, A., & Nalborczyk, L. (2021). A fully automated, transparent, reproducible, and blind protocol for sequential analyses. Meta-Psychology, 5. https://doi.org/10.15626/MP.2018.869\n\nLakens, D. (2014). Performing high-powered studies efficiently with sequential analyses. European Journal of Social Psychology, 44(7), 701–710. https://doi.org/10.1002/ejsp.2023\n\nSagarin, B. J., Ambler, J. K., & Lee, E. M. (2014). An ethical approach to peeking at data. Perspectives on Psychological Science, 9(3), 293–304. https://doi.org/10.1177/1745691614528214\n\nSchönbrodt, F. D., Wagenmakers, E.-J., Zehetleitner, M., & Perugini, M. (2017). Sequential hypothesis testing with Bayes factors: Efficiently testing mean differences. Psychological Methods, 22(2), 322–339. https://doi.org/10.1037/met0000061",
                "drafted_by": [
                    "Brice Beffara Bret; Bettina M. J. Kern"
                ],
                "reviewed_by": [
                    "Ali H. Al-Hoorie",
                    "Helena Hartmann",
                    "Catia M. Oliveira",
                    "Sam Parsons"
                ],
                "alt_related_terms": [
                    null
                ],
                "language": "german"
            },
            {
                "type": "glossary",
                "title": "Open Researcher and Contributor ID (ORCID (Open Researcher and Contributor ID))",
                "definition": "Eine Organisation, die ein Register mit dauerhaften eindeutigen Identifikatoren (ORCID iDs) für Forscher:innen und Akademiker:innen bereitstellt, so dass diese Nutzer:innen ihre digitalen Forschungsdokumente und andere Beiträge mit ihrem ORCID-Eintrag verknüpfen können. Die Registrierung für eine ORCID iD ist kostenlos aufhttps://orcid.org/register.",
                "related_terms": [
                    "Authorship",
                    "DOI (digital object identifier)",
                    "Name Ambiguity Problem"
                ],
                "references": "Haak, L. L., Fenner, M., Paglione, L., Pentz, E., & Ratner, H. (2012). ORCID: A system to uniquely identify researchers. Learned Publishing, 25(4), 259–264. https://doi.org/10.1087/20120404\n\nORCID. (n.d.). ORCID. https://orcid.org/",
                "drafted_by": [
                    "Martin Vasilev"
                ],
                "reviewed_by": [
                    "Bradley Baker",
                    "Mahmoud Elsherif",
                    "Shannon Francis",
                    "Charlotte R. Pennington",
                    "Emily A. Williams",
                    "Flávio Azevedo"
                ],
                "alt_related_terms": [
                    null
                ],
                "language": "german"
            },
            {
                "type": "glossary",
                "title": "Overlay Zeitschrift (Overlay Journal)",
                "definition": "Elektronische Open-Access-Zeitschriften, die Artikel aus anderen Quellen (in der Regel Preprint-Server wie arXiv) sammeln und kuratieren. Die Kuratierung der Artikel kann eine (nach der Veröffentlichung erfolgende) Peer-Review-Begutachtung oder redaktionelle Auswahl beinhalten. Overlay-Zeitschriften veröffentlichen kein neues Material, sondern organisieren und sammeln Artikel, die in bestehenden Repositories verfügbar sind.",
                "related_terms": [
                    "Open access",
                    "Preprint"
                ],
                "references": "Ginsparg, P. (1997). Winners and losers in the global research village. The Serials Librarian, 30(3–4), 83–95. https://doi.org/10.1300/J123v30n03_13\n\nGinsparg, P. (2001). Creating a global knowledge network. In Second Joint ICSU Press-UNESCO Expert Conference on Electronic Publishing in Science (pp. 19–23).\n\nBrown, J. (2010). An Introduction to Overlay Journals [Techreport]. Repositories Support Project: UK. https://discovery.ucl.ac.uk/id/eprint/19081/",
                "drafted_by": [
                    "Bradley Baker"
                ],
                "reviewed_by": [
                    "Christopher Graham",
                    "Helena Hartmann",
                    "Sam Parsons",
                    "Charlotte R. Pennington"
                ],
                "alt_related_terms": [
                    null
                ],
                "language": "german"
            },
            {
                "type": "glossary",
                "title": "P-Kurve (P-curve)",
                "definition": "Die P-Kurve ist ein Instrument zur Erkennung potenzieller Publikationsverzerrungen (bias) und nutzt die Verteilung signifikanter *p*\\-Werte in einer Reihe unabhängiger Befunde. Die Abweichung von der erwarteten rechtsschiefen Verteilung kann verwendet werden, um das Vorhandensein und das Ausmaß von Publikationsverzerrungen zu bewerten: Wenn die Kurve rechtsschief ist, gibt es mehr niedrige, hochsignifikante *p*\\-Werte, die einen zugrunde liegenden wahren Effekt widerspiegeln. Wenn die Kurve linksschief ist, gibt es viele kaum signifikante Ergebnisse knapp unter der .05-Schwelle. Dies deutet darauf hin, dass die Studien nicht aussagekräftig sind und möglicherweise durch fragwürdige Forschungspraktiken (questionable research practices, QRPs; z. B. *p*\\-hacking) untermauert werden. Wenn kein echter Effekt vorliegt (wahre Nullhypothese) und der p-Wert unverzerrt angegeben wird, sollte die P-Kurve eine flache, horizontale Linie sein, was die typische Verteilung der *p*\\-Werte darstellt.",
                "related_terms": [
                    "File-drawer",
                    "Hypothesis",
                    "*P*\\-hacking",
                    "*p*\\-value",
                    "Publication bias (File Drawer Problem)",
                    "Questionable Research Practices or Questionable Reporting Practices (QRPs)",
                    "Selective reporting",
                    "Z-curve"
                ],
                "references": "Bruns, S. B., & Ioannidis, J. P. (2016). P-curve and p-hacking in observational research. PLoS ONE, 11(2), e0149144. https://doi.org/10.1371/journal.pone.0149144\n\nSimonsohn, U., Nelson, L. D., & Simmons, J. P. (2014). P-curve: a key to the file-drawer. Journal of Experimental Psychology: General, 143(2), 534. https://doi.org/10.1037/a0030850\n\nSimonsohn, U., Nelson, L. D., & Simmons, J. P. (2014). P-curve and effect size: Correcting for publication bias using only significant results. Perspectives on Psychological Science, 9(6), 666–681. https://doi.org/10.1177/1745691614553988\n\nSimonsohn, U., Nelson, L. D., & Simmons, J. P. (2019). P-curve won’t do your laundry, but it will distinguish replicable from non-replicable findings in observational research: Comment on Bruns & Ioannidis (2016). PLoS ONE, 14(3), e0213454. https://doi.org/10.1371/journal.pone.0213454",
                "drafted_by": [
                    "Bettina M. J. Kern"
                ],
                "reviewed_by": [
                    "Sam Guay",
                    "Kamil Izydorczak",
                    "Charlotte R. Pennington",
                    "Robert M. Ross",
                    "Olmo van den Akker"
                ],
                "alt_related_terms": [
                    null
                ],
                "language": "german"
            },
            {
                "type": "glossary",
                "title": "p*****-hacking *",
                "definition": "Nutzung von Techniken, die die Wahrscheinlichkeit eines statistisch signifikanten Ergebnisses künstlich erhöhen können, indem sie das statistische Standard-Signifikanzkriterium (in der Regel α \\= .05) erfüllen. Zum Beispiel die Durchführung mehrerer Analysen und die Angabe nur derjenigen mit *p* \\< .05, das selektive Entfernen von Daten bis *p* \\< .05, die Auswahl von Variablen für die Verwendung in Analysen auf der Grundlage, ob diese Parameter statistisch signifikant sind.",
                "related_terms": [
                    "Analytic flexibility",
                    "Fishing",
                    "Garden of forking paths",
                    "HARKing",
                    "Questionable Research Practices or Questionable Reporting Practices (QRPs)",
                    "Selective reporting"
                ],
                "references": "Hardwicke, T. E., Jameel, L., Jones, M., Walczak, E. J., & Weinberg, L. M. (2014). Only human: Scientists, systems, and suspect statistics. Opticon1826, 16, 25. https://doi.org/10.5334/OPT.CH\n\nNeuroskeptic. (2012). The nine circles of scientific hell. Perspectives on Psychological Science, 7(6), 643–644. https://doi.org/10.1177/1745691612459519",
                "drafted_by": [
                    "Mahmoud Elsherif"
                ],
                "reviewed_by": [
                    "Tamara Kalandadze",
                    "William Ngiam",
                    "Sam Parsons",
                    "Martin Vasilev"
                ],
                "alt_related_terms": [
                    null
                ],
                "language": "german"
            },
            {
                "type": "glossary",
                "title": "*p*-Wert (p*****-value *)",
                "definition": "Eine Statistik, die zur Bewertung des Ergebnisses eines Hypothesentests bei der Nullhypothesen-Signifikanzprüfung (null hypothesis significance testing; NHST) verwendet wird. Sie bezieht sich auf die Wahrscheinlichkeit, einen Effekt oder einen extremeren Effekt zu beobachten, unter der Annahme, dass die Nullhypothese wahr ist (Lakens, 2021). In der Erklärung der American Statistical Association zu *p*\\-Werten (Wasserstein & Lazar, 2016\\) wird darauf hingewiesen, dass *p*\\-Werte kein Indikator für die Wahrheit der Nullhypothese sind, und stattdessen werden *p*\\-Werte wie folgt definiert: \"Informally, a *p*\\-value is the probability under a specified statistical model that a statistical summary of the data (e.g., the sample mean difference between two compared groups) would be equal to or more extreme than its observed value” (dt. Informell ist ein *p*\\-Wert die Wahrscheinlichkeit, dass eine statistische Zusammenfassung der Daten (z. B. die Stichprobenmittelwertdifferenz zwischen zwei verglichenen Gruppen) unter einem bestimmten statistischen Modell gleich oder extremer als der beobachtete Wert ist (S. 131).",
                "related_terms": [
                    "Null Hypothesis Statistical Testing (NHST)",
                    "statistical significance"
                ],
                "references": "psyTeachR Team. (n.d.). P | Glossary. psyTeachR. https://psyteachr.github.io/glossary\n\nLakens, D. (2021). The Practical Alternative to the p Value Is the Correctly Used p Value. Perspectives on Psychological Science, 16(3), 639–648. https://doi.org/10.1177/1745691620958012\n\nWasserstein, R. L., & Lazar, N. A. (2016). The ASA Statement on p-Values: Context, Process, and Purpose. The American Statistician, 70, 129–133. https://doi.org/10.1080/00031305.2016.1154108",
                "drafted_by": [
                    "Alaa AlDoh; Flávio Azevedo"
                ],
                "reviewed_by": [
                    "Jamie P. Cockcroft",
                    "Charlotte R. Pennington",
                    "Suzanne L. K. Stewart",
                    "Robbie C.M. van Aert",
                    "Marcel A.L.M. van Assen",
                    "Martin Vasilev"
                ],
                "alt_related_terms": [
                    null
                ],
                "language": "german"
            },
            {
                "type": "glossary",
                "title": "Papierfabrik (Papermill)",
                "definition": "Eine Organisation, die wissenschaftliches Fehlverhalten begeht, indem sie mehrere Arbeiten durch Fälschung oder Fabrikation von Daten erstellt, z. B. durch Bearbeitung von Abbildungen oder numerischen Daten oder durch Plagiat von geschriebenem Text. Es wird behauptet, dass Papierfabriken (aus dem Engl. Paper Mills) Produkte anbieten, die von Forschungsdaten bis hin zu betrügerischen oder gefälschten Ghostwriting-Manuskripten und Einreichungsdiensten reichen (“alleged to offer products ranging from research data through to ghostwritten fraudulent or fabricated manuscripts and submission services”, Byrne & Christopher, 2020, S. 583). Bei einer Papierfabrik geht es um die schnelle Produktion und Verbreitung einer Vielzahl angeblich neuer Arbeiten. Diese werden im wissenschaftlichen Veröffentlichungsprozess oft nicht entdeckt und daher entweder nie gefunden oder zurückgezogen, wenn sie entdeckt werden (z. B. durch Plagiaterkennungssoftware).",
                "related_terms": [
                    "Data fabrication",
                    "Data falsification",
                    "Fraud",
                    "Plagiarism",
                    "Questionable Research Practices or Questionable Reporting Practices (QRPs)",
                    "Scientific misconduct",
                    "Scientific publishing"
                ],
                "references": "Byrne, J. A., & Christopher, J. (2020). Digital magic, or the dark arts of the 21st century—how can journals and peer reviewers detect manuscripts and publications from paper mills? FEBS Letters, 594(4), 583–589. https://doi.org/10.1002/1873-3468.13747\n\nHackett, R., & Kelly, S. (2020). Publishing ethics in the era of paper mills. Biology Open, 9(10), bio056556. https://doi.org/10.1242/bio.056556",
                "drafted_by": [
                    "Helena Hartmann"
                ],
                "reviewed_by": [
                    "Sarah Ashcroft-Jones",
                    "Elizabeth Collins",
                    "Mahmoud Elsherif",
                    "Charlotte R. Pennington"
                ],
                "alt_related_terms": [
                    null
                ],
                "language": "german"
            },
            {
                "type": "glossary",
                "title": "Paradaten (Paradata)",
                "definition": "Daten, die über die Merkmale und den Kontext der von einer Person erhobenen Primärdaten erfasst werden \\- im Unterschied zu Metadaten. Paradaten können verwendet werden, um die Interaktion eines Befragten mit einer Umfrage oder einem Experiment auf einer Mikroebene zu untersuchen. Sie lassen sich am einfachsten bei computergestützten Umfragen erfassen, sind aber nicht darauf beschränkt. Beispiele hierfür sind die Antwortzeiten auf Umfrageelemente, wiederholte Antwortmuster wie die Wahl der gleichen Antwort für alle Fragen, kontextuelle Merkmale des Teilnehmers wie Verletzungen, die eine gute Leistung bei Aufgaben verhindern, die Anzahl der vorzeitigen Reaktionen auf Stimuli in einem Experiment. Paradaten wurden für die Untersuchung und Korrektur von Mess- und Stichprobenfehlern verwendet.",
                "related_terms": [
                    "Auxiliary data",
                    "Data collection",
                    "Data quality",
                    "Metadata",
                    "Process information"
                ],
                "references": "Kreuter, F. (Ed.). (2013). Improving Surveys with Paradata. https://doi.org/10.1002/9781118596869",
                "drafted_by": [
                    "Alexander Hart; Graham Reid"
                ],
                "reviewed_by": [
                    "Helena Hartmann",
                    "Charlotte R. Pennington",
                    "Marta Topor",
                    "Flávio Azevedo"
                ],
                "alt_related_terms": [
                    null
                ],
                "language": "german"
            },
            {
                "type": "glossary",
                "title": "PARKing",
                "definition": "PARKing (preregistering after results are known; dt. Präregistrieren nach Bekanntwerden der Ergebnisse) wird definiert als die Praxis, bei der Forschende ein Experiment (möglicherweise mit unendlichen Wiederholungsexperimenten) abschließen, bevor sie es präregistrieren. Diese Praxis macht den Zweck der Präregistrierung zunichte und ist eine der QRPs (questionable research practices; oder sogar wissenschaftliches Fehlverhalten), die nur versuchen, \"credibility that it has been preregistered\" (dt. die Glaubwürdigkeit, dass es präregistriert wurde\" zu erlangen.",
                "related_terms": [
                    "HARKing",
                    "Preregistration",
                    "Questionable Research Practices or Questionable Reporting Practices (QRPs)"
                ],
                "references": "Ikeda, A., Xu, H., Fuji, N., Zhu, S., & Yamada, Y. (2019). Questionable research practices following pre-registration. Japanese Psychological Review, 62, 281–295.\n\nYamada, Y. (2018). How to crack pre-registration: Toward transparent and open science. Frontiers in Psychology, 9, 1831. https://doi.org/10.3389/fpsyg.2018.01831",
                "drafted_by": [
                    "Qinyu Xiao"
                ],
                "reviewed_by": [
                    "Helena Hartmann",
                    "Sam Parsons",
                    "Yuki Yamada"
                ],
                "alt_related_terms": [
                    null
                ],
                "language": "german"
            },
            {
                "type": "glossary",
                "title": "Partizipative Forschung (Participatory Research)",
                "definition": "Partizipative Forschung bedeutet, die Ansichten von Menschen aus relevanten Gemeinschaften in den gesamten Forschungsprozess einzubeziehen, um gemeinsame Ziele zwischen Forscher:innen und diesen Gemeinschaften zu erreichen. Dieser Ansatz nimmt eine kollaborative Haltung ein, die darauf abzielt, das Machtungleichgewicht zwischen dem Forschenden und den Erforschten durch eine systematische Miterschaffung von neuem Wissen zu verringern (“systematic cocreation of new knowledge”, Andersson, 2018).",
                "related_terms": [
                    "Collaborative research",
                    "Inclusion",
                    "Neurodiversity",
                    "Patient and Public Involvement (PPI)",
                    "Transformative paradigm"
                ],
                "references": "Cornwall, A., & Jewkes, R. (1995). What is participatory research? Social Science & Medicine, 41(12), 1667–1676. https://doi.org/10.1016/0277-9536(95)00127-S\n\nFletcher-Watson, S., Adams, J., Brook, K., Charman, T., Crane, L., Cusack, J., Leekam, S., Milton, D., Parr, J. R., & Pellicano, E. (2019). Making the Future Together: Shaping Autism Research Through Meaningful Participation. Autism, 23(4), 943–953.\n\nKiernan, C. (1999). Participation in research by people with learning disability: Origins and issues. British Journal of Learning Disabilities, 27(2), 43–47. https://doi.org/10.1111/j.1468-3156.1999.tb00084.x\n\nLeavy, P. (2017). Research Design: Quantitative, Qualitative, Mixed Methods, Arts-Based, and Community-Based Participatory Research Approaches. The Guilford Press.\n\nOttmann, G., Laragy, C., Allen, J., & Feldman, P. (2011). Coproduction in practice: Participatory action research to develop a model of community aged care. Systemic Practice and Action Research, 24, 413–427. https://doi.org/10.1007/s11213-011-9192-x\n\nRose, D. (2018). Participatory research: Real or imagined. Social Psychiatry and Psychiatric Epidemiology, 53, 765–771. https://doi.org/10.1007/s00127-018-1549-3",
                "drafted_by": [
                    "Tamara Kalandadze"
                ],
                "reviewed_by": [
                    "Jamie P. Cockcroft",
                    "Bethan Iley",
                    "Halil E. Kocalar",
                    "Michele C. Lim"
                ],
                "alt_related_terms": [
                    null
                ],
                "language": "german"
            },
            {
                "type": "glossary",
                "title": "PPI, Einbindung von Patient:innen und Öffentlichkeit (Patient and Public Involvement (PPI))",
                "definition": "Aktive Forschungszusammenarbeit mit der interessierenden Population, im Gegensatz zur Forschung \"über\" sie. Forschende können die Lebenserfahrung und das Fachwissen von Patient:innen und der Öffentlichkeit in allen Phasen des Forschungsprozesses einbeziehen. So können Patient:innen beispielsweise bei der Entwicklung von Forschungsfragen helfen, die Eignung eines Studiendesigns überprüfen, Zusammenfassungen in einfacher Sprache für Drittmittel-/Ethikanträge und die Verbreitung genehmigen, Daten erheben und analysieren und bei der Veröffentlichung eines Projekts helfen. Dies wird zunehmend empfohlen und sogar von Drittmittelgebern gefordert (Boivin et al., 2018).",
                "related_terms": [
                    "Co-production",
                    "Participatory research"
                ],
                "references": "Boivin, A., Richards, T., Forsythe, L., Gregoire, A., L’Esperance, A., Abelson, J., & Carman, K. L. (2018). Evaluating the patient and public involvement in research. British Medical Journal, 363, k5147. https://doi.org/10.1136/bmj.k5147\n\nINVOLVE. (n.d.). INVOLVE – Supporting public involvement in NHS, public health and social care research. https://www.invo.org.uk/",
                "drafted_by": [
                    "Jade Pickering"
                ],
                "reviewed_by": [
                    "Mahmoud Elsherif",
                    "Sam Parsons",
                    "Catia M. Oliveira"
                ],
                "alt_related_terms": [
                    null
                ],
                "language": "german"
            },
            {
                "type": "glossary",
                "title": "Bezahlschranke (Paywall)",
                "definition": "Eine technische Hürde, die den Zugang zu Informationen nur für Personen ermöglicht, die \\- entweder persönlich oder über eine Organisation \\- eine bestimmte Gebühr oder ein Abonnement bezahlt haben.",
                "related_terms": [
                    "Accessibility",
                    "Open Access"
                ],
                "references": "Day, S., Rennie, S., Luo, D., & Tucker, J. D. (2020). Open to the public: Paywalls and the public rationale for open access medical research publishing. Research Involvement and Engagement, 6(1), 8. https://doi.org/10.1186/s40900-020-0182-y",
                "drafted_by": [
                    "Bradley Baker"
                ],
                "reviewed_by": [
                    "Mahmoud Elsherif",
                    "Sam Parsons",
                    "Charlotte R. Pennington",
                    "Julia Wolska"
                ],
                "alt_related_terms": [
                    null
                ],
                "language": "german"
            },
            {
                "type": "glossary",
                "title": "Peer Community In (PCI (Peer Community In))",
                "definition": "PCI ist eine gemeinnützige Organisation, die Gemeinschaften von Forschenden schafft, die unveröffentlichte Preprints auf der Grundlage eines hochwertigen Peer-Reviews von mindestens zwei Forschenden aus ihrem Fachgebiet überprüfen und empfehlen. Diesen Preprints wird dann ein DOI zugewiesen, ähnlich wie bei einem Zeitschriftenartikel. PCI wurde entwickelt, um ein kostenloses, transparentes und öffentliches wissenschaftliches Publikationssystem zu schaffen, das auf der Überprüfung und Empfehlung von Preprints beruht.",
                "related_terms": [
                    "Open Access",
                    "Open Archives",
                    "Open Peer Review",
                    "PCI Registered Reports",
                    "Peer review",
                    "Preprints"
                ],
                "references": "",
                "drafted_by": [
                    "Emma Henderson"
                ],
                "reviewed_by": [
                    "Jamie P. Cockcroft",
                    "Christopher Graham",
                    "Bethan Iley",
                    "Aleksandra Lazić",
                    "Charlotte R. Pennington"
                ],
                "alt_related_terms": [
                    null
                ],
                "language": "german"
            },
            {
                "type": "glossary",
                "title": "PCI Registered Reports",
                "definition": "Eine 2021 ins Leben gerufene Initiative, die sich der Aufnahme, Überprüfung und Empfehlung von Registered Reports (RRs) aus dem gesamten Spektrum der Naturwissenschaften, Technik, Ingenieurwissenschaften und Mathematik (MINT), Medizin, Sozial- und Geisteswissenschaften widmet. Peer Community In (PCI) RRs werden von einem \"Recommender\" (dt. Empfehlende:r, entspricht einem Action Editor) betreut und von mindestens zwei Expert:innen auf dem jeweiligen Gebiet begutachtet. Es bietet kostenlose und transparente Begutachtungen vor (Stufe 1\\) und nach der Studiendurchführung (Stufe 2\\) über alle Forschungsbereiche hinweg. Ein Netzwerk von PCI RR-freundlichen Zeitschriften unterstützt die PCI RR-Begutachtungskriterien und verpflichtet sich, RRs, die eine positive abschließende Empfehlung von PCI RR erhalten, ohne weitere Begutachtung zu akzeptieren.",
                "related_terms": [
                    "In Principle Acceptance (IPA)",
                    "Open Access",
                    "PCI (Peer Community In)",
                    "Publication bias (File Drawer Problem)",
                    "Registered Report",
                    "Results blind",
                    "Stage 1 study review",
                    "Stage 2 study review",
                    "Transparency"
                ],
                "references": "",
                "drafted_by": [
                    "Charlotte R. Pennington"
                ],
                "reviewed_by": [
                    "Sarah Ashcroft-Jones",
                    "Jamie P. Cockcroft",
                    "Mahmoud Elsherif",
                    "Helena Hartmann"
                ],
                "alt_related_terms": [
                    null
                ],
                "language": "german"
            },
            {
                "type": "glossary",
                "title": "Plan S",
                "definition": "Plan S ist eine im September 2018 von cOAlition S, einem Konsortium von drittmittelgebenden Organisationen, gestartete Initiative, die den Übergang zu vollständigem und sofortigem Open Access beschleunigen soll. Die teilnehmenden Förderorganisationen verlangen von den Empfänger:innen von Forschungsgeldern, dass sie ab 2021 in Open-Access-konformen Zeitschriften publizieren oder ihre Arbeiten sofort offen zugänglich machen. Die Förderer von cOAlition S haben sich verpflichtet, keine \"hybriden\" Open-Access-Publikationsgebühren in auf Abonnementen basierenden Publikationsplattformen finanziell zu unterstützen. Autor:innen können jedoch Plan S einhalten, indem sie Open Access in einer zu abonnierenden Zeitschrift im Rahmen einer \"transformativen Vereinbarung\" veröffentlichen, wie in den Durchführungsleitlinien näher beschrieben. Das \"S\" in Plan S steht für Schock.",
                "related_terms": [
                    "Open Access",
                    "DORA",
                    "Repository"
                ],
                "references": "",
                "drafted_by": [
                    "Olmo van den Akker"
                ],
                "reviewed_by": [
                    "Jamie P. Cockcroft",
                    "Helena Hartmann",
                    "Halil E. Kocalar",
                    "Birgit Schmidt"
                ],
                "alt_related_terms": [
                    null
                ],
                "language": "german"
            },
            {
                "type": "glossary",
                "title": "Positionalität (Positionality)",
                "definition": "Die Kontextualisierung sowohl der Forschungsumgebung als auch der Forschenden, um zu definieren, innerhalb welcher Grenzen die Forschungsarbeit durchgeführt wurde (Jaraf, 2018). Positionalität wird in der Regel in der qualitativen Forschung in den Mittelpunkt gestellt und gewürdigt, aber in jüngster Zeit wurde gefordert, sie auch in der quantitativen Forschung zu verwenden. Positionalitätserklärungen, in denen Forschende ihren Hintergrund und ihre \"Position\" innerhalb und gegenüber der Forschung darlegen, wurden als eine Methode zur Erkennung und Zentrierung des Bias (Voreingenommenheit) von Forschenden vorgeschlagen.",
                "related_terms": [
                    "Bias",
                    "Reflexivity",
                    "Perspective"
                ],
                "references": "Jafar, A. J. N. (2018). What is positionality and should it be expressed in quantitative studies? Emergency Medicine Journal, 35(5), 323–324. https://doi.org/10.1136/emermed-2017-207158",
                "drafted_by": [
                    "Joanne McCuaig"
                ],
                "reviewed_by": [
                    "Helena Hartmann",
                    "Aoife O’Mahony",
                    "Madeleine Pownall",
                    "Graham Reid"
                ],
                "alt_related_terms": [
                    null
                ],
                "language": "german"
            },
            {
                "type": "glossary",
                "title": "Positionalitätskarte (Positionality Map)",
                "definition": "Ein reflexives Instrument zur Anwendung von expliziter Positionalität in der qualitativ-kritischen Forschung. Die Karte soll als flexibler Ausgangspunkt verwendet werden, um Forscher:innen anzuleiten, über ihren sozialen Standort nachzudenken und ihn zu reflektieren. Die Karte umfasst drei Ebenen: die Identifizierung sozialer Identitäten (Ebene 1), wie diese Positionen unser Leben beeinflussen (Ebene 2\\) und Details, die mit den Besonderheiten unserer sozialen Identität verbunden sein können (Ebene 3). (aus dem Engl. “as a flexible starting point to guide researchers to reflect and be reflexive about their social location. The map involves three tiers: the identification of social identities (Tier 1), how these positions impact our life (Tier 2), and details that may be tied to the particularities of our social identity (Tier 3).”; Jacobson und Mustafa 2019, S. 1). Das Ziel der Karte ist es, dass Forschende ihre sozialen Positionen besser identifizieren und verstehen können, wie diese Herausforderungen und Aspekte der Einfachheit innerhalb des qualitativen Forschungsprozesses darstellen können (aus dem Engl. “for researchers to be able to better identify and understand their social locations and how they may pose challenges and aspects of ease within the qualitative research process.”)",
                "related_terms": [
                    "Positionality",
                    "Qualitative research",
                    "Social identity map",
                    "Transparency"
                ],
                "references": "Jacobson, D., & Mustafa, N. (2019). Social Identity Map: A Reflexivity Tool for Practicing Explicit Positionality in Critical Qualitative Research. International Journal of Qualitative Methods, 18, 1609406919870075. https://doi.org/10.1177/1609406919870075",
                "drafted_by": [
                    "Joanne McCuaig"
                ],
                "reviewed_by": [
                    "Helena Hartmann",
                    "Michele C. Lim",
                    "Charlotte R. Pennington",
                    "Graham Reid"
                ],
                "alt_related_terms": [
                    null
                ],
                "language": "german"
            },
            {
                "type": "glossary",
                "title": "Im Nachhinein (Post Hoc)",
                "definition": "Post hoc kommt aus dem Lateinischen und bedeutet \"im Nachhinein\". In der Statistik bezieht sich post hoc (oder Post-hoc-Analyse) auf die Prüfung von Hypothesen, die nicht vor der Datenanalyse festgelegt wurden. In der frequentistischen Statistik unterscheidet sich das Verfahren je nachdem, ob die Analyse geplant oder post hoc durchgeführt wurde, z. B. durch Anwendung einer strengeren Fehlerkontrolle. Im Gegensatz dazu unterscheiden sich Bayes'sche und Likelihood-Ansätze nicht in Abhängigkeit davon, wann die Hypothese festgelegt wurde.",
                "related_terms": [
                    "A priori, Ad hoc",
                    "HARKing",
                    "P-hacking"
                ],
                "references": "Dienes, Z. (2008). Understanding psychology as a science: An introduction to scientific and statistical inference. Macmillan International Higher Education.",
                "drafted_by": [
                    "Alaa Aldoh"
                ],
                "reviewed_by": [
                    "Sam Parsons",
                    "Jamie P. Cockcroft",
                    "Bethan Iley",
                    "Halil E. Kocalar",
                    "Graham Reid",
                    "Flávio Azevedo"
                ],
                "alt_related_terms": [
                    null
                ],
                "language": "german"
            },
            {
                "type": "glossary",
                "title": "Peer-Review nach der Veröffentlichung (Post Publication Peer Review)",
                "definition": "Peer-Review, das nach der Veröffentlichung einer Forschungsarbeit stattfindet. Es wird in der Regel auf einer speziellen Plattform (z. B. PubPeer) veröffentlicht. Es unterscheidet sich vom traditionellen Kommentar, der in der gleichen Zeitschrift veröffentlicht wird und in der Regel selbst einem Peer Review unterzogen wird.",
                "related_terms": [
                    "Open Peer Review",
                    "PeerPub",
                    "Peer review"
                ],
                "references": "",
                "drafted_by": [
                    "Ali H. Al-Hoorie"
                ],
                "reviewed_by": [
                    "Mahmoud Elsherif",
                    "Sam Parsons"
                ],
                "alt_related_terms": [
                    null
                ],
                "language": "german"
            },
            {
                "type": "glossary",
                "title": "Posterior distribution",
                "definition": "Eine Möglichkeit, das aktualisierte Wissen bei der Bayes'schen Inferenz zusammenzufassen, wobei das vorherige Wissen (prior knowledge) mit den beobachteten Daten abgeglichen wird. In der Statistik sind Posterior-Verteilungen proportional zum Produkt aus der Wahrscheinlichkeitsfunktion und dem Prior. Eine Posterior-Wahrscheinlichkeitsverteilung gibt die (Un-)Gewissheit über einen bestimmten Parameterwert an.",
                "related_terms": [
                    "Bayes Factor",
                    "Bayesian inference",
                    "Bayesian parameter estimation",
                    "Likelihood function",
                    "Prior distribution"
                ],
                "references": "Dienes, Z. (2014). Using Bayes to get the most out of non-significant results. Frontiers in Psychology, 5, 781. https://doi.org/10.3389/fpsyg.2014.00781\n\nLüdtke, O., Ulitzsch, E., & Robitzsch, A. (2020). A Comparison of Penalized Maximum Likelihood Estimation and Markov Chain Monte Carlo Techniques for Estimating Confirmatory Factor Analysis Models with Small Sample Sizes . https://doi.org/10.31234/osf.io/u3qag",
                "drafted_by": [
                    "Alaa AlDoh"
                ],
                "reviewed_by": [
                    "Adam Parker",
                    "Jamie P. Cockcroft",
                    "Julia Wolska",
                    "Yu-Fang Yang",
                    "Charlotte R. Pennington"
                ],
                "alt_related_terms": [
                    null
                ],
                "language": "german"
            },
            {
                "type": "glossary",
                "title": "Predatory Verlagswesen (Predatory Publishing)",
                "definition": "Unter Predatory (manchmal auch \"Vanity\") Veröffentlichung versteht man eine Reihe von Geschäftspraktiken, bei denen Verlage versuchen, in erster Linie durch die Erhebung von Bearbeitungsgebühren für Artikel (article processing charges, APCs) von der Veröffentlichung wissenschaftlicher Arbeiten zu profitieren, ohne notwendigerweise legitime Qualitätskontrollen (z. B. Peer Review) oder redaktionelle Dienstleistungen anzubieten. In der extremsten Form veröffentlichen solche Verlage jede Arbeit, solange die Gebühren bezahlt werden. Andere, weniger extreme Strategien, wie das Versenden einer großen Anzahl von unaufgeforderten Anfragen zum (Mit-)Herausgeben oder Veröffentlichen in kostenpflichtigen Sonderausgaben werden ebenfalls als \"predatory\" (dt. räuberisch) bezeichnet (Crosetto, 2021).",
                "related_terms": [
                    "Article Processing Charge (APC)",
                    "Gaming (the system)"
                ],
                "references": "Crosetto, P. (2021). Is MDPI a predatory publisher? https://paolocrosetto.wordpress.com/2021/04/12/is-mdpi-a-predatory-publisher/\n\nXia, J., Harmon, J. L., Connolly, K. G., Donnelly, R. M., Anderson, M. R., & Howard, H. A. (2015). Who publishes in “predatory” journals? Journal of the Association for Information Science and Technology, 66(7), 1406–1417. https://doi.org/10.1002/asi.23265",
                "drafted_by": [
                    "Nick Ballou"
                ],
                "reviewed_by": [
                    "Olmo van den Akker",
                    "Helena Hartmann",
                    "Aleksandra Lazić",
                    "Graham Reid",
                    "Flávio Azevedo"
                ],
                "alt_related_terms": [
                    null
                ],
                "language": "german"
            },
            {
                "type": "glossary",
                "title": "PREPARE Leitlinien (PREPARE Guidelines)",
                "definition": "Die PREPARE-Leitlinien und die Checkliste (Planning Research and Experimental Procedures on Animals: Recommendations for Excellence) sollen bei der Planung von Tierversuchen helfen, die Einhaltung der sogenannten 3Rs (Replacement, Reduction or Refinement, dt. Ersetzen, Reduzieren und Verfeinern) zu unterstützen und die Reproduzierbarkeit von Tierversuchen zu erleichtern.",
                "related_terms": [
                    "ARRIVE Guidelines",
                    "Reporting Guideline",
                    "STRANGE"
                ],
                "references": "Smith, A. J., Clutton, R. E., Lilley, E., Hansen, K. E. A., & Brattelid, T. (2018). PREPARE: Guidelines for planning animal research and testing. Laboratory Animals, 52(2), 135–141. https://doi.org/10.1177/0023677217724823",
                "drafted_by": [
                    "Ben Farrar"
                ],
                "reviewed_by": [
                    "Mahmoud Elsherif",
                    "Gilad Feldman",
                    "Elias Garcia-Pelegrin"
                ],
                "alt_related_terms": [
                    null
                ],
                "language": "german"
            },
            {
                "type": "glossary",
                "title": "Vorabdruck (Preprint)",
                "definition": "Eine öffentlich zugängliche Version eines wissenschaftlichen Manuskripts/Forschungsergebnisses, die der offiziellen Veröffentlichung vorausgeht und als eine Form des Grünen Open Access gilt. Preprints werden in der Regel auf einem Repository (z. B. arXiv) bereitgestellt, das die Verbreitung von Forschungsergebnissen durch eine schnellere Weitergabe als bei einer herkömmlichen Veröffentlichung erleichtert. Preprint-Repositorien vergeben in der Regel dauerhafte Identifikatoren (z. B. DOIs) für Preprints. Preprints können zu jedem Zeitpunkt des Forschungszyklus veröffentlicht werden, werden aber meist bei der Einreichung (d. h. vor der Begutachtung durch Fachkollegen über Peer Review) veröffentlicht. Angenommene und begutachtete Versionen von Artikeln werden ebenfalls häufig auf Preprintserver hochgeladen und als Postprints bezeichnet.",
                "related_terms": [
                    "Open Access",
                    "DOI (digital object identifier)",
                    "Postprint",
                    "Working Paper"
                ],
                "references": "Bourne, P. E., Polka, J. K., Vale, R. D., & Kiley, R. (2017). Ten simple rules to consider regarding preprint submission. PLoS Computational Biology, 13(5), e1005473. https://doi.org/10.1371/journal.pcbi.1005473\n\nElmore, S. A. (2018). Preprints: What Role Do These Have in Communicating Scientific Results? Toxicologic Pathology, 46(4), 364–365. https://doi.org/10.1177/0192623318767322",
                "drafted_by": [
                    "Mariella Paul"
                ],
                "reviewed_by": [
                    "Gisela H. Govaart",
                    "Helena Hartmann",
                    "Sam Parsons",
                    "Tobias Wingen",
                    "Flávio Azevedo"
                ],
                "alt_related_terms": [
                    null
                ],
                "language": "german"
            },
            {
                "type": "glossary",
                "title": "Präregistrierung (Preregistration)",
                "definition": "Die Praxis der Veröffentlichung des Plans für eine Studie, einschließlich der Forschungsfragen/Hypothesen, des Forschungsdesigns und der Datenanalyse, bevor die Daten erhoben oder untersucht wurden. Es ist auch möglich, Sekundärdatenanalysen zu präregistrieren (Merten & Krypotos, 2019). Eine Präregistrierung wird mit einem Zeitstempel versehen und in der Regel bei einer unabhängigen Partei (z. B. einem Repositorium) registriert, so dass es öffentlich mit anderen geteilt werden kann (möglicherweise nach einer Sperrfrist). Die Präregistrierung bietet eine transparente Dokumentation dessen, was zu einem bestimmten Zeitpunkt geplant war, und ermöglicht es Dritten, zu beurteilen, welche Änderungen sich im Nachhinein ergeben haben könnten. Je detaillierter eine Präregistrierung ist, desto besser können Dritte diese Änderungen und damit auch die Validität der durchgeführten Analysen beurteilen. Die Präregistrierung zielt darauf ab, konfirmatorische von explorativer Forschung klar zu unterscheiden.",
                "related_terms": [
                    "Confirmation bias",
                    "Confirmatory analyses",
                    "Exploratory Data Analysis",
                    "HARKing",
                    "Pre-analysis plan",
                    "Questionable Research Practices or Questionable Reporting Practices (QRPs)",
                    "Registered Report",
                    "Research Protocol",
                    "Transparency"
                ],
                "references": "Lewandowsky, S., & Bishop, D. (2016). Research integrity: Don’t let transparency damage science. Nature News, 529(7587), 459. https://doi.org/10.1038/529459a\n\nMertens, G., & Krypotos, A. M. (2019). Preregistration of analyses of preexisting data. Psychologica Belgica, 59(1), 338.\n\nNavarro, D. (2020). Paths in strange spaces: A comment on preregistration.\n\nNosek, B. A., Ebersole, C. R., DeHaven, A. C., & Mellor, D. T. (2018). The preregistration revolution. Proceedings of the National Academy of Sciences, 115(11), 2600–2606. https://doi.org/10.1073/pnas.1708274114\n\nSimmons, J., Nelson, L., & Simonsohn, U. (2021). Pre‐registration: Why and how. Journal of Consumer Psychology, 31(1), 151–162. https://doi.org/10.1002/jcpy.1208",
                "drafted_by": [
                    "Mahmoud Elsherif"
                ],
                "reviewed_by": [
                    "Gisela H. Govaart",
                    "Helena Hartmann",
                    "Tina Lonsdorf",
                    "William Ngiam",
                    "Eike Mark Rinke",
                    "Lisa Spitzer",
                    "Olmo van den Akker",
                    "Flávio Azevedo"
                ],
                "alt_related_terms": [
                    null
                ],
                "language": "german"
            },
            {
                "type": "glossary",
                "title": "Präregistrierungs-Versprechen (Preregistration Pledge)",
                "definition": "In einer \"kollektiven Aktion zur Unterstützung offener und reproduzierbarer Forschungspraktiken\" (aus dem Engl. “collective action in support of open and reproducible research practices'') ist der Preregistration Pledge eine Kampagne des Projekts Free Our Knowledge, die Forscher:innen auffordert, sich zu verpflichten, in den nächsten zwei Jahren mindestens eine Studie zu präregistrieren ([https://freeourknowledge.org/2020-12-03-preregistration-pledge/](https://freeourknowledge.org/2020-12-03-preregistration-pledge/)). Das Projekt ist eine Bewegung, die von Nachwuchsforscher:innen (early career researchers, ECRs) initiiert wurde.",
                "related_terms": [
                    "Preregistration"
                ],
                "references": "Knowledge, F. O. (2020). Preregistration Pledge. https://freeourknowledge.org/2020-12-03-preregistration-pledge/",
                "drafted_by": [
                    "Helena Hartmann"
                ],
                "reviewed_by": [
                    "Jamie P. Cockcroft",
                    "Mahmoud Elsherif",
                    "Aleksandra Lazić, Steven Verheyen"
                ],
                "alt_related_terms": [
                    null
                ],
                "language": "german"
            },
            {
                "type": "glossary",
                "title": "peer review openness initiative PRO Initiative (PRO (peer review openness) initiative)",
                "definition": "Die Vereinbarung mehrerer Forschender, dass sie ein Manuskript nur dann begutachten, wenn bestimmte Bedingungen erfüllt sind. Insbesondere sollten die Autor:innen des Manuskripts sicherstellen, dass die Daten und Materialien öffentlich zugänglich gemacht werden (oder begründen, warum sie nicht frei verfügbar sind oder weitergegeben werden), eine Dokumentation bereitstellen, in der detailliert beschrieben wird, wie Dateien oder Code zu interpretieren und auszuführen sind, und im Manuskript angeben, wo diese Dateien zu finden sind.",
                "related_terms": [
                    "Non-anonymised peer review",
                    "Open Science",
                    "Open Peer Review",
                    "Transparent peer review"
                ],
                "references": "Morey, R. D., Chambers, C. D., Etchells, P. J., Harris, C. R., Hoekstra, R., Lakens, D., Lewandowsky, S., Morey, C. C., Newman, D. P., Schönbrodt, F. D., Vanpaemel, W., Wagenmakers, E.-J., & Zwaan, R. A. (2016). The Peer Reviewers’ Openness Initiative: incentivizing open research practices through peer review. Royal Society Open Science, 3(1). https://doi.org/10.1098/rsos.150547",
                "drafted_by": [
                    "Jamie P. Cockcroft"
                ],
                "reviewed_by": [
                    "Sarah Ashcroft-Jones",
                    "Mahmoud Elsherif",
                    "Helena Hartmann",
                    "Steven Verheyen"
                ],
                "alt_related_terms": [
                    null
                ],
                "language": "german"
            },
            {
                "type": "glossary",
                "title": "a-priori Verteilung (Prior distribution)",
                "definition": "Überzeugungen, die Forscher:innen über die Parameter eines statistischen Modells haben, bevor weitere Erkenntnisse berücksichtigt werden. Ein \"Prior\" wird als Wahrscheinlichkeitsverteilung ausgedrückt und kann auf verschiedene Weise bestimmt werden (z. B. frühere Forschung, subjektive Einschätzung, Grundsätze wie die Maximierung der Entropie bei gegebenen Einschränkungen) und wird in der Regel mit der Likelihood Funktion unter Verwendung des Satzes von Bayes kombiniert, um eine Posterior-Verteilung zu erhalten.",
                "related_terms": [
                    "Bayes Factor",
                    "Bayesian inference",
                    "Bayesian Parameter Estimation",
                    "Likelihood function",
                    "Posterior distribution"
                ],
                "references": "",
                "drafted_by": [
                    "Alaa AlDoh"
                ],
                "reviewed_by": [
                    "Charlotte R. Pennington",
                    "Martin Vasilev"
                ],
                "alt_related_terms": [
                    null
                ],
                "language": "german"
            },
            {
                "type": "glossary",
                "title": "Pseudonymisierung (Pseudonymisation)",
                "definition": "Pseudonymisierung bezieht sich auf eine Technik, bei der alle Informationen, die zur Identifizierung von Versuchspersonen führen könnten, ersetzt oder entfernt werden, während sie durch die Kombination von Codenummer und Identifikatoren weiterhin identifizierbar bleiben. Dieser Prozess umfasst die folgenden Schritte: Entfernung aller Identifikatoren aus dem Forschungsdatensatz; Zuweisung eines spezifischen Identifikators (Pseudonym) für jede Versuchsperson und Verwendung dieses Pseudonyms zur Kennzeichnung jeweiligen Datensatzes; und Beibehaltung einer Verschlüsselung (z. B. einer Kodierliste), die die Codenummer mit der Identität der Versuchsperson in einem physisch vom Datensatz getrennten Dokument verknüpft. Die Pseudonymisierung ist in der Regel eine Mindestanforderung der Ethikkommissionen bei der Durchführung von Forschungsarbeiten, insbesondere bei menschlichen Teilnehmenden oder bei vertraulichen Informationen, um den Datenschutz zu gewährleisten.",
                "related_terms": [
                    "Anonymity",
                    "Confidentiality",
                    "Data privacy",
                    "De-identification",
                    "Pseudonymisation",
                    "Research ethics"
                ],
                "references": "Mourby, M., Mackey, E., Elliot, M., Gowans, H., Wallace, S. E., Bell, J., Smith, H., Aidinlis, S., & Kaye, J. (2018). Are ‘pseudonymised’ data always personal data? Implications of the GDPR for administrative data research in the UK. Computer Law & Security Review, 34(2), 222–233. https://doi.org/10.1016/j.clsr.2018.01.002\n\nMedical Research Council. (2019). Identifiability, anonymisation and pseudonymisation. Medical Research Council. https://mrc.ukri.org/documents/pdf/gdpr-guidance-note-5-identifiability-anonymisation-and-pseudonymisation/",
                "drafted_by": [
                    "Catia M. Oliveira"
                ],
                "reviewed_by": [
                    "Helena Hartmann",
                    "Sam Parsons",
                    "Charlotte R. Pennington",
                    "Birgit Schmidt"
                ],
                "alt_related_terms": [
                    null
                ],
                "language": "german"
            },
            {
                "type": "glossary",
                "title": "Pseudoreplikation (Pseudoreplication)",
                "definition": "Wenn die Daten nicht statistisch unabhängig sind und somit die Anzahl der Stichproben (d. h. der Wiederholungen) künstlich vergrößert wird. Zum Beispiel, wenn mehr als ein Datenpunkt von der gleichen Versuchseinheit (z. B. Versuchsperson oder Kulturpflanze) gesammelt wird. Es gibt zahlreiche Methoden, um dieses Problem zu lösen, wie z. B. die Mittelwertbildung über die Wiederholungen (z. B. die mittlere Reaktionszeit einer Versuchsperson) oder die Implementierung von Modellen mit gemischten Effekten (mixed effects), bei denen die Struktur der zufälligen Effekte die Pseudo-Vervielfältigung berücksichtigt (z. B. die Spezifizierung jeder einzelnen Reaktionszeit als zu derselben Person). Beachten Sie, dass die erste Option mit einem Verlust an Information und statistischer Aussagekraft verbunden wäre.",
                "related_terms": [
                    "Confounding",
                    "Generalizability",
                    "Replication",
                    "Validity"
                ],
                "references": "Davies, G. M., & Gray, A. (2015). Don’t let spurious accusations of pseudoreplication limit our ability to learn from natural experiments (and other messy kinds of ecological monitoring). Ecology and Evolution, 5(22), 5295–5304. https://doi.org/10.1002/ece3.1782\n\nHurlbert, S. H. (1984). Pseudoreplication and the Design of Ecological Field Experiments. Ecological Monographs, 54(2), 187–211. https://doi.org/10.2307/1942661\n\nLazic, S. E. (2019). Genuine replication and pseudoreplication: What’s the difference? In BMJ Open Science. https://blogs.bmj.com/openscience/2019/09/16/genuine-replication-and-pseudoreplication-whats-the-difference/",
                "drafted_by": [
                    "Ben Farrar"
                ],
                "reviewed_by": [
                    "Jamie P. Cockcroft",
                    "Mahmoud Elsherif",
                    "Elias Garcia-Pelegrin",
                    "Annalise A. LaPlume"
                ],
                "alt_related_terms": [
                    null
                ],
                "language": "german"
            },
            {
                "type": "glossary",
                "title": "Psychometrische Metaanalyse (Psychometric meta-analysis)",
                "definition": "Psychometrische Meta-Analysen zielen darauf ab, die Abschwächung der interessierenden Effektgrößen aufgrund von Messfehlern und anderen Artefakten durch Verfahren zu korrigieren, die auf psychometrischen Grundsätzen beruhen, z. B. der Zuverlässigkeit (Reliabilität) der Messgrößen. Diese Verfahren sollten durchgeführt werden, bevor die synthetisierten Effektgrößen in Korrelations- oder experimentellen Meta-Analysen verwendet werden, da die Durchführung dieser Korrekturen tendenziell zu größeren und weniger variablen Effektgrößen führt.",
                "related_terms": [
                    "Correlational meta-analysis",
                    "Hunter-Schmidt meta-analysis",
                    "Meta-analysis",
                    "Non-Intervention, Reproducible, and Open Systematic Reviews (NIRO-SR)",
                    "Publication bias (File Drawer Problem)",
                    "Validity generalization"
                ],
                "references": "Borenstein, M., Hedges, L. V., Higgins, J. P., & Rothstein, H. R. (2011). Introduction to meta-analysis. John Wiley & Sons.\n\nHunter, J. E., & Schmidt, F. L. (2015). Methods of Meta-Analysis: Correcting Error and Bias in Research Findings (Third). SAGE.",
                "drafted_by": [
                    "Adrien Fillon"
                ],
                "reviewed_by": [
                    "Mahmoud Elsherif",
                    "Eduardo Garcia-Garzon",
                    "Helena Hartmann",
                    "Catia M. Oliveira",
                    "Flávio Azevedo"
                ],
                "alt_related_terms": [
                    null
                ],
                "language": "german"
            },
            {
                "type": "glossary",
                "title": "File Drawer Problem Publikationsverzerrung; Aktenschubladenproblem (Publication bias (File Drawer Problem))",
                "definition": "Der Fehler, Ergebnisse nur auf der Grundlage der \"direction or strength of the study findings\" (dt. Richtung oder Stärke der Studienergebnisse) zu veröffentlichen (Dickersin & Min, 1993, S. 135). Die Voreingenommenheit (Bias) entsteht, wenn die Bewertung der Veröffentlichungswürdigkeit einer Studie unverhältnismäßig stark vom Ergebnis der Studie abhängt, oft mit der Tendenz, dass neuartige und signifikante Ergebnisse es mehr wert seien, veröffentlicht zu werden als Replikationen und Null-Befunde. Diese Voreingenommenheit äußert sich in der Regel durch eine unverhältnismäßig hohe Anzahl signifikanter Ergebnisse und überhöhte Effektgrößen. Dieser Prozess führt dazu, dass die veröffentlichte wissenschaftliche Literatur nicht repräsentativ für das gesamte Ausmaß der Forschung ist und insbesondere Null-Befunde unterrepräsentiert sind. Solche Ergebnisse wiederum landen in der so genannten \"Aktenschublade\" (File Drawer), wo sie nie veröffentlicht werden und keine auffindbare Dokumentation haben.",
                "related_terms": [
                    "Dissemination bias",
                    "P-curve",
                    "P-hacking",
                    "Selective reporting",
                    "Statistical significance",
                    "Trim and fill **Alternative definition:** In the context of meta-analysis, publication bias “...occurs whenever the research that appears in the published literature is systematically unrepresentative of the population of completed studies. Simply put, when the research that is readily available differs in its results from the results of all the research that has been done in an area, readers and reviewers of that research are in danger of drawing the wrong conclusion about what that body of research shows.” (Rothstein et al., 2005, p. 1\\) **Related terms to alternative definition:** meta-analysis"
                ],
                "references": "Dickersin, K., & Min, Y. (1993). Publication Bias: The Problem That Won’t Go Away. Annals of the New York Academy of Sciences, 703(1), 135–148. https://doi.org/10.1111/j.1749-6632.1993.tb26343.x\n\nDevito, N., & Goldacre, B. (2019). Publication Bias. Catalogue of Bias. https://catalogofbias.org/biases/publication-bias/\n\nDuval, S., & Tweedie, R. (2000). A nonparametric “trim and fill” method of accounting for publication bias in meta-analysis. Journal of the American Statistical Association, 95, 89–98. https://doi.org/10.2307/2669529\n\nDuval, S., & Tweedie, R. (2000). Trim and fill: A simple funnel-plot–based method of testing and adjusting for publication bias in meta-analysis. Biometrics, 56, 455–463. https://doi.org/10.1111/j.0006-341x.2000.00455.x\n\nFranco, A., Malhotra, N., & Simonovits, G. (2014). Publication bias in the social sciences: Unlocking the file drawer. Science, 345(6203), 1502–1505. https://doi.org/10.1126/science.1255484\n\nLindsay, D. S. (2020). Seven steps toward transparency and replicability in psychological science. Canadian Psychology/Psychologie Canadienne, 61(4), 310–317. https://doi.org/10.1037/cap0000222\n\nRothstein, H. R., Sutton, A. J., & Borenstein, M. (2005). Publication bias in meta-analysis. In Publication bias in meta-analysis: Prevention, assessment and adjustments (pp. 1–7). John Wiley & Sons, Ltd. https://doi.org/10.1002/0470870168.ch1",
                "drafted_by": [
                    "Mahmoud Elsherif"
                ],
                "reviewed_by": [
                    "Jamie P. Cockcroft",
                    "Gilad Feldman",
                    "Adrien Fillon",
                    "Helena Hartmann",
                    "Tamara Kalandadze",
                    "William Ngiam",
                    "Martin Vasilev",
                    "Olmo van den Akker",
                    "Flávio Azevedo"
                ],
                "alt_related_terms": [
                    null
                ],
                "language": "german"
            },
            {
                "type": "glossary",
                "title": "öffentliches Vertrauen in die Wissenschaft (Public Trust in Science)",
                "definition": "Vertrauen in das Wissen, die Leitlinien und Empfehlungen, die von Wissenschaftler:innen zum Nutzen der Zivilgesellschaft erarbeitet oder bereitgestellt wurden (Hendriks et al., 2016). Dies kann sich auch auf das Vertrauen in wissenschaftlich fundierte Empfehlungen zur öffentlichen Gesundheit (z. B. universelle Gesundheitsversorgung, Stammzellenforschung, Bundesmittel für die reproduktiven Rechte von Frauen, Präventivmaßnahmen für ansteckende Krankheiten und Impfungen), zum Klimawandel, zur Wirtschaftspolitik (z. B. Sozial-/Transferleistungen, Bekämpfung von Ungleichheit und Armut) und deren Überschneidungen beziehen. Das Vertrauen der Öffentlichkeit in die Wissenschaft wird nachweislich durch eine Vielzahl von Faktoren beeinflusst, wie Alter (Anderson et al., 2012), Geschlecht (Von Roten, 2004), Ablehnung wissenschaftlicher Normen (Lewandowsky & Oberauer, 2021), politische Ideologie (Azevedo & Jost, 2021; Brewer & Ley, 2012; Leiserowitz et al, 2010), Rechtsautoritarismus und soziale Dominanz (Kerr & Wilson, 2021), Bildung (Bak, 2001; Hayes & Tariq, 2000), Einkommen (Anderson et al., 2012), wissenschaftliche Kenntnisse (Evans & Durant, 1995; Nisbet et al., 2002), Nutzung sozialer Medien (Huber et al., 2019\\) und Religiosität (Azevedo, 2021; Brewer & Ley, 2013; Liu & Priest, 2009).",
                "related_terms": [
                    "Credibility of scientific claims",
                    "Epistemic Trust"
                ],
                "references": "Anderson, M. S., Ronning, E. A., Devries, R., & Martinson, B. C. (2010). Extending the Mertonian norms: Scientists’ subscription to norms of research. Journal of Higher Education, 81(3), 366–393. https://doi.org/10.1353/jhe.0.0095\n\nAzevedo, F., & Jost, J. T. (2021). The ideological basis of antiscientific attitudes: Effects of authoritarianism, conservatism, religiosity, social dominance, and system justification. Group Processes & Intergroup Relations, 24(4), 518–549. https://doi.org/10.1177/1368430221990104\n\nBak, H.-J. (2001). Education and Public Attitudes toward Science: Implications for the ‘Deficit Model’ of Education and Support for Science and Technology. Social Science Quarterly, 82(4), 779–795. https://www.jstor.org/stable/42955760\n\nBrewer, P. R., & Ley, B. L. (2013). Whose Science Do You Believe? Explaining Trust in Sources of Scientific Information About the Environment. Science Communication, 35(1), 115–137. https://doi.org/10.1177/1075547012441691\n\nEvans, G., & Durant, J. (1995). The relationship between knowledge and attitudes in the public understanding of science in Britain. Public Understanding of Science, 4(1), 57–74. https://doi.org/10.1088/0963-6625/4/1/004\n\nHayes, B. C., & Tariq, V. N. (2000). Gender differences in scientific knowledge and attitudes toward science: A comparative study of four Anglo-American nations. Public Understanding of Science, 9(4), 433–447. https://doi.org/10.1088/0963-6625/9/4/306\n\nHendriks, F., Kienhues, D., & Bromme, R. (2016). Trust in science and the science of trust. Trust and Communication in a Digitized World, 143–159.\n\nHuber, B., Barnidge, M., Gil de Zúñiga, H., & Liu, J. (2019). Fostering public trust in science: The role of social media. Public Understanding of Science, 28(7), 759–777. https://doi.org/10.1177/0963662519869097\n\nKerr, J. R., & Wilson, M. S. (2021). Right-wing authoritarianism and social dominance orientation predict rejection of science and scientists. Group Processes & Intergroup Relations, 24(4), 550–567. https://doi.org/10.1177/1368430221992126\n\nLewandowsky, S., & Oberauer, K. (2021). Worldview-motivated rejection of science and the norms of science. Cognition, 215, 104820. https://doi.org/10.1016/j.cognition.2021.104820\n\nLiu, H., & Priest, S. (2009). Understanding public support for stem cell research: Media communication, interpersonal communication and trust in key actors. Public Understanding of Science, 18(6), 704–718. https://doi.org/10.1177/0963662508097625\n\nNisbet, M. C., Scheufele, D. A., Shanahan, J., Moy, P., Brossard, D., & Lewenstein, B. V. (2002). Knowledge, Reservations, or Promise?: A Media Effects Model for Public Perceptions of Science and Technology. Communication Research, 29(5), 584–608. https://doi.org/10.1177/009365002236196\n\nSchneider, J., Merk, S., & Rosman, T. (2019). (Re)Building Trust? Investigating the effects of open science badges on perceived trustworthiness in journal articles. https://doi.org/10.17605/OSF.IO/VGBRS\n\nWingen, T., Berkessel, J. B., & Englich, B. (2020). No Replication, No Trust? How Low Replicability Influences Trust in Psychology. Social Psychological and Personality Science, 11(4). https://doi.org/10.1177/1948550619877412",
                "drafted_by": [
                    "Tobias Wingen; Flávio Azevedo"
                ],
                "reviewed_by": [
                    "Elias Garcia-Pelegrin",
                    "Helena Hartmann",
                    "Catia M. Oliveira",
                    "Olmo van den Akker"
                ],
                "alt_related_terms": [
                    null
                ],
                "language": "german"
            },
            {
                "type": "glossary",
                "title": "Publizieren oder untergehen (Publish or Perish)",
                "definition": "Ein Aphorismus, der den Druck beschreibt, unter dem Forschende stehen, akademische Manuskripte zu veröffentlichen, oft in hoch angesehenen akademischen Zeitschriften, um eine erfolgreiche akademische Karriere zu haben. Dieser Druck, eine große Anzahl von Manuskripten zu veröffentlichen, kann auf Kosten der Qualität der Manuskripte gehen. Dieser institutionelle Druck wird durch Einstellungs- und Berufungsverfahren und Finanzierungsentscheidungen verschärft, die sich stark an der Anzahl und dem Einfluss der Veröffentlichungen orientieren.",
                "related_terms": [
                    "Incentive structure",
                    "Journal Impact Factor",
                    "Reproducibility crisis (aka Replicability or replication crisis)",
                    "Salami slicing",
                    "Slow Science"
                ],
                "references": "Case, C. M. (1928). Scholarship in Sociology. Sociology and Social Research, 12, 323–340. http://www.sudoc.fr/036493414\n\nFanelli, D. (2010). Do Pressures to Publish Increase Scientists’ Bias? An Empirical Support from US States Data. PLOS ONE, 5(4), e10271. https://doi.org/10.1371/journal.pone.0010271",
                "drafted_by": [
                    "Eliza Woodward"
                ],
                "reviewed_by": [
                    "Nick Ballou",
                    "Mahmoud Elsherif",
                    "Helena Hartmann",
                    "Annalise A. LaPlume",
                    "Sam Parsons",
                    "Timo Roettger",
                    "Olmo van den Akker"
                ],
                "alt_related_terms": [
                    null
                ],
                "language": "german"
            },
            {
                "type": "glossary",
                "title": "PubPeer",
                "definition": "Eine Website, die es Nutzenden ermöglicht, anonyme Peer-Reviews zu publizierten Forschungsergebnissen zu veröffentlichen (d.h. Post-Publication Peer Review).",
                "related_terms": [
                    "Open Peer Review"
                ],
                "references": "PubPeer. (n.d.). PubPeer—Search publications and join the conversation. Pubpeer. https://www.pubpeer.com/",
                "drafted_by": [
                    "Ali H. Al-Hoorie"
                ],
                "reviewed_by": [
                    "Mahmoud ELsherif"
                ],
                "alt_related_terms": [
                    null
                ],
                "language": "german"
            },
            {
                "type": "glossary",
                "title": "Python",
                "definition": "Eine interpretierte Allzweckprogrammiersprache, die benutzerfreundlich und leicht lesbar sein soll und ursprünglich 1991 von Guido van Rossum entwickelt wurde. Python verfügt über eine umfangreiche Bibliothek mit zusätzlichen Funktionen und einer leicht zugänglichen Dokumentation für Aufgaben, die von der Datenanalyse bis zur Erstellung von Experimenten reichen. Sie ist eine beliebte Programmiersprache in den Bereichen Datenwissenschaft, maschinelles Lernen und Webentwicklung. Ähnlich wie R Markdown kann Python in einem interaktiven Online-Format, dem Jupyter-Notebook, präsentiert werden, das Code, Daten und Text kombiniert.",
                "related_terms": [
                    "Jupyter",
                    "Matplotlib",
                    "NumPy",
                    "OpenSesame",
                    "PsychoPy",
                    "R"
                ],
                "references": "Lutz, M. (2001). Programming Python. O’Reilly Media, Inc.",
                "drafted_by": [
                    "Shannon Francis"
                ],
                "reviewed_by": [
                    "James E. Bartlett",
                    "Alexander Hart",
                    "Helena Hartmann",
                    "Dominik Kiersz",
                    "Graham Reid",
                    "Andrew J. Stewart"
                ],
                "alt_related_terms": [
                    null
                ],
                "language": "german"
            },
            {
                "type": "glossary",
                "title": "Qualitative Forschung (Qualitative research)",
                "definition": "Forschung, die nicht-numerische Daten verwendet, wie z. B. Textantworten, Bilder, Videos oder andere Objekte, um Konzepte, Theorien oder Erfahrungen genauer zu untersuchen. Es gibt ein breites Spektrum an qualitativen Ansätzen, von der mikro-detaillierten Erforschung von Sprache oder der Konzentration auf persönliche subjektive Erfahrungen bis hin zu solchen, die soziale Erfahrungen und Meinungen auf der Makroebene untersuchen.",
                "related_terms": [
                    "Bracketing Interviews",
                    "Positionality",
                    "Quantitative research",
                    "Reflexivity **Alternative definition:** (if applicable) In Psychology, the **epistemology** of qualitative research is typically concerned with understanding people’s perspectives. Such epistemology proposes assuming the equity of researchers and participants as human beings, and in consequence, the need of sympathetic human understanding instead of data-driven conclusions"
                ],
                "references": "Aspers, P., & Corte, U. (2019). What is qualitative in qualitative research. Qualitative Sociology, 42(2), 139–160. https://doi.org/10.1007/s11133-019-9413-7\n\nLevitt, H. M., Motulsky, S. L., Wertz, F. J., Morrow, S. L., & Ponterotto, J. G. (2017). Recommendations for designing and reviewing qualitative research in psychology: Promoting methodological integrity. Qualitative Psychology, 4(1), 2. https://doi.org/10.1037/qup0000082",
                "drafted_by": [
                    "Madeleine Pownall"
                ],
                "reviewed_by": [
                    "Mahmoud Elsherif",
                    "Helena Hartmann",
                    "Oscar Lecuona",
                    "Claire Melia",
                    "Flávio Azevedo"
                ],
                "alt_related_terms": [
                    null
                ],
                "language": "german"
            },
            {
                "type": "glossary",
                "title": "Quantitative Forschung  translated, reviewed (Quantitative research)",
                "definition": "Quantitative Forschung umfasst ein breites Spektrum von Methoden zur systematischen Untersuchung einer Reihe von Phänomenen mit Hilfe von numerischen Daten, die mit Hilfe von Statistik analysiert werden können.",
                "related_terms": [
                    "Measuring",
                    "Qualitative research",
                    "Sample size",
                    "Statistical power",
                    "Statistics"
                ],
                "references": "Goertzen, M. J. (2017). Introduction to Quantitative Research and Data. Library Technology Reports, 53(4), 12–18.",
                "drafted_by": [
                    "Aoife O’Mahony"
                ],
                "reviewed_by": [
                    "Valeria Agostini",
                    "Tamara Kalandadze",
                    "Adam Parker"
                ],
                "alt_related_terms": [
                    null
                ],
                "language": "german"
            },
            {
                "type": "glossary",
                "title": "QRPs Fragwürdige Forschungs- oder Veröffentlichungspraktiken (Questionable Research Practices or Questionable Reporting Practices (QRPs))",
                "definition": "Eine Reihe von Aktivitäten, die absichtlich oder unabsichtlich Daten zugunsten der eigenen Hypothesen eines Forschenden verzerren \\- oder Auslassungen bei der Berichterstattung über solche Praktiken \\- einschließlich selektiver Einbeziehung von Daten, Hypothesenbildung nach Bekanntwerden der Ergebnisse (HARKing) und p-hacking. Verbreitet von John et al. (2012).",
                "related_terms": [
                    "Creative use of outliers",
                    "Fabrication",
                    "File-drawer",
                    "Garden of forking paths",
                    "HARKing",
                    "Nonpublication of data",
                    "*P*\\-hacking",
                    "*P*\\-value fishing",
                    "Partial publication of data",
                    "Post-hoc storytelling",
                    "Preregistration",
                    "Questionable Measurement Practices (QMP)",
                    "Researcher degrees of freedom",
                    "Reverse *p*\\-hacking",
                    "Salami slicing"
                ],
                "references": "Banks, G. C., Rogelberg, S. G., Woznyj, H. M., Landis, R. S., & Rupp, D. E. (2016). Editorial: Evidence on questionable research practices: The good, the bad, and the ugly. Journal of Business and Psychology, 31(3), 323–338. https://doi.org/10.1007/s10869-016-9456-7\n\nFiedler, K., & Schwarz, N. (2016). Questionable research practices revisited. Social Psychological and Personality Science, 7(1), 45–52. https://doi.org/10.1177/1948550615612150\n\nHardwicke, T. E., Jameel, L., Jones, M., Walczak, E. J., & Weinberg, L. M. (2014). Only human: Scientists, systems, and suspect statistics. Opticon1826, 16, 25. https://doi.org/10.5334/OPT.CH\n\nJohn, L. K., Loewenstein, G., & Prelec, D. (2012). Measuring the prevalence of questionable research practices with incentives for truth telling. Psychological Science, 23(5), 524–532. https://doi.org/10.1177/0956797611430953\n\nNeuroskeptic. (2012). The nine circles of scientific hell. Perspectives on Psychological Science, 7(6), 643–644. https://doi.org/10.1177/1745691612459519\n\nSijtsma, K. (2016). Playing with data—Or how to discourage questionable research practices and stimulate researchers to do things right. Psychometrika, 81(1), 1–15. https://doi.org/10.1007/s11336-015-9446-0",
                "drafted_by": [
                    "Mahmoud Elsherif"
                ],
                "reviewed_by": [
                    "Tamara Kalandadze",
                    "William Ngiam",
                    "Sam Parsons",
                    "Mariella Paul",
                    "Eike Mark Rinke",
                    "Timo Roettger",
                    "Flávio Azevedo"
                ],
                "alt_related_terms": [
                    null
                ],
                "language": "german"
            },
            {
                "type": "glossary",
                "title": "QMP; Fragwürdige Forschungspraktiken (Questionable Measurement Practices (QMP))",
                "definition": "Entscheidungen von Forschenden, die Zweifel an der Gültigkeit der in einer Studie verwendeten Messgrößen und letztlich an den endgültigen Schlussfolgerungen der Studie aufkommen lassen (Flake & Fried, 2020). Probleme ergeben sich aus mangelnder Transparenz bei der Berichterstattung über Messpraktiken, einem Versäumnis, die Konstruktvalidität zu berücksichtigen, Nachlässigkeit, Unwissenheit oder einer absichtlichen Falschdarstellung von Informationen.",
                "related_terms": [
                    "Construct validity",
                    "Measurement schmeasurement",
                    "*P*\\-hacking",
                    "Psychometrics",
                    "Questionable Research Practices or Questionable Reporting Practices (QRPs)",
                    "Validity"
                ],
                "references": "Flake, J. K., & Fried, E. I. (2020). Measurement schmeasurement: Questionable measurement practices and how to avoid them. Advances in Methods and Practices in Psychological Science, 3(4), 456–465. https://doi.org/10.1177/2515245920952393",
                "drafted_by": [
                    "Halil Emre Kocalar"
                ],
                "reviewed_by": [
                    "Jamie P. Cockcroft",
                    "Annalise A. LaPlume",
                    "Sam Parsons",
                    "Mirela Zaneva",
                    "Flávio Azevedo"
                ],
                "alt_related_terms": [
                    null
                ],
                "language": "german"
            },
            {
                "type": "glossary",
                "title": "R",
                "definition": "R ist eine freie, quelloffene Programmiersprache und Softwareumgebung, die zur Durchführung statistischer Analysen und zur Darstellung von Daten verwendet werden kann. R wurde von Ross Ihaka und Robert Gentleman an der Universität von Auckland entwickelt. R ermöglicht es Forschenden , reproduzierbare Analyseskripte weiterzugeben, was die Transparenz einer Studie erhöht. Häufig wird R in Verbindung mit einer integrierten Entwicklungsumgebung (integrated development environment, IDE) verwendet, die die Arbeit mit der Sprache vereinfacht, z. B. RStudio oder Visual Studio Code oder Tinn-R.",
                "related_terms": [
                    "Open-source",
                    "Statistical analysis"
                ],
                "references": "R Project for Statistical Computing. (n.d.). R: The R Project for Statistical Computing. R Project. https://www.r-project.org/",
                "drafted_by": [
                    "Lisa Spitzer"
                ],
                "reviewed_by": [
                    "Bradley Baker",
                    "Alexander Hart",
                    "Joanne McCuaig",
                    "Andrew J. Stewart"
                ],
                "alt_related_terms": [
                    null
                ],
                "language": "german"
            },
            {
                "type": "glossary",
                "title": "Rote Teams (Red Teams)",
                "definition": "Ein Ansatz, der die externe Kritik von Kolleg:innen und Peers in den Forschungsprozess integriert. Red Teams (Rote Teams) beruhen auf der Idee, dass Forschung, die kritischer und umfassender bewertet wird, zuverlässiger ist. Der Begriff geht auf eine militärische Praxis zurück: Eine Gruppe (das rote Team) greift etwas an, und eine andere Gruppe (das blaue Team) verteidigt es. Diese Praxis wurde auf Open Science angewandt, indem ein rotes Team (benannte kritische Einzelpersonen) finanzielle Anreize erhält, um Fehler in den Materialien oder Inhalten eines Forschungsprojekts (in den Materialien, im Code, in der Schrift usw.) zu finden oder Verbesserungen daran vorzuschlagen (Coles et al., 2020).",
                "related_terms": [
                    "Adversarial collaboration"
                ],
                "references": "Coles, N. A., Tiokhin, L., Arslan, R., Forscher, P., Scheel, A., & Lakens, D. (2020). Red Team Challenge. http://daniellakens.blogspot.com/2020/05/red-team-challenge.html\n\nLakens, D. (2020). The 20% Statistician: Red Team Challenge. The 20% Statistician. http://daniellakens.blogspot.com/2020/05/red-team-challenge.html",
                "drafted_by": [
                    "Annalise A. LaPlume"
                ],
                "reviewed_by": [
                    "Nick Ballou",
                    "Mahmoud Elsherif**;** Thomas Rhys Evans",
                    "Helena Hartmann",
                    "Timo Roettger"
                ],
                "alt_related_terms": [
                    null
                ],
                "language": "german"
            },
            {
                "type": "glossary",
                "title": "Reflexivität (Reflexivity)",
                "definition": "Der Prozess der Reflexivität bezieht sich auf die kritische Betrachtung des Wissens, das wir durch Forschung produzieren, wie es produziert wird und unserer eigenen Rolle als Forschende bei der Produktion dieses Wissens. Es gibt verschiedene Formen der Reflexivität: die persönliche Reflexivität, bei der Forschende die Auswirkungen ihrer eigenen persönlichen Erfahrungen berücksichtigen, und die funktionale Reflexivität, bei der Forschende die Art und Weise betrachten, in der unsere Forschungsinstrumente und \\-methoden die Wissensproduktion beeinflusst haben könnten. Reflexivität zielt darauf ab, die Aufmerksamkeit auf die zugrundeliegenden Faktoren zu lenken, die den Forschungsprozess beeinflussen können, einschließlich der Entwicklung von Forschungsfragen, der Datenerhebung und der Analyse.",
                "related_terms": [
                    "Bracketing Interviews",
                    "Qualitative Research"
                ],
                "references": "Braun, V., & Clarke, V. (2013). Successful Qualitative Research. SAGE Publications.\n\nFinlay, L., & Gough, B. (2008). Reflexivity: A practical guide for researchers in health and social sciences. John Wiley & Sons.",
                "drafted_by": [
                    "Claire Melia"
                ],
                "reviewed_by": [
                    "Gilad Feldman",
                    "Annalise A. LaPlume"
                ],
                "alt_related_terms": [
                    null
                ],
                "language": "german"
            },
            {
                "type": "glossary",
                "title": "Registered Report",
                "definition": "Ein wissenschaftliches Publikationsformat, das eine erste Runde der Begutachtung (Peer Review) des Hintergrunds und der Methoden (Studiendesign, Messung und Analyseplan) umfasst; Manuskripte von hinreichend hoher Qualität werden in dieser Phase zur In-Principle Acceptance (IPA) angenommen. In der Regel findet diese Phase 1 vor der Datenerhebung statt, jedoch sind Sekundärdatenanalysen in diesem Veröffentlichungsformat möglich. Im Anschluss an die Datenanalyse und die Verschriftlichung der Ergebnis- und Diskussionsabschnitte wird in Phase 2 bewertet, ob die Autor:innen ihren Studienplan hinreichend befolgt und über Abweichungen davon berichtet haben (diese Begutachtung erfolgt unabhängig von den Ergebnissen selbst). Damit verschiebt sich der Schwerpunkt der Überprüfung auf die vorgeschlagene Forschungsfrage und die Methodik der Studie und weg von dem wahrgenommenen Interesse an den Ergebnissen der Studie.",
                "related_terms": [
                    "Preregistration",
                    "Publication bias (File Drawer Problem)",
                    "Results-free review",
                    "PCI (Peer Community In)",
                    "Research Protocol"
                ],
                "references": "Chambers, C. D. (2013). Registered reports: a new publishing initiative at Cortex. Cortex, 49(3), 609–610. https://doi.org/10.1016/j.cortex.2012.12.016\n\nChambers, C. D., Dienes, Z., McIntosh, R. D., Rotshtein, P., & Willmes, K. (2015). Registered reports: realigning incentives in scientific publishing. Cortex, 66, A1–A2. https://doi.org/10.1016/j.cortex.2015.03.022\n\nChambers, C. D., & Tzavella, L. (2020). Registered Reports: Past, Present and Future. https://doi.org/10.31222/osf.io/43298\n\nFindley, M. G., Jensen, N. M., Malesky, E. J., & Pepinsky, T. B. (2016). Can results-free review reduce publication bias? The results and implications of a pilot study. Comparative Political Studies, 49(13), 1667–1703. https://doi.org/10.1177/0010414016655539",
                "drafted_by": [
                    "Madeleine Pownall"
                ],
                "reviewed_by": [
                    "Gilad Feldman",
                    "Emma Henderson",
                    "Aoife O’Mahony",
                    "Sam Parsons",
                    "Mariella Paul",
                    "Charlotte R. Pennington",
                    "Eike Mark Rinke",
                    "Timo Roettger",
                    "Olmo van den Akker",
                    "Yuki Yamada",
                    "Flávio Azevedo"
                ],
                "alt_related_terms": [
                    null
                ],
                "language": "german"
            },
            {
                "type": "glossary",
                "title": "Registry of Research Data Repositories",
                "definition": "Ein globales Register von Forschungsdaten-Repositorien aus verschiedenen akademischen Disziplinen. Es umfasst Repositorien, die Forschenden, Fördereinrichtungen, Verlagen und wissenschaftlichen Einrichtungen die dauerhafte Speicherung von Datensätzen, deren Beschreibung durch Metadaten und den Zugriff darauf ermöglichen.",
                "related_terms": [
                    "Metadata",
                    "Open Access",
                    "Open Data",
                    "Open Material",
                    "Repository"
                ],
                "references": "Anon. (n.d.). Home | re3data.org. Retrieved from https://www.re3data.org/",
                "drafted_by": [
                    "Aleksandra Lazić"
                ],
                "reviewed_by": [
                    "Mahmoud Elsherif",
                    "Sam Parsons",
                    "Charlotte R. Pennington",
                    "Helena Hartmann"
                ],
                "alt_related_terms": [
                    null
                ],
                "language": "german"
            },
            {
                "type": "glossary",
                "title": "Reliabilität (Reliability)",
                "definition": "Das Ausmaß, in dem wiederholte Messungen zu den gleichen Ergebnissen führen. In der Psychometrik bezieht sich die Reliabilität auf das Ausmaß, in dem die Befragten ähnliche Ergebnisse erzielen, wenn sie einen Fragebogen mehrmals ausfüllen. Es sei darauf hingewiesen, dass Reliabilität nicht Validität impliziert. Darüber hinaus gibt es neben der Test-Retest-Reliabilität noch weitere Arten der Reliabilität, darunter die interne Konsistenz, die Paralleltest-Reliabilität und die Interrater-Reliabilität.",
                "related_terms": [
                    "Consistency",
                    "Internal consistency",
                    "Quality Criteria",
                    "Replicability",
                    "Reproducibility",
                    "Validity"
                ],
                "references": "Bollen, K. A. (1989). Structural Equations with Latent Variables (pp. 179–225). John Wiley & Sons.\n\nDrost, E. A. (2011). Validity and reliability in social science research. Education Research and Perspectives, 38(1), 105–123.",
                "drafted_by": [
                    "Annalise A. LaPlume"
                ],
                "reviewed_by": [
                    "Mahmoud Elsherif**;** Eduardo Garcia-Garzon",
                    "Kai Krautter",
                    "Olmo van den Akker"
                ],
                "alt_related_terms": [
                    null
                ],
                "language": "german"
            },
            {
                "type": "glossary",
                "title": "Wiederholbarkeit (Repeatability)",
                "definition": "Synonym für die Test-Retest-Reliabilität. Sie bezieht sich auf die Übereinstimmung zwischen den Ergebnissen aufeinanderfolgender Messungen derselben Maße. Die Wiederholbarkeit erfordert die gleichen Versuchswerkzeuge, die gleichen Beobachtenden, das gleiche Messinstrument unter den gleichen Bedingungen, den gleichen Ort, Wiederholungen über einen kurzen Zeitraum und die gleichen Ziele (Joint Committee for Guidelines in Metrology, 2008\\)",
                "related_terms": [
                    "Reliability"
                ],
                "references": "ISO. (1993). Guide to the Expression of Uncertainty in Measurement (1st ed.). International Organization for Standardization.\n\nStodden, V. C. (2011). Trust your science? Open your data and code.",
                "drafted_by": [
                    "Mahmoud Elsherif, Adam Parker"
                ],
                "reviewed_by": [
                    "Gilad Feldman",
                    "Helena Hartmann",
                    "Joanne McCuaig",
                    "Sam Parsons"
                ],
                "alt_related_terms": [
                    null
                ],
                "language": "german"
            },
            {
                "type": "glossary",
                "title": "Replikabilität (Replicability)",
                "definition": "Ein Oberbegriff, der in verschiedenen Bereichen unterschiedlich verwendet wird und folgende Konzepte umfasst: direkte und konzeptionelle Replikation, komputationale Reproduzierbarkeit/Replizierbarkeit, Generalisierungsanalyse und Robustheitsanalysen. Einige der zuvor verwendeten Definitionen umfassen: ein anderes Team, das unter Verwendung der Materialien der ursprünglichen Autor:in zu denselben Ergebnissen gelangt (Barba 2018); eine Studie, die nach der Erhebung neuer Daten zu denselben Schlussfolgerungen gelangt (Claerbout und Karrenbach, 1992); sowie Studien, bei denen jedes Ergebnis als diagnostischer Beweis für eine Behauptung aus einer früheren Forschung angesehen wird (Nosek & Errington, 2020).",
                "related_terms": [
                    "Conceptual replication",
                    "Direct Replication",
                    "Generalizability",
                    "Reproducibility",
                    "Reliability",
                    "Robustness (analyses)"
                ],
                "references": "Barba, L. A. (2018). Terminologies for reproducible research. arXiv Preprint arXiv:1802.03311.\n\nCrüwell, S., van Doorn, J., Etz, A., Makel, M. C., Moshontz, H., Niebaum, J. C., Orben, A., Parsons, S., & Schulte-Mecklenbeck, M. (2019). Seven Easy Steps to Open Science: An Annotated Reading List. Zeitschrift Für Psychologie, 227(4), 237–248. https://doi.org/10.1027/2151-2604/a000387\n\nKing, G. (1995). Replication, replication. PS: Political Science & Politics, 28(3), 444–452. https://doi.org/10.2307/420301\n\nNosek, B. A., & Errington, T. M. (2020). What is replication? PLOS Biology, 18(3), e3000691. https://doi.org/10.1371/journal.pbio.3000691",
                "drafted_by": [
                    "Mahmoud Elsherif"
                ],
                "reviewed_by": [
                    "Jamie P. Cockcroft",
                    "Adrien Fillon",
                    "Gilad Feldman",
                    "Annalise A. LaPlume",
                    "Tina B. Lonsdorf",
                    "Sam Parsons",
                    "Eike Mark Rinke",
                    "Tobias Wingen"
                ],
                "alt_related_terms": [
                    null
                ],
                "language": "german"
            },
            {
                "type": "glossary",
                "title": "Replikationsmärkte (Replication Markets)",
                "definition": "Ein Replikationsmarkt ist eine Umgebung, in der Nutzer:innen auf die Replizierbarkeit bestimmter Effekte wetten. Für die Vorhersagenden besteht ein Anreiz, genaue Vorhersagen zu treffen, und die erfolgreichsten Vorhersagenden erhalten für ihre Wetten eine finanzielle Entlohnung die Erwähnung als Beitragende (Contributorship). Der Grundgedanke hinter einem Replikationsmarkt ist, dass er die kollektive Weisheit der wissenschaftlichen Gemeinschaft nutzt, um vorherzusagen, welcher Effekt sich am wahrscheinlichsten replizieren lässt, und so die Forschenden ermutigt, ihre begrenzten Ressourcen für die Replikation dieser Effekte einzusetzen.",
                "related_terms": [
                    "Citizen science",
                    "Crowdsourcing",
                    "Replicability",
                    "Reproducibility"
                ],
                "references": "Liu, Y., Gordon, M., Wang, J., Bishop, M., Chen, Y., Pfeiffer, T., Twardy, C., & Viganola, D. (2020). Replication Markets: Results, Lessons, Challenges and Opportunities in AI Replication. ArXiv:2005.04543 . http://arxiv.org/abs/2005.04543\n\nTierney, W., Hardy III, J. H., Ebersole, C. R., Leavitt, K., Viganola, D., Clemente, E. G., & others. (2020). Creative destruction in science. Organizational Behavior and Human Decision Processes, 161, 291–309. https://doi.org/10.1016/j.obhdp.2020.07.002\n\nTierney, W., Hardy III, J., Ebersole, C. R., Viganola, D., Clemente, E. G., Gordon, M., & others. (2021). A creative destruction approach to replication: Implicit work and sex morality across cultures. Journal of Experimental Social Psychology, 93, 104060. https://doi.org/10.1016/j.jesp.2020.104060\n\nReplication Markets. (n.d.). Replication Markets – Reliable research replicates…you can bet on it. Replication Markets. https://www.replicationmarkets.com/",
                "drafted_by": [
                    "Ali H. Al-Hoorie"
                ],
                "reviewed_by": [
                    "Mahmoud Elsherif",
                    "Leticia Micheli",
                    "Sam Parsons"
                ],
                "alt_related_terms": [
                    null
                ],
                "language": "german"
            },
            {
                "type": "glossary",
                "title": "Berichtleitlinien (Reporting Guideline)",
                "definition": "Ein Leitfaden für die Berichterstattung ist eine “checklist, flow diagram, or structured text to guide authors in reporting a specific type of research, developed using explicit methodology.” (dt. Checkliste, ein Flussdiagramm oder ein strukturierter Text, der die Autor:innen bei der Verschriftlichung einer bestimmten Art von Forschung anleitet und unter Verwendung einer expliziten Methodik entwickelt wurde, EQUATOR-Netzwerk, n.d.). Reporting Guidelines bieten ein Mindestmaß an Anleitung, um sicherzustellen, dass Forschungsergebnisse angemessen interpretiert, bewertet, zusammengefasst und reproduziert werden können. Ihre Anwendung ist oft in verschiedenen Journals oder Verlagen unterschiedlich..",
                "related_terms": [
                    "CONSORT",
                    "Non-Intervention, Reproducible, and Open Systematic Reviews (NIRO-SR)",
                    "PRISMA",
                    "STROBE"
                ],
                "references": "Moher, D., Liberati, A., Tetzlaff, J., & Altman, D. (2009). Preferred Reporting Items for Systematic Reviews and Meta-Analyses: The PRISMA Statement. PLoS Medicine, 6(7), e1000097. https://doi.org/10.1371/journal.pmed.1000097\n\nSchulz, K. F., Altman, D. G., & Moher, D. (2010). CONSORT 2010 statement: updated guidelines for reporting parallel group randomised trials. Trials, 11(1), 32. https://doi.org/10.1186/1745-6215-11-32\n\nNetwork, T. E. (n.d.). What is a reporting guideline? Retrieved 10 July 2021. https://www.equator-network.org/about-us/what-is-a-reporting-guideline/",
                "drafted_by": [
                    "Aidan Cashin"
                ],
                "reviewed_by": [
                    "Gilad Feldman",
                    "Helena Hartmann",
                    "Joanne McCuaig"
                ],
                "alt_related_terms": [
                    null
                ],
                "language": "german"
            },
            {
                "type": "glossary",
                "title": "Repositorium (Repository)",
                "definition": "Ein Online-Archiv für die Speicherung digitaler Objekte wie Forschungsergebnisse, Manuskripte, Analysecodes und/oder Daten. Beispiele sind Preprint-Server wie bioRxiv, MetaArXiv, PsyArXiv, institutionelle Forschungsrepositorien sowie Datenrepositorien, die Datensätze sammeln und speichern, z. B. zenodo.org, PsychData und Code-Repositorien wie Github, oder allgemeinere Repositorien für alle Arten von Forschungsdaten wie das Open Science Framework (OSF). Digitale Objekte, die in Repositorien gespeichert werden, werden in der Regel durch Metadaten beschrieben, die das Auffinden in verschiedenen Speicherorten erlauben.",
                "related_terms": [
                    "Data sharing",
                    "Github",
                    "Metadata",
                    "Open Access",
                    "Open data",
                    "Open Material",
                    "Open Science Framework",
                    "Open Source",
                    "Preprint"
                ],
                "references": "",
                "drafted_by": [
                    "Tina Lonsdorf"
                ],
                "reviewed_by": [
                    "Gilad Feldman",
                    "Connor Keating",
                    "Mariella Paul",
                    "Charlotte R. Pennington",
                    "Flávio Azevedo"
                ],
                "alt_related_terms": [
                    null
                ],
                "language": "german"
            },
            {
                "type": "glossary",
                "title": "ReproducibiliTea",
                "definition": "Eine basisdemokratische Initiative, die Forschenden hilft, an ihren Universitäten lokale Journal Clubs zu gründen, um verschiedene Themen im Zusammenhang mit offener Forschung und Scholarship zu diskutieren. Im Mittelpunkt eines jeden Treffens steht in der Regel eine bestimmte Veröffentlichung, in der z. B. Reproduzierbarkeit, Forschungspraxis, Forschungsqualität, soziale Gerechtigkeit und Inklusion sowie Ideen zur Verbesserung der Wissenschaft erörtert werden.",
                "related_terms": [
                    "Grassroots initiative",
                    "Journal club",
                    "Open science",
                    "Reproducibility"
                ],
                "references": "Orben, A. (2019). A journal club to fix science. Nature, 573(7775), 465–466. https://doi.org/10.1038/d41586-019-02842-8",
                "drafted_by": [
                    "Emma Norris"
                ],
                "reviewed_by": [
                    "Mahmoud Elsherif",
                    "Gilad Feldman",
                    "Connor Keating",
                    "Charlotte R. Pennington",
                    "Sam Parsons",
                    "Flávio Azevedo"
                ],
                "alt_related_terms": [
                    null
                ],
                "language": "german"
            },
            {
                "type": "glossary",
                "title": "Reproduzierbarkeit (Reproducibility)",
                "definition": "Ein Mindeststandard auf einem Spektrum von Aktivitäten (\"Reproduzierbarkeitsspektrum\") zur Bewertung des Wertes oder der Genauigkeit wissenschaftlicher Aussagen auf der Grundlage der ursprünglichen Methoden, Daten und Codes. Zum Beispiel, wenn die Daten und Computercodes der ursprünglichen Forschenden verwendet werden, um die Ergebnisse wieder zu erzeugen (Barba, 2018), was oft als komputationale Reproduzierbarkeit bezeichnet wird. Die Reproduzierbarkeit garantiert nicht die Qualität, Korrektheit oder Gültigkeit der veröffentlichten Ergebnisse (Peng, 2011). In einigen Bereichen wird diese Bedeutung stattdessen mit dem Begriff \"Replizierbarkeit\" (Replicability) oder \"Wiederholbarkeit\" (Repeatability) in Verbindung gebracht.",
                "related_terms": [
                    "Computational reproducibility",
                    "Replicability",
                    "repeatability"
                ],
                "references": "Barba, L. A. (2018). Terminologies for reproducible research. arXiv Preprint arXiv:1802.03311.\n\nCrüwell, S., van Doorn, J., Etz, A., Makel, M. C., Moshontz, H., Niebaum, J. C., Orben, A., Parsons, S., & Schulte-Mecklenbeck, M. (2019). Seven Easy Steps to Open Science: An Annotated Reading List. Zeitschrift Für Psychologie, 227(4), 237–248. https://doi.org/10.1027/2151-2604/a000387\n\nPeng, R. D. (2011). Reproducible Research in Computational Science. Science, 334(6060), 1226–1227. https://doi.org/10.1126/science.1213847\n\nStodden, V. C. (2011). Trust your science? Open your data and code.\n\nSyed, M. (2019). The Open Science Movement is for all of us. PsyArXiv.",
                "drafted_by": [
                    "Mahmoud Elsherif"
                ],
                "reviewed_by": [
                    "Helena Hartmann",
                    "Annalise A. LaPlume",
                    "Tina B. Lonsdorf",
                    "Sam Parsons",
                    "Charlotte R. Pennington",
                    "Suzanne L. K. Stewart"
                ],
                "alt_related_terms": [
                    null
                ],
                "language": "german"
            },
            {
                "type": "glossary",
                "title": "aka Replicability or replication crisis Reproduzierbarkeitskrise, auch Replizierbarkeitskrise oder Replikationskrise (Reproducibility crisis (aka Replicability or replication crisis))",
                "definition": "Die Feststellung und der damit verbundene Wandel in der akademischen Kultur und Denkweise, dass ein großer Teil der in verschiedenen Disziplinen veröffentlichten wissenschaftlichen Studien nicht replizierbar ist (z. B. Open Science Collaboration, 2015). Man geht davon aus, dass dies auf einen Mangel an Qualität und Integrität der Forschungs- und Veröffentlichungspraktiken zurückzuführen ist, z. B. Publikationsverzerrungen, QRPs und mangelnde Transparenz, was zu einer überhöhten Rate falsch-positiver Ergebnisse führt. Andere haben diesen Prozess als eine \"Glaubwürdigkeitsrevolution\" zur Verbesserung dieser Praktiken beschrieben.",
                "related_terms": [
                    "Credibility crisis",
                    "Publication bias (File Drawer Problem)",
                    "Questionable Research Practices or Questionable Reporting Practices (QRPs)",
                    "Replicability",
                    "Reproducibility"
                ],
                "references": "Fanelli, D. (2018). Opinion: Is science really facing a reproducibility crisis, and do we need it to? Proceedings of the National Academy of Sciences, 115(11), 2628–2631. https://doi.org/10.1073/pnas.1708272114",
                "drafted_by": [
                    "Mahmoud Elsherif"
                ],
                "reviewed_by": [
                    "Helena Hartmann",
                    "Annalise A. LaPlume",
                    "Mariella Paul",
                    "Sonia Rishi",
                    "Lisa Spitzer"
                ],
                "alt_related_terms": [
                    null
                ],
                "language": "german"
            },
            {
                "type": "glossary",
                "title": "Reproduzierbarkeitsnetzwerk (Reproducibility Network)",
                "definition": "Ein Reproduzierbarkeitsnetzwerk ist ein Konsortium offener Forschungsarbeitsgruppen, die häufig von Peers geleitet werden. Die Gruppen arbeiten in einem bestimmten Land nach dem wheel-and-spoke-Modell (dt. Rad-und-Speichen-Modell), bei dem das Netzwerk lokale disziplinübergreifende Forschende, Gruppen und Einrichtungen mit einer zentralen Lenkungsgruppe verbindet, die auch mit externen Akteuren im Forschungsökosystem in Verbindung steht. Zu den Zielen von Reproduzierbarkeitsnetzwerken gehören die Sensibilisierung für das Thema, die Förderung von Trainingsmaßnahmen und die Verbreitung bewährter Praktiken an der Basis, in den Institutionen und im Forschungsumfeld. Solche Netzwerke gibt es im Vereinigten Königreich, in Deutschland, in der Schweiz, in der Slowakei und in Australien (Stand: März 2021).",
                "related_terms": [],
                "references": "Network, U. R. (n.d.). UK Reproducibility Network. Retrieved 10 July 2021. https://www.ukrn.org/\n\nGRN · German Reproducibility Network. (n.d.). A German Reproducibility Network. Retrieved from https://reproducibilitynetwork.de/\n\nAnon. (n.d.). Domov | SKRN (Slovak Reproducibility network). Retrieved from https://slovakrn.wixsite.com/skrn\n\nAusRN. (n.d.). Australian Reproducibility Network. Retrieved from https://www.aus-rn.org/",
                "drafted_by": [
                    "Suzanne L. K. Stewart"
                ],
                "reviewed_by": [
                    "Annalise A. LaPlume",
                    "Sam Parsons",
                    "Flávio Azevedo"
                ],
                "alt_related_terms": [
                    null
                ],
                "language": "german"
            },
            {
                "type": "glossary",
                "title": "*p* (Research Contribution Metric (*p*))",
                "definition": "Eine Art semantometrisches Maß zur Bewertung der Ähnlichkeit von Veröffentlichungen, die in einem Zitationsnetzwerk miteinander verbunden sind. Diese Methode verwendet eine einfache Formel, um die Beiträge der Autor:innen zu bewerten. Die Publikation *p* kann auf der Grundlage der semantischen Distanz zwischen den von p zitierten Publikationen und den Publikationen, die *p* zitieren, geschätzt werden.",
                "related_terms": [
                    "Semantometrics"
                ],
                "references": "Knoth, P., & Herrmannova, D. (2014). Towards semantometrics: A new semantic similarity based measure for assessing a research publication’s contribution. D-Lib Magazine, 20(11), 8. https://doi.org/10.1045/november2014-knoth\n\nHolcombe, A. O. (2019). Contributorship, not authorship: Use CRediT to indicate who did what. Publications, 7(3), 48. https://doi.org/10.3390/publications7030048\n\nLarivière, V., Desrochers, N., Macaluso, B., Mongeon, P., Paul-Hus, A., & Sugimoto, C. R. (2016). Contributorship and division of labor in knowledge production. Social Studies of Science, 46(3), 417–435. https://doi.org/10.1177/0306312716650046",
                "drafted_by": [
                    "Alaa AlDoh"
                ],
                "reviewed_by": [
                    "Michele C. Lim",
                    "Jamie P. Cockcroft",
                    "Micah Vandegrift",
                    "Dominik Kiersz    ####"
                ],
                "alt_related_terms": [
                    null
                ],
                "language": "german"
            },
            {
                "type": "glossary",
                "title": "Forschungszyklus (Research Cycle)",
                "definition": "Beschreibt den zirkulären Prozess der wissenschaftlichen Forschung, bei dem Forschende in verschiedenen Stadien der Untersuchung arbeiten, von eher vorsichtigen und explorativen Untersuchungen bis hin zur Prüfung definierterer und gut belegter Behauptungen (aus dem Engl. “researchers working at various stages of inquiry, from more tentative and exploratory investigations to the testing of more definitive and well-supported claims”, Lieberman, 2020, S. 42). Der Zyklus umfasst die Literaturrecherche und Hypothesenbildung, die Datenerhebung und \\-analyse sowie die Verbreitung der Ergebnisse (z. B. durch die Veröffentlichung in Fachzeitschriften mit Peer-Review), aus der wiederum die Theorie und neue Hypothesen/Forschungen hervorgehen.",
                "related_terms": [
                    "Research process"
                ],
                "references": "Bramoullé, Y., & Saint-Paul, G. (2010). Research cycles. In Journal of Economic Theory (Vol. 145, pp. 1890–1920). https://doi.org/10.2139/ssrn.965816\n\nLieberman, E. (2020). Research Cycles. In C. Elman, J. Gerring, & J. Mahoney (Eds.), The Production of Knowledge: Enhancing Progress in Social Science (pp. 42–70). Cambridge University Press. https://doi.org/10.1017/9781108762519.003",
                "drafted_by": [
                    "Helena Hartmann"
                ],
                "reviewed_by": [
                    "Jamie P. Cockcroft",
                    "Aleksandra Lazić",
                    "Graham Reid",
                    "Beatrice Valentini"
                ],
                "alt_related_terms": [
                    null
                ],
                "language": "german"
            },
            {
                "type": "glossary",
                "title": "Forschungsdatenmanagement (Research Data Management)",
                "definition": "Research Data Management (dt. Forschungsdatenmanagement, FDM) ist ein weit gefasstes Konzept, das Prozesse zur Erstellung organisierter, dokumentierter, zugänglicher und wiederverwendbarer hochwertiger Forschungsdaten umfasst. Ein angemessenes Forschungsdatenmanagement bietet viele Vorteile, unter anderem eine geringere Wahrscheinlichkeit von Datenverlusten, eine größere Sichtbarkeit und Zusammenarbeit durch die gemeinsame Nutzung von Daten sowie die Demonstration von Integrität und Rechenschaft in der Forschung.",
                "related_terms": [
                    "Data curation",
                    "Data documentation",
                    "Data management plan (DMP)",
                    "Data sharing",
                    "Metadata",
                    "Research data management"
                ],
                "references": "Corti, L., Van den Eynden, V., Bishop, L., & Woollard, M. (2019). Managing and sharing research data: a guide to good practice. Sage.",
                "drafted_by": [
                    "Micah Vandegrift"
                ],
                "reviewed_by": [
                    "Helena Hartmann",
                    "Tina B. Lonsdorf",
                    "Catia M. Oliveira",
                    "Julia Wolska"
                ],
                "alt_related_terms": [
                    null
                ],
                "language": "german"
            },
            {
                "type": "glossary",
                "title": "Forschungsintegrität (Research integrity)",
                "definition": "Die Integrität der Forschung wird durch eine Reihe von guten Forschungspraktiken definiert, die auf den folgenden Grundprinzipien beruhen: Ehrlichkeit, Zuverlässigkeit, Respekt und Verantwortlichkeit (ALLEA, 2017). Gute Forschungspraktiken \\- die auf den Grundprinzipien der Integrität in der Forschung beruhen und die Forschende bei ihrer Arbeit sowie bei der Auseinandersetzung mit den praktischen, ethischen und intellektuellen Herausforderungen der Forschung leiten sollten \\- beziehen sich auf Bereiche wie: Forschungsumfeld (z. B. Forschungseinrichtungen und \\-organisationen fördern das Bewusstsein und sorgen für eine vorherrschende Kultur der Integrität in der Forschung), Ausbildung, Supervision und Betreuung (z. B. Forschungseinrichtungen und \\-organisationen entwickeln eine geeignete und angemessene Ausbildung in den Bereichen Ethik und Integrität in der Forschung, um sicherzustellen, dass alle Beteiligten mit den einschlägigen Kodizes und Vorschriften vertraut gemacht werden), Forschungsverfahren (z. B., Forschende berichten über ihre Ergebnisse in einer Art und Weise, die mit den Standards des Fachgebiets vereinbar ist und gegebenenfalls überprüft und reproduziert werden kann), Schutzmaßnahmen (z. B. achten Forschende in angemessener Weise auf die Gesundheit, die Sicherheit und das Wohlergehen der Gemeinschaft, der Mitarbeitenden und anderer Personen, die mit ihrer Forschung in Verbindung stehen), Datenpraktiken und \\-management (z. B. bieten Forschende, Forschungseinrichtungen und Organisationen Transparenz darüber, wie ihre Daten und Forschungsmaterialien zugänglich sind oder genutzt werden können), Zusammenarbeit, Veröffentlichung und Verbreitung (z. B. betrachten Autor:innen und Verleger:innen Null-Befunde (auch “negative Ergebnisse”) als ebenso gültig wie positive Ergebnisse für die Veröffentlichung und Verbreitung), Überprüfung, Bewertung und Bearbeitung (z. B. überprüfen und bewerten Forschende Einreichungen für die Veröffentlichung, Finanzierung, Ernennung, Beförderung oder Belohnung auf transparente und vertretbare Weise).",
                "related_terms": [
                    "Credibility of scientific claims",
                    "Error detection",
                    "Ethics",
                    "Open research",
                    "Questionable Research Practices or Questionable Reporting Practices (QRPs)",
                    "Responsible Research Practices",
                    "Rigour",
                    "Transparency",
                    "Trustworthy research"
                ],
                "references": "Academies, A. A. E. (2017). The European Code of Conduct for Research Integrity. Revised Edition. Retrieved from https://allea.org/code-of-conduct/\n\nMedin, D. L. (2012). Rigor without rigor mortis: The APS Board discusses research integrity. APS Observer, 25(5–9), 27–28. https://www.psychologicalscience.org/observer/scientific-rigor\n\nMoher, D., Bouter, L., Kleinert, S., Glasziou, P., Sham, M. H., Barbour, V., & Dirnagl, U. (2020). The Hong Kong Principles for assessing researchers: Fostering research integrity. PLoS Biology, 18(7), e3000737. https://doi.org/10.1371/journal.pbio.3000737",
                "drafted_by": [
                    "Ana Barbosa Mendes; Flávio Azevedo"
                ],
                "reviewed_by": [
                    "Valeria Agostini",
                    "Bradley Baker",
                    "Gilad Feldman",
                    "Tamara Kalandadze",
                    "Charlotte R. Pennington"
                ],
                "alt_related_terms": [
                    null
                ],
                "language": "german"
            },
            {
                "type": "glossary",
                "title": "Forschungsprotokoll (Research Protocol)",
                "definition": "Ein detailliertes Dokument, das vor der Durchführung einer Studie erstellt wird und häufig Teil von Ethik- und Drittmittelanträgen ist. Das Protokoll sollte Informationen über den Hintergrund, die Begründung und die Ziele der Studie sowie Hypothesen enthalten, die die Erwartungen der Forschenden widerspiegeln. Das Protokoll sollte auch ein \"Rezept\" für die Durchführung der Studie enthalten, einschließlich methodischer Details und klarer Analysepläne. Für bestimmte Methoden und Bereiche sollten Best-Practice-Leitlinien für die Erstellung eines Studienprotokolls verwendet werden. Es ist möglich, Forschungsprotokolle öffentlich zugänglich zu machen, um neue Kooperationspartner:innen zu gewinnen oder die effiziente Zusammenarbeit zwischen Gruppen zu erleichtern (z. B. https://www.protocols.io/). In der Medizin und im Bildungsbereich sind Protokolle oft ein eigener Artikeltyp, der sich für die Veröffentlichung in Fachzeitschriften eignet. Wo die gemeinsame Nutzung oder Veröffentlichung von Protokollen nicht üblich ist, können sich Forschende für eine Präregistrierung entscheiden.",
                "related_terms": [
                    "Many Labs",
                    "Preregistration"
                ],
                "references": "BMJ. (2015). Introducing ‘How to write and publish a Study Protocol’ using BMJ’s new eLearning programme: Research to Publication. Retrieved from https://blogs.bmj.com/bmjopen/2015/09/22/introducing-how-to-write-and-publish-a-study-protocol-using-bmjs-new-elearning-programme-research-to-publication/\n\nNosek, B. A., Ebersole, C. R., DeHaven, A. C., & Mellor, D. T. (2018). The preregistration revolution. Proceedings of the National Academy of Sciences, 115(11), 2600–2606. https://doi.org/10.1073/pnas.1708274114",
                "drafted_by": [
                    "Marta Topor"
                ],
                "reviewed_by": [
                    "Helena Hartmann",
                    "Bethan Iley",
                    "Annalise A. LaPlume",
                    "Charlotte Pennington"
                ],
                "alt_related_terms": [
                    null
                ],
                "language": "german"
            },
            {
                "type": "glossary",
                "title": "Forschungs-Arbeitsablauf (Research workflow)",
                "definition": "Der Prozess der Durchführung von Forschungsarbeiten von der Konzeption bis zur Verbreitung. Ein typischer Arbeitsablauf (workflow) könnte folgendermaßen aussehen: Beginnend mit der Konzeptualisierung, um eine Forschungsfrage zu ermitteln und eine Studie zu entwerfen. Nach der Konzeption der Studie müssen die Forschenden eine ethische Genehmigung einholen (falls erforderlich) und können beschließen, die endgültige Version zu präregistrieren. Anschließend erheben die Forschenden ihre Daten und analysieren sie. Schließlich endet der Prozess mit der Verbreitung, wobei zwischen Preprint- und Postprint-Phasen unterschieden wird, wenn das Manuskript bei einer Zeitschrift eingereicht wird.",
                "related_terms": [
                    "Open Research Workflow",
                    "Research cycle",
                    "Research pipeline"
                ],
                "references": "Kathawalla, U., Silverstein, P., & Syed, M. (2020). Easing into Open Science: A Guide for Graduate Students and Their Advisors. Collabra: Psychology. https://doi.org/10.31234/osf.io/vzjdp Retrieved from https://psyarxiv.com/vzjdp\n\nStodden, V. C. (2011). Trust your science? Open your data and code.",
                "drafted_by": [
                    "James E Bartlett"
                ],
                "reviewed_by": [
                    "Gilad Feldman",
                    "Helena Hartmann",
                    "Aleksandra Lazić",
                    "Joanne McCuaig",
                    "Timo Roettger",
                    "Sam Parsons",
                    "Steven Verheyen"
                ],
                "alt_related_terms": [
                    null
                ],
                "language": "german"
            },
            {
                "type": "glossary",
                "title": "Freiheitsgrade von Forschenden (Researcher degrees of freedom)",
                "definition": "Bezieht sich auf die Flexibilität, die dem wissenschaftlichen Prozess oft innewohnt, von der Hypothesenbildung über die Konzeption und Durchführung einer Forschungsstudie bis hin zur Verarbeitung der Daten und der Analyse sowie der Interpretation und Berichterstattung der Ergebnisse. Da es an genau definierten Theorien und/oder empirischen Beweisen mangelt, sind häufig mehrere Entscheidungen gleichermaßen vertretbar. Der Begriff wird manchmal verwendet, um auf die opportunistische (missbräuchliche) Nutzung dieser Flexibilität hinzuweisen, die darauf abzielt, gewünschte Ergebnisse zu erzielen \\- z. B. bei der Aufnahme oder dem Ausschluss bestimmter Daten \\-, auch wenn der Begriff technisch gesehen nicht von Natur aus wertend ist.",
                "related_terms": [
                    "Analytic Flexibility",
                    "Garden of forking paths",
                    "Model uncertainty",
                    "Multiverse analysis",
                    "*P*\\-hacking",
                    "Robustness (analyses)",
                    "Specification curve analysis"
                ],
                "references": "Gelman, A., & Loken, E. (n.d.). The garden of forking paths: Why multiple comparisons can be a problem, even when there is no “fishing expedition” or “p-hacking” and the research hypothesis was posited ahead of time. Retrieved from http://www.stat.columbia.edu/\n\nSimmons, J. P., Nelson, L. D., & Simonsohn, U. (2011). False-positive psychology: Undisclosed flexibility in data collection and analysis allows presenting anything as significant. Psychological Science, 22(11), 1359–1366. https://doi.org/10.1177/0956797611417632\n\nWicherts, J. M., Veldkamp, C. L., Augusteijn, H. E., Bakker, M., Van Aert, R., & Van Assen, M. A. (2016). Degrees of freedom in planning, running, analyzing, and reporting psychological studies: A checklist to avoid p-hacking. Frontiers in Psychology, 7, 1832. https://doi.org/10.3389/fpsyg.2016.01832",
                "drafted_by": [
                    "Tina Lonsdorf"
                ],
                "reviewed_by": [
                    "Gilad Feldman",
                    "Helena Hartmann",
                    "Timo Roettger",
                    "Robbie C.M. van Aert",
                    "Flávio Azevedo"
                ],
                "alt_related_terms": [
                    null
                ],
                "language": "german"
            },
            {
                "type": "glossary",
                "title": "RepliCATs project",
                "definition": "Kollaborative Bewertung für vertrauenswürdige Wissenschaft (aus dem Engl. Collaborative Assessment for Trustworthy Science). Ziel des repliCATS-Projekts ist es, Vorhersagen über die Zuverlässigkeit und Replizierbarkeit veröffentlichter Forschung in acht sozialwissenschaftlichen Bereichen zu treffen: Volkswirtschaft, Kriminologie, Betriebswirtschaft, Bildung, Politikwissenschaft, Psychologie, öffentliche Verwaltung und Soziologie.",
                "related_terms": [
                    "Replicability",
                    "Trustworthiness"
                ],
                "references": "Fraser, H., Bush, M., Wintle, B., Mody, F., Smith, E., Hanea, A., & others. (2021). Predicting reliability through structured expert elicitation with repliCATS (Collaborative Assessments for Trustworthy Science).",
                "drafted_by": [
                    "Tamara Kalandadze"
                ],
                "reviewed_by": [
                    "Sarah Ashcroft-Jones",
                    "Mahmoud Elsherif",
                    "Gilad Feldman",
                    "Helena Hartmann",
                    "Charlotte R. Pennington"
                ],
                "alt_related_terms": [
                    null
                ],
                "language": "german"
            },
            {
                "type": "glossary",
                "title": "Verantwortungsvolle Forschung und Innovation (Responsible Research and Innovation)",
                "definition": "Ein Ansatz, der die gesellschaftlichen Auswirkungen und Erwartungen in Bezug auf Forschung und Innovation berücksichtigt, mit dem Ziel, Inklusivität und Nachhaltigkeit zu fördern. Er trägt der Tatsache Rechnung, dass wissenschaftliche Bemühungen nicht von ihren breiteren Auswirkungen isoliert sind und dass die Forschung durch Faktoren motiviert wird, die über das Streben nach Wissen hinausgehen. Bei der Förderung einer verantwortungsvollen Forschung spielen daher viele Parteien eine wichtige Rolle, darunter Drittmittelgeber:innen, Forschungsteams, Interessensgruppen, Aktivist:innen und Mitglieder der Öffentlichkeit.",
                "related_terms": [
                    "Citizen Science",
                    "Public Engagement",
                    "Transdisciplinary Research"
                ],
                "references": "",
                "drafted_by": [
                    "Ana Barbosa Mendes"
                ],
                "reviewed_by": [
                    "Helena Hartmann",
                    "Joanne McCuaig",
                    "Sam Parsons",
                    "Graham Reid"
                ],
                "alt_related_terms": [
                    null
                ],
                "language": "german"
            },
            {
                "type": "glossary",
                "title": "Reverse p-hacking",
                "definition": "Ausnutzung der Freiheitsgrade des Forschenden bei der statistischen Analyse, um die Wahrscheinlichkeit zu erhöhen, dass die Nullhypothese beibehalten wird (z. B. *p* \\> .05).",
                "related_terms": [
                    "Analytic flexibility",
                    "HARKing",
                    "P-hacking",
                    "Questionable Research Practices or Questionable Reporting Practices (QRPs)",
                    "Researcher degrees of freedom",
                    "Selective reporting"
                ],
                "references": "Chuard, P. J. C., Vrtilek, M., Head, M. L., & Jennions, M. D. (2019). Evidence that non-significant results are sometimes preferred: Reverse P-hacking or selective reporting? PLoS Biol, 17(1), e3000127. https://doi.org/10.1371/journal.pbio.3000127",
                "drafted_by": [
                    "Robert M. Ross"
                ],
                "reviewed_by": [
                    "Mahmoud Elsherif",
                    "Alexander Hart",
                    "Sam Parsons",
                    "Timo Roettger"
                ],
                "alt_related_terms": [
                    null
                ],
                "language": "german"
            },
            {
                "type": "glossary",
                "title": "RIOT Science Club",
                "definition": "Der RIOT Science Club ist eine standortübergreifende Seminarreihe, die das Bewusstsein für reproduzierbare, interpretierbare, offene und transparente wissenschaftliche Praktiken schärft und Schulungen dazu anbietet. Es werden regelmäßig Vorträge, Workshops und Konferenzen angeboten, die alle offen zugänglich sind und auf den Websites der jeweiligen Standorte und auf Youtube nachverfolgt werden können.",
                "related_terms": [
                    "Early career researchers (ECRs)",
                    "Interpretability",
                    "Openness",
                    "Reproducibility",
                    "Transparency"
                ],
                "references": "",
                "drafted_by": [
                    "Tamara Kalandadze"
                ],
                "reviewed_by": [
                    "Helena Hartmann",
                    "Emma Henderson",
                    "Joanne McCuaig",
                    "Flávio Azevedo"
                ],
                "alt_related_terms": [
                    null
                ],
                "language": "german"
            },
            {
                "type": "glossary",
                "title": "analyses Robustheit-sanalysen (Robustness (analyses))",
                "definition": "Die Beständigkeit der Unterstützung für eine Hypothese bei Störungen der methodischen/analytischen Pipeline. Mit anderen Worten: Anwendung verschiedener Methoden/Analyseverfahren, um zu prüfen, ob dieselbe Schlussfolgerung unter verschiedenen analytischen Bedingungen unterstützt wird.",
                "related_terms": [
                    "Many Labs",
                    "Multiverse analysis",
                    "Sensitivity analyses",
                    "Specification Curve Analysis **Alternative definition:** “Robustness refers to the stability of experimental conclusions to variations in either baseline assumptions or experimental procedures. It is somewhat related to the concept of generalizability (also known as transportability), which refers to the persistence of an effect in settings different from and outside of an experimental framework \\[...\\] Whether a study design is similar enough to the original to be considered a replication, a “robustness test,” or some of many variations of pure replication that have been identified, particularly in the social sciences (for example, conceptual replication, pseudoreplication), is an unsettled question” (Goodman et al., 2016)."
                ],
                "references": "Goodman, S. N., Fanelli, D., & Ioannidis, J. P. A. (2016). What does research reproducibility mean? Science Translational Medicine, 8(341), 341ps12-341ps12. https://doi.org/10.1126/scitranslmed.aaf5027\n\nNosek, B. A., & Errington, T. M. (2020). What is replication? PLOS Biology, 18(3), e3000691. https://doi.org/10.1371/journal.pbio.3000691",
                "drafted_by": [
                    "Tina Lonsdorf; Flávio Azevedo"
                ],
                "reviewed_by": [
                    "Gilad Feldman",
                    "Adrien Fillon",
                    "Helena Hartmann",
                    "Timo Roettger"
                ],
                "alt_related_terms": [
                    null
                ],
                "language": "german"
            },
            {
                "type": "glossary",
                "title": "Salamischneiden (Salami slicing)",
                "definition": "Eine fragwürdige Forschungspraktik mit dem Ziel, im Nachhinein einer Datenerhebung (post-hoc) die Anzahl an Publikationen zu erhöhen. Beispielsweise werden die Daten einer einzelnen Erhebung mehrmals für unterschiedliche Publikationen in unterschiedlichen Zeitschriften verwendet, ohne dass dieser Zusammenhang zwischen den Publikationen und ihren Ergebnissen hinreichend transparent gemacht wird. Ein Datensatz (die *Salami*) wird also in mehrere “Publikationsscheiben” zerschnitten (*slicing*). Dieses Vorgehen führt zu einer Verzerrung in der Forschungsliteratur zu einem Thema, da es eine höhere Menge an unabhängigen Forschungsbefunden vortäuscht. Das wirkt sich auch stark auf die Ergebnisse von Meta-Analysen aus, bei denen davon ausgegangen wird, dass die einzelnen Effektmaße in den einzelnen Publikationen von unabhängigen Stichproben stammen. Dies ist bei Salami Slicing nicht der Fall.",
                "related_terms": [
                    "Gaming (the system)",
                    "Questionable Research Practices or Questionable Reporting Practices (QRPs)",
                    "Partial publication"
                ],
                "references": "Fanelli, D. (2018). Opinion: Is science really facing a reproducibility crisis, and do we need it to? Proceedings of the National Academy of Sciences, 115(11), 2628–2631. https://doi.org/10.1073/pnas.1708272114",
                "drafted_by": [
                    "Mahmoud Elsherif"
                ],
                "reviewed_by": [
                    "Sarah Ashcroft-Jones",
                    "Adrien Fillon",
                    "Helena Hartmann",
                    "Matt Jaquiery",
                    "Tamara Kalandadze",
                    "Charlotte R. Pennington",
                    "Graham Reid",
                    "Suzanne L. K. Stewart"
                ],
                "alt_related_terms": [
                    null
                ],
                "language": "german"
            },
            {
                "type": "glossary",
                "title": "Untergrabung (Scooping)",
                "definition": "Der Akt der Berichterstattung über oder Veröffentlichung eines neuen Ergebnisses vor anderen Forschenden/Teams. Umfragebasierte Untersuchungen deuten darauf hin, dass die Furcht davor, untergraben (scooped) zu werden, ein wichtiges angstbezogenes Hindernis für die gemeinsame Nutzung von Daten in der Psychologie ist, und agentenbasierte Modelle deuten darauf hin, dass der Wettbewerb um Prioritäten die wissenschaftliche Reliabilität beeinträchtigt (Tiokhin et al. 2021).",
                "related_terms": [
                    "Novelty",
                    "Open data",
                    "Preregistration"
                ],
                "references": "Houtkoop, B. L., Chambers, C., Macleod, M., Bishop, D. V. M., Nichols, T. E., & Wagenmekers, E.-J. (2018). Data sharing in psychology: A survey on barriers and preconditions. Advances in Methods and Practices in Psychological Science, 1(1), 70.85. https://doi.org/10.1177/2515245917751886\n\nLaine, H. (2017). Afraid of scooping – Case study on researcher strategies against fear of scooping in the context of open science. In Data Science Journal (Vol. 16, pp. 1–14). https://doi.org/10.5334/dsj-2017-029\n\nTiokhin, L., Yan, M., & Horgan, T. J. H. (2021). Competition for priority harms the reliability of science, but reforms can help. Nature Human Behaviour. https://doi.org/10.1038/s41562-020-01040-1",
                "drafted_by": [
                    "William Ngiam"
                ],
                "reviewed_by": [
                    "Ashley Blake",
                    "Thomas Rhys Evans",
                    "Connor Keating",
                    "Graham Reid",
                    "Timo Roettger",
                    "Robert M. Ross",
                    "Flávio Azevedo"
                ],
                "alt_related_terms": [
                    null
                ],
                "language": "german"
            },
            {
                "type": "glossary",
                "title": "Semantometrie (Semantometrics)",
                "definition": "Eine Klasse von Metriken zur Bewertung von Forschung unter Verwendung des vollständigen Veröffentlichungstextes zur Messung der semantischen Ähnlichkeit von Veröffentlichungen und zur Hervorhebung des Beitrags eines Artikels zum Fortschritt der wissenschaftlichen Diskussion. Sie ist eine Erweiterung von Instrumenten wie Bibliometrie, Webometrie und Altmetrik.",
                "related_terms": [
                    "Bibliometrics",
                    "Contribution(p)"
                ],
                "references": "Herrmannova, D., & Knoth, P. (n.d.). Semantometrics Towards Full text-based Research Evaluation. Retrieved from https://arxiv.org/pdf/1605.04180.pdf\n\nKnoth, P., & Herrmannova, D. (2014). Towards semantometrics: A new semantic similarity based measure for assessing a research publication’s contribution. D-Lib Magazine, 20(11), 8. https://doi.org/10.1045/november2014-knoth",
                "drafted_by": [
                    "Alaa AlDoh"
                ],
                "reviewed_by": [
                    "Jamie P. Cockcroft",
                    "Mahmoud Elsherif",
                    "Christopher Graham",
                    "Charlotte R. Pennington"
                ],
                "alt_related_terms": [
                    null
                ],
                "language": "german"
            },
            {
                "type": "glossary",
                "title": "Sensitive Forschung (Sensitive research)",
                "definition": "Forschung, die eine Bedrohung für diejenigen darstellt, die daran beteiligt sind oder waren, einschließlich der Forschenden, der Versuchspersonen und der breiteren Gesellschaft. Diese Bedrohung kann eine physische Gefahr (z. B. Selbstmord) oder eine negative emotionale Reaktion (z. B. Depression) für die am Forschungsprozess Beteiligten sein. Bei Forschungsarbeiten, die an Opfern von Selbstmord durchgeführt werden, könnten Forschende beispielsweise durch die Beschreibungen des suizidalen Verhaltens emotional traumatisiert werden. Die Kommunikation mit den Opfern könnte auch dazu führen, dass sie die traumatischen Erinnerungen wiedererleben, was zu negativen psychischen Reaktionen führt.",
                "related_terms": [
                    "Anonymity"
                ],
                "references": "Lee, R. M. (1993). Doing research on sensitive topics. Sage.\n\nAlbayrak-Aydemir, N. (2020). The hidden costs of being a scholar from the global south. Retrieved from https://blogs.lse.ac.uk/highereducation/2020/02/20/the-hidden-costs-of-being-a-scholar-from-the-global-south/",
                "drafted_by": [
                    "Nihan Albayrak-Aydemir"
                ],
                "reviewed_by": [
                    "Valeria Agostini",
                    "Mahmoud Elsherif",
                    "Helena Hartmann",
                    "Graham Reid"
                ],
                "alt_related_terms": [
                    null
                ],
                "language": "german"
            },
            {
                "type": "glossary",
                "title": "SDC (Sequence-determines-credit approach (SDC))",
                "definition": "Ein Autor:innenschaftssystem, das die Reihenfolge der Autor:innenschaft auf der Grundlage des Beitrags der einzelnen Autor:innen festlegt. Die Namen der Autor:innen werden entsprechend ihres Beitrags in absteigender Reihenfolge aufgeführt, wobei der/die Autor:in mit dem größten Beitrag an erster Stelle und der/die Autor:in mit dem geringsten Beitrag an letzter Stelle steht.",
                "related_terms": [
                    "Authorship",
                    "First-last-author-emphasis norm (FLAE)"
                ],
                "references": "Schmidt, R. H. (1987). A worksheet for authorship of scientific articles. The Bulletin of the Ecological Society of America, 68, 8–10. http://www.jstor.org/stable/20166549\n\nTscharntke, T., Hochberg, M. E., Rand, T. A., Resh, V. H., & Krauss, J. (2007). Author sequence and credit for contributions in multiauthored publications. PLoS Biology, 5(1), e18. https://doi.org/10.1371/journal.pbio.0050018",
                "drafted_by": [
                    "Myriam A. Baum"
                ],
                "reviewed_by": [
                    "Sam Parsons",
                    "Charlotte R. Pennington"
                ],
                "alt_related_terms": [
                    null
                ],
                "language": "german"
            },
            {
                "type": "glossary",
                "title": "Sherpa Romeo",
                "definition": "Eine Online-Ressource, die die Open-Access-Richtlinien von wissenschaftlichen Verlagen aus der ganzen Welt sammelt und Zusammenfassungen der Urheberrechts- und Open-Access-Archivierungsrichtlinien der einzelnen Zeitschriften bereitstellt.",
                "related_terms": [
                    "Embargo period",
                    "Open access",
                    "Paywall",
                    "Preprint",
                    "Repository"
                ],
                "references": "Anon. (n.d.). Welcome to Sherpa Romeo - v2.sherpa. Retrieved from https://v2.sherpa.ac.uk/romeo/",
                "drafted_by": [
                    "Aleksandra Lazić"
                ],
                "reviewed_by": [
                    "Mahmoud Elsherif",
                    "Christopher Graham",
                    "Sam Parsons",
                    "Martin Vasilev"
                ],
                "alt_related_terms": [
                    null
                ],
                "language": "german"
            },
            {
                "type": "glossary",
                "title": "Einfachblinde Peer Begutachtung (Single-blind peer review)",
                "definition": "Bewertung von Forschungsergebnissen durch qualifizierte Expert:innen, wobei die Gutachter:innen die Identität der Autor:innen kennen, die Begutachtenden aber gegenüber den Autor:innen anonym bleiben.",
                "related_terms": [
                    "Anonymous review",
                    "Double-blind peer review",
                    "Masked review",
                    "Open Peer Review",
                    "Peer review",
                    "Triple-blind peer review"
                ],
                "references": "Largent, E. A., & Snodgrass, R. T. (2016). Blind peer review by academic journals. In C. T. Robertson & A. S. Kesselheim (Eds.), Blinding as a solution to bias: Strengthening biomedical science, forensic science, and law (pp. 75–95). Academic Press. https://doi.org/10.1016/B978-0-12-802460-7.00005-X",
                "drafted_by": [
                    "Bradley Baker"
                ],
                "reviewed_by": [
                    "Ashley Blake",
                    "Christopher Graham",
                    "Helena Hartmann",
                    "Graham Reid"
                ],
                "alt_related_terms": [
                    null
                ],
                "language": "german"
            },
            {
                "type": "glossary",
                "title": "Langsame Forschung (Slow science)",
                "definition": "Open Scholarship-Praktiken umzusetzen führt zu einem längeren Forschungsprozess, bei dem mehr Wert auf Transparenz, Reproduzierbarkeit, Replizierbarkeit und Qualität als auf die Quantität der Ergebnisse gelegt wird. Langsame Forschung (Slow science) wendet sich gegen die \"*publish-or-perish*\"-Kultur (dt. Publizieren oder untergehen) und bezeichnet ein akademisches System, das Zeit und Ressourcen für die Produktion von hochwertigen und transparenten Ergebnissen bereitstellt, beispielsweise indem Forschende mehr Zeit für Datenerhebung, für das Lesen der Literatur, für das Nachdenken darüber, wie ihre Ergebnisse zur bestehenden Literatur passen, und für die Dokumentation, Bereitstellung und Austausch von Forschungsmaterialien aufwenden, anstatt zusätzliche Studien durchzuführen.",
                "related_terms": [
                    "collaboration",
                    "Incentive structure",
                    "Publish or Perish",
                    "research culture",
                    "research quality"
                ],
                "references": "Academy, S. S. (2010). The Slow Science Manifesto. Slow Science. http://slow-science.org/\n\nNelson, L. D., Simmons, J. P., & Simonsohn, U. (2012). Let’s Publish Fewer Papers. Psychological Inquiry, 23(3), 291–293. https://doi.org/10.1080/1047840X.2012.705245\n\nFrith, U. (2020). Fast lane to slow science. Trends in Cognitive Sciences, 24(1), 1–2. https://doi.org/10.1016/j.tics.2019.10.007",
                "drafted_by": [
                    "Sonia Rishi"
                ],
                "reviewed_by": [
                    "Adrien Fillon",
                    "Tamara Kalandadze",
                    "Sam Parsons Charlotte R. Pennington",
                    "Robert M Ross",
                    "Timo Roettger"
                ],
                "alt_related_terms": [
                    null
                ],
                "language": "german"
            },
            {
                "type": "glossary",
                "title": "SORTEE (Society for Open, Reliable, and Transparent Ecology and Evolutionary biology (SORTEE))",
                "definition": "SORTEE (https://www.sortee.org/) ist eine internationale Gesellschaft mit dem Ziel, die Transparenz und Zuverlässigkeit von Forschungsergebnissen in den Bereichen Ökologie, Evolution und verwandten Disziplinen durch kulturelle und institutionelle Veränderungen zu verbessern. SORTEE wurde im Dezember 2020 gegründet und richtet sich an alle, die an der Verbesserung der Forschung in diesen Disziplinen interessiert sind, unabhängig von ihrer Erfahrung. Die Gesellschaft ist in Bezug auf Umfang, Mitgliedschaft und Ziele international. SORTEE umfasst über 600 Mitglieder (Stand Mai, 2021).",
                "related_terms": [
                    "Society for the Improvement of Psychological Science (SIPS)"
                ],
                "references": "",
                "drafted_by": [
                    "Brice Beffara Bret; Dominique Roche"
                ],
                "reviewed_by": [
                    "Sarah Ashcroft-Jones",
                    "Mahmoud Elsherif",
                    "Charlotte R. Pennington",
                    "Graham Reid"
                ],
                "alt_related_terms": [
                    null
                ],
                "language": "german"
            },
            {
                "type": "glossary",
                "title": "SIPS (Society for the Improvement of Psychological Science (SIPS))",
                "definition": "Die SIPS ist eine Mitgliedsgesellschaft und wurde mit dem Ziel gegründet, bessere Praktiken und Methoden im Bereich der psychologischen Forschung zu fördern. Diese Mission möchte die SIPS durch verschiedene Initiativen, Maßnahmen und Kooperationen innerhalb und außerhalb der Psychologie erfüllen. Diese sollen dazu beitragen, die wissenschaftliche Ausbildung von Forschenden und Studierenden zu verbessern und eine Forschungskultur zu verbreiten, die eine bessere Qualität der Forschung ermöglicht. Zudem geht es auch darum, die Auswirkungen solcher Reformen empirisch zu bewerten und zu quantifizieren.",
                "related_terms": [
                    "Society for Open, Reliable, and Transparent Ecology and Evolutionary biology (SORTEE)"
                ],
                "references": "Improving Psychology. (n.d.). Improving Psychology. https://improvingpsych.org/",
                "drafted_by": [
                    "Mahmoud Elsherif"
                ],
                "reviewed_by": [
                    "Ashley Blake",
                    "Jade Pickering",
                    "Graham Reid",
                    "Flávio Azevedo"
                ],
                "alt_related_terms": [
                    null
                ],
                "language": "german"
            },
            {
                "type": "glossary",
                "title": "Soziale Schicht (Social class)",
                "definition": "Die soziale Schicht wird in der Regel sowohl durch objektive als auch durch subjektive Messungen erfasst, wie von der American Psychological Association empfohlen (American Psychological Association, Task Force on Socioeconomic Status, 2007). Im Gegensatz zum herkömmlichen Konzept, bei dem nur ein Faktor, entweder Bildung oder Einkommen (d. h. wirtschaftliche Variablen), berücksichtigt wird, wird die soziale Schicht einer Person als eine Kombination aus Bildung, Einkommen, beruflichem Prestige, subjektivem sozialem Status und selbst identifizierter sozialer Klasse betrachtet. Die soziale Schicht ist zum Teil eine kulturelle Variable, da es sich um eine stabile Variable handelt, die sich im Laufe der Jahre wahrscheinlich nur langsam verändert. Die soziale Schicht kann wichtige Auswirkungen auf die schulischen Leistungen haben. Eine Person kann einen hohen sozioökonomischen Status haben und sich dennoch als Arbeiter bezeichnen. Studierende mit nicht-akademischem Hintergrund haben in der Regel andere Lebensumstände und oft restriktivere Verpflichtungen als Studierende aus der Mittelschicht, was ihre Integration mit anderen Studierenden erschwert (Rubin, 2021). Der Mangel an Zeit und Geld behindert ihre sozialen Erfahrungen an der Universität. Studierende mit nicht-akademischem Hintergrund müssen eher arbeiten, um ihren Lebensunterhalt zu bestreiten, was dazu führt, dass sie weniger Zeit für akademische Aktivitäten und soziale Kontakte mit anderen Studierenden und weniger Geld für den Kauf von Waren haben, die mit sozialen Erfahrungen verbunden sind (z. B. Essen).",
                "related_terms": [
                    "Social integration"
                ],
                "references": "Evans, O., & Rubin, M. (2021). In a Class on Their Own: Investigating the Role of Social Integration in the Association Between Social Class and Mental Well-Being. Personality and Social Psychology Bulletin, 014616722110211. https://doi.org/10.1177/01461672211021190\n\nRubin, M., Evans, O., & McGuffog, R. (2019). Social class differences in social integration at university: Implications for academic outcomes and mental health. In J. Jetten & K. Peters (Eds.), The social psychology of inequality (pp. 87–102). Springer. https://doi.org/10.1007/978-3-030-28856-3_6\n\nRubin, M. (2021). Explaining the association between subjective social status and mental health among university students using an impact ratings approach. SN Social Sciences, 1(1), 1–21. https://doi.org/10.1007/s43545-020-00031-3",
                "drafted_by": [
                    "Mahmoud Elsherif"
                ],
                "reviewed_by": [
                    "Leticia Micheli",
                    "Eliza Woodward",
                    "Julika Wolska",
                    "Gerald Vineyard**;** Yu-Fang Yang"
                ],
                "alt_related_terms": [
                    null
                ],
                "language": "german"
            },
            {
                "type": "glossary",
                "title": "Soziale Integration (Social integration)",
                "definition": "Soziale Integration ist ein mehrdimensionales Konstrukt. Im akademischen Kontext bezieht sich die soziale Integration auf die Quantität und Qualität der sozialen Interaktionen mit Mitarbeitenden und Studierenden sowie auf das Gefühl der Verbundenheit und Zugehörigkeit zur Universität und zu den Menschen innerhalb des Instituts. Genauer gesagt, sind soziale Unterstützung, Vertrauen und Verbundenheit allesamt Variablen, die zur sozialen Integration beitragen. Die soziale Integration hat wichtige Auswirkungen auf die akademischen Ergebnisse und das psychische Wohlbefinden (Evans & Rubin, 2021). Bei Studierenden mit nicht-akademischem Hintergrund ist die Wahrscheinlichkeit geringer, dass sie sich mit anderen Studierenden zusammenschließen, da sie einen unterschiedlichen sozialen und ökonomischen Hintergrund und ein geringeres verfügbares Einkommen haben. Daher können sie nicht so viele Bildungsmöglichkeiten und finanzielle Hilfen in Anspruch nehmen wie andere. Dies wiederum kann zu einer schlechten psychischen Gesundheit und dem Gefühl der Ausgrenzung führen (Rubin, 2021).",
                "related_terms": [
                    "Social class"
                ],
                "references": "Evans, O., & Rubin, M. (2021). In a Class on Their Own: Investigating the Role of Social Integration in the Association Between Social Class and Mental Well-Being. Personality and Social Psychology Bulletin, 014616722110211. https://doi.org/10.1177/01461672211021190\n\nRubin, M., Evans, O., & McGuffog, R. (2019). Social class differences in social integration at university: Implications for academic outcomes and mental health. In J. Jetten & K. Peters (Eds.), The social psychology of inequality (pp. 87–102). Springer. https://doi.org/10.1007/978-3-030-28856-3_6\n\nRubin, M. (2021). Explaining the association between subjective social status and mental health among university students using an impact ratings approach. SN Social Sciences, 1(1), 1–21. https://doi.org/10.1007/s43545-020-00031-3",
                "drafted_by": [
                    "Mahmoud Elsherif"
                ],
                "reviewed_by": [
                    "Leticia Micheli",
                    "Eliza Woodward",
                    "Julika Wolska**;** Gerald Vineyard",
                    "Yu-Fang Yang",
                    "Flávio Azevedo"
                ],
                "alt_related_terms": [
                    null
                ],
                "language": "german"
            },
            {
                "type": "glossary",
                "title": "Spezifikationskurvenanalyse (Specification Curve Analysis)",
                "definition": "Ein analytischer Ansatz, der darin besteht, alle sinnvollen Spezifikationen für eine bestimmte Forschungsfrage (mittels Inferenzstatistik) zu identifizieren, zu berechnen, zu visualisieren und zu interpretieren (siehe Simonsohn et al. 2015). Die Spezifikationskurvenanalyse hilft, den Einfluss von vermutlich willkürlichen Entscheidungen im wissenschaftlichen Verlauf (z. B. Versuchsplanung, Konstruktoperationalisierung, statistische Modelle oder mehrere davon), die Forschende getroffen haben, transparent zu machen, indem sie alle nicht redundanten, sinnvollen Tests der Forschungsfrage umfassend darstellt. Voracek et al. (2019) weisen darauf hin, dass sich die Spezifikationskurvenanalyse von der Multiversumsanalyse in Bezug auf die grafischen Darstellungen (ein Spezifikationskurvendiagramm anstelle eines Histogramms und Kacheldiagramms) und die Verwendung von Inferenzstatistiken zur Interpretation der Ergebnisse unterscheidet.",
                "related_terms": [
                    "Multiverse analysis",
                    "Research synthesis",
                    "Robustness (analyses)",
                    "Selective reporting",
                    "Vibration of effects"
                ],
                "references": "Simonsohn, U., Simmons, J. P., & Nelson, L. D. (2015). Specification curve: Descriptive and inferential statistics on all reasonable specifications. http://sticerd.lse.ac.uk/seminarpapers/psyc16022016.pdf\n\nSimonsohn, U., Simmons, J. P., & Nelson, L. D. (2020). Specification curve analysis. Nature Human Behaviour, 4(11), 1208–1214. https://doi.org/10.1038/s41562-020-0912-z\n\nVoracek, M., Kossmeier, M., & Tran, U. S. (2019). Which Data to Meta-Analyze, and How? Zeitschrift Für Psychologie. https://doi.org/10.1027/2151-2604/a000357",
                "drafted_by": [
                    "Bradley Baker"
                ],
                "reviewed_by": [
                    "Tina B. Lonsdorf",
                    "Sam Parsons",
                    "Charlotte R. Pennington"
                ],
                "alt_related_terms": [
                    null
                ],
                "language": "german"
            },
            {
                "type": "glossary",
                "title": "Statistische Vorannahmen (Statistical Assumptions)",
                "definition": "Analytische Ansätze und Modelle gehen davon aus, dass die Daten, auf die sie angewendet werden, bestimmte Eigenschaften aufweisen (beispielsweise statistische Unabhängigkeit, Ziehung einer Zufallsstichprobe, Normalverteilung, Varianzhomogenität, …). Vor der Datenanalyse sollten diese Annahmen überprüft werden, da es die Ergebnisse und die Schlussfolgerung der Analyse ändern kann, wenn die Annahmen nicht erfüllt sind. Im Rahmen von offener, reproduzierbarer Forschung gehört es zur guten Praxis, das Testen der Annahmen zu dokumentieren und zu berichten, welche Annahmen geprüft wurden, durch welche Tests sie geprüft wurden, was die Ergebnisse waren und welche Maßnahmen oder Korrekturen durchgeführt wurden.",
                "related_terms": [
                    "Null Hypothesis Significance Testing (NHST)",
                    "Statistical Significance",
                    "Statistical Validity",
                    "Transparency",
                    "Type I error",
                    "Type II error",
                    "Type M error",
                    "Type S error"
                ],
                "references": "Garson, G. D. (2012). Testing Statistical Assumptions (2012th ed.). North Carolina State University.\n\nHahn, G. J., & Meeker, W. Q. (1993). Assumptions for Statistical Inference. The American Statistician, 47(1), 1–11. https://doi.org/10.1080/00031305.1993.10475924\n\nHoekstra, R., Kiers, H., & Johnson, A. (2012). Are assumptions of well-known statistical techniques checked, and why (not)? Frontiers in Psychology, 3(137), 1–9. https://doi.org/10.3389/fpsyg.2012.00137",
                "drafted_by": [
                    "Graham Reid"
                ],
                "reviewed_by": [
                    "Jamie P. Cockcroft",
                    "Sam Parsons",
                    "Martin Vasilev",
                    "Julia Wolska"
                ],
                "alt_related_terms": [
                    null
                ],
                "language": "german"
            },
            {
                "type": "glossary",
                "title": "Statistische Teststärke (Statistical power)",
                "definition": "Die Teststärke (power), manchmal auch als statistische Aussagekraft bezeichnet, ist die langfristige Wahrscheinlichkeit, dass die Alternativhypothese zutrifft und ein statistischer Test korrekterweise die Nullhypothese verwirft. Die Teststärke geht von 0 bis 1, wird häufig aber als Prozentzahl ausgedrückt. Berechnet wird sie aus dem gewählten Signifikanzniveau, der Effektstärke und der Stichprobengröße in Abhängigkeit der jeweiligen Analysemethode. Es gibt zwei Hauptanwendungsgebiete: Bei der a-priori Teststärke geht es um die Frage, wie groß die Stichprobe sein muss, damit ein Effekt mit X-prozentiger langfristiger Wahrscheinlichkeit auch tatsächlich entdeckt wird. Die Sensitivitäts-Teststärke  dreht sich um die Frage, welche Effektgröße man finden kann, wenn eine bestimmte Stichprobengröße gegeben ist.",
                "related_terms": [
                    "Effect Size",
                    "Meta-analysis",
                    "Null Hypothesis Significance Testing (NHST)",
                    "Power Analysis",
                    "Positive Predictive Value",
                    "Quantitative research",
                    "Sample size",
                    "Significance criterion (alpha)",
                    "Type I error",
                    "Type II error **Related terms to alternative definition:** Type II Error"
                ],
                "references": "Carter, A., Tilling, K., & Munafo, M. R. (2021). Considerations of sample size and power calculations given a range of analytical scenarios. https://doi.org/10.31234/osf.io/tcqrn\n\nCohen, J. (1962). The statistical power of abnormal-social psychological research: A review. The Journal of Abnormal and Social Psychology, 65(3), 145–153. https://doi.org/10.1037/h0045186\n\nCohen, J. (1969). Statistical power analysis for the behavioral sciences. Academic Press.\n\nDienes, Z. (2008). Understanding psychology as a science: An introduction to scientific and statistical inference. Macmillan International Higher Education.\n\nGiner-Sorolla, R., Aberson, C. L., Bostyn, D. H., Carpenter, T., Conrique, B. G., Lewis, N. A., & Soderberg, C. (2019). Power to detect what? Considerations for planning and evaluating sample size. Retrieved from https://osf.io/jnmya/\n\nIoannidis, J. P. (2005). Why most published research findings are false. PLoS Medicine, 2(8), e124. https://doi.org/10.1371/journal.pmed.0020124\n\nLakens, D. (2021). Sample Size Justification. https://doi.org/10.31234/osf.io/9d3yf",
                "drafted_by": [
                    "Thomas Rhys Evans"
                ],
                "reviewed_by": [
                    "James E. Bartlett",
                    "Jamie P. Cockcroft",
                    "Adrien Fillon",
                    "Emma Henderson",
                    "Tamara Kalandadze",
                    "William Ngiam",
                    "Catia M. Oliveira",
                    "Charlotte R. Pennington",
                    "Graham Reid",
                    "Martin Vasilev",
                    "Qinyu Xiao",
                    "Flávio Azevedo"
                ],
                "alt_related_terms": [
                    null
                ],
                "language": "german"
            },
            {
                "type": "glossary",
                "title": "Statistische Signifikanz (Statistical significance)",
                "definition": "Eine Eigenschaft eines Ergebnisses unter Verwendung von Nullhypothesen-Signifikanztestung (Null Hypothesis Significance Testing, NHST), das gegeben eines Signifikanzniveaus als unwahrscheinlich angesehen wird wenn die Nullhypothese gilt. Tenny und Abdelgawad (2017) definierten sie als \"a measure of the probability of obtaining your data or more extreme data assuming the null hypothesis is true, compared to a pre-selected acceptable level of uncertainty regarding the true answer” (dt. ein Maß für die Wahrscheinlichkeit, die vorliegenden Daten oder extremere Daten zu erhalten, unter der Annahme, dass die Nullhypothese wahr ist, im Vergleich zu einem im Voraus gewählten akzeptierbaren Unsicherheitsgrad hinsichtlich der wahren Antwort\" (S. 1). Die Konventionen zur Bestimmung des Schwellenwerts variieren je nach Anwendung und Disziplin, hängen aber letztlich von den Überlegungen der Forschenden über eine angemessene Fehlerquote ab. In der Erklärung der American Statistical Association (Wasserstein & Lazar, 2016\\) heißt es: \"Researchers often wish to turn a *p*\\-value into a statement about the truth of a null hypothesis, or about the probability that random chance produced the observed data. The *p*\\-value is neither. It is a statement about data in relation to a specified hypothetical explanation, and is not a statement about the explanation itself” (dt. Forschende wollen einen *p*\\-Wert oft in eine Aussage über die Wahrheit einer Nullhypothese oder über die Wahrscheinlichkeit verwandeln, dass die beobachteten Daten durch Zufall entstanden sind. Der *p*\\-Wert ist weder das eine noch das andere. Er ist eine Aussage über die Daten in Bezug auf eine bestimmte hypothetische Erklärung und ist keine Aussage über die Erklärung selbst\" (S. 131).",
                "related_terms": [
                    "Alpha error",
                    "Frequentist statistics",
                    "Null hypothesis",
                    "Null Hypothesis Significance Testing (NHST)",
                    "*P*\\-value",
                    "Type I error **Incorrect definition:** Statistical significance describes the likelihood of the observed result against chance (regardless of the null hypotheses)"
                ],
                "references": "Cassidy, S. A., Dimova, R., Giguère, B., Spence, J. R., & Stanley, D. J. (2019). Failing grade: 89% of introduction-to-psychology textbooks that define or explain statistical significance do so incorrectly. Advances in Methods and Practices in Psychological Science, 2(3), 233–239. https://doi.org/10.1177/2515245919858072\n\nWasserstein, R. L., & Lazar, N. A. (2016). The ASA Statement on p-Values: Context, Process, and Purpose. The American Statistician, 70, 129–133. https://doi.org/10.1080/00031305.2016.1154108",
                "drafted_by": [
                    "Alaa AlDoh; Flávio Azevedo"
                ],
                "reviewed_by": [
                    "James E. Bartlett",
                    "Alexander Hart**;** Annalise A. LaPlume",
                    "Charlotte R. Pennington",
                    "Graham Reid",
                    "Timo Roettger",
                    "Suzanne L. K. Stewart"
                ],
                "alt_related_terms": [
                    null
                ],
                "language": "german"
            },
            {
                "type": "glossary",
                "title": "Statistische Validität (Statistical validity)",
                "definition": "Das Ausmaß, in dem die Schlussfolgerungen aus einem statistischen Test akkurat sind und den in der Natur vorkommenden wahren Effekt widerspiegeln. Mit anderen Worten, ob ein Zusammenhang zwischen zwei Variablen besteht und mit den durchgeführten Analysen genau festgestellt werden kann. Zu den Gefahren für die statistische Validität gehören eine geringe Power (Teststärke), die Verletzung von Annahmen, die Zuverlässigkeit von Messungen usw., die die Zuverlässigkeit und Allgemeinheit der Schlussfolgerungen beeinträchtigen.",
                "related_terms": [
                    "Power",
                    "Validity",
                    "Statistical assumptions"
                ],
                "references": "Cook, T. D., & Campbell, D. T. (1979). Quasi-Experimentation. Rand McNally.\n\nDrost, E. A. (2011). Validity and reliability in social science research. Education Research and Perspectives, 38(1), 105–123.",
                "drafted_by": [
                    "Annalise A. LaPlume"
                ],
                "reviewed_by": [
                    "Jamie P. Cockcroft, Zoltan Kekecs",
                    "Graham Reid"
                ],
                "alt_related_terms": [
                    null
                ],
                "language": "german"
            },
            {
                "type": "glossary",
                "title": "STRANGE",
                "definition": "Das STRANGE \"Framework\" ist ein Vorschlag und eine Reihe von Fragen, die Tierverhaltensforschenden helfen sollen, Stichprobenverzerrungen bei der Planung, Durchführung und Interpretation von Forschungsarbeiten mit Tieren zu berücksichtigen. STRANGE ist ein Akronym, das verschiedene mögliche Quellen für Stichprobenverzerrungen in der Tierforschung hervorhebt, wie z. B. den Sozialen Hintergrund der Tiere, die Einfangbarkeit (Trappability) und Selbstselektion, die Aufzuchtgeschichte (Rearing), die Akklimatisierung und Gewöhnung, Natürliche Veränderungen der Reaktionsfähigkeit, die Genetische Ausstattung und die Erfahrung.",
                "related_terms": [
                    "Bias",
                    "Constraints on Generality (COG)",
                    "Populations",
                    "Sampling bias",
                    "WEIRD"
                ],
                "references": "Webster, M. M., & Rutz, C. (2020). How STRANGE are your study animals? Nature, 582, 337–340. https://doi.org/10.1038/d41586-020-01751-5",
                "drafted_by": [
                    "Mahmoud Elsherif"
                ],
                "reviewed_by": [
                    "Ben Farrar",
                    "Zoe Flack",
                    "Elias Garcia-Pelegrin",
                    "Charlotte R. Pennington",
                    "Graham Reid"
                ],
                "alt_related_terms": [
                    null
                ],
                "language": "german"
            },
            {
                "type": "glossary",
                "title": "StudySwap",
                "definition": "Eine kostenlose Online-Plattform, auf der Forschende kurze Beschreibungen von Forschungsprojekten oder Ressourcen einstellen, die sie zur Nutzung freigeben können (\"haves\") oder die sie benötigen und die ein/e andere/r Forschende/r haben könnte (\"needs\"). StudySwap ist ein Crowdsourcing-Ansatz für die Forschung, der dafür sorgen kann, dass weniger Forschungsressourcen ungenutzt bleiben und mehr Forschende Zugang zu den Ressourcen haben, die sie benötigen.",
                "related_terms": [
                    "Collaboration",
                    "Crowdsourcing",
                    "Team science"
                ],
                "references": "Chartier, C. R., Riegelman, A., & McCarthy, R. J. (2018). StudySwap: A platform for interlab replication, collaboration, and resource exchange. Advances in Methods and Practices in Psychological Science, 1(4), 574–579. https://doi.org/10.1177/2515245918808767",
                "drafted_by": [
                    "Charlotte R. Pennington"
                ],
                "reviewed_by": [
                    "Mahmoud Elsherif",
                    "Helena Hartmann",
                    "Emma Henderson",
                    "Graham Reid"
                ],
                "alt_related_terms": [
                    null
                ],
                "language": "german"
            },
            {
                "type": "glossary",
                "title": "systematisches Review (Systematic Review)",
                "definition": "Eine Form der Literaturübersicht und Evidenzsynthese. Ein systematisches Review umfasst in der Regel eine gründliche, wiederholbare (reproduzierbare) Suchstrategie mit Suchbegriffen und Datenbanken, um relevante Literatur zu einem bestimmten Thema oder einer Forschungsfrage zu finden. Systematische Reviewer:innen führen ein Screening der durch ihre Suche gefundenen Arbeiten durch, bis sie eine Reihe von Arbeiten herausgefiltert haben, die ihren vordefinierten Einschlusskriterien entsprechen. Diese Arbeiten können dann in einer schriftlichen Übersichtsarbeit zusammengefasst werden, die optional auch eine statistische Synthese in Form einer Meta-Analyse enthalten kann. Ein systematisches Review sollte einem standardisierten Satz von Leitlinien folgen, um sicherzustellen, dass Verzerrungen auf ein Minimum reduziert werden, z. B. PRISMA (Moher et al., 2009; Page et al., 2021), Cochrane Systematic Reviews (Higgins et al., 2019\\) oder NIRO-SR (Topor et al., 2021).",
                "related_terms": [
                    "Meta-analysis",
                    "CONSORT",
                    "Non-Intervention, Reproducible, and Open Systematic Reviews (NIRO-SR)",
                    "PRISMA"
                ],
                "references": "Higgins, J. P. T., Thomas, J., Chandler, J., Cumpston, M., Li, T., Page, M. J., & Welch, V. A. (Eds.). (2019). Cochrane Handbook for Systematic Reviews of Interventions. 2nd Edition. John Wiley & Sons.\n\nMoher, D., Liberati, A., Tetzlaff, J., & Altman, D. (2009). Preferred Reporting Items for Systematic Reviews and Meta-Analyses: The PRISMA Statement. PLoS Medicine, 6(7), e1000097. https://doi.org/10.1371/journal.pmed.1000097\n\nPage, M. J., Moher, D., Bossuyt, P. M., Boutron, I., Hoffmann, T. C., Mulrow, C. D., & McKenzie, J. E. (2021). PRISMA 2020 explanation and elaboration: updated guidance and exemplars for reporting systematic reviews. British Medical Journal, 372. https://doi.org/10.1136/bmj.n160\n\nTopor, M., Pickering, J. S., Barbosa Mendes, A., Bishop, D. V. M., Büttner, F. C., Elsherif, M. M., & others. (2021). An integrative framework for planning and conducting Non-Intervention, Reproducible, and Open Systematic Reviews (NIRO-SR). https://doi.org/10.31222/osf.io/8gu5z",
                "drafted_by": [
                    "Jade Pickering"
                ],
                "reviewed_by": [
                    "Mahmoud Elsherif",
                    "Adam Parker",
                    "Charlotte R. Pennington",
                    "Timo Roettger",
                    "Marta Topor",
                    "Emily A. Williams",
                    "Flávio Azevedo    ###"
                ],
                "alt_related_terms": [
                    null
                ],
                "language": "german"
            },
            {
                "type": "glossary",
                "title": "Tenzing",
                "definition": "Tenzing* ist eine Online-Webapp und ein R-Paket, das Forschenden hilft, die Beiträge jedes Teammitglieds unter Verwendung der CRediT-Taxonomie auf effiziente Weise zu verfolgen und zu melden. Die Teammitglieder eines Forschungsprojekts können ihre Beiträge zu jeder CRediT-Rolle mithilfe einer Online-Tabelle angeben und zusätzliche Autor:innenangaben machen (z. B. Name, Institution, Reihenfolge in der Veröffentlichung, E-Mail-Adresse und ORCID iD). Somit kann man mit Tenzing automatisiert eine Liste der Mitwirkenden am Manuskript erstellen, die zu einer CRediT-Rolle beigetragen haben und in die Contributions aufgenommen werden müssen, und auch die Titelseite des Manuskripts erstellen.",
                "related_terms": [
                    "Authorship",
                    "Consortium authorship",
                    "Contributions",
                    "CRediT"
                ],
                "references": "Holcombe, A. O., Kovacs, M., Aust, F., & Aczel, B. (2020). Documenting contributions to scholarly articles using CRediT and tenzing. Plos One, 15(12), e0244611.",
                "drafted_by": [
                    "Marton Kovacs"
                ],
                "reviewed_by": [
                    "Balazs Aczel",
                    "Mahmoud Elsherif",
                    "Helena Hartmann",
                    "Charlotte R. Pennington",
                    "Graham Reid",
                    "Flávio Azevedo"
                ],
                "alt_related_terms": [
                    null
                ],
                "language": "german"
            },
            {
                "type": "glossary",
                "title": "Theorie (Theory)",
                "definition": "Eine Theorie ist eine vereinheitlichende Erklärung oder Beschreibung eines Prozesses oder Phänomens, die wiederholt getestet werden kann und durch wissenschaftliche Untersuchungen überprüfbar ist, wobei verschiedene Experimente von mehreren unabhängigen Forschenden durchgeführt werden. Theorien können verworfen oder als unzureichende Erklärung eines Phänomens angesehen werden, nachdem eine neue Hypothese, die die Phänomene besser erklärt oder ihnen zu widersprechen scheint, aber für eine breitere Palette von Erkenntnissen verallgemeinerbar ist, einer strengen Prüfung unterzogen wurde.",
                "related_terms": [
                    "Hypothesis",
                    "Model (philosophy)",
                    "Theory building"
                ],
                "references": "Schafersman, S. D. (1997). An Introduction to Science. https://www.geo.sunysb.edu/esp/files/scientific-method.html\n\nWacker, J. (1998). A definition of theory: research guidelines for different theory-building research methods in operations management. Journal of Operations Management, 16(4), 361–385. https://doi.org/10.1016/s0272-6963(98)00019-9",
                "drafted_by": [
                    "Aoife O’Mahony"
                ],
                "reviewed_by": [
                    "Mahmoud Elsherif",
                    "Helena Hartmann",
                    "Charlotte R. Pennington",
                    "Graham Reid"
                ],
                "alt_related_terms": [
                    null
                ],
                "language": "german"
            },
            {
                "type": "glossary",
                "title": "Theoriebildung (Theory building)",
                "definition": "Der Prozess der Erstellung und Entwicklung einer Erklärung von Konzepten und deren Zusammenhängen, um zu zeigen, wie und/oder warum ein Phänomen auftritt. Die Theoriebildung ermöglicht dann die Überprüfung der Theorie.",
                "related_terms": [
                    "Hypothesis",
                    "Model (philosophy)",
                    "Theory",
                    "Theoretical contribution",
                    "Theoretical model"
                ],
                "references": "Borsboom, D., van der Maas, H., Dalege, J., Kievit, R., & Haig, B. (2020). Theory Construction Methodology: A practical framework for theory formation in psychology. https://doi.org/10.31234/osf.io/w5tp8\n\nCorley, K. G., & Gioia, D. A. (2011). Building theory about theory building: what constitutes a theoretical contribution? Academy of Management Review, 36(1), 12–32. https://doi.org/10.5465/amr.2009.0486\n\nGioia, D. A., & Pitre, E. (1990). Multiparadigm perspectives on theory building. Academy of Management Review, 15(4), 584–602. https://doi.org/10.5465/amr.1990.4310758\n\nWacker, J. (1998). A definition of theory: research guidelines for different theory-building research methods in operations management. Journal of Operations Management, 16(4), 361–385. https://doi.org/10.1016/s0272-6963(98)00019-9",
                "drafted_by": [
                    "Filip Dechterenko"
                ],
                "reviewed_by": [
                    "Mahmoud Elsherif",
                    "Helena Hartmann",
                    "Charlotte R. Pennington"
                ],
                "alt_related_terms": [
                    null
                ],
                "language": "german"
            },
            {
                "type": "glossary",
                "title": "Die dreisten Drei (The Troubling Trio)",
                "definition": "Beschrieben als eine Kombination aus geringer statistischer Teststärke (Power), einem überraschenden Ergebnis und einem *p*\\-Wert, der nur geringfügig niedriger als .05 ist.",
                "related_terms": [
                    "Replication",
                    "Reproducibility",
                    "Null Hypothesis Significance Testing (NHST)",
                    "*P*\\-hacking",
                    "Questionable Research Practices or Questionable Reporting Practices (QRPs)"
                ],
                "references": "Lindsay, D. S. (2015). Replication in Psychological Science . Psychological Science, 26(12), 1827–1832. https://doi.org/10.1177/0956797615616374",
                "drafted_by": [
                    "Halil Emre Kocalar"
                ],
                "reviewed_by": [
                    "",
                    "Catia M. Oliveira",
                    "Adam Parker",
                    "Sam Parsons;Charlotte R. Pennington"
                ],
                "alt_related_terms": [
                    null
                ],
                "language": "german"
            },
            {
                "type": "glossary",
                "title": "Transparenz (Transparency)",
                "definition": "Transparenz bedeutet, das eigene Handeln offen und zugänglich für eine externe Bewertung zu machen. Transparent zu forschen bedeutet, dass Forschende sämtliche theoretischen, methodischen und analytischen Entscheidungen nachvollziehbar offenlegen, die sie im Laufe des Forschungsprozesses getroffen haben. Transparenz kann in \"wissenschaftlich relevante Transparenz\" und \"gesellschaftlich relevante Transparenz\" unterschieden werden. Während erstere im Mittelpunkt der frühen Open-Science Diskurse stand, ist letztere erforderlich, um wissenschaftliche Informationen in einer Weise bereitzustellen, die für Entscheidungsträger und die Öffentlichkeit relevant ist",
                "related_terms": [
                    "Credibility of scientific claims",
                    "Open science",
                    "Preregistration",
                    "Reproducibility",
                    "Trustworthiness"
                ],
                "references": "Elliott, K. C., & Resnik, D. B. (2019). Making open science work for science and society. Environmental Health Perspectives, 127(7). https://doi.org/10.1289/EHP4808\n\nLyon, L. (2016). Transparency: The Emerging Third Dimension of Open Science and Open Data. LIBER Quarterly, 25(4), 153–171. http://doi.org/10.18352/lq.10113\n\nSyed, M. (2019). The Open Science Movement is for all of us. PsyArXiv.",
                "drafted_by": [
                    "William Ngiam"
                ],
                "reviewed_by": [
                    "Tamara Kalandadze",
                    "Aoife O’Mahony",
                    "Eike Mark Rinke",
                    "Flávio Azevedo"
                ],
                "alt_related_terms": [
                    null
                ],
                "language": "german"
            },
            {
                "type": "glossary",
                "title": "Transparenz-Checkliste (Transparency Checklist)",
                "definition": "Die Transparenz-Checkliste ist eine konsensbasierte, umfassende Checkliste mit 36 Punkten, die die Präregistrierung, Methoden, Ergebnisse und Diskussion sowie die Verfügbarkeit von Daten, Analysecode und Materialien abdeckt. Eine verkürzte Version der Checkliste mit 12 Punkten ist ebenfalls verfügbar. Die Antworten auf die Checkliste können zusammen mit einem Manuskript zur Begutachtung eingereicht werden. Die Checkliste kann zwar auch zu Bildungszwecken eingesetzt werden, zielt aber vor allem darauf ab, Forschende bei der Ermittlung konkreter Maßnahmen zur Erhöhung der Transparenz ihrer Forschung zu unterstützen, während eine offengelegte Checkliste den Lesenden und Gutachter:innen helfen kann, kritische Informationen über verschiedene Aspekte der Transparenz der eingereichten Forschungsarbeiten zu gewinnen.",
                "related_terms": [
                    "Credibility of scientific claims",
                    "Open science",
                    "Preregistration",
                    "Reproducibility",
                    "Trustworthiness"
                ],
                "references": "",
                "drafted_by": [
                    "Barnabas Szaszi"
                ],
                "reviewed_by": [
                    "Sarah Ashcroft-Jones",
                    "Mahmoud Elsherif",
                    "Helena Hartmann",
                    "Graham Reid",
                    "Flávio Azevedo"
                ],
                "alt_related_terms": [
                    null
                ],
                "language": "german"
            },
            {
                "type": "glossary",
                "title": "Dreifach-blindes Peer Review (Triple-blind peer review)",
                "definition": "Bewertung von Forschungsergebnissen durch qualifizierte Expert:innen im Begutachtungsprozess (Peer-Review), wobei die verfassenden Personen sowohl für die Begutachtenden als auch für den/die Herausgebenden (Editor:innen) anonym bleiben. Die Verblindung der verfassenden Personen und ihrer institutionellen Zugehörigkeit sowohl gegenüber den Herausgebenden als auch den Gutachtenden zielt darauf ab, zu verhindern, dass institutionelle, persönliche und geschlechtsspezifische Vorurteile bei der Bewertung des Manuskripts eine Rolle spielen (Tvina et al., 2019, S. 1082).",
                "related_terms": [
                    "Double-blind peer review",
                    "Open Peer Review",
                    "Single-blind peer review"
                ],
                "references": "Largent, E. A., & Snodgrass, R. T. (2016). Blind peer review by academic journals. In C. T. Robertson & A. S. Kesselheim (Eds.), Blinding as a solution to bias: Strengthening biomedical science, forensic science, and law (pp. 75–95). Academic Press. https://doi.org/10.1016/B978-0-12-802460-7.00005-X\n\nTvina, A., Spellecy, R., & Palatnik, A. (2019). Bias in the peer review process: can we do better? Obstetrics & Gynecology, 133(6), 1081–1083. https://doi.org/10.1097/AOG.0000000000003260",
                "drafted_by": [
                    "Mahmoud Elsherif"
                ],
                "reviewed_by": [
                    "Bradley Baker",
                    "Helena Hartmann",
                    "Charlotte R. Pennington",
                    "Christopher Graham"
                ],
                "alt_related_terms": [
                    null
                ],
                "language": "german"
            },
            {
                "type": "glossary",
                "title": "TRUST Prinzipien (TRUST Principles)",
                "definition": "Eine Reihe von Leitprinzipien, die Transparenz, Verantwortung, Nutzer:innenorientierung, Nachhaltigkeit und Technologie (Transparency, Responsibility, User focus, Sustainability, and Technology, TRUST) als die wesentlichen Komponenten für die Bewertung, Entwicklung und Aufrechterhaltung der Vertrauenswürdigkeit digitaler Datenrepositorien (insbesondere solcher, die Forschungsdaten speichern) betrachten. Sie ergänzen die FAIR Daten Prinzipien.",
                "related_terms": [
                    "FAIR principles",
                    "Metadata",
                    "Open Access",
                    "Open Data",
                    "Open Material",
                    "Repository"
                ],
                "references": "Lin, D., Crabtree, J., Dillo, I., Downs, R. R., Edmunds, R., Giaretta, D., De Giusti, M., L’Hours, H., Hugo, W., Jenkyns, R., Khodiyar, V., Martone, M. E., Mokrane, M., Navale, V., Petters, J., Sierman, B., Sokolova, D. V., Stockhause, M., & Westbrook, J. (2020). The TRUST Principles for digital repositories. Scientific Data, 7(1), 144. https://doi.org/10.1038/s41597-020-0486-7",
                "drafted_by": [
                    "Aleksandra Lazić"
                ],
                "reviewed_by": [
                    "Jamie P. Cockcroft",
                    "Mahmoud Elsherif",
                    "Helena Hartmann",
                    "Sam Parsons"
                ],
                "alt_related_terms": [
                    null
                ],
                "language": "german"
            },
            {
                "type": "glossary",
                "title": "Typ-I-Fehler (Type I error)",
                "definition": "Incorrect rejection of a null hypothesis” (dt. fälschliches Verwerfen einer Nullhypothese; Simmons et al., 2011, S. 1359), d.h. das Auffinden von Beweisen, um die Nullhypothese, dass es keinen Effekt gibt, zu verwerfen, wenn die Beweise tatsächlich für die Beibehaltung der Nullhypothese sprechen, dass es keinen Effekt gibt (z. B. ein Richter, der eine unschuldige Person inhaftiert). Die Schlussfolgerung, dass ein signifikanter Effekt vorliegt, und die Zurückweisung der Nullhypothese, wenn die Ergebnisse tatsächlich zufällig entstanden sind.",
                "related_terms": [
                    "Frequentist statistics",
                    "Null Hypothesis Significance Testing (NHST)",
                    "Null Result",
                    "*P* value",
                    "Questionable Research Practices or Questionable Reporting Practices (QRPs)",
                    "Reproducibility crisis (aka Replicability or replication crisis)",
                    "Scientific integrity",
                    "Statistical power",
                    "True positive result",
                    "Type II error"
                ],
                "references": "Simmons, J. P., Nelson, L. D., & Simonsohn, U. (2011). False-positive psychology: Undisclosed flexibility in data collection and analysis allows presenting anything as significant. Psychological Science, 22(11), 1359–1366. https://doi.org/10.1177/0956797611417632",
                "drafted_by": [
                    "Lisa Spitzer"
                ],
                "reviewed_by": [
                    "Mahmoud Elsherif",
                    "Adrien Fillon",
                    "Helena Hartmann",
                    "Matt Jaquiery",
                    "Mariella Paul",
                    "Charlotte R. Pennington",
                    "Graham Reid",
                    "Olly Robertson",
                    "Mirela Zaneva"
                ],
                "alt_related_terms": [
                    null
                ],
                "language": "german"
            },
            {
                "type": "glossary",
                "title": "Typ-II-Fehler (Type II error)",
                "definition": "Ein Fehler 2\\. Art liegt vor, wenn die Alternativhypothese in der Population wahr ist, fälschlicherweise aber die Nullhypothese angenommen wird (Hartgerink et al., 2017). Es wird also ein nicht-signifikantes statistisches Ergebnis gefunden, obwohl es einen wahren Effekt gibt (z. B. ein Arzt, der eine vorliegende Schwangerschaft nicht erkennt). Falsch-negative Ergebnisse sind weniger wahrscheinlich Gegenstand von Replikationen als positive Ergebnisse (Fiedler et al., 2012\\) und bleiben ein ungelöstes Problem in der wissenschaftlichen Forschung (Hartgerink et al., 2017).",
                "related_terms": [
                    "Effect size",
                    "Null Hypothesis Significance Testing (NHST)",
                    "Questionable Research Practices or Questionable Reporting Practices (QRPs)",
                    "Reproducibility crisis (aka Replicability or replication crisis)",
                    "Scientific integrity",
                    "Statistical power",
                    "True positive result",
                    "Type I error"
                ],
                "references": "Fiedler, K., Kutzner, F., & Krueger, J. I. (2012). The long way from α-error control to validity proper: Problems with a short-sighted false-positive debate. Perspectives on Psychological Science, 7(6), 661–669. https://doi.org/10.1177/1745691612462587\n\nHartgerink, C. H., Wicherts, J. M., & Van Assen, M. A. L. M. (2017). Too good to be false: Nonsignificant results revisited. Collabra: Psychology, 3(1). https://doi.org/10.1525/collabra.71",
                "drafted_by": [
                    "Olly Robertson"
                ],
                "reviewed_by": [
                    "Mahmoud Elsherif",
                    "Charlotte R. Pennington"
                ],
                "alt_related_terms": [
                    null
                ],
                "language": "german"
            },
            {
                "type": "glossary",
                "title": "Typ-M-Fehler (Type M error)",
                "definition": "Ein Typ-M-Fehler tritt auf, wenn ein/e Forscher:in zu dem Schluss kommt, dass ein Effekt beobachtet wurde, dessen Ausmaß geringer oder größer ist als der wahre Effekt. Ein Typ-M-Fehler tritt beispielsweise auf, wenn ein/e Forscher:in behauptet, dass ein Effekt von geringer Größe beobachtet wurde, obwohl er in Wahrheit groß ist, oder umgekehrt.",
                "related_terms": [
                    "Statistical power",
                    "Type S error",
                    "Type I error",
                    "Type II error"
                ],
                "references": "Gelman, A., & Carlin, J. (2014). Beyond Power Calculations: Assessing Type S (Sign) and Type M (Magnitude) Errors. Perspectives on Psychological Science, 9(6), 641–651. https://doi.org/10.1177/1745691614551642\n\nLu, J., Qiu, Y., & Deng, A. (2018). A note on Type S/M errors in hypothesis testing. British Journal of Mathematical and Statistical Psychology, 72(1), 1–17. https://doi.org/10.1111/bmsp.12132",
                "drafted_by": [
                    "Eduardo Garcia-Garzon"
                ],
                "reviewed_by": [
                    "Mahmoud Elsherif",
                    "Helena Hartmann",
                    "Graham Reid",
                    "Mirela Zaneva"
                ],
                "alt_related_terms": [
                    null
                ],
                "language": "german"
            },
            {
                "type": "glossary",
                "title": "Typ S Fehler (Type S error)",
                "definition": "Ein Fehler vom Typ S tritt auf, wenn ein:e Forschende:r zu dem Schluss kommt, dass ein Effekt mit einem anderen Vorzeichen als dem tatsächlichen beobachtet wurde. Ein Fehler vom Typ S tritt beispielsweise auf, wenn ein:e Forschende:r behauptet, dass ein positiver Effekt beobachtet wurde, obwohl er in Wirklichkeit negativ ist, oder umgekehrt.",
                "related_terms": [
                    "Statistical power",
                    "Type M error",
                    "Type I error",
                    "Type II error"
                ],
                "references": "Gelman, A., & Carlin, J. (2014). Beyond Power Calculations: Assessing Type S (Sign) and Type M (Magnitude) Errors. Perspectives on Psychological Science, 9(6), 641–651. https://doi.org/10.1177/1745691614551642\n\nLu, J., Qiu, Y., & Deng, A. (2018). A note on Type S/M errors in hypothesis testing. British Journal of Mathematical and Statistical Psychology, 72(1), 1–17. https://doi.org/10.1111/bmsp.12132",
                "drafted_by": [
                    "Eduardo Garcia-Garzon"
                ],
                "reviewed_by": [
                    "Helena Hartmann",
                    "Sam Parsons",
                    "Graham Reid",
                    "Mirela Zaneva"
                ],
                "alt_related_terms": [
                    null
                ],
                "language": "german"
            },
            {
                "type": "glossary",
                "title": "Unterrepräsentation (Under-representation)",
                "definition": "Nicht alle Stimmen, Perspektiven und Mitglieder der Gemeinschaft sind angemessen vertreten. Unterrepräsentation tritt typischerweise dann auf, wenn die Stimmen oder Perspektiven einer Gruppe dominieren, was zur Marginalisierung einer anderen führt. Dies betrifft häufig Gruppen, die in Bezug auf bestimmte persönliche Merkmale in der Minderheit sind.",
                "related_terms": [
                    "Equity",
                    "Fairness",
                    "Inequality",
                    "WEIRD"
                ],
                "references": "",
                "drafted_by": [
                    "Madeleine Pownall"
                ],
                "reviewed_by": [
                    "Mahmoud Elsherif",
                    "Helena Hartmann",
                    "Bethan Iley",
                    "Adam Parker",
                    "Charlotte R. Pennington, Mirela Zaneva"
                ],
                "alt_related_terms": [
                    null
                ],
                "language": "german"
            },
            {
                "type": "glossary",
                "title": "UDL (Universal design for learning (UDL))",
                "definition": "Ein Konzept zur Verbesserung des Lernens und zur Optimierung des Unterrichtens, das auf wissenschaftlichen Erkenntnissen darüber beruht, wie Menschen lernen. Es wird häufig als evidenzbasierter und wissenschaftlich fundierter Rahmen für die pädagogische Praxis angesehen, der aus drei Schlüsselprinzipien besteht: Engagement, Repräsentation sowie Aktion und Ausdruck. Darüber hinaus ist UDL in den Higher Education Opportunity Act von 2008 aufgenommen worden (Edyburn, 2010).",
                "related_terms": [
                    "Equal opportunities",
                    "Inclusivity",
                    "Pedagogy",
                    "Teaching practice"
                ],
                "references": "Hitchcock, C., Meyer, A., Rose, D., & Jackson, R. (2002). Providing new access to the general curriculum: Universal design for learning. Teaching Exceptional Children, 35(2), 8–17. https://www.proquest.com/scholarly-journals/providing-new-access-general-curriculum/docview/201139970/se-2?accountid=8630\n\nRose, D. (2000). Universal design for learning. Journal of Special Education Technology, 15(3), 45–49. https://doi.org/10.1177/016264340001500307\n\nRose, D. H., & Meyer, A. (2002). Teaching every student in the digital age: Universal design for learning. In The Corsini Encyclopedia of Psychology. Association for Supervision.",
                "drafted_by": [
                    "Charlotte R. Pennington"
                ],
                "reviewed_by": [
                    "Valeria Agostini",
                    "Mahmoud Elsherif",
                    "Graham Reid",
                    "Mirela Zaneva",
                    "Flávio Azevedo"
                ],
                "alt_related_terms": [
                    null
                ],
                "language": "german"
            },
            {
                "type": "glossary",
                "title": "Validität (Validity)",
                "definition": "Validität bezieht sich auf die Anwendung statistischer Prinzipien, um zu fundierten \\- d. h. wahrscheinlich genau der realen Welt entsprechenden \\- Konzepten, Schlussfolgerungen oder Messungen zu gelangen. In der Psychometrie bezieht sich die Validität auf das Ausmaß, in dem etwas das misst, was es zu messen beabsichtigt oder vorgibt zu messen. Unter diesem Oberbegriff gibt es verschiedene Arten von Validität (z. B. interne Validität, Konstruktvalidität, Augenscheinvalidität, Kriteriumsvalidität, diagnostische Validität, diskriminante Validität, übereinstimmende Validität, konvergente Validität, prädiktive Validität, externe Validität).",
                "related_terms": [
                    "Causality",
                    "Construct validity",
                    "Content validity",
                    "Criterion validity",
                    "External validity",
                    "Face validity",
                    "Internal validity",
                    "Measurement",
                    "Questionable Measurement Practices (QMP)",
                    "Psychometry",
                    "Reliability",
                    "Statistical power",
                    "Statistical validity",
                    "Test"
                ],
                "references": "Campbell, D. T. (1957). Factors relevant to the validity of experiments in social settings. Psychological Bulletin, 54(4), 297–312. https://doi.org/10.1037/h0040950\n\nKelley, T. L. (1927). Interpretation of educational measurements. Macmillan.",
                "drafted_by": [
                    "Tamara Kalandadze; Madeleine Pownall; Flávio Azevedo"
                ],
                "reviewed_by": [
                    "Eduardo Garcia-Garzon",
                    "Halil E. Kocalar",
                    "Annalise A. LaPlume",
                    "Joanne McCuaig",
                    "Adam Parker",
                    "Charlotte R. Pennington"
                ],
                "alt_related_terms": [
                    null
                ],
                "language": "german"
            },
            {
                "type": "glossary",
                "title": "Versionskontrolle (Version control)",
                "definition": "Die Praxis der Verwaltung und Aufzeichnung von Änderungen an digitalen Ressourcen (z. B. Dateien, Websites, Programme usw.) im Laufe der Zeit, sodass Sie bestimmte Versionen später wieder aufrufen können. Versionskontrollsysteme sind so konzipiert, dass sie den Verlauf der Änderungen (wer, was und wann) aufzeichnen und dazu beitragen, menschliche Fehler (z. B. die Arbeit an der falschen Version) zu vermeiden. Das Versionskontrollsystem Git beispielsweise ist ein weit verbreitetes Softwaretool, das ursprünglich Softwareentwickelnden bei der Versionskontrolle von gemeinsamem Code half und heute in vielen wissenschaftlichen Disziplinen zur Verwaltung und gemeinsamen Nutzung von Dateien eingesetzt wird.",
                "related_terms": [
                    "Git",
                    "Reproducibility",
                    "Software configuration management",
                    "Source code management",
                    "Source control"
                ],
                "references": "Git. (n.d.). Git—About Version Control. https://git-scm.com/book/en/v2/Getting-Started-About-Version-Control",
                "drafted_by": [
                    "Mahmoud Elsherif"
                ],
                "reviewed_by": [
                    "Sarah Ashcroft-Jones",
                    "Thomas Rhys Evans",
                    "Helena Hartmann",
                    "Matt Jaquiery",
                    "Adam Parker",
                    "Charlotte R. Pennington",
                    "Robert M. Ross",
                    "Timo Roettger",
                    "Andrew J. Stewart"
                ],
                "alt_related_terms": [
                    null
                ],
                "language": "german"
            },
            {
                "type": "glossary",
                "title": "Webometrie (Webometrics)",
                "definition": "Die Webometrie befasst sich mit der Untersuchung von Online-Inhalten. Die Webometrie konzentriert sich auf die Anzahl und Art der Hyperlinks zwischen verschiedenen Online-Seiten. Solche Ansätze werden als eine Art von Altmetrik betrachtet. \"The study of the quantitative aspects of the construction and use of information resources, structures and technologies on the Web drawing on [bibliometric](https://en.wikipedia.org/wiki/Bibliometrics) and [informetric](https://en.wikipedia.org/wiki/Informetrics) approaches” (dt. Die Untersuchung der quantitativen Aspekte des Aufbaus und der Nutzung von Informationsressourcen, \\-strukturen und \\-technologien im Web auf der Grundlage bibliometrischer und informatischer Ansätze; Björneborn & Ingwersen, 2004).",
                "related_terms": [
                    "Altmetrics",
                    "Bibliometrics"
                ],
                "references": "Bjørneborn, L., & Ingwersen, P. (2004). Toward a basic framework for webometrics. Journal of the American Society for Information Science and Technology, 55(14), 1216–1227. https://doi.org/10.1002/asi.20077",
                "drafted_by": [
                    "Charlotte R. Pennington"
                ],
                "reviewed_by": [
                    "Christopher Graham",
                    "Mirela Zaneva"
                ],
                "alt_related_terms": [
                    null
                ],
                "language": "german"
            },
            {
                "type": "glossary",
                "title": "WEIRD",
                "definition": "Dieses Akronym steht für westliche, gebildete, industrialisierte, reiche und demokratische (Western, Educated, Industrialized, Rich and Democratic) Gesellschaften. Die meisten Forschungsarbeiten werden an relativ homogenen Stichproben aus WEIRD-Gesellschaften durchgeführt und von diesen durchgeführt. Dies schränkt die Verallgemeinerbarkeit zahlreicher Forschungsergebnisse ein, zumal WEIRD-Personen oft psychologische Ausreißer sind. Es wurde argumentiert, dass sich die \"WEIRD Psychologie\" kulturell als Ergebnis gesellschaftlicher Veränderungen und religiöser Überzeugungen im Mittelalter in Europa zu entwickeln begann. Kritiker\\*innen dieses Begriffs sind der Meinung, dass er ein binäres Bild der Weltbevölkerung zeichnet und die Unterschiede zwischen und innerhalb von Gesellschaften ausblendet und dass andere Aspekte der Vielfalt nicht erfasst werden.",
                "related_terms": [
                    "**Alternative definition:** (if applicable) **Related terms to alternative definition:** (if applicable)"
                ],
                "references": "",
                "drafted_by": [],
                "reviewed_by": [],
                "alt_related_terms": [
                    null
                ],
                "language": "german"
            },
            {
                "type": "glossary",
                "title": "Z-Kurve (Z-Curve)",
                "definition": "Die Berechnung einer z-Kurve ist ein statistischer Ansatz, der hauptsächlich verwendet wird, um die \"geschätzte Replikationsrate\" (Estimated Replication Rate, ERR) und die \"erwartete Entdeckungsrate\" (Expected Discovery Rate, EDR) für eine Reihe von berichteten Studien zu ermitteln. Die Berechnung einer *z*\\-Kurve für eine Sammlung statistisch signifikanter Studien beinhaltet die Umwandlung der berichteten *p*\\-Werte in *z*\\-Werte, die Anpassung eines Finite Mixture Modell an die Verteilung der *z*\\-Werte und die Schätzung der mittleren Teststärke (Power) auf der Grundlage des Modells. Die *z*\\-Kurven-Analyse kann in R mit einem speziellen Paket durchgeführt werden \\- [https://cran.r-project.org/web/packages/zcurve/index.html](https://cran.r-project.org/web/packages/zcurve/index.html).",
                "related_terms": [
                    "Altmetrics",
                    "File drawer ratio",
                    "P-curve",
                    "P-hacking",
                    "Replication",
                    "Statistical power"
                ],
                "references": "Bartoš, F., & Schimmack, U. (2020). Z-Curve 2.0: Estimating replication rates and discovery rates. https://doi.org/10.31234/osf.io/urgtn\n\nBrunner, J., & Schimmack, U. (2020). Estimating population mean power under conditions of heterogeneity and selection for significance. Meta-Psychology, 4, MP.2018.874. https://doi.org/10.15626/MP.2018.874",
                "drafted_by": [
                    "Bradley J. Baker"
                ],
                "reviewed_by": [
                    "Kamil Izydorczak",
                    "Sam Parsons",
                    "Charlotte R. Pennington",
                    "Mirela Zaneva"
                ],
                "alt_related_terms": [
                    null
                ],
                "language": "german"
            },
            {
                "type": "glossary",
                "title": "Zenodo",
                "definition": "Ein offenes wissenschaftliches Repositorium, in dem Forschende Forschungsarbeiten, Berichte, Datensätze, Forschungssoftware und jedwede andere forschungsbezogene digitale Artefakte hinterlegen können. Zenodo erstellt eine dauerhafte digitale Objektkennung (DOI) für jeden Beitrag, um ihn zitierfähig zu machen. Diese Plattform wurde im Rahmen des europäischen Programms OpenAIRE entwickelt und wird vom CERN betrieben.",
                "related_terms": [
                    "DOI (digital object identifier)",
                    "figshare",
                    "Open data",
                    "Open Science Framework",
                    "Preprint"
                ],
                "references": "Zenodo. (n.d.). Zenodo—Research. Shared. https://www.zenodo.org/",
                "drafted_by": [
                    "Ali H. Al-Hoorie"
                ],
                "reviewed_by": [],
                "alt_related_terms": [
                    null
                ],
                "language": "german"
            }
        ]
    },
    {
        "english": [
            {
                "type": "glossary",
                "title": "Abstract Bias",
                "definition": "The tendency to report only significant results in the abstract, while reporting non-significant results within the main body of the manuscript (not reporting non-significant results altogether would constitute selective reporting). The consequence of abstract bias is that studies reporting non-significant results may not be captured with standard meta-analytic search procedures (which rely on information in the title, abstract and keywords) and thus biasing the results of meta-analyses.",
                "related_terms": [
                    "Cherry-picking",
                    "Publication bias (File Drawer Problem)",
                    "Selective reporting"
                ],
                "references": "Duyx, B., Swaen, G. M., Urlings, M. J., Bouter, L. M., & Zeegers, M. P. (2019). The strong focus on positive results in abstracts may cause bias in systematic reviews: A case study on abstract reporting bias. Systematic Reviews, 8(1), 1–8.",
                "drafted_by": [
                    "Ali H. Al-Hoorie"
                ],
                "reviewed_by": [
                    "Mahmoud Elsherif",
                    "Bethan Iley",
                    "Sam Parsons",
                    "Gerald Vineyard",
                    "Eliza Woodward",
                    "Flávio Azevedo"
                ],
                "alt_related_terms": [
                    null
                ],
                "language": "english",
                "aliases": [
                    "/glossary/abstract_bias"
                ]
            },
            {
                "type": "glossary",
                "title": "Academic Impact",
                "definition": "The contribution that a research output (e.g., published manuscript) makes in shifting understanding and advancing scientific theory, method, and application, across and within disciplines. Impact can also refer to the degree to which an output or research programme influences change outside of academia, e.g. societal and economic impact (cf. ESRC: https://esrc.ukri.org/research/impact-toolkit/what-is-impact/).",
                "related_terms": [
                    "Beneficiaries",
                    "DORA",
                    "Reach",
                    "REF"
                ],
                "references": "Anon. (2021). What is impact?. Retrieved from https://esrc.ukri.org/research/impact-toolkit/what-is-impact/",
                "drafted_by": [
                    "Connor Keating"
                ],
                "reviewed_by": [
                    "Myriam A. Baum",
                    "Adam Parker",
                    "Charlotte R. Pennington",
                    "Suzanne L. K. Stewart",
                    "Flávio Azevedo"
                ],
                "alt_related_terms": [
                    null
                ],
                "language": "english",
                "aliases": [
                    "/glossary/academic_impact"
                ]
            },
            {
                "type": "glossary",
                "title": "Accessibility",
                "definition": "Accessibility refers to the ease of access and re-use of materials (e.g., data, code, outputs, publications) for academic purposes, particularly the ease of access is afforded to people with a chronic illness, disability and/or neurodivergence. These groups face numerous financial, legal and/or technical barriers within research, including (but not limited to) the acquisition of appropriately formatted materials and physical access to spaces. Accessibility also encompasses structural concerns about diversity, equity, inclusion, and representation (Pownall et al., 2021). Interfaces, events and spaces should be designed with accessibility in mind to ensure full participation, such as by ensuring that web-based images are colorblind friendly and have alternative text, or by using live captions at events (Brown et al., 2018; Pollet & Bond, 2021; World Wide Web Consortium, 2021).",
                "related_terms": [
                    "Availability",
                    "Data availability statements",
                    "Inclusion",
                    "Open Access",
                    "Under-representation",
                    "Universal design for learning (UDL)"
                ],
                "references": "Brown, N., Thompson, P., & Leigh, J. S. (2018). Making academia more accessible. Journal of Perspectives in Applied Academic Practice, 6(2), 82–90. https://doi.org/10.14297/jpaap.v6i2.348\n\nPollet, I. L., & Bond, A. L. (2021). Evaluation and recommendations for greater accessibility of colour figures in ornithology. Ibis, 163, 292–295. https://doi.org/10.1111/ibi.12887\n\nSuber, P. (2004). The primacy of authors in achieving Open Access. In Nature. Retrieved from http://dash.harvard.edu/handle/1/4391161)\n\nPownall, M., Talbot, C. V., Henschel, A., Lautarescu, A., Lloyd, K. E., Hartmann, H., Darda, K. M., Tang, K. T. Y., Carmichael-Murphy, P., & Siegel, J. A. (2021). Navigating Open Science as Early Career Feminist Researchers. Psychology of Women Quarterly, 45(4), 526–539. https://doi.org/10.1177/03616843211029255 Retrieved from https://journals.sagepub.com/doi/10.1177/03616843211029255",
                "drafted_by": [
                    "Kai Krautter"
                ],
                "reviewed_by": [
                    "Valeria Agostini",
                    "Myriam A. Baum",
                    "Mahmoud Elsherif",
                    "Bethan Iley",
                    "Tamara Kalandadze",
                    "Ryan Millager",
                    "Sara Middleton",
                    "Charlotte R. Pennington",
                    "Madeleine Pownall",
                    "Robert M. Ross",
                    "Flávio Azevedo"
                ],
                "alt_related_terms": [
                    null
                ],
                "language": "english",
                "aliases": [
                    "/glossary/accessibility"
                ]
            },
            {
                "type": "glossary",
                "title": "Ad hominem bias",
                "definition": "From Latin meaning “to the person”; Judgment of an argument or piece of work influenced by the characteristics of the person who forwarded it, not the characteristics of the argument itself. Ad hominem bias can be negative, as when work from a competitor or target of personal animosity is viewed more critically than the quality of the work merits, or positive, as when work from a friend benefits from overly favorable evaluation.",
                "related_terms": [
                    "Peer review"
                ],
                "references": "Barnes, R. M., Johnston, H. M., MacKenzie, N., Tobin, S. J., & Taglang, C. M. (2018). The effect of ad hominem attacks on the evaluation of claims promoted by scientists. PloS One, 13(1), e0192025. https://doi.org/10.1371/journal.pone.0192025\n\nTvina, A., Spellecy, R., & Palatnik, A. (2019). Bias in the peer review process: can we do better? Obstetrics & Gynecology, 133(6), 1081–1083. https://doi.org/10.1097/AOG.0000000000003260",
                "drafted_by": [
                    "Mahmoud Elsherif"
                ],
                "reviewed_by": [
                    "Bradley Baker",
                    "Filip Dechterenko",
                    "Bethan Iley",
                    "Madeleine Ingham",
                    "Graham Reid"
                ],
                "alt_related_terms": [
                    null
                ],
                "language": "english",
                "aliases": [
                    "/glossary/ad_hominem_bias"
                ]
            },
            {
                "type": "glossary",
                "title": "Adversarial collaboration",
                "definition": "A collaboration where two or more researchers with opposing or contradictory theoretical views —and likely diverging predictions about study results— work together on one project. The aim is to minimise biases and methodological weaknesses as well as to establish a shared base of facts for which competing theories must account.",
                "related_terms": [
                    "Collaboration",
                    "Many Analysts",
                    "Many Labs",
                    "Preregistration",
                    "Publication bias (File Drawer Problem)"
                ],
                "references": "Bateman, I., Kahneman, D., Munro, A., Starmer, C., & Sugden, R. (2005). Testing competing models of loss aversion: An adversarial collaboration. Journal of Public Economics, 89(8), 1561–1580. https://doi.org/10.1016/j.jpubeco.2004.06.013\n\nCowan, N., Belletier, C., Doherty, J. M., Jaroslawska, A. J., Rhodes, S., Forsberg, A., & Logie, R. H. (2020). How do scientific views change? Notes from an extended adversarial collaboration. Perspectives on Psychological Science, 15(4), 1011–1025. https://doi.org/10.1177/1745691620906415\n\nKerr, N. L., Ao, X., Hogg, M. A., & Zhang, J. (2018). Addressing replicability concerns via adversarial collaboration: Discovering hidden moderators of the minimal intergroup discrimination effect. Journal of Experimental Social Psychology, 78, 66–76. https://doi.org/10.1016/j.jesp.2018.05.001\n\nMellers, B., Hertwig, R., & Kahneman, D. (2001). Do frequency representations eliminate conjunction effects? An exercise in adversarial collaboration. Psychological Science, 12(4), 269–275. https://doi.org/10.1111/1467-9280.00350\n\nRakow, T., Thompson, V., Ball, L., & Markovits, H. (2014). Rationale and guidelines for empirical adversarial collaboration: A Thinking & Reasoning initiative. Thinking & Reasoning, 21(2), 167–175. https://doi.org/10.1080/13546783.2015.975405",
                "drafted_by": [
                    "Siu Kit Yeung"
                ],
                "reviewed_by": [
                    "Matt Jaquiery",
                    "Aoife O’Mahony",
                    "Charlotte R. Pennington",
                    "Flávio Azevedo",
                    "Madeleine Pownall**;** Martin Vasilev"
                ],
                "alt_related_terms": [
                    null
                ],
                "language": "english",
                "aliases": [
                    "/glossary/adversarial_collaboration"
                ]
            },
            {
                "type": "glossary",
                "title": "Adversarial (collaborative) commentary",
                "definition": "A commentary in which the original authors of a work and critics of said work collaborate to draft a consensus statement. The aim is to draft a commentary that is free of ad hominem attacks and communicates a common understanding or at least identifies where both parties agree and disagree. In doing so, it provides a clear take-home message and path forward, rather than leaving the reader to decide between opposing views conveyed in separate commentaries.",
                "related_terms": [
                    "Adversarial collaboration",
                    "Collaborative commentary"
                ],
                "references": "Heyman, T., Moors, P., & Rabagliati, H. (2020). The benefits of adversarial collaboration for commentaries. Nature Human Behavior, 4, 1217. https://doi.org/10.1038/s41562-020-00978-6\n\nRabagliati, H., Moors, P., & Heyman, T. (2019). Can item effects explain away the evidence for unconscious sound symbolism? An adversarial commentary on Heyman, Maerten, Vankrunkelsven, Voorspoels, and Moors (2019). Psychological Science, 31(9), 1200–1204. https://doi.org/10.1177/0956797620949461\n\nSilberzahn, R., Simonsohn, U., & Ulhmann, E. L. (2014). Matched-names analysis reveals no evidence of name-meaning effects: A collaborative commentary on Silberzahn and Uhlmann (2013). Psychological Science, 25(7), 1504–1505. https://doi.org/10.1177/0956797614533802",
                "drafted_by": [
                    "Steven Verheyen"
                ],
                "reviewed_by": [
                    "Mahmoud Elsherif",
                    "Sam Parsons",
                    "Emma Henderson",
                    "Michele C. Lim",
                    "Flávio Azevedo"
                ],
                "alt_related_terms": [
                    null
                ],
                "language": "english",
                "aliases": [
                    "/glossary/adversarial"
                ]
            },
            {
                "type": "glossary",
                "title": "Affiliation bias",
                "definition": "This bias occurs when one’s opinions or judgements about the quality of research are influenced by the affiliation of the author(s). When publishing manuscripts, a potential example of an affiliation bias could be when editors prefer to publish work from prestigious institutions (Tvina et al., 2019).",
                "related_terms": [
                    "Peer review"
                ],
                "references": "Tvina, A., Spellecy, R., & Palatnik, A. (2019). Bias in the peer review process: can we do better? Obstetrics & Gynecology, 133(6), 1081–1083. https://doi.org/10.1097/AOG.0000000000003260",
                "drafted_by": [
                    "Mahmoud Elsherif"
                ],
                "reviewed_by": [
                    "Christopher Graham",
                    "Madeleine Ingham",
                    "Adam Parker",
                    "Graham Reid"
                ],
                "alt_related_terms": [
                    null
                ],
                "language": "english",
                "aliases": [
                    "/glossary/affiliation_bias"
                ]
            },
            {
                "type": "glossary",
                "title": "Aleatoric uncertainty",
                "definition": "Variability in outcomes due to unknowable or inherently random factors. The stochastic component of outcome uncertainty that cannot be reduced through additional sources of information. For example, when flipping a coin, uncertainty about whether it will land on heads or tails.",
                "related_terms": [
                    "Epistemic uncertainty",
                    "Knightian uncertainty"
                ],
                "references": "Der Kiureghian, A., & Ditlevsen, O. (2009). Aleatory or epistemic? Does it matter? Structural Safety, 31(2), 105–112. https://doi.org/10.1016/j.strusafe.2008.06.020",
                "drafted_by": [
                    "Bradley Baker"
                ],
                "reviewed_by": [
                    "Nihan Albayrak-Aydemir**;** Brett Gall",
                    "Magdalena Grose-Hodge",
                    "Bethan Iley",
                    "Charlotte R. Pennington"
                ],
                "alt_related_terms": [
                    null
                ],
                "language": "english",
                "aliases": [
                    "/glossary/aleatoric_uncertainty"
                ]
            },
            {
                "type": "glossary",
                "title": "Altmetrics",
                "definition": "Departing from traditional citation measures, altmetrics (short for “alternative metrics”) provide an assessment of the attention and broader impact of research work based on diverse sources such as social media (e.g. Twitter), digital news media, number of preprint downloads, etc. Altmetrics have been criticized in that sensational claims usually receive more attention than serious research (Ali, 2021).",
                "related_terms": [
                    "Academic impact",
                    "Alternative metrics",
                    "Bibliometrics",
                    "H-index",
                    "Impact assessment",
                    "Journal impact factor"
                ],
                "references": "Ali, M. J. (2021). Understanding the Altmetrics. Seminars in Ophthalmology. https://doi.org/10.1080/08820538.2021.1930806\n\nGalligan, F., & Dyas-Correia, S. (2013). Altmetrics: rethinking the way we measure. Serials Review, 39(1), 56–61. https://doi.org/10.1016/j.serrev.2013.01.003",
                "drafted_by": [
                    "Mirela Zaneva"
                ],
                "reviewed_by": [
                    "Ali H. Al-Hoorie",
                    "Charlotte R. Pennington",
                    "Birgit Schmidt",
                    "Flávio Azevedo"
                ],
                "alt_related_terms": [
                    null
                ],
                "language": "english",
                "aliases": [
                    "/glossary/altmetrics"
                ]
            },
            {
                "type": "glossary",
                "title": "AMNESIA",
                "definition": "AMNESIA is a free anonymization tool to remove identifying information from data. After uploading a dataset that contains personal data, the original dataset is transformed by the tool, resulting in a dataset that is anonymized regarding personal and sensitive data.",
                "related_terms": [
                    "Anonymity",
                    "Confidentiality",
                    "Research ethics"
                ],
                "references": "",
                "drafted_by": [
                    "Norbert Vanek"
                ],
                "reviewed_by": [
                    "Ali H. Al-Hoorie",
                    "Myriam A. Baum",
                    "Charlotte R. Pennington"
                ],
                "alt_related_terms": [
                    null
                ],
                "language": "english",
                "aliases": [
                    "/glossary/amnesia"
                ]
            },
            {
                "type": "glossary",
                "title": "Analytic Flexibility",
                "definition": "Analytic flexibility is a type of researcher degrees of freedom (Simmons, Nelson, & Simonsohn, 2011\\) that refers specifically to the large number of choices made during data preprocessing and statistical analysis. “\\[T\\]he range of analysis outcomes across different acceptable analysis methods” (Carp, 2012, p. 1). Analytic flexibility can be problematic, as this variability in analytic strategies can translate into variability in research outcomes, particularly when several strategies are applied, but not transparently reported (Masur, 2021).",
                "related_terms": [
                    "Garden of forking paths",
                    "Multiverse analysis",
                    "Researcher degrees of freedom"
                ],
                "references": "Breznau, N. (2021). I saw you in the crowd: Credibility, reproducibility, and meta-utility. PS: Political Science & Politics, 54(2), 309–313. https://doi.org/10.1017/S1049096520000980\n\nJones, A., Dr, J., Duckworth, & Christiansen, P. (2020). May I have your attention, please? Methodological and Analytical Flexibility in the Addiction Stroop. https://doi.org/10.31234/osf.io/ws8xp\n\nMasur, P. K. (2020). Understanding the Effects of Analytical Choices on Finding the Privacy Paradox: A Specification Curve Analysis of Large-Scale Survey Data. Preprint. https://osf.io/m72gb/\n\nSimmons, J. P., Nelson, L. D., & Simonsohn, U. (2011). False-positive psychology: Undisclosed flexibility in data collection and analysis allows presenting anything as significant. Psychological Science, 22(11), 1359–1366. https://doi.org/10.1177/0956797611417632",
                "drafted_by": [
                    "Mariella Paul"
                ],
                "reviewed_by": [
                    "Adrien Fillon",
                    "Bettina M. J . Kern",
                    "Adam Parker",
                    "Charlotte R. Pennington",
                    "Flávio Azevedo"
                ],
                "alt_related_terms": [
                    null
                ],
                "language": "english",
                "aliases": [
                    "/glossary/analytic_flexibility"
                ]
            },
            {
                "type": "glossary",
                "title": "Anonymity",
                "definition": "Anonymising data refers to removing, generalising, aggregating or distorting any information which may potentially identify participants, peer-reviewers, and authors, among others. Data should be anonymised so that participants are not personally identifiable. The most basic level of anonymisation is to replace participants’ names with pseudonyms (fake names) and remove references to specific places. Anonymity is particularly important for open data and data may not be made open for anonymity concerns. Anonymity and open data has been discussed within qualitative research which often focuses on personal experiences and opinions, and in quantitative research that includes participants from clinical populations.",
                "related_terms": [
                    "Anonymising",
                    "Clinical populations",
                    "Confidentiality",
                    "Research ethics",
                    "Research participants",
                    "Vulnerable population"
                ],
                "references": "Braun, V., & Clarke, V. (2013). Successful Qualitative Research. SAGE Publications.",
                "drafted_by": [
                    "Claire Melia"
                ],
                "reviewed_by": [
                    "Tsvetomira Dumbalska",
                    "Bethan Iley",
                    "Tamara Kalandadze",
                    "Bettina M.J. Kern",
                    "Sam Parsons",
                    "Charlotte R. Pennington",
                    "Flávio Azevedo",
                    "Madeleine Pownall",
                    "Birgit Schmidt"
                ],
                "alt_related_terms": [
                    null
                ],
                "language": "english",
                "aliases": [
                    "/glossary/anonymity"
                ]
            },
            {
                "type": "glossary",
                "title": "ARRIVE Guidelines",
                "definition": "The ARRIVE guidelines (Animal Research: Reporting of In Vivo Experiments) are a checklist-based set of reporting guidelines developed to improve reporting standards, and enhance replicability, within living (i.e. *in vivo*) animal research. The second generation ARRIVE guidelines, ARRIVE 2.0, were released in 2020\\. In these new guidelines, the clarity has been improved, items have been prioritised and new information has been added with an accompanying “Explanation” and “Elaboration” document to provide a rationale for each item and a recommended set to add context to the study being described.",
                "related_terms": [
                    "PREPARE Guidelines",
                    "Reporting Guideline",
                    "STRANGE"
                ],
                "references": "",
                "drafted_by": [
                    "Ben Farrar"
                ],
                "reviewed_by": [
                    "Jamie P. Cockcroft",
                    "Mahmoud Elsherif",
                    "Elias Garcia-Pelegrin",
                    "Helena Hartmann",
                    "Wanyin Li",
                    "Charlotte R. Pennington"
                ],
                "alt_related_terms": [
                    null
                ],
                "language": "english",
                "aliases": [
                    "/glossary/arrive_guidelines"
                ]
            },
            {
                "type": "glossary",
                "title": "Article Processing Charge (APC)",
                "definition": "An article (sometimes author) processing charge (APC) is a fee charged to authors by a publisher in exchange for publishing and hosting an open access article. APCs are often intended to compensate for a potential loss of revenue the journal may experience when moving from traditional publication models, such as subscription services or pay-per-view, to open access. While some journals charge only about US$300, APCs vary widely, from US$1000 (Advances in Methods and Practice in Psychological Science) or less to over US$10,000 (Nature). While some publishers offer waivers for researchers from certain regions of the world or who lack funds, some APCs have been criticized for being disproportionate compared to actual processing and hosting costs (Grossmann & Brembs, 2021\\) and for creating possible inequities with regard to which scientists can afford to make their works freely available (Smith et al. 2020).",
                "related_terms": [
                    "Open Access",
                    "Under-representation"
                ],
                "references": "Grossmann, A., & Brembs, B. (2021). Current market rates for scholarly publishing services. F1000Research, 10(20), 20. https://doi.org/10.12688/f1000research.27468.1\n\nSmith, A. C., Merz, L., Borden, J. B., Gulick, C., Kshirsagar, A. R., & Bruna, E. M. (2020). Assessing the effect of article processing charges on the geographic diversity of authors using Elsevier’s ‘Mirror Journal’ system. https://doi.org/10.31222/osf.io/s7cx4",
                "drafted_by": [
                    "Nick Ballou"
                ],
                "reviewed_by": [
                    "Ali H. Al-Hoorie",
                    "Bethan Iley",
                    "Flávio Azevedo",
                    "Robert Ross",
                    "Tobias Wingen \\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_"
                ],
                "alt_related_terms": [
                    null
                ],
                "language": "english",
                "aliases": [
                    "/glossary/article_processing_charge"
                ]
            },
            {
                "type": "glossary",
                "title": "Authorship",
                "definition": "Authorship assigns credit for research outputs (e.g. manuscripts, data, and software) and accountability for content (McNutt et al. 2018; Patience et al. 2019). Conventions differ across disciplines, cultures, and even research groups, in their expectations of what efforts earn authorship, what the order of authorship signifies (if anything), how much accountability for the research the corresponding author assumes, and the extent to which authors are accountable for aspects of the work that they did not personally conduct.",
                "related_terms": [
                    "Co-authorship",
                    "Consortium authorship",
                    "Contributorship",
                    "CRediT",
                    "First-last-author-emphasis norm (FLAE)",
                    "Gift (or Guest) Authorship",
                    "Sequence-determines-credit approach (SDC)"
                ],
                "references": "Academies, A. A. E. (2017). The European Code of Conduct for Research Integrity. Revised Edition. Retrieved from https://allea.org/code-of-conduct/\n\nGerman Research Foundation. (2019). Guidelines for Safeguarding Good Research Practice. Code of Conduct.\n\nMcNutt, M. K., Bradford, M., Drazen, J. M., Hanson, B., Howard, B., Jamieson, K. H., Kiermer, V., Marcus, E., Pope, B. K., Schekman, R., Swaminathan, S., Stang, P. J., & Verma, I. M. (2018). Transparency in authors’ contributions and responsibilities to promote integrity in scientific publication. Proceedings of the National Academy of Sciences of the United States of America, 115(11), 2557–2560. https://doi.org/10.1073/pnas.1715374115\n\nPatience, G. S., Galli, F., Patience, P. A., & Boffito, D. C. (2019). Intellectual contributions meriting authorship: Survey results from the top cited authors across all science categories. PLoS One, 14(1), e0198117. https://doi.org/10.1371/journal.pone.0198117",
                "drafted_by": [
                    "Jacob Miranda"
                ],
                "reviewed_by": [
                    "Bradley Baker",
                    "Brett J. Gall",
                    "Matt Jaquiery",
                    "Charlotte R. Pennington",
                    "Flávio Azevedo",
                    "Birgit Schmidt",
                    "Yuki Yamada"
                ],
                "alt_related_terms": [
                    null
                ],
                "language": "english",
                "aliases": [
                    "/glossary/authorship"
                ]
            },
            {
                "type": "glossary",
                "title": "Auxiliary Hypothesis",
                "definition": "All theories contain assumptions about the nature of constructs and how they can be measured. However, not all predictions are derived from theories and assumptions can sometimes be drawn from other premises. Additional assumptions that are made to deduce a prediction and tested by making links to observable data. These auxiliary hypotheses are sometimes invoked to explain why a replication attempt has failed.",
                "related_terms": [
                    "Epistemic uncertainty",
                    "Hypothesis",
                    "Statistical assumptions",
                    "Hidden moderators"
                ],
                "references": "Dienes, Z. (2008). Understanding psychology as a science: An introduction to scientific and statistical inference. Macmillan International Higher Education.\n\nLakatos, I. (1978). The Methodology of Scientific Research Programs: Vol. I. Cambridge University Press.",
                "drafted_by": [
                    "Alaa Aldoh"
                ],
                "reviewed_by": [
                    "Ali H. Al-Hoorie",
                    "Nihan Albayrak-Aydemir",
                    "Mahmoud Elsherif",
                    "Bethan Iley",
                    "Sam Parsons",
                    "Flávio Azevedo"
                ],
                "alt_related_terms": [
                    null
                ],
                "language": "english",
                "aliases": [
                    "/glossary/auxiliary_hypothesis"
                ]
            },
            {
                "type": "glossary",
                "title": "Badges (Open Science)",
                "definition": "Badges are symbols that editorial teams add to published manuscripts to acknowledge open science practices and act as incentives for researchers to share data, materials, or to embed study preregistration. As clearly-visible symbols, they are intended to signal to the reader that content has met the standard of open research required to receive the badge (typically from that journal). Different badges may be assigned for different practices, such as research having been made available and accessible in a persistent location (“open material badge” and “open data badge”), or study preregistration (“preregistration badge”).",
                "related_terms": [
                    "Incentives",
                    "Open Data badge",
                    "Preregistration",
                    "Triple badge"
                ],
                "references": "Hardwicke, T. E., Bohn, M., MacDonald, K., Hembacher, E., Nuijten, M. B., Peloquin, B. N., & others. (2020). Analytic reproducibility in articles receiving open data badges at the journal Psychological Science: an observational study. Royal Society Open Science, 8(1), 201494. https://doi.org/10.1098/rsos.201494\n\nKidwell, M. C., Lazarević, L. B., Baranski, E., Hardwicke, T. E., Piechowski, S., Falkenberg, L. S., & Nosek, B. A. (2016). Badges to acknowledge open practices: A simple, low-cost, effective method for increasing transparency. PLoS Biology, 14(5), e1002456. https://doi.org/10.1371/journal.pbio.1002456",
                "drafted_by": [
                    "Jacob Miranda"
                ],
                "reviewed_by": [
                    "Brett Gall",
                    "Helena Hartmann",
                    "Mariella Paul",
                    "Charlotte R. Pennington",
                    "Lisa Spitzer",
                    "Suzanne L. K. Stewart"
                ],
                "alt_related_terms": [
                    null
                ],
                "language": "english",
                "aliases": [
                    "/glossary/badges"
                ]
            },
            {
                "type": "glossary",
                "title": "Bayes Factor",
                "definition": "A continuous statistical measure for model selection used in Bayesian inference, describing the *relative* evidence for one model over another, regardless of whether the models are correct. Bayes factors (BF) range from 0 to infinity, indicating the relative strength of the evidence, and where 1 is a neutral point of no evidence. In contrast to *p*\\-values, Bayes factors allow for 3 types of conclusions: a) evidence for the alternative hypothesis, b) evidence for the null hypothesis, and c) no sufficient evidence for either. Thus, BF are typically expressed as BF10 for evidence regarding the alternative compared to the null hypothesis, and as BF01 for evidence regarding the null compared to the alternative hypothesis.",
                "related_terms": [
                    "Bayesian inference",
                    "Bayesian statistics",
                    "Likelihood function",
                    "Null Hypothesis Significance Testing (NHST)",
                    "*p*\\-value"
                ],
                "references": "Hoijtink, H., Mulder, J., van Lissa, C., & Gu, X. (2019). A tutorial on testing hypotheses using the Bayes factor. Psychological Methods, 24(5), 539–556. https://doi.org/10.1037/met0000201\n\nMakowski, D., Ben-Shachar, M. S., Chen, S. H. A., & Lüdecke, D. (2019). Indices of Effect Existence and Significance in the Bayesian Framework. https://doi.org/10.3389/fpsyg.2019.02767",
                "drafted_by": [
                    "Meng Liu"
                ],
                "reviewed_by": [
                    "Alaa AlDoh",
                    "Helena Hartmann",
                    "Connor Keating",
                    "Kai Krautter",
                    "Michele C. Lim",
                    "Suzanne L. K. Stewart",
                    "Ana Todorovic"
                ],
                "alt_related_terms": [
                    null
                ],
                "language": "english",
                "aliases": [
                    "/glossary/bayes_factor"
                ]
            },
            {
                "type": "glossary",
                "title": "Bayesian Inference",
                "definition": "A method of statistical inference based upon Bayes’ theorem, which makes use of epistemological (un)certainty using the mathematical language of probability. Bayesian inference is based on allocating (and reallocating, based on newly-observed data or evidence) credibility across possibilities. Two existing approaches to Bayesian inference include “Bayes factors” (BF) and Bayesian parameter estimation.",
                "related_terms": [
                    "Bayes Factor",
                    "Bayesian statistics",
                    "Bayesian Parameter Estimation"
                ],
                "references": "Dienes, Z. (2011). Bayesian versus orthodox statistics: Which side are you on? Perspectives on Psychological Science, 6(3), 274–290. https://doi.org/10.1177/1745691611406920\n\nDienes, Z. (2014). Using Bayes to get the most out of non-significant results. Frontiers in Psychology, 5, 781. https://doi.org/10.3389/fpsyg.2014.00781\n\nDienes, Z. (2016). How Bayes factors change scientific practice. Journal of Mathematical Psychology, 72, 78–89. https://doi.org/10.1016/j.jmp.2015.10.003\n\nEtz, A., Gronau, Q. F., Dablander, F., & others. (2018). How to become a Bayesian in eight easy steps: An annotated reading list. Psychonomic Bulletin & Review, 25, 219–234. https://doi.org/10.3758/s13423-017-1317-5\n\nKruschke, J. K. (2015). Doing Bayesian data analysis: A tutorial with R, JAGS, and Stan (2nd ed.). Academic Press.\n\nMcElreath, R. (2020). Statistical rethinking: A Bayesian course with examples in R and Stan (2nd ed.). Taylor.\n\nWagenmakers, E.-J., Marsman, M., Jamil, T., Ly, A., Verhagen, J., Love, J., Selker, R., Gronau, Q. F., Šmíra, M., Epskamp, S., Matzke, D., Rouder, J. N., & Morey, R. D. (2018). Bayesian inference for psychology. Part I: Theoretical advantages and practical ramifications. Psychonomic Bulletin & Review, 25(1), 35–57. https://doi.org/10.3758/s13423-017-1343-3",
                "drafted_by": [
                    "Charlotte R. Pennington"
                ],
                "reviewed_by": [
                    "Alaa AlDoh",
                    "Bradley Baker",
                    "Robert Ross",
                    "Markus Weinmann",
                    "Tobias Wingen",
                    "Steven Verheyen"
                ],
                "alt_related_terms": [
                    null
                ],
                "language": "english",
                "aliases": [
                    "/glossary/bayesian_inference"
                ]
            },
            {
                "type": "glossary",
                "title": "Bayesian Parameter Estimation",
                "definition": "A Bayesian approach to estimating parameter values by updating a prior belief about model parameters (i.e., prior distribution) with new evidence (i.e., observed data) via a likelihood function, resulting in a posterior distribution. The posterior distribution may be summarised in a number of ways including: point estimates (mean/mode/median of a posterior probability distribution), intervals of defined boundaries, and intervals of defined mass (typically referred to as a credible interval). In turn, a posterior distribution may become a prior distribution in a subsequent estimation. A posterior distribution can also be sampled using Monte-Carlo Markov Chain methods which can be used to determine complex model uncertainties (e.g. Foreman-Mackey et al., 2013).",
                "related_terms": [
                    "Bayes Factor",
                    "Bayesian inference",
                    "Bayesian statistics",
                    "Null Hypothesis Significance Testing (NHST)"
                ],
                "references": "Foreman-Mackey, D., Hogg, D. W., Lang, D., & Goodman, J. (2013). emcee: The MCMC Hammer. Publications of the Astronomical Society of the Pacific, 125(925), 306–312. https://doi.org/10.1086/670067\n\nMcElreath, R. (2020). Statistical rethinking: A Bayesian course with examples in R and Stan (2nd ed.). Taylor.\n\nPress, W. (2007). Numerical recipes: the art of scientific computing, 3rd edition.\n\nHuber, C. (2016). Introduction to Bayesian statistics, part 2: MCMC and the Metropolis–Hastings algorithm. In The Stata Blog. https://blog.stata.com/2016/11/15/introduction-to-bayesian-statistics-part-2-mcmc-and-the-metropolis-hastings-algorithm/",
                "drafted_by": [
                    "Alaa AlDoh"
                ],
                "reviewed_by": [
                    "Mahmoud Elsherif",
                    "Helena Hartmann",
                    "Dominik Kiersz",
                    "Meng Liu",
                    "Ana Todorovic",
                    "Markus Weinmann"
                ],
                "alt_related_terms": [
                    null
                ],
                "language": "english",
                "aliases": [
                    "/glossary/bayesian_parameter_estimation"
                ]
            },
            {
                "type": "glossary",
                "title": "BIDS data structure",
                "definition": "The Brain Imaging Data Structure (BIDS) describes a simple and easy-to-adopt way of organizing neuroimaging, electrophysiological, and behavioral data (i.e., file formats, folder structures). BIDS is a community effort developed *by* the community *for* the community and was inspired by the format used internally by the OpenfMRI repository known as [OpenNeuro](https://openneuro.org). Having initially been developed for fMRI data, the BIDS data structure has been extended for many other measures, such as EEG (Pernet et al., 2019).",
                "related_terms": [
                    "Open Data"
                ],
                "references": "Gorgolewski, K., Auer, T., Calhoun, V., & others. (2016). The brain imaging data structure, a format for organizing and describing outputs of neuroimaging experiments. Scientific Data, 3, 160044. https://doi.org/10.1038/sdata.2016.44\n\nBIDS. (2020). About BIDS. Retrieved from https://bids.neuroimaging.io",
                "drafted_by": [
                    "Tina Lonsdorf"
                ],
                "reviewed_by": [
                    "Ali H. Al-Hoorie",
                    "David Moreau",
                    "Mariella Paul",
                    "Charlotte R. Pennington"
                ],
                "alt_related_terms": [
                    null
                ],
                "language": "english",
                "aliases": [
                    "/glossary/bids_data_structure"
                ]
            },
            {
                "type": "glossary",
                "title": "BIZARRE",
                "definition": "This acronym refers to Barren, Institutional, Zoo, and other Rare Rearing Environments (BIZARRE). Most research for chimpanzees is conducted on this specific sample. This limits the generalizability of a large number of research findings in the chimpanzee population. The BIZARRE has been argued to reflect the universal concept of what is a chimpanzee (see also WEIRD, which has been argued to be a universal concept for what is a human).",
                "related_terms": [
                    "Populations",
                    "STRANGE",
                    "WEIRD"
                ],
                "references": "Clark, H., Elsherif, M. M., & Leavens, D. A. (2019). Ontogeny vs. phylogeny in primate/canid comparisons: a meta-analysis of the object choice task. Neuroscience & Biobehavioral Reviews, 105, 178–189. https://doi.org/10.1016/j.neubiorev.2019.06.001\n\nLeavens, D. A., Bard, K. A., & Hopkins, W. D. (2010). BIZARRE chimpanzees do not represent “the chimpanzee.” Behavioral and Brain Sciences, 33(2–3), 100–101. https://doi.org/10.1017/S0140525X10000166",
                "drafted_by": [
                    "Mahmoud Elsherif"
                ],
                "reviewed_by": [
                    "Zoe Flack",
                    "Charlotte R. Pennington"
                ],
                "alt_related_terms": [
                    null
                ],
                "language": "english",
                "aliases": [
                    "/glossary/bizarre"
                ]
            },
            {
                "type": "glossary",
                "title": "Bottom-up approach (to Open Scholarship)",
                "definition": "Within academic culture, an approach focusing on the intrinsic interest of academics to improve the quality of research and research culture, for instance by making it supportive, collaborative, creative and inclusive. Usually indicates leadership from early-career researchers acting as the changemakers driving shifts and change in scientific methodology through enthusiasm and innovation, compared to a “top-down” approach initiated by more senior researchers. \"Bottom-up approaches take into account the specific local circumstances of the case itself, often using empirical data, lived experience, personal accounts, and circumstances as the starting point for developing policy solutions.\"",
                "related_terms": [
                    "Early Career Researchers (ECRs)",
                    "Grassroot initiatives"
                ],
                "references": "Button, K. S., Lawrence, N. S., Chambers, C. D., & Munafò, M. R. (2016). Instilling scientific rigour at the grassroots. Psychologist, 29(3), 158–159.\n\nButton, K. S., Chambers, C. D., Lawrence, N., & Munafò, M. R. (2020). Grassroots training for reproducible science: a consortium-based approach to the empirical dissertation. Psychology Learning & Teaching, 19(1), 77–90. https://doi.org/10.1177/1475725719857659\n\nHart, D. D., & Silka, L. (2020). Rebuilding the Ivory Tower: A Bottom-Up Experiment in Aligning Research with Societal Needs. Issues in Science and Technology, 79–85. Retrieved from https://issues.org/aligning-research-with-societal-needs/\n\nMeslin, E. M. (2009). Achieving global justice in health through global research ethics: supplementing Macklin’s ‘top-down’ approach with one from the ‘ground up’. In R. M. Green, A. Donovan, & S. A. Jauss (Eds.), Global Bioethics: Issues of Conscience for the Twenty-First Century (pp. 163–177). University Press.\n\nMoran, H., Karlin, L., Lauchlan, E., Rappaport, S. J., Bleasdale, B., Wild, L., & Dorr, J. (2020). Understanding Research Culture: What researchers think about the culture they work in. Wellcome Open Research, 5, 201. https://doi.org/10.12688/wellcomeopenres.15832.1\n\nNosek, B. A. (2019). Strategy for Culture Change. Center for Open Science. https://www.cos.io/blog/strategy-for-culture-change",
                "drafted_by": [
                    "Catherine Laverty"
                ],
                "reviewed_by": [
                    "Helena Hartmann",
                    "Michele C. Lim",
                    "Adam Parker",
                    "Charlotte R. Pennington",
                    "Birgit Schmidt",
                    "Marta Topor",
                    "Flávio Azevedo"
                ],
                "alt_related_terms": [
                    null
                ],
                "language": "english",
                "aliases": [
                    "/glossary/bottom_up_approach"
                ]
            },
            {
                "type": "glossary",
                "title": "Bracketing Interviews",
                "definition": "Bracketing interviews are commonly used within qualitative approaches. During these interviews researchers explore their personal subjectivities and assumptions surrounding their ongoing research. This allows researchers to be aware of their own interests and helps them to become both more reflective and critical about their research, considering how their own experiences may impact the research process. Bracketing interviews can also be subject to qualitative analysis.",
                "related_terms": [
                    "Qualitative research",
                    "Reflexivity",
                    "Researcher bias **Reference (s)**:  \\[@RollsRelf2006\\], \\[@Sorsa2015\\]"
                ],
                "references": "",
                "drafted_by": [
                    "Claire Melia"
                ],
                "reviewed_by": [
                    "Tamara Kalandadze",
                    "Charlotte R. Pennington",
                    "Graham Reid",
                    "Marta Topor"
                ],
                "alt_related_terms": [
                    null
                ],
                "language": "english",
                "aliases": [
                    "/glossary/bracketing_interviews"
                ]
            },
            {
                "type": "glossary",
                "title": "Bropenscience",
                "definition": "A tongue-in-cheek expression intended to raise awareness of the lack of diverse voices in open science (Bahlai, Bartlett, Burgio et al. 2019; Onie, 2020), in addition to the presence of behavior and communication styles that can be toxic or exclusionary. Importantly, not all bros are men; rather, they are individuals who demonstrate rigid thinking, lack self-awareness, and tend towards hostility, unkindness, and exclusion (Pownall et al., 2021; Whitaker & Guest, 2020). They generally belong to dominant groups who benefit from structural privileges. To address \\#bropenscience, researchers should examine and address structural inequalities within academic systems and institutions.",
                "related_terms": [
                    "Diversity",
                    "Inclusion",
                    "Intersectionality",
                    "Open Science **Reference (s)**: \\[@GuestTweet2017\\], \\[@Whitaker2020\\], \\[@Pownall20210\\]"
                ],
                "references": "",
                "drafted_by": [
                    "Zoe Flack"
                ],
                "reviewed_by": [
                    "Magdalena Grose-Hodge",
                    "Helena Hartmann",
                    "Bethan Iley",
                    "Tamara Kalandadze",
                    "Adam Parker",
                    "Charlotte R. Pennington",
                    "Flávio Azevedo",
                    "Bradley Baker",
                    "Mahmoud Elsherif"
                ],
                "alt_related_terms": [
                    null
                ],
                "language": "english",
                "aliases": [
                    "/glossary/bropenscience"
                ]
            },
            {
                "type": "glossary",
                "title": "CARKing",
                "definition": "Critiquing After the Results are Known (CARKing) refers to presenting a criticism of a design as one that you would have made in advance of the results being known. It usually forms a reaction or criticism to unwelcome or unfavourable results, results whether the critic is conscious of this fact or not.",
                "related_terms": [
                    "HARKing",
                    "Preregistration",
                    "Registered Report"
                ],
                "references": "Bardsley, N. (2018). What lessons does the “replication crisis” in psychology hold for experimental economics? In Handbook of Psychology and Economic Behaviour, 2nd edition. Cambridge University Press. Retrieved from http://centaur.reading.ac.uk/69874/\n\nNosek, B. A., & Lakens, D. (2014). Registered reports. Social Psychology, 45, 137–141. https://doi.org/10.1027/1864-9335/a000192",
                "drafted_by": [
                    "Mahmoud Elsherif"
                ],
                "reviewed_by": [
                    "Ali H. Al-Hoorie",
                    "Ashley Blake",
                    "Adrien Fillon",
                    "Charlotte R. Pennington"
                ],
                "alt_related_terms": [
                    null
                ],
                "language": "english",
                "aliases": [
                    "/glossary/carking"
                ]
            },
            {
                "type": "glossary",
                "title": "Center for Open Science (COS)",
                "definition": "A non-profit technology organization based in Charlottesville, Virginia with the mission “to increase openness, integrity, and reproducibility of research.” Among other resources, the COS hosts the Open Science Framework (OSF) and the Open Scholarship Knowledge Base.",
                "related_terms": [
                    "Open Science badges",
                    "Open Science Framework",
                    "OSF collections",
                    "OSF institutions",
                    "OSF meetings",
                    "OSF preprints",
                    "OSF registries",
                    "Registrations (Preregistrations & Registered Reports)",
                    "Transparency and Openness Promotion Guidelines (TOP)"
                ],
                "references": "for Open Science, C. (n.d.). Show Your Work. Share Your Work. Advance Science. That’s Open Science. https://www.cos.io/",
                "drafted_by": [
                    "Beatrix Arendt"
                ],
                "reviewed_by": [
                    "Ali H. Al-Hoorie",
                    "Mariella Paul",
                    "Charlotte R. Pennington",
                    "Lisa Spitzer"
                ],
                "alt_related_terms": [
                    null
                ],
                "language": "english",
                "aliases": [
                    "/glossary/center_for_open_science"
                ]
            },
            {
                "type": "glossary",
                "title": "Citation bias",
                "definition": "A biased selection of papers or authors cited and included in the references section. When citation bias is present, it is often in a way which would benefit the author(s) or reviewers, over-represents statistically significant studies, or reflects pervasive gender or racial biases (Brooks, 1985; Jannot et al., 2013; Zurn et al., 2020). One proposed solution is the use of Citation Diversity Statements, in which authors reflect on their citation practices and identify biases which may have emerged (Zurn et al., 2020).",
                "related_terms": [
                    "Citation diversity statement",
                    "Reporting bias"
                ],
                "references": "Brooks, T. A. (1985). Private acts and public objects: An investigation of citer motivations. Journal of the American Society for Information Science, 36(4), 223–229. https://doi.org/10.1002/asi.4630360402\n\nJannot, A. S., Agoritsas, T., Gayet-Ageron, A., & Perneger, T. V. (2013). Citation bias favoring statistically significant studies was present in medical research. Journal of Clinical Epidemiology, 66(3), 296–301. https://doi.org/10.1016/j.jclinepi.2012.09.015\n\nThombs, B. D., Levis, A. W., Razykov, I., Syamchandra, A., Leentjens, A. F., Levenson, J. L., & Lumley, M. A. (2015). Potentially coercive self-citation by peer reviewers: a cross-sectional study. Journal of Psychosomatic Research, 78(1), 1–6. https://doi.org/10.1016/j.jpsychores.2014.09.015\n\nZurn, P., Bassett, D. S., & Rust, N. C. (2020). The Citation Diversity Statement: A Practice of Transparency, A Way of Life. Trends in Cognitive Sciences, 24(9), 669–672. https://doi.org/10.1016/j.tics.2020.06.009",
                "drafted_by": [
                    "Bettina M. J. Kern"
                ],
                "reviewed_by": [
                    "Mahmoud Elsherif",
                    "Annalise A. LaPlume",
                    "Helena Hartmann",
                    "Bethan Iley",
                    "Charlotte R. Pennington",
                    "Timo Roettger",
                    "Tobias Wingen"
                ],
                "alt_related_terms": [
                    null
                ],
                "language": "english",
                "aliases": [
                    "/glossary/citation_bias"
                ]
            },
            {
                "type": "glossary",
                "title": "Citation Diversity Statement",
                "definition": "A current effort trying to increase awareness and mitigate the citation bias in relation to gender and race is the Citation Diversity Statement, a short paragraph where “the authors consider their own bias and quantify the equitability of their reference lists. It states: (i) the importance of citation diversity, (ii) the percentage breakdown (or other diversity indicators) of citations in the paper, (iii) the method by which percentages were assessed and its limitations, and (iv) a commitment to improving equitable practices in science” (Zurn et al., 2020, p. 669).",
                "related_terms": [
                    "Citation bias",
                    "Diversity",
                    "Under-representation"
                ],
                "references": "Zurn, P., Bassett, D. S., & Rust, N. C. (2020). The Citation Diversity Statement: A Practice of Transparency, A Way of Life. Trends in Cognitive Sciences, 24(9), 669–672. https://doi.org/10.1016/j.tics.2020.06.009",
                "drafted_by": [
                    "Helena Hartmann"
                ],
                "reviewed_by": [
                    "Mahmoud Elsherif",
                    "Magdalena Grose-Hodge",
                    "Sam Parsons",
                    "Timo Roettger"
                ],
                "alt_related_terms": [
                    null
                ],
                "language": "english",
                "aliases": [
                    "/glossary/citation_diversity_statement"
                ]
            },
            {
                "type": "glossary",
                "title": "Citizen Science",
                "definition": "Citizen science refers to projects that actively involve the general public in the scientific endeavour, with the goal of democratizing science. Citizen scientists can be involved in all stages of research, acting as collaborators, contributors or project leaders. An example of a major citizen science project involved individuals identifying astronomical bodies (Lintott, 2008).",
                "related_terms": [
                    "Crowd science",
                    "Crowdsourcing **Alternative definition:** (if applicable) In the past, citizen science mostly referred to volunteers who participate as field assistants in scientific studies (Cohn, 2008, p. 193)."
                ],
                "references": "Cohn, J. P. (2008). Citizen science: Can volunteers do real research? BioScience, 58(3), 192–197. https://doi.org/10.1641/B580303\n\nLintott, C. J., Schawinski, K., Slosar, A., Land, K., Bamford, S., Thomas, D., & Vandenberg, J. (2008). Galaxy Zoo: morphologies derived from visual inspection of galaxies from the Sloan Digital Sky Survey. Monthly Notices of the Royal Astronomical Society, 389(3), 1179–1189. https://doi.org/10.1111/j.1365-2966.2008.13689.x",
                "drafted_by": [
                    "Mahmoud Elsherif; Ana Barbosa Mendes"
                ],
                "reviewed_by": [
                    "Gisela H. Govaart",
                    "Tamara Kalandadze",
                    "Dominik Kiersz",
                    "Charlotte R. Pennington",
                    "Robert M. Ross"
                ],
                "alt_related_terms": [
                    null
                ],
                "language": "english",
                "aliases": [
                    "/glossary/citizen_science"
                ]
            },
            {
                "type": "glossary",
                "title": "CKAN",
                "definition": "The Comprehensive Knowledge Archive Network (CKAN) is an open-source data platform and free software that aims to provide tools to streamline publishing and data sharing. CKAN supports governments, research institutions and other organizations in managing and publishing large amounts of data.",
                "related_terms": [
                    "Data platforms",
                    "Data sharing"
                ],
                "references": "Anon. (n.d.). Ckan. Retrieved from https://ckan.org/",
                "drafted_by": [
                    "Tsvetomira Dumbalska"
                ],
                "reviewed_by": [
                    "Ali H. Al-Hoorie",
                    "Kai Krautter",
                    "Charlotte R. Pennington"
                ],
                "alt_related_terms": [
                    null
                ],
                "language": "english",
                "aliases": [
                    "/glossary/ckan"
                ]
            },
            {
                "type": "glossary",
                "title": "COAR Community Framework for Good Practices in Repositories",
                "definition": "A framework which identifies best practices for scientific repositories and evaluation criteria for these practices. Its flexible and multidimensional approach means that it can be applied to different types of repositories, including those which host publications or data, across geographical and thematic contexts.",
                "related_terms": [
                    "Metadata",
                    "Open Access",
                    "Open Data",
                    "Open Material",
                    "Repository",
                    "TRUST principles"
                ],
                "references": "of Open Access Repositories, C. (2020). COAR Community Framework for Best Practices in Repositories (Version 1). Zenodo. https://doi.org/10.5281/zenodo.4110829",
                "drafted_by": [
                    "Aleksandra Lazić"
                ],
                "reviewed_by": [
                    "Ashley Blake",
                    "Jamie P. Cockcroft",
                    "Bethan Iley",
                    "Sam Parsons"
                ],
                "alt_related_terms": [
                    null
                ],
                "language": "english",
                "aliases": [
                    "/glossary/coar_community_framework_for_good_practices_in_repositories"
                ]
            },
            {
                "type": "glossary",
                "title": "Codebook",
                "definition": "A codebook is a high-level summary that describes the contents, structure, nature and layout of a data set. A well-documented codebook contains information intended to be complete and self-explanatory for each variable in a data file, such as the wording and coding of the item, and the underlying construct. It provides transparency to researchers who may be unfamiliar with the data but wish to reproduce analyses or reuse the data.",
                "related_terms": [
                    "Data dictionary",
                    "Metadata"
                ],
                "references": "Arslan, R. C. (2019). How to Automatically Document Data With the codebook Package to Facilitate Data Reuse. Advances in Methods and Practices in Psychological Science, 2(2), 169–187. https://doi.org/10.1177/2515245919838783",
                "drafted_by": [
                    "Tina Lonsdorf"
                ],
                "reviewed_by": [
                    "Ali H. Al-Hoorie",
                    "Ashley Blake, Kai Krautter",
                    "Charlotte R. Pennington",
                    "Flávio Azevedo"
                ],
                "alt_related_terms": [
                    null
                ],
                "language": "english",
                "aliases": [
                    "/glossary/codebook"
                ]
            },
            {
                "type": "glossary",
                "title": "Code review",
                "definition": "The process of checking another researcher's programming (specifically, computer source code) including but not limited to statistical code and data modelling. This process is designed to detect and resolve mistakes, thereby improving code quality. In practice, a modern peer review process may take place via a hosted online repository such as GitHub, GitLab or SourceForge.",
                "related_terms": [
                    "Reproducibility",
                    "Version control"
                ],
                "references": "Petre, M., & Wilson, G. (2014). Code review for and by scientists. arXiv Preprint arXiv:1407.5648. https://arxiv.org/abs/1407.5648\n\nScopatz, A. M., & Huff, K. D. (2015). Effective Computation in Physics: Field Guide to Research with Python (1st ed.). O’Reilly Media. http://shop.oreilly.com/product/0636920033424.do",
                "drafted_by": [
                    "Filip Dechterenko"
                ],
                "reviewed_by": [
                    "Ali H. Al-Hoorie",
                    "Dominik Kiersz",
                    "Sam Parsons",
                    "Charlotte R. Pennington"
                ],
                "alt_related_terms": [
                    null
                ],
                "language": "english",
                "aliases": [
                    "/glossary/code_review"
                ]
            },
            {
                "type": "glossary",
                "title": "Collaborative Replication and Education Project (CREP)",
                "definition": "The Collaborative Replication and Education Project (CREP) is an initiative designed to organize and structure replication efforts of highly-cited empirical studies in psychology to satisfy the dual needs for more high-quality direct replications and more training in empirical research techniques for psychology students. CREP aims to address the need for replications of highly cited studies, and to provide training, support and professional growth opportunities for academics completing replication projects.",
                "related_terms": [
                    "Direct replication",
                    "Exact replication"
                ],
                "references": "Wagge, J. R., Baciu, C., Banas, K., Nadler, J. T., Schwarz, S., Weisberg, Y., & others. (2019). A demonstration of the collaborative replication and education project: Replication attempts of the red-romance effect. Collabra: Psychology, 5(1). https://doi.org/10.1525/collabra.177",
                "drafted_by": [
                    "Connor Keating"
                ],
                "reviewed_by": [
                    "Bradley Baker",
                    "Mahmoud Elsherif",
                    "Zoe Flack",
                    "Charlotte R. Pennington"
                ],
                "alt_related_terms": [
                    null
                ],
                "language": "english",
                "aliases": [
                    "/glossary/collaborative_replication_and_education_project"
                ]
            },
            {
                "type": "glossary",
                "title": "Committee on Best Practices in Data Analysis and Sharing (COBIDAS)",
                "definition": "The Organization for Human Brain Mapping (OHBM) neuroimaging community has developed a guideline for best practices in neuroimaging data acquisition, analysis, reporting, and sharing of both data and analysis code. It contains eight elements that should be included when writing up or submitting a manuscript in order to improve reporting methods and the resulting neuroimages in order to optimize transparency and reproducibility. **Alternative definition:** (if applicable) Checklist for data analysis and sharing",
                "related_terms": [],
                "references": "Nichols, T. E., Das, S., Eickhoff, S. B., Evans, A. C., Glatard, T., Hanke, M., & others. (2017). Best practices in data analysis and sharing in neuroimaging using MRI. Nature Neuroscience, 20(3), 299–303. https://doi.org/10.1038/nn.4500\n\nPernet, C., Garrido, M. I., Gramfort, A., Maurits, N., Michel, C. M., Pang, E., & others. (2020). Issues and recommendations from the OHBM COBIDAS MEEG committee for reproducible EEG and MEG research. Nature Neuroscience, 23(12), 1473–1483. https://doi.org/10.1038/s41593-020-00709-0",
                "drafted_by": [
                    "Yu-Fang Yang"
                ],
                "reviewed_by": [
                    "Jamie P. Cockcroft",
                    "Helena Hartmann",
                    "Adam Parker",
                    "Sam Parsons"
                ],
                "alt_related_terms": [
                    null
                ],
                "language": "english",
                "aliases": [
                    "/glossary/committee_on_best_practices_in_data_analysis_and_sharing"
                ]
            },
            {
                "type": "glossary",
                "title": "Communality",
                "definition": "The common ownership of scientific results and methods and the consequent imperative to share both freely. Communality is based on the fact that every scientific finding is seen as a product of the effort of a number of agents. This norm is followed when scientists openly share their new findings with colleagues.",
                "related_terms": [
                    "Mertonian norms",
                    "Objectivity **Alternative definition:** Communism (in Merton, 1942\\) **Related terms to alternative definition** (if applicable)"
                ],
                "references": "Anderson, M. S., Ronning, E. A., Devries, R., & Martinson, B. C. (2010). Extending the Mertonian norms: Scientists’ subscription to norms of research. Journal of Higher Education, 81(3), 366–393. https://doi.org/10.1353/jhe.0.0095\n\nHardwicke, T. E., Jameel, L., Jones, M., Walczak, E. J., & Weinberg, L. M. (2014). Only human: Scientists, systems, and suspect statistics. Opticon1826, 16, 25. https://doi.org/10.5334/OPT.CH\n\nMerton, R. K. (1938). Science and the social order. Philosophy of Science, 5(3), 321–337. https://doi.org/10.1086/286513\n\nMerton, R. K. (1942). A note on science and democracy. Journal of Legal and Political Sociology, 1, 115–126. https://doi.org/10.1515/9783110375008-013",
                "drafted_by": [
                    "David Moreau"
                ],
                "reviewed_by": [
                    "Ashley Blake",
                    "Mahmoud Elsherif",
                    "Charlotte R. Pennington",
                    "Beatrice Valentini"
                ],
                "alt_related_terms": [
                    null
                ],
                "language": "english",
                "aliases": [
                    "/glossary/communality"
                ]
            },
            {
                "type": "glossary",
                "title": "Community Projects",
                "definition": "Collaborative projects that involve researchers from different career levels, disciplines, institutions or countries. Projects may have different goals including peer support and learning, conducting research, teaching and education. They can be short-term (e.g., conference events or hackathons) or long-term (e.g., journal clubs or consortium-led research projects). Collaborative culture and community building are key to achieving project goals.",
                "related_terms": [
                    "Bottom-up approach (to Open Scholarship)",
                    "Crowdsourced research",
                    "Hackathon",
                    "Many Labs",
                    "ReproducibiliTea"
                ],
                "references": "Ellemers, N. (2021). Science as collaborative knowledge generation. British Journal of Social Psychology, 60(1), 1–28. https://doi.org/10.1111/bjso.12430\n\nOrben, A. (2019). A journal club to fix science. Nature, 573(7775), 465–466. https://doi.org/10.1038/d41586-019-02842-8\n\nShepard, B. (2015). Community projects as social activism. SAGE.",
                "drafted_by": [
                    "Marta Topor"
                ],
                "reviewed_by": [
                    "Ali H. Al-Hoorie",
                    "Jamie P. Cockcroft",
                    "Mahmoud Elsherif",
                    "Kai Krautter",
                    "Gerald Vineyard"
                ],
                "alt_related_terms": [
                    null
                ],
                "language": "english",
                "aliases": [
                    "/glossary/community_projects"
                ]
            },
            {
                "type": "glossary",
                "title": "Compendium",
                "definition": "A collection of files prepared by a researcher to support a report or publication that include the data, metadata, programming code, software dependencies, licenses, and other instructions necessary for another researcher to independently reproduce the findings presented in the report or publication.",
                "related_terms": [
                    "Compendia",
                    "Replication",
                    "Reproducibility",
                    "Research compendium",
                    "**References:** \\[@Claerbout1992\\], \\[@Gentleman2005\\], \\[@Marwick2018\\], \\[@Nust2018\\]"
                ],
                "references": "",
                "drafted_by": [
                    "Ben Marwick"
                ],
                "reviewed_by": [
                    "Ali H. Al-Hoorie",
                    "Charlotte R. Pennington"
                ],
                "alt_related_terms": [
                    null
                ],
                "language": "english",
                "aliases": [
                    "/glossary/compendium"
                ]
            },
            {
                "type": "glossary",
                "title": "Computational reproducibility",
                "definition": "Ability to recreate the same results as the original study (including tables, figures, and quantitative findings), using the same input data, computational methods, and conditions of analysis. The availability of code and data facilitates computational reproducibility, as does preparation of these materials (annotating data, delineating software versions used, sharing computational environments, etc). Ideally, computational reproducibility should be achievable by another second researcher (or the original researcher, at a future time), using only a set of files and written instructions. Also referred to as analytic reproducibility (LeBel et al., 2018).",
                "related_terms": [
                    "FAIR principles",
                    "Replicability",
                    "Reproducibility"
                ],
                "references": "on Reproducibility, C., & in Science et al., R. (2019). Reproducibility and Replicability in Science (p. 25303). National Academies Press. https://doi.org/10.17226/25303\n\nKitzes, J., Turek, D., & Deniz, F. (2017). The practice of reproducible research: Case studies and lessons from the data-intensive sciences. University of California Press.\n\nLeBel, E. P., McCarthy, R. J., Earp, B. D., Elson, M., & Vanpaemel, W. (2018). A unified framework to quantify the credibility of scientific findings. Advances in Methods and Practices in Psychological Science, 1(3), 389–402. https://doi.org/10.1177/2515245918787489\n\nNosek, B. A., & Errington, T. M. (2020). What is replication? PLOS Biology, 18(3), e3000691. https://doi.org/10.1371/journal.pbio.3000691\n\nObels, P., Lakens, D., Coles, N. A., Gottfried, J., & Green, S. A. (2020). Analysis of open data and computational reproducibility in registered reports in psychology. Advances in Methods and Practices in Psychological Science, 3(2), 229–237. https://doi.org/10.1177/2515245920917961",
                "drafted_by": [
                    "Tina Lonsdorf"
                ],
                "reviewed_by": [
                    "Sarah Ashcroft-Jones",
                    "Helena Hartmann",
                    "Annalise A. LaPlume",
                    "Adam Parker",
                    "Charlotte R. Pennington",
                    "Eike Mark Rinke"
                ],
                "alt_related_terms": [
                    null
                ],
                "language": "english",
                "aliases": [
                    "/glossary/computational_reproducibility"
                ]
            },
            {
                "type": "glossary",
                "title": "Conceptual replication",
                "definition": "A replication attempt whereby the primary effect of interest is the same but tested in a different sample and captured in a different way to that originally reported (i.e., using different operationalisations, data processing and statistical approaches and/or different constructs; LeBel et al., 2018). The purpose of a conceptual replication is often to explore what conditions limit the extent to which an effect can be observed and generalised (e.g., only within certain contexts, with certain samples, using certain measurement approaches) towards evaluating and advancing theory (Hüffmeier et al., 2016).",
                "related_terms": [
                    "Direct replication",
                    "Generalizability"
                ],
                "references": "Crüwell, S., van Doorn, J., Etz, A., Makel, M. C., Moshontz, H., Niebaum, J. C., Orben, A., Parsons, S., & Schulte-Mecklenbeck, M. (2019). Seven Easy Steps to Open Science: An Annotated Reading List. Zeitschrift Für Psychologie, 227(4), 237–248. https://doi.org/10.1027/2151-2604/a000387\n\nHüffmeier, J., Mazei, J., & Schultze, T. (2016). Reconceptualizing replication as a sequence of different studies: A replication typology. Journal of Experimental Social Psychology, 66, 81–92. https://doi.org/10.1016/j.jesp.2015.09.009\n\nLeBel, E. P., McCarthy, R. J., Earp, B. D., Elson, M., & Vanpaemel, W. (2018). A unified framework to quantify the credibility of scientific findings. Advances in Methods and Practices in Psychological Science, 1(3), 389–402. https://doi.org/10.1177/2515245918787489",
                "drafted_by": [
                    "Mahmoud Elsherif; Thomas Rhys Evans"
                ],
                "reviewed_by": [
                    "Adrien Fillon",
                    "Helena Hartmann",
                    "Matt Jaquiery",
                    "Tina B. Lonsdorf",
                    "Catia M. Oliveira",
                    "Charlotte R. Pennington",
                    "Graham Reid",
                    "Timo Roettger",
                    "Lisa Spitzer",
                    "Suzanne L. K. Stewart",
                    "Flávio Azevedo"
                ],
                "alt_related_terms": [
                    null
                ],
                "language": "english",
                "aliases": [
                    "/glossary/conceptual_replication"
                ]
            },
            {
                "type": "glossary",
                "title": "Confirmation bias",
                "definition": "The tendency to seek out, interpret, favor and recall information in a way that supports one’s prior values, beliefs, expectations, or hypothesis.",
                "related_terms": [
                    "Confirmatory bias",
                    "Congeniality bias",
                    "Myside bias"
                ],
                "references": "Bishop, D. V. (2020). The psychology of experimental psychologists: Overcoming cognitive constraints to improve research: The 47th Sir Frederic Bartlett Lecture. Quarterly Journal of Experimental Psychology, 73(1), 1–19. https://doi.org/10.1177/1747021819886519\n\nNickerson, R. S. (1998). Confirmation bias: A ubiquitous phenomenon in many guises. Review of General Psychology, 2(2), 175–220. https://doi.org/10.1037/1089-2680.2.2.175\n\nSpencer, E. A., & Heneghan, C. (2018). Confirmation bias. Catalogue Of Bias. https://catalogofbias.org/biases/confirmation-bias/\n\nWason, P. C. (1960). On the failure to eliminate hypotheses in a conceptual task. Quarterly Journal of Experimental Psychology, 12(3), 129–140. https://doi.org/10.1080/17470216008416717",
                "drafted_by": [
                    "Barnabas Szaszi; Jenny Terry"
                ],
                "reviewed_by": [
                    "Mahmoud Elsherif",
                    "Tamara Kalandadze",
                    "Sam Parsons",
                    "Charlotte R. Pennington"
                ],
                "alt_related_terms": [
                    null
                ],
                "language": "english",
                "aliases": [
                    "/glossary/confirmation_bias"
                ]
            },
            {
                "type": "glossary",
                "title": "Confirmatory analyses",
                "definition": "Part of the confirmatory-exploratory distinction (Wagenmakers et al., 2012), where confirmatory analyses refer to analyses that were set *a priori* and test existent hypotheses. The lack of this distinction within published research findings has been suggested to explain replicability issues and is suggested to be overcome through study preregistration which clearly distinguishes confirmatory from exploratory analyses. Other researchers have questioned these terms and recommended a replacement with ‘discovery-oriented’ and ‘theory-testing research’ (Oberauer & Lewandowsky, 2019; see also Szollosi & Donkin, 2019).",
                "related_terms": [
                    "Exploratory data analysis",
                    "Preregistration"
                ],
                "references": "Box, G. E. P. (1976). Science and statistics. Journal of the American Statistical Association, 71(356), 791–799.\n\nOberauer, K., & Lewandowsky, S. (2019). Addressing the theory crisis in psychology. Psychonomic Bulletin & Review, 26(5), 1596–1618. https://doi.org/10.3758/s13423-019-01645-2\n\nSzollosi, A., & Donkin, C. (2019). Arrested theory development: The misguided distinction between exploratory and confirmatory research. PsyArXiv. https://doi.org/10.31234/osf.io/your_doi_placeholder\n\nTukey, J. W. (1977). Exploratory data analysis. Addison-Wesley.\n\nWagenmakers, E. J., Wetzels, R., Borsboom, D., van der Maas, H. L., & Kievit, R. A. (2012). An agenda for purely confirmatory research. Perspectives on Psychological Science, 7(6), 632–638. https://doi.org/10.1177/1745691612463078",
                "drafted_by": [
                    "Jenny Terry"
                ],
                "reviewed_by": [
                    "Mahmoud Elsherif",
                    "Eduardo Garcia-Garzon",
                    "Helena Hartmann",
                    "Mariella Paul",
                    "Charlotte R. Pennington",
                    "Timo Roettger",
                    "Lisa Spitzer"
                ],
                "alt_related_terms": [
                    null
                ],
                "language": "english",
                "aliases": [
                    "/glossary/confirmatory_analyses"
                ]
            },
            {
                "type": "glossary",
                "title": "Conflict of interest",
                "definition": "A conflict of interest (COI, also ‘competing interest’) is a financial or non-financial relationship, activity or other interest that might compromise objectivity or professional judgement on the part of an author, reviewer, editor, or editorial staff. The *Principles of Transparency and Best Practice in Scholarly Publishing* by the Committee on Publication Ethics (COPE), the Directory of Open Access Journals (DOAJ), the Open Access Scholarly Publishers Association (OASPA), and the World Association of Medical Editors (WAME) states that journals should have policies on publication ethics, including policies on COI (DOAJ, 2018). COIs should be made transparent so that readers can properly evaluate research and assess for potential or actual bias(es). Outside publishing, academic presenters, panel members and educators should also declare COIs. Purposeful failure to disclose a COI may be considered a form of misconduct.",
                "related_terms": [
                    "Objectivity",
                    "Peer review",
                    "Public Trust in Science",
                    "Publication ethics",
                    "Transparency"
                ],
                "references": "Directory of Open Access Journals. (n.d.). https://doaj.org/apply/transparency/",
                "drafted_by": [
                    "Christopher Graham"
                ],
                "reviewed_by": [
                    "Flávio Azevedo"
                ],
                "alt_related_terms": [
                    null
                ],
                "language": "english",
                "aliases": [
                    "/glossary/conflict_of_interest"
                ]
            },
            {
                "type": "glossary",
                "title": "Consortium authorship",
                "definition": "Only the name of the consortium or organization appears in the author column, and the individuals' names do not appear in the literature: For example, ‘FORRT’ as an author. This can be seen in the products of collaborative projects with a very large number of collaborators and/or contributors. Depending on the journal policy, individual researchers may be recorded as one of the authors of the product in literature databases such as ORCID and Scopus. Consortium authorship can also be termed group, corporate, organisation/organization or collective authorship (e.g. [https://www.bmj.com/about-bmj/resources-authors/article-submission/authorship-contributorship](https://www.bmj.com/about-bmj/resources-authors/article-submission/authorship-contributorship) ), or collaborative authorship (e.g. [https://support.jmir.org/hc/en-us/articles/115001449591-What-is-a-group-author-collaborative-author-and-does-it-need-an-ORCID](https://support.jmir.org/hc/en-us/articles/115001449591-What-is-a-group-author-collaborative-author-and-does-it-need-an-ORCID))",
                "related_terms": [
                    "Authorship",
                    "CRediT"
                ],
                "references": "Tierney, W., Hardy III, J. H., Ebersole, C. R., Leavitt, K., Viganola, D., Clemente, E. G., & others. (2020). Creative destruction in science. Organizational Behavior and Human Decision Processes, 161, 291–309. https://doi.org/10.1016/j.obhdp.2020.07.002\n\nTierney, W., Hardy III, J., Ebersole, C. R., Viganola, D., Clemente, E. G., Gordon, M., & others. (2021). A creative destruction approach to replication: Implicit work and sex morality across cultures. Journal of Experimental Social Psychology, 93, 104060. https://doi.org/10.1016/j.jesp.2020.104060",
                "drafted_by": [
                    "Yuki Yamada"
                ],
                "reviewed_by": [
                    "Adam Parker",
                    "Charlotte R. Pennington",
                    "Beatrice Valentini",
                    "Qinyu Xiao",
                    "Flávio Azevedo"
                ],
                "alt_related_terms": [
                    null
                ],
                "language": "english",
                "aliases": [
                    "/glossary/consortium_authorship"
                ]
            },
            {
                "type": "glossary",
                "title": "Constraints on Generality (COG)",
                "definition": "A statement that explicitly identifies and justifies the target population, and conditions, for the reported findings. Researchers should be explicit about potential boundary conditions for their generalisations (Simons et al., 2017). Researchers should provide detailed descriptions of the sampled population and/or contextual factors that might have affected the results such that future replication attempts can take these factors into account (Brandt et al., 2014). Conditions not explicitly listed are assumed not to have theoretical relevance to the replicability of the effect.",
                "related_terms": [
                    "BIZARRE",
                    "Diversity",
                    "Equity",
                    "Generalizability",
                    "Inclusion",
                    "Reproducibility",
                    "Replication",
                    "STRANGE",
                    "WEIRD"
                ],
                "references": "Busse, C., Kach, A. P., & Wagner, S. M. (2017). Boundary Conditions: What They Are, How to Explore Them, Why We Need Them, and When to Consider Them. Organizational Research Methods, 20(4), 574–609. https://doi.org/10.1177/1094428116641191\n\nBrandt, M. J., IJzerman, H., Dijksterhuis, A., Farach, F. J., Geller, J., Giner-Sorolla, R., & others. (2014). The replication recipe: What makes for a convincing replication? Journal of Experimental Social Psychology, 50, 217–224. https://doi.org/10.1016/j.jesp.2013.10.005\n\nSimons, D. J., Shoda, Y., & Lindsay, D. S. (2017). Constraints on generality (COG): A proposed addition to all empirical papers. Perspectives on Psychological Science, 12(6), 1123–1128. https://doi.org/10.1177/1745691617708630\n\nYarkoni, T. (2020). The generalizability crisis. Behavioral and Brain Sciences, 1–37. https://doi.org/10.1017/S0140525X20001685",
                "drafted_by": [
                    "Mahmoud Elsherif"
                ],
                "reviewed_by": [
                    "Ali H. Al-Hoorie",
                    "Jamie P. Cockcroft",
                    "Sam Parsons",
                    "Charlotte R. Pennington",
                    "Timo Roettger"
                ],
                "alt_related_terms": [
                    null
                ],
                "language": "english",
                "aliases": [
                    "/glossary/constraints_on_generality"
                ]
            },
            {
                "type": "glossary",
                "title": "Construct validity",
                "definition": "When used in the context of measurement and testing, construct validity refers to the degree to which a test measures what it claims to be measuring. In fields that study hypothetical unobservable entities, construct validation is essentially theory testing, because it involves determining whether an objective measure (a questionnaire, lab task, etc.) is a valid representation of a hypothetical construct (i.e., conforms to a theory). When used in a broader sense about a research study, or a claim, conclusion, or observed effect in a research study, construct validity concerns the extent to which the sampling particulars used in the study (participants, settings, treatments, and dependent variables) map onto the higher order constructs the study, the claim, the conclusion is about. According to Shadish et al. (2002), construct validity can be defined as “the degree to which inferences are warranted from the observed persons, settings, and cause and effect operations included in a study to the constructs that these instances might represent” (p. 38).",
                "related_terms": [
                    "Measurement crisis",
                    "Measurement validity",
                    "Questionable Measurement Practices (QMP)",
                    "Theory",
                    "Validity",
                    "Validation"
                ],
                "references": "Cronbach, L. J., & Meehl, P. E. (1955). Construct validity in psychological tests. Psychological Bulletin, 52(4), 281–302. https://doi.org/10.1037/h0040957\n\nShadish, W. R., Cook, T. D., & Campbell, D. T. (2002). Experimental and quasi-experimental designs for generalized causal inference. Houghton Mifflin.\n\nSmith, G. T. (2005). On Construct Validity: Issues of Method and Measurement. Psychological Assessment, 17(4), 396–408. https://doi.org/10.1037/1040-3590.17.4.396",
                "drafted_by": [
                    "Annalise A. LaPlume"
                ],
                "reviewed_by": [
                    "Ali H. Al-Hoorie",
                    "Mahmoud Elsherif",
                    "Zoltan Kekecs",
                    "Charlotte R. Pennington"
                ],
                "alt_related_terms": [
                    null
                ],
                "language": "english",
                "aliases": [
                    "/glossary/construct_validity"
                ]
            },
            {
                "type": "glossary",
                "title": "Content validity",
                "definition": "The degree to which a measurement includes all aspects of the concept that the researcher claims to measure; “A qualitative type of validity where the domain of the concept is made clear and the analyst judges whether the measures fully represent the domain” (Bollen, 1989, p.185). It is a component of *construct validity* and can be established using both quantitative and qualitative methods, often involving expert assessment.",
                "related_terms": [
                    "Construct validity",
                    "Validity"
                ],
                "references": "Bollen, K. A. (1989). Structural Equations with Latent Variables (pp. 179–225). John Wiley & Sons.\n\nBrod, M., Tesler, L., & Christensen, T. (2009). Qualitative research and content validity: Developing best practices based on science and experience. Quality of Life Research, 18(9), 1263–1278. https://doi.org/10.1007/s11136-009-9540-9\n\nDrost, E. A. (2011). Validity and reliability in social science research. Education Research and Perspectives, 38(1), 105–123.\n\nHaynes, S. N., Richard, D. C. S., & Kubany, E. S. (1995). Content validity in psychological assessment: A functional approach to concepts and methods. Psychological Assessment, 7(3), 238–247. https://doi.org/10.1037/1040-3590.7.3.238",
                "drafted_by": [
                    "Annalise A. LaPlume"
                ],
                "reviewed_by": [
                    "Mahmoud Elsherif",
                    "Wanyin Li",
                    "Aoife O’Mahony",
                    "Eike Mark Rinke",
                    "Sam Parsons",
                    "Graham Reid"
                ],
                "alt_related_terms": [
                    null
                ],
                "language": "english",
                "aliases": [
                    "/glossary/content_validity"
                ]
            },
            {
                "type": "glossary",
                "title": "Contribution",
                "definition": "",
                "related_terms": [
                    "authorship",
                    "CRediT",
                    "Semantometrics"
                ],
                "references": "Knoth, P., & Herrmannova, D. (2014). Towards semantometrics: A new semantic similarity based measure for assessing a research publication’s contribution. D-Lib Magazine, 20(11), 8. https://doi.org/10.1045/november2014-knoth\n\nLarivière, V., Desrochers, N., Macaluso, B., Mongeon, P., Paul-Hus, A., & Sugimoto, C. R. (2016). Contributorship and division of labor in knowledge production. Social Studies of Science, 46(3), 417–435. https://doi.org/10.1177/0306312716650046\n\nHolcombe, A. O. (2019). Contributorship, not authorship: Use CRediT to indicate who did what. Publications, 7(3), 48. https://doi.org/10.3390/publications7030048",
                "drafted_by": [
                    "Micah Vandegrift"
                ],
                "reviewed_by": [
                    "Jamie P. Cockcroft",
                    "Dominik Kiersz",
                    "Michele C. Lim",
                    "Leticia Micheli",
                    "Sam Parsons",
                    "Gerald Vineyard"
                ],
                "alt_related_terms": [
                    null
                ],
                "language": "english",
                "aliases": [
                    "/glossary/contribution"
                ]
            },
            {
                "type": "glossary",
                "title": "Corrigendum",
                "definition": "A corrigendum (pl. corrigenda, Latin: 'to correct') documents one or multiple errors within a published work that do not alter the central claim or conclusions and thus does not rise to the standard of requiring a retraction of the work. Corrigenda are typically available alongside the original work to aid transparency. Some publishers refer to this document as an erratum (pl. errata, Latin: 'error'), while others draw a distinction between the two (corrigenda as author-errors and errata as publisher-errors).",
                "related_terms": [
                    "Correction",
                    "Errata",
                    "Retraction"
                ],
                "references": "Anon. (2006). Correction or retraction? In Nature (Vol. 444, pp. 123–124). https://doi.org/10.1038/444123b",
                "drafted_by": [
                    "Charlotte R. Pennington"
                ],
                "reviewed_by": [
                    "Bradley Baker",
                    "Nick Ballou",
                    "Wanyin Li",
                    "Adam Parker",
                    "Emily A. Williams"
                ],
                "alt_related_terms": [
                    null
                ],
                "language": "english",
                "aliases": [
                    "/glossary/corrigendum"
                ]
            },
            {
                "type": "glossary",
                "title": "Co-production",
                "definition": "An approach to research where stakeholders who are not traditionally involved in the research process are empowered to collaborate, either at the start of the project or throughout the research lifecycle. For example, co-produced health research may involve health professionals and patients, while co-produced education research may involve teaching staff and pupils/students. This is motivated by principles such as respecting and valuing the experiences of non-researchers, addressing power dynamics, and building mutually beneficial relationships.",
                "related_terms": [
                    "Citizen science",
                    "Collaboration",
                    "Collaborative research",
                    "Crowd science",
                    "Engaged scholarship",
                    "Integrated Knowledge Translation (IKT)",
                    "Mode 2 of knowledge production",
                    "Participatory research",
                    "Patient and Public Involvement (PPI)"
                ],
                "references": "Filipe, A., Renedo, A., & Marston, C. (2017). The co-production of what? Knowledge, values, and social relations in health care. PLoS Biology, 15(5), e2001403. https://doi.org/10.1371/journal.pbio.2001403\n\nGraham, I. D., McCutcheon, C., & Kothari, A. (2019). Exploring the frontiers of research co-production: the Integrated Knowledge Translation Research Network concept papers. Health Research Policy and Systems, 17, 88. https://doi.org/10.1186/s12961-019-0501-7\n\nNIHR Guidance on Co-Producing a Research Project. (2021). https://www.learningforinvolvement.org.uk/?opportunity=nihr-guidance-on-co-producing-a-research-project\n\nCo-Production Collective. (n.d.). Our Approach. Co-Production Collective. https://www.coproductioncollective.co.uk/what-is-co-production/our-approach",
                "drafted_by": [
                    "Emma Norris"
                ],
                "reviewed_by": [
                    "Gisela H. Govaart",
                    "Magdalena Grose-Hodge",
                    "Helena Hartmann;Charlotte R. Pennington",
                    "Sonia Rishi",
                    "Emily A. Williams"
                ],
                "alt_related_terms": [
                    null
                ],
                "language": "english",
                "aliases": [
                    "/glossary/co_production"
                ]
            },
            {
                "type": "glossary",
                "title": "Creative Commons (CC) license",
                "definition": "A set of free and easy-to-use copyright licences that define the rights of the authors and users of open data and materials in a standardized way. CC licenses enable authors or creators to share copyright-law-protected work with the public and come in different varieties with more or less clauses. For example, the Attribution-NonCommercial-ShareAlike 4.0 International (CC BY-NC-SA 4.0) allows you to share and adapt the material, under the condition that you; give credit to the original creators, indicate if changes were made, and share under the same license as the original, and you cannot use the material for commercial purposes.",
                "related_terms": [
                    "Copyright",
                    "Licence **Alternative definition:** (if applicable) Creative Commons is an international nonprofit organization that provides Creative Commons licences, with the goal to minimize legal obstacles to the sharing of knowledge and creativity."
                ],
                "references": "Anon. (n.d.). About CC Licenses. Retrieved from https://creativecommons.org/about/cclicenses/",
                "drafted_by": [
                    "Tina Lonsdorf"
                ],
                "reviewed_by": [
                    "Adrien Fillon",
                    "Gisela H. Govaart",
                    "Annalise A. LaPlume",
                    "Sam Parsons",
                    "Charlotte R. Pennington"
                ],
                "alt_related_terms": [
                    null
                ],
                "language": "english",
                "aliases": [
                    "/glossary/creative_commons"
                ]
            },
            {
                "type": "glossary",
                "title": "Credibility revolution",
                "definition": "The problems and the solutions resulting from a growing distrust in scientific findings, following concerns about the credibility of scientific claims (e.g., low replicability). The term has been proposed as a more positive alternative to the term replicability crisis, and includes the many solutions to improve the credibility of research, such as preregistration, transparency, and replication.",
                "related_terms": [
                    "Credibility of scientific claims",
                    "High standards of evidence",
                    "Openness",
                    "Open Science;Reproducibility crisis (aka Replicability or replication crisis)",
                    "Transparency"
                ],
                "references": "Angrist, J. D., & Pischke, J. S. (2010). The credibility revolution in empirical economics: How better research design is taking the con out of econometrics. Journal of Economic Perspectives, 24, 3–30. https://doi.org/10.1257/jep.24.2.3\n\nVazire, S. (2018). Implications of the Credibility Revolution for Productivity, Creativity, and Progress. Perspectives on Psychological Science, 13(4), 411–417. https://doi.org/10.1177/1745691617751884\n\nVazire, S., Schiavone, S. R., & Bottesini, J. G. (2020). Credibility Beyond Replicability: Improving the Four Validities in Psychological Science. https://doi.org/10.31234/osf.io/bu4d3",
                "drafted_by": [
                    "Tamara Kalandadze"
                ],
                "reviewed_by": [
                    "Bradley Baker",
                    "Mahmoud Elsherif",
                    "Helena Hartmann",
                    "Kai Krautter",
                    "Annalise A. LaPlume",
                    "Oscar Lecuona",
                    "Charlotte R. Pennington",
                    "Robert Ross",
                    "Tobias Wingen",
                    "Flávio Azevedo"
                ],
                "alt_related_terms": [
                    null
                ],
                "language": "english",
                "aliases": [
                    "/glossary/credibility_revolution"
                ]
            },
            {
                "type": "glossary",
                "title": "Creative destruction approach to replication",
                "definition": "Replication efforts should seek not just to support or question the original findings, but also to replace them with revised, stronger theories with greater explanatory power. This approach therefore involves ‘pruning’ existing theories, comparing all the alternative theories, and making replication efforts more generative and engaged in theory-building (Tierney et al. 2020, 2021).",
                "related_terms": [
                    "Crowdsourced research",
                    "Falsification",
                    "Replication",
                    "Theory"
                ],
                "references": "Tierney, W., Hardy III, J. H., Ebersole, C. R., Leavitt, K., Viganola, D., Clemente, E. G., & others. (2020). Creative destruction in science. Organizational Behavior and Human Decision Processes, 161, 291–309. https://doi.org/10.1016/j.obhdp.2020.07.002\n\nTierney, W., Hardy III, J., Ebersole, C. R., Viganola, D., Clemente, E. G., Gordon, M., & others. (2021). A creative destruction approach to replication: Implicit work and sex morality across cultures. Journal of Experimental Social Psychology, 93, 104060. https://doi.org/10.1016/j.jesp.2020.104060",
                "drafted_by": [
                    "Mahmoud Elsherif"
                ],
                "reviewed_by": [
                    "Magdalena Grose-Hodge",
                    "Aoife O’Mahony",
                    "Adam Parker",
                    "Charlotte R. Pennington",
                    "Sonia Rishi",
                    "Beatrice Valentini"
                ],
                "alt_related_terms": [
                    null
                ],
                "language": "english",
                "aliases": [
                    "/glossary/creative_destruction_approach_to_replication"
                ]
            },
            {
                "type": "glossary",
                "title": "CRediT",
                "definition": "The Contributor Roles Taxonomy (CRediT) is a high-level taxonomy used to indicate the roles typically adopted by contributors to scientific scholarly output. There are currently 14 roles that describe each contributor’s specific contribution to the scholarly output. They can be assigned multiple times to different authors and one author can also be assigned multiple roles. CRediT includes the following roles: Conceptualization, Data curation, Formal Analysis, Funding acquisition, Investigation, Methodology, Project administration, Resources, Software, Supervision, Validation, Visualization, Writing – original draft, Writing – review & editing. A description of the different roles can be found in the work of Brand et al., (2015).",
                "related_terms": [
                    "Authorship",
                    "Contributions"
                ],
                "references": "Brand, A., Allen, L., Altman, M., Hlava, M., & Scott, J. (2015). Beyond authorship: attribution, contribution, collaboration, and credit. Learned Publishing, 28(2), 151–155. https://doi.org/10.1087/20150211\n\nHolcombe, A. O. (2019). Contributorship, not authorship: Use CRediT to indicate who did what. Publications, 7(3), 48. https://doi.org/10.3390/publications7030048",
                "drafted_by": [
                    "Sam Parsons"
                ],
                "reviewed_by": [
                    "Myriam A. Baum",
                    "Matt Jaquiery",
                    "Tamara Kalandadze",
                    "Connor Keating",
                    "Charlotte R. Pennington",
                    "Yuki Yamada",
                    "Flávio Azevedo"
                ],
                "alt_related_terms": [
                    null
                ],
                "language": "english",
                "aliases": [
                    "/glossary/credit"
                ]
            },
            {
                "type": "glossary",
                "title": "Criterion validity",
                "definition": "The degree to which a measure corresponds to other valid measures of the same concept. Criterion validity is usually established by calculating regression coefficients or bivariate correlations estimating the direction and strength of relation between test measure and criterion measure. It is often confused with *construct validity* although it differs from it in intent (merely predictive rather than theoretical) and interest (predicting an observable outcome rather than a latent construct). Unreliability in either test or criterion scores usually diminishes criterion validity. Also called criterion-related or concrete validity.",
                "related_terms": [
                    "Construct validity",
                    "Validity"
                ],
                "references": "DeVellis, R. F. (2017). Scale development: Theory and applications (4th ed.). Sage.\n\nDrost, E. A. (2011). Validity and reliability in social science research. Education Research and Perspectives, 38(1), 105–123.",
                "drafted_by": [
                    "Annalise A. LaPlume"
                ],
                "reviewed_by": [
                    "Helena Hartmann",
                    "Kai Krautter",
                    "Sam Parsons",
                    "Eike Mark Rinke"
                ],
                "alt_related_terms": [
                    null
                ],
                "language": "english",
                "aliases": [
                    "/glossary/criterion_validity"
                ]
            },
            {
                "type": "glossary",
                "title": "Crowdsourced Research",
                "definition": "Crowdsourced research is a model of the social organisation of research as a large-scale collaboration in which one or more research projects are conducted by multiple teams in an independent yet coordinated manner. Crowdsourced research aims at achieving efficiency and scalability gains by pooling resources, promoting transparency and social inclusion, as well as increasing the rigor, reliability, and trustworthiness by enhancing statistical power and mutual social vetting. It stands in contrast to the traditional model of academic research production, which is dominated by the independent work of individual or small groups of researchers (‘small science’). Examples of crowdsourced research include so-called ‘many labs replication’ studies (Klein et al., 2018), ‘many analysts, one dataset’ studies (Silberzahn et al., 2018), distributive collaborative networks (Moshontz et al., 2018\\) and open collaborative writing projects such as Massively Open Online Papers (MOOPs) (Himmelstein et al., 2019; Tennant et al., 2019). Alternatively, crowdsourced research can refer to the use of a large number of research “crowdworkers” in data collection hired through online labor markets like Amazon Mechanical Turk or Prolific, for example in content analysis (Benoit et al., 2016; Lind et al., 2017\\) or experimental research (Peer et al., 2017). Crowdsourced research that is both open for participation and open through shared intermediate outputs has been referred to as *crowd science* (Franzoni & Sauermann, 2014).",
                "related_terms": [
                    "Citizen science",
                    "Collaboration",
                    "Crowdsourcing",
                    "Team science"
                ],
                "references": "Benoit, K., Conway, D., Lauderdale, B. E., Laver, M., & Mikhaylov, S. (2016). Crowd-sourced text analysis: Reproducible and agile production of political data. American Political Science Review, 110(2), 278–295. https://doi.org/10.1017/S0003055416000058\n\nBreznau, N. (2021). I saw you in the crowd: Credibility, reproducibility, and meta-utility. PS: Political Science & Politics, 54(2), 309–313. https://doi.org/10.1017/S1049096520000980\n\nFranzoni, C., & Sauermann, H. (2014). Crowd science: The organization of scientific research in open collaborative projects. Research Policy, 43(1), 1–20. https://doi.org/10.1016/j.respol.2013.07.005\n\nHimmelstein, D. S., Rubinetti, V., Slochower, D. R., Hu, D., Malladi, V. S., Greene, C. S., & Gitter, A. (2019). Open collaborative writing with Manubot. PLOS Computational Biology, 15(6), e1007128. https://doi.org/10.1371/journal.pcbi.1007128\n\nKlein, R. A., Vianello, M., Hasselman, F., Adams, B. G., Adams, R. B., Alper, S., & … Nosek, B. A. (2018). Many Labs 2: Investigating Variation in Replicability Across Samples and Settings. Advances in Methods and Practices in Psychological Science, 1(4), 443–490. https://doi.org/10.1177/2515245918810225\n\nLind, F., Gruber, M., & Boomgaarden, H. G. (2017). Content analysis by the crowd: Assessing the usability of crowdsourcing for coding latent constructs. Communication Methods and Measures, 11(3), 191–209. https://doi.org/10.1080/19312458.2017.1317338\n\nMoshontz, H., Campbell, L., Ebersole, C. R., IJzerman, H., Urry, H. L., Forscher, P. S., & Chartier, C. R. (2018). The Psychological Science Accelerator: Advancing psychology through a distributed collaborative network. Advances in Methods and Practices in Psychological Science, 1(4), 501–515. https://doi.org/10.1177/2515245918797607\n\nPeer, E., Brandimarte, L., Samat, S., & Acquisti, A. (2017). Beyond the Turk: Alternative platforms for crowdsourcing behavioral research. Journal of Experimental Social Psychology, 70, 153–163. https://doi.org/10.1016/j.jesp.2017.01.006\n\nSilberzahn, R., Uhlmann, E. L., Martin, D. P., Anselmi, P., Aust, F., Awtrey, E., Bahník, Š., Bai, F., Bannard, C., Bonnier, E., & others. (2018). Many Analysts, One Data Set: Making Transparent How Variations in Analytic Choices Affect Results. Advances in Methods and Practices in Psychological Science, 337–356. https://doi.org/10.1177/2515245917747646\n\nStewart, N., Chandler, J., & Paolacci, G. (2017). Crowdsourcing samples in cognitive science. Trends in Cognitive Sciences, 21(10), 736–748. https://doi.org/10.1016/j.tics.2017.06.007\n\nTennant, J., Bielczyk, N. Z., Cheplygina, V., Greshake Tzovaras, B., Hartgerink, C. H. J., Havemann, J., Masuzzo, P., & Steiner, T. (2019). Ten simple rules for researchers collaborating on Massively Open Online Papers (MOOPs). MetaArXiv. https://doi.org/10.31222/osf.io/et8ak\n\nUhlmann, E. L., Ebersole, C. R., Chartier, C. R., Errington, T. M., Kidwell, M. C., Lai, C. K., McCarthy, R. J., Riegelman, A., Silberzahn, R., & Nosek, B. A. (2019). Scientific utopia III: Crowdsourcing science. Perspectives on Psychological Science, 14(5), 711–733. https://doi.org/10.1177/1745691619850561\n\nWeek, C. (2021). What is Crowdsourcing? https://crowdsourcingweek.com/what-is-crowdsourcing/",
                "drafted_by": [
                    "Eike Mark Rinke"
                ],
                "reviewed_by": [
                    "Ali H. Al-Hoorie",
                    "Sam Parsons",
                    "Charlotte R. Pennington",
                    "Suzanne L. K. Stewart",
                    "Flávio Azevedo"
                ],
                "alt_related_terms": [
                    null
                ],
                "language": "english",
                "aliases": [
                    "/glossary/crowdsourced_research"
                ]
            },
            {
                "type": "glossary",
                "title": "Cultural taxation",
                "definition": "The additional labor expected or demanded of members of underrepresented or marginalized minority groups, particularly scholars of color. This labor often comes from service roles providing ethnic, cultural, or gender representation and diversity. These roles can be formal or informal, and are generally unrewarded or uncompensated. Such labor includes providing expertise on matters of diversity, educating members of majority groups, acting as a liaison to minority communities, and formal and informal roles as mentor and support system for minority students.",
                "related_terms": [
                    "Invisible labor",
                    "Power imbalances",
                    "Power relations"
                ],
                "references": "Joseph, T. D., & Hirshfield, L. E. (2011). `Why don’t you get somebody new to do it?’ Race and cultural taxation in the academy. Ethnic and Racial Studies, 34(1), 121–141. https://doi.org/10.1080/01419870.2010.496489\n\nLedgerwood, A., Hudson, S. T. J., Lewis, Jr., N. A., Maddox, K. B., Pickett, C., Remedios, J. D., & Wilkins, C. L. (2021). The Pandemic as a Portal: Reimagining Psychological Science as Truly Open and Inclusive. https://doi.org/10.31234/osf.io/gdzue\n\nPadilla, A. M. (1994). Research news and comment: Ethnic minority scholars; research, and mentoring: Current and future issues. Educational Researcher, 23(4), 24–27. https://doi.org/10.3102/0013189X023004024",
                "drafted_by": [
                    "Bradley Baker"
                ],
                "reviewed_by": [
                    "Helena Hartmann",
                    "Bethan Iley",
                    "Aoife O’Mahony",
                    "Charlotte R. Pennington",
                    "Flávio Azevedo"
                ],
                "alt_related_terms": [
                    null
                ],
                "language": "english",
                "aliases": [
                    "/glossary/cultural_taxation"
                ]
            },
            {
                "type": "glossary",
                "title": "Cumulative science",
                "definition": "Goal of any empirical science, it is the pursuit of “the construction of a cumulative base of knowledge upon which the future of the science may be built” (Curran, 2009, p. 1). The idea that science will create more complete and accurate theories as a function of the amount of evidence and data that has been collected. Cumulative science develops in gradual and incremental steps, as opposed to one abrupt discovery. While revolutionary science occurs scarcely, cumulative science is the most common form of science.",
                "related_terms": [
                    "Slow Science"
                ],
                "references": "Curran, P. J. (2009). The seemingly quixotic pursuit of a cumulative psychological science: Introduction to the special issue. Psychological Methods, 14(2), 77–80. https://doi.org/10.1037/a0015972\n\nd’Espagnat, B. (2008). Is science cumulative? A physicist viewpoint. In Rethinking Scientific Change and Theory Comparison (pp. 145–151). Springer. https://doi.org/10.1007/978-1-4020-6279-7_10\n\nKuhn, T. (1962). The Structure of Scientific Revolutions. University of Chicago Press.\n\nMischel, W. (2009). Becoming a Cumulative Science. Association for Psychological Science. https://www.psychologicalscience.org/observer/becoming-a-cumulative-science",
                "drafted_by": [
                    "Beatrice Valentini"
                ],
                "reviewed_by": [
                    "Sarah Ashcroft-Jones",
                    "Mahmoud Elsherif",
                    "Helena Hartmann",
                    "Oscar Lecuona",
                    "Wanyin Li",
                    "Sonia Rishi",
                    "Flávio Azevedo"
                ],
                "alt_related_terms": [
                    null
                ],
                "language": "english",
                "aliases": [
                    "/glossary/cumulative_science"
                ]
            },
            {
                "type": "glossary",
                "title": "Data Access and Research Transparency (DA-RT)",
                "definition": "Data Access and Research Transparency ([DA-RT](https://www.dartstatement.org/)) is an initiative aimed at increasing data access and research transparency in the social sciences. It is a multi-epistemic and multi-method initiative, created in 2014 by the Council of the American Political Science Association (APSA), to bolster the rigor of empirical social inquiry. In addition to other activities, DA-RT developed the Journal Editors' Transparency Statement (JETS), which requires subscribing journals to (a) making relevant data publicly available if the study is published, (b) following a strict data citation policy, (c) transparently describing the analytical procedures and, if possible, providing public access to analytical code, and (d) updating their journal style guides, codes of ethics to include improved data access and research transparency requirements.",
                "related_terms": [
                    "Accessibility",
                    "Data sharing",
                    "Replicability",
                    "Reproducibility"
                ],
                "references": "Carsey, T. M. (2014). Making DA-RT a reality. PS: Political Science & Politics, 47(1), 72–77. https://doi.org/10.1017/S1049096513001753\n\nMonroe, K. R. (2018). The rush to transparency: DA-RT and the potential dangers for qualitative research. Perspectives on Politics, 16(1), 141–148. https://doi.org/10.1017/S153759271700336X",
                "drafted_by": [
                    "Eike Mark Rinke"
                ],
                "reviewed_by": [
                    "Filip Dechterenko",
                    "Kai Krautter",
                    "Flávio Azevedo"
                ],
                "alt_related_terms": [
                    null
                ],
                "language": "english",
                "aliases": [
                    "/glossary/data_access_and_research_transparency"
                ]
            },
            {
                "type": "glossary",
                "title": "Data management plan (DMP)",
                "definition": "A structured document that describes the process of data acquisition, analysis, management and storage during a research project. It also describes data ownership and how the data will be preserved and shared during and upon completion of a project. Data management templates also provide guidance on how to make research data FAIR and where possible, openly available.",
                "related_terms": [
                    "Data archiving",
                    "Data sharing",
                    "Data storage",
                    "FAIR principles",
                    "Open data"
                ],
                "references": "Burnette, M., Williams, S., & Imker, H. (2016). From Plan to Action: Successful Data Management Plan Implementation in a Multidisciplinary Project. Journal of eScience Librarianship, 5(1), e1101. https://doi.org/10.7191/jeslib.2016.1101\n\nMichener, W. K. (2015). Ten simple rules for creating a good data management plan. PLoS Computational Biology, 11(10), e1004525. https://doi.org/10.1371/journal.pcbi.1004525",
                "drafted_by": [
                    "Dominique Roche"
                ],
                "reviewed_by": [
                    "Charlotte R. Pennington",
                    "Sam Parsons",
                    "Birgit Schmidt",
                    "Flávio Azevedo"
                ],
                "alt_related_terms": [
                    null
                ],
                "language": "english",
                "aliases": [
                    "/glossary/data_management_plan"
                ]
            },
            {
                "type": "glossary",
                "title": "Data sharing",
                "definition": "collection of practices, technologies, cultural elements and legal frameworks that are relevant to the practice of making data used for scholarly research available to other investigators. Gollwitzer et al. (2020) describe two types of data sharing: Type 1: Data that is necessary to reproduce the findings of a published research article. Type 2: data that have been collected in a research project but have not (or only partly) been analysed or reported after the completion of the project and are hence typically shared under a specified embargo period.",
                "related_terms": [
                    "FAIR principles",
                    "Open data"
                ],
                "references": "Abele-Brehm, A. E., Gollwitzer, M., Steinberg, U., & Schönbrodt, F. D. (2019). Attitudes toward open science and public data sharing. Social Psychology, 50, 252–260. https://doi.org/10.1027/1864-9335/a000384\n\nGollwitzer, M., Abele-Brehm, A., Fiebach, C., Ramthun, R., Scheel, A. M., Schönbrodt, F. D., & Steinberg, U. (2020). Data Management and Data Sharing in Psychological Science: Revision of the DGPs Recommendations.\n\nfor Data Sharing, S. C. (n.d.). What is data sharing? Retrieved 11 July 2021. https://eudatasharing.eu/what-data-sharing",
                "drafted_by": [
                    "Mahmoud Elsherif"
                ],
                "reviewed_by": [
                    "Helena Hartmann",
                    "Tina Lonsdorf",
                    "Charlotte R. Pennington",
                    "Timo Roettger",
                    "Flávio Azevedo"
                ],
                "alt_related_terms": [
                    null
                ],
                "language": "english",
                "aliases": [
                    "/glossary/data_sharing"
                ]
            },
            {
                "type": "glossary",
                "title": "Data visualisation",
                "definition": "Graphical representation of data or information. Data visualisation takes advantage of humans’ well-developed visual processing capacity to convey insight and communicate key information. Data visualisations often display the raw data, descriptive statistics, and/or inferential statistics.",
                "related_terms": [
                    "Figure",
                    "Graph",
                    "Plot"
                ],
                "references": "Healy, K. (2018). Data visualization: A practical introduction. Princeton University Press.\n\nTufte, E. R. (1983). The visual display of quantitative information. Graphics Press.",
                "drafted_by": [
                    "Bradley Baker"
                ],
                "reviewed_by": [
                    "Mahmoud Elsherif",
                    "Charlotte R. Pennington",
                    "Suzanne L. K. Stewart;"
                ],
                "alt_related_terms": [
                    null
                ],
                "language": "english",
                "aliases": [
                    "/glossary/data_visualisation"
                ]
            },
            {
                "type": "glossary",
                "title": "Decolonisation",
                "definition": "Coloniality can be described as the naturalisation of concepts such as imperialism, capitalism, and nationalism. Together these concepts can be thought of as a matrix of power (and power relations) that can be traced to the colonial period. Decoloniality seeks to break down and decentralize those power relations, with the aim to understand their persistence and to reconstruct the norms and values of a given domain. In an academic setting, decolonisation refers to the rethinking of the lens through which we teach, research, and co-exist, so that the lens generalises beyond Western-centred and colonial perspectives. Decolonising academia involves reconstructing the historical and cultural frameworks being used, redistributing a sense of belonging in universities, and empowering and including voices and knowledge types that have historically been excluded from academia. This is done when people engage with their past, present, and future whilst holding a perspective that is separate from the socially dominant perspective. Also, by including, not rejecting, an individuals’ internalised norms and taboos from the specific colony.",
                "related_terms": [
                    "Diversity",
                    "Equity",
                    "Inclusion"
                ],
                "references": "Albayrak, N. (2018). Diversity helps but decolonisation is the key to equality in higher education. Retrieved from https://lsepgcertcitl.wordpress.com/2018/04/16/diversity-helps-but-decolonisation-is-the-key-to-equality-in-higher-education/",
                "drafted_by": [
                    "Nihan Albayrak-Aydemir"
                ],
                "reviewed_by": [
                    "Sarah Ashcroft-Jones",
                    "Mahmoud Elsherif",
                    "Michele C. Lim",
                    "Emma Norris",
                    "Flávio Azevedo"
                ],
                "alt_related_terms": [
                    null
                ],
                "language": "english",
                "aliases": [
                    "/glossary/decolonisation"
                ]
            },
            {
                "type": "glossary",
                "title": "Demarcation criterion",
                "definition": "A criterion for distinguishing science from non-science which aims to indicate an optimal way for knowledge of the world to grow. In a Popperian approach, the demarcation criterion was falsifiability and the application of a falsificationist attitude. Alternative approaches include that of Kuhn, who believed that the criterion was puzzle solving with the aim of understanding nature, and Lakatos, who argued that science is marked by working within a progressive research programme.",
                "related_terms": [
                    "Hypothesis",
                    "Falsification"
                ],
                "references": "Dienes, Z. (2008). Understanding psychology as a science: An introduction to scientific and statistical inference. Macmillan International Higher Education.",
                "drafted_by": [
                    "Alaa AlDoh"
                ],
                "reviewed_by": [
                    "Bethan Iley",
                    "Sara Middleton"
                ],
                "alt_related_terms": [
                    null
                ],
                "language": "english",
                "aliases": [
                    "/glossary/demarcation_criterion"
                ]
            },
            {
                "type": "glossary",
                "title": "DOI (digital object identifier)",
                "definition": "Digital Object Identifiers (DOI) are alpha-numeric strings that can be assigned to any entity, including: publications (including preprints), materials, datasets, and feature films \\- the use of DOIs is not restricted to just scholarly or academic material. There are many different DOI registration agencies that operate DOIs, but the two that researchers would most likely encounter are [Crossref](https://www.crossref.org/) and [Datacite](https://datacite.org/).",
                "related_terms": [
                    "arXiv and BibTex",
                    "Crossref, Datacite, ISBN, ISO, ORCID",
                    "Permalink"
                ],
                "references": "Bilder, G. (2013). DOIs unambiguously and persistently identify published, trustworthy, citable online scholarly literature. Right? https://www.crossref.org/blog/dois-unambiguously-and-persistently-identify-published-trustworthy-citable-online-scholarly-literature-right/\n\nMorgan, C. (1998). The DOI (Digital Object Identifier). Serials, 11(1), 47–51. http://doi.org/10.1629/1147\n\nAnon. (2019). The DOI Handbook.",
                "drafted_by": [
                    "Tina Lonsdorf"
                ],
                "reviewed_by": [
                    "Ashley Blake",
                    "Helena Hartmann",
                    "Sam Parsons",
                    "Charlotte R. Pennington",
                    "Flávio Azevedo"
                ],
                "alt_related_terms": [
                    null
                ],
                "language": "english",
                "aliases": [
                    "/glossary/doi"
                ]
            },
            {
                "type": "glossary",
                "title": "Double-blind peer review",
                "definition": "Evaluation of research products by qualified experts where both the author(s) and reviewer(s) are anonymous to each other. “This approach conceals the identity of the authors and their affiliations from reviewers and would, in theory, remove biases of professional reputation, gender, race, and institutional affiliation, allowing the reviewer to avoid bias and to focus on the manuscript’s merit alone.” (Tvina et al., 2019, 1082). Like all types of peer-review, double-blind peer review is not without flaws. Anonymity can be difficult, if not impossible, to achieve for certain researchers working in a niche area.",
                "related_terms": [
                    "Ad hominem bias",
                    "Affiliation bias",
                    "Anonymous review",
                    "Masked review",
                    "Open peer review",
                    "Peer review",
                    "Single-blind peer review",
                    "Traditional peer review",
                    "Triple-Blind peer review"
                ],
                "references": "Largent, E. A., & Snodgrass, R. T. (2016). Blind peer review by academic journals. In C. T. Robertson & A. S. Kesselheim (Eds.), Blinding as a solution to bias: Strengthening biomedical science, forensic science, and law (pp. 75–95). Academic Press. https://doi.org/10.1016/B978-0-12-802460-7.00005-X\n\nTvina, A., Spellecy, R., & Palatnik, A. (2019). Bias in the peer review process: can we do better? Obstetrics & Gynecology, 133(6), 1081–1083. https://doi.org/10.1097/AOG.0000000000003260",
                "drafted_by": [
                    "Mahmoud Elsherif"
                ],
                "reviewed_by": [
                    "Bradley Baker",
                    "Helena Hartmann",
                    "Meng Liu",
                    "Emma Norris"
                ],
                "alt_related_terms": [
                    null
                ],
                "language": "english",
                "aliases": [
                    "/glossary/double_blind_peer_review"
                ]
            },
            {
                "type": "glossary",
                "title": "Double consciousness",
                "definition": "An identity confusion, as the individual feels like they have two distinct identities. One is to assimilate to the dominant culture at university when the individual is with colleagues and professors, while the other is when the individual is with their families. This continuous shift may cause a lack of certainty about the individual’s identity and a belief that the individual does not fully belong anywhere. This lack of belonging can lead to poor social integration within the academic culture that can manifest in less opportunities and more mental health issues in the individual (Rubin, 2021; Rubin et al., 2019).",
                "related_terms": [
                    "Social class",
                    "Social integration"
                ],
                "references": "Albayrak, N., & Okoroji, C. (2019). Facing the challenges of postgraduate study as a minority student. A Guide for Psychology Postgraduates, 63.\n\nDu Bois, W. E. B. (1968). The souls of black folk; essays and sketches. Johnson Reprint Corp.\n\nGilroy, P. (1993). The black Atlantic: Modernity and double consciousness. Harvard University Press.",
                "drafted_by": [
                    "Nihan Albayrak-Aydemir"
                ],
                "reviewed_by": [
                    "Mahmoud Elsherif",
                    "Wanyin Li",
                    "Michele C. Lim",
                    "Adam Parker"
                ],
                "alt_related_terms": [
                    null
                ],
                "language": "english",
                "aliases": [
                    "/glossary/double_consciousness"
                ]
            },
            {
                "type": "glossary",
                "title": "DORA",
                "definition": "The San Francisco Declaration on Research Assessment (DORA) is a global initiative aiming to reduce dependence on journal-based metrics (e.g. journal impact factor and citation counts) and, instead, promote a culture which emphasises the intrinsic value of research. The DORA declaration targets research funders, publishers, research institutes and researchers and signing it represents a commitment to aligning research practices and procedures with the declaration’s principles.",
                "related_terms": [
                    "Generalizability",
                    "Journal Impact Factor",
                    "Open Science"
                ],
                "references": "Health Research Board. (n.d.). Declaration on Research Assessment. Retrieved from https://www.hrb.ie/funding/funding-schemes/before-you-apply/how-we-assess-applications/declaration-on-research-assessment/",
                "drafted_by": [
                    "Aoife O’Mahony"
                ],
                "reviewed_by": [
                    "Connor Keating",
                    "Charlotte R. Pennington"
                ],
                "alt_related_terms": [
                    null
                ],
                "language": "english",
                "aliases": [
                    "/glossary/dora"
                ]
            },
            {
                "type": "glossary",
                "title": "Direct replication",
                "definition": "As ‘direct replication’ does not have a widely-agreed technical meaning nor there is no clear cut distinction between a direct and conceptual replication, below we list several contributions towards a consensus. Rather than debating the ‘exactness’ of a replication, it is more helpful to discuss the relevant differences between a replication and its target, and their implications for the reliability and generality of the target’s results. Generally, direct replication refers to a new data collection that attempts to replicate original studies’ methods as closely as possible. A replication attempt that “seek(s) to duplicate the necessary elements that produced the original finding.” (Cruwell et al., 2019; p.243). The purpose of a direct replication can be to identify type 1 errors and/or experimenter effects, determine the replicability of an effect using the same or improved practices, or to create more specific estimates of effect size (Hűffmeier et al., 2016). Directness of replication is a continuum between repeating specific observations (data) and observing generalised effects (phenomena). How closely a replication replicates an original study is often a matter for debate, often with differences being cited as hidden moderators of effects. Furthermore, there can be debate over the relevant importance of technical equivalence (i.e., using identical materials) versus psychological equivalence (i.e., realizing the identical psychological conditions) to the original study (Schwarz and Strack, 2014). For example, consider a study on Trust in the US- President conducted in 2018\\. A technical equivalent replication would use Trump as stimulus (he was president in 2018\\) a psychological equivalent study would use Biden (he is the current president).",
                "related_terms": [
                    "close replication",
                    "Conceptual replication",
                    "exact replication",
                    "hidden moderators"
                ],
                "references": "Crüwell, S., van Doorn, J., Etz, A., Makel, M. C., Moshontz, H., Niebaum, J. C., Orben, A., Parsons, S., & Schulte-Mecklenbeck, M. (2019). Seven Easy Steps to Open Science: An Annotated Reading List. Zeitschrift Für Psychologie, 227(4), 237–248. https://doi.org/10.1027/2151-2604/a000387\n\nHüffmeier, J., Mazei, J., & Schultze, T. (2016). Reconceptualizing replication as a sequence of different studies: A replication typology. Journal of Experimental Social Psychology, 66, 81–92. https://doi.org/10.1016/j.jesp.2015.09.009\n\nLeBel, E. P., Vanpaemel, W., Cheung, I., & Campbell, L. (2017). A brief guide to evaluate replications. Meta-Psychology, 3. https://doi.org/10.15626/MP.2018.843\n\nSchwarz, N., & Strack, F. (2014). Does Merely Going Through the Same Moves Make for a “Direct” Replication?: Concepts, Contexts, and Operationalizations. Social Psychology, 45(4), 305–306.",
                "drafted_by": [
                    "Mahmoud Elsherif (original); Thomas Rhys Evans (alternative); Tina Lonsdorf (alternative)"
                ],
                "reviewed_by": [
                    "Beatrix Arendt",
                    "Adrien Fillon",
                    "Matt Jaquiery",
                    "Charlotte R. Pennington",
                    "Graham Reid",
                    "Lisa Spitzer",
                    "Tobias Wingen",
                    "Flávio Azevedo"
                ],
                "alt_related_terms": [
                    null
                ],
                "language": "english",
                "aliases": [
                    "/glossary/direct_replication"
                ]
            },
            {
                "type": "glossary",
                "title": "Diversity",
                "definition": "Diversity refers to between-person (i.e., interindividual) variation in humans, e.g. ability, age, beliefs, cognition, country, disability, ethnicity, gender, language, race, religion or sexual orientation. Diversity can refer to diversity of researchers (who do the research), the diversity of participant samples (who is included in the study), and diversity of perspectives (the views and beliefs researchers bring into their work; Syed & Kathawalla, 2020).",
                "related_terms": [
                    "Bropenscience",
                    "BIZARRE",
                    "Decolonisation",
                    "Double Consciousness",
                    "Equity",
                    "Inclusion",
                    "STRANGE",
                    "WEIRD"
                ],
                "references": "Syed, M., & Kathawalla, U. (2020). Cultural Psychology, Diversity, and Representation in Open Science. https://doi.org/10.31234/osf.io/t7hp2",
                "drafted_by": [
                    "Ryan Millager; Mariella Paul"
                ],
                "reviewed_by": [
                    "Nihan Albayrak-Aydemir",
                    "Mahmoud Elsherif",
                    "Helena Hartmann",
                    "Madeleine Ingham",
                    "Annalise A. LaPlume",
                    "Wanyin Li",
                    "Charlotte R. Pennington",
                    "Olly Robertson",
                    "Flávio Azevedo"
                ],
                "alt_related_terms": [
                    null
                ],
                "language": "english",
                "aliases": [
                    "/glossary/diversity"
                ]
            },
            {
                "type": "glossary",
                "title": "Early career researchers (ECRs)",
                "definition": "A label given to researchers who “range from senior doctoral students to postdoctoral workers who may have up to 10 years postdoctoral education; the latter group may therefore include early career or junior academics” (Eley et al., 2012, p. 3). What specifically (e.g. age, time since PhD inclusive or exclusive of career breaks and leave, title, funding awarded) constitutes an ECR can vary across funding bodies, academic organisations, and countries.",
                "related_terms": [
                    "Early Career Investigator"
                ],
                "references": "Bazeley, P. (2003). Defining “Early Career” in Research. Higher Education, 45, 257–279. https://doi.org/10.1023/A:1022698529612\n\nEley, A. R. (2012). Becoming a successful early career researcher. Routledge. Retrieved from http://www.worldcat.org/oclc/934369360\n\nPownall, M., Talbot, C. V., Henschel, A., Lautarescu, A., Lloyd, K. E., Hartmann, H., Darda, K. M., Tang, K. T. Y., Carmichael-Murphy, P., & Siegel, J. A. (2021). Navigating Open Science as Early Career Feminist Researchers. Psychology of Women Quarterly, 45(4), 526–539. https://doi.org/10.1177/03616843211029255 Retrieved from https://journals.sagepub.com/doi/10.1177/03616843211029255",
                "drafted_by": [
                    "Micah Vandegrift"
                ],
                "reviewed_by": [
                    "Thomas Rhys Evans",
                    "Sam Parsons",
                    "Olly Robertson",
                    "Suzanne L. K. Stewart",
                    "Flávio Azevedo"
                ],
                "alt_related_terms": [
                    null
                ],
                "language": "english",
                "aliases": [
                    "/glossary/early_career_researchers"
                ]
            },
            {
                "type": "glossary",
                "title": "Economic and societal impact",
                "definition": "The contribution a research item makes to the broader economy and society. It also captures the benefits of research to individuals, organisations, and/or nations.",
                "related_terms": [
                    "Academic Impact"
                ],
                "references": "Economic, & Council, S. R. (n.d.). What is impact? Retrieved 8 July 2021. https://esrc.ukri.org/research/impact-toolkit/what-is-impact/",
                "drafted_by": [
                    "Adam Parker"
                ],
                "reviewed_by": [
                    "Helena Hartmann",
                    "Aoife O’Mahony",
                    "Charlotte R. Pennington"
                ],
                "alt_related_terms": [
                    null
                ],
                "language": "english",
                "aliases": [
                    "/glossary/economic_and_societal_impact"
                ]
            },
            {
                "type": "glossary",
                "title": "Embargo Period",
                "definition": "Applied to Open Scholarship, in academic publishing, the period of time after an article has been published and before it can be made available as Open Access. If an author decides to self-archive their article (e.g., in an Open Access repository) they need to observe any embargo period a publisher might have in place. Embargo periods vary from instantaneous up to 48 months, with 6 and 12 months being common (Laakso & Björk, 2013). Embargo periods may also apply to pre-registrations, materials, and data, when authors decide to only make these available to the public after a certain period of time, for instance upon publication or even later when they have additional publication plans and want to avoid being scooped (Klein et al., 2018).",
                "related_terms": [
                    "Open access",
                    "Paywall",
                    "Preprint"
                ],
                "references": "Klein, O., Hardwicke, T. E., Aust, F., Breuer, J., Danielsson, H., Mohr, A. H., IJzerman, H., Nilsonne, G., Vanpaemel, W., & Frank, M. C. (2018). A practical guide for transparency in psychological science. Collabra: Psychology, 4(1), 20. https://doi.org/10.1525/collabra.158\n\nLaakso, M., & Björk, B. C. (2013). Delayed open access: An overlooked high‐impact category of openly available scientific literature. Journal of the American Society for Information Science and Technology, 64(7), 1323–1329.\n\nEmbargo (academic publishing). (2021). https://en.wikipedia.org/w/index.php?title=Embargo_(academic_publishing)&oldid=1016895567",
                "drafted_by": [
                    "Aleksandra Lazić"
                ],
                "reviewed_by": [
                    "Bradley Baker",
                    "Adam Parker",
                    "Sam Parsons",
                    "Steven Verheyen",
                    "Flávio Azevedo"
                ],
                "alt_related_terms": [
                    null
                ],
                "language": "english",
                "aliases": [
                    "/glossary/embargo_period"
                ]
            },
            {
                "type": "glossary",
                "title": "Epistemic uncertainty",
                "definition": "Systematic uncertainty due to limited data, measurement precision, model or process specification, or lack of knowledge. That is, uncertainty due to lack of knowledge that could, in theory, be reduced through conducting additional research to increase understanding. Such uncertainty is said to be personal, since knowledge differs across scientists, and temporary since it can change as new data become available.",
                "related_terms": [
                    "Aleatoric uncertainty",
                    "Knightian uncertainty"
                ],
                "references": "Der Kiureghian, A., & Ditlevsen, O. (2009). Aleatory or epistemic? Does it matter? Structural Safety, 31(2), 105–112. https://doi.org/10.1016/j.strusafe.2008.06.020\n\nFerson, S., Joslyn, C. A., Helton, J. C., Oberkampf, W. L., & Sentz, K. (2004). Summary from the epistemic uncertainty workshop: consensus amid diversity. Reliability Engineering & System Safety, 85(1–3), 355–369. https://doi.org/10.1016/j.ress.2004.03.023",
                "drafted_by": [
                    "Bradley Baker"
                ],
                "reviewed_by": [
                    "Jamie P. Cockcroft",
                    "Elizabeth Collins",
                    "Charlotte R. Pennington",
                    "Graham Reid"
                ],
                "alt_related_terms": [
                    null
                ],
                "language": "english",
                "aliases": [
                    "/glossary/epistemic_uncertainty"
                ]
            },
            {
                "type": "glossary",
                "title": "Epistemology",
                "definition": "Alongside ethics, logic, and metaphysics, epistemology is one of the four main branches of philosophy. Epistemology is largely concerned with nature, origin, and scope of knowledge, as well as the rationality of beliefs.",
                "related_terms": [
                    "Meta-science or Meta-research ",
                    "Ontology (Artificial Intelligence)"
                ],
                "references": "Steup, M., & Neta, R. (2020). Epistemology. Stanford Encyclopedia of Philosophy. https://plato.stanford.edu/entries/epistemology/",
                "drafted_by": [
                    "Amélie Beffara Bret"
                ],
                "reviewed_by": [
                    "Emma Norris",
                    "Adam Parker",
                    "Robert M Ross",
                    "Steven Verheyen"
                ],
                "alt_related_terms": [
                    null
                ],
                "language": "english",
                "aliases": [
                    "/glossary/epistemology"
                ]
            },
            {
                "type": "glossary",
                "title": "Equity",
                "definition": "Different individuals have different starting positions (cf. “opportunity gaps”) and needs. Whereas equal treatment focuses on treating all individuals *equally*, equitable treatment aims to level the playing field by actively increasing opportunities for under-represented minorities. Equitable treatment aims to attain equality through “fairness”: taking into account different needs for support for different individuals, instead of focusing merely on the needs of the majority.",
                "related_terms": [
                    "Diversity",
                    "Equality",
                    "Fairness",
                    "Inclusion",
                    "Social justice"
                ],
                "references": "Albayrak-Aydemir, N. (2020). The hidden costs of being a scholar from the global south. Retrieved from https://blogs.lse.ac.uk/highereducation/2020/02/20/the-hidden-costs-of-being-a-scholar-from-the-global-south/\n\nPosselt, J. R. (2020). Equity in Science: Representation, Culture, and the Dynamics of Change in Graduate Education. Stanford University Press. https://books.google.de/books?id=2CjwDwAAQBAJ",
                "drafted_by": [
                    "Gisela H. Govaart"
                ],
                "reviewed_by": [
                    "Nihan Albayrak-Aydemir",
                    "Mahmoud Elsherif",
                    "Ryan Millager",
                    "Charlotte R. Pennington",
                    "Beatrice Valentini",
                    "Flávio Azevedo"
                ],
                "alt_related_terms": [
                    null
                ],
                "language": "english",
                "aliases": [
                    "/glossary/equity"
                ]
            },
            {
                "type": "glossary",
                "title": "Equivalence Testing",
                "definition": "Equivalence tests statistically assess the null hypothesis that a given effect exceeds a minimum criterion to be considered meaningful. Thus, rejection of the null hypothesis provides evidence of a lack of (meaningful) effect. Based upon frequentist statistics, equivalence tests work by specifying equivalence bounds: a lower and upper value that reflect the smallest effect size of interest. Two one-sided *t*\\-tests are then conducted against each of these equivalence bounds to assess whether effects that are deemed meaningful can be *rejected* (see Schuirmann, 1972; Lakens et al., 2018; 2020).",
                "related_terms": [
                    "Equivalence bounds",
                    "Falsification",
                    "Frequentist analyses",
                    "Inference by confidence intervals",
                    "Null Hypothesis Significance Testing (NHST)",
                    "Smallest effect size of interest (SESOI)",
                    "TOSTER",
                    "TOST procedure."
                ],
                "references": "Lakens, D., Scheel, A. M., & Isager, P. M. (2018). Equivalence testing for psychological research: A tutorial. Advances in Methods and Practices in Psychological Science, 1(2), 259–269. https://doi.org/10.1177/2515245918770963\n\nLakens, D., McLatchie, N., Isager, P. M., Scheel, A. M., & Dienes, Z. (2020). Improving inferences about null effects with Bayes factors and equivalence tests. The Journals of Gerontology: Series B, 75(1), 45–57. https://doi.org/10.1093/geronb/gby065\n\nSchuirmann, D. J. (1987). A comparison of the two one-sided tests procedure and the power approach for assessing the equivalence of average bioavailability. Journal of Pharmacokinetics and Biopharmaceutics, 15, 657–680. https://doi.org/10.1007/BF01068419",
                "drafted_by": [
                    "Charlotte R. Pennington"
                ],
                "reviewed_by": [
                    "Bradley Baker",
                    "James E. Bartlett",
                    "Jamie P. Cockcroft",
                    "Tobias Wingen",
                    "Flávio Azevedo"
                ],
                "alt_related_terms": [
                    null
                ],
                "language": "english",
                "aliases": [
                    "/glossary/equivalence_testing"
                ]
            },
            {
                "type": "glossary",
                "title": "Error detection",
                "definition": "Broadly refers to examining research data and manuscripts for mistakes or inconsistencies in reporting. Commonly discussed approaches include: checking inconsistencies in descriptive statistics (e.g. summary statistics that are not possible given the sample size and measure characteristics; Brown & Heathers, 2017; Heathers et al. 2018), inconsistencies in reported statistics (e.g. *p*\\-values that do not match the reported *F* statistics and accompanying degrees of freedom; Epskamp, & Nuijten, 2016; Nuijten et al. 2016), and image manipulation (Bik et al., 2016). Error detection is one motivation for data and analysis code to be openly available, so that peer review can confirm a manuscript’s findings, or if already published, the record can be corrected. Detected errors can result in corrections or retractions of published articles, though these actions are often delayed, long after erroneous findings have influenced and impacted further research.",
                "related_terms": [
                    "Research integrity",
                    "correction",
                    "retraction"
                ],
                "references": "Bik, E. M., Casadevall, A., & Fang, F. C. (2016). The prevalence of inappropriate image duplication in biomedical research publications. MBio, 7(3), e00809-16.\n\nBrown, N. J., & Heathers, J. A. (2017). The grim test: A simple technique detects numerous anomalies in the reporting of results in psychology. Social Psychological and Personality Science, 8(4), 363–369.\n\nEpskamp, S., & Nuijten, M. B. (2016). statcheck: Extract statistics from articles and recompute p values. Retrieved from http://CRAN.R-project.org/package=statcheck\n\nHeathers, J. A., Anaya, J., van der Zee, T., & Brown, N. J. (2018). Recovering data from summary statistics: Sample Parameter Reconstruction via Iterative TEchniques (SPRITE). PeerJ Preprints, 6, e26968v1. https://doi.org/10.7287/peerj.preprints.26968v1\n\nNuijten, M. B., Hartgerink, C. H., van Assen, M. A., Epskamp, S., & Wicherts, J. M. (2016). The prevalence of statistical reporting errors in psychology (1985–2013). Behavior Research Methods, 48(4), 1205–1226.\n\nRetraction Watch. (n.d.). Retraction Watch. Retraction Watch. https://retractionwatch.com/",
                "drafted_by": [
                    "William Ngiam"
                ],
                "reviewed_by": [
                    "Ali H. Al-Hoorie",
                    "Jamie P. Cockcroft",
                    "Dominik Kiersz",
                    "Sam Parsons",
                    "Suzanne L. K. Stewart",
                    "Marta Topor"
                ],
                "alt_related_terms": [
                    null
                ],
                "language": "english",
                "aliases": [
                    "/glossary/error_detection"
                ]
            },
            {
                "type": "glossary",
                "title": "Evidence Synthesis",
                "definition": "This is a type of research method which aims to draw general conclusions to address a research question on a certain topic, phenomenon or effect by reviewing research outcomes and information from a range of different sources. Information which is subject to synthesis can be extracted from both qualitative and quantitative studies. The method used to synthesise the gathered information can be qualitative (narrative synthesis), quantitative (meta-analysis) or mixed (meta-synthesis, systematic mapping). Evidence synthesis has many applications and is often used in the context of healthcare, public policy as well as understanding and advancement of specific research fields.",
                "related_terms": [
                    "Literature Review",
                    "Meta-analysis",
                    "Meta-synthesis",
                    "Meta-science or Meta-research",
                    "Narrative review",
                    "Scoping review",
                    "Systematic map",
                    "Systematic review"
                ],
                "references": "for Evaluation, C. (n.d.). Evidence Synthesis. https://www.lshtm.ac.uk/research/centres/centre-evaluation/evidence-synthesis\n\nJames, K. L., Randall, N. P., & Haddaway, N. R. (2016). A methodology for systematic mapping in environmental sciences. Environmental Evidence, 5(1), 1–13. https://doi.org/10.1186/s13750-016-0059-6\n\nSiddaway, A. P., Wood, A. M., & Hedges, L. V. (2019). How to do a systematic review: a best practice guide for conducting and reporting narrative reviews, meta-analyses, and meta-syntheses. Annual Review of Psychology, 70, 747–770. https://doi.org/10.1146/annurev-psych-010418-102803",
                "drafted_by": [
                    "Marta Topor"
                ],
                "reviewed_by": [
                    "Aoife O’Mahony",
                    "Tamara Kalandadze",
                    "Adam Parker",
                    "Charlotte R. Pennington"
                ],
                "alt_related_terms": [
                    null
                ],
                "language": "english",
                "aliases": [
                    "/glossary/evidence_synthesis"
                ]
            },
            {
                "type": "glossary",
                "title": "Exploratory data analysis",
                "definition": "Exploratory Data Analysis (EDA) is a well-established statistical tradition that provides conceptual and computational tools for discovering patterns in data to foster hypothesis development and refinement. These tools and attitudes complement the use of hypothesis tests used in confirmatory data analysis (CDA). Even when well-specified theories are held, EDA helps one interpret the results of CDA and may reveal unexpected or misleading patterns in the data.",
                "related_terms": [
                    "Confirmatory analyses",
                    "Data-driven research",
                    "Exploratory research"
                ],
                "references": "Behrens, J. T. (1997). Principles and procedures of exploratory data analysis. Psychological Methods, 2(2), 131–160. https://doi.org/10.1037/1082-989X.2.2.131\n\nBox, G. E. P. (1976). Science and statistics. Journal of the American Statistical Association, 71(356), 791–799.\n\nTukey, J. W. (1977). Exploratory data analysis. Addison-Wesley.\n\nWagenmakers, E. J., Wetzels, R., Borsboom, D., van der Maas, H. L., & Kievit, R. A. (2012). An agenda for purely confirmatory research. Perspectives on Psychological Science, 7(6), 632–638. https://doi.org/10.1177/1745691612463078",
                "drafted_by": [
                    "Jenny Terry"
                ],
                "reviewed_by": [
                    "Helena Hartmann",
                    "Timo Roettger",
                    "Charlotte R. Pennington",
                    "Flávio Azevedo"
                ],
                "alt_related_terms": [
                    null
                ],
                "language": "english",
                "aliases": [
                    "/glossary/exploratory_data_analysis"
                ]
            },
            {
                "type": "glossary",
                "title": "External Validity",
                "definition": "Whether the findings of a scientific study can be generalized to other contexts outside the study context (different measures, settings, people, places, and times). Statistically, threats to external validity may reflect interactions whereby the effect of one factor (the independent variable) depends on another factor (a confounding variable). External validity may also be limited by the study design (e.g., an artificial laboratory setting or a non-representative sample).",
                "related_terms": [
                    "Constraints on Generality (COG)",
                    "Internal validity",
                    "Generalizability",
                    "Representativity",
                    "Validity"
                ],
                "references": "Lynch, J. G., Jr. (1982). On the External Validity of Experiments in Consumer Research. Journal of Consumer Research, 9(3), 225. https://doi.org/10.1086/208919\n\nSteckler, A., & McLeroy, K. R. (2008). The Importance of External Validity. American Journal of Public Health, 98(1), 9–10. https://doi.org/10.2105/AJPH.2007.126847",
                "drafted_by": [
                    "Annalise A. LaPlume"
                ],
                "reviewed_by": [
                    "Mahmoud Elsherif",
                    "Helena Hartmann",
                    "Kai Krautter",
                    "Oscar Lecuona",
                    "Flávio Azevedo"
                ],
                "alt_related_terms": [
                    null
                ],
                "language": "english",
                "aliases": [
                    "/glossary/external_validity"
                ]
            },
            {
                "type": "glossary",
                "title": "Face validity",
                "definition": "A subjective judgement of how suitable a measure appears to be on the surface, that is, how well a measure is operationalized. For example, judging whether questionnaire items should relate to a construct of interest at face value. Face validity is related to construct validity, but since it is subjective/informal, it is considered an easy but weak form of validity.",
                "related_terms": [
                    "Construct Validity",
                    "Content Validity",
                    "Logical Validity",
                    "Operationalization",
                    "Validity"
                ],
                "references": "Holden, R. B. (2010). Face Validity. In I. B. Weiner & W. E. Craighead (Eds.), The Corsini Encyclopedia of Psychology (4th ed.). Wiley. http://dx.doi.org/10.1002/9780470479216.corpsy0341",
                "drafted_by": [
                    "Annalise A. LaPlume"
                ],
                "reviewed_by": [
                    "Helena Hartmann",
                    "Kai Krautter",
                    "Adam Parker",
                    "Sam Parsons"
                ],
                "alt_related_terms": [
                    null
                ],
                "language": "english",
                "aliases": [
                    "/glossary/face_validity"
                ]
            },
            {
                "type": "glossary",
                "title": "FAIR principles",
                "definition": "Describes making scholarly materials Findable, Accessible, Interoperable and Reusable (FAIR). ‘Findable’ and ‘Accessible’ are concerned with where materials are stored (e.g. in data repositories), while ‘Interoperable’ and ‘Reusable’ focus on the importance of data formats and how such formats might change in the future.",
                "related_terms": [
                    "Metadata",
                    "Open Access",
                    "Open Code",
                    "Open Data",
                    "Open Material",
                    "Repository"
                ],
                "references": "Crüwell, S., van Doorn, J., Etz, A., Makel, M. C., Moshontz, H., Niebaum, J. C., Orben, A., Parsons, S., & Schulte-Mecklenbeck, M. (2019). Seven Easy Steps to Open Science: An Annotated Reading List. Zeitschrift Für Psychologie, 227(4), 237–248. https://doi.org/10.1027/2151-2604/a000387\n\nWilkinson, M. D., Dumontier, M., Aalbersberg, I. J., Appleton, G., Axton, M., Baak, A., & others. (2016). The FAIR Guiding Principles for scientific data management and stewardship. Scientific Data, 3(1), 1–9. https://doi.org/10.1038/sdata.2016.18",
                "drafted_by": [
                    "Sonia Rishi"
                ],
                "reviewed_by": [
                    "Mahmoud Elsherif",
                    "Sam Parsons",
                    "Charlotte R. Pennington"
                ],
                "alt_related_terms": [
                    null
                ],
                "language": "english",
                "aliases": [
                    "/glossary/fair_principles"
                ]
            },
            {
                "type": "glossary",
                "title": "Feminist psychology",
                "definition": "With a particular focus on gender and sexuality, feminist psychology is inherently concerned with representation, diversity, inclusion, accessibility, and equality. Feminist psychology initially grew out out of a concern for representing the lived experiences of girls and women, but has since evolved into a more nuanced, intersectional and comprehensive concern for all aspects of equality (e.g., Eagly & Riger, 2014). Feminist psychologists have advocated for more rigorous consideration of equality, diversity, and inclusion within Open Science spaces (Pownall et al., 2021).",
                "related_terms": [
                    "Inclusion",
                    "Positionality",
                    "Reflexivity",
                    "Under-representation",
                    "Equity"
                ],
                "references": "Eagly, A. H., & Riger, S. (2014). Feminism and psychology: Critiques of methods and epistemology. American Psychologist, 69(7), 685–702. https://doi.org/10.1037/a0037372\n\nGrzanka, P. R. (2020). From buzzword to critical psychology: An invitation to take intersectionality seriously. Women & Therapy, 43(3–4), 244–261.\n\nPownall, M., Talbot, C. V., Henschel, A., Lautarescu, A., Lloyd, K. E., Hartmann, H., Darda, K. M., Tang, K. T. Y., Carmichael-Murphy, P., & Siegel, J. A. (2021). Navigating Open Science as Early Career Feminist Researchers. Psychology of Women Quarterly, 45(4), 526–539. https://doi.org/10.1177/03616843211029255 Retrieved from https://journals.sagepub.com/doi/10.1177/03616843211029255",
                "drafted_by": [
                    "Madeleine Pownall"
                ],
                "reviewed_by": [
                    "Mahmoud Elsherif",
                    "Helena Hartmann",
                    "Kai Krautter",
                    "Charlotte R. Pennington"
                ],
                "alt_related_terms": [
                    null
                ],
                "language": "english",
                "aliases": [
                    "/glossary/feminist_psychology"
                ]
            },
            {
                "type": "glossary",
                "title": "First-last-author-emphasis norm (FLAE)",
                "definition": "An authorship system that assigns the order of authorship depending on the contributions of a given author while simultaneously valuing the first and last position of the authorship order most. According to this system, the two main authors are indicated as the first and last author \\- the order of the authors between the first and last position is determined by contribution in a descending order.",
                "related_terms": [
                    "Authorship",
                    "Author contributions",
                    "CreDit taxonomy"
                ],
                "references": "Tscharntke, T., Hochberg, M. E., Rand, T. A., Resh, V. H., & Krauss, J. (2007). Author sequence and credit for contributions in multiauthored publications. PLoS Biology, 5(1), e18. https://doi.org/10.1371/journal.pbio.0050018",
                "drafted_by": [
                    "Myriam A. Baum"
                ],
                "reviewed_by": [
                    "Charlotte R. Pennington"
                ],
                "alt_related_terms": [
                    null
                ],
                "language": "english",
                "aliases": [
                    "/glossary/first_last_author_emphasis_norm"
                ]
            },
            {
                "type": "glossary",
                "title": "FORRT",
                "definition": "Framework of Open Reproducible Research and Teaching. It aims to provide a pedagogical infrastructure designed to recognize and support the teaching and mentoring of open and reproducible research in tandem with prototypical subject matters in higher education. FORRT strives to be an effective, evolving, and community-driven organization raising awareness of the pedagogical implications of open and reproducible science and its associated challenges (i.e., curricular reform, epistemological uncertainty, methods of education). FORRT also advocates for the opening of teaching and mentoring materials as a means to facilitate access, discovery, and learning to those who otherwise would be educationally disenfranchised.",
                "related_terms": [
                    "Integrating open and reproducible science tenets into higher education"
                ],
                "references": "",
                "drafted_by": [
                    "Tamara Kalandadze"
                ],
                "reviewed_by": [
                    "Mahmoud Elsherif",
                    "Charlotte R. Pennington",
                    "Flávio Azevedo"
                ],
                "alt_related_terms": [
                    null
                ],
                "language": "english",
                "aliases": [
                    "/glossary/forrt"
                ]
            },
            {
                "type": "glossary",
                "title": "Free Our Knowledge Platform",
                "definition": "A collective action platform aiming to support the open science movement by obtaining pledges from researchers that they will implement certain research practices (e.g., pre-registration, pre-print). Initially pledges will be anonymous until a sufficient number of people pledge, upon which names of pledges will be released. The initiative is a grassroots movement instigated by early career researchers.",
                "related_terms": [
                    "Open Science",
                    "Preregistration Pledge"
                ],
                "references": "Free Our Knowledge. (n.d.). About. Retrieved from https://freeourknowledge.org/about/",
                "drafted_by": [
                    "Jamie P. Cockcroft"
                ],
                "reviewed_by": [
                    "Ashley Blake",
                    "Elizabeth Collins",
                    "Mahmoud Elsherif",
                    "Sam Parsons",
                    "Flávio Azevedo"
                ],
                "alt_related_terms": [
                    null
                ],
                "language": "english",
                "aliases": [
                    "/glossary/free_our_knowledge_platform"
                ]
            },
            {
                "type": "glossary",
                "title": "GPower",
                "definition": "Free to use statistical software for performing power analyses. The user specifies the desired statistical test (e.g. t-test, regression, ANOVA), and three of the following: the number of groups/observations, effect size, significance level, or power, in order to calculate the unspecified aspect.",
                "related_terms": [
                    "Power analysis",
                    "Sample size justification",
                    "Sample size planning",
                    "Statistical power"
                ],
                "references": "Faul, F., Erdfelder, E., Lang, A.-G., & Buchner, A. (2007). G*Power 3: A flexible statistical power analysis program for the social, behavioral, and biomedical sciences. Behavior Research Methods, 39, 175–191. https://doi.org/10.3758/BF03193146\n\nFaul, F., Erdfelder, E., Buchner, A., & Lang, A.-G. (2009). Statistical power analyses using G*Power 3.1: Tests for correlation and regression analyses. Behavior Research Methods, 41, 1149–1160. https://doi.org/10.3758/BRM.41.4.1149",
                "drafted_by": [
                    "Filip Dechterenko"
                ],
                "reviewed_by": [
                    "Thomas Rhys Evans",
                    "Kai Krautter",
                    "Charlotte R. Pennington"
                ],
                "alt_related_terms": [
                    null
                ],
                "language": "english",
                "aliases": [
                    "/glossary/gpower"
                ]
            },
            {
                "type": "glossary",
                "title": "Gaming (the system)",
                "definition": "Adopting questionable research practices (QRPs, e.g., salami slicing of an academic paper) that would align with academic incentive structures that benefit the academic (e.g. in prestige, hiring, or promotion) regardless of whether they support the process of scholarship. If systems rely on metrics to determine an outcome (e.g. academic credit) those metrics can be subject to intentional manipulation (Naudet et al., 2018\\) or “gamed”. Where promotions, hiring, and tenure are based on flawed metrics they may disfavor openness, rigor, and transparent work (Naudet et al., 2018\\) \\- for example favoring “quantity over quality” \\- and exacerbate existing inequalities.",
                "related_terms": [
                    "Incentive structure",
                    "Journal Impact Factor",
                    "*P*\\-hacking"
                ],
                "references": "Moher, D., Naudet, F., Cristea, I. A., Miedema, F., Ioannidis, J. P. A., & Goodman, S. N. (2018). Assessing scientists for hiring, promotion, and tenure. PLOS Biology, 16(3), e2004089. https://doi.org/10.1371/journal.pbio.2004089\n\nNaudet, F., Ioannidis, J., Miedema, F., Cristea, I. A., Goodman, S. N., & Moher, D. (2018). Six principles for assessing scientists for hiring, promotion, and tenure. Impact of Social Sciences Blog.",
                "drafted_by": [
                    "Adrien Fillon"
                ],
                "reviewed_by": [
                    "Sarah Ashcroft-Jones",
                    "Helena Hartmann",
                    "Sam Parsons",
                    "Charlotte R. Pennington"
                ],
                "alt_related_terms": [
                    null
                ],
                "language": "english",
                "aliases": [
                    "/glossary/gaming"
                ]
            },
            {
                "type": "glossary",
                "title": "Garden of forking paths",
                "definition": "The typically-invisible decision tree traversed during operationalization and statistical analysis given that ‘there is a one-to-many mapping from scientific to statistical hypotheses' (Gelman and Loken, 2013, p. 6). In other words, even in absence of p-hacking or fishing expeditions and when the research hypothesis was posited ahead of time, there can be a plethora of statistical results that can appear to be supported by theory given data. “The problem is there can be a large number of potential comparisons when the details of data analysis are highly contingent on data, without the researcher having to perform any conscious procedure of fishing or examining multiple p-values” (Gelman and Loken, 2013, p. 1). The term aims to highlight the uncertainty ensuing from idiosyncratic analytical and statistical choices in mapping theory-to-test, and contrasting intentional (and unethical) questionable research practices (e.g. p-hacking and fishing expeditions) versus non-intentional research practices that can, potentially, have the same effect despite not having intent to corrupt their results. The garden of forking paths refers to the decisions during the scientific process that inflate the false-positive rate as a consequence of the potential paths which could have been taken (had other decisions been made).",
                "related_terms": [
                    "False-positive",
                    "Familywise error",
                    "Multiverse Analysis",
                    "Preregistration",
                    "Researcher degrees of freedom",
                    "Specification Curve Analysis"
                ],
                "references": "Gelman, A., & Loken, E. (n.d.). The garden of forking paths: Why multiple comparisons can be a problem, even when there is no “fishing expedition” or “p-hacking” and the research hypothesis was posited ahead of time. Retrieved from http://www.stat.columbia.edu/",
                "drafted_by": [
                    "Flávio Azevedo; Mahmoud Elsherif"
                ],
                "reviewed_by": [
                    "Gisela H. Govaart",
                    "Matt Jaquiery",
                    "Tamara Kalandadze",
                    "Charlotte R. Pennington"
                ],
                "alt_related_terms": [
                    null
                ],
                "language": "english",
                "aliases": [
                    "/glossary/garden_of_forking_paths"
                ]
            },
            {
                "type": "glossary",
                "title": "General Data Protection Regulation (GDPR)",
                "definition": "A legal framework of seven principles implemented across the European Union (EU) that aims to safeguard individuals’ information. The framework seeks to commission citizens with control over their personal data, whilst regulating the parties involved in storing and processing these data. This set of legislation dictates the free movement of individuals’ personal information both within and outside the EU and must be considered by researchers when designing and running studies.",
                "related_terms": [
                    "Anonymity",
                    "Data Management Plan (DMP)",
                    "Data sharing",
                    "Repeatability",
                    "Replicability",
                    "Reproducibility"
                ],
                "references": "Crutzen, R., Ygram Peters, G. J., & Mondschein, C. (2019). Why and how we should care about the General Data Protection Regulation. Psychology & Health, 34(11), 1347–1357. https://doi.org/10.1080/08870446.2019.1606222\n\nInformation Commissioner’s Office. (2021). Guide to the UK General Data Protection Regulation (UK GDPR). ICO. https://ico.org.uk/for-organisations/guide-to-data-protection/guide-to-the-general-data-protection-regulation-gdpr/",
                "drafted_by": [
                    "Graham Reid"
                ],
                "reviewed_by": [
                    "Elizabeth Collins",
                    "Mahmoud Elsherif",
                    "Christopher Graham",
                    "Sam Parsons"
                ],
                "alt_related_terms": [
                    null
                ],
                "language": "english",
                "aliases": [
                    "/glossary/general_data_protection_regulation"
                ]
            },
            {
                "type": "glossary",
                "title": "Generalizability",
                "definition": "Generalizability refers to how applicable a study’s results are to broader groups of people, settings, or situations they study and how the findings relate to this wider context (Frey, 2018; Kukull & Ganguli, 2012).",
                "related_terms": [
                    "Conceptual replication",
                    "External Validity",
                    "Opportunistic sampling",
                    "Sampling bias",
                    "WEIRD **Alternative definition:** Applying modified materials and/or analysis pipelines to new data or samples to answer the same hypothesis (different materials, different data) to test how generalizable the effect under study is (The Turing Way Community & Scriberia, 2021). **Related terms to alternative definition:** (if applicable): Conceptual Replication"
                ],
                "references": "Esterling, K., Brady, D., & Schwitzgebel, E. (2021). The Necessity of Construct and External Validity for Generalized Causal Claims. Retrieved from https://doi.org/10.31219/osf.io/2s8w5.\n\nGeneralizability. (2018). Generalizability. In B. B. Frey (Ed.), The SAGE Encyclopedia of Educational Research, Measurement, and Evaluation. SAGE Publications, Inc. https://doi.org/10.4135/9781506326139.n284\n\nKukull, W. A., & Ganguli, M. (2012). Generalizability: The trees, the forest, and the low-hanging fruit. Neurology, 78(23), 1886–1891. https://doi.org/10.1212/WNL.0b013e318258f812\n\nLeBel, E. P., Vanpaemel, W., Cheung, I., & Campbell, L. (2017). A brief guide to evaluate replications. Meta-Psychology, 3. https://doi.org/10.15626/MP.2018.843\n\nNosek, B. A., & Errington, T. M. (2020). What is replication? PLOS Biology, 18(3), e3000691. https://doi.org/10.1371/journal.pbio.3000691\n\nYarkoni, T. (2020). The generalizability crisis. Behavioral and Brain Sciences, 1–37. https://doi.org/10.1017/S0140525X20001685",
                "drafted_by": [
                    "Aoife O’Mahony"
                ],
                "reviewed_by": [
                    "Adrien Fillon",
                    "Matt Jaquiery",
                    "Tina Lonsdorf",
                    "Sam Parsons",
                    "Julia Wolska"
                ],
                "alt_related_terms": [
                    null
                ],
                "language": "english",
                "aliases": [
                    "/glossary/generalizability"
                ]
            },
            {
                "type": "glossary",
                "title": "Gift (or Guest) Authorship",
                "definition": "The inclusion in an article’s author list of individuals who do not meet the criteria for authorship. As authorship is associated with benefits including peer recognition and financial rewards, there are incentives for inclusion as an author on published research. Gifting authorship, or extending authorship credit to an individual who does not merit such recognition, can be intended to help the gift recipient, repay favors (including reciprocal gift authorship), maintain personal and professional relationships, and enhance chances of publication. Gift authorship is widely considered an unethical practice.",
                "related_terms": [
                    "Authorship",
                    "CRediT"
                ],
                "references": "Bhopal, R., Rankin, J., McColl, E., Thomas, L., Kaner, E., Stacy, R., Pearson, P., Vernon, B., & Rodgers, H. (1997). The vexed question of authorship: views of researchers in a British medical faculty. BMJ, 314, 1009–1012. https://doi.org/10.1136/bmj.314.7086.1009\n\nof Medical Journal Editors, I. C. (2019). Recommendations for the conduct, reporting, eduting, and publication of scholarly work in medical journals. http://www.icmje.org/icmje-recommendations.pdf",
                "drafted_by": [
                    "Bradley Baker"
                ],
                "reviewed_by": [
                    "Helena Hartmann",
                    "Aoife O’Mahony",
                    "Sam Parsons",
                    "Charlotte R. Pennington",
                    "Flávio Azevedo"
                ],
                "alt_related_terms": [
                    null
                ],
                "language": "english",
                "aliases": [
                    "/glossary/gift"
                ]
            },
            {
                "type": "glossary",
                "title": "Git",
                "definition": "A software package for tracking changes in a local set of files (local version control), initially developed by Linus Torvalds. In general, it is used by programmers to track and develop computer source code within a set directory, folder or a file system. Git can access remote repository hosting services (e.g. GitHub) for remote version control that enables collaborative software development by uploading contributions from a local system. This process found its way into the scientific process to enable open data, open code and reproducible analyses.",
                "related_terms": [
                    "GitHub",
                    "Repository",
                    "Version control"
                ],
                "references": "Kalliamvakou, E., Gousios, G., Blincoe, K., Singer, L., German, D. M., & Damian, D. (2014). The promises and perils of mining github. Proceedings of the 11th Working Conference on Mining Software Repositories, 92–101.\n\nScopatz, A. M., & Huff, K. D. (2015). Effective Computation in Physics: Field Guide to Research with Python (1st ed.). O’Reilly Media. http://shop.oreilly.com/product/0636920033424.do\n\nVuorre, M., & Curley, J. P. (2018). Curating research assets: A tutorial on the Git version control system. Advances in Methods and Practices in Psychological Science, 1(2), 219–236. https://doi.org/10.1177/2515245918754826\n\ngit/git. (n.d.). Initial revision of ‘git’, the information manager from hell. GitHub. https://github.com/git/git/commit/e83c5163316f89bfbde7d9ab23ca2e25604af290",
                "drafted_by": [
                    "Emma Norris"
                ],
                "reviewed_by": [
                    "Adrien Fillon",
                    "Bettina M.J. Kern",
                    "Dominik Kiersz",
                    "Robert M. Ross"
                ],
                "alt_related_terms": [
                    null
                ],
                "language": "english",
                "aliases": [
                    "/glossary/git"
                ]
            },
            {
                "type": "glossary",
                "title": "Goodhart’s Law",
                "definition": "A term coined by economist Charles Goodhart to refer to the observation that measuring something inherently changes user behaviour. In relation to examination performance, Strathern (1997) stated that “when a measure becomes a target, it ceases to be a good measure” (p. 308). Applied to open scholarship, and the structure of incentives in academia, Goodhart’s Law would predict that metrics of scientific evaluation will likely be abused and exploited, as evidenced by Muller (2019)",
                "related_terms": [
                    "Campbell's law",
                    "DORA",
                    "Reification (fallacy) **Reference (s):** \\[@Muller2018\\], \\[@Strathern1997\\]"
                ],
                "references": "",
                "drafted_by": [
                    "Adam Parker"
                ],
                "reviewed_by": [
                    "Sam Parsons",
                    "Flávio Azevedo"
                ],
                "alt_related_terms": [
                    null
                ],
                "language": "english",
                "aliases": [
                    "/glossary/goodhart_s_law"
                ]
            },
            {
                "type": "glossary",
                "title": "H-index",
                "definition": "Hirsch’s index, abbreviated as H-index, intends to measure both productivity and research impact by combining the number of publications and the number of citations to these publications. Hirsch (2005) defined the index as “the number of papers with citation number ≥ *h*” (p. 16569). That is, the greatest number such that an author (or journal) has published at least that many papers that have been cited at least that many times. The index is perceived as a superior measure to measures that only assess, for instance, the number of citations and number of publications but this index has been criticised for the purpose of researcher assessment (e.g. Wendl, 2007).",
                "related_terms": [
                    "Citation",
                    "DORA",
                    "I10-index",
                    "Impact"
                ],
                "references": "Hirsch, J. E. (2005). An index to quantify an individual’s scientific research output. Proceedings of the National Academy of Sciences, 102(46), 16569–16572. https://doi.org/10.1073/pnas.0507655102\n\nWendl, M. C. (2007). H-index: however ranked, citations need context. Nature, 449(7161), 403–403. https://doi.org/10.1038/449403b",
                "drafted_by": [
                    "Jacob Miranda"
                ],
                "reviewed_by": [
                    "Bradley J. Baker",
                    "Mahmoud M. Elsherif",
                    "Brett J. Gall",
                    "Charlotte R. Pennington"
                ],
                "alt_related_terms": [
                    null
                ],
                "language": "english",
                "aliases": [
                    "/glossary/h_index"
                ]
            },
            {
                "type": "glossary",
                "title": "Hackathon",
                "definition": "An organized event where experts, designers, or researchers collaborate for a relatively short amount of time to work intensively on a project or problem. The term is originally borrowed from computer programmer and software development events whose goal is to create a fully fledged product (resources, research, software, hardware) by the end of the event, which can last several hours to several days.",
                "related_terms": [
                    "Collaboration",
                    "Edithaton"
                ],
                "references": "Kienzler, H., & Fontanesi, C. (2017). Learning through inquiry: A global health hackathon. Teaching in Higher Education, 22(2), 129–142. https://doi.org/10.1080/13562517.2016.1221805",
                "drafted_by": [
                    "Flávio Azevedo"
                ],
                "reviewed_by": [
                    "Tsvetomira Dumbalska",
                    "Brett J. Gall",
                    "Emma Norris"
                ],
                "alt_related_terms": [
                    null
                ],
                "language": "english",
                "aliases": [
                    "/glossary/hackathon"
                ]
            },
            {
                "type": "glossary",
                "title": "HARKing",
                "definition": "A questionable research practice termed ‘Hypothesizing After the Results are Known’ (HARKing). “HARKing is defined as presenting a post hoc hypothesis (i.e., one based on or informed by one's results) in a research report as if it was, in fact, *a priori*” (Kerr, 1998, p. 196). For example, performing subgroup analyses, finding an effect in one subgroup, and writing the introduction with a ‘hypothesis’ that matches these results.",
                "related_terms": [
                    "Analytic Flexibility",
                    "Confirmatory analyses",
                    "Exploratory data analysis",
                    "Fudging",
                    "Garden of forking paths",
                    "P-hacking",
                    "Questionable Research Practices or Questionable Reporting Practices (QRPs)"
                ],
                "references": "Kerr, N. L. (1998). HARKing: Hypothesizing after the results are known. Personality and Social Psychology Review, 2(3), 196–217. https://doi.org/10.1207/s15327957pspr0203_4\n\nNosek, B. A., & Lakens, D. (2014). Registered reports. Social Psychology, 45, 137–141. https://doi.org/10.1027/1864-9335/a000192",
                "drafted_by": [
                    "Beatrix Arendt"
                ],
                "reviewed_by": [
                    "Matt Jaquiery",
                    "Charlotte R. Pennington",
                    "Martin Vasilev",
                    "Flávio Azevedo"
                ],
                "alt_related_terms": [
                    null
                ],
                "language": "english",
                "aliases": [
                    "/glossary/harking"
                ]
            },
            {
                "type": "glossary",
                "title": "Hidden Moderators",
                "definition": "Contextual conditions that can, unbeknownst to researchers, make the results of a replication attempt deviate from those of the original study. Hidden moderators are sometimes invoked to explain (away) failed replications. Also called hidden assumptions.",
                "related_terms": [
                    "Auxiliary Hypothesis"
                ],
                "references": "Zwaan, R., Etz, A., Lucas, R., & Donnellan, M. (2018). Making replication mainstream. Behavioral and Brain Sciences, 41, E120. https://doi.org/10.1017/S0140525X17001972",
                "drafted_by": [
                    "Ali H. Al-Hoorie"
                ],
                "reviewed_by": [
                    "Mahmoud Elsherif",
                    "Sam Parsons"
                ],
                "alt_related_terms": [
                    null
                ],
                "language": "english",
                "aliases": [
                    "/glossary/hidden_moderators"
                ]
            },
            {
                "type": "glossary",
                "title": "Hypothesis",
                "definition": "A hypothesis is an unproven statement relating the connection between variables (Glass & Hall, 2008\\) and can be based on prior experiences, scientific knowledge, preliminary observations, theory and/or logic. In scientific testing, a hypothesis can be usually formulated with (e.g. a positive correlation) or without a direction (e.g. there will be a correlation). Popper (1959) posits that hypotheses must be falsifiable, that is, it must be conceivably possible to prove the hypothesis false. However, hypothesis testing based on falsification has been argued to be vague, as it is contingent on many other untested assumptions in the hypothesis (i.e., auxiliary hypotheses). Longino (1990, 1992\\) argued that ontological heterogeneity should be valued more than ontological simplicity for the biological sciences, which considers we should investigate differences between and within biological organisms.",
                "related_terms": [
                    "Auxiliary Hypothesis",
                    "Confirmatory analyses",
                    "False negative result",
                    "False positive result",
                    "Modelling",
                    "Predictions",
                    "Quantitative research",
                    "Theory",
                    "Theory building",
                    "Type I error",
                    "Type II error"
                ],
                "references": "Beller, S., & Bender, A. (2017). Theory, the final frontier? A corpus-based analysis of the role of theory in psychological articles. Frontiers in Psychology, 8, 951. https://doi.org/10.3389/fpsyg.2017.00951\n\nGlass, D. J., & Hall, N. (2008). A brief history of the hypothesis. Cell, 134(3), 378–381. https://doi.org/10.1016/j.cell.2008.07.033\n\nLongino, H. E. (1990). Science as Social Knowledge: Values and Objectivity in Scientific Inquiry. Princeton University Press.\n\nLongino, H. E. (1992). Taking gender seriously in philosophy of science. PSA, 2, 333–340.\n\nPopper, K. (1959). The logic of scientific discovery. Routledge.",
                "drafted_by": [
                    "Ana Barbosa Mendes"
                ],
                "reviewed_by": [
                    "Ali H. Al-Hoorie",
                    "Mahmoud Elsherif",
                    "Helena Hartmann",
                    "Charlotte R. Pennington**;** Graham Reid",
                    "Olly Robertson"
                ],
                "alt_related_terms": [
                    null
                ],
                "language": "english",
                "aliases": [
                    "/glossary/hypothesis"
                ]
            },
            {
                "type": "glossary",
                "title": "i10-index",
                "definition": "A research metric created by Google Scholar that represents the number of publications a researcher has with at least 10 citations.",
                "related_terms": [
                    "Citation",
                    "DORA",
                    "H-index",
                    "Impact"
                ],
                "references": "University, C. (2020). Measuring your research impact: i10 index. Cornell University Library. https://guides.library.cornell.edu/impact/author-impact-10",
                "drafted_by": [
                    "Emma Norris"
                ],
                "reviewed_by": [
                    "Flávio Azevedo",
                    "Sam Parsons"
                ],
                "alt_related_terms": [
                    null
                ],
                "language": "english",
                "aliases": [
                    "/glossary/i10_index"
                ]
            },
            {
                "type": "glossary",
                "title": "Ideological bias",
                "definition": "The idea that pre-existing opinions about the quality of research can depend on the ideological views of the author(s). One of the many biases in the peer review process, it expects that favourable opinions towards the research would be more likely if friends, collaborators, or scientists agree with an editor or reviewer’s political viewpoints (Tvina et al. 2019). This could potentially lead to a variety of conflicts of interest that undermine diverse perspectives, for example: speeding or delaying peer-review, or influencing the chances of an individual being invited to present their research, thus promoting their work.",
                "related_terms": [
                    "Ad hominem bias",
                    "Peer review"
                ],
                "references": "Tvina, A., Spellecy, R., & Palatnik, A. (2019). Bias in the peer review process: can we do better? Obstetrics & Gynecology, 133(6), 1081–1083. https://doi.org/10.1097/AOG.0000000000003260",
                "drafted_by": [
                    "Mahmoud Elsherif"
                ],
                "reviewed_by": [
                    "Elizabeth Collins",
                    "Flávio Azevedo",
                    "Madeleine Ingham",
                    "Sam Parsons",
                    "Graham Reid"
                ],
                "alt_related_terms": [
                    null
                ],
                "language": "english",
                "aliases": [
                    "/glossary/ideological_bias"
                ]
            },
            {
                "type": "glossary",
                "title": "Inclusion",
                "definition": "Inclusion, or inclusivity, refers to a sense of welcome and respect within a given collaborative project or environment (such as academia) where *diversity* simply indicates a wide range of backgrounds, perspectives, and experiences, efforts to increase *inclusion* go further to promote engagement and equal valuation among diverse individuals, who might otherwise be marginalized. Increasing inclusivity often involves minimising the impact of, or even removing, systemic barriers to accessibility and engagement.",
                "related_terms": [
                    "Diversity",
                    "Equity",
                    "Social Justice"
                ],
                "references": "Calvert, D. (2019). How to Make Inclusivity More Than Just an Office Buzzword. Retrieved from https://insight.kellogg.northwestern.edu/article/how-to-make-inclusivity-more-than-just-an-office-buzzword\n\nMartinez-Acosta, V. G., & Favero, C. B. (2018). A discussion of diversity and inclusivity at the institutional level: The need for a strategic plan. Journal of Undergraduate Neuroscience Education, 16(3), A252.",
                "drafted_by": [
                    "Ryan Millager"
                ],
                "reviewed_by": [
                    "Mahmoud Elsherif",
                    "Graham Reid",
                    "Kai Krautter",
                    "Suzanne L. K. Stewart",
                    "Flávio Azevedo"
                ],
                "alt_related_terms": [
                    null
                ],
                "language": "english",
                "aliases": [
                    "/glossary/inclusion"
                ]
            },
            {
                "type": "glossary",
                "title": "Incentive structure",
                "definition": "The set of evaluation and reward mechanisms (explicit and implicit) for scientists and their work. Incentivised areas within the broader structure include hiring and promotion practices, track record for awarding funding, and prestige indicators such as publication in journals with high impact factors, invited presentations, editorships, and awards. It is commonly believed that these criteria are often misaligned with the telos of science, and therefore do not promote rigorous scientific output. Initiatives like DORA aim to reduce the field’s dependency on evaluation criteria such as journal impact factors in favor of assessments based on the intrinsic quality of research outputs.",
                "related_terms": [
                    "DORA",
                    "Metrics",
                    "Pressure",
                    "Publish or perish",
                    "Quantity",
                    "Reward structure",
                    "Scientific publications",
                    "Slow science",
                    "Structural factors"
                ],
                "references": "Koole, S. L., & Lakens, D. (2012). Rewarding replications: A sure and simple way to improve psychological science. Perspectives on Psychological Science, 7(6), 608–614. https://doi.org/10.1177/1745691612462586\n\nNosek, B. A., Spies, J. R., & Motyl, M. (2012). Scientific utopia: II. Restructuring incentives and practices to promote truth over publishability. Perspectives on Psychological Science, 7(6), 615–631. https://doi.org/10.1177/1745691612459058\n\nSchönbrodt, F. (2019). Training students for the Open Science future. Nature Human Behaviour, 3(10), 1031–1031. https://doi.org/10.1038/s41562-019-0726-z\n\nSmaldino, P. E., & McElreath, R. (2016). The natural selection of bad science. Royal Society Open Science, 3(9), 160384. https://doi.org/10.1098/rsos.160384",
                "drafted_by": [
                    "Charlotte R. Pennington; Olmo van den Akker"
                ],
                "reviewed_by": [
                    "Helena Hartmann",
                    "Flávio Azevedo",
                    "Robert M. Ross",
                    "Graham Reid",
                    "Suzanne L. K. Stewart"
                ],
                "alt_related_terms": [
                    null
                ],
                "language": "english",
                "aliases": [
                    "/glossary/incentive_structure"
                ]
            },
            {
                "type": "glossary",
                "title": "Induction",
                "definition": "“Reasoning by drawing a conclusion not guaranteed by the premises; for example, by inferring a general rule from a limited number of observations. Popper believed that there was no such logical process; we may guess general rules but such guesses are not rendered even more probable by any number of observations. By contrast, Bayesians inductively work out the increase in probability of a hypothesis that follows from the observations.” Dienes (p. 164, 2008\\)",
                "related_terms": [
                    "Hypothesis"
                ],
                "references": "Dienes, Z. (2008). Understanding psychology as a science: An introduction to scientific and statistical inference. Macmillan International Higher Education.",
                "drafted_by": [
                    "Alaa Aldoh"
                ],
                "reviewed_by": [],
                "alt_related_terms": [
                    null
                ],
                "language": "english",
                "aliases": [
                    "/glossary/induction"
                ]
            },
            {
                "type": "glossary",
                "title": "Interaction Fallacy",
                "definition": "A statistical error in which a formal test is not conducted to assess the difference between a significant and non-significant correlation (or other measures, such as Odds Ratio). This fallacy occurs when a significant and non-significant correlation coefficient are assumed to represent a statistically significant difference but the comparison itself is not explicitly tested.",
                "related_terms": [
                    "Comparison of Correlations",
                    "Null Hypothesis Significance Testing (NHST)",
                    "Statistical Validity",
                    "Type I error",
                    "Type II error"
                ],
                "references": "Gelman, A., & Stern, H. (2006). The difference between “significant” and “not significant” is not itself statistically significant. The American Statistician, 60(4), 328–331. https://doi.org/10.1198/000313006X152649\n\nMorabia, A., Have, T. T., & Landis, J. R. (1997). Interaction Fallacy. Journal of Clinical Epidemiology, 50(7), 809–812. https://doi.org/10.1016/S0895-4356(97)00053-X\n\nNieuwenhuis, S., Forstmann, B. U., & Wagenmakers, E. J. (2011). Erroneous analyses of interactions in neuroscience: a problem of significance. Nature Neuroscience, 14(9), 1105–1107. https://doi.org/10.1038/nn.2886",
                "drafted_by": [
                    "Graham Reid"
                ],
                "reviewed_by": [
                    "Ali H. Al-Hoorie",
                    "Mahmoud Elsherif",
                    "Kai Krautter",
                    "Sam Parsons"
                ],
                "alt_related_terms": [
                    null
                ],
                "language": "english",
                "aliases": [
                    "/glossary/interaction_fallacy"
                ]
            },
            {
                "type": "glossary",
                "title": "Interlocking",
                "definition": "An analysis at the core of intersectionality to analyse power, inequality and exclusion, as efforts to reform academic culture cannot be completed by investigating only one avenue in isolation (e.g. race, gender or ability) but by considering all the systems of exclusion. In contrast to intersectionality (which refers to the individual having multiple social identities), interlocking is usually used to describe the systems that combine to serve as oppressive measures toward the individual based on these identities.",
                "related_terms": [
                    "Bropenscience",
                    "Equity",
                    "Diversity",
                    "Inclusion",
                    "Intersectionality",
                    "Open Science",
                    "Social Justice"
                ],
                "references": "Ledgerwood, A., Hudson, S. T. J., Lewis, Jr., N. A., Maddox, K. B., Pickett, C., Remedios, J. D., & Wilkins, C. L. (2021). The Pandemic as a Portal: Reimagining Psychological Science as Truly Open and Inclusive. https://doi.org/10.31234/osf.io/gdzue",
                "drafted_by": [
                    "Christina Pomareda"
                ],
                "reviewed_by": [
                    "Ali H. Al-Hoorie",
                    "Flávio Azevedo",
                    "Mahmoud Elsherif",
                    "Eliza Woodward",
                    "Gerald Vineyard;"
                ],
                "alt_related_terms": [
                    null
                ],
                "language": "english",
                "aliases": [
                    "/glossary/interlocking"
                ]
            },
            {
                "type": "glossary",
                "title": "Internal Validity",
                "definition": "An indicator of the extent to which a study’s findings are representative of the true effect in the population of interest and not due to research confounds, such as methodological shortcomings. In other words, whether the observed evidence or covariation between the independent (predictor) and dependent (criterion) variables can be taken as a bona fide relationship and not a spurious effect owing to uncontrolled aspects of the study’s set up. Since it involves the quality of the study itself, internal validity is a priority for scientific research.",
                "related_terms": [
                    "External validity",
                    "Validity"
                ],
                "references": "Campbell, D. T., & Stanley, J. C. (1966). Experimental and Quasi Experimental Designs. Rand McNally.",
                "drafted_by": [
                    "Annalise A. LaPlume"
                ],
                "reviewed_by": [
                    "Helena Hartmann",
                    "Oscar Lecuona",
                    "Meng Liu",
                    "Sam Parsons",
                    "Graham Reid",
                    "Flávio Azevedo"
                ],
                "alt_related_terms": [
                    null
                ],
                "language": "english",
                "aliases": [
                    "/glossary/internal_validity"
                ]
            },
            {
                "type": "glossary",
                "title": "Intersectionality",
                "definition": "A term which derives from Black feminist thought and broadly describes how social identities exist within ‘interlocking systems of oppression’ and structures of (in)equalities (Crenshaw, 1989\\)**.** Intersectionality offers a perspective on the way multiple forms of inequality operate together to compound or exacerbate each other. Multiple concurrent forms of identity can have a multiplicative effect and are not merely the sum of the component elements. One implication is that identity cannot be adequately understood through examining a single axis (e.g., race, gender, sexual orientation, class) at a time in isolation, but requires simultaneous consideration of overlapping forms of identity.",
                "related_terms": [
                    "Bropenscience",
                    "Diversity",
                    "Inclusion",
                    "Interlocking",
                    "Open Science"
                ],
                "references": "Crenshaw, K. W. (1989). Demarginalizing the Intersection of Race and Sex: A Black Feminist Critique of Antidiscrimination Doctrine. University of Chicago Legal Forum, 1989(8), 139–168.\n\nGrzanka, P. R. (2020). From buzzword to critical psychology: An invitation to take intersectionality seriously. Women & Therapy, 43(3–4), 244–261.\n\nLedgerwood, A., Hudson, S. T. J., Lewis, Jr., N. A., Maddox, K. B., Pickett, C., Remedios, J. D., & Wilkins, C. L. (2021). The Pandemic as a Portal: Reimagining Psychological Science as Truly Open and Inclusive. https://doi.org/10.31234/osf.io/gdzue",
                "drafted_by": [
                    "Madeleine Pownall"
                ],
                "reviewed_by": [
                    "Ali H. Al-Hoorie",
                    "Bradley Baker",
                    "Mahmoud Elsherif",
                    "Wanyin Li",
                    "Ryan Millager",
                    "Charlotte R. Pennington",
                    "Flávio Azevedo"
                ],
                "alt_related_terms": [
                    null
                ],
                "language": "english",
                "aliases": [
                    "/glossary/intersectionality"
                ]
            },
            {
                "type": "glossary",
                "title": "JabRef",
                "definition": "An open-sourced, cross-platform citation and reference management tool that is available free of charge. It allows editing BibTeX files, importing data from online scientific databases, and managing and searching BibTeX files.",
                "related_terms": [
                    "Open source software"
                ],
                "references": "Team, J. D. (2021). JabRef - An open-source, cross-platform citation and reference management software. https://www.jabref.org",
                "drafted_by": [
                    "Aleksandra Lazić"
                ],
                "reviewed_by": [
                    "Christopher Graham",
                    "Michele C. Lim",
                    "Sam Parsons",
                    "Steven Verheyen"
                ],
                "alt_related_terms": [
                    null
                ],
                "language": "english",
                "aliases": [
                    "/glossary/jabref"
                ]
            },
            {
                "type": "glossary",
                "title": "Jamovi",
                "definition": "Free and open source software for data analysis based on the R language. The software has a graphical user interface and provides the R code to the analyses. Jamovi supports computational reproducibility by saving the data, code, analyses, and results in a single file.",
                "related_terms": [
                    "JASP",
                    "Open source",
                    "R",
                    "Reproducibility"
                ],
                "references": "",
                "drafted_by": [
                    "Amélie Beffara Bret"
                ],
                "reviewed_by": [
                    "Adrien Fillon",
                    "Alexander Hart",
                    "Charlotte R. Pennington"
                ],
                "alt_related_terms": [
                    null
                ],
                "language": "english",
                "aliases": [
                    "/glossary/jamovi"
                ]
            },
            {
                "type": "glossary",
                "title": "JASP",
                "definition": "Named after Sir Harold Jeffreys, JASP stands for Jeffrey’s Amazing Statistics Program. It is a free and open source software for data analysis. JASP relies on a user interface and offers both null hypothesis tests and their Bayesian counterparts. JASP supports computational reproducibility by saving the data, code, analyses, and results in a single file.",
                "related_terms": [
                    "Jamovi",
                    "Open source"
                ],
                "references": "Team, J. (2020). JASP (Version 0.14.1) [Computer software].",
                "drafted_by": [
                    "Amélie Beffara Bret"
                ],
                "reviewed_by": [
                    "Adrien Fillon, Adam Parker",
                    "Sam Parsons"
                ],
                "alt_related_terms": [
                    null
                ],
                "language": "english",
                "aliases": [
                    "/glossary/jasp"
                ]
            },
            {
                "type": "glossary",
                "title": "Journal Impact Factor™",
                "definition": "The mean number of citations to research articles in that journal over the preceding two years. It is a proprietary and opaque calculation marketed by Clarivate**™**. Journal Impact Factors are not associated with the content quality or the peer review process.",
                "related_terms": [
                    "DORA",
                    "H-index"
                ],
                "references": "Brembs, B., Button, K., & Munafò, M. (2013). Deep impact: unintended consequences of journal rank. Frontiers in Human Neuroscience, 7, 291. https://doi.org/10.3389/fnhum.2013.00291\n\nCurry, S. (2012). Sick of impact factors. http://occamstypewriter.org/scurry/2012/08/13/sick-of-impact-factors/\n\nNaudet, F., Ioannidis, J., Miedema, F., Cristea, I. A., Goodman, S. N., & Moher, D. (2018). Six principles for assessing scientists for hiring, promotion, and tenure. Impact of Social Sciences Blog.\n\nRossner, M., Van Epps, H., & Hill, E. (2008). Show me the data. https://doi.org/10.1083/jcb.200711140\n\nSharma, M., Sarin, A., Gupta, P., Sachdeva, S., & Desai, A. (2014). Journal impact factor: its use, significance and limitations. World Journal of Nuclear Medicine, 13(2), 146. https://doi.org/10.4103/1450-1147.139151",
                "drafted_by": [
                    "Jacob Miranda"
                ],
                "reviewed_by": [
                    "Tsvetomira Dumbalska",
                    "Adam Parker"
                ],
                "alt_related_terms": [
                    null
                ],
                "language": "english",
                "aliases": [
                    "/glossary/journal_impact_factor_"
                ]
            },
            {
                "type": "glossary",
                "title": "JSON file",
                "definition": "JavaScript Object Notation (JSON) is a data format for structured data that can be used to represent attribute-value pairs. Values thereby can contain further JSON notation (i.e., nested information). JSON files can be formally encoded as strings of text and thus are human-readable. Beyond storing information this feature makes them suitable for annotating other content. For example, JSON files are used in Brain Imaging Data Structure (BIDS) for describing the metadata dataset by following a standardized format (dataset\\_description.json).",
                "related_terms": [
                    "BIDS data structure",
                    "Metadata"
                ],
                "references": "BIDS. (n.d.). Modality agnostic files. Retrieved from https://bids-specification.readthedocs.io/en/stable/03-modality-agnostic-files.html",
                "drafted_by": [
                    "Tina Lonsdorf"
                ],
                "reviewed_by": [
                    "Alexander Hart",
                    "Matt Jaquiery",
                    "Emma Norris",
                    "Charlotte R. Pennington"
                ],
                "alt_related_terms": [
                    null
                ],
                "language": "english",
                "aliases": [
                    "/glossary/json_file"
                ]
            },
            {
                "type": "glossary",
                "title": "Knowledge acquisition",
                "definition": "The process by which the mind decodes or extracts, stores, and relates new information to existing information in long term memory. Given the complex structure and nature of knowledge, this process is studied in the philosophical field of epistemology, as well as the psychological field of learning and memory.",
                "related_terms": [
                    "Epistemology",
                    "Information",
                    "Learning"
                ],
                "references": "Brule, J., & Blount, A. (1989). Knowledge acquisition. McGraw-Hill.",
                "drafted_by": [
                    "Oscar Lecuona"
                ],
                "reviewed_by": [
                    "Bradley Baker",
                    "Helena Hartmann",
                    "Kai Krautter",
                    "Graham Reid"
                ],
                "alt_related_terms": [
                    null
                ],
                "language": "english",
                "aliases": [
                    "/glossary/knowledge_acquisition"
                ]
            },
            {
                "type": "glossary",
                "title": "Likelihood function",
                "definition": "A statistical model of the data used in frequentist and Bayesian analyses, defined up to a constant of proportionality. A likelihood function represents the likeliness of different parameters for your distribution given the data. Given that probability distributions have unknown population parameters, the likelihood function indicates how well the sample data summarise these parameters. As such, the likelihood function gives an idea of the goodness of fit of a model to the sample data for a given set of values of the unknown population parameters.",
                "related_terms": [
                    "Bayes factor",
                    "Bayesian inference",
                    "Bayesian parameter estimation",
                    "Posterior distribution",
                    "Prior distribution **Alternative definition:** For a more statistically-informed definition, given a parametric model specified by a probability (densidity) function f(x|theta), a likelihood *for* a statistical model is defined by the same formula as the density except that the roles of the data *x* and the parameter *theta* are interchanged, and thus the likelihood can be considered a function of *theta* for fixed data *x*. Here, then, the likelihood function would describe a curve or hypersurface whose peak, if it exists, represents the combination of model parameter values that maximize the probability of drawing the sample obtained."
                ],
                "references": "Dienes, Z. (2008). Understanding psychology as a science: An introduction to scientific and statistical inference. Macmillan International Higher Education.\n\nHogg, D., Bovy, J., & Lang, D. (2010). Data analysis recipes: Fitting a model to data. arXiv:1008.4686 [astro-ph.IM].\n\nGeyer, C. J. (2003). Maximum Likelihood in R (pp. 1–9). Open Science Framework.\n\nGeyer, C. J. (2007). Stat 5102 Notes: Maximum Likelihood (pp. 1–8). Open Science Framework.\n\nHuber, C. (2016). The Stata Blog: Introduction to Bayesian statistics, part 1: The basic concepts. In The Stata Blog. https://blog.stata.com/2016/11/01/introduction-to-bayesian-statistics-part-1-the-basic-concepts/",
                "drafted_by": [
                    "Alaa AlDoh"
                ],
                "reviewed_by": [
                    "Dominik Kiersz",
                    "Graham Reid",
                    "Sam Parsons",
                    "Flávio Azevedo"
                ],
                "alt_related_terms": [
                    null
                ],
                "language": "english",
                "aliases": [
                    "/glossary/likelihood_function"
                ]
            },
            {
                "type": "glossary",
                "title": "Likelihood Principle",
                "definition": "The notion that all information relevant to inference contained in data is provided by the likelihood. The principle suggests that the likelihood function can be used to compare the plausibility of various parameter values. While Bayesians and likelihood theorists subscribe to the likelihood principle, Neyman-Pearson theorists do not, as significance tests violate the likelihood principle because they take into account information not in the likelihood.",
                "related_terms": [
                    "Bayesian inference",
                    "Likelihood Function"
                ],
                "references": "Dienes, Z. (2008). Understanding psychology as a science: An introduction to scientific and statistical inference. Macmillan International Higher Education.\n\nGeyer, C. J. (2003). Maximum Likelihood in R (pp. 1–9). Open Science Framework.\n\nGeyer, C. J. (2007). Stat 5102 Notes: Maximum Likelihood (pp. 1–8). Open Science Framework.",
                "drafted_by": [
                    "Alaa Aldoh"
                ],
                "reviewed_by": [
                    "Sam Parsons",
                    "Flávio Azevedo"
                ],
                "alt_related_terms": [
                    null
                ],
                "language": "english",
                "aliases": [
                    "/glossary/likelihood_principle"
                ]
            },
            {
                "type": "glossary",
                "title": "Literature Review",
                "definition": "Researchers often review research records on a given topic to better understand effects and phenomena of interest before embarking on a new research project, to understand how theory links to evidence or to investigate common themes and directions of existing study results and claims. Different types of reviews can be conducted depending on the research question and literature scope. To determine the scope and key concepts in a given field, researchers may want to conduct a scoping literature review. Systematic reviews aim to access and review all available records for the most accurate and unbiased representation of existing literature. Non-systematic or focused literature reviews synthesise information from a selection of studies relevant to the research question although they are uncommon due to susceptibility to biases (e.g. researcher bias; Siddaway et al., 2019).",
                "related_terms": [
                    "Evidence synthesis",
                    "Meta-research",
                    "Narrative reviews",
                    "Systematic reviews"
                ],
                "references": "Huelin, R., Iheanacho, I., Payne, K., & Sandman, K. (2015). What’s in a name? Systematic and non-systematic literature reviews, and why the distinction matters. The Evidence Forum, 34–37. Retrieved from https://www.evidera.com/wp-content/uploads/2015/06/Whats-in-a-Name-Systematic-and-Non-Systematic-Literature-Reviews-and-Why-the-Distinction-Matters.pdf\n\nMunn, Z., Peters, M. D., Stern, C., Tufanaru, C., McArthur, A., & Aromataris, E. (2018). Systematic review or scoping review? Guidance for authors when choosing between a systematic or scoping review approach. BMC Medical Research Methodology, 18(1), 1–7. https://doi.org/10.1186/s12874-018-0611-x\n\nPautasso, M. (2013). Ten Simple Rules for Writing a Literature Review. PLoS Computational Biology, 9(7), e1003149. https://doi.org/10.1371/journal.pcbi.1003149\n\nSiddaway, A. P., Wood, A. M., & Hedges, L. V. (2019). How to do a systematic review: a best practice guide for conducting and reporting narrative reviews, meta-analyses, and meta-syntheses. Annual Review of Psychology, 70, 747–770. https://doi.org/10.1146/annurev-psych-010418-102803",
                "drafted_by": [
                    "Marta Topor"
                ],
                "reviewed_by": [],
                "alt_related_terms": [
                    null
                ],
                "language": "english",
                "aliases": [
                    "/glossary/literature_review"
                ]
            },
            {
                "type": "glossary",
                "title": "Manel",
                "definition": "Portmanteau for ‘male panel’, usually to refer to speaker panels at conferences entirely composed of (usually caucasian) males. Typically discussed in the context of gender disparities in academia (e.g., women being less likely to be recognised as experts by their peers and, subsequently, having fewer opportunities for career development).",
                "related_terms": [
                    "Bropenscience",
                    "Diversity",
                    "Equity",
                    "Feminist psychology",
                    "Inclusion",
                    "Under-representation"
                ],
                "references": "Bouvy, J. C., & Mujoomdar, M. (2019). All-Male Panels and Gender Diversity of Issue Panels and Plenary Sessions at ISPOR Europe. PharmacoEconomics-Open, 3(3), 419–422. https://doi.org/10.1007/s41669-019-0153-0\n\nGoodman, S. W., & Pepinsky, T. B. (2019). Gender Representation and Strategies for Panel Diversity: Lessons from the APSA Annual Meeting. PS: Political Science & Politics, 52(4), 669–676. https://doi.org/10.1017/S1049096519000908\n\nNittrouer, C., Hebl, M., Ashburn-Nardo, L., Trump-Steele, R., Lane, D., & Valian, V. (2018). Gender disparities in colloquium speakers. Proceedings of the National Academy of Sciences, 115(1), 104–108. https://doi.org/10.1073/pnas.1708414115\n\nRodriguez, J. K., & Günther, E. A. (2020). What’s wrong with manels and what can we do about it. The Conversation. https://theconversation.com/whats-wrong-with-manels-and-what-can-we-do-about-them-148068",
                "drafted_by": [
                    "Sam Parsons"
                ],
                "reviewed_by": [
                    "Mahmoud Elsherif",
                    "Thomas Rhys Evans",
                    "Beatrice Valentini",
                    "Christopher Graham",
                    "Flávio Azevedo"
                ],
                "alt_related_terms": [
                    null
                ],
                "language": "english",
                "aliases": [
                    "/glossary/manel"
                ]
            },
            {
                "type": "glossary",
                "title": "Many authors",
                "definition": "Large-scale collaborative projects involving tens or hundreds of authors from different institutions. This kind of approach has become increasingly common in psychology and other sciences in recent years as opposed to research carried out by small teams of authors, following earlier trends which have been observed e.g. for high-energy physics or biomedical research in the 1990s. These large international scientific consortia work on a research project to bring together a broader range of expertise and work collaboratively to produce manuscripts.",
                "related_terms": [
                    "Collaboration",
                    "Consortia",
                    "Consortium authorship",
                    "Crowdsourcing",
                    "Hyperauthorship",
                    "Multiple-authors",
                    "Team science"
                ],
                "references": "Cronin, B. (2001). Hyperauthorship: A postmodern perversion or evidence of a structural shift in scholarly communication practices? Journal of the American Society for Information Science and Technology, 52(7), 558–569. https://doi.org/10.1002/asi.1097\n\nMoshontz, H., Ebersole, C. R., Weston, S. J., & Klein, R. A. (2021). A guide for many authors: Writing manuscripts in large collaborations. Social and Personality Psychology Compass, 15(4). https://doi.org/10.1111/spc3.12590\n\nWuchty, S., Jones, B. F., & Uzzi, B. (2007). The increasing dominance of teams in production of knowledge. Science, 316(5827), 1036–1039. https://doi.org/10.1126/science.1136099",
                "drafted_by": [
                    "Yu-Fang Yang"
                ],
                "reviewed_by": [
                    "Christopher Graham",
                    "Adam Parker",
                    "Charlotte R. Pennington",
                    "Birgit Schmidt",
                    "Beatrice Valentini"
                ],
                "alt_related_terms": [
                    null
                ],
                "language": "english",
                "aliases": [
                    "/glossary/many_authors"
                ]
            },
            {
                "type": "glossary",
                "title": "Many Labs",
                "definition": "A crowdsourcing initiative led by the Open Science Collaboration (2015) whereby several hundred separate research groups from various universities run replication studies of published effects. This initiative is also known as “Many Labs I” and was subsequently followed by a “Many Labs II” project that assessed variation in replication results across samples and settings. Similar projects include ManyBabies, EEGManyLabs, and the Psychological Science Accelerator.",
                "related_terms": [
                    "Collaboration",
                    "Many analysts",
                    "Many Labs I",
                    "Many Labs II",
                    "Open Science Collaboration",
                    "Replication"
                ],
                "references": "Ebersole, C. R., Atherton, O. E., Belanger, A. L., Skulborstad, H. M., Allen, J. M., Banks, J. B., & Nosek, B. A. (2016). Many Labs 3: Evaluating participant pool quality across the academic semester via replication. Journal of Experimental Social Psychology, 67, 68–82. https://doi.org/10.1016/j.jesp.2015.10.012\n\nFrank, M. C., Bergelson, E., Bergmann, C., Cristia, A., Floccia, C., Gervain, J., Hamlin, J. K., Hannon, E. E., Kline, M., Levelt, C., Lew-Williams, C., Nazzi, T., Panneton, R., Rabagliati, H., Soderstrom, M., Sullivan, J., Waxman, S., & Yurovsky, D. (2017). A Collaborative Approach to Infant Research: Promoting Reproducibility, Best Practices, and Theory-Building. Infancy, 22, 421–435. https://doi.org/10.1111/infa.12182\n\nKlein, R. A., Ratliff, K. A., Vianello, M., Adams, R. B., Bahník, Š., Bernstein, M. J., & et al. (2014). Investigating variation in replicability: A “many labs” replication project. Social Psychology, 45, 142–152. https://doi.org/10.1027/1864-9335/a000178\n\nKlein, R. A., Vianello, M., Hasselman, F., Adams, B. G., Adams, R. B., Alper, S., & … Nosek, B. A. (2018). Many Labs 2: Investigating Variation in Replicability Across Samples and Settings. Advances in Methods and Practices in Psychological Science, 1(4), 443–490. https://doi.org/10.1177/2515245918810225\n\nMoshontz, H., Campbell, L., Ebersole, C. R., IJzerman, H., Urry, H. L., Forscher, P. S., & Chartier, C. R. (2018). The Psychological Science Accelerator: Advancing psychology through a distributed collaborative network. Advances in Methods and Practices in Psychological Science, 1(4), 501–515. https://doi.org/10.1177/2515245918797607\n\nPavlov, Y. G., Adamian, N., Appelhoff, S., Arvaneh, M., Benwell, C., Beste, C., & Mushtaq, F. (2020). #EEGManyLabs: Investigating the Replicability of Influential EEG Experiments. PsyArXiv Preprint. https://doi.org/10.31234/osf.io/528nr",
                "drafted_by": [
                    "Sam Parsons"
                ],
                "reviewed_by": [
                    "Helena Hartmann",
                    "Charlotte R. Pennington",
                    "Mirela Zaneva"
                ],
                "alt_related_terms": [
                    null
                ],
                "language": "english",
                "aliases": [
                    "/glossary/many_labs"
                ]
            },
            {
                "type": "glossary",
                "title": "Massive Open Online Courses (MOOCs)",
                "definition": "Exclusively online courses which are accessible to any learner at any time, are typically free to access (while not necessarily openly licensed), and provide video-based instructions and downloadable data sets and exercises. The “massive” aspect describes the high volume of students that can access the course at any one time due to their flexibility, low or no cost, and online nature of the materials.",
                "related_terms": [
                    "Accessibility",
                    "Distance education",
                    "Inclusion",
                    "Open learning"
                ],
                "references": "Baturay, M. H. (2015). An overview of the world of MOOCs. Procedia-Social and Behavioral Sciences, 174, 427–433. https://doi.org/10.1016/j.sbspro.2015.01.685",
                "drafted_by": [
                    "Elizabeth Collins"
                ],
                "reviewed_by": [
                    "Tsvetomira Dumbalska",
                    "Mahmoud Elsherif",
                    "Helena Hartmann",
                    "Sam Parsons",
                    "Charlotte R. Pennington"
                ],
                "alt_related_terms": [
                    null
                ],
                "language": "english",
                "aliases": [
                    "/glossary/massive_open_online_courses"
                ]
            },
            {
                "type": "glossary",
                "title": "Massively Open Online Papers (MOOPs)",
                "definition": "Unlike the traditional collaborative article, a MOOP follows an open participatory and dynamic model that is not restricted by a predetermined list of contributors.",
                "related_terms": [
                    "Citizen science",
                    "Collaboration",
                    "Crowdsourced Research",
                    "Many authors",
                    "Team science"
                ],
                "references": "Himmelstein, D. S., Rubinetti, V., Slochower, D. R., Hu, D., Malladi, V. S., Greene, C. S., & Gitter, A. (2019). Open collaborative writing with Manubot. PLOS Computational Biology, 15(6), e1007128. https://doi.org/10.1371/journal.pcbi.1007128\n\nTennant, J., Bielczyk, N. Z., Greshake Tzovaras, B., Masuzzo, P., & Steiner, T. (2019). Introducing Massively Open Online Papers (MOOPs). MetaArXiv. https://doi.org/10.31222/osf.io/et8ak",
                "drafted_by": [
                    "Ali H. Al-Hoorie"
                ],
                "reviewed_by": [],
                "alt_related_terms": [
                    null
                ],
                "language": "english",
                "aliases": [
                    "/glossary/massively_open_online_papers"
                ]
            },
            {
                "type": "glossary",
                "title": "Matthew effect (in science)",
                "definition": "Named for the ‘rich get richer; poor get poorer’ paraphrase of the Gospel of Matthew. Eminent scientists and early-career researchers with a prestigious fellowship are disproportionately attributed greater levels of credit and funding for their contributions to science while relatively unknown or early-career researchers without a prestigious fellowship tend to get disproportionately little credit for comparable contributions. The impact is a substantial cumulative advantage that results from modest initial comparative advantages (and vice versa).",
                "related_terms": [
                    "Matthew effect in education",
                    "Stigler’s law of eponymy"
                ],
                "references": "Bol, T., de Vaan, M., & van de Rijt, A. (2018). The Matthew effect in science funding. Proceedings of the National Academy of Sciences, 115(19), 4887–4890. https://doi.org/10.1073/pnas.1719557115\n\nBornmann, L., Ganser, C., Tekles, A., & Leydesdorff, L. (2019). Does the hα index reinforce the Matthew effect in science? Agent-based simulations using Stata and R. arXiv preprint https://arxiv.org/abs/1905.11052.\n\nMerton, R. K. (1968). The Matthew Effect in Science. Science, 159(3810), 56–63. https://doi.org/10.1126/science.159.3810.56",
                "drafted_by": [
                    "Tamara Kalandadze"
                ],
                "reviewed_by": [
                    "Bradley Baker",
                    "Tsvetomira Dumbalska",
                    "Mahmoud Elsherif",
                    "Matt Jaquiery",
                    "Charlotte R. Pennington"
                ],
                "alt_related_terms": [
                    null
                ],
                "language": "english",
                "aliases": [
                    "/glossary/matthew_effect"
                ]
            },
            {
                "type": "glossary",
                "title": "Meta-analysis",
                "definition": "A meta-analysis is a statistical synthesis of results from a series of studies examining the same phenomenon. A variety of meta-analytic approaches exist, including random or fixed effects models or meta-regressions, which allow for an examination of moderator effects. By aggregating data from multiple studies, a meta-analysis could provide a more precise estimate for a phenomenon (e.g. type of treatment) than individual studies. Results are usually visualized in a forest plot. Meta-analyses can also help examine heterogeneity across study results. Meta-analyses are often carried out in conjunction with systematic reviews and similarly require a systematic search and screening of studies. Publication bias is also commonly examined in the context of a meta-analysis and is typically visually presented via a funnel plot.",
                "related_terms": [
                    "CONSORT",
                    "Correlational Meta-Analysis",
                    "Effect size",
                    "Evidence synthesis",
                    "Non-Intervention, Reproducible, and Open Systematic Reviews (NIRO-SR)",
                    "PRISMA",
                    "Publication bias (File Drawer Problem)",
                    "STROBE",
                    "Systematic Review"
                ],
                "references": "Borenstein, M., Hedges, L. V., Higgins, J. P., & Rothstein, H. R. (2011). Introduction to meta-analysis. John Wiley & Sons.\n\nYeung, S. K., Feldman, G., Fillon, A., Protzko, J., Elsherif, M. M., Xiao, Q., & Pickering, J. (2020). Experimental Studies Meta-Analysis Registered Report Templates. https://osf.io/ytgrp/",
                "drafted_by": [
                    "Martin Vasilev; Siu Kit Yeung"
                ],
                "reviewed_by": [
                    "Thomas Rhys Evans",
                    "Tamara Kalandadze",
                    "Charlotte R. Pennington",
                    "Mirela Zaneva"
                ],
                "alt_related_terms": [
                    null
                ],
                "language": "english",
                "aliases": [
                    "/glossary/meta_analysis"
                ]
            },
            {
                "type": "glossary",
                "title": "Metadata",
                "definition": "Structured data that describes and synthesises other data. Metadata can help find, organize, and understand data. Examples of metadata include creator, title, contributors, keywords, tags, as well as any kind of information necessary to verify and understand the results and conclusions of a study such as codebook on data labels, descriptions, the sample and data collection process.",
                "related_terms": [
                    "Data",
                    "Open Data **Alternative definition:** (if applicable) Data about data"
                ],
                "references": "Gollwitzer, M., Abele-Brehm, A., Fiebach, C., Ramthun, R., Scheel, A. M., Schönbrodt, F. D., & Steinberg, U. (2020). Data Management and Data Sharing in Psychological Science: Revision of the DGPs Recommendations.",
                "drafted_by": [
                    "Matt Jaquiery"
                ],
                "reviewed_by": [
                    "Helena Hartmann",
                    "Tina Lonsdorf",
                    "Charlotte R. Pennington",
                    "Mirela Zaneva"
                ],
                "alt_related_terms": [
                    null
                ],
                "language": "english",
                "aliases": [
                    "/glossary/metadata"
                ]
            },
            {
                "type": "glossary",
                "title": "Meta-science or Meta-research",
                "definition": "The scientific study of science itself with the aim to describe, explain, evaluate and/or improve scientific practices. Meta-science typically investigates scientific methods, analyses, the reporting and evaluation of data, the reproducibility and replicability of research results, and research incentives.",
                "related_terms": [],
                "references": "Ioannidis, J. P., Fanelli, D., Dunne, D. D., & Goodman, S. N. (2015). Meta-research: Eevaluation and improvement of research methods and practices. PLoS Biology, 13(10), e1002264. https://doi.org/10.1371/journal.pbio.1002264\n\nPeterson, D., & Panofsky, A. (2020). Metascience as a scientific social movement. https://doi.org/10.31235/osf.io/4dsqa",
                "drafted_by": [
                    "Elizabeth Collins"
                ],
                "reviewed_by": [
                    "Tamara Kalandadze",
                    "Lisa Spitzer",
                    "Olmo van den Akker"
                ],
                "alt_related_terms": [
                    null
                ],
                "language": "english",
                "aliases": [
                    "/glossary/meta_science_or_meta_research"
                ]
            },
            {
                "type": "glossary",
                "title": "Model (computational)",
                "definition": "Computational models aim to mathematically translate the phenomena under study to better understand, communicate and predict complex behaviours.",
                "related_terms": [
                    "algorithms",
                    "data simulation",
                    "hypothesis",
                    "theory",
                    "theory building"
                ],
                "references": "Guest, O., & Martin, A. E. (2020). How computational modeling can force theory building in psychological science. Perspectives on Psychological Science. https://doi.org/10.1177/1745691620970585\n\nWilson, R. C., & Collins, A. G. (2019). Ten simple rules for the computational modeling of behavioral data. eLife, 8, e49547. https://doi.org/10.7554/eLife.49547",
                "drafted_by": [
                    "Charlotte R. Pennington"
                ],
                "reviewed_by": [
                    "Jamie P. Cockcroft",
                    "Mahmoud Elsherif",
                    "Meng Liu",
                    "Yu-Fang Yang",
                    "Michele C. Lim"
                ],
                "alt_related_terms": [
                    null
                ],
                "language": "english",
                "aliases": [
                    "/glossary/model"
                ]
            },
            {
                "type": "glossary",
                "title": "Model (statistical)",
                "definition": "A mathematical representation of observed data that aims to reflect the population under study, allowing for the better understanding of the phenomenon of interest, identification of relationships among variables and predictions about future instances. A classic example would be the application of Chi square to understand the relationship between smoking and cancer (Doll & Hill, 1954\\)**.**",
                "related_terms": [
                    "Bayesian Inference",
                    "Model (computational)",
                    "Model (philosophy)",
                    "Null Hypothesis Significance Testing (NHST) **Alternative definition:** A mathematical model that embodies a set of statistical assumptions concerning the generation of sample data and is used to apply statistical analysis."
                ],
                "references": "Doll, R., & Hill, A. B. (1954). The mortality of doctors in relation to their smoking habits; a preliminary report. British Medical Journal, 1(4877), 1451–1455. https://doi.org/10.1136/bmj.1.4877.1451",
                "drafted_by": [
                    "Jamie P. Cockcroft"
                ],
                "reviewed_by": [
                    "Alaa AlDoh",
                    "Mahmoud Elsherif",
                    "Meng Liu",
                    "Catia M. Oliveira",
                    "Charlotte R. Pennington"
                ],
                "alt_related_terms": [
                    null
                ],
                "language": "english",
                "aliases": [
                    "/glossary/model"
                ]
            },
            {
                "type": "glossary",
                "title": "Model (philosophy)",
                "definition": "The process by which a verbal description is formalised to remove ambiguity, while also constraining the dimensions a theory can span. The model is thus data derived. “Many scientific models are representational models: they represent a selected part or aspect of the world, which is the model’s target system” (Frigg & Hartman, 2020).",
                "related_terms": [
                    "Hypothesis",
                    "Theory",
                    "Theory building"
                ],
                "references": "Guest, O., & Martin, A. E. (2020). How computational modeling can force theory building in psychological science. Perspectives on Psychological Science. https://doi.org/10.1177/1745691620970585",
                "drafted_by": [
                    "Mahmoud Elsherif"
                ],
                "reviewed_by": [
                    "Charlotte R. Pennington",
                    "Michele C. Lim"
                ],
                "alt_related_terms": [
                    null
                ],
                "language": "english",
                "aliases": [
                    "/glossary/model"
                ]
            },
            {
                "type": "glossary",
                "title": "Multi-Analyst Studies",
                "definition": "In typical empirical studies, a single researcher or research team conducts the analysis, which creates uncertainty about the extent to which the choice of analysis influences the results. In multi-analyst studies, two or more researchers independently analyse the same research question or hypothesis on the same dataset. According to Aczel and colleagues (2021), a multi-analyst approach may be beneficial in increasing our confidence in a particular finding; uncovering the impact of analytical preferences across research teams; and highlighting the variability in such analytical approaches.",
                "related_terms": [
                    "Analytic flexibility",
                    "Crowdsourcing science",
                    "Data Analysis",
                    "Garden of Forking Paths",
                    "Multiverse Analysis",
                    "Researcher Degrees of Freedom",
                    "Scientific Transparency"
                ],
                "references": "Aczel, B., Szaszi, B., Nilsonne, G., Van den Akker, O., Albers, C. J., van Assen, M. A. L. M., ..., & Wagenmakers, E. (2021). Guidance for Multi-Analyst Studies. https://doi.org/10.31222/osf.io/5ecnh\n\nSilberzahn, R., Uhlmann, E. L., Martin, D. P., Anselmi, P., Aust, F., Awtrey, E., Bahník, Š., Bai, F., Bannard, C., Bonnier, E., & others. (2018). Many Analysts, One Data Set: Making Transparent How Variations in Analytic Choices Affect Results. Advances in Methods and Practices in Psychological Science, 337–356. https://doi.org/10.1177/2515245917747646",
                "drafted_by": [
                    "Sam Parsons"
                ],
                "reviewed_by": [
                    "Tsvetomira Dumbalska",
                    "Mahmoud Elsherif",
                    "William Ngiam",
                    "Charlotte R. Pennington",
                    "Graham Reid",
                    "Barnabas Szaszi",
                    "Flávio Azevedo"
                ],
                "alt_related_terms": [
                    null
                ],
                "language": "english",
                "aliases": [
                    "/glossary/multi_analyst_studies"
                ]
            },
            {
                "type": "glossary",
                "title": "Multiplicity",
                "definition": "Potential inflation of Type I error rates (incorrectly rejecting the null hypothesis) because of multiple statistical testing, for example, multiple outcomes, multiple follow-up time points, or multiple subgroup analyses. To overcome issues with multiplicity, researchers will often apply controlling procedures (e.g., Bonferroni, Holm-Bonferroni; Tukey) that correct the alpha value to control for inflated Type I errors. However, by controlling for Type I errors, one can increase the possibility of Type II errors (i.e., incorrectly accepting the null hypothesis).",
                "related_terms": [
                    "Alpha",
                    "False Discovery Rate",
                    "Multiple comparisons problem",
                    "Multiple testing",
                    "Null Hypothesis Significance Testing (NHST)"
                ],
                "references": "Sato, T. (1996). Type I and Type II error in multiple comparisons. The Journal of Psychology, 130(3), 293–302. https://doi.org/10.1080/00223980.1996.9915010\n\nSchulz, K. F., & Grimes, D. A. (2005). Multiplicity in randomised trials I: endpoints and treatments. The Lancet, 365(9470), 1591–1595. https://doi.org/10.1016/S0140-6736(05)66461-6",
                "drafted_by": [
                    "Aidan Cashin"
                ],
                "reviewed_by": [
                    "Jamie P. Cockcroft",
                    "Mahmoud Elsherif",
                    "Meng Liu",
                    "Charlotte R. Pennington"
                ],
                "alt_related_terms": [
                    null
                ],
                "language": "english",
                "aliases": [
                    "/glossary/multiplicity"
                ]
            },
            {
                "type": "glossary",
                "title": "Multiverse analysis",
                "definition": "Multiverse analyses are based on all potentially equally justifiable data processing and statistical analysis pipelines that can be employed to test a single hypothesis. In a data multiverse analysis, a single set of raw data is processed into a multiverse of data sets by applying all possible combinations of justifiable preprocessing choices. Model multiverse analyses apply equally justifiable statistical models to the same data to answer the same hypothesis. The statistical analysis is then conducted on all data sets in the multiverse and all results are reported which enhances promoting transparency and illustrates the robustness of results against different data processing (data multiverse) or statistical (model multiverse) pipelines). Multiverse analysis differs from Specification curve analysis with regards to the graphical displays (a histogram and tile plota rather than a specification curve plot).",
                "related_terms": [
                    "Garden of forking paths",
                    "Robustness (analyses)",
                    "Specification curve analysis",
                    "Vibration of effects"
                ],
                "references": "Del Giudice, M., & Gangestad, S. W. (2021). A traveler’s guide to the multiverse: Promises, pitfalls, and a framework for the evaluation of analytic decisions. Advances in Methods and Practices in Psychological Science, 4(1), 2515245920954925. https://doi.org/10.1177/2515245920954925\n\nSteegen, S., Tuerlinckx, F., Gelman, A., & Vanpaemel, W. (2016). Increasing Transparency through a Multiverse Analysis. Perspectives on Psychological Science, 11, 702–712. https://doi.org/10.1177/1745691616658637",
                "drafted_by": [
                    "Tina Lonsdorf; Flávio Azevedo"
                ],
                "reviewed_by": [
                    "Mahmoud Elsherif",
                    "Adrien Fillon",
                    "William Ngiam",
                    "Sam Parsons"
                ],
                "alt_related_terms": [
                    null
                ],
                "language": "english",
                "aliases": [
                    "/glossary/multiverse_analysis"
                ]
            },
            {
                "type": "glossary",
                "title": "Name Ambiguity Problem",
                "definition": "An attribution issue arising from two related problems: authors may use multiple names or monikers to publish work, and multiple authors in a single field may share full names. This makes accurate identification of authors on names and specialisms alone a difficult task. This can be addressed through the creation and use of unique digital identifiers that act akin to digital fingerprints such as ORCID.",
                "related_terms": [
                    "Authorship",
                    "DOI (digital object identifier)",
                    "ORCID (Open Researcher and Contributor ID)"
                ],
                "references": "Wilson, B., & Fenner, M. (2012). Open Researcher & Contributor ID (ORCID): Solving the Name Ambiguity Problem. Educause Review - E-Content, 47(3), 54–55. https://er.educause.edu/articles/2012/5/open-researcher--contributor-id-orcid-solving-the-name-ambiguity-problem",
                "drafted_by": [
                    "Shannon Francis"
                ],
                "reviewed_by": [
                    "Tsvetomira Dumbalska",
                    "Mahmoud Elsherif",
                    "Helena Hartmann",
                    "Wanyin Li",
                    "Charlotte R. Pennington"
                ],
                "alt_related_terms": [
                    null
                ],
                "language": "english",
                "aliases": [
                    "/glossary/name_ambiguity_problem"
                ]
            },
            {
                "type": "glossary",
                "title": "Named entity-based Text Anonymization for Open Science (NETANOS)",
                "definition": "A free, open-source anonymisation software that identifies and modifies named entities (e.g. persons, locations, times, dates). Its key feature is that it preserves critical context needed for secondary analyses. The aim is to assist researchers in sharing their raw text data, while adhering to research ethics.",
                "related_terms": [
                    "Anonymity",
                    "Confidentiality",
                    "Data sharing",
                    "Research ethics"
                ],
                "references": "Kleinberg, B., Mozes, M., van der Toolen, Y., & Verschuere, B. (2017). NETANOS - Named entity-based Text Anonymization for Open Science. https://osf.io/w9nhb/",
                "drafted_by": [
                    "Norbert Vanek"
                ],
                "reviewed_by": [
                    "Jamie P. Cockcroft",
                    "Aleksandra Lazić",
                    "Charlotte R. Pennington",
                    "Sam Parsons"
                ],
                "alt_related_terms": [
                    null
                ],
                "language": "english",
                "aliases": [
                    "/glossary/named_entity_based_text_anonymization_for_open_science"
                ]
            },
            {
                "type": "glossary",
                "title": "Non-Intervention, Reproducible, and Open Systematic Reviews (NIRO-SR)",
                "definition": "A comprehensive set of tools to facilitate the development, preregistration and dissemination of systematic literature reviews for non-intervention research. Part A represents detailed guidelines for creating and preregistering a systematic review protocol in the context of non-intervention research whilst preparing for transparency. Part B represents guidelines for writing up the completed systematic review, with a focus on enhancing reproducibility.",
                "related_terms": [
                    "Knowledge accumulation",
                    "Systematic review",
                    "Systematic Review Protocol"
                ],
                "references": "Topor, M., Pickering, J. S., Barbosa Mendes, A., Bishop, D. V. M., Büttner, F. C., Elsherif, M. M., & others. (2021). An integrative framework for planning and conducting Non-Intervention, Reproducible, and Open Systematic Reviews (NIRO-SR). https://doi.org/10.31222/osf.io/8gu5z",
                "drafted_by": [
                    "Asma Assaneea"
                ],
                "reviewed_by": [
                    "Tsvetomira Dumbalska",
                    "Thomas Rhys Evans",
                    "Tamara Kalandadze",
                    "Jade Pickering",
                    "Mirela Zaneva"
                ],
                "alt_related_terms": [
                    null
                ],
                "language": "english",
                "aliases": [
                    "/glossary/non_intervention_reproducible_and_open_systematic_reviews"
                ]
            },
            {
                "type": "glossary",
                "title": "Null Hypothesis Significance Testing (NHST)",
                "definition": "A frequentist approach to inference used to test the probability of an observed effect against the null hypothesis of no effect/relationship (Pernet, 2015). Such a conclusion is arrived at through use of an index called the *p*\\-value. Specifically, researchers will conclude an effect is present when an a priori alpha threshold, set by the researchers, is satisfied; this determines the acceptable level of uncertainty and is closely related to Type I error.",
                "related_terms": [
                    "Inference",
                    "P-value",
                    "Statistical significance",
                    "Type I error"
                ],
                "references": "Lakens, D., Scheel, A. M., & Isager, P. M. (2018). Equivalence testing for psychological research: A tutorial. Advances in Methods and Practices in Psychological Science, 1(2), 259–269. https://doi.org/10.1177/2515245918770963\n\nPernet, C. R. (2015). Null hypothesis significance testing: a short tutorial. F1000Research, 4, 621. https://doi.org/10.12688/f1000research.6963.3\n\nSpence, J. R., & Stanley, D. J. (2018). Concise, simple, and not wrong: In search of a short-hand interpretation of statistical significance. Frontiers in Psychology, 9, 2185. https://doi.org/10.3389/fpsyg.2018.02185",
                "drafted_by": [
                    "Alaa AlDoh"
                ],
                "reviewed_by": [
                    "Jamie P. Cockcroft",
                    "Annalise A. LaPlume",
                    "Charlotte R. Pennington",
                    "Sonia Rishi"
                ],
                "alt_related_terms": [
                    null
                ],
                "language": "english",
                "aliases": [
                    "/glossary/null_hypothesis_significance_testing"
                ]
            },
            {
                "type": "glossary",
                "title": "Objectivity",
                "definition": "The idea that scientific claims, methods, results and scientists themselves should remain value-free and unbiased, and thus not be affected by cultural, political, racial or religious bias as well as any personal interests (Merton, 1942).",
                "related_terms": [
                    "Communality",
                    "Mertonian norms",
                    "Neutrality"
                ],
                "references": "Macfarlane, B., & Cheng, M. (2008). Communism, Universalism and Disinterestedness: Re-examining Contemporary Support among Academics for Merton’s Scientific Norms. Journal of Academic Ethics, 6(1), 67–78. https://doi.org/10.1007/s10805-008-9055-y\n\nMerton, R. K. (1942). A note on science and democracy. Journal of Legal and Political Sociology, 1, 115–126. https://doi.org/10.1515/9783110375008-013",
                "drafted_by": [
                    "Ryan Millager"
                ],
                "reviewed_by": [
                    "Mahmoud Elsherif",
                    "Madeleine Ingham",
                    "Kai Krautter",
                    "Sam Parsons",
                    "Charlotte R. Pennington"
                ],
                "alt_related_terms": [
                    null
                ],
                "language": "english",
                "aliases": [
                    "/glossary/objectivity"
                ]
            },
            {
                "type": "glossary",
                "title": "Ontology (Artificial Intelligence)",
                "definition": "A set of axioms in a subject area that help classify and explain the nature of the entities under study and the relationships between them.",
                "related_terms": [
                    "Axiology",
                    "Epistemology",
                    "Taxonomy"
                ],
                "references": "Noy, N. F., & McGuinness, D. L. (2001). Ontology development 101: A guide to creating your first ontology. https://corais.org/sites/default/files/ontology_development_101_aguide_to_creating_your_first_ontology.pdf",
                "drafted_by": [
                    "Emma Norris"
                ],
                "reviewed_by": [
                    "Charlotte R. Pennington",
                    "Graham Reid"
                ],
                "alt_related_terms": [
                    null
                ],
                "language": "english",
                "aliases": [
                    "/glossary/ontology"
                ]
            },
            {
                "type": "glossary",
                "title": "Open access",
                "definition": "“Free availability of scholarship on the public internet, permitting any users to read, download, copy, distribute, print, search, or link to the full texts of these research articles, crawl them for indexing, pass them as data to software, or use them for any other lawful purpose, without financial, legal, or technical barriers other than those inseparable from gaining access to the internet itself” (Boai, 2002). Different methods of achieving open access (OA) are often referred to by color, including Green Open Access (when the work is openly accessible from a public repository), Gold Open Access (when the work is immediately openly accessible upon publication via a journal website), and Platinum (or Diamond) Open Access (a subset of Gold OA in which all works in the journal are immediately accessible after publication from the journal website without the authors needing to pay an article processing fee \\[APC\\]).",
                "related_terms": [
                    "Article Processing Charge",
                    "FAIR principles",
                    "Paywall",
                    "Preprint",
                    "Repository"
                ],
                "references": "Budapest Open Access Initiative. (2002). Read the Budapest open access initiative. Retrieved from https://www.budapestopenaccessinitiative.org/read\n\nSuber, P. (2015). Open Access Overview. http://legacy.earlham.edu/~peters/fos/overview.htm",
                "drafted_by": [
                    "Mahmoud Elsherif"
                ],
                "reviewed_by": [
                    "Nick Ballou",
                    "Helena Hartmann",
                    "Aoife O’Mahony",
                    "Ross Mounce",
                    "Mariella Paul",
                    "Charlotte R. Pennington"
                ],
                "alt_related_terms": [
                    null
                ],
                "language": "english",
                "aliases": [
                    "/glossary/open_access"
                ]
            },
            {
                "type": "glossary",
                "title": "Open Code",
                "definition": "Making computer code (e.g., programming, analysis code, stimuli generation) freely and publicly available in order to make research methodology and analysis transparent and allow for reproducibility and collaboration. Code can be made available via open code websites, such as GitHub, the Open Science Framework, and Codeshare (to name a few), enabling others to evaluate and correct errors and re-use and modify the code for subsequent research.",
                "related_terms": [
                    "Computational Reproducibility",
                    "Open Access",
                    "Open Licensing",
                    "Open Material",
                    "Open Source",
                    "Open Source Software",
                    "Reproducibility",
                    "Syntax"
                ],
                "references": "Easterbrook, S. M. (2014). Open code for open science? Nature Geoscience, 7(11), 779–781. https://doi.org/10.1038/ngeo2283",
                "drafted_by": [
                    "Charlotte R. Pennington"
                ],
                "reviewed_by": [
                    "Elizabeth Collins",
                    "Mahmoud Elsherif",
                    "Christopher Graham",
                    "Emma Henderson"
                ],
                "alt_related_terms": [
                    null
                ],
                "language": "english",
                "aliases": [
                    "/glossary/open_code"
                ]
            },
            {
                "type": "glossary",
                "title": "Open Data",
                "definition": "Open data refers to data that is freely available and readily accessible for use by others without restriction, “Open data and content can be freely used, modified, and shared by anyone for any purpose” ([https://opendefinition.org/](https://opendefinition.org/) ). Open data are subject to the requirement to attribute and share alike, thus it is important to consider appropriate Open Licenses. Sensitive or time-sensitive datasets can be embargoed or shared with more selective access options to ensure data integrity is upheld.",
                "related_terms": [
                    "Badges (Open Science)",
                    "Data availability",
                    "FAIR principles",
                    "Metadata",
                    "Open Licenses",
                    "Open Material",
                    "Reproducibility",
                    "Secondary data analysis"
                ],
                "references": "Definition, T. O. (n.d.). The Open Definition—Open Definition—Defining Open in Open Data, Open Content and Open Knowledge. Open Knowledge Foundation. https://opendefinition.org/\n\nHandbook, O. D. (n.d.). What is Open Data? Retrieved 9 July 2021. https://opendatahandbook.org/guide/en/what-is-open-data/",
                "drafted_by": [
                    "Lisa Spitzer"
                ],
                "reviewed_by": [
                    "Sarah Ashcroft-Jones",
                    "Mahmoud Elsherif",
                    "Helena Hartmann",
                    "Matt Jaquiery",
                    "Flávio Azevedo",
                    "Ross Mounce",
                    "Charlotte R. Pennington",
                    "Steven Verheyen"
                ],
                "alt_related_terms": [
                    null
                ],
                "language": "english",
                "aliases": [
                    "/glossary/open_data"
                ]
            },
            {
                "type": "glossary",
                "title": "Open Educational Resources (OERs)",
                "definition": "Learning materials that can be modified and enhanced because their creators have given others permission to do so. The individuals or organizations that create OERs—which can include materials such as presentation slides, podcasts, syllabi, images, lesson plans, lecture videos, maps, worksheets, and even entire textbooks—waive some (if not all) of the copyright associated with their works, typically via legal tools like Creative Commons licenses, so others can freely access, reuse, translate, and modify them.",
                "related_terms": [
                    "Accessibility",
                    "FORRT",
                    "Open access",
                    "Open Licenses",
                    "Open Material"
                ],
                "references": "Opensource.Com. (n.d.). What is open education? Retrieved 9 July 2021. https://opensource.com/resources/what-open-education",
                "drafted_by": [
                    "Aleksandra Lazić"
                ],
                "reviewed_by": [
                    "Sam Parsons",
                    "Charlotte R. Pennington",
                    "Steven Verheyen",
                    "Elizabeth Collins"
                ],
                "alt_related_terms": [
                    null
                ],
                "language": "english",
                "aliases": [
                    "/glossary/open_educational_resources"
                ]
            },
            {
                "type": "glossary",
                "title": "Open Educational Resources (OER) Commons",
                "definition": "OER Commons (with OER standing for open educational resources) is a freely accessible online library allowing teachers to create, share and remix educational resources. The goal of the OER movement is to stimulate “collaborative teaching and learning” ([https://www.oercommons.org/about](https://www.oercommons.org/about)) and provide high-quality educational resources that are accessible for everyone.",
                "related_terms": [
                    "Equity",
                    "FORRT",
                    "Inclusion",
                    "Open Scholarship Knowledge Base",
                    "Open Science Framework"
                ],
                "references": "OER Commons. (n.d.). OER Commons. https://www.oercommons.org/",
                "drafted_by": [
                    "Ali H. Al-Hoorie"
                ],
                "reviewed_by": [
                    "Mahmoud Elsherif, Gisela H. Govaart"
                ],
                "alt_related_terms": [
                    null
                ],
                "language": "english",
                "aliases": [
                    "/glossary/open_educational_resources"
                ]
            },
            {
                "type": "glossary",
                "title": "Open Licenses",
                "definition": "Open licenses are provided with open data and open software (e.g., analysis code) to define how others can (re)use the licensed material. In setting out the permissions and restrictions, open licenses often permit the unrestricted access, reuse and retribution of an author’s original work. Datasets are typically licensed under a type of open licence known as a Creative Commons license (e.g., MIT, Apache, and GPL). These can differ in relatively subtle ways with GPL licenses (and their variants) being Copyleft licenses that require that any derivative work is licensed under the same terms as the original.",
                "related_terms": [
                    "Creative Commons (CC) License",
                    "Copyleft",
                    "Copyright",
                    "Licence",
                    "Open Data",
                    "Open Source"
                ],
                "references": "",
                "drafted_by": [
                    "Andrew J. Stewart"
                ],
                "reviewed_by": [
                    "Elizabeth Collins",
                    "Sam Parsons",
                    "Graham Reid",
                    "Steven Verheyen"
                ],
                "alt_related_terms": [
                    null
                ],
                "language": "english",
                "aliases": [
                    "/glossary/open_licenses"
                ]
            },
            {
                "type": "glossary",
                "title": "Open Material",
                "definition": "Author’s public sharing of materials that were used in a study, “such as survey items, stimulus materials, and experiment programs” (Kidwell et al., 2016, p. 3). Digitally-shareable materials are posted on open access repositories, which makes them publicly available and accessible. Depending on licensing, the material can be reused by other authors for their own studies. Components that are not digitally-shareable (e.g. biological materials, equipment) must be described in sufficient detail to allow reproducibility.",
                "related_terms": [
                    "Badges (Open Science)",
                    "Credibility of scientific claims",
                    "FAIR principles",
                    "Open Access",
                    "Open Code",
                    "Open Data",
                    "Reproducibility",
                    "Transparency"
                ],
                "references": "Blohowiak, B. B., Cohoon, J., de Wit, L., Eich, E., Farach, F. J., Hasselman, F., & others. (2020). Badges to Acknowledge Open Practices. Retrieved from https://osf.io/tvyxz\n\nKidwell, M. C., Lazarević, L. B., Baranski, E., Hardwicke, T. E., Piechowski, S., Falkenberg, L. S., & Nosek, B. A. (2016). Badges to acknowledge open practices: A simple, low-cost, effective method for increasing transparency. PLoS Biology, 14(5), e1002456. https://doi.org/10.1371/journal.pbio.1002456",
                "drafted_by": [
                    "Lisa Spitzer"
                ],
                "reviewed_by": [
                    "Sam Parsons",
                    "Charlotte R. Pennington",
                    "Olly Robertson",
                    "Emily A. Williams",
                    "Flávio Azevedo"
                ],
                "alt_related_terms": [
                    null
                ],
                "language": "english",
                "aliases": [
                    "/glossary/open_material"
                ]
            },
            {
                "type": "glossary",
                "title": "OpenNeuro",
                "definition": "A free platform where researchers can freely and openly share, browse, download and re-use brain imaging data (e.g., MRI, MEG, EEG, iEEG, ECoG, ASL, and PET data).",
                "related_terms": [
                    "BIDS data structure",
                    "Open data",
                    "OpenfMRI"
                ],
                "references": "Poldrack, R. A., Barch, D. M., Mitchell, J. P., Wager, T. D., Wagner, A. D., Devlin, J. T., Cumba, C., Koyejo, O., & Milham, M. P. (2013). Toward open sharing of task-based fMRI data: The OpenfMRI project. Frontiers in Neuroinformatics, 7, 1–12. https://doi.org/10.3389/fninf.2013.00012\n\nPoldrack, R. A., & Gorgolewski, K. J. (2014). Making big data open: Data sharing in neuroimaging. Nature Neuroscience, 17(11), 1510–1517. https://doi.org/10.1038/nn.3818\n\nOpenNeuro. (n.d.). A free and open platform for sharing MRI, MEG, EEG, iEEG, ECoG, ASL, and PET data—OpenNeuro. OpenNeuro. https://openneuro.org/",
                "drafted_by": [
                    "Ali H. Al-Hoorie"
                ],
                "reviewed_by": [
                    "Leticia Micheli, Gisela H. Govaart"
                ],
                "alt_related_terms": [
                    null
                ],
                "language": "english",
                "aliases": [
                    "/glossary/openneuro"
                ]
            },
            {
                "type": "glossary",
                "title": "Open Peer Review",
                "definition": "A scholarly review mechanism providing disclosure of any combination of author and referee identities, as well as peer-review reports and editorial decision letters, to one another or publicly at any point during or after the peer review or publication process. It may also refer to the removal of restrictions on who can participate in peer review and the platforms for doing so. Note that ‘open peer review’ has been used interchangeably to refer to any, or all, of the above practices.",
                "related_terms": [
                    "Non-anonymised peer review",
                    "Open science",
                    "PRO (peer review openness) initiative",
                    "Transparent peer review"
                ],
                "references": "",
                "drafted_by": [
                    "Sonia Rishi"
                ],
                "reviewed_by": [
                    "Mahmoud Elsherif",
                    "Sam Parsons",
                    "Charlotte R. Pennington",
                    "Yuki Yamada",
                    "Flávio Azevedo"
                ],
                "alt_related_terms": [
                    null
                ],
                "language": "english",
                "aliases": [
                    "/glossary/open_peer_review"
                ]
            },
            {
                "type": "glossary",
                "title": "Open Scholarship",
                "definition": "‘**Open scholarship’ is often used synonymously with ‘open science’, but extends to all disciplines, drawing in those which might not traditionally identify as science-based. It reflects the idea that knowledge of all kinds should be openly shared, transparent, rigorous, reproducible, replicable, accumulative, and inclusive (allowing for all knowledge systems). Open scholarship includes all scholarly activities that are not solely limited to research such as teaching and pedagogy.",
                "related_terms": [
                    "Bropenscience",
                    "Decolonisation",
                    "Knowledge",
                    "Open Research",
                    "Open Science"
                ],
                "references": "Tennant, J., Beamer, J. E., Bosman, J., Brembs, B., Chung, N. C., Clement, G., Crick, T., Dugan, J., Dunning, A., Eccles, D., Enkhbayar, A., Graziotin, D., Harding, R., Havemann, J., Katz, D. S., Khanal, K., Kjaer, J. N., Koder, T., Macklin, P., & Turner, A. (2019). Foundations for Open Scholarship Strategy Development. MetaArXiv. https://doi.org/10.31222/osf.io/b4v8p",
                "drafted_by": [
                    "Gerald Vineyard"
                ],
                "reviewed_by": [
                    "Mahmoud Elsherif",
                    "Zoe Flack",
                    "Sam Parsons",
                    "Charlotte R. Pennington"
                ],
                "alt_related_terms": [
                    null
                ],
                "language": "english",
                "aliases": [
                    "/glossary/open_scholarship"
                ]
            },
            {
                "type": "glossary",
                "title": "Open Scholarship Knowledge Base",
                "definition": "The Open Scholarship Knowledge Base (OSKB) is a collaborative initiative to share knowledge on the what, why and how of open scholarship to make this knowledge easy to find and apply. Information is curated and created by the community. The OSKB is a community under the Center for Open Science (COS).",
                "related_terms": [
                    "Center for Open Science (COS), Open Educational Resources (OERs)",
                    "Open scholarship",
                    "Open Science"
                ],
                "references": "",
                "drafted_by": [
                    "Ali H. Al-Hoorie"
                ],
                "reviewed_by": [
                    "Mahmoud Elsherif",
                    "Samuel Guay",
                    "Tamara Kalandadze"
                ],
                "alt_related_terms": [
                    null
                ],
                "language": "english",
                "aliases": [
                    "/glossary/open_scholarship_knowledge_base"
                ]
            },
            {
                "type": "glossary",
                "title": "Open Science",
                "definition": "An umbrella term reflecting the idea that scientific knowledge of all kinds, where appropriate, should be openly accessible, transparent, rigorous, reproducible, replicable, accumulative, and inclusive, all which are considered fundamental features of the scientific endeavour. Open science consists of principles and behaviors that promote transparent, credible, reproducible, and accessible science. Open science has six major aspects: open data, open methodology, open source, open access, open peer review, and open educational resources.",
                "related_terms": [
                    "Accessibility",
                    "Credibility",
                    "Open Data",
                    "Open Material",
                    "Open Peer Review",
                    "Open Research",
                    "Open Science Practices",
                    "Open Scholarship",
                    "Reproducibility crisis (aka Replicability or replication crisis)",
                    "Reproducibility",
                    "Transparency"
                ],
                "references": "Abele-Brehm, A. E., Gollwitzer, M., Steinberg, U., & Schönbrodt, F. D. (2019). Attitudes toward open science and public data sharing. Social Psychology, 50, 252–260. https://doi.org/10.1027/1864-9335/a000384\n\nCrüwell, S., van Doorn, J., Etz, A., Makel, M. C., Moshontz, H., Niebaum, J. C., Orben, A., Parsons, S., & Schulte-Mecklenbeck, M. (2019). Seven Easy Steps to Open Science: An Annotated Reading List. Zeitschrift Für Psychologie, 227(4), 237–248. https://doi.org/10.1027/2151-2604/a000387\n\nKathawalla, U., Silverstein, P., & Syed, M. (2020). Easing into Open Science: A Guide for Graduate Students and Their Advisors. Collabra: Psychology. https://doi.org/10.31234/osf.io/vzjdp Retrieved from https://psyarxiv.com/vzjdp\n\nSyed, M. (2019). The Open Science Movement is for all of us. PsyArXiv.\n\nWoelfle, M., Olliaro, P., & Todd, M. H. (2011). Open science is a research accelerator. Nature Chemistry, 3(10), 745–748. https://doi.org/10.1038/nchem.1149",
                "drafted_by": [
                    "Mahmoud Elsherif"
                ],
                "reviewed_by": [
                    "Zoe Flack",
                    "Tamara Kalandadze",
                    "Charlotte R. Pennington",
                    "Qinyu Xiao"
                ],
                "alt_related_terms": [
                    null
                ],
                "language": "english",
                "aliases": [
                    "/glossary/open_science"
                ]
            },
            {
                "type": "glossary",
                "title": "Open Science Framework",
                "definition": "A free and open source platform for researchers to organize and share their research project and to encourage collaboration. Often used as an open repository for research code, data and materials, preprints and preregistrations, while managing a more efficient workflow. Created and maintained by the Center for Open Science.",
                "related_terms": [
                    "Archive",
                    "Center for Open Science (COS)",
                    "Open Code",
                    "Open Data",
                    "Preprint",
                    "Preregistration"
                ],
                "references": "Foster, E. D., & Deardorff, A. (2017). Open science framework (OSF). Journal of the Medical Library Association, 105(2), 203. https://doi.org/10.5195/jmla.2017.88\n\nfor Open Science, C. (2011–2021). Open Science Framework. https://osf.io/",
                "drafted_by": [
                    "William Ngiam"
                ],
                "reviewed_by": [
                    "Mahmoud Elsherif",
                    "Charlotte R. Pennington",
                    "Lisa Spitzer"
                ],
                "alt_related_terms": [
                    null
                ],
                "language": "english",
                "aliases": [
                    "/glossary/open_science_framework"
                ]
            },
            {
                "type": "glossary",
                "title": "Open Source software",
                "definition": "A type of computer software in which source code is released under a license that permits others to use, change, and distribute the software to anyone and for any purpose. Open source is more than openly accessible: the distribution terms of open-source software must comply with 10 specific criteria (see: [https://opensource.org/osd](https://opensource.org/osd)).",
                "related_terms": [
                    "Github",
                    "Open Access",
                    "Open Code",
                    "Open Data",
                    "Open Licenses",
                    "Python",
                    "R",
                    "Repository"
                ],
                "references": "Anon. (n.d.). Open Source in Open Science | FOSTER. Retrieved from https://www.fosteropenscience.eu/foster-taxonomy/open-source-open-science",
                "drafted_by": [
                    "Connor Keating"
                ],
                "reviewed_by": [
                    "Jamie P. Cockcroft",
                    "Helena Hartmann",
                    "Charlotte R. Pennington",
                    "Andrew J. Stewart"
                ],
                "alt_related_terms": [
                    null
                ],
                "language": "english",
                "aliases": [
                    "/glossary/open_source_software"
                ]
            },
            {
                "type": "glossary",
                "title": "Open washing",
                "definition": "Open washing, termed after “greenwashing”, refers to the act of claiming openness to secure perceptions of rigor or prestige associated with open practices. It has been used to characterise the marketing strategy of software companies that have the appearance of open-source and open-licensing, while engaging in proprietary practices. Open washing is a growing concern for those adopting open science practices as their actions are undermined by misleading uses of the practices, and actions designed to facilitate progressive developments are reduced to ‘ticking the box’ without clear quality control.",
                "related_terms": [
                    "Open Access",
                    "Open Data",
                    "Open Source"
                ],
                "references": "Farrow, R. (2017). Open Education and Critical Pedagogy. Learning, Media and Technology, 42(2), 130–146. https://doi.org/10.1080/17439884.2016.1113991\n\nMoretti, M. (2020). Beyond Open-washing: Are Narratives the Future of Open Data Portals? Medium Blog. https://medium.com/nightingale/beyond-open-washing-are-stories-and-narratives-the-future-of-open-data-portals-93228d8882f3\n\nVillum, C. (2016). “Open-washing” – The difference between opening your data and simply making them available. https://blog.okfn.org/2014/03/10/open-washing-the-difference-between-opening-your-data-and-simply-making-them-available/\n\nVlaeminck, S., & Podkrajac, F. (2017). Journals in Economic Sciences: Paying Lip Service to Reproducible Research? IASSIST Quarterly, 41(1–4), 16. https://doi.org/10.29173/iq6",
                "drafted_by": [
                    "Meng Liu"
                ],
                "reviewed_by": [
                    "Thomas Rhys Evans",
                    "Sam Guay",
                    "Sam Parsons",
                    "Charlotte R. Pennington",
                    "Beatrice Valentini"
                ],
                "alt_related_terms": [
                    null
                ],
                "language": "english",
                "aliases": [
                    "/glossary/open_washing"
                ]
            },
            {
                "type": "glossary",
                "title": "Optional Stopping",
                "definition": "The practice of (repeatedly) analyzing data during the data collection process and deciding to stop data collection if a statistical criterion (e.g. *p*\\-value, or bayes factor) reaches a specified threshold. If appropriate methodological precautions are taken to control the type 1 error rate, this can be an efficient analysis procedure (e.g. Lakens, 2014). However, without transparent reporting or appropriate error control the type 1 error can increase greatly and optional stopping could be considered a Questionable Research Practice (QRP) or a form of p-hacking.",
                "related_terms": [
                    "*P*\\-hacking",
                    "Questionable Research Practices or Questionable Reporting Practices (QRPs)",
                    "Sequential testing"
                ],
                "references": "Beffara Bret, B., Beffara Bret, A., & Nalborczyk, L. (2021). A fully automated, transparent, reproducible, and blind protocol for sequential analyses. Meta-Psychology, 5. https://doi.org/10.15626/MP.2018.869\n\nLakens, D. (2014). Performing high-powered studies efficiently with sequential analyses. European Journal of Social Psychology, 44(7), 701–710. https://doi.org/10.1002/ejsp.2023\n\nSagarin, B. J., Ambler, J. K., & Lee, E. M. (2014). An ethical approach to peeking at data. Perspectives on Psychological Science, 9(3), 293–304. https://doi.org/10.1177/1745691614528214\n\nSchönbrodt, F. D., Wagenmakers, E.-J., Zehetleitner, M., & Perugini, M. (2017). Sequential hypothesis testing with Bayes factors: Efficiently testing mean differences. Psychological Methods, 22(2), 322–339. https://doi.org/10.1037/met0000061",
                "drafted_by": [
                    "Brice Beffara Bret; Bettina M. J. Kern"
                ],
                "reviewed_by": [
                    "Ali H. Al-Hoorie",
                    "Helena Hartmann",
                    "Catia M. Oliveira",
                    "Sam Parsons"
                ],
                "alt_related_terms": [
                    null
                ],
                "language": "english",
                "aliases": [
                    "/glossary/optional_stopping"
                ]
            },
            {
                "type": "glossary",
                "title": "ORCID (Open Researcher and Contributor ID)",
                "definition": "A organisation that provides a registry of persistent unique identifiers (ORCID iDs) for researchers and scholars, allowing these users to link their digital research documents and other contributions to their ORCID record. This avoids the name ambiguity problem in scholarly communication. ORCID iDs provide unique, persistent identifiers connecting researchers and their scholarly work. It is free to register for an ORCID iD at https://orcid.org/register.",
                "related_terms": [
                    "Authorship",
                    "DOI (digital object identifier)",
                    "Name Ambiguity Problem"
                ],
                "references": "Haak, L. L., Fenner, M., Paglione, L., Pentz, E., & Ratner, H. (2012). ORCID: A system to uniquely identify researchers. Learned Publishing, 25(4), 259–264. https://doi.org/10.1087/20120404\n\nORCID. (n.d.). ORCID. https://orcid.org/",
                "drafted_by": [
                    "Martin Vasilev"
                ],
                "reviewed_by": [
                    "Bradley Baker",
                    "Mahmoud Elsherif",
                    "Shannon Francis",
                    "Charlotte R. Pennington",
                    "Emily A. Williams",
                    "Flávio Azevedo"
                ],
                "alt_related_terms": [
                    null
                ],
                "language": "english",
                "aliases": [
                    "/glossary/orcid"
                ]
            },
            {
                "type": "glossary",
                "title": "Overlay Journal",
                "definition": "Open access electronic journals that collect and curate articles available from other sources (typically preprint servers, such as arXiv). Article curation may include (post-publication) peer review or editorial selection. Overlay journals do not publish novel material; rather, they organize and collate articles available in existing repositories.",
                "related_terms": [
                    "Open access",
                    "Preprint"
                ],
                "references": "Ginsparg, P. (1997). Winners and losers in the global research village. The Serials Librarian, 30(3–4), 83–95. https://doi.org/10.1300/J123v30n03_13\n\nGinsparg, P. (2001). Creating a global knowledge network. In Second Joint ICSU Press-UNESCO Expert Conference on Electronic Publishing in Science (pp. 19–23).\n\nBrown, J. (2010). An Introduction to Overlay Journals [Techreport]. Repositories Support Project: UK. https://discovery.ucl.ac.uk/id/eprint/19081/",
                "drafted_by": [
                    "Bradley Baker"
                ],
                "reviewed_by": [
                    "Christopher Graham",
                    "Helena Hartmann",
                    "Sam Parsons",
                    "Charlotte R. Pennington"
                ],
                "alt_related_terms": [
                    null
                ],
                "language": "english",
                "aliases": [
                    "/glossary/overlay_journal"
                ]
            },
            {
                "type": "glossary",
                "title": "P-curve",
                "definition": "P-curve is a tool for identifying potential publication bias and makes use of the distribution of significant *p*\\-values in a series of independent findings. The deviation from the expected right-skewed distribution can be used to assess the existence and degree of publication bias: if the curve is right-skewed, there are more low, highly significant *p*\\-values, reflecting an underlying true effect. If the curve is left-skewed, there are many barely significant results just under the 0.05-threshold. This suggests that the studies lack evidential value and may be underpinned by questionable research practices (QRPs; e.g., p-hacking). In the case of no true effect present (true null hypothesis) and unbiased *p*\\-value reporting, the *p*\\-curve should be a flat, horizontal line, representing the typical distribution of *p*\\-values.",
                "related_terms": [
                    "File-drawer",
                    "Hypothesis",
                    "*P*\\-hacking",
                    "*p*\\-value",
                    "Publication bias (File Drawer Problem)",
                    "Questionable Research Practices or Questionable Reporting Practices (QRPs)",
                    "Selective reporting",
                    "Z-curve"
                ],
                "references": "Bruns, S. B., & Ioannidis, J. P. (2016). P-curve and p-hacking in observational research. PLoS ONE, 11(2), e0149144. https://doi.org/10.1371/journal.pone.0149144\n\nSimonsohn, U., Nelson, L. D., & Simmons, J. P. (2014). P-curve: a key to the file-drawer. Journal of Experimental Psychology: General, 143(2), 534. https://doi.org/10.1037/a0030850\n\nSimonsohn, U., Nelson, L. D., & Simmons, J. P. (2014). P-curve and effect size: Correcting for publication bias using only significant results. Perspectives on Psychological Science, 9(6), 666–681. https://doi.org/10.1177/1745691614553988\n\nSimonsohn, U., Nelson, L. D., & Simmons, J. P. (2019). P-curve won’t do your laundry, but it will distinguish replicable from non-replicable findings in observational research: Comment on Bruns & Ioannidis (2016). PLoS ONE, 14(3), e0213454. https://doi.org/10.1371/journal.pone.0213454",
                "drafted_by": [
                    "Bettina M. J. Kern"
                ],
                "reviewed_by": [
                    "Sam Guay",
                    "Kamil Izydorczak",
                    "Charlotte R. Pennington",
                    "Robert M. Ross",
                    "Olmo van den Akker"
                ],
                "alt_related_terms": [
                    null
                ],
                "language": "english",
                "aliases": [
                    "/glossary/p_curve"
                ]
            },
            {
                "type": "glossary",
                "title": "p*****-hacking *",
                "definition": "Exploiting techniques that may artificially increase the likelihood of obtaining a statistically significant result by meeting the standard statistical significance criterion (typically α \\= .05). For example, performing multiple analyses and reporting only those at *p* \\< .05, selectively removing data until *p* \\< .05, selecting variables for use in analyses based on whether those parameters are statistically significant.",
                "related_terms": [
                    "Analytic flexibility",
                    "Fishing",
                    "Garden of forking paths",
                    "HARKing",
                    "Questionable Research Practices or Questionable Reporting Practices (QRPs)",
                    "Selective reporting"
                ],
                "references": "Hardwicke, T. E., Jameel, L., Jones, M., Walczak, E. J., & Weinberg, L. M. (2014). Only human: Scientists, systems, and suspect statistics. Opticon1826, 16, 25. https://doi.org/10.5334/OPT.CH\n\nNeuroskeptic. (2012). The nine circles of scientific hell. Perspectives on Psychological Science, 7(6), 643–644. https://doi.org/10.1177/1745691612459519",
                "drafted_by": [
                    "Mahmoud Elsherif"
                ],
                "reviewed_by": [
                    "Tamara Kalandadze",
                    "William Ngiam",
                    "Sam Parsons",
                    "Martin Vasilev"
                ],
                "alt_related_terms": [
                    null
                ],
                "language": "english",
                "aliases": [
                    "/glossary/p___hacking_"
                ]
            },
            {
                "type": "glossary",
                "title": "p*****-value *",
                "definition": "A statistic used to evaluate the outcome of a hypothesis test in Null Hypothesis Significance Testing (NHST). It refers to the probability of observing an effect, or more extreme effect, assuming the null hypothesis is true (Lakens, 2021b). The American Statistical Association’s statement on p-values (Wasserstein & Lazar, 2016\\) notes that p-values are not an indicator of the truth of the null hypothesis and instead defines p-values in this way: “Informally, a p-value is the probability under a specified statistical model that a statistical summary of the data (e.g., the sample mean difference between two compared groups) would be equal to or more extreme than its observed value” (p. 131).",
                "related_terms": [
                    "Null Hypothesis Statistical Testing (NHST)",
                    "statistical significance"
                ],
                "references": "psyTeachR Team. (n.d.). P | Glossary. psyTeachR. https://psyteachr.github.io/glossary\n\nLakens, D. (2021). The Practical Alternative to the p Value Is the Correctly Used p Value. Perspectives on Psychological Science, 16(3), 639–648. https://doi.org/10.1177/1745691620958012\n\nWasserstein, R. L., & Lazar, N. A. (2016). The ASA Statement on p-Values: Context, Process, and Purpose. The American Statistician, 70, 129–133. https://doi.org/10.1080/00031305.2016.1154108",
                "drafted_by": [
                    "Alaa AlDoh; Flávio Azevedo"
                ],
                "reviewed_by": [
                    "Jamie P. Cockcroft",
                    "Charlotte R. Pennington",
                    "Suzanne L. K. Stewart",
                    "Robbie C.M. van Aert",
                    "Marcel A.L.M. van Assen",
                    "Martin Vasilev"
                ],
                "alt_related_terms": [
                    null
                ],
                "language": "english",
                "aliases": [
                    "/glossary/p___value_"
                ]
            },
            {
                "type": "glossary",
                "title": "Papermill",
                "definition": "An organization that is engaged in scientific misconduct wherein multiple papers are produced by falsifying or fabricating data, e.g. by editing figures or numerical data or plagiarizing written text. Papermills are “alleged to offer products ranging from research data through to ghostwritten fraudulent or fabricated manuscripts and submission services” (Byrne & Christopher, 2020, p. 583). A papermill relates to the fast production and dissemination of multiple allegedly new papers. These are often not detected in the scientific publishing process and therefore either never found or retracted if discovered (e.g. through plagiarism software).",
                "related_terms": [
                    "Data fabrication",
                    "Data falsification",
                    "Fraud",
                    "Plagiarism",
                    "Questionable Research Practices or Questionable Reporting Practices (QRPs)",
                    "Scientific misconduct",
                    "Scientific publishing"
                ],
                "references": "Byrne, J. A., & Christopher, J. (2020). Digital magic, or the dark arts of the 21st century—how can journals and peer reviewers detect manuscripts and publications from paper mills? FEBS Letters, 594(4), 583–589. https://doi.org/10.1002/1873-3468.13747\n\nHackett, R., & Kelly, S. (2020). Publishing ethics in the era of paper mills. Biology Open, 9(10), bio056556. https://doi.org/10.1242/bio.056556",
                "drafted_by": [
                    "Helena Hartmann"
                ],
                "reviewed_by": [
                    "Sarah Ashcroft-Jones",
                    "Elizabeth Collins",
                    "Mahmoud Elsherif",
                    "Charlotte R. Pennington"
                ],
                "alt_related_terms": [
                    null
                ],
                "language": "english",
                "aliases": [
                    "/glossary/papermill"
                ]
            },
            {
                "type": "glossary",
                "title": "Paradata",
                "definition": "Data that are captured about the characteristics and context of primary data collected from an individual \\- distinct from metadata. Paradata can be used to investigate a respondent’s interaction with a survey or an experiment on a micro-level. They can be most easily collected during computer mediated surveys but are not limited to them. Examples include response times to survey questions, repeated patterns of responses such as choosing the same answer for all questions, contextual characteristics of the participant such as injuries that prevent good performance on tasks, the number of premature responses to stimuli in an experiment. Paradata have been used for the investigation and adjustment of measurement and sampling errors.",
                "related_terms": [
                    "Auxiliary data",
                    "Data collection",
                    "Data quality",
                    "Metadata",
                    "Process information"
                ],
                "references": "Kreuter, F. (Ed.). (2013). Improving Surveys with Paradata. https://doi.org/10.1002/9781118596869",
                "drafted_by": [
                    "Alexander Hart; Graham Reid"
                ],
                "reviewed_by": [
                    "Helena Hartmann",
                    "Charlotte R. Pennington",
                    "Marta Topor",
                    "Flávio Azevedo"
                ],
                "alt_related_terms": [
                    null
                ],
                "language": "english",
                "aliases": [
                    "/glossary/paradata"
                ]
            },
            {
                "type": "glossary",
                "title": "PARKing",
                "definition": "PARKing (preregistering after results are known) is defined as the practice where researchers complete an experiment (possibly with infinite re-experimentation) before preregistering. This practice invalidates the purpose of preregistration, and is one of the QRPs (or, even scientific misconduct) that try to gain only \"credibility that it has been preregistered.\"",
                "related_terms": [
                    "HARKing",
                    "Preregistration",
                    "Questionable Research Practices or Questionable Reporting Practices (QRPs)"
                ],
                "references": "Ikeda, A., Xu, H., Fuji, N., Zhu, S., & Yamada, Y. (2019). Questionable research practices following pre-registration. Japanese Psychological Review, 62, 281–295.\n\nYamada, Y. (2018). How to crack pre-registration: Toward transparent and open science. Frontiers in Psychology, 9, 1831. https://doi.org/10.3389/fpsyg.2018.01831",
                "drafted_by": [
                    "Qinyu Xiao"
                ],
                "reviewed_by": [
                    "Helena Hartmann",
                    "Sam Parsons",
                    "Yuki Yamada"
                ],
                "alt_related_terms": [
                    null
                ],
                "language": "english",
                "aliases": [
                    "/glossary/parking"
                ]
            },
            {
                "type": "glossary",
                "title": "Participatory Research",
                "definition": "Participatory research refers to incorporating the views of people from relevant communities in the entire research process to achieve shared goals between researchers and the communities. This approach takes a collaborative stance that seeks to reduce the power imbalance between the researcher and those researched through a “systematic cocreation of new knowledge” (Andersson, 2018).",
                "related_terms": [
                    "Collaborative research",
                    "Inclusion",
                    "Neurodiversity",
                    "Patient and Public Involvement (PPI)",
                    "Transformative paradigm"
                ],
                "references": "Cornwall, A., & Jewkes, R. (1995). What is participatory research? Social Science & Medicine, 41(12), 1667–1676. https://doi.org/10.1016/0277-9536(95)00127-S\n\nFletcher-Watson, S., Adams, J., Brook, K., Charman, T., Crane, L., Cusack, J., Leekam, S., Milton, D., Parr, J. R., & Pellicano, E. (2019). Making the Future Together: Shaping Autism Research Through Meaningful Participation. Autism, 23(4), 943–953.\n\nKiernan, C. (1999). Participation in research by people with learning disability: Origins and issues. British Journal of Learning Disabilities, 27(2), 43–47. https://doi.org/10.1111/j.1468-3156.1999.tb00084.x\n\nLeavy, P. (2017). Research Design: Quantitative, Qualitative, Mixed Methods, Arts-Based, and Community-Based Participatory Research Approaches. The Guilford Press.\n\nOttmann, G., Laragy, C., Allen, J., & Feldman, P. (2011). Coproduction in practice: Participatory action research to develop a model of community aged care. Systemic Practice and Action Research, 24, 413–427. https://doi.org/10.1007/s11213-011-9192-x\n\nRose, D. (2018). Participatory research: Real or imagined. Social Psychiatry and Psychiatric Epidemiology, 53, 765–771. https://doi.org/10.1007/s00127-018-1549-3",
                "drafted_by": [
                    "Tamara Kalandadze"
                ],
                "reviewed_by": [
                    "Jamie P. Cockcroft",
                    "Bethan Iley",
                    "Halil E. Kocalar",
                    "Michele C. Lim"
                ],
                "alt_related_terms": [
                    null
                ],
                "language": "english",
                "aliases": [
                    "/glossary/participatory_research"
                ]
            },
            {
                "type": "glossary",
                "title": "Patient and Public Involvement (PPI)",
                "definition": "Active research collaboration with the population of interest, as opposed to conducting research “about” them. Researchers can incorporate the lived experience and expertise of patients and the public at all stages of the research process. For example, patients can help to develop a set of research questions, review the suitability of a study design, approve plain English summaries for grant/ethics applications and dissemination, collect and analyse data, and assist with writing up a project for publication. This is becoming highly recommended and even required by funders (Boivin et al., 2018).",
                "related_terms": [
                    "Co-production",
                    "Participatory research"
                ],
                "references": "Boivin, A., Richards, T., Forsythe, L., Gregoire, A., L’Esperance, A., Abelson, J., & Carman, K. L. (2018). Evaluating the patient and public involvement in research. British Medical Journal, 363, k5147. https://doi.org/10.1136/bmj.k5147\n\nINVOLVE. (n.d.). INVOLVE – Supporting public involvement in NHS, public health and social care research. https://www.invo.org.uk/",
                "drafted_by": [
                    "Jade Pickering"
                ],
                "reviewed_by": [
                    "Mahmoud Elsherif",
                    "Sam Parsons",
                    "Catia M. Oliveira"
                ],
                "alt_related_terms": [
                    null
                ],
                "language": "english",
                "aliases": [
                    "/glossary/patient_and_public_involvement"
                ]
            },
            {
                "type": "glossary",
                "title": "Paywall",
                "definition": "A technological barrier that permits access to information only to individuals who have paid \\- either personally, or via an organisation \\- a designated fee or subscription.",
                "related_terms": [
                    "Accessibility",
                    "Open Access"
                ],
                "references": "Day, S., Rennie, S., Luo, D., & Tucker, J. D. (2020). Open to the public: Paywalls and the public rationale for open access medical research publishing. Research Involvement and Engagement, 6(1), 8. https://doi.org/10.1186/s40900-020-0182-y",
                "drafted_by": [
                    "Bradley Baker"
                ],
                "reviewed_by": [
                    "Mahmoud Elsherif",
                    "Sam Parsons",
                    "Charlotte R. Pennington",
                    "Julia Wolska"
                ],
                "alt_related_terms": [
                    null
                ],
                "language": "english",
                "aliases": [
                    "/glossary/paywall"
                ]
            },
            {
                "type": "glossary",
                "title": "PCI (Peer Community In)",
                "definition": "PCI is a non-profit organisation that creates communities of researchers who review and recommend unpublished preprints based upon high-quality peer review from at least two researchers in their field. These preprints are then assigned a DOI, similarly to a journal article. PCI was developed to establish a free, transparent and public scientific publication system based on the review and recommendation of preprints.",
                "related_terms": [
                    "Open Access",
                    "Open Archives",
                    "Open Peer Review",
                    "PCI Registered Reports",
                    "Peer review",
                    "Preprints"
                ],
                "references": "",
                "drafted_by": [
                    "Emma Henderson"
                ],
                "reviewed_by": [
                    "Jamie P. Cockcroft",
                    "Christopher Graham",
                    "Bethan Iley",
                    "Aleksandra Lazić",
                    "Charlotte R. Pennington"
                ],
                "alt_related_terms": [
                    null
                ],
                "language": "english",
                "aliases": [
                    "/glossary/pci"
                ]
            },
            {
                "type": "glossary",
                "title": "PCI Registered Reports",
                "definition": "An initiative launched in 2021 dedicated to receiving, reviewing, and recommending Registered Reports (RRs) across the full spectrum of Science, technology, engineering, and mathematics (STEM), medicine, social sciences and humanities. Peer Community In (PCI) RRs are overseen by a ‘Recommender’ (equivalent to an Action Editor) and reviewed by at least two experts in the relevant field. It provides free and transparent pre- (Stage 1\\) and post-study (Stage 2\\) reviews across research fields. A network of PCI RR-friendly journals endorse the PCI RR review criteria and commit to accepting, without further peer review, RRs that receive a positive final recommendation from PCI RR.",
                "related_terms": [
                    "In Principle Acceptance (IPA)",
                    "Open Access",
                    "PCI (Peer Community In)",
                    "Publication bias (File Drawer Problem)",
                    "Registered Report",
                    "Results blind",
                    "Stage 1 study review",
                    "Stage 2 study review",
                    "Transparency"
                ],
                "references": "",
                "drafted_by": [
                    "Charlotte R. Pennington"
                ],
                "reviewed_by": [
                    "Sarah Ashcroft-Jones",
                    "Jamie P. Cockcroft",
                    "Mahmoud Elsherif",
                    "Helena Hartmann"
                ],
                "alt_related_terms": [
                    null
                ],
                "language": "english",
                "aliases": [
                    "/glossary/pci_registered_reports"
                ]
            },
            {
                "type": "glossary",
                "title": "Plan S",
                "definition": "Plan S is an initiative, launched in September 2018 by cOAlition S, a consortium of research funding organisations, which aims to accelerate the transition to full and immediate Open Access. Participating funders require recipients of research grants to publish their research in compliant Open Access journals or platforms, or make their work openly and immediately available in an Open Access repository, from 2021 onwards. cOAlition S funders have commited to not financially support ‘hybrid’ Open Access publication fees in subscription venues. However, authors can comply with plan S through publishing Open Access in a subscription journal under a “transformative arrangement” as further described in the implementation guidance. The “S” in Plan S stands for shock.",
                "related_terms": [
                    "Open Access",
                    "DORA",
                    "Repository"
                ],
                "references": "",
                "drafted_by": [
                    "Olmo van den Akker"
                ],
                "reviewed_by": [
                    "Jamie P. Cockcroft",
                    "Helena Hartmann",
                    "Halil E. Kocalar",
                    "Birgit Schmidt"
                ],
                "alt_related_terms": [
                    null
                ],
                "language": "english",
                "aliases": [
                    "/glossary/plan_s"
                ]
            },
            {
                "type": "glossary",
                "title": "Positionality",
                "definition": "The contextualization of both the research environment and the researcher, to define the boundaries within the research was produced (Jaraf, 2018). Positionality is typically centred and celebrated in qualitative research, but there have been recent calls for it to also be used in quantitative research as well. Positionality statements, whereby a researcher outlines their background and ‘position’ within and towards the research, have been suggested as one method of recognising and centring researcher bias.",
                "related_terms": [
                    "Bias",
                    "Reflexivity",
                    "Perspective"
                ],
                "references": "Jafar, A. J. N. (2018). What is positionality and should it be expressed in quantitative studies? Emergency Medicine Journal, 35(5), 323–324. https://doi.org/10.1136/emermed-2017-207158",
                "drafted_by": [
                    "Joanne McCuaig"
                ],
                "reviewed_by": [
                    "Helena Hartmann",
                    "Aoife O’Mahony",
                    "Madeleine Pownall",
                    "Graham Reid"
                ],
                "alt_related_terms": [
                    null
                ],
                "language": "english",
                "aliases": [
                    "/glossary/positionality"
                ]
            },
            {
                "type": "glossary",
                "title": "Positionality Map",
                "definition": "A reflexive tool for practicing explicit positionality in critical qualitative research. The map is to be used “as a flexible starting point to guide researchers to reflect and be reflexive about their social location. The map involves three tiers: the identification of social identities (Tier 1), how these positions impact our life (Tier 2), and details that may be tied to the particularities of our social identity (Tier 3).” (Jacobson and Mustafa 2019, p. 1). The aim of the map is “for researchers to be able to better identify and understand their social locations and how they may pose challenges and aspects of ease within the qualitative research process.”",
                "related_terms": [
                    "Positionality",
                    "Qualitative research",
                    "Social identity map",
                    "Transparency"
                ],
                "references": "Jacobson, D., & Mustafa, N. (2019). Social Identity Map: A Reflexivity Tool for Practicing Explicit Positionality in Critical Qualitative Research. International Journal of Qualitative Methods, 18, 1609406919870075. https://doi.org/10.1177/1609406919870075",
                "drafted_by": [
                    "Joanne McCuaig"
                ],
                "reviewed_by": [
                    "Helena Hartmann",
                    "Michele C. Lim",
                    "Charlotte R. Pennington",
                    "Graham Reid"
                ],
                "alt_related_terms": [
                    null
                ],
                "language": "english",
                "aliases": [
                    "/glossary/positionality_map"
                ]
            },
            {
                "type": "glossary",
                "title": "Post Hoc",
                "definition": "Post hoc is borrowed from Latin, meaning “after this”. In statistics, post hoc (or post hoc analysis) refers to the testing of hypotheses not specified prior to data analysis. In frequentist statistics, the procedure differs based on whether the analysis was planned or post-hoc, for example by applying more stringent error control. In contrast, Bayesian and likelihood approaches do not differ as a function of when the hypothesis was specified.",
                "related_terms": [
                    "A priori, Ad hoc",
                    "HARKing",
                    "P-hacking"
                ],
                "references": "Dienes, Z. (2008). Understanding psychology as a science: An introduction to scientific and statistical inference. Macmillan International Higher Education.",
                "drafted_by": [
                    "Alaa Aldoh"
                ],
                "reviewed_by": [
                    "Sam Parsons",
                    "Jamie P. Cockcroft",
                    "Bethan Iley",
                    "Halil E. Kocalar",
                    "Graham Reid",
                    "Flávio Azevedo"
                ],
                "alt_related_terms": [
                    null
                ],
                "language": "english",
                "aliases": [
                    "/glossary/post_hoc"
                ]
            },
            {
                "type": "glossary",
                "title": "Post Publication Peer Review",
                "definition": "Peer review that takes place after research has been published. It is typically posted on a dedicated platform (e.g., PubPeer). It is distinct from the traditional commentary which is published in the same journal and which is itself usually peer reviewed.",
                "related_terms": [
                    "Open Peer Review",
                    "PeerPub",
                    "Peer review"
                ],
                "references": "",
                "drafted_by": [
                    "Ali H. Al-Hoorie"
                ],
                "reviewed_by": [
                    "Mahmoud Elsherif",
                    "Sam Parsons"
                ],
                "alt_related_terms": [
                    null
                ],
                "language": "english",
                "aliases": [
                    "/glossary/post_publication_peer_review"
                ]
            },
            {
                "type": "glossary",
                "title": "Posterior distribution",
                "definition": "A way to summarize one’s updated knowledge in Bayesian inference, balancing prior knowledge with observed data. In statistical terms, posterior distributions are proportional to the product of the likelihood function and the prior. A posterior probability distribution captures (un)certainty about a given parameter value.",
                "related_terms": [
                    "Bayes Factor",
                    "Bayesian inference",
                    "Bayesian parameter estimation",
                    "Likelihood function",
                    "Prior distribution"
                ],
                "references": "Dienes, Z. (2014). Using Bayes to get the most out of non-significant results. Frontiers in Psychology, 5, 781. https://doi.org/10.3389/fpsyg.2014.00781\n\nLüdtke, O., Ulitzsch, E., & Robitzsch, A. (2020). A Comparison of Penalized Maximum Likelihood Estimation and Markov Chain Monte Carlo Techniques for Estimating Confirmatory Factor Analysis Models with Small Sample Sizes . https://doi.org/10.31234/osf.io/u3qag",
                "drafted_by": [
                    "Alaa AlDoh"
                ],
                "reviewed_by": [
                    "Adam Parker",
                    "Jamie P. Cockcroft",
                    "Julia Wolska",
                    "Yu-Fang Yang",
                    "Charlotte R. Pennington"
                ],
                "alt_related_terms": [
                    null
                ],
                "language": "english",
                "aliases": [
                    "/glossary/posterior_distribution"
                ]
            },
            {
                "type": "glossary",
                "title": "Predatory Publishing",
                "definition": "Predatory (sometimes “vanity”) publishing describes a range of business practices in which publishers seek to profit, primarily by collecting article processing charges (APCs), from publishing scientific works without necessarily providing legitimate quality checks (e.g., peer review) or editorial services. In its most extreme form, predatory publishers will publish any work, so long as charges are paid. Other less extreme strategies, such as sending out high numbers of unsolicited requests for editing or publishing in fee-driven special issues, have also been accused as predatory (Crosetto, 2021).",
                "related_terms": [
                    "Article Processing Charge (APC)",
                    "Gaming (the system)"
                ],
                "references": "Crosetto, P. (2021). Is MDPI a predatory publisher? https://paolocrosetto.wordpress.com/2021/04/12/is-mdpi-a-predatory-publisher/\n\nXia, J., Harmon, J. L., Connolly, K. G., Donnelly, R. M., Anderson, M. R., & Howard, H. A. (2015). Who publishes in “predatory” journals? Journal of the Association for Information Science and Technology, 66(7), 1406–1417. https://doi.org/10.1002/asi.23265",
                "drafted_by": [
                    "Nick Ballou"
                ],
                "reviewed_by": [
                    "Olmo van den Akker",
                    "Helena Hartmann",
                    "Aleksandra Lazić",
                    "Graham Reid",
                    "Flávio Azevedo"
                ],
                "alt_related_terms": [
                    null
                ],
                "language": "english",
                "aliases": [
                    "/glossary/predatory_publishing"
                ]
            },
            {
                "type": "glossary",
                "title": "PREPARE Guidelines",
                "definition": "The PREPARE guidelines and checklist (Planning Research and Experimental Procedures on Animals: Recommendations for Excellence) aim to help the planning of animal research, and support adherence to the 3Rs (Replacement, Reduction or Refinement) and facilitate the reproducibility of animal research.",
                "related_terms": [
                    "ARRIVE Guidelines",
                    "Reporting Guideline",
                    "STRANGE"
                ],
                "references": "Smith, A. J., Clutton, R. E., Lilley, E., Hansen, K. E. A., & Brattelid, T. (2018). PREPARE: Guidelines for planning animal research and testing. Laboratory Animals, 52(2), 135–141. https://doi.org/10.1177/0023677217724823",
                "drafted_by": [
                    "Ben Farrar"
                ],
                "reviewed_by": [
                    "Mahmoud Elsherif",
                    "Gilad Feldman",
                    "Elias Garcia-Pelegrin"
                ],
                "alt_related_terms": [
                    null
                ],
                "language": "english",
                "aliases": [
                    "/glossary/prepare_guidelines"
                ]
            },
            {
                "type": "glossary",
                "title": "Preprint",
                "definition": "A publicly available version of any type of scientific manuscript/research output preceding formal publication, considered a form of Green Open Access. Preprints are usually hosted on a repository (e.g. arXiv) that facilitates dissemination by sharing research results more quickly than through traditional publication. Preprint repositories typically provide persistent identifiers (e.g. DOIs) to preprints. Preprints can be published at any point during the research cycle, but are most commonly published upon submission (i.e., before peer-review). Accepted and peer-reviewed versions of articles are also often uploaded to preprint servers, and are called postprints.",
                "related_terms": [
                    "Open Access",
                    "DOI (digital object identifier)",
                    "Postprint",
                    "Working Paper"
                ],
                "references": "Bourne, P. E., Polka, J. K., Vale, R. D., & Kiley, R. (2017). Ten simple rules to consider regarding preprint submission. PLoS Computational Biology, 13(5), e1005473. https://doi.org/10.1371/journal.pcbi.1005473\n\nElmore, S. A. (2018). Preprints: What Role Do These Have in Communicating Scientific Results? Toxicologic Pathology, 46(4), 364–365. https://doi.org/10.1177/0192623318767322",
                "drafted_by": [
                    "Mariella Paul"
                ],
                "reviewed_by": [
                    "Gisela H. Govaart",
                    "Helena Hartmann",
                    "Sam Parsons",
                    "Tobias Wingen",
                    "Flávio Azevedo"
                ],
                "alt_related_terms": [
                    null
                ],
                "language": "english",
                "aliases": [
                    "/glossary/preprint"
                ]
            },
            {
                "type": "glossary",
                "title": "Preregistration",
                "definition": "The practice of publishing the plan for a study, including research questions/hypotheses, research design, data analysis before the data has been collected or examined. It is also possible to preregister secondary data analyses (Merten & Krypotos, 2019). A preregistration document is time-stamped and typically registered with an independent party (e.g., a repository) so that it can be publicly shared with others (possibly after an embargo period). Preregistration provides a transparent documentation of what was planned at a certain time point, and allows third parties to assess what changes may have occurred afterwards. The more detailed a preregistration is, the better third parties can assess these changes and with that the validity of the performed analyses. Preregistration aims to clearly distinguish confirmatory from exploratory research.",
                "related_terms": [
                    "Confirmation bias",
                    "Confirmatory analyses",
                    "Exploratory Data Analysis",
                    "HARKing",
                    "Pre-analysis plan",
                    "Questionable Research Practices or Questionable Reporting Practices (QRPs)",
                    "Registered Report",
                    "Research Protocol",
                    "Transparency"
                ],
                "references": "Lewandowsky, S., & Bishop, D. (2016). Research integrity: Don’t let transparency damage science. Nature News, 529(7587), 459. https://doi.org/10.1038/529459a\n\nMertens, G., & Krypotos, A. M. (2019). Preregistration of analyses of preexisting data. Psychologica Belgica, 59(1), 338.\n\nNavarro, D. (2020). Paths in strange spaces: A comment on preregistration.\n\nNosek, B. A., Ebersole, C. R., DeHaven, A. C., & Mellor, D. T. (2018). The preregistration revolution. Proceedings of the National Academy of Sciences, 115(11), 2600–2606. https://doi.org/10.1073/pnas.1708274114\n\nSimmons, J., Nelson, L., & Simonsohn, U. (2021). Pre‐registration: Why and how. Journal of Consumer Psychology, 31(1), 151–162. https://doi.org/10.1002/jcpy.1208",
                "drafted_by": [
                    "Mahmoud Elsherif"
                ],
                "reviewed_by": [
                    "Gisela H. Govaart",
                    "Helena Hartmann",
                    "Tina Lonsdorf",
                    "William Ngiam",
                    "Eike Mark Rinke",
                    "Lisa Spitzer",
                    "Olmo van den Akker",
                    "Flávio Azevedo"
                ],
                "alt_related_terms": [
                    null
                ],
                "language": "english",
                "aliases": [
                    "/glossary/preregistration"
                ]
            },
            {
                "type": "glossary",
                "title": "Preregistration Pledge",
                "definition": "In a “collective action in support of open and reproducible research practices'', the preregistration pledge is a campaign from the Project Free Our Knowledge that asks a researcher to commit to preregistering at least one study in the next two years ([https://freeourknowledge.org/2020-12-03-preregistration-pledge/](https://freeourknowledge.org/2020-12-03-preregistration-pledge/) [https://freeourknowledge.org/about/](https://freeourknowledge.org/about/)). The project is a grassroots movement initiated by early career researchers (ECRs).",
                "related_terms": [
                    "Preregistration"
                ],
                "references": "Knowledge, F. O. (2020). Preregistration Pledge. https://freeourknowledge.org/2020-12-03-preregistration-pledge/",
                "drafted_by": [
                    "Helena Hartmann"
                ],
                "reviewed_by": [
                    "Jamie P. Cockcroft",
                    "Mahmoud Elsherif",
                    "Aleksandra Lazić, Steven Verheyen"
                ],
                "alt_related_terms": [
                    null
                ],
                "language": "english",
                "aliases": [
                    "/glossary/preregistration_pledge"
                ]
            },
            {
                "type": "glossary",
                "title": "PRO (peer review openness) initiative",
                "definition": "The agreement made by several academics that they will not provide a peer review of a manuscript unless certain conditions are met. Specifically, the manuscript authors should ensure the data and materials will be made publically available (or give a justification as to why they are not freely available or shared), provide documentation detailing how to interpret and run any files or code and detail where these files can be located via the manuscript itself.",
                "related_terms": [
                    "Non-anonymised peer review",
                    "Open Science",
                    "Open Peer Review",
                    "Transparent peer review"
                ],
                "references": "Morey, R. D., Chambers, C. D., Etchells, P. J., Harris, C. R., Hoekstra, R., Lakens, D., Lewandowsky, S., Morey, C. C., Newman, D. P., Schönbrodt, F. D., Vanpaemel, W., Wagenmakers, E.-J., & Zwaan, R. A. (2016). The Peer Reviewers’ Openness Initiative: incentivizing open research practices through peer review. Royal Society Open Science, 3(1). https://doi.org/10.1098/rsos.150547",
                "drafted_by": [
                    "Jamie P. Cockcroft"
                ],
                "reviewed_by": [
                    "Sarah Ashcroft-Jones",
                    "Mahmoud Elsherif",
                    "Helena Hartmann",
                    "Steven Verheyen"
                ],
                "alt_related_terms": [
                    null
                ],
                "language": "english",
                "aliases": [
                    "/glossary/pro"
                ]
            },
            {
                "type": "glossary",
                "title": "Prior distribution",
                "definition": "Beliefs held by researchers about the parameters in a statistical model before further evidence is taken into account. A ‘prior’ is expressed as a probability distribution and can be determined in a number of ways (e.g., previous research, subjective assessment, principles such as maximising entropy given constraints), and is typically combined with the likelihood function using Bayes’ theorem to obtain a posterior distribution.",
                "related_terms": [
                    "Bayes Factor",
                    "Bayesian inference",
                    "Bayesian Parameter Estimation",
                    "Likelihood function",
                    "Posterior distribution"
                ],
                "references": "",
                "drafted_by": [
                    "Alaa AlDoh"
                ],
                "reviewed_by": [
                    "Charlotte R. Pennington",
                    "Martin Vasilev"
                ],
                "alt_related_terms": [
                    null
                ],
                "language": "english",
                "aliases": [
                    "/glossary/prior_distribution"
                ]
            },
            {
                "type": "glossary",
                "title": "Pseudonymisation",
                "definition": "Pseudonymisation refers to a technique that involves replacing or removing any information that could lead to identification of research subjects’ identity whilst still being able to make them identifiable through the use of the combination of code number and identifiers. This process comprises the following steps: removal of all identifiers from the research dataset; attribution of a specific identifier (pseudonym) for each participant and using it to label each research record; and maintenance of a cipher that links the code number to the participant in a document physically separate from the dataset. Pseudonymisation is typically a minimum requirement from ethical committees when conducting research, especially on human participants or involving confidential information, in order to ensure upholding of data privacy.",
                "related_terms": [
                    "Anonymity",
                    "Confidentiality",
                    "Data privacy",
                    "De-identification",
                    "Pseudonymisation",
                    "Research ethics"
                ],
                "references": "Mourby, M., Mackey, E., Elliot, M., Gowans, H., Wallace, S. E., Bell, J., Smith, H., Aidinlis, S., & Kaye, J. (2018). Are ‘pseudonymised’ data always personal data? Implications of the GDPR for administrative data research in the UK. Computer Law & Security Review, 34(2), 222–233. https://doi.org/10.1016/j.clsr.2018.01.002\n\nMedical Research Council. (2019). Identifiability, anonymisation and pseudonymisation. Medical Research Council. https://mrc.ukri.org/documents/pdf/gdpr-guidance-note-5-identifiability-anonymisation-and-pseudonymisation/",
                "drafted_by": [
                    "Catia M. Oliveira"
                ],
                "reviewed_by": [
                    "Helena Hartmann",
                    "Sam Parsons",
                    "Charlotte R. Pennington",
                    "Birgit Schmidt"
                ],
                "alt_related_terms": [
                    null
                ],
                "language": "english",
                "aliases": [
                    "/glossary/pseudonymisation"
                ]
            },
            {
                "type": "glossary",
                "title": "Pseudoreplication",
                "definition": "When there is a lack of statistical independence presented in the data and thus artificially inflating the number of samples (i.e. replicates). For instance, collecting more than one data point from the same experimental unit (e.g. participant or crops). Numerous methods can overcome this, such as averaging across replicates (e.g., taking the mean RT for a participant) or implementing mixed effects models with the random effects structure accounting for the pseudoreplication (e.g., specifying each individual RT as belonging to the same subject). Note, the former option would be associated with a loss of information and statistical power.",
                "related_terms": [
                    "Confounding",
                    "Generalizability",
                    "Replication",
                    "Validity"
                ],
                "references": "Davies, G. M., & Gray, A. (2015). Don’t let spurious accusations of pseudoreplication limit our ability to learn from natural experiments (and other messy kinds of ecological monitoring). Ecology and Evolution, 5(22), 5295–5304. https://doi.org/10.1002/ece3.1782\n\nHurlbert, S. H. (1984). Pseudoreplication and the Design of Ecological Field Experiments. Ecological Monographs, 54(2), 187–211. https://doi.org/10.2307/1942661\n\nLazic, S. E. (2019). Genuine replication and pseudoreplication: What’s the difference? In BMJ Open Science. https://blogs.bmj.com/openscience/2019/09/16/genuine-replication-and-pseudoreplication-whats-the-difference/",
                "drafted_by": [
                    "Ben Farrar"
                ],
                "reviewed_by": [
                    "Jamie P. Cockcroft",
                    "Mahmoud Elsherif",
                    "Elias Garcia-Pelegrin",
                    "Annalise A. LaPlume"
                ],
                "alt_related_terms": [
                    null
                ],
                "language": "english",
                "aliases": [
                    "/glossary/pseudoreplication"
                ]
            },
            {
                "type": "glossary",
                "title": "Psychometric meta-analysis",
                "definition": "Psychometric meta-analyses aim to correct for attenuation of the effect sizes of interest due to measurement error and other artifacts by using procedures based on psychometric principles, e.g. reliability of the measures. These procedures should be implemented before using the synthesised effect sizes in correlational or experimental meta-analysis, as making these corrections tends to lead to larger and less variable effect sizes.",
                "related_terms": [
                    "Correlational meta-analysis",
                    "Hunter-Schmidt meta-analysis",
                    "Meta-analysis",
                    "Non-Intervention, Reproducible, and Open Systematic Reviews (NIRO-SR)",
                    "Publication bias (File Drawer Problem)",
                    "Validity generalization"
                ],
                "references": "Borenstein, M., Hedges, L. V., Higgins, J. P., & Rothstein, H. R. (2011). Introduction to meta-analysis. John Wiley & Sons.\n\nHunter, J. E., & Schmidt, F. L. (2015). Methods of Meta-Analysis: Correcting Error and Bias in Research Findings (Third). SAGE.",
                "drafted_by": [
                    "Adrien Fillon"
                ],
                "reviewed_by": [
                    "Mahmoud Elsherif",
                    "Eduardo Garcia-Garzon",
                    "Helena Hartmann",
                    "Catia M. Oliveira",
                    "Flávio Azevedo"
                ],
                "alt_related_terms": [
                    null
                ],
                "language": "english",
                "aliases": [
                    "/glossary/psychometric_meta_analysis"
                ]
            },
            {
                "type": "glossary",
                "title": "Publication bias (File Drawer Problem)",
                "definition": "The failure to publish results based on the \"direction or strength of the study findings\" (Dickersin & Min, 1993, p. 135). The bias arises when the evaluation of a study’s publishability disproportionately hinges on the outcome of the study, often with the inclination that novel and significant results are worth publishing more than replications and null results. This bias typically materializes through a disproportionate number of significant findings and inflated effect sizes. This process leads to the published scientific literature not being representative of the full extent of all research, and specifically underrepresents null finding. Such findings, in turn, land in the so called “file drawer”, where they are never published and have no findable documentation.",
                "related_terms": [
                    "Dissemination bias",
                    "P-curve",
                    "P-hacking",
                    "Selective reporting",
                    "Statistical significance",
                    "Trim and fill **Alternative definition:** In the context of meta-analysis, publication bias “...occurs whenever the research that appears in the published literature is systematically unrepresentative of the population of completed studies. Simply put, when the research that is readily available differs in its results from the results of all the research that has been done in an area, readers and reviewers of that research are in danger of drawing the wrong conclusion about what that body of research shows.” (Rothstein et al., 2005, p. 1\\) **Related terms to alternative definition:** meta-analysis"
                ],
                "references": "Dickersin, K., & Min, Y. (1993). Publication Bias: The Problem That Won’t Go Away. Annals of the New York Academy of Sciences, 703(1), 135–148. https://doi.org/10.1111/j.1749-6632.1993.tb26343.x\n\nDevito, N., & Goldacre, B. (2019). Publication Bias. Catalogue of Bias. https://catalogofbias.org/biases/publication-bias/\n\nDuval, S., & Tweedie, R. (2000). A nonparametric “trim and fill” method of accounting for publication bias in meta-analysis. Journal of the American Statistical Association, 95, 89–98. https://doi.org/10.2307/2669529\n\nDuval, S., & Tweedie, R. (2000). Trim and fill: A simple funnel-plot–based method of testing and adjusting for publication bias in meta-analysis. Biometrics, 56, 455–463. https://doi.org/10.1111/j.0006-341x.2000.00455.x\n\nFranco, A., Malhotra, N., & Simonovits, G. (2014). Publication bias in the social sciences: Unlocking the file drawer. Science, 345(6203), 1502–1505. https://doi.org/10.1126/science.1255484\n\nLindsay, D. S. (2020). Seven steps toward transparency and replicability in psychological science. Canadian Psychology/Psychologie Canadienne, 61(4), 310–317. https://doi.org/10.1037/cap0000222\n\nRothstein, H. R., Sutton, A. J., & Borenstein, M. (2005). Publication bias in meta-analysis. In Publication bias in meta-analysis: Prevention, assessment and adjustments (pp. 1–7). John Wiley & Sons, Ltd. https://doi.org/10.1002/0470870168.ch1",
                "drafted_by": [
                    "Mahmoud Elsherif"
                ],
                "reviewed_by": [
                    "Jamie P. Cockcroft",
                    "Gilad Feldman",
                    "Adrien Fillon",
                    "Helena Hartmann",
                    "Tamara Kalandadze",
                    "William Ngiam",
                    "Martin Vasilev",
                    "Olmo van den Akker",
                    "Flávio Azevedo"
                ],
                "alt_related_terms": [
                    null
                ],
                "language": "english",
                "aliases": [
                    "/glossary/publication_bias"
                ]
            },
            {
                "type": "glossary",
                "title": "Public Trust in Science",
                "definition": "Trust in the knowledge, guidelines and recommendations that has been produced or provided by scientists to the benefit of civil society (Hendriks et al., 2016). These may also refer to trust in scientific-based recommendations on public health (e.g., universal health-care, stem cell research, federal funds for women’s reproductive rights, preventive measures of contagious diseases, and vaccination), climate change, economic policies (e.g., welfare, inequality- and poverty-control) and their intersections. The trust a member of the public has in science has been shown to be influenced by a vast number of factors such as age (Anderson et al., 2012), gender (Von Roten, 2004), rejection of scientific norms (Lewandowsky & Oberauer, 2021), political ideology (Azevedo & Jost, 2021; Brewer & Ley, 2012; Leiserowitz et al., 2010), \tright-wing authoritarianism and social dominance (Kerr & Wilson, 2021), education (Bak, 2001; Hayes & Tariq, 2000), income (Anderson et al., 2012), science knowledge (Evans & Durant, 1995; Nisbet et al., 2002), social media use (Huber et al., 2019), and religiosity (Azevedo, 2021; Brewer & Ley, 2013; Liu & Priest, 2009).",
                "related_terms": [
                    "Credibility of scientific claims",
                    "Epistemic Trust"
                ],
                "references": "Anderson, M. S., Ronning, E. A., Devries, R., & Martinson, B. C. (2010). Extending the Mertonian norms: Scientists’ subscription to norms of research. Journal of Higher Education, 81(3), 366–393. https://doi.org/10.1353/jhe.0.0095\n\nAzevedo, F., & Jost, J. T. (2021). The ideological basis of antiscientific attitudes: Effects of authoritarianism, conservatism, religiosity, social dominance, and system justification. Group Processes & Intergroup Relations, 24(4), 518–549. https://doi.org/10.1177/1368430221990104\n\nBak, H.-J. (2001). Education and Public Attitudes toward Science: Implications for the ‘Deficit Model’ of Education and Support for Science and Technology. Social Science Quarterly, 82(4), 779–795. https://www.jstor.org/stable/42955760\n\nBrewer, P. R., & Ley, B. L. (2013). Whose Science Do You Believe? Explaining Trust in Sources of Scientific Information About the Environment. Science Communication, 35(1), 115–137. https://doi.org/10.1177/1075547012441691\n\nEvans, G., & Durant, J. (1995). The relationship between knowledge and attitudes in the public understanding of science in Britain. Public Understanding of Science, 4(1), 57–74. https://doi.org/10.1088/0963-6625/4/1/004\n\nHayes, B. C., & Tariq, V. N. (2000). Gender differences in scientific knowledge and attitudes toward science: A comparative study of four Anglo-American nations. Public Understanding of Science, 9(4), 433–447. https://doi.org/10.1088/0963-6625/9/4/306\n\nHendriks, F., Kienhues, D., & Bromme, R. (2016). Trust in science and the science of trust. Trust and Communication in a Digitized World, 143–159.\n\nHuber, B., Barnidge, M., Gil de Zúñiga, H., & Liu, J. (2019). Fostering public trust in science: The role of social media. Public Understanding of Science, 28(7), 759–777. https://doi.org/10.1177/0963662519869097\n\nKerr, J. R., & Wilson, M. S. (2021). Right-wing authoritarianism and social dominance orientation predict rejection of science and scientists. Group Processes & Intergroup Relations, 24(4), 550–567. https://doi.org/10.1177/1368430221992126\n\nLewandowsky, S., & Oberauer, K. (2021). Worldview-motivated rejection of science and the norms of science. Cognition, 215, 104820. https://doi.org/10.1016/j.cognition.2021.104820\n\nLiu, H., & Priest, S. (2009). Understanding public support for stem cell research: Media communication, interpersonal communication and trust in key actors. Public Understanding of Science, 18(6), 704–718. https://doi.org/10.1177/0963662508097625\n\nNisbet, M. C., Scheufele, D. A., Shanahan, J., Moy, P., Brossard, D., & Lewenstein, B. V. (2002). Knowledge, Reservations, or Promise?: A Media Effects Model for Public Perceptions of Science and Technology. Communication Research, 29(5), 584–608. https://doi.org/10.1177/009365002236196\n\nSchneider, J., Merk, S., & Rosman, T. (2019). (Re)Building Trust? Investigating the effects of open science badges on perceived trustworthiness in journal articles. https://doi.org/10.17605/OSF.IO/VGBRS\n\nWingen, T., Berkessel, J. B., & Englich, B. (2020). No Replication, No Trust? How Low Replicability Influences Trust in Psychology. Social Psychological and Personality Science, 11(4). https://doi.org/10.1177/1948550619877412",
                "drafted_by": [
                    "Tobias Wingen; Flávio Azevedo"
                ],
                "reviewed_by": [
                    "Elias Garcia-Pelegrin",
                    "Helena Hartmann",
                    "Catia M. Oliveira",
                    "Olmo van den Akker"
                ],
                "alt_related_terms": [
                    null
                ],
                "language": "english",
                "aliases": [
                    "/glossary/public_trust_in_science"
                ]
            },
            {
                "type": "glossary",
                "title": "Publish or Perish",
                "definition": "An aphorism describing the pressure researchers feel to publish academic manuscripts, often in high prestige academic journals, in order to have a successful academic career. This pressure to publish a high quantity of manuscripts can go at the expense of the quality of the manuscripts. This institutional pressure is exacerbated by hiring procedures and funding decisions strongly focusing on the number and impact of publications.",
                "related_terms": [
                    "Incentive structure",
                    "Journal Impact Factor",
                    "Reproducibility crisis (aka Replicability or replication crisis)",
                    "Salami slicing",
                    "Slow Science"
                ],
                "references": "Case, C. M. (1928). Scholarship in Sociology. Sociology and Social Research, 12, 323–340. http://www.sudoc.fr/036493414\n\nFanelli, D. (2010). Do Pressures to Publish Increase Scientists’ Bias? An Empirical Support from US States Data. PLOS ONE, 5(4), e10271. https://doi.org/10.1371/journal.pone.0010271",
                "drafted_by": [
                    "Eliza Woodward"
                ],
                "reviewed_by": [
                    "Nick Ballou",
                    "Mahmoud Elsherif",
                    "Helena Hartmann",
                    "Annalise A. LaPlume",
                    "Sam Parsons",
                    "Timo Roettger",
                    "Olmo van den Akker"
                ],
                "alt_related_terms": [
                    null
                ],
                "language": "english",
                "aliases": [
                    "/glossary/publish_or_perish"
                ]
            },
            {
                "type": "glossary",
                "title": "PubPeer",
                "definition": "A website that allows users to post anonymous peer reviews of research that has been published (i.e. post-publication peer review).",
                "related_terms": [
                    "Open Peer Review"
                ],
                "references": "PubPeer. (n.d.). PubPeer—Search publications and join the conversation. Pubpeer. https://www.pubpeer.com/",
                "drafted_by": [
                    "Ali H. Al-Hoorie"
                ],
                "reviewed_by": [
                    "Mahmoud ELsherif"
                ],
                "alt_related_terms": [
                    null
                ],
                "language": "english",
                "aliases": [
                    "/glossary/pubpeer"
                ]
            },
            {
                "type": "glossary",
                "title": "Python",
                "definition": "An interpreted general-purpose programming language, intended to be user-friendly and easily readable, originally created by Guido van Rossum in 1991\\. Python has an extensive library of additional features with accessible documentation for tasks ranging from data analysis to experiment creation. It is a popular programming language in data science, machine learning and web development. Similar to R Markdown, Python can be presented in an interactive online format called a Jupyter notebook, combining code, data, and text.",
                "related_terms": [
                    "Jupyter",
                    "Matplotlib",
                    "NumPy",
                    "OpenSesame",
                    "PsychoPy",
                    "R"
                ],
                "references": "Lutz, M. (2001). Programming Python. O’Reilly Media, Inc.",
                "drafted_by": [
                    "Shannon Francis"
                ],
                "reviewed_by": [
                    "James E. Bartlett",
                    "Alexander Hart",
                    "Helena Hartmann",
                    "Dominik Kiersz",
                    "Graham Reid",
                    "Andrew J. Stewart"
                ],
                "alt_related_terms": [
                    null
                ],
                "language": "english",
                "aliases": [
                    "/glossary/python"
                ]
            },
            {
                "type": "glossary",
                "title": "Qualitative research",
                "definition": "Research which uses non-numerical data, such as textual responses, images, videos or other artefacts, to explore in-depth concepts, theories, or experiences. There are a wide range of qualitative approaches, from micro-detailed exploration of language or focusing on personal subjective experiences, to those which explore macro-level social experiences and opinions.",
                "related_terms": [
                    "Bracketing Interviews",
                    "Positionality",
                    "Quantitative research",
                    "Reflexivity **Alternative definition:** (if applicable) In Psychology, the **epistemology** of qualitative research is typically concerned with understanding people’s perspectives. Such epistemology proposes assuming the equity of researchers and participants as human beings, and in consequence, the need of sympathetic human understanding instead of data-driven conclusions"
                ],
                "references": "Aspers, P., & Corte, U. (2019). What is qualitative in qualitative research. Qualitative Sociology, 42(2), 139–160. https://doi.org/10.1007/s11133-019-9413-7\n\nLevitt, H. M., Motulsky, S. L., Wertz, F. J., Morrow, S. L., & Ponterotto, J. G. (2017). Recommendations for designing and reviewing qualitative research in psychology: Promoting methodological integrity. Qualitative Psychology, 4(1), 2. https://doi.org/10.1037/qup0000082",
                "drafted_by": [
                    "Madeleine Pownall"
                ],
                "reviewed_by": [
                    "Mahmoud Elsherif",
                    "Helena Hartmann",
                    "Oscar Lecuona",
                    "Claire Melia",
                    "Flávio Azevedo"
                ],
                "alt_related_terms": [
                    null
                ],
                "language": "english",
                "aliases": [
                    "/glossary/qualitative_research"
                ]
            },
            {
                "type": "glossary",
                "title": "Quantitative research",
                "definition": "Quantitative research encompasses a diverse range of methods to systematically investigate a range of phenomena via the use of numerical data which can be analysed with statistics.",
                "related_terms": [
                    "Measuring",
                    "Qualitative research",
                    "Sample size",
                    "Statistical power",
                    "Statistics"
                ],
                "references": "Goertzen, M. J. (2017). Introduction to Quantitative Research and Data. Library Technology Reports, 53(4), 12–18.",
                "drafted_by": [
                    "Aoife O’Mahony"
                ],
                "reviewed_by": [
                    "Valeria Agostini",
                    "Tamara Kalandadze",
                    "Adam Parker"
                ],
                "alt_related_terms": [
                    null
                ],
                "language": "english",
                "aliases": [
                    "/glossary/quantitative_research"
                ]
            },
            {
                "type": "glossary",
                "title": "Questionable Research Practices or Questionable Reporting Practices (QRPs)",
                "definition": "A range of activities that intentionally or unintentionally distort data in favour of a researcher’s own hypotheses \\- or omissions in reporting such practices \\- including; selective inclusion of data, hypothesising after the results are known (HARKing), and *p*\\-hacking. Popularized by John et al. (2012).",
                "related_terms": [
                    "Creative use of outliers",
                    "Fabrication",
                    "File-drawer",
                    "Garden of forking paths",
                    "HARKing",
                    "Nonpublication of data",
                    "*P*\\-hacking",
                    "*P*\\-value fishing",
                    "Partial publication of data",
                    "Post-hoc storytelling",
                    "Preregistration",
                    "Questionable Measurement Practices (QMP)",
                    "Researcher degrees of freedom",
                    "Reverse *p*\\-hacking",
                    "Salami slicing"
                ],
                "references": "Banks, G. C., Rogelberg, S. G., Woznyj, H. M., Landis, R. S., & Rupp, D. E. (2016). Editorial: Evidence on questionable research practices: The good, the bad, and the ugly. Journal of Business and Psychology, 31(3), 323–338. https://doi.org/10.1007/s10869-016-9456-7\n\nFiedler, K., & Schwarz, N. (2016). Questionable research practices revisited. Social Psychological and Personality Science, 7(1), 45–52. https://doi.org/10.1177/1948550615612150\n\nHardwicke, T. E., Jameel, L., Jones, M., Walczak, E. J., & Weinberg, L. M. (2014). Only human: Scientists, systems, and suspect statistics. Opticon1826, 16, 25. https://doi.org/10.5334/OPT.CH\n\nJohn, L. K., Loewenstein, G., & Prelec, D. (2012). Measuring the prevalence of questionable research practices with incentives for truth telling. Psychological Science, 23(5), 524–532. https://doi.org/10.1177/0956797611430953\n\nNeuroskeptic. (2012). The nine circles of scientific hell. Perspectives on Psychological Science, 7(6), 643–644. https://doi.org/10.1177/1745691612459519\n\nSijtsma, K. (2016). Playing with data—Or how to discourage questionable research practices and stimulate researchers to do things right. Psychometrika, 81(1), 1–15. https://doi.org/10.1007/s11336-015-9446-0",
                "drafted_by": [
                    "Mahmoud Elsherif"
                ],
                "reviewed_by": [
                    "Tamara Kalandadze",
                    "William Ngiam",
                    "Sam Parsons",
                    "Mariella Paul",
                    "Eike Mark Rinke",
                    "Timo Roettger",
                    "Flávio Azevedo"
                ],
                "alt_related_terms": [
                    null
                ],
                "language": "english",
                "aliases": [
                    "/glossary/questionable_research_practices_or_questionable_reporting_practices"
                ]
            },
            {
                "type": "glossary",
                "title": "Questionable Measurement Practices (QMP)",
                "definition": "Decisions researchers make that raise doubts about the validity of measures used in a study, and ultimately the study’s final conclusions (Flake & Fried, 2020). Issues arise from a lack of transparency in reporting measurement practices, a failure to address construct validity, negligence, ignorance, or deliberate misrepresentation of information.",
                "related_terms": [
                    "Construct validity",
                    "Measurement schmeasurement",
                    "*P*\\-hacking",
                    "Psychometrics",
                    "Questionable Research Practices or Questionable Reporting Practices (QRPs)",
                    "Validity"
                ],
                "references": "Flake, J. K., & Fried, E. I. (2020). Measurement schmeasurement: Questionable measurement practices and how to avoid them. Advances in Methods and Practices in Psychological Science, 3(4), 456–465. https://doi.org/10.1177/2515245920952393",
                "drafted_by": [
                    "Halil Emre Kocalar"
                ],
                "reviewed_by": [
                    "Jamie P. Cockcroft",
                    "Annalise A. LaPlume",
                    "Sam Parsons",
                    "Mirela Zaneva",
                    "Flávio Azevedo"
                ],
                "alt_related_terms": [
                    null
                ],
                "language": "english",
                "aliases": [
                    "/glossary/questionable_measurement_practices"
                ]
            },
            {
                "type": "glossary",
                "title": "R",
                "definition": "R is a free, open-source programming language and software environment that can be used to conduct statistical analyses and plot data. R was created by Ross Ihaka and Robert Gentleman at the University of Auckland. R enables authors to share reproducible analysis scripts, which increases the transparency of a study. Often, R is used in conjunction with an integrated development environment (IDE) which simplifies working with the language, for example RStudio or Visual Studio Code, or Tinn-R .",
                "related_terms": [
                    "Open-source",
                    "Statistical analysis"
                ],
                "references": "R Project for Statistical Computing. (n.d.). R: The R Project for Statistical Computing. R Project. https://www.r-project.org/",
                "drafted_by": [
                    "Lisa Spitzer"
                ],
                "reviewed_by": [
                    "Bradley Baker",
                    "Alexander Hart",
                    "Joanne McCuaig",
                    "Andrew J. Stewart"
                ],
                "alt_related_terms": [
                    null
                ],
                "language": "english",
                "aliases": [
                    "/glossary/r"
                ]
            },
            {
                "type": "glossary",
                "title": "Red Teams",
                "definition": "An approach that integrates external criticism by colleagues and peers into the research process. Red teams are based on the idea that research that is more critically and widely evaluated is more reliable. The term originates from a military practice: One group (the red team) attacks something, and another group (the blue team) defends it. The practice has been applied to open science, by giving a red team (designated critical individuals) financial incentives to find errors in or identify improvements to the materials or content of a research project (in the materials, code, writing, etc.; Coles et al., 2020).",
                "related_terms": [
                    "Adversarial collaboration"
                ],
                "references": "Coles, N. A., Tiokhin, L., Arslan, R., Forscher, P., Scheel, A., & Lakens, D. (2020). Red Team Challenge. http://daniellakens.blogspot.com/2020/05/red-team-challenge.html\n\nLakens, D. (2020). The 20% Statistician: Red Team Challenge. The 20% Statistician. http://daniellakens.blogspot.com/2020/05/red-team-challenge.html",
                "drafted_by": [
                    "Annalise A. LaPlume"
                ],
                "reviewed_by": [
                    "Nick Ballou",
                    "Mahmoud Elsherif**;** Thomas Rhys Evans",
                    "Helena Hartmann",
                    "Timo Roettger"
                ],
                "alt_related_terms": [
                    null
                ],
                "language": "english",
                "aliases": [
                    "/glossary/red_teams"
                ]
            },
            {
                "type": "glossary",
                "title": "Reflexivity",
                "definition": "The process of reflexivity refers to critically considering the knowledge that we produce through research, how it is produced, and our own role as researchers in producing this knowledge. There are different forms of reflexivity; personal reflexivity whereby researchers consider the impact of their own personal experiences, and functional whereby researchers consider the way in which our research tools and methods may have impacted knowledge production. Reflexivity aims to bring attention to underlying factors which may impact the research process, including development of research questions, data collection, and the analysis.",
                "related_terms": [
                    "Bracketing Interviews",
                    "Qualitative Research"
                ],
                "references": "Braun, V., & Clarke, V. (2013). Successful Qualitative Research. SAGE Publications.\n\nFinlay, L., & Gough, B. (2008). Reflexivity: A practical guide for researchers in health and social sciences. John Wiley & Sons.",
                "drafted_by": [
                    "Claire Melia"
                ],
                "reviewed_by": [
                    "Gilad Feldman",
                    "Annalise A. LaPlume"
                ],
                "alt_related_terms": [
                    null
                ],
                "language": "english",
                "aliases": [
                    "/glossary/reflexivity"
                ]
            },
            {
                "type": "glossary",
                "title": "Registered Report",
                "definition": "A scientific publishing format that includes an initial round of peer review of the background and methods (study design, measurement, and analysis plan); sufficiently high quality manuscripts are accepted for in-principle acceptance (IPA) at this stage. Typically, this stage 1 review occurs before data collection, however secondary data analyses are possible in this publishing format. Following data analyses and write up of results and discussion sections, the stage 2 review assesses whether authors sufficiently followed their study plan and reported deviations from it (and remains indifferent to the results). This shifts the focus of the review to the study’s proposed research question and methodology and away from the perceived interest in the study’s results.",
                "related_terms": [
                    "Preregistration",
                    "Publication bias (File Drawer Problem)",
                    "Results-free review",
                    "PCI (Peer Community In)",
                    "Research Protocol"
                ],
                "references": "Chambers, C. D. (2013). Registered reports: a new publishing initiative at Cortex. Cortex, 49(3), 609–610. https://doi.org/10.1016/j.cortex.2012.12.016\n\nChambers, C. D., Dienes, Z., McIntosh, R. D., Rotshtein, P., & Willmes, K. (2015). Registered reports: realigning incentives in scientific publishing. Cortex, 66, A1–A2. https://doi.org/10.1016/j.cortex.2015.03.022\n\nChambers, C. D., & Tzavella, L. (2020). Registered Reports: Past, Present and Future. https://doi.org/10.31222/osf.io/43298\n\nFindley, M. G., Jensen, N. M., Malesky, E. J., & Pepinsky, T. B. (2016). Can results-free review reduce publication bias? The results and implications of a pilot study. Comparative Political Studies, 49(13), 1667–1703. https://doi.org/10.1177/0010414016655539",
                "drafted_by": [
                    "Madeleine Pownall"
                ],
                "reviewed_by": [
                    "Gilad Feldman",
                    "Emma Henderson",
                    "Aoife O’Mahony",
                    "Sam Parsons",
                    "Mariella Paul",
                    "Charlotte R. Pennington",
                    "Eike Mark Rinke",
                    "Timo Roettger",
                    "Olmo van den Akker",
                    "Yuki Yamada",
                    "Flávio Azevedo"
                ],
                "alt_related_terms": [
                    null
                ],
                "language": "english",
                "aliases": [
                    "/glossary/registered_report"
                ]
            },
            {
                "type": "glossary",
                "title": "Registry of Research Data Repositories",
                "definition": "A global registry of research data repositories from different academic disciplines. It includes repositories that enable permanent storage of, description via metadata and access to, data sets by researchers, funding bodies, publishers, and scholarly institutions.",
                "related_terms": [
                    "Metadata",
                    "Open Access",
                    "Open Data",
                    "Open Material",
                    "Repository"
                ],
                "references": "Anon. (n.d.). Home | re3data.org. Retrieved from https://www.re3data.org/",
                "drafted_by": [
                    "Aleksandra Lazić"
                ],
                "reviewed_by": [
                    "Mahmoud Elsherif",
                    "Sam Parsons",
                    "Charlotte R. Pennington",
                    "Helena Hartmann"
                ],
                "alt_related_terms": [
                    null
                ],
                "language": "english",
                "aliases": [
                    "/glossary/registry_of_research_data_repositories"
                ]
            },
            {
                "type": "glossary",
                "title": "Reliability",
                "definition": "The extent to which repeated measurements lead to the same results. In psychometrics, reliability refers to the extent to which respondents have similar scores when they take a questionnaire on multiple occasions. Noteworthy, reliability does not imply validity. Furthermore, additional types of reliability besides internal consistency exist, including: test-retest reliability, parallel forms reliability and interrater reliability.",
                "related_terms": [
                    "Consistency",
                    "Internal consistency",
                    "Quality Criteria",
                    "Replicability",
                    "Reproducibility",
                    "Validity"
                ],
                "references": "Bollen, K. A. (1989). Structural Equations with Latent Variables (pp. 179–225). John Wiley & Sons.\n\nDrost, E. A. (2011). Validity and reliability in social science research. Education Research and Perspectives, 38(1), 105–123.",
                "drafted_by": [
                    "Annalise A. LaPlume"
                ],
                "reviewed_by": [
                    "Mahmoud Elsherif**;** Eduardo Garcia-Garzon",
                    "Kai Krautter",
                    "Olmo van den Akker"
                ],
                "alt_related_terms": [
                    null
                ],
                "language": "english",
                "aliases": [
                    "/glossary/reliability"
                ]
            },
            {
                "type": "glossary",
                "title": "Repeatability",
                "definition": "Synonymous with *test-retest* *reliability*. It refers to the agreement between the results of successive measurements of the same measure. Repeatability requires the same experimental tools, the same observer, the same measuring instrument administered under the same conditions, the same location, repetition over a short period of time, and the same objectives (Joint Committee for Guidelines in Metrology, 2008\\)",
                "related_terms": [
                    "Reliability"
                ],
                "references": "ISO. (1993). Guide to the Expression of Uncertainty in Measurement (1st ed.). International Organization for Standardization.\n\nStodden, V. C. (2011). Trust your science? Open your data and code.",
                "drafted_by": [
                    "Mahmoud Elsherif, Adam Parker"
                ],
                "reviewed_by": [
                    "Gilad Feldman",
                    "Helena Hartmann",
                    "Joanne McCuaig",
                    "Sam Parsons"
                ],
                "alt_related_terms": [
                    null
                ],
                "language": "english",
                "aliases": [
                    "/glossary/repeatability"
                ]
            },
            {
                "type": "glossary",
                "title": "Replicability",
                "definition": "An umbrella term, used differently across fields, covering concepts of: direct and conceptual replication, computational reproducibility/replicability, generalizability analysis and robustness analyses. Some of the definitions used previously include: a different team arriving at the same results using the original author's artifacts (Barba 2018); a study arriving at the same conclusion after collecting new data (Claerbout and Karrenbach, 1992); as well as studies for which any outcome would be considered diagnostic evidence about a claim from prior research (Nosek & Errington, 2020).",
                "related_terms": [
                    "Conceptual replication",
                    "Direct Replication",
                    "Generalizability",
                    "Reproducibility",
                    "Reliability",
                    "Robustness (analyses)"
                ],
                "references": "Barba, L. A. (2018). Terminologies for reproducible research. arXiv Preprint arXiv:1802.03311.\n\nCrüwell, S., van Doorn, J., Etz, A., Makel, M. C., Moshontz, H., Niebaum, J. C., Orben, A., Parsons, S., & Schulte-Mecklenbeck, M. (2019). Seven Easy Steps to Open Science: An Annotated Reading List. Zeitschrift Für Psychologie, 227(4), 237–248. https://doi.org/10.1027/2151-2604/a000387\n\nKing, G. (1995). Replication, replication. PS: Political Science & Politics, 28(3), 444–452. https://doi.org/10.2307/420301\n\nNosek, B. A., & Errington, T. M. (2020). What is replication? PLOS Biology, 18(3), e3000691. https://doi.org/10.1371/journal.pbio.3000691",
                "drafted_by": [
                    "Mahmoud Elsherif"
                ],
                "reviewed_by": [
                    "Jamie P. Cockcroft",
                    "Adrien Fillon",
                    "Gilad Feldman",
                    "Annalise A. LaPlume",
                    "Tina B. Lonsdorf",
                    "Sam Parsons",
                    "Eike Mark Rinke",
                    "Tobias Wingen"
                ],
                "alt_related_terms": [
                    null
                ],
                "language": "english",
                "aliases": [
                    "/glossary/replicability"
                ]
            },
            {
                "type": "glossary",
                "title": "Replication Markets",
                "definition": "A replication market is an environment where users bet on the replicability of certain effects. Forecasters are incentivized to make accurate predictions and the top successful forecasters receive monetary compensation or contributorship for their bets. The rationale behind a replication market is that it leverages the collective wisdom of the scientific community to predict which effect will most likely replicate, thus encouraging researchers to channel their limited resources to replicating these effects.",
                "related_terms": [
                    "Citizen science",
                    "Crowdsourcing",
                    "Replicability",
                    "Reproducibility"
                ],
                "references": "Liu, Y., Gordon, M., Wang, J., Bishop, M., Chen, Y., Pfeiffer, T., Twardy, C., & Viganola, D. (2020). Replication Markets: Results, Lessons, Challenges and Opportunities in AI Replication. ArXiv:2005.04543 . http://arxiv.org/abs/2005.04543\n\nTierney, W., Hardy III, J. H., Ebersole, C. R., Leavitt, K., Viganola, D., Clemente, E. G., & others. (2020). Creative destruction in science. Organizational Behavior and Human Decision Processes, 161, 291–309. https://doi.org/10.1016/j.obhdp.2020.07.002\n\nTierney, W., Hardy III, J., Ebersole, C. R., Viganola, D., Clemente, E. G., Gordon, M., & others. (2021). A creative destruction approach to replication: Implicit work and sex morality across cultures. Journal of Experimental Social Psychology, 93, 104060. https://doi.org/10.1016/j.jesp.2020.104060\n\nReplication Markets. (n.d.). Replication Markets – Reliable research replicates…you can bet on it. Replication Markets. https://www.replicationmarkets.com/",
                "drafted_by": [
                    "Ali H. Al-Hoorie"
                ],
                "reviewed_by": [
                    "Mahmoud Elsherif",
                    "Leticia Micheli",
                    "Sam Parsons"
                ],
                "alt_related_terms": [
                    null
                ],
                "language": "english",
                "aliases": [
                    "/glossary/replication_markets"
                ]
            },
            {
                "type": "glossary",
                "title": "Reporting Guideline",
                "definition": "A reporting guideline is a “checklist, flow diagram, or structured text to guide authors in reporting a specific type of research, developed using explicit methodology.” (EQUATOR Network, n.d.). Reporting guidelines provide the minimum guidance required to ensure that research findings can be appropriately interpreted, appraised, synthesized and replicated. Their use often differs per scientific journal or publisher.",
                "related_terms": [
                    "CONSORT",
                    "Non-Intervention, Reproducible, and Open Systematic Reviews (NIRO-SR)",
                    "PRISMA",
                    "STROBE"
                ],
                "references": "Moher, D., Liberati, A., Tetzlaff, J., & Altman, D. (2009). Preferred Reporting Items for Systematic Reviews and Meta-Analyses: The PRISMA Statement. PLoS Medicine, 6(7), e1000097. https://doi.org/10.1371/journal.pmed.1000097\n\nSchulz, K. F., Altman, D. G., & Moher, D. (2010). CONSORT 2010 statement: updated guidelines for reporting parallel group randomised trials. Trials, 11(1), 32. https://doi.org/10.1186/1745-6215-11-32\n\nNetwork, T. E. (n.d.). What is a reporting guideline? Retrieved 10 July 2021. https://www.equator-network.org/about-us/what-is-a-reporting-guideline/",
                "drafted_by": [
                    "Aidan Cashin"
                ],
                "reviewed_by": [
                    "Gilad Feldman",
                    "Helena Hartmann",
                    "Joanne McCuaig"
                ],
                "alt_related_terms": [
                    null
                ],
                "language": "english",
                "aliases": [
                    "/glossary/reporting_guideline"
                ]
            },
            {
                "type": "glossary",
                "title": "Repository",
                "definition": "An online archive for the storage of digital objects including research outputs, manuscripts, analysis code and/or data. Examples include preprint servers such as bioRxiv, MetaArXiv, PsyArXiv, institutional research repositories, as well as data repositories that collect and store datasets including zenodo.org, PsychData, and code repositories such as Github, or more general repositories for all kinds of research data, such as the Open Science Framework (OSF). Digital objects stored in repositories are typically described through metadata which enables discovery across different storage locations.",
                "related_terms": [
                    "Data sharing",
                    "Github",
                    "Metadata",
                    "Open Access",
                    "Open data",
                    "Open Material",
                    "Open Science Framework",
                    "Open Source",
                    "Preprint"
                ],
                "references": "",
                "drafted_by": [
                    "Tina Lonsdorf"
                ],
                "reviewed_by": [
                    "Gilad Feldman",
                    "Connor Keating",
                    "Mariella Paul",
                    "Charlotte R. Pennington",
                    "Flávio Azevedo"
                ],
                "alt_related_terms": [
                    null
                ],
                "language": "english",
                "aliases": [
                    "/glossary/repository"
                ]
            },
            {
                "type": "glossary",
                "title": "ReproducibiliTea",
                "definition": "A grassroots initiative that helps researchers create local journal clubs at their universities to discuss a range of topics relating to open research and scholarship. Each meeting usually centres around a specific paper that discusses, for example, reproducibility, research practice, research quality, social justice and inclusion, and ideas for improving science.",
                "related_terms": [
                    "Grassroots initiative",
                    "Journal club",
                    "Open science",
                    "Reproducibility"
                ],
                "references": "Orben, A. (2019). A journal club to fix science. Nature, 573(7775), 465–466. https://doi.org/10.1038/d41586-019-02842-8",
                "drafted_by": [
                    "Emma Norris"
                ],
                "reviewed_by": [
                    "Mahmoud Elsherif",
                    "Gilad Feldman",
                    "Connor Keating",
                    "Charlotte R. Pennington",
                    "Sam Parsons",
                    "Flávio Azevedo"
                ],
                "alt_related_terms": [
                    null
                ],
                "language": "english",
                "aliases": [
                    "/glossary/reproducibilitea"
                ]
            },
            {
                "type": "glossary",
                "title": "Reproducibility",
                "definition": "A minimum standard on a spectrum of activities (\"reproducibility spectrum\") for assessing the value or accuracy of scientific claims based on the original methods, data, and code. For instance, where the original researcher's data and computer codes are used to regenerate the results (Barba, 2018), often referred to as computational reproducibility. Reproducibility does not guarantee the quality, correctness, or validity of the published results (Peng, 2011). In some fields, this meaning is, instead, associated with the term “replicability” or ‘repeatability’.",
                "related_terms": [
                    "Computational reproducibility",
                    "Replicability",
                    "repeatability"
                ],
                "references": "Barba, L. A. (2018). Terminologies for reproducible research. arXiv Preprint arXiv:1802.03311.\n\nCrüwell, S., van Doorn, J., Etz, A., Makel, M. C., Moshontz, H., Niebaum, J. C., Orben, A., Parsons, S., & Schulte-Mecklenbeck, M. (2019). Seven Easy Steps to Open Science: An Annotated Reading List. Zeitschrift Für Psychologie, 227(4), 237–248. https://doi.org/10.1027/2151-2604/a000387\n\nPeng, R. D. (2011). Reproducible Research in Computational Science. Science, 334(6060), 1226–1227. https://doi.org/10.1126/science.1213847\n\nStodden, V. C. (2011). Trust your science? Open your data and code.\n\nSyed, M. (2019). The Open Science Movement is for all of us. PsyArXiv.",
                "drafted_by": [
                    "Mahmoud Elsherif"
                ],
                "reviewed_by": [
                    "Helena Hartmann",
                    "Annalise A. LaPlume",
                    "Tina B. Lonsdorf",
                    "Sam Parsons",
                    "Charlotte R. Pennington",
                    "Suzanne L. K. Stewart"
                ],
                "alt_related_terms": [
                    null
                ],
                "language": "english",
                "aliases": [
                    "/glossary/reproducibility"
                ]
            },
            {
                "type": "glossary",
                "title": "Reproducibility crisis (aka Replicability or replication crisis)",
                "definition": "The finding, and related shift in academic culture and thinking, that a large proportion of scientific studies published across disciplines do not replicate (e.g. Open Science Collaboration, 2015). This is considered to be due to a lack of quality and integrity of research and publication practices, such as publication bias, QRPs and a lack of transparency, leading to an inflated rate of false positive results. Others have described this process as a ‘Credibility revolution’ towards improving these practices.",
                "related_terms": [
                    "Credibility crisis",
                    "Publication bias (File Drawer Problem)",
                    "Questionable Research Practices or Questionable Reporting Practices (QRPs)",
                    "Replicability",
                    "Reproducibility"
                ],
                "references": "Fanelli, D. (2018). Opinion: Is science really facing a reproducibility crisis, and do we need it to? Proceedings of the National Academy of Sciences, 115(11), 2628–2631. https://doi.org/10.1073/pnas.1708272114",
                "drafted_by": [
                    "Mahmoud Elsherif"
                ],
                "reviewed_by": [
                    "Helena Hartmann",
                    "Annalise A. LaPlume",
                    "Mariella Paul",
                    "Sonia Rishi",
                    "Lisa Spitzer"
                ],
                "alt_related_terms": [
                    null
                ],
                "language": "english",
                "aliases": [
                    "/glossary/reproducibility_crisis"
                ]
            },
            {
                "type": "glossary",
                "title": "Reproducibility Network",
                "definition": "A reproducibility network is a consortium of open research working groups, often peer-led. The groups operate on a wheel-and-spoke model across a particular country, in which the network connects local cross-disciplinary researchers, groups, and institutions with a central steering group, who also connect with external stakeholders in the research ecosystem. The goals of reproducibility networks include; advocating for greater awareness, promoting training activities, and disseminating best-practices at grassroots, institutional, and research ecosystem levels. Such networks exist in the UK, Germany, Switzerland, Slovakia, and Australia (as of March 2021).",
                "related_terms": [],
                "references": "Network, U. R. (n.d.). UK Reproducibility Network. Retrieved 10 July 2021. https://www.ukrn.org/\n\nGRN · German Reproducibility Network. (n.d.). A German Reproducibility Network. Retrieved from https://reproducibilitynetwork.de/\n\nAnon. (n.d.). Domov | SKRN (Slovak Reproducibility network). Retrieved from https://slovakrn.wixsite.com/skrn\n\nAusRN. (n.d.). Australian Reproducibility Network. Retrieved from https://www.aus-rn.org/",
                "drafted_by": [
                    "Suzanne L. K. Stewart"
                ],
                "reviewed_by": [
                    "Annalise A. LaPlume",
                    "Sam Parsons",
                    "Flávio Azevedo"
                ],
                "alt_related_terms": [
                    null
                ],
                "language": "english",
                "aliases": [
                    "/glossary/reproducibility_network"
                ]
            },
            {
                "type": "glossary",
                "title": "Research Contribution Metric (*p*)",
                "definition": "Type of semantometric measure assessing similarity of publications connected in a citation network. This method uses a simple formula to assess authors’ contributions. Publication *p* can be estimated based on the semantic distance from the publications cited by *p* to publications citing *p*.",
                "related_terms": [
                    "Semantometrics"
                ],
                "references": "Knoth, P., & Herrmannova, D. (2014). Towards semantometrics: A new semantic similarity based measure for assessing a research publication’s contribution. D-Lib Magazine, 20(11), 8. https://doi.org/10.1045/november2014-knoth\n\nHolcombe, A. O. (2019). Contributorship, not authorship: Use CRediT to indicate who did what. Publications, 7(3), 48. https://doi.org/10.3390/publications7030048\n\nLarivière, V., Desrochers, N., Macaluso, B., Mongeon, P., Paul-Hus, A., & Sugimoto, C. R. (2016). Contributorship and division of labor in knowledge production. Social Studies of Science, 46(3), 417–435. https://doi.org/10.1177/0306312716650046",
                "drafted_by": [
                    "Alaa AlDoh"
                ],
                "reviewed_by": [
                    "Michele C. Lim",
                    "Jamie P. Cockcroft",
                    "Micah Vandegrift",
                    "Dominik Kiersz    ####"
                ],
                "alt_related_terms": [
                    null
                ],
                "language": "english",
                "aliases": [
                    "/glossary/research_contribution_metric"
                ]
            },
            {
                "type": "glossary",
                "title": "Research Cycle",
                "definition": "Describes the circular process of conducting scientific research, with “researchers working at various stages of inquiry, from more tentative and exploratory investigations to the testing of more definitive and well-supported claims” (Lieberman, 2020, p. 42). The cycle includes literature research and hypothesis generation, data collection and analysis, as well as dissemination of results (e.g. through publication in peer-reviewed journals), which again informs theory and new hypotheses/research.",
                "related_terms": [
                    "Research process"
                ],
                "references": "Bramoullé, Y., & Saint-Paul, G. (2010). Research cycles. In Journal of Economic Theory (Vol. 145, pp. 1890–1920). https://doi.org/10.2139/ssrn.965816\n\nLieberman, E. (2020). Research Cycles. In C. Elman, J. Gerring, & J. Mahoney (Eds.), The Production of Knowledge: Enhancing Progress in Social Science (pp. 42–70). Cambridge University Press. https://doi.org/10.1017/9781108762519.003",
                "drafted_by": [
                    "Helena Hartmann"
                ],
                "reviewed_by": [
                    "Jamie P. Cockcroft",
                    "Aleksandra Lazić",
                    "Graham Reid",
                    "Beatrice Valentini"
                ],
                "alt_related_terms": [
                    null
                ],
                "language": "english",
                "aliases": [
                    "/glossary/research_cycle"
                ]
            },
            {
                "type": "glossary",
                "title": "Research Data Management",
                "definition": "Research Data Management (RDM) is a broad concept that includes processes undertaken to create organized, documented, accessible, and reusable quality research data. Adequate research data management provides many benefits including, but not limited to, reduced likelihood of data loss, greater visibility and collaborations due to data sharing, demonstration of research integrity and accountability.",
                "related_terms": [
                    "Data curation",
                    "Data documentation",
                    "Data management plan (DMP)",
                    "Data sharing",
                    "Metadata",
                    "Research data management"
                ],
                "references": "Corti, L., Van den Eynden, V., Bishop, L., & Woollard, M. (2019). Managing and sharing research data: a guide to good practice. Sage.",
                "drafted_by": [
                    "Micah Vandegrift"
                ],
                "reviewed_by": [
                    "Helena Hartmann",
                    "Tina B. Lonsdorf",
                    "Catia M. Oliveira",
                    "Julia Wolska"
                ],
                "alt_related_terms": [
                    null
                ],
                "language": "english",
                "aliases": [
                    "/glossary/research_data_management"
                ]
            },
            {
                "type": "glossary",
                "title": "Research integrity",
                "definition": "Research integrity is defined by a set of good research practices based on fundamental principles: honesty, reliability, respect and accountability (ALLEA, 2017). Good research practices —which are based on fundamental principles of research integrity and should guide researchers in their work as well as in their engagement with the practical, ethical and intellectual challenges inherent in research— refer to areas such as: research environment (e.g., research institutions and organisations promote awareness and ensure a prevailing culture of research integrity), training, supervision and mentoring (e.g., Research institutions and organisations develop appropriate and adequate training in ethics and research integrity to ensure that all concerned are made aware of the relevant codes and regulations), research procedures (e.g., researchers report their results in a way that is compatible with the standards of the discipline and, where applicable, can be verified and reproduced), safeguards (e.g., researchers have due regard for the health, safety and welfare of the community, of collaborators and others connected with their research), data practices and management (e.g., researchers, research institutions and organisations provide transparency about how to access or make use of their data and research materials), collaborative working, publication and dissemination (e.g., authors and publishers consider negative results to be as valid as positive findings for publication and dissemination), reviewing, evaluating and editing (e.g., researchers review and evaluate submissions for publication, funding, appointment, promotion or reward in a transparent and justifiable manner).",
                "related_terms": [
                    "Credibility of scientific claims",
                    "Error detection",
                    "Ethics",
                    "Open research",
                    "Questionable Research Practices or Questionable Reporting Practices (QRPs)",
                    "Responsible Research Practices",
                    "Rigour",
                    "Transparency",
                    "Trustworthy research"
                ],
                "references": "Academies, A. A. E. (2017). The European Code of Conduct for Research Integrity. Revised Edition. Retrieved from https://allea.org/code-of-conduct/\n\nMedin, D. L. (2012). Rigor without rigor mortis: The APS Board discusses research integrity. APS Observer, 25(5–9), 27–28. https://www.psychologicalscience.org/observer/scientific-rigor\n\nMoher, D., Bouter, L., Kleinert, S., Glasziou, P., Sham, M. H., Barbour, V., & Dirnagl, U. (2020). The Hong Kong Principles for assessing researchers: Fostering research integrity. PLoS Biology, 18(7), e3000737. https://doi.org/10.1371/journal.pbio.3000737",
                "drafted_by": [
                    "Ana Barbosa Mendes; Flávio Azevedo"
                ],
                "reviewed_by": [
                    "Valeria Agostini",
                    "Bradley Baker",
                    "Gilad Feldman",
                    "Tamara Kalandadze",
                    "Charlotte R. Pennington"
                ],
                "alt_related_terms": [
                    null
                ],
                "language": "english",
                "aliases": [
                    "/glossary/research_integrity"
                ]
            },
            {
                "type": "glossary",
                "title": "Research Protocol",
                "definition": "A detailed document prepared before conducting a study, often written as part of ethics and funding applications. The protocol should include information relating to the background, rationale and aims of the study, as well as hypotheses which reflect the researchers’ expectations. The protocol should also provide a “recipe” for conducting the study, including methodological details and clear analysis plans. Best practice guidelines for creating a study protocol should be used for specific methodologies and fields. It is possible to publically share research protocols to attract new collaborators or facilitate efficient collaboration across labs (e.g. [https://www.protocols.io/](https://www.protocols.io/)). In medical and educational fields, protocols are often a separate article type suitable for publication in journals. Where protocol sharing or publication is not common practice, researchers can choose preregistration.",
                "related_terms": [
                    "Many Labs",
                    "Preregistration"
                ],
                "references": "BMJ. (2015). Introducing ‘How to write and publish a Study Protocol’ using BMJ’s new eLearning programme: Research to Publication. Retrieved from https://blogs.bmj.com/bmjopen/2015/09/22/introducing-how-to-write-and-publish-a-study-protocol-using-bmjs-new-elearning-programme-research-to-publication/\n\nNosek, B. A., Ebersole, C. R., DeHaven, A. C., & Mellor, D. T. (2018). The preregistration revolution. Proceedings of the National Academy of Sciences, 115(11), 2600–2606. https://doi.org/10.1073/pnas.1708274114",
                "drafted_by": [
                    "Marta Topor"
                ],
                "reviewed_by": [
                    "Helena Hartmann",
                    "Bethan Iley",
                    "Annalise A. LaPlume",
                    "Charlotte Pennington"
                ],
                "alt_related_terms": [
                    null
                ],
                "language": "english",
                "aliases": [
                    "/glossary/research_protocol"
                ]
            },
            {
                "type": "glossary",
                "title": "Research workflow",
                "definition": "The process of conducting research from conceptualisation to dissemination. A typical workflow may look like the following: Starting with conceptualisation to identify a research question and design a study. After study design, researchers need to gain ethical approval (if necessary) and may decide to preregister the final version. Researchers then collect and analyse their data. Finally, the process ends with dissemination; moving between pre-print and post-print stages as the manuscript is submitted to a journal.",
                "related_terms": [
                    "Open Research Workflow",
                    "Research cycle",
                    "Research pipeline"
                ],
                "references": "Kathawalla, U., Silverstein, P., & Syed, M. (2020). Easing into Open Science: A Guide for Graduate Students and Their Advisors. Collabra: Psychology. https://doi.org/10.31234/osf.io/vzjdp Retrieved from https://psyarxiv.com/vzjdp\n\nStodden, V. C. (2011). Trust your science? Open your data and code.",
                "drafted_by": [
                    "James E Bartlett"
                ],
                "reviewed_by": [
                    "Gilad Feldman",
                    "Helena Hartmann",
                    "Aleksandra Lazić",
                    "Joanne McCuaig",
                    "Timo Roettger",
                    "Sam Parsons",
                    "Steven Verheyen"
                ],
                "alt_related_terms": [
                    null
                ],
                "language": "english",
                "aliases": [
                    "/glossary/research_workflow"
                ]
            },
            {
                "type": "glossary",
                "title": "Researcher degrees of freedom",
                "definition": "refers to the flexibility often inherent in the scientific process, from hypothesis generation, designing and conducting a research study to processing the data and analyzing as well as interpreting and reporting results. Due to a lack of precisely defined theories and/or empirical evidence, multiple decisions are often equally justifiable. The term is sometimes used to refer to the opportunistic (ab-)use of this flexibility aiming to achieve desired results —e.g., when in- or excluding certain data— albeit the fact that technically the term is not inherently value-laden.",
                "related_terms": [
                    "Analytic Flexibility",
                    "Garden of forking paths",
                    "Model uncertainty",
                    "Multiverse analysis",
                    "*P*\\-hacking",
                    "Robustness (analyses)",
                    "Specification curve analysis"
                ],
                "references": "Gelman, A., & Loken, E. (n.d.). The garden of forking paths: Why multiple comparisons can be a problem, even when there is no “fishing expedition” or “p-hacking” and the research hypothesis was posited ahead of time. Retrieved from http://www.stat.columbia.edu/\n\nSimmons, J. P., Nelson, L. D., & Simonsohn, U. (2011). False-positive psychology: Undisclosed flexibility in data collection and analysis allows presenting anything as significant. Psychological Science, 22(11), 1359–1366. https://doi.org/10.1177/0956797611417632\n\nWicherts, J. M., Veldkamp, C. L., Augusteijn, H. E., Bakker, M., Van Aert, R., & Van Assen, M. A. (2016). Degrees of freedom in planning, running, analyzing, and reporting psychological studies: A checklist to avoid p-hacking. Frontiers in Psychology, 7, 1832. https://doi.org/10.3389/fpsyg.2016.01832",
                "drafted_by": [
                    "Tina Lonsdorf"
                ],
                "reviewed_by": [
                    "Gilad Feldman",
                    "Helena Hartmann",
                    "Timo Roettger",
                    "Robbie C.M. van Aert",
                    "Flávio Azevedo"
                ],
                "alt_related_terms": [
                    null
                ],
                "language": "english",
                "aliases": [
                    "/glossary/researcher_degrees_of_freedom"
                ]
            },
            {
                "type": "glossary",
                "title": "RepliCATs project",
                "definition": "Collaborative Assessment for Trustworthy Science. The repliCATS project’s aim is to crowdsource predictions about the reliability and replicability of published research in eight social science fields: business research, criminology, economics, education, political science, psychology, public administration, and sociology.",
                "related_terms": [
                    "Replicability",
                    "Trustworthiness"
                ],
                "references": "Fraser, H., Bush, M., Wintle, B., Mody, F., Smith, E., Hanea, A., & others. (2021). Predicting reliability through structured expert elicitation with repliCATS (Collaborative Assessments for Trustworthy Science).",
                "drafted_by": [
                    "Tamara Kalandadze"
                ],
                "reviewed_by": [
                    "Sarah Ashcroft-Jones",
                    "Mahmoud Elsherif",
                    "Gilad Feldman",
                    "Helena Hartmann",
                    "Charlotte R. Pennington"
                ],
                "alt_related_terms": [
                    null
                ],
                "language": "english",
                "aliases": [
                    "/glossary/replicats_project"
                ]
            },
            {
                "type": "glossary",
                "title": "Responsible Research and Innovation",
                "definition": "An approach that considers societal implications and expectations, relating to research and innovation, with the aim to foster inclusivity and sustainability. It accounts for the fact that scientific endeavours are not isolated from their wider effects and that research is motivated by factors beyond the pursuit of knowledge. As such, many parties are important in fostering responsible research, including funding bodies, research teams, stakeholders, activists, and members of the public.",
                "related_terms": [
                    "Citizen Science",
                    "Public Engagement",
                    "Transdisciplinary Research"
                ],
                "references": "",
                "drafted_by": [
                    "Ana Barbosa Mendes"
                ],
                "reviewed_by": [
                    "Helena Hartmann",
                    "Joanne McCuaig",
                    "Sam Parsons",
                    "Graham Reid"
                ],
                "alt_related_terms": [
                    null
                ],
                "language": "english",
                "aliases": [
                    "/glossary/responsible_research_and_innovation"
                ]
            },
            {
                "type": "glossary",
                "title": "Reverse p-hacking",
                "definition": "Exploiting researcher degrees of freedom during statistical analysis in order to increase the likelihood of accepting the null hypothesis (for instance, *p* \\> .05).",
                "related_terms": [
                    "Analytic flexibility",
                    "HARKing",
                    "P-hacking",
                    "Questionable Research Practices or Questionable Reporting Practices (QRPs)",
                    "Researcher degrees of freedom",
                    "Selective reporting"
                ],
                "references": "Chuard, P. J. C., Vrtilek, M., Head, M. L., & Jennions, M. D. (2019). Evidence that non-significant results are sometimes preferred: Reverse P-hacking or selective reporting? PLoS Biol, 17(1), e3000127. https://doi.org/10.1371/journal.pbio.3000127",
                "drafted_by": [
                    "Robert M. Ross"
                ],
                "reviewed_by": [
                    "Mahmoud Elsherif",
                    "Alexander Hart",
                    "Sam Parsons",
                    "Timo Roettger"
                ],
                "alt_related_terms": [
                    null
                ],
                "language": "english",
                "aliases": [
                    "/glossary/reverse_p_hacking"
                ]
            },
            {
                "type": "glossary",
                "title": "RIOT Science Club",
                "definition": "The RIOT Science Club is a multi-site seminar series that raises awareness and provides training in Reproducible, Interpretable, Open & Transparent science practices. It provides regular talks, workshops and conferences, all of which are openly available and rewatchable on the respective location’s websites and Youtube.",
                "related_terms": [
                    "Early career researchers (ECRs)",
                    "Interpretability",
                    "Openness",
                    "Reproducibility",
                    "Transparency"
                ],
                "references": "",
                "drafted_by": [
                    "Tamara Kalandadze"
                ],
                "reviewed_by": [
                    "Helena Hartmann",
                    "Emma Henderson",
                    "Joanne McCuaig",
                    "Flávio Azevedo"
                ],
                "alt_related_terms": [
                    null
                ],
                "language": "english",
                "aliases": [
                    "/glossary/riot_science_club"
                ]
            },
            {
                "type": "glossary",
                "title": "Robustness (analyses)",
                "definition": "The persistence of support for a hypothesis under perturbations of the methodological/analytical pipeline In other words, applying different methods/analysis pipelines to examine if the same conclusion is supported under analytical different conditions.",
                "related_terms": [
                    "Many Labs",
                    "Multiverse analysis",
                    "Sensitivity analyses",
                    "Specification Curve Analysis **Alternative definition:** “Robustness refers to the stability of experimental conclusions to variations in either baseline assumptions or experimental procedures. It is somewhat related to the concept of generalizability (also known as transportability), which refers to the persistence of an effect in settings different from and outside of an experimental framework \\[...\\] Whether a study design is similar enough to the original to be considered a replication, a “robustness test,” or some of many variations of pure replication that have been identified, particularly in the social sciences (for example, conceptual replication, pseudoreplication), is an unsettled question” (Goodman et al., 2016)."
                ],
                "references": "Goodman, S. N., Fanelli, D., & Ioannidis, J. P. A. (2016). What does research reproducibility mean? Science Translational Medicine, 8(341), 341ps12-341ps12. https://doi.org/10.1126/scitranslmed.aaf5027\n\nNosek, B. A., & Errington, T. M. (2020). What is replication? PLOS Biology, 18(3), e3000691. https://doi.org/10.1371/journal.pbio.3000691",
                "drafted_by": [
                    "Tina Lonsdorf; Flávio Azevedo"
                ],
                "reviewed_by": [
                    "Gilad Feldman",
                    "Adrien Fillon",
                    "Helena Hartmann",
                    "Timo Roettger"
                ],
                "alt_related_terms": [
                    null
                ],
                "language": "english",
                "aliases": [
                    "/glossary/robustness"
                ]
            },
            {
                "type": "glossary",
                "title": "Salami slicing",
                "definition": "A questionable research/reporting practice strategy, often done *post hoc*, to increase the number of publishable manuscripts by ‘slicing’ up the data from a single study \\- one example of a method of ‘gaming the system’ of academic incentives. For instance, this may involve publishing multiple studies based on a single dataset, or publishing multiple studies from different data collection sites without transparently stating where the data originally derives from. Such practices distort the literature, and particularly meta-analyses, because it is unclear that the findings were obtained from the same dataset, thereby concealing the dependencies across the separately published papers.",
                "related_terms": [
                    "Gaming (the system)",
                    "Questionable Research Practices or Questionable Reporting Practices (QRPs)",
                    "Partial publication"
                ],
                "references": "Fanelli, D. (2018). Opinion: Is science really facing a reproducibility crisis, and do we need it to? Proceedings of the National Academy of Sciences, 115(11), 2628–2631. https://doi.org/10.1073/pnas.1708272114",
                "drafted_by": [
                    "Mahmoud Elsherif"
                ],
                "reviewed_by": [
                    "Sarah Ashcroft-Jones",
                    "Adrien Fillon",
                    "Helena Hartmann",
                    "Matt Jaquiery",
                    "Tamara Kalandadze",
                    "Charlotte R. Pennington",
                    "Graham Reid",
                    "Suzanne L. K. Stewart"
                ],
                "alt_related_terms": [
                    null
                ],
                "language": "english",
                "aliases": [
                    "/glossary/salami_slicing"
                ]
            },
            {
                "type": "glossary",
                "title": "Scooping",
                "definition": "The act of reporting or publishing a novel finding prior to another researcher/team. Survey-based research indicates that fear of being scooped is an important fear-related barrier for data sharing in psychology, and agent-based models suggest that competition for priority harms scientific reliability (Tiokhin et al. 2021).",
                "related_terms": [
                    "Novelty",
                    "Open data",
                    "Preregistration"
                ],
                "references": "Houtkoop, B. L., Chambers, C., Macleod, M., Bishop, D. V. M., Nichols, T. E., & Wagenmekers, E.-J. (2018). Data sharing in psychology: A survey on barriers and preconditions. Advances in Methods and Practices in Psychological Science, 1(1), 70.85. https://doi.org/10.1177/2515245917751886\n\nLaine, H. (2017). Afraid of scooping – Case study on researcher strategies against fear of scooping in the context of open science. In Data Science Journal (Vol. 16, pp. 1–14). https://doi.org/10.5334/dsj-2017-029\n\nTiokhin, L., Yan, M., & Horgan, T. J. H. (2021). Competition for priority harms the reliability of science, but reforms can help. Nature Human Behaviour. https://doi.org/10.1038/s41562-020-01040-1",
                "drafted_by": [
                    "William Ngiam"
                ],
                "reviewed_by": [
                    "Ashley Blake",
                    "Thomas Rhys Evans",
                    "Connor Keating",
                    "Graham Reid",
                    "Timo Roettger",
                    "Robert M. Ross",
                    "Flávio Azevedo"
                ],
                "alt_related_terms": [
                    null
                ],
                "language": "english",
                "aliases": [
                    "/glossary/scooping"
                ]
            },
            {
                "type": "glossary",
                "title": "Semantometrics",
                "definition": "A class of metrics for evaluating research using full publication text to measure semantic similarity of publications and highlighting an article’s contribution to the progress of scholarly discussion. It is an extension of tools such as bibliometrics, webometrics, and altmetrics.",
                "related_terms": [
                    "Bibliometrics",
                    "Contribution(p)"
                ],
                "references": "Herrmannova, D., & Knoth, P. (n.d.). Semantometrics Towards Full text-based Research Evaluation. Retrieved from https://arxiv.org/pdf/1605.04180.pdf\n\nKnoth, P., & Herrmannova, D. (2014). Towards semantometrics: A new semantic similarity based measure for assessing a research publication’s contribution. D-Lib Magazine, 20(11), 8. https://doi.org/10.1045/november2014-knoth",
                "drafted_by": [
                    "Alaa AlDoh"
                ],
                "reviewed_by": [
                    "Jamie P. Cockcroft",
                    "Mahmoud Elsherif",
                    "Christopher Graham",
                    "Charlotte R. Pennington"
                ],
                "alt_related_terms": [
                    null
                ],
                "language": "english",
                "aliases": [
                    "/glossary/semantometrics"
                ]
            },
            {
                "type": "glossary",
                "title": "Sensitive research",
                "definition": "Research that poses a threat to those who are or have been involved in it, including the researchers, the participants, and the wider society. This threat can be physical danger (e.g. suicide) or a negative emotional response (e.g. depression) to those who are involved in the research process. For instance, research conducted on victims of suicide, the researcher might be emotionally traumatised by the descriptions of the suicidal behaviours. Indeed, the communication with the victims might also make them re-experience the traumatic memories, leading to negative psychological responses.",
                "related_terms": [
                    "Anonymity"
                ],
                "references": "Lee, R. M. (1993). Doing research on sensitive topics. Sage.\n\nAlbayrak-Aydemir, N. (2020). The hidden costs of being a scholar from the global south. Retrieved from https://blogs.lse.ac.uk/highereducation/2020/02/20/the-hidden-costs-of-being-a-scholar-from-the-global-south/",
                "drafted_by": [
                    "Nihan Albayrak-Aydemir"
                ],
                "reviewed_by": [
                    "Valeria Agostini",
                    "Mahmoud Elsherif",
                    "Helena Hartmann",
                    "Graham Reid"
                ],
                "alt_related_terms": [
                    null
                ],
                "language": "english",
                "aliases": [
                    "/glossary/sensitive_research"
                ]
            },
            {
                "type": "glossary",
                "title": "Sequence-determines-credit approach (SDC)",
                "definition": "An authorship system that assigns authorship order based on the contribution of each author. The names of the authors are listed according to their contribution in descending order with the most contributing author first and the least contributing author last.",
                "related_terms": [
                    "Authorship",
                    "First-last-author-emphasis norm (FLAE)"
                ],
                "references": "Schmidt, R. H. (1987). A worksheet for authorship of scientific articles. The Bulletin of the Ecological Society of America, 68, 8–10. http://www.jstor.org/stable/20166549\n\nTscharntke, T., Hochberg, M. E., Rand, T. A., Resh, V. H., & Krauss, J. (2007). Author sequence and credit for contributions in multiauthored publications. PLoS Biology, 5(1), e18. https://doi.org/10.1371/journal.pbio.0050018",
                "drafted_by": [
                    "Myriam A. Baum"
                ],
                "reviewed_by": [
                    "Sam Parsons",
                    "Charlotte R. Pennington"
                ],
                "alt_related_terms": [
                    null
                ],
                "language": "english",
                "aliases": [
                    "/glossary/sequence_determines_credit_approach"
                ]
            },
            {
                "type": "glossary",
                "title": "Sherpa Romeo",
                "definition": "An online resource that collects and presents open access policies from publishers, from across the world, providing summaries of individual journal's copyright and open access archiving policies.",
                "related_terms": [
                    "Embargo period",
                    "Open access",
                    "Paywall",
                    "Preprint",
                    "Repository"
                ],
                "references": "Anon. (n.d.). Welcome to Sherpa Romeo - v2.sherpa. Retrieved from https://v2.sherpa.ac.uk/romeo/",
                "drafted_by": [
                    "Aleksandra Lazić"
                ],
                "reviewed_by": [
                    "Mahmoud Elsherif",
                    "Christopher Graham",
                    "Sam Parsons",
                    "Martin Vasilev"
                ],
                "alt_related_terms": [
                    null
                ],
                "language": "english",
                "aliases": [
                    "/glossary/sherpa_romeo"
                ]
            },
            {
                "type": "glossary",
                "title": "Single-blind peer review",
                "definition": "Evaluation of research products by qualified experts where the reviewer(s) knows the identity of the author(s), but the reviewer(s) remains anonymous to the author(s).",
                "related_terms": [
                    "Anonymous review",
                    "Double-blind peer review",
                    "Masked review",
                    "Open Peer Review",
                    "Peer review",
                    "Triple-blind peer review"
                ],
                "references": "Largent, E. A., & Snodgrass, R. T. (2016). Blind peer review by academic journals. In C. T. Robertson & A. S. Kesselheim (Eds.), Blinding as a solution to bias: Strengthening biomedical science, forensic science, and law (pp. 75–95). Academic Press. https://doi.org/10.1016/B978-0-12-802460-7.00005-X",
                "drafted_by": [
                    "Bradley Baker"
                ],
                "reviewed_by": [
                    "Ashley Blake",
                    "Christopher Graham",
                    "Helena Hartmann",
                    "Graham Reid"
                ],
                "alt_related_terms": [
                    null
                ],
                "language": "english",
                "aliases": [
                    "/glossary/single_blind_peer_review"
                ]
            },
            {
                "type": "glossary",
                "title": "Slow science",
                "definition": "Adopting Open Scholarship practices leads to a longer research process overall, with more focus on transparency, reproducibility, replicability and quality, over the quantity of outputs. Slow Science opposes publish-or-perish culture and describes an academic system that allows time and resources to produce fewer higher-quality and transparent outputs, for instance prioritising researcher time towards collecting more data, more time to read the literature, think about how their findings fit the literature and documenting and sharing research materials instead of running additional studies.",
                "related_terms": [
                    "collaboration",
                    "Incentive structure",
                    "Publish or Perish",
                    "research culture",
                    "research quality"
                ],
                "references": "Academy, S. S. (2010). The Slow Science Manifesto. Slow Science. http://slow-science.org/\n\nNelson, L. D., Simmons, J. P., & Simonsohn, U. (2012). Let’s Publish Fewer Papers. Psychological Inquiry, 23(3), 291–293. https://doi.org/10.1080/1047840X.2012.705245\n\nFrith, U. (2020). Fast lane to slow science. Trends in Cognitive Sciences, 24(1), 1–2. https://doi.org/10.1016/j.tics.2019.10.007",
                "drafted_by": [
                    "Sonia Rishi"
                ],
                "reviewed_by": [
                    "Adrien Fillon",
                    "Tamara Kalandadze",
                    "Sam Parsons Charlotte R. Pennington",
                    "Robert M Ross",
                    "Timo Roettger"
                ],
                "alt_related_terms": [
                    null
                ],
                "language": "english",
                "aliases": [
                    "/glossary/slow_science"
                ]
            },
            {
                "type": "glossary",
                "title": "Society for Open, Reliable, and Transparent Ecology and Evolutionary biology (SORTEE)",
                "definition": "SORTEE ([https://www.sortee.org/](https://www.sortee.org/)) is an international society with the aim of improving the transparency and reliability of research results in the fields of ecology, evolution, and related disciplines through cultural and institutional changes. SORTEE was launched in December 2020 to anyone interested in improving research in these disciplines, regardless of experience. The society is international in scope, membership, and objectives. As of May 2021, SORTEE comprises of over 600 members.",
                "related_terms": [
                    "Society for the Improvement of Psychological Science (SIPS)"
                ],
                "references": "",
                "drafted_by": [
                    "Brice Beffara Bret; Dominique Roche"
                ],
                "reviewed_by": [
                    "Sarah Ashcroft-Jones",
                    "Mahmoud Elsherif",
                    "Charlotte R. Pennington",
                    "Graham Reid"
                ],
                "alt_related_terms": [
                    null
                ],
                "language": "english",
                "aliases": [
                    "/glossary/society_for_open_reliable_and_transparent_ecology_and_evolutionary_biology"
                ]
            },
            {
                "type": "glossary",
                "title": "Society for the Improvement of Psychological Science (SIPS)",
                "definition": "A membership society founded to further promote improved methods and practices in the psychological research field. The society aims to complete its mission statement by enhancing the training of psychological researchers; by promoting research cultures that are more conducive to better quality research; by quantifying and empirically assessing the impact of such reforms; and by leading outreach events within and outside psychology to better the current state of research norms.",
                "related_terms": [
                    "Society for Open, Reliable, and Transparent Ecology and Evolutionary biology (SORTEE)"
                ],
                "references": "Improving Psychology. (n.d.). Improving Psychology. https://improvingpsych.org/",
                "drafted_by": [
                    "Mahmoud Elsherif"
                ],
                "reviewed_by": [
                    "Ashley Blake",
                    "Jade Pickering",
                    "Graham Reid",
                    "Flávio Azevedo"
                ],
                "alt_related_terms": [
                    null
                ],
                "language": "english",
                "aliases": [
                    "/glossary/society_for_the_improvement_of_psychological_science"
                ]
            },
            {
                "type": "glossary",
                "title": "Social class",
                "definition": "Social class is usually measured using both objective and subjective measurements, as recommended by the American Psychological Association (American Psychological Association,Task Force on Socioeconomic Status, 2007). Unlike the conventional concept, which only considers one factor, either education or income (e.g., economic variables), an individual's social class is considered to be a combination of their education, income, occupational prestige, subjective social status, and self-identified social class. Social class is partly a cultural variable, as it is a stable variable and likely to change slowly over the years. Social class can have important implications to academic outcomes. An individual may have a high socio-economic status yet identify as a working class individual. Working class students tend to have different life circumstances and often more restrictive commitments than middle-class students, which make their integration with other students more difficult (Rubin, 2021). The lack of time and money is obstructive to their social experience at university. Working class students are more likely to work to support themselves, resulting in less time for academic activities and for socializing with other students as well as less money to purchase items linked to social experiences (e.g. food).",
                "related_terms": [
                    "Social integration"
                ],
                "references": "Evans, O., & Rubin, M. (2021). In a Class on Their Own: Investigating the Role of Social Integration in the Association Between Social Class and Mental Well-Being. Personality and Social Psychology Bulletin, 014616722110211. https://doi.org/10.1177/01461672211021190\n\nRubin, M., Evans, O., & McGuffog, R. (2019). Social class differences in social integration at university: Implications for academic outcomes and mental health. In J. Jetten & K. Peters (Eds.), The social psychology of inequality (pp. 87–102). Springer. https://doi.org/10.1007/978-3-030-28856-3_6\n\nRubin, M. (2021). Explaining the association between subjective social status and mental health among university students using an impact ratings approach. SN Social Sciences, 1(1), 1–21. https://doi.org/10.1007/s43545-020-00031-3",
                "drafted_by": [
                    "Mahmoud Elsherif"
                ],
                "reviewed_by": [
                    "Leticia Micheli",
                    "Eliza Woodward",
                    "Julika Wolska",
                    "Gerald Vineyard**;** Yu-Fang Yang"
                ],
                "alt_related_terms": [
                    null
                ],
                "language": "english",
                "aliases": [
                    "/glossary/social_class"
                ]
            },
            {
                "type": "glossary",
                "title": "Social integration",
                "definition": "Social integration is a multi-dimensional construct. In an academic context, social integration is related to the quantity and quality of the social interactions with staff and students, as well as the sense of connection and belonging to the university and the people within the institute. To be more specific, social support, trust, and connectedness are all variables that contribute to social integration. Social integration has important implications for academic outcomes and mental wellbeing (Evans & Rubin, 2021). Working class students are less likely to integrate with other students, since they have differing social and economic backgrounds and less disposable income. Thus they are not able to experience as many educational and fiscal opportunities than others. In turn, this can lead to poor mental health and feelings of ostracism (Rubin, 2021).",
                "related_terms": [
                    "Social class"
                ],
                "references": "Evans, O., & Rubin, M. (2021). In a Class on Their Own: Investigating the Role of Social Integration in the Association Between Social Class and Mental Well-Being. Personality and Social Psychology Bulletin, 014616722110211. https://doi.org/10.1177/01461672211021190\n\nRubin, M., Evans, O., & McGuffog, R. (2019). Social class differences in social integration at university: Implications for academic outcomes and mental health. In J. Jetten & K. Peters (Eds.), The social psychology of inequality (pp. 87–102). Springer. https://doi.org/10.1007/978-3-030-28856-3_6\n\nRubin, M. (2021). Explaining the association between subjective social status and mental health among university students using an impact ratings approach. SN Social Sciences, 1(1), 1–21. https://doi.org/10.1007/s43545-020-00031-3",
                "drafted_by": [
                    "Mahmoud Elsherif"
                ],
                "reviewed_by": [
                    "Leticia Micheli",
                    "Eliza Woodward",
                    "Julika Wolska**;** Gerald Vineyard",
                    "Yu-Fang Yang",
                    "Flávio Azevedo"
                ],
                "alt_related_terms": [
                    null
                ],
                "language": "english",
                "aliases": [
                    "/glossary/social_integration"
                ]
            },
            {
                "type": "glossary",
                "title": "Specification Curve Analysis",
                "definition": "An analytic approach that consists of identifying, calculating, visualising and interpreting results (through inferential statistics) for *all* reasonable specifications for a particular research question (see Simonsohn et al. 2015). Specification curve analysis helps make transparent the influence of presumably arbitrary decisions during the scientific progress (e.g., experimental design, construct operationalization, statistical models or several of these) made by a researcher by comprehensively reporting all non-redundant, sensible tests of the research question. Voracek et al. (2019) suggest that SCA differs from multiverse analysis with regards to the graphical displays (a specification curve plot rather than a histogram and tile plot) and the use of inferential statistics to interpret findings.",
                "related_terms": [
                    "Multiverse analysis",
                    "Research synthesis",
                    "Robustness (analyses)",
                    "Selective reporting",
                    "Vibration of effects"
                ],
                "references": "Simonsohn, U., Simmons, J. P., & Nelson, L. D. (2015). Specification curve: Descriptive and inferential statistics on all reasonable specifications. http://sticerd.lse.ac.uk/seminarpapers/psyc16022016.pdf\n\nSimonsohn, U., Simmons, J. P., & Nelson, L. D. (2020). Specification curve analysis. Nature Human Behaviour, 4(11), 1208–1214. https://doi.org/10.1038/s41562-020-0912-z\n\nVoracek, M., Kossmeier, M., & Tran, U. S. (2019). Which Data to Meta-Analyze, and How? Zeitschrift Für Psychologie. https://doi.org/10.1027/2151-2604/a000357",
                "drafted_by": [
                    "Bradley Baker"
                ],
                "reviewed_by": [
                    "Tina B. Lonsdorf",
                    "Sam Parsons",
                    "Charlotte R. Pennington"
                ],
                "alt_related_terms": [
                    null
                ],
                "language": "english",
                "aliases": [
                    "/glossary/specification_curve_analysis"
                ]
            },
            {
                "type": "glossary",
                "title": "Statistical Assumptions",
                "definition": "Analytical approaches and models assume certain characteristics of one’s data (e.g., statistical independence, random samples, normality, equal variance,...). Before running an analysis, these assumptions should be checked since their violation can change the results and conclusion of a study. Good practice in open and reproducible science is to report assumption testing in terms of the assumptions verified and the results of such checks or corrections applied.",
                "related_terms": [
                    "Null Hypothesis Significance Testing (NHST)",
                    "Statistical Significance",
                    "Statistical Validity",
                    "Transparency",
                    "Type I error",
                    "Type II error",
                    "Type M error",
                    "Type S error"
                ],
                "references": "Garson, G. D. (2012). Testing Statistical Assumptions (2012th ed.). North Carolina State University.\n\nHahn, G. J., & Meeker, W. Q. (1993). Assumptions for Statistical Inference. The American Statistician, 47(1), 1–11. https://doi.org/10.1080/00031305.1993.10475924\n\nHoekstra, R., Kiers, H., & Johnson, A. (2012). Are assumptions of well-known statistical techniques checked, and why (not)? Frontiers in Psychology, 3(137), 1–9. https://doi.org/10.3389/fpsyg.2012.00137",
                "drafted_by": [
                    "Graham Reid"
                ],
                "reviewed_by": [
                    "Jamie P. Cockcroft",
                    "Sam Parsons",
                    "Martin Vasilev",
                    "Julia Wolska"
                ],
                "alt_related_terms": [
                    null
                ],
                "language": "english",
                "aliases": [
                    "/glossary/statistical_assumptions"
                ]
            },
            {
                "type": "glossary",
                "title": "Statistical power",
                "definition": "Statistical power is the long-run probability that a statistical test correctly rejects the null hypothesis if the alternative hypothesis is true. It ranges from 0 to 1, but is often expressed as a percentage. Power can be estimated using the significance criterion (alpha), effect size, and sample size used for a specific analysis technique. There are two main applications of statistical power. A priori power where the researcher asks the question “given an effect size, how many participants would I need for X% power?”. Sensitivity power asks the question “given a known sample size, what effect size could I detect with X% power?”.",
                "related_terms": [
                    "Effect Size",
                    "Meta-analysis",
                    "Null Hypothesis Significance Testing (NHST)",
                    "Power Analysis",
                    "Positive Predictive Value",
                    "Quantitative research",
                    "Sample size",
                    "Significance criterion (alpha)",
                    "Type I error",
                    "Type II error **Related terms to alternative definition:** Type II Error"
                ],
                "references": "Carter, A., Tilling, K., & Munafo, M. R. (2021). Considerations of sample size and power calculations given a range of analytical scenarios. https://doi.org/10.31234/osf.io/tcqrn\n\nCohen, J. (1962). The statistical power of abnormal-social psychological research: A review. The Journal of Abnormal and Social Psychology, 65(3), 145–153. https://doi.org/10.1037/h0045186\n\nCohen, J. (1969). Statistical power analysis for the behavioral sciences. Academic Press.\n\nDienes, Z. (2008). Understanding psychology as a science: An introduction to scientific and statistical inference. Macmillan International Higher Education.\n\nGiner-Sorolla, R., Aberson, C. L., Bostyn, D. H., Carpenter, T., Conrique, B. G., Lewis, N. A., & Soderberg, C. (2019). Power to detect what? Considerations for planning and evaluating sample size. Retrieved from https://osf.io/jnmya/\n\nIoannidis, J. P. (2005). Why most published research findings are false. PLoS Medicine, 2(8), e124. https://doi.org/10.1371/journal.pmed.0020124\n\nLakens, D. (2021). Sample Size Justification. https://doi.org/10.31234/osf.io/9d3yf",
                "drafted_by": [
                    "Thomas Rhys Evans"
                ],
                "reviewed_by": [
                    "James E. Bartlett",
                    "Jamie P. Cockcroft",
                    "Adrien Fillon",
                    "Emma Henderson",
                    "Tamara Kalandadze",
                    "William Ngiam",
                    "Catia M. Oliveira",
                    "Charlotte R. Pennington",
                    "Graham Reid",
                    "Martin Vasilev",
                    "Qinyu Xiao",
                    "Flávio Azevedo"
                ],
                "alt_related_terms": [
                    null
                ],
                "language": "english",
                "aliases": [
                    "/glossary/statistical_power"
                ]
            },
            {
                "type": "glossary",
                "title": "Statistical significance",
                "definition": "A property of a result using Null Hypothesis Significance Testing (NHST) that, given a significance level, is deemed unlikely to have occurred given the null hypothesis. Tenny and Abdelgawad (2017) defined it as “a measure of the probability of obtaining your data or more extreme data assuming the null hypothesis is true, compared to a pre-selected acceptable level of uncertainty regarding the true answer” (p. 1). Conventions for determining the threshold vary between applications and disciplines but ultimately depend on the considerations of the researcher about an appropriate error margin. The American Statistical Association’s statement (Wasserstein & Lazar, 2016\\) notes that “Researchers often wish to turn a p-value into a statement about the truth of a null hypothesis, or about the probability that random chance produced the observed data. The p-value is neither. It is a statement about data in relation to a specified hypothetical explanation, and is not a statement about the explanation itself” (p. 131).",
                "related_terms": [
                    "Alpha error",
                    "Frequentist statistics",
                    "Null hypothesis",
                    "Null Hypothesis Significance Testing (NHST)",
                    "*P*\\-value",
                    "Type I error **Incorrect definition:** Statistical significance describes the likelihood of the observed result against chance (regardless of the null hypotheses)"
                ],
                "references": "Cassidy, S. A., Dimova, R., Giguère, B., Spence, J. R., & Stanley, D. J. (2019). Failing grade: 89% of introduction-to-psychology textbooks that define or explain statistical significance do so incorrectly. Advances in Methods and Practices in Psychological Science, 2(3), 233–239. https://doi.org/10.1177/2515245919858072\n\nWasserstein, R. L., & Lazar, N. A. (2016). The ASA Statement on p-Values: Context, Process, and Purpose. The American Statistician, 70, 129–133. https://doi.org/10.1080/00031305.2016.1154108",
                "drafted_by": [
                    "Alaa AlDoh; Flávio Azevedo"
                ],
                "reviewed_by": [
                    "James E. Bartlett",
                    "Alexander Hart**;** Annalise A. LaPlume",
                    "Charlotte R. Pennington",
                    "Graham Reid",
                    "Timo Roettger",
                    "Suzanne L. K. Stewart"
                ],
                "alt_related_terms": [
                    null
                ],
                "language": "english",
                "aliases": [
                    "/glossary/statistical_significance"
                ]
            },
            {
                "type": "glossary",
                "title": "Statistical validity",
                "definition": "The extent to which conclusions from a statistical test are accurate and reflective of the true effect found in nature. In other words, whether or not a relationship exists between two variables and can be accurately detected with the conducted analyses. Threats to statistical validity include low power, violation of assumptions, reliability of measures, etc, affecting the reliability and generality of the conclusions.",
                "related_terms": [
                    "Power",
                    "Validity",
                    "Statistical assumptions"
                ],
                "references": "Cook, T. D., & Campbell, D. T. (1979). Quasi-Experimentation. Rand McNally.\n\nDrost, E. A. (2011). Validity and reliability in social science research. Education Research and Perspectives, 38(1), 105–123.",
                "drafted_by": [
                    "Annalise A. LaPlume"
                ],
                "reviewed_by": [
                    "Jamie P. Cockcroft, Zoltan Kekecs",
                    "Graham Reid"
                ],
                "alt_related_terms": [
                    null
                ],
                "language": "english",
                "aliases": [
                    "/glossary/statistical_validity"
                ]
            },
            {
                "type": "glossary",
                "title": "STRANGE",
                "definition": "The STRANGE “framework” is a proposal and series of questions to help animal behaviour researchers consider sampling biases when planning, performing and interpreting research with animals. STRANGE is an acronym highlighting several possible sources of sampling bias in animal research, such as the animals’ Social background; Trappability and self-selection; Rearing history; Acclimation and habituation; Natural changes in responsiveness; Genetic make-up, and Experience.",
                "related_terms": [
                    "Bias",
                    "Constraints on Generality (COG)",
                    "Populations",
                    "Sampling bias",
                    "WEIRD"
                ],
                "references": "Webster, M. M., & Rutz, C. (2020). How STRANGE are your study animals? Nature, 582, 337–340. https://doi.org/10.1038/d41586-020-01751-5",
                "drafted_by": [
                    "Mahmoud Elsherif"
                ],
                "reviewed_by": [
                    "Ben Farrar",
                    "Zoe Flack",
                    "Elias Garcia-Pelegrin",
                    "Charlotte R. Pennington",
                    "Graham Reid"
                ],
                "alt_related_terms": [
                    null
                ],
                "language": "english",
                "aliases": [
                    "/glossary/strange"
                ]
            },
            {
                "type": "glossary",
                "title": "StudySwap",
                "definition": "A free online platform through which researchers post brief descriptions of research projects or resources that are available for use (“haves”) or that they require and another researcher may have (“needs”). StudySwap is a crowdsourcing approach to research which can ensure that fewer research resources go unused and more researchers have access to the resources they need.",
                "related_terms": [
                    "Collaboration",
                    "Crowdsourcing",
                    "Team science"
                ],
                "references": "Chartier, C. R., Riegelman, A., & McCarthy, R. J. (2018). StudySwap: A platform for interlab replication, collaboration, and resource exchange. Advances in Methods and Practices in Psychological Science, 1(4), 574–579. https://doi.org/10.1177/2515245918808767",
                "drafted_by": [
                    "Charlotte R. Pennington"
                ],
                "reviewed_by": [
                    "Mahmoud Elsherif",
                    "Helena Hartmann",
                    "Emma Henderson",
                    "Graham Reid"
                ],
                "alt_related_terms": [
                    null
                ],
                "language": "english",
                "aliases": [
                    "/glossary/studyswap"
                ]
            },
            {
                "type": "glossary",
                "title": "Systematic Review",
                "definition": "A form of literature review and evidence synthesis. A systematic review will usually include a thorough, repeatable (reproducible) search strategy including key terms and databases in order to find relevant literature on a given topic or research question. Systematic reviewers follow a process of screening the papers found through their search, until they have filtered down to a set of papers that fit their predefined inclusion criteria. These papers can then be synthesised in a written review which may optionally include statistical synthesis in the form of a meta-analysis as well. A systematic review should follow a standard set of guidelines to ensure that bias is kept to a minimum for example PRISMA (Moher et al., 2009; Page et al., 2021), Cochrane Systematic Reviews (Higgins et al., 2019), or NIRO-SR (Topor et al., 2021).",
                "related_terms": [
                    "Meta-analysis",
                    "CONSORT",
                    "Non-Intervention, Reproducible, and Open Systematic Reviews (NIRO-SR)",
                    "PRISMA"
                ],
                "references": "Higgins, J. P. T., Thomas, J., Chandler, J., Cumpston, M., Li, T., Page, M. J., & Welch, V. A. (Eds.). (2019). Cochrane Handbook for Systematic Reviews of Interventions. 2nd Edition. John Wiley & Sons.\n\nMoher, D., Liberati, A., Tetzlaff, J., & Altman, D. (2009). Preferred Reporting Items for Systematic Reviews and Meta-Analyses: The PRISMA Statement. PLoS Medicine, 6(7), e1000097. https://doi.org/10.1371/journal.pmed.1000097\n\nPage, M. J., Moher, D., Bossuyt, P. M., Boutron, I., Hoffmann, T. C., Mulrow, C. D., & McKenzie, J. E. (2021). PRISMA 2020 explanation and elaboration: updated guidance and exemplars for reporting systematic reviews. British Medical Journal, 372. https://doi.org/10.1136/bmj.n160\n\nTopor, M., Pickering, J. S., Barbosa Mendes, A., Bishop, D. V. M., Büttner, F. C., Elsherif, M. M., & others. (2021). An integrative framework for planning and conducting Non-Intervention, Reproducible, and Open Systematic Reviews (NIRO-SR). https://doi.org/10.31222/osf.io/8gu5z",
                "drafted_by": [
                    "Jade Pickering"
                ],
                "reviewed_by": [
                    "Mahmoud Elsherif",
                    "Adam Parker",
                    "Charlotte R. Pennington",
                    "Timo Roettger",
                    "Marta Topor",
                    "Emily A. Williams",
                    "Flávio Azevedo    ###"
                ],
                "alt_related_terms": [
                    null
                ],
                "language": "english",
                "aliases": [
                    "/glossary/systematic_review"
                ]
            },
            {
                "type": "glossary",
                "title": "Tenzing",
                "definition": "*tenzing* is an online webapp and R package that helps researchers to track and report the contributions of each team member using the CRediT taxonomy in an efficient way. Team members of a research project can indicate their contributions to each CRediT role using an online spreadsheet template, and provide any additional authors' information (e.g., name, affiliation, order in publication, email address, and ORCID iD). Upon writing the manuscript, *tenzing* can automatically create a list of contributors belonging to each CRediT role to be included in the contributions section and create the manuscript’s title page.",
                "related_terms": [
                    "Authorship",
                    "Consortium authorship",
                    "Contributions",
                    "CRediT"
                ],
                "references": "Holcombe, A. O., Kovacs, M., Aust, F., & Aczel, B. (2020). Documenting contributions to scholarly articles using CRediT and tenzing. Plos One, 15(12), e0244611.",
                "drafted_by": [
                    "Marton Kovacs"
                ],
                "reviewed_by": [
                    "Balazs Aczel",
                    "Mahmoud Elsherif",
                    "Helena Hartmann",
                    "Charlotte R. Pennington",
                    "Graham Reid",
                    "Flávio Azevedo"
                ],
                "alt_related_terms": [
                    null
                ],
                "language": "english",
                "aliases": [
                    "/glossary/tenzing"
                ]
            },
            {
                "type": "glossary",
                "title": "Theory",
                "definition": "A theory is a unifying explanation or description of a process or phenomenon, which is amenable to repeated testing and verifiable through scientific investigation, using various experiments led by several independent researchers. Theories may be rejected or deemed an unsatisfactory explanation of a phenomenon after rigorous testing of a new hypothesis that explains the phenomena better or seems to contradict them but is more generalisable to a wider array of findings.",
                "related_terms": [
                    "Hypothesis",
                    "Model (philosophy)",
                    "Theory building"
                ],
                "references": "Schafersman, S. D. (1997). An Introduction to Science. https://www.geo.sunysb.edu/esp/files/scientific-method.html\n\nWacker, J. (1998). A definition of theory: research guidelines for different theory-building research methods in operations management. Journal of Operations Management, 16(4), 361–385. https://doi.org/10.1016/s0272-6963(98)00019-9",
                "drafted_by": [
                    "Aoife O’Mahony"
                ],
                "reviewed_by": [
                    "Mahmoud Elsherif",
                    "Helena Hartmann",
                    "Charlotte R. Pennington",
                    "Graham Reid"
                ],
                "alt_related_terms": [
                    null
                ],
                "language": "english",
                "aliases": [
                    "/glossary/theory"
                ]
            },
            {
                "type": "glossary",
                "title": "Theory building",
                "definition": "The process of creating and developing a statement of concepts and their interrelationships to show how and/or why a phenomenon occurs. Theory building leads to theory testing.",
                "related_terms": [
                    "Hypothesis",
                    "Model (philosophy)",
                    "Theory",
                    "Theoretical contribution",
                    "Theoretical model"
                ],
                "references": "Borsboom, D., van der Maas, H., Dalege, J., Kievit, R., & Haig, B. (2020). Theory Construction Methodology: A practical framework for theory formation in psychology. https://doi.org/10.31234/osf.io/w5tp8\n\nCorley, K. G., & Gioia, D. A. (2011). Building theory about theory building: what constitutes a theoretical contribution? Academy of Management Review, 36(1), 12–32. https://doi.org/10.5465/amr.2009.0486\n\nGioia, D. A., & Pitre, E. (1990). Multiparadigm perspectives on theory building. Academy of Management Review, 15(4), 584–602. https://doi.org/10.5465/amr.1990.4310758\n\nWacker, J. (1998). A definition of theory: research guidelines for different theory-building research methods in operations management. Journal of Operations Management, 16(4), 361–385. https://doi.org/10.1016/s0272-6963(98)00019-9",
                "drafted_by": [
                    "Filip Dechterenko"
                ],
                "reviewed_by": [
                    "Mahmoud Elsherif",
                    "Helena Hartmann",
                    "Charlotte R. Pennington"
                ],
                "alt_related_terms": [
                    null
                ],
                "language": "english",
                "aliases": [
                    "/glossary/theory_building"
                ]
            },
            {
                "type": "glossary",
                "title": "The Troubling Trio",
                "definition": "Described as a combination of low statistical power, a surprising result, and a *p*\\-value only slightly lower than .05.",
                "related_terms": [
                    "Replication",
                    "Reproducibility",
                    "Null Hypothesis Significance Testing (NHST)",
                    "*P*\\-hacking",
                    "Questionable Research Practices or Questionable Reporting Practices (QRPs)"
                ],
                "references": "Lindsay, D. S. (2015). Replication in Psychological Science . Psychological Science, 26(12), 1827–1832. https://doi.org/10.1177/0956797615616374",
                "drafted_by": [
                    "Halil Emre Kocalar"
                ],
                "reviewed_by": [
                    "",
                    "Catia M. Oliveira",
                    "Adam Parker",
                    "Sam Parsons;Charlotte R. Pennington"
                ],
                "alt_related_terms": [
                    null
                ],
                "language": "english",
                "aliases": [
                    "/glossary/the_troubling_trio"
                ]
            },
            {
                "type": "glossary",
                "title": "Transparency",
                "definition": "Having one’s actions open and accessible for external evaluation. Transparency pertains to researchers being honest about theoretical, methodological, and analytical decisions made throughout the research cycle. Transparency can be usefully differentiated into “scientifically relevant transparency” and “socially relevant transparency”. While the former has been the focus of early Open Science discourses, the latter is needed to provide scientific information in ways that are relevant to decision makers and members of the public (Elliott & Resnik, 2019).",
                "related_terms": [
                    "Credibility of scientific claims",
                    "Open science",
                    "Preregistration",
                    "Reproducibility",
                    "Trustworthiness"
                ],
                "references": "Elliott, K. C., & Resnik, D. B. (2019). Making open science work for science and society. Environmental Health Perspectives, 127(7). https://doi.org/10.1289/EHP4808\n\nLyon, L. (2016). Transparency: The Emerging Third Dimension of Open Science and Open Data. LIBER Quarterly, 25(4), 153–171. http://doi.org/10.18352/lq.10113\n\nSyed, M. (2019). The Open Science Movement is for all of us. PsyArXiv.",
                "drafted_by": [
                    "William Ngiam"
                ],
                "reviewed_by": [
                    "Tamara Kalandadze",
                    "Aoife O’Mahony",
                    "Eike Mark Rinke",
                    "Flávio Azevedo"
                ],
                "alt_related_terms": [
                    null
                ],
                "language": "english",
                "aliases": [
                    "/glossary/transparency"
                ]
            },
            {
                "type": "glossary",
                "title": "Transparency Checklist",
                "definition": "The transparency checklist is a consensus-based, comprehensive checklist that contains 36 items that cover the prepregistration, methods, results and discussion and data, code and materials availability. A shortened 12-item version of the checklist is also available. Checklist responses can be submitted alongside a manuscript for review. While the checklist can also work for educational purposes, it mainly aims to support researchers to identify concrete actions that can increase the transparency of their research while a disclosed checklist can help the readers and reviewers gain critical information about different aspects of transparency of the submitted research.",
                "related_terms": [
                    "Credibility of scientific claims",
                    "Open science",
                    "Preregistration",
                    "Reproducibility",
                    "Trustworthiness"
                ],
                "references": "",
                "drafted_by": [
                    "Barnabas Szaszi"
                ],
                "reviewed_by": [
                    "Sarah Ashcroft-Jones",
                    "Mahmoud Elsherif",
                    "Helena Hartmann",
                    "Graham Reid",
                    "Flávio Azevedo"
                ],
                "alt_related_terms": [
                    null
                ],
                "language": "english",
                "aliases": [
                    "/glossary/transparency_checklist"
                ]
            },
            {
                "type": "glossary",
                "title": "Triple-blind peer review",
                "definition": "Evaluation of research products by qualified experts where the author(s) are anonymous to both the reviewer(s) and editor(s). **“**Blinding of the authors and their affiliations to both editors and reviewers. This approach aims to eliminate institutional, personal, and gender biases” (Tvina et al., 2019, p. 1082).",
                "related_terms": [
                    "Double-blind peer review",
                    "Open Peer Review",
                    "Single-blind peer review"
                ],
                "references": "Largent, E. A., & Snodgrass, R. T. (2016). Blind peer review by academic journals. In C. T. Robertson & A. S. Kesselheim (Eds.), Blinding as a solution to bias: Strengthening biomedical science, forensic science, and law (pp. 75–95). Academic Press. https://doi.org/10.1016/B978-0-12-802460-7.00005-X\n\nTvina, A., Spellecy, R., & Palatnik, A. (2019). Bias in the peer review process: can we do better? Obstetrics & Gynecology, 133(6), 1081–1083. https://doi.org/10.1097/AOG.0000000000003260",
                "drafted_by": [
                    "Mahmoud Elsherif"
                ],
                "reviewed_by": [
                    "Bradley Baker",
                    "Helena Hartmann",
                    "Charlotte R. Pennington",
                    "Christopher Graham"
                ],
                "alt_related_terms": [
                    null
                ],
                "language": "english",
                "aliases": [
                    "/glossary/triple_blind_peer_review"
                ]
            },
            {
                "type": "glossary",
                "title": "TRUST Principles",
                "definition": "A set of guiding principles that consider Transparency, Responsibility, User focus, Sustainability, and Technology (TRUST) as the essential components for assessing, developing, and sustaining the trustworthiness of digital data repositories (especially those that store research data). They are complementary to the FAIR Data Principles.",
                "related_terms": [
                    "FAIR principles",
                    "Metadata",
                    "Open Access",
                    "Open Data",
                    "Open Material",
                    "Repository"
                ],
                "references": "Lin, D., Crabtree, J., Dillo, I., Downs, R. R., Edmunds, R., Giaretta, D., De Giusti, M., L’Hours, H., Hugo, W., Jenkyns, R., Khodiyar, V., Martone, M. E., Mokrane, M., Navale, V., Petters, J., Sierman, B., Sokolova, D. V., Stockhause, M., & Westbrook, J. (2020). The TRUST Principles for digital repositories. Scientific Data, 7(1), 144. https://doi.org/10.1038/s41597-020-0486-7",
                "drafted_by": [
                    "Aleksandra Lazić"
                ],
                "reviewed_by": [
                    "Jamie P. Cockcroft",
                    "Mahmoud Elsherif",
                    "Helena Hartmann",
                    "Sam Parsons"
                ],
                "alt_related_terms": [
                    null
                ],
                "language": "english",
                "aliases": [
                    "/glossary/trust_principles"
                ]
            },
            {
                "type": "glossary",
                "title": "Type I error",
                "definition": "“Incorrect rejection of a null hypothesis” (Simmons et al., 2011, p. 1359), i.e. finding evidence to reject the null hypothesis that there is no effect when the evidence is actually in favouring of retaining the null that there is no effect (For example, a judge imprisoning an innocent person). Concluding that there is a significant effect and rejecting the null hypothesis when your findings actually occured by chance.",
                "related_terms": [
                    "Frequentist statistics",
                    "Null Hypothesis Significance Testing (NHST)",
                    "Null Result",
                    "*P* value",
                    "Questionable Research Practices or Questionable Reporting Practices (QRPs)",
                    "Reproducibility crisis (aka Replicability or replication crisis)",
                    "Scientific integrity",
                    "Statistical power",
                    "True positive result",
                    "Type II error"
                ],
                "references": "Simmons, J. P., Nelson, L. D., & Simonsohn, U. (2011). False-positive psychology: Undisclosed flexibility in data collection and analysis allows presenting anything as significant. Psychological Science, 22(11), 1359–1366. https://doi.org/10.1177/0956797611417632",
                "drafted_by": [
                    "Lisa Spitzer"
                ],
                "reviewed_by": [
                    "Mahmoud Elsherif",
                    "Adrien Fillon",
                    "Helena Hartmann",
                    "Matt Jaquiery",
                    "Mariella Paul",
                    "Charlotte R. Pennington",
                    "Graham Reid",
                    "Olly Robertson",
                    "Mirela Zaneva"
                ],
                "alt_related_terms": [
                    null
                ],
                "language": "english",
                "aliases": [
                    "/glossary/type_i_error"
                ]
            },
            {
                "type": "glossary",
                "title": "Type II error",
                "definition": "A false negative result occurs when the alternative hypothesis is true in the population but the null hypothesis is accepted as part of the analysis (Hartgerink et al., 2017). That is, finding a non-significant statistical result when the effect is true (For example, a judge passing an innocent verdict on a guilty person). False negatives are less likely to be the subject of replications than positive results (Fiedler et al., 2012), and remain an unresolved issue in scientific research (Hartgerink et al., 2017).",
                "related_terms": [
                    "Effect size",
                    "Null Hypothesis Significance Testing (NHST)",
                    "Questionable Research Practices or Questionable Reporting Practices (QRPs)",
                    "Reproducibility crisis (aka Replicability or replication crisis)",
                    "Scientific integrity",
                    "Statistical power",
                    "True positive result",
                    "Type I error"
                ],
                "references": "Fiedler, K., Kutzner, F., & Krueger, J. I. (2012). The long way from α-error control to validity proper: Problems with a short-sighted false-positive debate. Perspectives on Psychological Science, 7(6), 661–669. https://doi.org/10.1177/1745691612462587\n\nHartgerink, C. H., Wicherts, J. M., & Van Assen, M. A. L. M. (2017). Too good to be false: Nonsignificant results revisited. Collabra: Psychology, 3(1). https://doi.org/10.1525/collabra.71",
                "drafted_by": [
                    "Olly Robertson"
                ],
                "reviewed_by": [
                    "Mahmoud Elsherif",
                    "Charlotte R. Pennington"
                ],
                "alt_related_terms": [
                    null
                ],
                "language": "english",
                "aliases": [
                    "/glossary/type_ii_error"
                ]
            },
            {
                "type": "glossary",
                "title": "Type M error",
                "definition": "A Type M error occurs when a researcher concludes that an effect was observed with magnitude lower or higher than the real one. For example, a type M error occurs when a researcher claims that an effect of small magnitude was observed when it is large in truth or vice versa.",
                "related_terms": [
                    "Statistical power",
                    "Type S error",
                    "Type I error",
                    "Type II error"
                ],
                "references": "Gelman, A., & Carlin, J. (2014). Beyond Power Calculations: Assessing Type S (Sign) and Type M (Magnitude) Errors. Perspectives on Psychological Science, 9(6), 641–651. https://doi.org/10.1177/1745691614551642\n\nLu, J., Qiu, Y., & Deng, A. (2018). A note on Type S/M errors in hypothesis testing. British Journal of Mathematical and Statistical Psychology, 72(1), 1–17. https://doi.org/10.1111/bmsp.12132",
                "drafted_by": [
                    "Eduardo Garcia-Garzon"
                ],
                "reviewed_by": [
                    "Mahmoud Elsherif",
                    "Helena Hartmann",
                    "Graham Reid",
                    "Mirela Zaneva"
                ],
                "alt_related_terms": [
                    null
                ],
                "language": "english",
                "aliases": [
                    "/glossary/type_m_error"
                ]
            },
            {
                "type": "glossary",
                "title": "Type S error",
                "definition": "A Type S error occurs when a researcher concludes that an effect was observed with an opposite sign than real one. For example, a type S error occurs when a researcher claims that a positive effect was observed when it is negative in reality or vice versa.",
                "related_terms": [
                    "Statistical power",
                    "Type M error",
                    "Type I error",
                    "Type II error"
                ],
                "references": "Gelman, A., & Carlin, J. (2014). Beyond Power Calculations: Assessing Type S (Sign) and Type M (Magnitude) Errors. Perspectives on Psychological Science, 9(6), 641–651. https://doi.org/10.1177/1745691614551642\n\nLu, J., Qiu, Y., & Deng, A. (2018). A note on Type S/M errors in hypothesis testing. British Journal of Mathematical and Statistical Psychology, 72(1), 1–17. https://doi.org/10.1111/bmsp.12132",
                "drafted_by": [
                    "Eduardo Garcia-Garzon"
                ],
                "reviewed_by": [
                    "Helena Hartmann",
                    "Sam Parsons",
                    "Graham Reid",
                    "Mirela Zaneva"
                ],
                "alt_related_terms": [
                    null
                ],
                "language": "english",
                "aliases": [
                    "/glossary/type_s_error"
                ]
            },
            {
                "type": "glossary",
                "title": "Under-representation",
                "definition": "Not all voices, perspectives, and members of the community are adequately represented. Under-representation typically occurs when the voices or perspectives of one group dominate, resulting in the marginalization of another. This often affects groups who are a minority in relation to certain personal characteristics.",
                "related_terms": [
                    "Equity",
                    "Fairness",
                    "Inequality",
                    "WEIRD"
                ],
                "references": "",
                "drafted_by": [
                    "Madeleine Pownall"
                ],
                "reviewed_by": [
                    "Mahmoud Elsherif",
                    "Helena Hartmann",
                    "Bethan Iley",
                    "Adam Parker",
                    "Charlotte R. Pennington, Mirela Zaneva"
                ],
                "alt_related_terms": [
                    null
                ],
                "language": "english",
                "aliases": [
                    "/glossary/under_representation"
                ]
            },
            {
                "type": "glossary",
                "title": "Universal design for learning (UDL)",
                "definition": "A framework for improving learning and optimising teaching based upon scientific insights of how humans learn. It aims to make learning inclusive and transformative for all people in which the focus is on catering to the differing needs of different students. It is often regarded as an evidence-based and scientifically valid framework to guide educational practice, consisting of three key principles: engagement, representation, and action and expression. In addition, UDL is included in the Higher Education Opportunity Act of 2008 (Edyburn, 2010).",
                "related_terms": [
                    "Equal opportunities",
                    "Inclusivity",
                    "Pedagogy",
                    "Teaching practice"
                ],
                "references": "Hitchcock, C., Meyer, A., Rose, D., & Jackson, R. (2002). Providing new access to the general curriculum: Universal design for learning. Teaching Exceptional Children, 35(2), 8–17. https://www.proquest.com/scholarly-journals/providing-new-access-general-curriculum/docview/201139970/se-2?accountid=8630\n\nRose, D. (2000). Universal design for learning. Journal of Special Education Technology, 15(3), 45–49. https://doi.org/10.1177/016264340001500307\n\nRose, D. H., & Meyer, A. (2002). Teaching every student in the digital age: Universal design for learning. In The Corsini Encyclopedia of Psychology. Association for Supervision.",
                "drafted_by": [
                    "Charlotte R. Pennington"
                ],
                "reviewed_by": [
                    "Valeria Agostini",
                    "Mahmoud Elsherif",
                    "Graham Reid",
                    "Mirela Zaneva",
                    "Flávio Azevedo"
                ],
                "alt_related_terms": [
                    null
                ],
                "language": "english",
                "aliases": [
                    "/glossary/universal_design_for_learning"
                ]
            },
            {
                "type": "glossary",
                "title": "Validity",
                "definition": "Validity refers to the application of statistical principles to arrive at well-founded —i.e., likely corresponding accurately to the real world— concepts, conclusions or measurement. In psychometrics, validity refers to the extent to which something measures what it intends to or claims to measure. Under this generic term, there are different types of validity (e.g., internal validity, construct validity, face validity, criterion validity, diagnostic validity, discriminant validity, concurrent validity, convergent validity, predictive validity, external validity).",
                "related_terms": [
                    "Causality",
                    "Construct validity",
                    "Content validity",
                    "Criterion validity",
                    "External validity",
                    "Face validity",
                    "Internal validity",
                    "Measurement",
                    "Questionable Measurement Practices (QMP)",
                    "Psychometry",
                    "Reliability",
                    "Statistical power",
                    "Statistical validity",
                    "Test"
                ],
                "references": "Campbell, D. T. (1957). Factors relevant to the validity of experiments in social settings. Psychological Bulletin, 54(4), 297–312. https://doi.org/10.1037/h0040950\n\nKelley, T. L. (1927). Interpretation of educational measurements. Macmillan.",
                "drafted_by": [
                    "Tamara Kalandadze; Madeleine Pownall; Flávio Azevedo"
                ],
                "reviewed_by": [
                    "Eduardo Garcia-Garzon",
                    "Halil E. Kocalar",
                    "Annalise A. LaPlume",
                    "Joanne McCuaig",
                    "Adam Parker",
                    "Charlotte R. Pennington"
                ],
                "alt_related_terms": [
                    null
                ],
                "language": "english",
                "aliases": [
                    "/glossary/validity"
                ]
            },
            {
                "type": "glossary",
                "title": "Version control",
                "definition": "The practice of managing and recording changes to digital resources (e.g. files, websites, programmes, etc.) over time so that you can recall specific versions later. Version control systems are designed to record the history of changes (who, what and when), and help to avoid human errors (e.g. working on the wrong version). For example, the Git version control system is a widely used software tool that originally helped software developers to version control shared code and is now used across many scientific disciplines to manage and share files.",
                "related_terms": [
                    "Git",
                    "Reproducibility",
                    "Software configuration management",
                    "Source code management",
                    "Source control"
                ],
                "references": "Git. (n.d.). Git—About Version Control. https://git-scm.com/book/en/v2/Getting-Started-About-Version-Control",
                "drafted_by": [
                    "Mahmoud Elsherif"
                ],
                "reviewed_by": [
                    "Sarah Ashcroft-Jones",
                    "Thomas Rhys Evans",
                    "Helena Hartmann",
                    "Matt Jaquiery",
                    "Adam Parker",
                    "Charlotte R. Pennington",
                    "Robert M. Ross",
                    "Timo Roettger",
                    "Andrew J. Stewart"
                ],
                "alt_related_terms": [
                    null
                ],
                "language": "english",
                "aliases": [
                    "/glossary/version_control"
                ]
            },
            {
                "type": "glossary",
                "title": "Webometrics",
                "definition": "Webometrics involves the study of online content. Webometrics focuses on the numbers and types of hyperlinks between different online sites. Such approaches have been considered as a type of altmetrics. “The study of the quantitative aspects of the construction and use of information resources, structures and technologies on the Web drawing on [bibliometric](https://en.wikipedia.org/wiki/Bibliometrics) and [informetric](https://en.wikipedia.org/wiki/Informetrics) approaches” (Björneborn & Ingwersen, 2004).",
                "related_terms": [
                    "Altmetrics",
                    "Bibliometrics"
                ],
                "references": "Bjørneborn, L., & Ingwersen, P. (2004). Toward a basic framework for webometrics. Journal of the American Society for Information Science and Technology, 55(14), 1216–1227. https://doi.org/10.1002/asi.20077",
                "drafted_by": [
                    "Charlotte R. Pennington"
                ],
                "reviewed_by": [
                    "Christopher Graham",
                    "Mirela Zaneva"
                ],
                "alt_related_terms": [
                    null
                ],
                "language": "english",
                "aliases": [
                    "/glossary/webometrics"
                ]
            },
            {
                "type": "glossary",
                "title": "WEIRD",
                "definition": "",
                "related_terms": [
                    "**Alternative definition:** (if applicable) **Related terms to alternative definition:** (if applicable)"
                ],
                "references": "",
                "drafted_by": [],
                "reviewed_by": [],
                "alt_related_terms": [
                    null
                ],
                "language": "english",
                "aliases": [
                    "/glossary/weird"
                ]
            },
            {
                "type": "glossary",
                "title": "Z-Curve",
                "definition": "Computing a Z-score is a statistical approach mainly used to obtain the ‘Estimated Replication Rate’ (ERR) and ‘Expected Discovery Rate’ (EDR) for a set of reported studies. Calculating a *z*\\-curve for a set of statistically significant studies involves converting reported *p*\\-values to *z*\\-scores, fitting a finite mixture model to the distribution of *z*\\-scores, and estimating mean power based on the mixture model. The Z-curve analysis can be performed in R through a dedicated package \\- https://cran.r-project.org/web/packages/zcurve/index.html.",
                "related_terms": [
                    "Altmetrics",
                    "File drawer ratio",
                    "P-curve",
                    "P-hacking",
                    "Replication",
                    "Statistical power"
                ],
                "references": "Bartoš, F., & Schimmack, U. (2020). Z-Curve 2.0: Estimating replication rates and discovery rates. https://doi.org/10.31234/osf.io/urgtn\n\nBrunner, J., & Schimmack, U. (2020). Estimating population mean power under conditions of heterogeneity and selection for significance. Meta-Psychology, 4, MP.2018.874. https://doi.org/10.15626/MP.2018.874",
                "drafted_by": [
                    "Bradley J. Baker"
                ],
                "reviewed_by": [
                    "Kamil Izydorczak",
                    "Sam Parsons",
                    "Charlotte R. Pennington",
                    "Mirela Zaneva"
                ],
                "alt_related_terms": [
                    null
                ],
                "language": "english",
                "aliases": [
                    "/glossary/z_curve"
                ]
            },
            {
                "type": "glossary",
                "title": "Zenodo",
                "definition": "An open science repository where researchers can deposit research papers, reports, data sets, research software, and any other research-related digital artifacts. Zenodo creates a persistent digital object identifier (DOI) for each submission to make it citable. This platform was developed under the European OpenAIRE program and operated by CERN.",
                "related_terms": [
                    "DOI (digital object identifier)",
                    "figshare",
                    "Open data",
                    "Open Science Framework",
                    "Preprint"
                ],
                "references": "Zenodo. (n.d.). Zenodo—Research. Shared. https://www.zenodo.org/",
                "drafted_by": [
                    "Ali H. Al-Hoorie"
                ],
                "reviewed_by": [],
                "alt_related_terms": [
                    null
                ],
                "language": "english",
                "aliases": [
                    "/glossary/zenodo"
                ]
            }
        ]
    }
]