{
    "type": "glossary",
    "title": "البحث الجماعي [Crowdsourced Research]",
    "definition": "التعريف:** البحث الجماعي هو نموذج للتَّنظيم الاجتماعي؛ للبحث كتعاون واسع النِّطاق يتم فيه تنفيذ مشروع بحثي واحد، أو أكثر من قبل فرق متعدِّدة بطريقة مستقلَّة ولكن منسَّقة. يهدف البحث الجماعي إلى تحقيق مكاسب الكفاءة، وقابليَّة التَّوسع من خلال تجميع الموارد، وتعزيز الشَّفافيَّة، والاندماج الاجتماعي، بالإضافة إلى زيادة الدِّقة والموثوقية والجدارة من خلال تعزيز القوة الإحصائيَّة، والتَّحقق الاجتماعي المتبادل. يتعارض البحث الجماعي مع النَّموذج التَّقليدي لإنتاج البحث الأكاديمي، والذي يهيمن عليه العمل المستقلّ للأفراد، أو مجموعات صغيرة من الباحثين (\"العلوم الصغيرة\"). من أمثلة البحث الجماعي ما يسمى بدراسات \"التِّكرار المتعدِّد المعامل'' (Klein et al., 2018)، ودراسات \"العديد من المحلِّلين لمجموعة بيانات واحدة '' (Silberzahn et al., 2018)، والشَّبكات التَّعاونيَّة التَّوزيعيَّة (Moshontz et al., 2018\\) ومشاريع الكتابة التَّعاونية المفتوحة مثل: أوراق الإنترنت المفتوحة الضَّخمة (Himmelstein et al., 2019; Tennant et al., 2019). وكذلك يمكن أن يشير البحث الجماعي إلى استخدام عدد كبير من \"حشود العمال\" في جمع البيانات من خلال أسواق العمل عبر الإنترنت مثل Amazon Mechanical Turk أو Prolific، مثلًا في تحليل المحتوى (Benoit et al., 2016; Lind et al., 2017\\) أو البحث التَّجريبي (Peer et al., 2017). يُشار إلى البحث الجماعي المفتوح للمشاركة، والمفتوح من خلال المخرجات الوسيطة المشتركة باسم علم الحشود (Franzoni & Sauermann, 2014). **المصطلحات ذات الصِّلة:** علم المواطن، التَّعاون، توظيف الحشود، علم الفريق",
    "related_terms": [
        "Citizen science",
        "Collaboration",
        "Crowdsourcing",
        "Team science"
    ],
    "references": [
        "Benoit, K., Conway, D., Lauderdale, B. E., Laver, M., & Mikhaylov, S. (2016). Crowd-sourced text analysis: Reproducible and agile production of political data. American Political Science Review, 110(2), 278–295. https://doi.org/10.1017/S0003055416000058",
        "Breznau, N. (2021). I saw you in the crowd: Credibility, reproducibility, and meta-utility. PS: Political Science & Politics, 54(2), 309–313. https://doi.org/10.1017/S1049096520000980",
        "Franzoni, C., & Sauermann, H. (2014). Crowd science: The organization of scientific research in open collaborative projects. Research Policy, 43(1), 1–20. https://doi.org/10.1016/j.respol.2013.07.005",
        "Himmelstein, D. S., Rubinetti, V., Slochower, D. R., Hu, D., Malladi, V. S., Greene, C. S., & Gitter, A. (2019). Open collaborative writing with Manubot. PLOS Computational Biology, 15(6), e1007128. https://doi.org/10.1371/journal.pcbi.1007128",
        "Klein, R. A., Vianello, M., Hasselman, F., Adams, B. G., Adams, R. B., Alper, S., & … Nosek, B. A. (2018). Many Labs 2: Investigating Variation in Replicability Across Samples and Settings. Advances in Methods and Practices in Psychological Science, 1(4), 443–490. https://doi.org/10.1177/2515245918810225",
        "Lind, F., Gruber, M., & Boomgaarden, H. G. (2017). Content analysis by the crowd: Assessing the usability of crowdsourcing for coding latent constructs. Communication Methods and Measures, 11(3), 191–209. https://doi.org/10.1080/19312458.2017.1317338",
        "Moshontz, H., Campbell, L., Ebersole, C. R., IJzerman, H., Urry, H. L., Forscher, P. S., & Chartier, C. R. (2018). The Psychological Science Accelerator: Advancing psychology through a distributed collaborative network. Advances in Methods and Practices in Psychological Science, 1(4), 501–515. https://doi.org/10.1177/2515245918797607",
        "Peer, E., Brandimarte, L., Samat, S., & Acquisti, A. (2017). Beyond the Turk: Alternative platforms for crowdsourcing behavioral research. Journal of Experimental Social Psychology, 70, 153–163. https://doi.org/10.1016/j.jesp.2017.01.006",
        "Silberzahn, R., Uhlmann, E. L., Martin, D. P., Anselmi, P., Aust, F., Awtrey, E., Bahník, Š., Bai, F., Bannard, C., Bonnier, E., & others. (2018). Many Analysts, One Data Set: Making Transparent How Variations in Analytic Choices Affect Results. Advances in Methods and Practices in Psychological Science, 337–356. https://doi.org/10.1177/2515245917747646",
        "Stewart, N., Chandler, J., & Paolacci, G. (2017). Crowdsourcing samples in cognitive science. Trends in Cognitive Sciences, 21(10), 736–748. https://doi.org/10.1016/j.tics.2017.06.007",
        "Tennant, J., Bielczyk, N. Z., Cheplygina, V., Greshake Tzovaras, B., Hartgerink, C. H. J., Havemann, J., Masuzzo, P., & Steiner, T. (2019). Ten simple rules for researchers collaborating on Massively Open Online Papers (MOOPs). MetaArXiv. https://doi.org/10.31222/osf.io/et8ak",
        "Uhlmann, E. L., Ebersole, C. R., Chartier, C. R., Errington, T. M., Kidwell, M. C., Lai, C. K., McCarthy, R. J., Riegelman, A., Silberzahn, R., & Nosek, B. A. (2019). Scientific utopia III: Crowdsourcing science. Perspectives on Psychological Science, 14(5), 711–733. https://doi.org/10.1177/1745691619850561",
        "Week, C. (2021). What is Crowdsourcing? https://crowdsourcingweek.com/what-is-crowdsourcing/"
    ],
    "drafted_by": [
        "Eike Mark Rinke"
    ],
    "reviewed_by": [
        "Ali H. Al-Hoorie",
        "Sam Parsons",
        "Charlotte R. Pennington",
        "Suzanne L. K. Stewart",
        "Flávio Azevedo"
    ],
    "alt_related_terms": [
        null
    ],
    "language": "arabic",
    "translated_by": [
        "Naif Masrahi"
    ],
    "translation_reviewed_by": [
        "Asma Alzahrani",
        "Awatif Alruwaili",
        "Ali H. Al-Hoorie",
        "Mohammed Mohsen"
    ]
}