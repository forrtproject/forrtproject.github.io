+++
# A Demo section created with the Blank widget.
# Any elements can be added in the body: https://sourcethemes.com/academic/docs/writing-markdown-latex/
# Add more sections by duplicating this file and customizing to your requirements.

widget = "blank"  # See https://sourcethemes.com/academic/docs/page-builder/
headless = true  # This file represents a page section.
active = true  # Activate this widget? true/false
weight = 10  # Order that this section will appear.

title = "Cluster 1: Reproducibility Crisis and Credibility Revolution"
subtitle = ""

[design]
  # Choose how many columns the section has. Valid values: 1 or 2.
  columns = "1"

[design.background]
  # Apply a background color, gradient, or image.
  #   Uncomment (by removing `#`) an option to apply it.
  #   Choose a light or dark text color by setting `text_color_light`.
  #   Any HTML color name or Hex value is valid.

  # Background color.
  # color = "red"
  color = "#cacfdc" # greenish
  
  # Background gradient.
  # gradient_start = "DeepSkyBlue"
  # gradient_end = "SkyBlue"
  
  # Background image.
  # image = "headers/bubbles-wide.webp"  # Name of image in `static/img/`.
  # image_darken = 0.6  # Darken the image? Range 0-1 where 0 is transparent and 1 is opaque.
  # image_size = "cover"  #  Options are `cover` (default), `contain`, or `actual` size.
  # image_position = "center"  # Options include `left`, `center` (default), or `right`.
  # image_parallax = true  # Use a fun parallax-like fixed background effect? true/false

  # Text color (true=light or false=dark).
  text_color_light = false

[design.spacing]
  # Customize the section spacing. Order is top, right, bottom, left.
  padding = ["60px", "0", "60px", "0"]

[advanced]
 # Custom CSS. 
 css_style = "font-size: 1rem;"
 
 # CSS class.
 css_class = ""
+++

### Description

Attainment of foundational knowledge on the emergence of, and importance of, reproducible and open research (i.e., grounding the motivations and theoretical underpinnings of Open and Reproducible Science). Integration with field specific content (i.e., grounded in the history of replicability). There are 6 sub-clusters which aim to further parse the learning and teaching process:

* History of the reproducibility crisis & credibility revolution.
* Exploratory and confirmatory analyses.
* Questionable research practices and their prevalence.
* Proposed improvement science initiatives on statistics, measurement, teaching, data sharing, code sharing, pre-registration, replication.
* Ongoing debates (e.g., incentives for and against open science).
* Ethical considerations for improved practices.

<br>

<ul class="nav nav-tabs" id="myTab" role="tablist">
  <li class="nav-item">
    <a class="nav-link active" id="C1S1-tab" data-toggle="tab" href="#C1S1" role="tab" aria-controls="C1S1"
      aria-selected="true">History</a>
  </li>
  <li class="nav-item">
    <a class="nav-link" id="C1S2-tab" data-toggle="tab" href="#C1S2" role="tab" aria-controls="C1S2"
      aria-selected="false">Analyses</a>
  </li>
  <li class="nav-item">
    <a class="nav-link" id="C1S3-tab" data-toggle="tab" href="#C1S3" role="tab" aria-controls="C1S3"
      aria-selected="false">QRPs</a>
  </li>
    <li class="nav-item">
    <a class="nav-link" id="C1S4-tab" data-toggle="tab" href="#C1S4" role="tab" aria-controls="C1S4"
      aria-selected="false">Improvements</a>
  </li>
    <li class="nav-item">
    <a class="nav-link" id="C1S5-tab" data-toggle="tab" href="#C1S5" role="tab" aria-controls="C1S5"
      aria-selected="false">Ongoing debates</a>
  </li>
    <li class="nav-item">
    <a class="nav-link" id="C1S6-tab" data-toggle="tab" href="#C1S6" role="tab" aria-controls="C1S6"
      aria-selected="false">Ethics</a>
  </li>
</ul>

<div class="tab-content" id="myTabContent">
  <div class="tab-pane fade show active" id="C1S1" role="tabpanel" aria-labelledby="C1S1-tab"><br>

## History of the reproducibility crisis & credibility revolution

* Baker, M. (2016). 1,500 scientists lift the lid on reproducibility. Nature News, 533(7604), 452. doi: https://doi.org/10.1038/533452a

* Baker, M. (2016). Is there a reproducibility crisis? Nature, 533(7604), 3–5. doi: https://doi.org/10.1038/d41586-019-00067-3

* Chambers, C. (2017). The seven deadly sins of psychology: A manifesto for reforming the culture of scientific practice. Princeton University Press.
http://dx.doi.org/10.1515/9781400884940

* Crüwell, S., van Doorn, J., Etz, A., Makel, M. C., Moshontz, H., Niebaum, J., … SchulteMecklenbeck, M. (2018, November 16). 7 easy steps to open science: An annotated reading list. https://doi.org/10.31234/osf.io/cfzyx

* Edwards, M. A., & Roy, S. (2016). Academic research in the 21st century: Maintaining scientific integrity in a climate of perverse incentives and hypercompetition. Environmental Engineering Science, 34(1), 51-61. DOI: https://doi.org/10.1089/ees.2016.0223

* Merton, R., K. (1968). The Matthew effect in science. Science, 159(3810), 56-63.
10.1126/science.159.3810.56

* Merton, R., K. (1988). The Matthew Effect in Science, II: Cumulative Advantage and the Symbolism of Intellectual Property. ISIS, 79(4), 606-623. 10.1086/354848

* Munafo, M. R., et al. (2017). A manifesto for reproducible science. Nature Human Behaviour, 1, 0021. DOI: 10.0138/s41562-016-0021

* Vazire, S. (2018). Implications of the Credibility Revolution for Productivity, Creativity, and Progress. Perspectives on Psychological Science, 13(4), 411-417.
https://doi.org/10.1177/1745691617751884

<br>
</div>
  <div class="tab-pane fade" id="C1S2" role="tabpanel" aria-labelledby="C1S2-tab"><br>

## Exploratory and confirmatory analyses

***Confirmatory analyses refer to tests of hypotheses that are formulated prior to data collection. Exploratory analyses refer to everything else.***

* Chambers, C. (2017). The seven deadly sins of psychology: A manifesto for reforming the culture of scientific practice. Princeton University Press.
http://dx.doi.org/10.1515/9781400884940

* Lin, W., & Green, D. P. (2016). Standard operating procedures: A safety net for pre-analysis plans. PS: Political Science & Politics, 49(3), 495-500.

* Wagenmakers, E.-J., Wetzels, R., Borsboom, D., van der Mass, H. L. J., & Kievit, R. A. (2012). An agenda for purely confirmatory research. Perspectives on Psychological Science, 7(6), 632–638. doi:10.1177/1745691612463078

* Wagenmakers , E.-J., Dutilh, G., & Sarafoglou, A. (2018). The Creativity-Verification Cycle in Psychological Science: New Methods to Combat Old Idols. Perspectives on Psychological Science, 13(4), 418–427. https://doi.org/10.1177/1745691618771357

<br>
</div>
  <div class="tab-pane fade" id="C1S3" role="tabpanel" aria-labelledby="C1S3-tab"><br>

## Questionable research practices and their prevalence

***The ways in which researchers engage in behaviors and decision-making that increase the probability of their (consciously or unconsciously) desired result.***

* Gelman, A., & Loken, E. (2013). The garden of forking paths: Why multiple comparisons can be a problem, even when there is no “fishing expedition” or “p-hacking” and the research hypothesis was posited ahead of time. Unpublished manuscript. http://www.stat.columbia.edu/~gelman/research/unpublished/p_hacking.pdf

* John, L. K., Loewenstein, G., & Prelec, D. (2012). Measuring the prevalence of questionable research practices with incentives for truth telling. Psychological Science, 23(5), 524-532. https://doi.org/10.1177/0956797611430953
 
* Simmons, J. P., Nelson, L. D., & Simonsohn, U. (2011). False-Positive Psychology: Undisclosed Flexibility in Data Collection and Analysis Allows Presenting Anything as Significant. Psychological Science, 22(11), 1359–1366.https://doi.org/10.1177/0956797611417632

* Smaldino, P. E., & McElreath, R. (2016). The natural selection of bad science. Royal Society open science, 3(9), 160384.https://doi.org/10.1098/rsos.160384

* Wicherts, J. M., Veldkamp, C. L., Augusteijn, H. E., Bakker, M., Van Aert, R. C., & Van Assen, M. A. (2016). Degrees of freedom in planning, running, analyzing, and reporting psychological studies: A checklist to avoid p-hacking. Frontiers in psychology, 7. 

<br>
</div>
  <div class="tab-pane fade" id="C1S4" role="tabpanel" aria-labelledby="C1S4-tab"><br>

## Proposed improvement science initiatives on statistics, measurement, teaching, data sharing, code sharing, pre-registration, replication

***Published checklists and other resources that can be used to shift behavior more toward improved practices.***

* Crüwell, S., van Doorn, J., Etz, A., Makel, M. C., Moshontz, H., Niebaum, J., … SchulteMecklenbeck, M. (2018, November 16). 7 easy steps to open science: An annotated reading list. https://doi.org/10.31234/osf.io/cfzyx

* Lindsay (2020) Seven steps toward transparency and replicability in psychological science. Canadian Psychology/Psychologie canadienne.

* Ioannidis, J. P., Munafo, M. R., Fusar-Poli, P., Nosek, B. A., & David, S. P. (2014). Publication and other reporting biases in cognitive sciences: detection, prevalence, and prevention. Trends in cognitive sciences, 18(5), 235-241.

* Klein, R. A., Vianello, M., Hasselman, F., Adams, B. G., Adams, R. B., Alper, S., … Nosek, B. A. (2018). Many Labs 2: Investigating Variation in Replicability Across Samples and Settings. Advances in Methods and Practices in Psychological Science, 1(4), 443–490. https://doi.org/10.1177/2515245918810225

* Munafo, M. R., et al. (2017). A manifesto for reproducible science. Nature Human Behaviour, 1, 0021. DOI: 10.0138/s41562-016-0021

* Peng, R. (2015). The reproducibility crisis in science: A statistical counterattack. Significance, 12(3). https://doi.org/10.1111/j.1740-9713.2015.00827.x


<br>
</div>
  <div class="tab-pane fade" id="C1S5" role="tabpanel" aria-labelledby="C1S5-tab"><br>

## Ongoing debates (e.g., incentives for and against open science)

* Bahlai et al. (2019). Open science isn't always open to all scientists. American Scientist, 107(2), 78. DOI: https://doi.org/10.1511/2019.107.2.78

* Chen, X., Dallmeier-Tiessen, S., Dasler, R., Feger, S., Fokianos, P., Gonzalez, J. B., ... & Rodriguez, D. R. et al. (2019). Open is not enough. Nature : Physics, 15 (2), 113-119. https://doi.org/10.1038/s41567-018-0342-2

* Drummond, C. (2018).; Reproducible research: a minority opinion. Journal of Experimental & Theoretical Artificial Intelligence, 30(1), 1-11. https://doi.org/10.1080/0952813X.2017.1413140

* Fanelli, D. (2018). Opinion: Is science really facing a reproducibility crisis, and do we need it to? Proceedings of the National Academy of Sciences, 115(11), 2628-2631. https://doi.org/10.1073/pnas.1708272114

* Fanelli, D., & Ioannidis, J. P. (2013). US studies may overestimate effect sizes in softer research. Proceedings of the National Academy of Sciences, 110(37), 15031-15036. https://doi.org/10.1073/pnas.1302997110

* Fell, M. J. (2019). The economic impacts of open science: A rapid evidence assessment. Publications, 7(3), 46. https://doi.org/10.3390/publications7030046

* Pashler, H., & Harris, C. R. (2012). Is the replicability crisis overblown? Three arguments examined. Perspectives on Psychological Science, 7, 531‑536. https://doi.org/10.1177/1745691612463401

<br>
</div>
  <div class="tab-pane fade" id="C1S6" role="tabpanel" aria-labelledby="C1S6-tab"><br>

## Ethical considerations for improved practices

* Brabeck, M. M. (2021). Open science and feminist ethics: Promises and challenges of open access. Psychology of Women Quarterly, 45(4), 457-474. https://doi.org/10.1177/03616843211030926

* Bol, T., de Vaan, M., & van de Rijt, A. (2018). The Matthew effect in science funding. Proceedings of the National Academy of Sciences, 115(19), 4887-4890. https://doi.org/10.1073/pnas.1719557115

* Chopik, W. J., Bremner, R. H., Defever, A. M., & Keller, V. N. (2018). How (and whether) to teach undergraduates about the replication crisis in psychological science. Teaching of Psychology, 45(2), 158–163. https://doi.org/10.1177/0098628318762900

* Edwards, M. A., & Roy, S. (2016). Academic research in the 21st century: Maintaining scientific integrity in a climate of perverse incentives and hypercompetition. Environmental Engineering Science, 34(1), 51-61. DOI: https://doi.org/10.1089/ees.2016.0223

* Fell, M. J. (2019). The economic impacts of open science: A rapid evidence assessment. Publications, 7(3), 46. https://doi.org/10.3390/publications7030046

* Jones, NL. (2007). A code of ethics for the life sciences. Science and Engineering Ethics, 13, 25-43. DOI:https://doi.org/ 0.1007/s11948-006-0007-x

<br>
</div>
</div>
