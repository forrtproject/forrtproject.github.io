{
    "timestamp": "2020-06-08T18:10:23.082Z",
    "title": "A Duty to Describe: Better the Devil You Know Than the Devil You Don't",
    "link_to_resource": "https://doi.org/10.1177/1745691614551749",
    "creators": [
        "Sacha D Brown",
        "David Furrow",
        "Daniel F Hill",
        "Jonathon C Gable",
        "Liam P Porter",
        "W Jake Jacobs"
    ],
    "material_type": [
        "Primary Source",
        "Reading",
        "Paper"
    ],
    "education_level": [
        "College / Upper Division (Undergraduates)"
    ],
    "abstract": "Although many researchers have discussed replication as a means to facilitate self-correcting science, in this article, we identify meta-analyses and evaluating the validity of correlational and causal inferences as additional processes crucial to self-correction. We argue that researchers have a duty to describe sampling decisions they make; without such descriptions, self-correction becomes difficult, if not impossible. We developed the Replicability and Meta-Analytic Suitability Inventory (RAMSI) to evaluate the descriptive adequacy of a sample of studies taken from current psychological literature. Authors described only about 30% of the sampling decisions necessary for self-correcting science. We suggest that a modified RAMSI can be used by authors to guide their written reports and by reviewers to inform editorial recommendations. Finally, we claim that when researchers do not describe their sampling decisions, both readers and reviewers may assume that those decisions do not matter to the outcome of the study, do not affect inferences made from the research findings, do not inhibit inclusion in meta-analyses, and do not inhibit replicability of the study. If these assumptions are in error, as they often are, and the neglected decisions are relevant, then the neglect may create a good deal of mischief in the field.",
    "language": [
        "English"
    ],
    "conditions_of_use": "I don't see any of these",
    "primary_user": [
        "Student"
    ],
    "subject_areas": [
        "Social Science"
    ],
    "FORRT_clusters": [
        "Reproducibility and Replicability Knowledge",
        "Replication Research"
    ],
    "tags": [
        "Reproducibility Crisis and Credibility Revolution",
        "Open Science"
    ]
}