{
    "timestamp": "12/12/2024 9:47:46",
    "title": "Self-reported checklists and quality scoring tools in radiomics: a meta-research",
    "link_to_resource": "https://doi.org/10.1007/s00330-023-10487-5",
    "creators": [
        "Burak Kocak",
        "Tugba Akinci D\u2019Antonoli",
        "Ece Ates Kus",
        "Ali Keles",
        "Ahmet Kala",
        "Fadime Kose",
        "Mehmet Kadioglu",
        "Sila Solak",
        "Seyma Sunman",
        "Zisan Hayriye Temiz"
    ],
    "material_type": [
        "Reading"
    ],
    "education_level": [
        "Graduate / Professional"
    ],
    "abstract": "Objective\nTo evaluate the use of reporting checklists and quality scoring tools for self-reporting purposes in radiomics literature.\n\nMethods\nLiterature search was conducted in PubMed (date, April 23, 2023). The radiomics literature was sampled at random after a sample size calculation with a priori power analysis. A systematic assessment for self-reporting, including the use of documentation such as completed checklists or quality scoring tools, was conducted in original research papers. These eligible papers underwent independent evaluation by a panel of nine readers, with three readers assigned to each paper. Automatic annotation was used to assist in this process. Then, a detailed item-by-item confirmation analysis was carried out on papers with checklist documentation, with independent evaluation of two readers.\n\nResults\nThe sample size calculation yielded 117 papers. Most of the included papers were retrospective (94%; 110/117), single-center (68%; 80/117), based on their private data (89%; 104/117), and lacked external validation (79%; 93/117). Only seven papers (6%) had at least one self-reported document (Radiomics Quality Score (RQS), Transparent Reporting of a multivariable prediction model for Individual Prognosis Or Diagnosis (TRIPOD), or Checklist for Artificial Intelligence in Medical Imaging (CLAIM)), with a statistically significant binomial test (p\u2009<\u20090.001). Median rate of confirmed items for all three documents was 81% (interquartile range, 6). For quality scoring tools, documented scores were higher than suggested scores, with a mean difference of\u2009\u2212\u20097.2 (standard deviation, 6.8).\n\nConclusion\nRadiomic publications often lack self-reported checklists or quality scoring tools. Even when such documents are provided, it is essential to be cautious, as the accuracy of the reported items or scores may be questionable.\n\nClinical relevance statement\nCurrent state of radiomic literature reveals a notable absence of self-reporting with documentation and inaccurate reporting practices. This critical observation may serve as a catalyst for motivating the radiomics community to adopt and utilize such tools appropriately, thereby fostering rigor, transparency, and reproducibility of their research, moving the field forward.",
    "language": [
        "English"
    ],
    "conditions_of_use": "CC BY",
    "primary_user": [
        "Student",
        "Teacher"
    ],
    "subject_areas": [
        "Life Science",
        "Physical Science"
    ],
    "FORRT_clusters": [
        "Reproducible Analyses"
    ],
    "tags": [
        "Radiomics",
        "Research Quality",
        "Self-Reporting",
        "Reporting Checklists",
        "Scoring Tools",
        "Literature Review"
    ]
}