{
    "timestamp": "2/13/2025 9:59:50",
    "title": "Open Science interventions to improve reproducibility and replicability of research: a scoping review preprint",
    "link_to_resource": "https://doi.org/10.31222/osf.io/a8rmu",
    "creators": [
        "Leonie Dudda",
        "Eva Kormann",
        "Magdalena Kozula",
        "Nicholas J DeVito",
        "Thomas Klebel",
        "Ayu Putu Madri Dewi",
        "Ren\u00e9 Spijker",
        "Inge Stegeman",
        "Veerle Van den Eynden",
        "Tony Ross-Hellauer",
        "and Mariska Leeflang"
    ],
    "material_type": [
        "Reading"
    ],
    "education_level": [
        "College / Upper Division (Undergraduates)",
        "Graduate / Professional"
    ],
    "abstract": "Various interventions \u2013 especially those related to open science \u2013 have been proposed to improve the reproducibility and replicability of scientific research. To assess whether and which interventions have been formally tested for their effectiveness in improving reproducibility and replicability, we conducted a scoping review of the literature on interventions to improve reproducibility. We systematically searched Medline, Embase, Web of Science, PsycINFO, Scopus and Eric, on August 18, 2023. Grey literature was requested from experts in the fields of reproducibility and open science. Any study empirically evaluating the effectiveness of interventions aimed at improving the reproducibility or replicability of scientific methods and findings was included. An intervention could be any action taken by either individual researchers or scientific institutions (e.g., research institutes, publishers and funders). We summarized the retrieved evidence narratively and in an evidence gap map. Of the 104 distinct studies we included, 15 directly measured the effect of an intervention on reproducibility or replicability, while the other research questions addressed a proxy outcome that might be expected to increase reproducibility or replicability, such as data sharing, methods transparency or preregistration. Thirty research questions within included studies were non-comparative and 27 were comparative but cross-sectional, precluding any causal inference. Possible limitations of our review may be the search and selection strategy, which was done by a large team including researchers from different disciplines and different expertise levels. Despite studies investigating a range of interventions and addressing various outcomes, our findings indicate that in general the evidence-base for which various interventions to improve reproducibility of research remains remarkably limited in many respects.\n",
    "language": [
        "English"
    ],
    "conditions_of_use": "CC BY",
    "primary_user": [
        "Student",
        "Teacher"
    ],
    "subject_areas": [
        "Social Science"
    ],
    "FORRT_clusters": [
        "Reproducibility and Replicability Knowledge",
        "Reproducible Analyses",
        "Replication Research"
    ],
    "tags": [
        "Data Sharing",
        "Ethics",
        "Open Science",
        "Replicability",
        "Reporting Guidelines",
        "Reproducibility Research Integrity"
    ]
}