[{"authors":["Name \"Alaa Aldoh\""],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1674830952,"objectID":"8161d5c7a084ed9c3316b07652533187","permalink":"https://forrt.org/author/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/","section":"authors","summary":"","tags":null,"title":"","type":"authors"},{"authors":["Name \"Aleksandra Lazić\""],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1646138511,"objectID":"6e27f6db0e77282f5818bd186a9e5fff","permalink":"https://forrt.org/author/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/","section":"authors","summary":"","tags":null,"title":"","type":"authors"},{"authors":["Name \"Alexander Hart\""],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1646138511,"objectID":"d2d05afd18d52dbdaf4de91360c3f8ea","permalink":"https://forrt.org/author/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/","section":"authors","summary":"","tags":null,"title":"","type":"authors"},{"authors":["Name \"Alice Rees\""],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1608150137,"objectID":"edebd83da6e0e3aad79de79c643d2d38","permalink":"https://forrt.org/author/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/","section":"authors","summary":"","tags":null,"title":"","type":"authors"},{"authors":["Name \"Amanda Moehring\""],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1646138511,"objectID":"3fe75f04c8fcde2121edc35433f0c2a0","permalink":"https://forrt.org/author/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/","section":"authors","summary":"","tags":null,"title":"","type":"authors"},{"authors":["Name \"Amani Aloufi\""],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1674830952,"objectID":"2c0f8517e98396be0a4cfb27414962dd","permalink":"https://forrt.org/author/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/","section":"authors","summary":"","tags":null,"title":"","type":"authors"},{"authors":["Name \"Amélie Bret\""],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1674830952,"objectID":"b430477d7ba85464907c09d88db19972","permalink":"https://forrt.org/author/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/","section":"authors","summary":"","tags":null,"title":"","type":"authors"},{"authors":["Name \"Amelie Gourdon Kanhukamwe\""],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1646138511,"objectID":"965aae2edb6166ccc3af95ea3a29b52f","permalink":"https://forrt.org/author/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/","section":"authors","summary":"","tags":null,"title":"","type":"authors"},{"authors":["Name \"Anna Meier\""],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1685464384,"objectID":"666f2b7970732e7864394f0b44ef2224","permalink":"https://forrt.org/author/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/","section":"authors","summary":"","tags":null,"title":"","type":"authors"},{"authors":["Name \"Annalisa Myer\""],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1650618293,"objectID":"d2b7f5c73919bc73d5699aaa37d97bf6","permalink":"https://forrt.org/author/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/","section":"authors","summary":"","tags":null,"title":"","type":"authors"},{"authors":["Name \"Annalise LaPlume\""],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1646138511,"objectID":"c0b590528587180261adbba565410741","permalink":"https://forrt.org/author/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/","section":"authors","summary":"","tags":null,"title":"","type":"authors"},{"authors":["Name \"Aoife O'Mahony\""],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1646138511,"objectID":"ba0ca0a928f64df2906423fb9e760fe5","permalink":"https://forrt.org/author/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/","section":"authors","summary":"","tags":null,"title":"","type":"authors"},{"authors":["Name \"Ashley Blake\""],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1674830952,"objectID":"2d5da9e7cf7cc3ec57127087ac94a627","permalink":"https://forrt.org/author/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/","section":"authors","summary":"","tags":null,"title":"","type":"authors"},{"authors":["Name \"Balazs Aczel\""],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1592830477,"objectID":"36a9df81c7e7b97a8d83ed44c5e6561b","permalink":"https://forrt.org/author/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/","section":"authors","summary":"","tags":null,"title":"","type":"authors"},{"authors":["Name \"Ben Saunders\""],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1592830477,"objectID":"e44b75b8bc6bba3f0cb3c18e876b9388","permalink":"https://forrt.org/author/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/","section":"authors","summary":"","tags":null,"title":"","type":"authors"},{"authors":["Name \"Bethan Iley\""],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1675619649,"objectID":"18a45bb7b669411024ffe80be3d148d7","permalink":"https://forrt.org/author/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/","section":"authors","summary":"","tags":null,"title":"","type":"authors"},{"authors":["Name \"Bradley Baker\""],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1646138511,"objectID":"742b315f4f5b36d3d44d812a3499619d","permalink":"https://forrt.org/author/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/","section":"authors","summary":"","tags":null,"title":"","type":"authors"},{"authors":["Name \"Catherine Laverty\""],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1646138511,"objectID":"0869e384e022678c0038a8b76c4b00bc","permalink":"https://forrt.org/author/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/","section":"authors","summary":"","tags":null,"title":"","type":"authors"},{"authors":["Name \"Catia Oliveira\""],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1602363467,"objectID":"dff5f34033647da9be953463b02227e1","permalink":"https://forrt.org/author/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/","section":"authors","summary":"","tags":null,"title":"","type":"authors"},{"authors":["Name \"Charlotte R. Pennington\""],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1675939942,"objectID":"cc86160f8ab80b0f71dce92c5ae4f111","permalink":"https://forrt.org/author/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/","section":"authors","summary":"","tags":null,"title":"","type":"authors"},{"authors":["Name \"Christopher Eaker\""],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1646138511,"objectID":"d9437187704bcdef32a500bbded4a3c7","permalink":"https://forrt.org/author/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/","section":"authors","summary":"","tags":null,"title":"","type":"authors"},{"authors":["Name \"Ciara Egan\""],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1674830952,"objectID":"3996ad00be8957a9c28b8affc33d8556","permalink":"https://forrt.org/author/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/","section":"authors","summary":"","tags":null,"title":"","type":"authors"},{"authors":["Name \"Connor Keating\""],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1646138511,"objectID":"b95548949e16d92d35223c72432e905d","permalink":"https://forrt.org/author/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/","section":"authors","summary":"","tags":null,"title":"","type":"authors"},{"authors":["Name \"Crystal Steltenpohl\""],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1702593473,"objectID":"381849c7951def6fd40a5d33bb001b18","permalink":"https://forrt.org/author/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/","section":"authors","summary":"","tags":null,"title":"","type":"authors"},{"authors":["Name \"Daniela Duca\""],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1613065032,"objectID":"86f9a7aee1456c3a67aa04483a4ec902","permalink":"https://forrt.org/author/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/","section":"authors","summary":"","tags":null,"title":"","type":"authors"},{"authors":["Name \"David Moreau\""],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1674830952,"objectID":"0e0e62d5d7b1bfdbd45d795852f8f00f","permalink":"https://forrt.org/author/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/","section":"authors","summary":"","tags":null,"title":"","type":"authors"},{"authors":["Name \"Dermot Lynott\""],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1715341372,"objectID":"e529abc6ee521f3856463084ada756a0","permalink":"https://forrt.org/author/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/","section":"authors","summary":"","tags":null,"title":"","type":"authors"},{"authors":["Name \"Eike Mark Rinke\""],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1592830477,"objectID":"f00502d9cbec8387c3f8bb8529360225","permalink":"https://forrt.org/author/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/","section":"authors","summary":"","tags":null,"title":"","type":"authors"},{"authors":["Name \"Filip Děchtěrenko\""],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1646138511,"objectID":"ef5c6f4f7aa7fa75404958ca7d5cc784","permalink":"https://forrt.org/author/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/","section":"authors","summary":"","tags":null,"title":"","type":"authors"},{"authors":["Name \"Flavio Azevedo\""],"categories":null,"content":"Flavio Azevedo, a Fulbright fellow, is currently working as a research associate at the Institute for Communication Science (IfKW) at Friedrich Schiller University Jena, Germany. Recently, Flavio was named as one of the 100 most influential early career Portuguese via the “Global Shapers” initiative by the World Economic Forum.\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1633801483,"objectID":"009ebc9f365f93911a11a4959663485f","permalink":"https://forrt.org/author/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/","section":"authors","summary":"Flavio Azevedo, a Fulbright fellow, is currently working as a research associate at the Institute for Communication Science (IfKW) at Friedrich Schiller University Jena, Germany. Recently, Flavio was named as one of the 100 most influential early career Portuguese via the “Global Shapers” initiative by the World Economic Forum.","tags":null,"title":"","type":"authors"},{"authors":["Name \"Gavin Leech\""],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1638372245,"objectID":"92d704a2cbc9f4bdd0dfd60623e5db0a","permalink":"https://forrt.org/author/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/","section":"authors","summary":"","tags":null,"title":"","type":"authors"},{"authors":["Name \"Gilad Feldman\""],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1646138511,"objectID":"3df4d14b19acdc6afa7a0692a1e5784c","permalink":"https://forrt.org/author/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/","section":"authors","summary":"","tags":null,"title":"","type":"authors"},{"authors":["Name \"Giorgia Andreolli\""],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1660038643,"objectID":"033d9476c8305fb4d1ab0be15ce3795a","permalink":"https://forrt.org/author/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/","section":"authors","summary":"","tags":null,"title":"","type":"authors"},{"authors":["Name \"Heather Urry\""],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1601933290,"objectID":"826e341b085d95247b78857ba130502d","permalink":"https://forrt.org/author/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/","section":"authors","summary":"","tags":null,"title":"","type":"authors"},{"authors":["Name \"Helena Hartmann\""],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1652896910,"objectID":"653a4b9cd5b9c1d089a03a57b0792ba7","permalink":"https://forrt.org/author/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/","section":"authors","summary":"","tags":null,"title":"","type":"authors"},{"authors":["Name \"Henrik Schönemann\""],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1711537561,"objectID":"d69cc87de45405ebd572ab909c7fb944","permalink":"https://forrt.org/author/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/","section":"authors","summary":"","tags":null,"title":"","type":"authors"},{"authors":["Name \"Jacob Miranda\""],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1602086269,"objectID":"7061be5a9a57da37c485766a0b6c548b","permalink":"https://forrt.org/author/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/","section":"authors","summary":"","tags":null,"title":"","type":"authors"},{"authors":["Name \"Jade Pickering\""],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1646138511,"objectID":"333bd911558427d19b406e8ed0c52ab8","permalink":"https://forrt.org/author/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/","section":"authors","summary":"","tags":null,"title":"","type":"authors"},{"authors":["Name \"James Bartlett\""],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1607689525,"objectID":"71f1f8aaf597e72fd806dc57521401a7","permalink":"https://forrt.org/author/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/","section":"authors","summary":"","tags":null,"title":"","type":"authors"},{"authors":["Name \"Jan Philipp Röer\""],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1646138511,"objectID":"948275ef2e4bc815e6b080e3b14670f1","permalink":"https://forrt.org/author/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/","section":"authors","summary":"","tags":null,"title":"","type":"authors"},{"authors":["Name \"Jeef Lees\""],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1646138511,"objectID":"eaaaecb926de2a887cc939c495aa9020","permalink":"https://forrt.org/author/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/","section":"authors","summary":"","tags":null,"title":"","type":"authors"},{"authors":["Name \"Joanne McCuaig\""],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1646138511,"objectID":"7948eefb45f5502430f64f1dedad3e6f","permalink":"https://forrt.org/author/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/","section":"authors","summary":"","tags":null,"title":"","type":"authors"},{"authors":["Name \"John Shaw\""],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1646138511,"objectID":"d93d0938207a86eb8f1e0af7ebc4e907","permalink":"https://forrt.org/author/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/","section":"authors","summary":"","tags":null,"title":"","type":"authors"},{"authors":["Jordan Wagge"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1697812962,"objectID":"889dd8785f26cf142b0a08f2cd73f554","permalink":"https://forrt.org/author/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/","section":"authors","summary":"","tags":null,"title":"","type":"authors"},{"authors":["Name \"Julia Pauquet\""],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1711535566,"objectID":"a9c7c79922984391a8fb454bd15fc30c","permalink":"https://forrt.org/author/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/","section":"authors","summary":"","tags":null,"title":"","type":"authors"},{"authors":["Name \"Julia Strand\""],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1608206399,"objectID":"7b5aa811934fc68ef5dee0ca41035c3f","permalink":"https://forrt.org/author/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/","section":"authors","summary":"","tags":null,"title":"","type":"authors"},{"authors":["Name \"Julia Wolska\""],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1674830952,"objectID":"053794ff706262793b3889612d0f237d","permalink":"https://forrt.org/author/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/","section":"authors","summary":"","tags":null,"title":"","type":"authors"},{"authors":["Name \"Kai Krautter\""],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1646138511,"objectID":"42a467bc620a3deacb5c3482d8a71ba6","permalink":"https://forrt.org/author/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/","section":"authors","summary":"","tags":null,"title":"","type":"authors"},{"authors":["Name \"Kai Li Chung\""],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1674830952,"objectID":"2de4194587ede0feb344adeddb37f829","permalink":"https://forrt.org/author/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/","section":"authors","summary":"","tags":null,"title":"","type":"authors"},{"authors":["Name \"Kamil Izydorczak\""],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1646138511,"objectID":"8c2ac39851f514daaeabc5b4103e5b46","permalink":"https://forrt.org/author/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/","section":"authors","summary":"","tags":null,"title":"","type":"authors"},{"authors":["Name \"Karen Matvienko-Sikar\""],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1646138511,"objectID":"3917722b00e81fb812667588a851e12f","permalink":"https://forrt.org/author/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/","section":"authors","summary":"","tags":null,"title":"","type":"authors"},{"authors":["Name \"Karolina Urbanska\""],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1657370446,"objectID":"c789a8f267d0c3ec86234ee6a4edec5c","permalink":"https://forrt.org/author/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/","section":"authors","summary":"","tags":null,"title":"","type":"authors"},{"authors":["Name \"Kelly Lloyd\""],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1631376665,"objectID":"da3907e06eba0307f7d42e4ec7060d70","permalink":"https://forrt.org/author/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/","section":"authors","summary":"","tags":null,"title":"","type":"authors"},{"authors":["Kimberly Quinn"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1592830477,"objectID":"1ef95c133c4d37e6a60c937ea02007e2","permalink":"https://forrt.org/author/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/","section":"authors","summary":"","tags":null,"title":"","type":"authors"},{"authors":["Name \"Laurel Standiford\""],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1592830477,"objectID":"8ddb13432120bf3c3e3e62643066bea4","permalink":"https://forrt.org/author/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/","section":"authors","summary":"","tags":null,"title":"","type":"authors"},{"authors":["Name \"Leticia Micheli\""],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1652896910,"objectID":"8b3eaf2ce5f2cb223ca0ea435fac245b","permalink":"https://forrt.org/author/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/","section":"authors","summary":"","tags":null,"title":"","type":"authors"},{"authors":["Name \"Louise Bezuidenhout\""],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1674830952,"objectID":"7cace082ee80d21cb713ff1f1d3c6043","permalink":"https://forrt.org/author/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/","section":"authors","summary":"","tags":null,"title":"","type":"authors"},{"authors":["Name \"Lydia Riedl\""],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1711630319,"objectID":"872ce4ea2bf171289031282df2f986f8","permalink":"https://forrt.org/author/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/","section":"authors","summary":"","tags":null,"title":"","type":"authors"},{"authors":["Name \"Maddi Pownall\""],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1646138511,"objectID":"316458ae94635aa6f2e6ec19f6de96b0","permalink":"https://forrt.org/author/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/","section":"authors","summary":"","tags":null,"title":"","type":"authors"},{"authors":["Magdalena Grose-Hodge"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1646138511,"objectID":"4b3464559fafdfff264dd5e077734eb0","permalink":"https://forrt.org/author/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/","section":"authors","summary":"","tags":null,"title":"","type":"authors"},{"authors":["Name \"Mahmoud Elsherif\""],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1646138511,"objectID":"ae79b28ff047d31dba700a7de14bbad3","permalink":"https://forrt.org/author/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/","section":"authors","summary":"","tags":null,"title":"","type":"authors"},{"authors":["Name \"Malika Ihle\""],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1592830477,"objectID":"8f2489908a38cdaaaca6f79cf116c4be","permalink":"https://forrt.org/author/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/","section":"authors","summary":"","tags":null,"title":"","type":"authors"},{"authors":["Name \"Markus Konkol\""],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1646138511,"objectID":"87a765cb158a9b2f7a106bb04ed948e3","permalink":"https://forrt.org/author/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/","section":"authors","summary":"","tags":null,"title":"","type":"authors"},{"authors":["Name \"Martin Vasilev\""],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1602086269,"objectID":"b4b92e7eb835c340c2a35f63fb031093","permalink":"https://forrt.org/author/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/","section":"authors","summary":"","tags":null,"title":"","type":"authors"},{"authors":["Name \"Mehdi Shaahdadi Goughari\""],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1674830952,"objectID":"e1aaed0ae401057b88bae4b0d6af5271","permalink":"https://forrt.org/author/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/","section":"authors","summary":"","tags":null,"title":"","type":"authors"},{"authors":["Name \"Meng Liu\""],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1646138511,"objectID":"b1f53567bf506aec200003bb5af95769","permalink":"https://forrt.org/author/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/","section":"authors","summary":"","tags":null,"title":"","type":"authors"},{"authors":["Mike Galang"],"categories":null,"content":" In a nutshell, FORRT is a Framework for Open and Reproducible Research Training. It aims to provide a pedagogical infrastructure designed to recognize and support the teaching and mentoring of open and reproducible science tenets in tandem with prototypical subject matters in higher education. FORRT strives to be an effective, evolving, and community-driven organization raising awareness of the pedagogical implications of open and reproducible science and its associated challenges (i.e., curricular reform, epistemological uncertainty, methods of education). FORRT also advocates for the opening of teaching and mentoring materials as a means to facilitate access, discovery, and learning to those who otherwise would be educationally disenfranchised.\nExpand to learn more The problem Recently, the scientific community took steps to reflect a widespread awareness of, and call for, improved practices ushering in the “credibility revolution” including higher standards of evidence, preregistration, direct replication, transparency, and openness. Structurally, three pillars were proposed to carry social sciences towards academic utopia: opening scientific communication, restructuring incentives and practices, and collaborative and crowdsourced science. However, ongoing attempts have neglected an essential aspect of the academic machinery: students. And indeed, current norms for the teaching and mentoring in higher-education are rooted in obsolete practices of bygone eras. Improving transparency and rigor of science is the responsibility of all who engage in it.\nThe solution In our view, a scientific utopia has a fourth pillar, whose principal goal is to familiarize students, who are future consumers of science and perhaps themselves knowledge producers, with the intricacies of the process of science. We believe the teaching and mentoring of reproducible and open research practices is the clearest indicator of the degree to which institutions and/or departments embody principles of credible science. This demonstration goes beyond paying lip service to best practices, and ensures that students are trained to engage in these practices. FORRT is designed by, and envisioned for, educators who wish to integrate typical discipline content with open and reproducible science tenets.\nFORRT supports this endeavor through a three-pronged approach:\nProviding to educators a comprehensive, straightforward, and accessible framework to qualify and quantify the degree of open and reproducible research practices in their teaching and mentoring. Through FORRT’s self-assessment tool, educators are supplied with a personalized pathway toward incremental integration of discipline content with open and reproducible science tenets.\nFORRT\u0026rsquo;s clusters\nFORRT’s self-assessment tool\nEquipping scholars with high quality pedagogical tools on open and reproducible research practices. Instructors can then adapt successful and implemented pedagogies to help ease the transition and reduce the instructor’s burden.\nFORRT’s Educational Nexus\nFORRT’s Pedagogies\nModifying the academic incentive structure regarding teaching and mentoring through recognition and commendations of excellent teaching and mentoring, which in turn are documentable and relevant to the researchers’ visibility, prestige, tenure and promotion reviews; foster social justice through the opening and democratization of scientific-educational resources to those who otherwise would be educationally disenfranchised; and building a community with existing educational initiatives and strengthening our missions, identify and reduce redundancies, and streamline the advancement of open educational practices.\nInstitutional Partnerships\nFORRT’s Manuscript\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1646138511,"objectID":"65df75a2cb3d9e6c4c48f85419bfd184","permalink":"https://forrt.org/author/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/","section":"authors","summary":"In a nutshell, FORRT is a Framework for Open and Reproducible Research Training. It aims to provide a pedagogical infrastructure designed to recognize and support the teaching and mentoring of open and reproducible science tenets in tandem with prototypical subject matters in higher education. FORRT strives to be an effective, evolving, and community-driven organization raising awareness of the pedagogical implications of open and reproducible science and its associated challenges (i.","tags":null,"title":"","type":"authors"},{"authors":["Monica Gonzales-Marquez"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1646138511,"objectID":"7ac22620cc65fe1540cad9fcddc2c0c4","permalink":"https://forrt.org/author/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/","section":"authors","summary":"","tags":null,"title":"","type":"authors"},{"authors":["Name \"Myriam Baum\""],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1646138511,"objectID":"56eaec6866d88a77df551de2687d0fa2","permalink":"https://forrt.org/author/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/","section":"authors","summary":"","tags":null,"title":"","type":"authors"},{"authors":["Name \"Nihan Albayrak-Aydemir\""],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1646138511,"objectID":"0576a3d0b253f443195246cd748e7382","permalink":"https://forrt.org/author/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/","section":"authors","summary":"","tags":null,"title":"","type":"authors"},{"authors":["Name \"Olly Robertson\""],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1646138511,"objectID":"ac083830df7d0f5b67bbec9671a43530","permalink":"https://forrt.org/author/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/","section":"authors","summary":"","tags":null,"title":"","type":"authors"},{"authors":["Name \"Patrick Forscher\""],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1646138511,"objectID":"78b041cb157be7985315dddc9f0c0878","permalink":"https://forrt.org/author/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/","section":"authors","summary":"","tags":null,"title":"","type":"authors"},{"authors":["Name \"Priya Silverstein\""],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1662126710,"objectID":"9b1897125e9056db8e948de0742a1648","permalink":"https://forrt.org/author/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/","section":"authors","summary":"","tags":null,"title":"","type":"authors"},{"authors":["Name \"Rachel Renbarger\""],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1646138511,"objectID":"6d1e2aff1f63dafdd663f20c6ae4e9f4","permalink":"https://forrt.org/author/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/","section":"authors","summary":"","tags":null,"title":"","type":"authors"},{"authors":["Name \"richard-dushime\""],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1721575190,"objectID":"acc6528ad1cea47209d19c10dc4464f4","permalink":"https://forrt.org/author/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/","section":"authors","summary":"","tags":null,"title":"","type":"authors"},{"authors":["Name \"Sam Parsons\""],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1652896910,"objectID":"9a8dc15bf97e34891b6e76521cdf60e6","permalink":"https://forrt.org/author/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/","section":"authors","summary":"","tags":null,"title":"","type":"authors"},{"authors":["Name \"Sam Westwood\""],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1646138511,"objectID":"789917a5d0b23056c5125f41ad5f4af7","permalink":"https://forrt.org/author/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/","section":"authors","summary":"","tags":null,"title":"","type":"authors"},{"authors":["Name \"Samantha Tyler\""],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1646138511,"objectID":"25cdaad7fba49018ef0bb2d79f1befb0","permalink":"https://forrt.org/author/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/","section":"authors","summary":"","tags":null,"title":"","type":"authors"},{"authors":["Name \"Samuel Alarie\""],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1660659319,"objectID":"c9fdc3a1d1084bb8de2108dede3c15ef","permalink":"https://forrt.org/author/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/","section":"authors","summary":"","tags":null,"title":"","type":"authors"},{"authors":["Name \"Samuel Guay\""],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1592830477,"objectID":"8e94cdd946306a1ac935760708908565","permalink":"https://forrt.org/author/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/","section":"authors","summary":"","tags":null,"title":"","type":"authors"},{"authors":["Name \"Sandra Grinschgl\""],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1674830952,"objectID":"1526cab110fd7693b976e095ba0746f3","permalink":"https://forrt.org/author/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/","section":"authors","summary":"","tags":null,"title":"","type":"authors"},{"authors":["Name \"Sara Middleton\""],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1646138511,"objectID":"73188d7bb89021b48cf37fe5224adff3","permalink":"https://forrt.org/author/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/","section":"authors","summary":"","tags":null,"title":"","type":"authors"},{"authors":["Name \"Sau-Chin Chen\""],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1646138511,"objectID":"52ca05680b3e5676b73be3cc355c2d50","permalink":"https://forrt.org/author/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/","section":"authors","summary":"","tags":null,"title":"","type":"authors"},{"authors":["Name \"Shilaan Alzahawi\""],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1646138511,"objectID":"44a237d78806e17fb00f9a29d0854495","permalink":"https://forrt.org/author/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/","section":"authors","summary":"","tags":null,"title":"","type":"authors"},{"authors":["Name \"Siu Kit Yeung\""],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1646138511,"objectID":"8889b0c3f2bc46bfac3e6c6cf0ca09ce","permalink":"https://forrt.org/author/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/","section":"authors","summary":"","tags":null,"title":"","type":"authors"},{"authors":["Name \"Sonia Rishi\""],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1611577969,"objectID":"b7992ab2e57aa3695d40273c00efee90","permalink":"https://forrt.org/author/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/","section":"authors","summary":"","tags":null,"title":"","type":"authors"},{"authors":["Name \"Susanne Vogel\""],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1674830952,"objectID":"99185cc2d0cba6376526d62b2a12d45b","permalink":"https://forrt.org/author/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/","section":"authors","summary":"","tags":null,"title":"","type":"authors"},{"authors":["Name \"Tamara Kalandadze\""],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1646138511,"objectID":"b5c20b5740e187a5186880ba3768c853","permalink":"https://forrt.org/author/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/","section":"authors","summary":"","tags":null,"title":"","type":"authors"},{"authors":["Name \"Tamara Marques\""],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1607254637,"objectID":"74adca373cf3eb63b1f85458423c1870","permalink":"https://forrt.org/author/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/","section":"authors","summary":"","tags":null,"title":"","type":"authors"},{"authors":["Name \"Thomas Rhys Evans\""],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1646138511,"objectID":"86dffa59c3ef639b5700182ccc160c9f","permalink":"https://forrt.org/author/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/","section":"authors","summary":"","tags":null,"title":"","type":"authors"},{"authors":["Name \"Timo B. Roettger\""],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1674830952,"objectID":"0d6ccc7a4b6ab159d9d3e3a184451dca","permalink":"https://forrt.org/author/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/","section":"authors","summary":"","tags":null,"title":"","type":"authors"},{"authors":["Name \"Tina Lonsdorf\""],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1646138511,"objectID":"80d42cfce049747a7004714c3c029610","permalink":"https://forrt.org/author/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/","section":"authors","summary":"","tags":null,"title":"","type":"authors"},{"authors":["Name \"Waleed Alsubhi\""],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1674830952,"objectID":"b35dee9de5da53b04a8134b336b9f0cc","permalink":"https://forrt.org/author/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/","section":"authors","summary":"","tags":null,"title":"","type":"authors"},{"authors":["Name \"William Ngiam\""],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1646138511,"objectID":"84e21eaa9ff44e34407fa2158608a21a","permalink":"https://forrt.org/author/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/","section":"authors","summary":"","tags":null,"title":"","type":"authors"},{"authors":["Name \"Yanna Weisberg\""],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1646138511,"objectID":"d9d7278360cd253d7d3308be08a98472","permalink":"https://forrt.org/author/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/","section":"authors","summary":"","tags":null,"title":"","type":"authors"},{"authors":["Name \"Yu-Fang Yang\""],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1674830952,"objectID":"4c3bb799474a7b1918f0527e5dcfabef","permalink":"https://forrt.org/author/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/","section":"authors","summary":"","tags":null,"title":"","type":"authors"},{"authors":["Name \"Zoe Flack\""],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1654106059,"objectID":"0645ad28e8ca285044db8ebdeb03e4ea","permalink":"https://forrt.org/author/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/","section":"authors","summary":"","tags":null,"title":"","type":"authors"},{"authors":["Name \"Zoran Pavlović\""],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1674830952,"objectID":"35ae2d73299e8173e7c525b90b9c0962","permalink":"https://forrt.org/author/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/","section":"authors","summary":"","tags":null,"title":"","type":"authors"},{"authors":["forrt"],"categories":null,"content":" In a nutshell, FORRT is a Framework for Open and Reproducible Research Training advancing research transparency, reproducibility, rigor, and ethics through pedagogical reform and meta-scientific research. FORRT provides a pedagogical infrastructure \u0026amp; didactic resources designed to recognize and support the teaching and mentoring of open and reproducible science. FORRT strives to raise awareness of the pedagogical implications of open and reproducible science and its associated challenges (i.e., curricular reform, epistemological uncertainty, methods of education) and advocates for the opening and formal recognition of teaching and mentoring materials to facilitate access, discovery, and learning to those who otherwise would be educationally disenfranchised.\nExpand to learn more Presentation of FORRT projects The problem Recently, the scientific community took steps to reflect a widespread awareness of, and call for, improved practices ushering in the “credibility revolution” including higher standards of evidence, preregistration, direct replication, transparency, and openness. Structurally, three pillars were proposed to carry social sciences towards academic utopia: opening scientific communication, restructuring incentives and practices, and collaborative and crowdsourced science. However, ongoing attempts have neglected an essential aspect of the academic machinery: students. And indeed, current norms for the teaching and mentoring in higher-education are rooted in obsolete practices of bygone eras. Improving transparency and rigor of science is the responsibility of all who engage in it.\nThe solution In our view, a scientific utopia has a fourth pillar, whose principal goal is to familiarize higher-ED students, who are future consumers of science and perhaps themselves knowledge producers, with the intricacies of the process of science. We believe the teaching and mentoring of reproducible and open research practices is the clearest indicator of the degree to which institutions and/or departments embody principles of credible science. This demonstration goes beyond paying lip service to best practices, and ensures that students are trained to engage in the best available scientific practices. FORRT is designed by, and envisioned for, educators who wish to integrate typical discipline content with open and reproducible science tenets.\nFORRT supports this endeavor through a three-pronged approach:\nProvide educators a comprehensive but straightforward framework to teach and mentor open and reproducible science as well as qualify and quantify the degree of open and reproducible research practices in their teaching and mentoring. FORRT\u0026rsquo;s clusters FORRT’s self-assessment tool\nEquipping educators with high quality pedagogical tools on open and reproducible science practices to facilitate its integration into curricula and educators’ teaching mentoring, and research practices. FORRT’s Educational Nexus\nFORRT’s Pedagogies\nModifying the academic incentive structure regarding teaching and mentoring through recognition and commendations of excellent teaching and mentoring; foster social justice through the opening and democratization of scientific-educational resources to those who otherwise would be educationally disenfranchised; and building a community with existing educational initiatives and strengthening our missions, identify and reduce redundancies, and streamline the advancement of open educational practices. FORRT’s Toward Social Justice in Academia initiatives\nFORRT\u0026rsquo;s Neurodiversity Team\nFORRT’s Educators\u0026rsquo; Corner\nFORRT’s Publications\nFORRT’s Institutional Partnerships\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1711900114,"objectID":"e16f2d6dbb0f0fbb9e796cdd09e35def","permalink":"https://forrt.org/author/forrt/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/forrt/","section":"authors","summary":"In a nutshell, FORRT is a Framework for Open and Reproducible Research Training advancing research transparency, reproducibility, rigor, and ethics through pedagogical reform and meta-scientific research. FORRT provides a pedagogical infrastructure \u0026amp; didactic resources designed to recognize and support the teaching and mentoring of open and reproducible science. FORRT strives to raise awareness of the pedagogical implications of open and reproducible science and its associated challenges (i.","tags":null,"title":"FORRT","type":"authors"},{"authors":null,"categories":null,"content":"100+ Summaries of Open and Reproducible Science Literature Overview The FORRT community has prepared 100+ summaries of Open and Reproducible Science literature. The purpose of these summaries is to reduce some of the burden on educators looking to incorporate open and reproducible research principles into their teaching as well as facilitate the edification of anyone wishing to learn or disseminate open and reproducible science tenets.\nThese summaries are very much a work in progress. We would love to receive your criticism, areas for improvement, ideas, and help.\nYou can find the summaries via the menu in the left. We made a distinction between \u0026ldquo; Open and Reproducible Science\u0026rdquo; summaries and \u0026ldquo; Diversity, Equity, \u0026amp; Inclusion\u0026rdquo; summaries to highlight that the topics of social injustices and DEI (diversity, equity and inclusion) are often neglected in academia, and in open and reproducible science literature. We have also prepared a .pdf version (coming soon!) in case you want to keep a copy for yourself. If you are an educator, you may also be interested in our FORRT Syllabus on Open and Reproducible Science (.pdf \u0026amp; G-doc), which is based on FORRT Clusters.\nWhy make these summaries? The FORRT\u0026rsquo;s summaries is community-curated resource aims to satisfy three of FORRT’s Goals:\nSupport scholars in their efforts to learn and stay up-to-date on best practices regarding open and reproducible research; Facilitating conversations about the ethics and social impact of teaching substantive topics with due regard to scientific openness, epistemic uncertainty and the credibility revolution; Foster social justice through the democratization of scientific educational resources and its pedagogies. and four of FORRT’s Mission:\nDismantling hierarchies surrounding research, teaching, and service; Building community among educators and various non-academic communities working to improve scientific communication and literacy across academia and the general public; Building capacity for advocacy; and Advocacy for the creation and maintenance of educational resources. Future plans Our community will continue to curate and expand on these summaries. Our goal is to keep integrating community feedback and its multitude of perspectives hoping to produce ever more useful summaries. We also would like community feedback on how to best classify the summaries into useful and didactic categories and distill its knowledge (at this stage, we also intend to making the methodology used transparent).\nOne idea to achieve these goals would be to plan a hackaton to make these summaries better to the community. Perhaps Open and Reproducible Science organizations (e.g., International Network of Open Science \u0026amp; Scholarship Communities (INOSC): OSCU, OSCNL, OSCA, OSCN, OSCR, OSC/e, OSCT, OSCG but also national organizations UKRN and NOSI) could help us in this process.\nWe also aim to further integrate this initiative with other FORRT initiatives. First, we want to make sure that all open and reproducible sources cited in the FORRT’s Manuscript have an associated summary to facilitate the reading of FORRT’s manifesto. The same goes for the FORRT’s self-assessment tool where educators evaluate their teaching and/or mentoring \u0026ndash;i.e., the extent to which their teaching abides by open and reproducible research practices\u0026ndash; and receive personalized feedback (on topics they requested). Finally, we also hope to include each summary as a resource in FORRT’s curated list of resources, as well include FORRT Clusters’ tags for each summary. Not yet at a \u0026lsquo;plans\u0026rsquo; stage, but we are considering formalizing this initiative with the drafting of a manuscript, summarizing lessons we learned and hopefully making it a more useful resource to the community.\nProcess \u0026amp; Credit Any and all contributions in FORRT are formally recognized. In large part, these summaries are a result of the heroic effort of Mahmoud Elsherif, a cherished member of FORRT community, who bore the brunt of most of the initial work. Flavio Azevedo conceptualized the project, reviewed summaries, and managed the initiative. This project would not have been possible without the tireless work of Leticia Micheli, Martin Vasilev, Jacob Miranda, Tamara Marques, Esther Plomp, Alice Rees, and Catia Oliveira who revised and improved the summaries to its current stage. Furthermore, members of CSCCE community have done a fair bit of reviewing of the DEI summaries. We would like to thank CSCCE for editing, commenting, and enriching it. These summaries are truly a community effort in the name of supporting scholars in their efforts to learn, stay up-to-date and teach open and reproducible science in Higher Education.\nFeedback Do you have feedback to give us? We would love to receive your criticism, areas for improvement, ideas, and help. You can also write to us at info@forrt.org or at Twitter, or join our community.\nJoin and help us Help us make better education for everyone, join our community. Help us support educators and elevate their efforts. Help us facilitate the process of integrating open and reproducible science into the pipeline of all teachers and mentors in Higher-Ed. Help us advocate for open educational resources and expedite access to open and reproducible science trainings. Help us spread the word by sharing this FORRT resource via your professional listservs and other social media.\n","date":1536451200,"expirydate":-62135596800,"kind":"section","lang":"en","lastmod":1633800216,"objectID":"ddb4f31a42f60e3e344d7aeb260101ab","permalink":"https://forrt.org/summaries/","publishdate":"2018-09-09T00:00:00Z","relpermalink":"/summaries/","section":"summaries","summary":"The FORRT community has prepared 100+ summaries of Open and Reproducible Science literature. The purpose of these summaries is to reduce some of the burden on educators looking to incorporate open and reproducible research principles into their teaching as well as facilitate the edification of anyone wishing to learn or disseminate open and reproducible science tenets.","tags":null,"title":"FORRT Summaries","type":"docs"},{"authors":null,"categories":null,"content":" The symbol ◈ stands for non-peer-reviewed work.\nThe symbol ⌺ stands for summaries on the topic of Diversity, Equity, and Inclusion.\nTrust Your Science? Open Your Data and Code (Stodden, 2011)◈ Main Takeaways: Computational results suffer from problems of errors in final published conclusions. In order to allow independent replication and reproducible work, release the scripts and data files, and if the researcher uses MATLAB for graphs etc, please provide the graphical user interface. The standards for code quality are more precise definitions of verification, validation, and error quantification in scientific computing. Research workflow involves changes made to data, including analysis, that affects data interpretation. To conclude, open data is a prerequisite for verifiable research. Quote “Science has never been about open data per se, but openness is something hard fought and won in the context of reproducibility” (p. 22). Abstract This is a view on the reproducibility of computational sciences by Victoria Stodden. It contains information on the Reproducibility, Replicability, and Repeatability of code created by the other sciences. Stodden also talks about the rising prominence of computational sciences as we are in the digital age and what that means for the future of science and collecting data. APA Style Reference Stodden, V. C. (2011). Trust your science? Open your data and code. https://doi.org/10.7916/D8CJ8Q0P\nYou may also be interested in Attitudes Toward Open Science and Public Data Sharing: A Survey Among Members of the German Psychological Society (Abele-Brehm et al., 2019) Willingness to Share Research Data Is Related to the Strength of the Evidence and the Quality of Reporting of Statistical Results (Wicherts et al., 2011) Open Data in Qualitative Research (Chauvette et al., 2019) CJEP Will Offer Open Science Badges (Pexman, 2017) Badges to Acknowledge Open Practices: A Simple, Low-Cost, Effective Method for Increasing Transparency (Kidwell et al., 2016) Only Human: Scientists, Systems, and Suspect Statistics A review of: Improving Scientific Practice: Dealing With The Human Factors, University of Amsterdam, Amsterdam, September 11, 2014 (Hardwicke et al., 2014) Rein in the four horsemen of irreproducibility (Bishop, 2019) Seven Easy Steps to Open Science: An Annotated Reading List (Crüwell et al., 2019) Seven Steps Toward Transparency and Replicability in Psychological Science (Lindsay, 2020) Using OSF to Share Data: A Step-by-Step Guide (Soderberg, 2018) The digital Archaeologists (Perkel, 2020) Registered reports: a method to increase the credibility of published results (Nosek \u0026amp; Lakens, 2014) Main Takeaways: This editorial discusses the value of pre-registration and replication, as not all articles are published. Direct replication adds data that increases the precision of effect size estimates for meta-analytic research. No direct replication, means it is difficult to identify false positives. Conceptual replications are more popular than direct replications, as the former conceptualises a phenomenon from its original operationalisation, thus contributing to our theoretical understanding of the effect. Direct replication encourages generalisability of effects, providing evidence that the effect was not due to sampling, procedural or contextual error. If direct replication produces negative results, this improves the identification of boundary conditions for real effects. The benefit of a registered report is that the feedback provided from peer review allows researchers to improve their experimental design. Following peer review, the manuscript can be resubmitted for review and acceptance or rejection based on feedback. Successful proposals tend to be high-powered, high quality, and faithful replication designs. One benefit of a registered report is that this can be all done before the research is conducted. Peer reviewers will focus on the methodological quality of the research, allowing conflict of interests to be reduced and peer reviewers can provide a fair assessment of the manuscript. The original studies can provide an exaggerated effect size. When this study is replicated, the effect size usually decreases as a result of a larger sample size. Registered reports enable exploratory and confirmatory analyses, but a distinction is required. However, more trust can be placed in confirmatory analyses, as it follows a plan and ensures the interpretability of reported p value. Quote “No single replication provides the definitive word for or against the reality of an effect, just as no original study provides definitive evidence for it. Original and replication research each provides a piece of accumulating evidence for understanding an effect and the conditions necessary to obtain it. Following this special issue, Social Psychology will publish some commentaries and responses by original and replication authors of their reflections on the inferences from the accumulated data, and questions that could be addressed in follow-up research.” (p. 139) Abstract Professor Daniel Laken and Professor Brian Nosek provide an editorial on how pre-registration and registered reports can be used for the journal of Social Psychology in order to increase the credibility of individual results and findings. APA Style Reference Nosek, B. A., \u0026amp; Lakens, D. (2014). Registered reports : a method to increase the credibility of published results. Social Psychology, 45(3), 137-141. https://doi.org/10.1027/1864-9335/a000192\nYou may also be interested in Registered Reports: A new publishing initiative at Cortex (Chambers, 2013) Registered Reports: A step change in scientific publishing (Chambers, 2014) Registered Reports: Realigning incentives in scientific publishing (Chambers et al., 2015) Registered reports (Jamieson et al., 2019) Rein in the four horsemen of irreproducibility (Bishop, 2019) Seven Easy Steps to Open Science: An Annotated Reading List (Crüwell et al., 2019) Seven Steps Toward Transparency and Replicability in Psychological Science (Lindsay, 2020) On the persistence of low power in psychological science (Vankov et al., 2014) Main Takeaways: Abstract APA Style Reference You may also be interested in Education and Socio-economic status (APA, 2017b) ◈⌺ Main Takeaways: Children from low socio-economic status take longer to develop academic skills than children from higher socio-economic status groups (e.g. poor cognitive development), leading to poorer academic achievement. Children from low socio-economic status are less likely to attain experiences for the development of reading acquisition and reading competence. As a result of fewer learning materials and experiences at home, children from low socio-economic status enter high school with literacy skills 5 years behind their affluent age-matched peers. Children from lower socio-economic status households are twice as likely as those from high SES households to show learning related behaviour problems. High school dropout rate was evident in low-income families compared to high-income families. Placing low-socio-economic status students in higher-quality classrooms will help them earn more disposable income, more likely to attend college, live in affluent neighbourhoods and save more income for retirement. Students from low socio-economic status are less likely to have access to resources about colleges (e.g. career offices and familial experience with higher/further education) and are more at-risk of being in debt to student loans than their affluent peers. Low income students are less likely to succeed in STEM disciplines, 8 times less likely to obtain a bachelor’s degree by the age of 24 and have less career-related self-efficacy when it came to vocational aspirations than high income students. These problems are worsened for people of colour, women, people who are disabled and LGBTIQ-identified individuals. Abstract This fact sheet explains the impact socioeconomic status on educational outcomes. APA Style Reference APA (2017, July). Education and Socioeconomic Status [Blog post]. Retrieved from https://www.apa.org/pi/ses/resources/publications/education\nYou may also be interested in ls There a Positive Correlation between Socioeconomic Status and Academic Achievement? (Quagliata, 2008) Ethnic and Racial minorities and socio-economic status (APA, 2017) Women and Socio-economic status (APA, 2010) Disability and Socio-economic status (APA, 2010) Lesbian, Gay, Bisexual and Transgender Persons \u0026amp; Socioeconomic Status (APA, 2010) Ethnic and Racial minorities and socio-economic status (APA, 2017) ◈ ⌺ Main Takeaways: The relationship between SES, race and ethnicity is intimately intertwined. Communities are segregated by socio-economic status, race and ethnicity. Low economic development, poor health conditions and low levels of educational attainment are often comorbidities shared in these communities. Discrimination hinders social mobility of ethnic and racial minorities. In the US, 39% of African American children and adolescents, and 33% of Latino children and adolescents are living in poverty, which is more than double than the 14% poverty rate for non-Latino, White and Asian children and adolescents. Minority racial groups are more likely to experience multidimensional poverty than their White counterparts. American Indian/Alaska Native, Hispanic, Pacific Islander, and Native Hawaiian families are more likely than Caucasian and Asian families to live in poverty. “African Americans (53%) and Latinos (43%) are more likely to receive high-cost mortgages than Caucasians (18%; Logan, 2008).” (p.9). African American unemployment rates are double of Caucasian Americans. African American men working full time earn only 72% of Caucasian men\u0026#39;s average earnings, and 85% of earnings of Caucasian women. African Americans and Latinos are more likely to attend high-poverty schools than Asian Americans and Caucasians. From 2000 to 2013, dropout rates between racial groups narrowed significantly. High school dropouts were highest for Latinos, followed by African Americans and Whites. High achieving African American students may be exposed to less rigorous curriculums, attend schools with fewer resources, and have teachers who expect less of them academically than similarly situated Caucasian students. 12% of African American college graduates were unemployed, which is more than double the rate of unemployment among all college graduates in the same age range. Racial and ethnic minorities have worse health than that of White Americans. Health disparities stem from economic determinants, education, geography, neighbourhood, environment, lower-quality care, inadequate access to care, inability to navigate the system, provider ignorance or bias, and stress. “At each level of income or education, African American have worse outcomes than Whites. This could be due to adverse health effects of more concentrated disadvantage or a range of experiences related to racial bias (Braveman, Cubbin, Egerter, Williams, \u0026amp; Pamuk, 2010).” (p.10). In pre-retirement years, Hispanics and American Indians are much less likely than Whites, African Americans, and Asians to have any health insurance. Negative net worth, zero net worth, and not owning a home in young adulthood are linked to depressive symptoms independent of other socio-economic indicators. Hispanics and African Americans report a lower risk of psychiatric disorder relative to White counterparts, but those who become ill tend to have more persistent disorders. African Americans, Hispanics, Asians, American Indians, and Native Hawaiians have higher rates of post-traumatic stress disorders than Whites, which is not explained by Socio-economic status and a history of psychiatric disorders. However discrimination is factor that contributes to increasing mental health disorders among the Asian and African American communities (i.e., compared to the White community, African American communities are more frequently diagnosed with schizophrenia, a low prevalence but serious condition). Abstract Learn how socioeconomic status affects the lives of many racial and ethnic minorities. APA Style Reference APA (2017, July). Ethnic and Racial Minorities \u0026amp; Socioeconomic Status [Blog post]. Retrieved from https://www.apa.org/pi/ses/resources/publications/minorities\nYou may also be interested in ls There a Positive Correlation between Socioeconomic Status and Academic Achievement? (Quagliata, 2008) Education and Socio-economic status (APA, 2017b) Women and Socio-economic status (APA, 2010) Disability and Socio-economic status (APA, 2010) Lesbian, Gay, Bisexual and Transgender Persons \u0026amp; Socioeconomic Status (APA, 2010) Faculty promotion must assess reproducibility (Flier, 2017) ⌺ Main Takeaways: Inadequate training, increased competition, problems in peer review and publishing, and occasionally scientific misconduct are some of the variables behind irreproducible research in the biomedical field. Diverse causes make finding solutions for the problem of irreproducibility difficult, especially, as they must be implemented by independent constituencies including funders and publishers. Academic institutions can and must do better to make science more reliable. One of the most effective (but least discussed) measures is to change how we appoint and promote our faculty members. Promotion criteria has changed over time. Committees now consider how well a candidate participates in team science, but we still depend on imperfect metrics for judging research publications and our ability to assess reliability and accuracy is underdeveloped. Reproducibility and robustness are under-emphasised when job applicants are evaluated and when faculty members are promoted. Currently, reviewers of committees are asked to assess how a field would be different without a candidate’s contributions, and to survey a candidate’s accomplishment, scholarship, and recognition. The promotion process should also encourage evaluators to say whether they feel candidates’ work is problematic or over-stated and whether it has been reproduced and broadly accepted. If not, they should say whether they believe widespread reproducibility is likely or whether work will advance the field. Applicants should also be asked to critically evaluate their research, including unanswered questions, controversies and uncertainties. This signals the importance of assessment and creates a mechanism to judge a candidate’s capacity for critical self-reflection. Evaluators should be asked to consider how technical and statistical issues were handled by candidates. Research and discovery is not simple and unidirectional, and evaluators should be sceptical of candidates who oversimplify. Institutions need to incentivise data sharing and transparency. Efforts are more urgent as increasingly interdisciplinary projects extend beyond individual investigators’ expertise. Success will need creativity, pragmatism and diplomacy, because investigators bristle at any perceived imposition on their academic freedom. Quote “Over time, efforts to increase the ratio of self-reflection to self-promotion may be the best way to improve science. It will be a slog, but if we don’t take this on, formally and explicitly, nothing will change.” (p.133) Abstract Research institutions should explicitly seek job candidates who can be frankly self-critical of their work, says Jeffrey Flier. APA Style Reference Flier J. (2017) Faculty promotion must assess reproducibility. Nature, 549(7671),133. https://doi.org/10.1038/549133a\nYou may also be interested in Publication metrics and success on the academic job market (Van Dijk et al., 2014) Six principles for assessing scientists for hiring, promotion, and tenure (Naudet et al, 2018) Women and Socio-economic status (APA, 2010)◈ ⌺ Main Takeaways: Socioeconomic status encompasses quality of life attributes and opportunities and privileges afforded to people in society. Socio-economic status is a consistent and reliable predictor of outcomes across lifespan. Low socio-economic status and its correlates (e.g., lower educational achievement, poverty and poor health) affect society. Inequities in health distribution, resource distribution and quality of life are increasing in the US and globally. Socio-economic status is a key factor in determining the quality of life for women and, by extension, strongly affects the lives of children and families. Inequities in wealth and quality of life for women are long-standing and exist both locally and globally. Women are more likely to live in poverty than men. Men are paid more than women despite similar levels of education and fields of occupation. Reduced income for women coupled with longer life expectancy and increased responsibility to raise children, increase probabilities that women face economic disadvantages. Pay gap has narrowed over time but recently the progress has plateaued. Women with a high school diploma are paid 80% of what men with the same qualifications are paid. Single mother families are more than 5 times as likely to live in poverty as married-couples families. Pregnancy affects work and educational opportunities for women and costs associated with pregnancy are higher for women than men. 46% of women believed they have experienced gender discrimination. Pregnant women with low socio-economic status report more depressive symptoms, suggesting the third trimester may be more stressful for low-income women. At 2 and 3 months postpartum, women with low income have been found to experience more depressive symptoms than women with high-income. Women with insecure and low-status jobs with little to no decision-making authority experience higher-levels of negative life events, insecure housing tenure, more chronic stressors, and reduced social support. Depression and anxiety have increased significantly for poor women in developing countries undergoing restructuring. Women with low income develop alcoholism and drug addiction influenced by social stressors linked to poverty. Improved balance in gender roles, obligations, pay equity, poverty reduction and renewed attention to maintenance of social capital redress the gender disparities in mental health. SES also affects physical health, with women living with breast cancer being11% more likely to die if they live in lower SES communities. Low-income women who have no insurance have lowest rates of mammography screening among women aged 40-64, increasing risk of death from breast cancer. Obesity and staying obese from adolescence to young adulthood is linked to poverty among women. Relative to HIV-positive men, women with HIV have disproportionately low-income in the US. Abstract Learn how socioeconomic status affects the lives of women. APA Style Reference APA. (2017, July). Women \u0026amp; Socioeconomic Status [Blog post]. Retrieved from https://www.apa.org/pi/ses/resources/publications/women You may also be interested in ls There a Positive Correlation between Socioeconomic Status and Academic Achievement? (Quagliata, 2008) Education and Socio-economic status (APA, 2017b) Disability and Socio-economic status (APA, 2010) Ethnic and Racial minorities and socio-economic status (APA, 2017) Lesbian, Gay, Bisexual and Transgender Persons \u0026amp; Socioeconomic Status (APA, 2010) The Gender Gap: Who Is (and Is Not) Included on Graduate-Level Syllabi in Social/Personality Psychology (Skitka et al., 2020) ⌺ Main Takeaways: This article investigates whether there is a gender gap in Social/Personality Psychology syllabi. One factor contributing to gender gaps is whose work we choose to teach in graduate seminars. It is hypothesised that one link in the broad chain of factors contributing to the eminent gender gap is that female authors are likely to be under-represented on graduate course syllabi compared to their male peers (gender gap hypothesis). Reasons why female authors might be under-represented on course syllabi could be varied. Instructors may internalise cultural prejudices and biases favouring men over women. This might result in a greater preference for male over female-authored papers (i.e., bias hypothesis). Another possibility is that instructors might prefer older over contemporary papers (i.e., classic hypothesis). Yet another possibility is that there are more male-authored papers available to include in syllabi than female-authored papers (i.e., availability hypothesis). The present study investigates whether there is a gender gap in representation on graduate level syllabi and whether it is explained by preference for classic over contemporary papers or relative availability of male- versus female-authored manuscripts. Method: The authors identified every social and/or personality PhD program in the US using the Social Psychology Network’s PhD ranking list and Graduate Programs GeoSearch. 120 programs were identified and a list of social/personality faculty names and email addresses for each program were put together by going to psychology department websites. Main interest was in courses for first-year graduate students. Inclusion criteria for syllabi were: (1) course name includes words: social or personality, (2) course was at the graduate level. Papers cited in the syllabi were coded for the following characteristics: gender of all authors, each author’s h-index, total number of authors, journal where the article was published, number of citations the article received since publication and topic in social/personality psychology. To understand whether the gender representation on graduate syllabi is (or is not) consistent with the number of high-quality papers from which instructors can select, the present study obtained all names of authors, authorship order and year of publication for all papers published in the Journal of Social and Personality Psychology from 1965 to 2017 and published in the Personality and Social Psychology Bulletin from 1974 until April 2018. These journals accounted for 33% of reading on sample course syllabi and formed benchmarks. Results: Less than 30% of papers referenced on syllabi were written by female first authors. The gender gap on syllabi, differed as a function of instructor gender and decade papers were published: female instructors assigned more recently published papers (post-1990) and female first-authored papers at levels significantly higher than their male counterparts. Difference in inclusion rates of female first-authored paper could not be explained by preference for classic over contemporary papers in syllabi or relative availability of female first-authored papers in the published literature. The gender gap differed depending on the content of the course. Male and female authors were approximately equally represented on graduate-level syllabi of topics as prejudice, close relationships, culture and health. The gender gap was much larger in syllabi of topics as best practices, replicability, attitude change and persuasion. Male and female-authored papers included on syllabi had similar citation rates, although they had different h-index scores. Increasing representation of female scholars’ work on graduate course syllabi would have beneficial consequences, moving toward greater gender inclusiveness in social/personality psychology. Abstract We contacted a random sample of social/personality psychologists in the United States and asked for copies of their graduate syllabi. We coded more than 3,400 papers referenced on these syllabi for gender of authors as well as other characteristics. Less than 30% of the papers referenced on these syllabi were written by female first authors, with no evidence of a trend toward greater inclusion of papers published by female first authors since the 1980s. The difference in inclusion rates of female first-authored papers could not be explained by a preference for including classic over contemporary papers in syllabi (there was evidence of a recency bias instead) or the relative availability of female first-authored papers in the published literature. Implications are discussed. APA Style Reference Skitka, L. J., Melton, Z. J., Mueller, A. B., \u0026amp; Wei, K. Y. (2020). The Gender Gap: Who Is (and Is Not) Included on Graduate-Level Syllabi in Social/Personality Psychology. Personality and Social Psychology Bulletin, 0146167220947326. https://doi.org/10.1177/0146167220947326\nYou may also be interested in Unequal effects of the COVID-19 pandemic on scientists (Myers et al., 2019) Against Eminence (Vazire, 2017) Scientific Eminence: Where Are the Women? (Eagly \u0026amp; Miller, 2016) Leveraging a collaborative consortium model of mentee/mentor training to foster career progression of under-represented post-doctoral researchers and promote institutional diversity and inclusion (Risner et al., 2020) ⌺ Main Takeaways: The goal of the study is to empower post-doctoral students and make them active participants in the mentoring relationships by emphasising the mentees’ contributions in shaping more productive interactions to be built upon to develop their own skills as a future mentor. The study used several metrics by which they assessed the success of this collaborative, multi-institutional approach, using National Research Mentoring Network (NRMN), the Committee on Institutional Cooperation Academic Network (CAN) approach to provide mentor facilitator training for faculty and senior administrators and mentoring-Up training for post-doctoral students. Background: “Establishing a functioning consortium needs buy-in and high-level cooperation from all partners. Prior to initiating programming, all potential institutional representatives set initial goals to address campus needs for mentor-up skill development for post-docs and mentor facilitator training for staff and faculty, establish sustainable communities of practice for mentor training and develop mechanisms for central coordination, outreach to campus constituents, templates for recruitment of participants and strategies to sustain collaboration and develop mechanisms for central coordination, outreach to campus constituents, templates for recruitment of participants, and strategies to sustain collaboration.” (p.4) Method: “The seven Core Principles [of “Mentoring-UP”] are: 1. Two-way communication, 2. Aligning expectations, 3. Assessing understanding, 4. Fostering independence, 5. Ethics, 6. Addressing equity and inclusion, 7. Promoting professional development. This curriculum provided postdocs opportunities for: i.) self-evaluation and reflection to become aware of their personal biases, attitudes, and behaviors; ii.) exploring strengths, weaknesses, and challenges in their interpersonal and professional relationships; iii.) understanding and learning how to use the mentor principles; and iv.) focusing on cognitive processes that may lead to behavioral changes and strategies to facilitate those changes in a process-based approach over 1.5–2 day workshops.” (p.5) Method: “The 1.5–2 day workshops included case studies and activities that: i.) engage mentors in peer discussion of a mentor framework; ii.) explore strategies to improve mentoring relationships; iii.) address mentoring problems; iv.) reflect on mentoring philosophies; v.) and create mentoring action plans to model the interactive, collaborative, and problem-solving ways to develop and implement this set of trainings in the future. The training goals provided tools and mechanisms to implement mentor training venues at the participating institutions, thereby establishing sustainable Mentor-training programs for undergrads, graduate students, postdocs and faculty” (p.5). “A specific NRMN-CAN survey was developed for all four postdoc cohorts to ascertain whether mentor training: i.) influenced career progression; ii.) impacted the postdocs’ relationship with their PIs; and iii.) components of the mentor training that were implemented by the postdoc mentees... A dedicated NRMN-CAN survey for faculty and senior administrators was also developed to ascertain whether participation in Mentor Facilitator training led to: i.) implementation of training workshops on their campuses; ii.) the level and number of participants; iii.) and whether facilitated sessions were carried out in partnership with others.” (p.5) Results: Post-doctoral students reported improvements in their mentoring proficiency and improved relationships with the Principal Investigators. 29% of post-doc respondents transitioned to faculty positions, and 85% of these respondents were under-represented and 75% were female. 59 out of 120 faculty and administrators provided mentor training to over 3000 undergraduate, graduate and postdoctoral students and faculty on their campus for the duration of this project. The findings showed that the majority of post-doctoral students indicate that mentor training positively influenced their relationship with their Mentors in several domains (e.g. confidence building). In addition, this curriculum has guided most post-doctoral students to better understand their mentoring needs, develop strategies to manage their mentoring relationships and empower them to make critical career decisions to pursue an academic career. In addition, early-career scientists stated they had more confidence to pursue an academic career with increased self-efficiency and advocacy. Impressively, 29% of the responding postdocs, predominantly females (75%) and underrepresented postdocs (85%) have successfully migrated to faculty. Some postdocs also indicated that their mentor training and experiences were valuable skills when applying for academic positions and definitely aided in adapting to responsibilities as a faculty mentor. Abstract Changing institutional culture to be more diverse and inclusive within the biomedical academic community is difficult for many reasons. Herein we present evidence that a collaborative model involving multiple institutions of higher education can initiate and execute individual institutional change directed at enhancing diversity and inclusion at the postdoctoral researcher (postdoc) and junior faculty level by implementing evidence-based mentoring practices. A higher education consortium, the Big Ten Academic Alliance, invited individual member institutions to send participants to one of two types of annual mentor training: 1) “Mentoring-Up” training for postdocs, a majority of whom were from underrepresented groups; 2) Mentor Facilitator training—a train-the-trainer model—for faculty and senior leadership. From 2016 to 2019, 102 postdocs and 160 senior faculty and administrative leaders participated. Postdocs reported improvements in their mentoring proficiency (87%) and improved relationships with their PIs (71%). 29% of postdoc respondents transitioned to faculty positions, and 85% of these were underrepresented and 75% were female. 59 out of the 120 faculty and administrators (49%) trained in the first three years provided mentor training on their campuses to over 3000 undergraduate and graduate students, postdocs and faculty within the project period. We conclude that early stage biomedical professionals as well as individual institutions of higher education benefited significantly from this collaborative mentee/mentor training model APA Style Reference Risner, L. E., Morin, X. K., Erenrich, E. S., Clifford, P. S., Franke, J., Hurley, I., \u0026amp; Schwartz, N. B. (2020). Leveraging a collaborative consortium model of mentee/mentor training to foster career progression of underrepresented postdoctoral researchers and promote institutional diversity and inclusion. PloS one, 15(9), e0238518. https://doi.org/10.1371/journal.pone.0238518\nYou may also be interested in The mental of PhD cry for help (Nature, 2019) Postdocs in crisis: science risks losing the next generation (Nature, 2020) Unequal effects of the COVID-19 pandemic on scientists (Myers et al., 2019) An index to quantify an individual’s scientific research output (Hirsch, 2005) Main Takeaways: The author introduces the h index as a tool to quantify (in an unbiased way) the scientific output of researchers. Scientists who earn a Nobel prize have unquestionably relevant and impactful research. How do we quantify the impact and relevance of the work produced by other researchers? Current evaluation criteria are based on number of publications, number of citations for each paper, journal where papers were published and impact parameters. All these parameters are likely to be evaluated differently by different people. H is a preferable index to evaluate scientific output to a researcher. Total number of papers: measures productivity, yet does not measure importance of impact of papers. Citations measure total impact, yet hard to find and may be accentuated by “big hits” that are not representative of individuals if they are co-authors. Another disadvantage is that this measure gives a higher weight to review articles than original research articles. Citations per paper allow comparison of scientists of different ages, yet hard to find, rewards low productivity and penalises high productivity. Number of significant papers (defined as number of papers with citations higher than a certain number - let’s say “y” ). While this measure eliminates the disadvantages of the other measures (mentioned above), y is arbitrarily defined and will favour or disfavour individuals randomly. It needs to be adjusted for levels of seniority. Number of citations to each of the q most-cited papers (let’s say q=5). While it overcomes many of the disadvantages mentioned above, this measure does not yield a single number and is difficult to obtain and compare. The h index overcomes all the disadvantages of the other measures (mentioned above). The higher the h, the more accomplished the scientist is. H should increase with time. H will smoothly level off as the number of papers increases instead of a discontinuous change in slope. In reality, not all papers contribute to the h index. This is especially the case of papers with low citations when the h index of the researcher is already an appreciable number. H cannot decrease with time. Contrary to other parameters, the h parameter is useful for cumulative achievement continuing over time even after the end of the scientist’s publication. The author suggests that h ≈ 12 might be a typical value for advancement to tenure (associate professor) and that h ≈ 18 might be a typical value for advancement to full professor. However, a single number can never give more than an estimation to an individual’s multi-faceted profile and many other factors need to be combined to evaluate an individual. Differences in typical h values in different fields are expected (determined by average number of papers produced by each scientist in an specific field and size of field). Moreover, scientists working in non-mainstream areas will not achieve the same high h values as those working in highly topical areas. High h is a reliable indicator of high accomplishment; the opposite is not always not true. Although self-citations can obviously increase a scientist\u0026#39;s h, their effect on h is much smaller than on the total citation count. Nobel prize winners have an h index of 30, meaning their success did not occur in one stroke of luck but in a body of scientific work. H index could be important for rankings of groups or departments in the chosen area and administrators could be interested in this. Abstract I propose the index h, defined as the number of papers with citation number \u0026gt;h, as a useful index to characterize the scientific output of a researcher. APA Style Reference Hirsch, J. E. (2005). An index to quantify an individual\u0026#39;s scientific research output. Proceedings of the National academy of Sciences, 102(46), 16569-16572. https://doi.org/10.1073/pnas.0507655102\nYou may also be interested in High Impact =High Statistical Standards? Not Necessarily So (Tressoldi et al., 2013) The Leiden Manifesto for research metrics (Hicks et al., 2015) High Impact =High Statistical Standards? Not Necessarily So (Tressoldi et al., 2013) Main Takeaways: The present study investigates whether there are differences in statistical standards of papers published in journals with high and low impact. Journals with the highest impact factor are often taken to be a measure of high scientific value and rigorous methodological quality. The present study investigated how often null hypothesis significance testing and alternative methods are used in leading scientific journals compared to journals with lower impact factors. How many studies published in journals with the highest impact factor adopt the recommendations of basing their conclusions on their observed effect sizes and confidence intervals on those effect sizes? Are there differences with journals with lower impact factors in which editorial policy requires the adoption of these recommendations? Method: 6 Journals with high impact and 6 journals with low impact were chosen. “Our aim was to compare across journals, using all relevant articles, noting that many variables could contribute to any differences we found.” (p.3). Results: In 89% of Nature articles and 42% of Science articles, p values was more commonly used without any mention of confidence intervals, effect sizes, prospective power and model estimation, while other journals, both high- and low-impact factor, report confidence intervals and/or effect size measures. The best reporting practice was present in 80% of articles published in New England Journal of Medicine and Lancet, while this dropped to less than 30% for articles published in Science and less than 11% in the articles published in Nature journals. Reporting confidence intervals and effect sizes does not guarantee researchers use them in the interpretation of their findings or refer to them in text. The lack of interpretation of confidence intervals and effect sizes means that just observing high percentage of confidence intervals and effect sizes reporting may overestimate the impact of the statistical reform. Quote “It is not sufficient merely to report ESs and CIs—they need to be used as the basis of discussion and interpretation.” (p.6). Abstract What are the statistical practices of articles published in journals with a high impact factor? Are there differences compared with articles published in journals with a somewhat lower impact factor that have adopted editorial policies to reduce the impact of limitations of Null Hypothesis Significance Testing? To investigate these questions, the current study analyzed all articles related to psychological, neuropsychological and medical issues, published in 2011 in four journals with high impact factors: Science, Nature, The New England Journal of Medicine and The Lancet, and three journals with relatively lower impact factors: Neuropsychology, Journal of Experimental Psychology-Applied and the American Journal of Public Health. Results show that Null Hypothesis Significance Testing without any use of confidence intervals, effect size, prospective power and model estimation, is the prevalent statistical practice used in articles published in Nature, 89%, followed by articles published in Science, 42%. By contrast, in all other journals, both with high and lower impact factors, most articles report confidence intervals and/or effect size measures. We interpreted these differences as consequences of the editorial policies adopted by the journal editors, which are probably the most effective means to improve the statistical practices in journals with high or low impact factors. APA Style Reference Tressoldi, P. E., Giofré, D., Sella, F., \u0026amp; Cumming, G. (2013). High impact= high statistical standards? Not necessarily so. PloS one, 8(2), e56180. https://doi.org/10.1371/journal.pone.0056180\nYou may also be interested in An index to quantify an individual’s scientific research output (Hirsch, 2005) The Leiden Manifesto for research metrics (Hicks et al., 2015) Disability and Socio-economic status (APA, 2010) ◈ ⌺ Main Takeaways: The Disabilities Act assures equal opportunities in education and employment for people with disabilities and prohibits discrimination based on disability. Despite the Disabilities Act, people with disabilities remain over-represented among America’s poor and under-educated. The federal government has two major programs to assist individuals with disabilities: the Social Security Disability Insurance and the Supplemental Security Income. The Social Security Disability Insurance is a program for workers who have become disabled and unable to work after paying Social Security taxes for at least 40 quarters. In this program, a higher income yields higher SSDI earnings. The Supplemental Security Income is a welfare program for individuals with low income, fewer overall resources and no or an abbreviated work history. Current federal benefit for a single person using Supplemental Security Income is $735 a month. Despite these programs, people with disabilities are more likely to be unemployed and live in poverty. For individuals who are blind and visually impaired, unemployment rates exceed 70 percent while for people with intellectual and developmental disabilities, the unemployment rate exceeds 80 percent. Also, one in ten veterans with disabilities are unemployed. The American Association of People with Disabilities estimates that two thirds of people with disabilities are of working age and want to work. There are disparities in median incomes for people with and without disabilities, such that individuals with disabilities often earn lower incomes. A study surveyed human resources and project managers about perceptions of hiring persons with disabilities. Results show professionals held negative perceptions related to productivity, social maturity, interpersonal skills and psychological adjustment of persons with disabilities. Disparities in education have been ongoing for generations. 20.9% of individuals 65 years and older without a disability failed to complete high school, relative to 25.1% and 38.6% of elder individuals with a non-severe or severe disability. Great disparities exist when comparing attainment of higher degrees. 15.1% of the population aged 25 and over with disability obtain a bachelor’s degree, whereas 33% of individuals in the same age category with no disability attain the same educational status. Individuals with a disability experience increased barriers to obtaining health care as a result of accessibility concerns, such as transportation, problems with communication and insurance. Family members who provide care to individuals with chronic or disabling conditions are themselves at risk of developing emotional, mental and physical health problems due to complex caregiving situations. Abstract Learn how socioeconomic status affects individuals with disabilities. APA Style Reference APA (2010). Disability \u0026amp; Socioeconomic Status [Blog post]. Retrieved from https://www.apa.org/pi/ses/resources/publications/disability\nYou may also be interested in ls There a Positive Correlation between Socioeconomic Status and Academic Achievement? (Quagliata, 2008) Education and Socio-economic status (APA, 2017b) Ethnic and Racial minorities and socio-economic status (APA, 2017) Women and Socio-economic status (APA, 2010) Lesbian, Gay, Bisexual and Transgender Persons \u0026amp; Socioeconomic Status (APA, 2010) Registered reports (Jamieson et al., 2019) Main Takeaways: Stage I article submitted with title page, abstract, introduction, methods, analysis plan, and conclusions of a study before carrying out research. Stage I includes the article and a cover letter, confirming all support and approval is in place, timeline for completing this study, statement confirming authors share raw data, digital materials, analysis. Authors confirm registration of Stage I article with the Open Science Framework or another repository. Method section includes justification of sample sizes compared to question, description of participants, problems investigated, a priori justification, procedures to deduce inclusion and exclusion criteria and clear protocol of experimental procedures. Data analysis: outline and justify how data is treated including all pre-processing steps. Two step review of Stage I paper. Triaged by an editorial team before passing to peer review. Peer reviewers assess importance of research question, introduction, plausibility, quality of hypotheses and methodological quality and appropriateness of data analysis plan, validity of inferential conclusions based on data. One of three outcomes: conditional approval, revise decision, or reject. Revise decision allows authors to respond to editors and reviewers criticisms. Reject ends the review process. Following conditional approval, authors submit a Stage II article. Stage II article must be consistent with Stage I report. Hypotheses, rationale, and reasoning approved in Stage I must reappear in Stage II. Stage II provides a complete and final report of the approved Stage I article, which also includes raw data, digital materials and analyses. Stage II focuses on quality of data reported, soundness of conclusions drawn from data and consistency with arguments and reasoning. Are data sufficiently resolved to support conclusions? Does the data answer the authors proposed hypotheses? Does the introduction and analyses match the Stage I submission? Any unregistered and post-hoc analyses justified, methodologically sound, and informative? Are conclusions consistent with collected data. Editor can ask for revisions or reject Stage II articles. Abstract Professor Randall K. Jamieson provides an editorial on registered reports for the journal Canadian Journal of Psychology and how it works in this specific journal. APA Style Reference Jamieson, R. K., Bodner, G. E., Saint-Aubin, J., \u0026amp; Titone, D. (2019) Editorial: Registered reports. Canadian Journal of Experimental Psychology, 73, 3-4.\nYou may also be interested in Registered Reports: A new publishing initiative at Cortex (Chambers, 2013) Registered Reports: A step change in scientific publishing (Chambers, 2014) Registered Reports: Realigning incentives in scientific publishing (Chambers et al., 2015) Registered reports : a method to increase the credibility of published results (Nosek \u0026amp; Lakens, 2014) Rein in the four horsemen of irreproducibility (Bishop, 2019) Seven Easy Steps to Open Science: An Annotated Reading List (Crüwell et al., 2019) Seven Steps Toward Transparency and Replicability in Psychological Science (Lindsay, 2020) On the persistence of low power in psychological science (Vankov et al., 2014) The mental health of PhD researchers demands urgent attention (Nature, 2019) Main Takeaways: 29% of 5700 respondents to a survey in 2017 listed their mental health as an area of concern while less than half of those sought help for anxiety or depression caused by their PhD study. A new survey with 6300 graduate students from around the world show that 71% are satisfied with their experience of research, while 36% had help for their anxiety or depression related to their PhD. How can graduate students be both broadly satisfied, but increasingly unwell? One of the reasons might lie on the fact that 1/5 of respondents report being bullied and experience harassment or discrimination. Although universities should take more effective action, only ¼ of respondents said their institution provides support while 1/3 said they seek help elsewhere. Another reason for graduate students to be broadly satisfied, but increasingly unwell is that career success is measured by publications, citations, funding and impact. To progress, a researcher must hit high scores in all of these measures. Most students embark on a PhD as a foundation of an academic career. They believe they will have the freedom to discover and invent. However, problems arise when autonomy is reduced or removed, which occurs when targets for funding, impact and publications become part of the universities’ formal monitoring and evaluation systems. As student’s supervisors judge their success or failure, it is not surprising many feel unable to open up about vulnerabilities or mental-health concerns. Solutions do not solely lie in institutions doing more to provide on-campus mental health support, but also in the recognition of ill mental health as a consequence of excessive focus on measuring performance. Much has been written about how to overhaul the system and find a better way to define success in research, including promoting that many non-academic careers are open to researchers. The academic system is making young people ill and the research community needs to protect and empower the next generation of researchers. Abstract Without systemic change to research cultures, graduate-student mental health could worsen. APA Style Reference Nature. (2019). The mental health of PhD researchers demands urgent attention. Nature, 575, 257-258. https://www.nature.com/articles/d41586-019-03489-1\nYou may also be interested in Leveraging a collaborative consortium model of mentee/mentor training to foster career progression of under-represented post-doctoral researchers and promote institutional diversity and inclusion (Risner et al., 2020) Postdocs in crisis: science risks losing the next generation (Nature, 2020) Boosting research without supporting universities is wrong-headed (Nature, 2020b) Seeking an exit plan (Woolston, 2020) Unequal effects of the COVID-19 pandemic on scientists (Myers et al., 2019) Postdocs in crisis: science risks losing the next generation (Nature, 2020) Main Takeaways: Post-doctoral researchers often spend years in a succession of short-term contracts, which creates immense anxiety and uncertainty. Nature conducted a survey with postdocs and asked how the pandemic is affecting their current and future career plans, their health and well-being; and whether they feel supported by their supervisors. Respondents spam across 93 countries (and different fields), but most are from the US and Europe. Results show that the pandemic adds to postdocs’ distress. The pandemic worsened career prospects and supervisors have not done enough to support them during pandemic. 51% of respondents are considering leaving active research due to work-related mental health concerns. All efforts to help workers are welcome but on their own, small measures will not be enough to save many academic science careers. Universities cannot be expected to bear this extra cost. Universities are already feeling the consequences of the pandemic for their finances. This is especially the case of institutions dependent on income from international students’ fees. Global student mobility will be much lower than usual in the coming academic year, and some institutions will lose a good fraction of their fee income as a result. In places where research is cross-subsidized from tuition-fee income, contract-research workers such as post-docs are most vulnerable to losing their jobs and women and people from minority groups who form a high share of post-doctoral workforce, will likely be disproportionately affected. As many post-docs are looking to leave their posts now, anticipating worse is to come, research and university leaders must find innovative ways to support early-career researchers. Principal investigators should show flexibility, patience and support for everyone in their group. Principal investigators and their institutions must push harder than ever for accessible mental health services. Abstract The pandemic has worsened the plight of postdoctoral researchers. Funders need to be offering more than moral support. APA Style Reference Nature. (2020). Postdocs in crisis: science cannot risk losing the next generation. Nature, 585. 10.1038/d41586-020-02541-9 https://www.nature.com/articles/d41586-020-02541-9\nYou may also be interested in Leveraging a collaborative consortium model of mentee/mentor training to foster career progression of under-represented post-doctoral researchers and promote institutional diversity and inclusion (Risner et al., 2020) ⌺ The mental health of PhD researchers demands urgent attention (Nature, 2019) Boosting research without supporting universities is wrong-headed (Nature, 2020b) Seeking an exit plan (Woolston, 2020) Unequal effects of the COVID-19 pandemic on scientists (Myers et al., 2019) Boosting research without supporting universities is wrong-headed (Nature, 2020b) ⌺ Main Takeaways: Coronavirus lockdowns have precipitated a crisis in university funding and academic morale. Universities all over the world closed their doors. Classes and some research activities were moved online. Staff were given little or no time to prepare and few resources or training to help them with online teaching. Fewer students are expected to enrol in the coming academic year, instead waiting until institutions open fully. This means young people will lose a year of their education and universities will lose out financially. Governments have plans to boost post-lockdown research but these plans will be undermined if universities make job cuts and end up with staff shortages. Universities need support at this crucial time. Low- and middle-income countries face extra challenges from sudden transition to online learning. The main concern is for students unable to access digital classrooms (those who live in areas without fast, reliable and affordable broadband or where students have no access to laptops, tablets, smartphones and other essential hardware). Teachers report students struggle to keep up since lockdown began. Students from poorer households in remote regions travel to the nearest city to access the Internet and pay commercial internet cafes to download course materials. To solve this issue, governments and funding bodies need to accept that students and universities should be eligible for the same kinds of temporary emergency funding as other industries are asking for. Governments have denied requests to negotiate with universities or delayed decisions. In high-income countries, this is partly because universities are functioning and might be seen as less deserving of government help than businesses and professions that had no choice but to close. In poorer countries, public funding for universities is under threat because economies have crashed during lockdowns. Cuts in universities’ budgets will disproportionately affect poorest students and more vulnerable members of staff (those with fixed-term contracts). Students and staff on short-term contracts would welcome more support from academic colleagues in senior positions and from others with permanent positions. Colleagues should make the case for managers that failing to provide more help to low-income students or cutting the number of post-doctoral staff and teaching fellows presents a harm to the next generation of researchers and teachers. It will reduce departments’ capacity to teach and increase load on those who remain. Cutting back on scholarly capacity while increasing spending on research and development is wrong-headed, slowing down economic recovery and jeopardising plans to make research more inclusive. Abstract Universities face a severe financial crisis, and some contract staff are hanging by a thread. Senior colleagues need to speak up now. APA Style Reference Nature. (2020). Boosting research without supporting universities is wrong-headed. Nature, 582, 313-314. https://www.nature.com/articles/d41586-020-01788-6\nYou may also be interested in The mental health of PhD researchers demands urgent attention (Nature, 2019) Postdocs in crisis: science risks losing the next generation (Nature, 2020) Seeking an exit plan (Woolston, 2020) Unequal effects of the COVID-19 pandemic on scientists (Myers et al., 2019) Seeking an exit plan (Woolston, 2020) Main Takeaways: Full impact of COVID-19 pandemic on scientific careers might not be known for years, but hiring freezes and other signs of turmoil at universities shake faith in academia as career options. A growing number of PhD students and other early-career researchers start to look at careers in industry, government and other sectors. It is unclear how many of these researchers will eventually leave academia out of choice or necessity, but a significant academic exodus is expected. It is suggested that the shortage of tenured and tenure-track university positions will deepen in coming years. History shows that, in the United States, recession coincided with a strong shift towards gig or temporary work. Academic escapees have to prepare themselves to navigate a new career landscape. As competition for industry jobs will be stiff, it is important to learn how to emphasise skills developed in university careers. Abstract The pandemic is prompting some early-career researchers to rethink their hopes for a university post. By Chris Woolston. APA Style Reference Nature. (2020). Seeking an ‘exit plan’ for leaving academia amid coronavirus worries. Nature 583, 645-646. Doi: 10.1038/d41586-020-02029-6.\nYou may also be interested in Leveraging a collaborative consortium model of mentee/mentor training to foster career progression of under-represented post-doctoral researchers and promote institutional diversity and inclusion (Risner et al., 2020) ⌺ Postdocs in crisis: science risks losing the next generation (Nature, 2020) Boosting research without supporting universities is wrong-headed (Nature, 2020b) Lesbian, Gay, Bisexual and Transgender Persons \u0026amp; Socioeconomic Status (APA, 2010) ◈ ⌺ Main Takeaways: Individuals who identify as Lesbian, gay, bisexual and/or transgender are specially susceptible to socio-economic disadvantages. Socioeconomic status is inherently linked to rights, quality of life, and general well-being of Lesbian, Gay, Bisexual and/or transgender persons. Low income LGBT individuals and same-sex/gender couples have been found to be more likely to receive cash assistance and food stamps benefits compared to heterosexual individuals or couples. Transgender adults were nearly 4 times more likely to have household income of less than $10,000 per year relative to the general population. Raising the federal minimum wage benefits LGBT individuals and couples in the United States. An increase in minimum wage should reduce poverty rates by 25% for same-sex/gender female couples and 30% for same-sex/gender male couples. Due to an increase in minimum wage, poverty rates would be projected to fall for the most vulnerable individuals in same-sex/gender couples, including African American, couples with children, people with disabilities, individuals under 24 years of age, people without high school diplomas or the equivalent, and those living in rural areas. The socio-economic position may be linked to experiences of discrimination. Gay and bisexual men who earned higher income were less likely to report discrimination relative to those in lower socio-economic positions. Discrimination against and unfair treatment of LGBT persons remains legally permitted. 47% of transgender individuals report being discriminated against in hiring, firing and promotion, over 25% had lost a job due to discrimination based on gender identity. A lack of acceptance and fear of persecution lead many LGBT youth to leave their homes and live in transitional housing or on the street. Many LGBT youth may be rejected by their family of origin or caregivers and forced to leave home as minors. LGBT youth experience homeless at a disproportionate rate. LGBT homeless youth are more likely than their homeless heterosexual counterparts to have poorer mental and physical health outcomes. Although since 2015 states must issue marriage licenses to same-sex couples and recognise same-sex unions, legal barriers continue to exist. Workplace and housing discrimination contribute to increasing socio-economic status disparities for LGBT persons and families. 20 states and District of Columbia prohibit discrimination in workplace based on sexual orientation and gender identity, while 18 states have no laws prohibiting workplace discrimination against LGBT people. 19% of transgender individuals report in a previous study that they were refused a home or apartment and 11% report being evicted because of their gender identity or expression. Abstract Evidence indicates individuals who identify as lesbian, gay, bisexual and/or transgender (LGBT) are especially susceptible to socioeconomic disadvantages. Thus, SES is inherently related to the rights, quality of life and general well-being of LGBT persons. APA Style Reference APA (2010). Lesbian, Gay, Bisexual and Transgender Persons \u0026amp; Socioeconomic Status. [Blog post]. Retrieved from https://www.apa.org/pi/ses/resources/publications/lgbt\nYou may also be interested in ls There a Positive Correlation between Socioeconomic Status and Academic Achievement? (Quagliata, 2008) Education and Socio-economic status (APA, 2017b) Ethnic and Racial minorities and socio-economic status (APA, 2017) Women and Socio-economic status (APA, 2010) Disability and Socio-economic status (APA, 2010) The Focus on Fame distorts Science (Innes-Ker, 2017) ◈ ⌺ Main Takeaways: The author argues that instead of focusing on individual merit it is important that science is focused on scientific ideas and collaborative groups. Asking if you are famous is a wrong question. It focuses on the individual scientist, as if science is a lonely enterprise of hopeful geniuses. We should focus on ideas and knowledge and refining those ideas. H-index is not an objective measure. It presupposes that peer-review papers are solid and that citations are a proxy for quality. Science is argued to advance in an evolutionary manner. A wealth of ideas is produced, but only some are selected and survive depending on scientific merit and social process (production of papers, citations and engagement of groups of scientists). Ideas that engage groups of scientists will grow and change and bring knowledge closer to the truth. Ideas that are not interacted with, on the other hand, will likely die. This is far from focus on eminence and individual fame prevalent in science. Competition is a factor but cooperation is vital. For ideas to survive, multiple labs need to engage with them as champions or severe adversarial testers. If we focus on who may become eminent, we lose some power of the scientific process. Eminent scientists would be nowhere without collaborators and adversaries willing to engage with the ideas. The tendency to overwhelmingly publish only positive results with no clear avenue for publishing failures to confirm, means scientists are not grappling with the real field. Recent work to improve methods, statistics and publishing practices is an example of collaboration. In science, scientific ideas are the ones that need to be stress-tested, not scientists. We need to move away from the cultural market model of science focusing on individuals rather than on robustness of ideas. Science is a low yield, high risk business. Assigning individual merit based on productivity and citation encourages poor scientific practices and discourages collaboration and argumentative engagement with ideas. It results in a waste of talent. Objectivity in Science is not a characteristic of individual researchers, but a characteristic of scientific communities. Abstract The 2016 symposium on Scholarly Merit focused on individual eminence and fame. I argue, with some evidence, that the focus on individual merit distorts science. Instead we need to focus on the scientific ideas, and the creation of collaborative groups. APA Style Reference Innes-Ker, Å. (2017). The Focus on Fame Distorts Science. https://psyarxiv.com/vyr3e/\nYou may also be interested in Fame: I’m Skeptical (Ferreira, 2017) Let’s Look at the Big Picture: A System-Level Approach to Assessing Scholarly Merit (Pickett, 2017) “Fame” is the Problem: Conflation of Visibility With Potential for Long-Term Impact in Psychological Science (Shiota, 2017) Why a Focus on Eminence is Misguided: A Call to Return to Basic Scientific Values (Corker, 2017) Am I Famous Yet? Judging Scholarly Merit in Psychological Science: An Introduction (Sternberg, 2016) Against Eminence (Vazire, 2017) Giving Credit Where Credit’s Due: Why It’s So Hard to Do in Psychological Science (Simonton, 2016) Eminence and Omniscience: Statistical and Clinical Prediction of Merit (Foss, 2016) Taking Advantage of Citation Measures of Scholarly Impact: Hip Hip h Index! (Ruscio, 2016) Improving Departments of Psychology (Diener, 2016) Varieties of Fame in Psychology (Roediger III, 2016) Scientific Eminence: Where Are the Women? (Eagly \u0026amp; Miller, 2016) Intrinsic and Extrinsic Science: A Dialectic of Scientific Fame (Feist, 2016) Fame: I’m Skeptical (Ferreira, 2017) ◈ ⌺ Main Takeaways: The author argues that fame and quality sometimes diverge and that reliance on fame helps to perpetuate stereotypes that keep women and underrepresented groups from participating in science. Most of us believe we have the respect of our peers and acknowledge we wish to be admired and viewed as successful and important. No psychologist and no rational person would deny that evaluating people and the quality of their work is necessary and inevitable in any field. We like to admit most promising candidates to graduate programs, hire the best faculty, tenure only those who have long productive careers and reward scientists with prizes if they contributed more than most to uncover the nature of psychological processes. We must not conflate fame and scientific quality, integrity and impact. All of us point to colleagues who completed excellent work but are barely known or who are not famous until long after their research careers have ended. Some scientists are well known because they have been called out for unethical practices, including data fabrication and other forms of cheating. We need to discriminate between two questions: (i) what one must do to become famous and (ii) what leads a person to end up famous. While the second question is merely an attempt to reconstruct someone’s path to fame, the motivations of the first question need to be challenged. Fame should not be a goal in science and valuing people or ideas because they are famous comes at a risk. Fame should be viewed with caution and scepticism to avoid temptation to assume that if someone is famous, their work is significant. Fame perpetuates discrimination and overlook excellent people and work. Science is based on critical thinking. As such, we should never hesitate to question the ideas of someone who is famous. We should not refuse to view the work of famous people positively or refuse to give it its due, but we must be careful to think an idea is useful due to the person being famous. Abstract Fame is often deserved, emerging from a person’s significant and timely contributions to science. It is also true that fame and quality clearly sometimes diverge: many people who do excellent work are barely known, and some people are famous even though their work is mediocre. Reliance on fame and name recognition when identifying psychologists as candidates for honors and awards helps to perpetuate a range of stereotypes and prevents us from broadening participation in our field, particularly from women and underrepresented groups. The pursuit of fame may also be contributing to the current crisis in psychology concerning research integrity, because it incentivizes quantity and speed in publishing. The right attitude towards fame is to use it wisely if it happens to come, but to focus our efforts on conducting excellent research and nurturing talent in others. APA Style Reference Ferreira, F. (2017). Fame: I\u0026#39;m Skeptical (2017). https://psyarxiv.com/6zb4f/\nYou may also be interested in The Focus on Fame distorts Science (Innes-Ker, 2017) Let’s Look at the Big Picture: A System-Level Approach to Assessing Scholarly Merit (Pickett, 2017) “Fame” is the Problem: Conflation of Visibility With Potential for Long-Term Impact in Psychological Science (Shiota, 2017) Why a Focus on Eminence is Misguided: A Call to Return to Basic Scientific Values (Corker, 2017) Am I Famous Yet? Judging Scholarly Merit in Psychological Science: An Introduction (Sternberg, 2016) Against Eminence (Vazire, 2017) Giving Credit Where Credit’s Due: Why It’s So Hard to Do in Psychological Science (Simonton, 2016) Eminence and Omniscience: Statistical and Clinical Prediction of Merit (Foss, 2016) Taking Advantage of Citation Measures of Scholarly Impact: Hip Hip h Index! (Ruscio, 2016) Improving Departments of Psychology (Diener, 2016) Varieties of Fame in Psychology (Roediger III, 2016) Scientific Eminence: Where Are the Women? (Eagly \u0026amp; Miller, 2016) Intrinsic and Extrinsic Science: A Dialectic of Scientific Fame (Feist, 2016) Let’s Look at the Big Picture: A System-Level Approach to Assessing Scholarly Merit (Pickett, 2017) ◈ ⌺ Main Takeaways: Why do we care about judging scientific merit? There is a need to have a system to determine whether to award tenure and promotion to faculty members, leading to a development of criteria in order to judge and measure the scholarly merit of individuals. Science is a collective enterprise whose goal is to explain and understand the natural world and to build knowledge. Science cares about advancements and discoveries, not about individuals. Individual scientists are valued to the extent that they further the goals of the collective system. However, science comprises lab workers, scientists, institutions, agencies, and broader society. At organisation level, features facilitate scientific discovery-organisational autonomy, organisational flexibility, moderate scientific diversity and frequent and intense interaction among scientists with different viewpoints. An individual scientist contributes to scientific discovery directly through their own scientific products or indirectly by positively affecting other aspects of the system. More senior graduate students train incoming graduate students- when good at this the output of an entire lab can skyrocket as a result. Graduate students not only conduct their own personal research but their presence in the lab facilitates scientific progress of others. Scientists promote productivity of other scientists by reviewing manuscripts, sharing data, creating and serving scientific organisations, and developing scientific tools and paradigms used by others. Individual research scientists do not have resources to create large research centres, but can organise conferences and symposia, create and contribute to scientific discussion platforms, and make their research protocols and data easily shareable. Scholarly merit should include an individual’s system-level contributions, not only their productivity. Abstract When judging scientific merit, the traditional method has been to use measures that assess the quality and/or quantity of an individual’s research program. In today’s academic world, a meritorious scholar is one who publishes high quality work that is frequently cited, who receives plentiful funding and scientific awards, and who is well regarded among his or her peers. In other words, merit is defined by how successful the scholar has been in terms of promoting his or her own career. In this commentary, I argue that there has been an overemphasis on measuring individual career outcomes and that we should be more concerned with the effect that scholars have on the scientific system in which they are embedded. Put simply, the question we should be asking is whether and to what extent a scholar has advanced the scientific discipline and moved the field forward collectively. APA Style Reference Pickett, C. (2017). Let\u0026#39;s Look at the Big Picture: A System-Level Approach to Assessing Scholarly Merit. https://psyarxiv.com/tv6nb/\nYou may also be interested in The Focus on Fame distorts Science (Innes-Ker, 2017) Fame: I’m Skeptical (Ferreira, 2017) “Fame” is the Problem: Conflation of Visibility With Potential for Long-Term Impact in Psychological Science (Shiota, 2017) Why a Focus on Eminence is Misguided: A Call to Return to Basic Scientific Values (Corker, 2017) Am I Famous Yet? Judging Scholarly Merit in Psychological Science: An Introduction (Sternberg, 2016) Against Eminence (Vazire, 2017) Giving Credit Where Credit’s Due: Why It’s So Hard to Do in Psychological Science (Simonton, 2016) Eminence and Omniscience: Statistical and Clinical Prediction of Merit (Foss, 2016) Taking Advantage of Citation Measures of Scholarly Impact: Hip Hip h Index! (Ruscio, 2016) Improving Departments of Psychology (Diener, 2016) Varieties of Fame in Psychology (Roediger III, 2016) Scientific Eminence: Where Are the Women? (Eagly \u0026amp; Miller, 2016) Intrinsic and Extrinsic Science: A Dialectic of Scientific Fame (Feist, 2016) “Fame” is the Problem: Conflation of Visibility With Potential for Long-Term Impact in Psychological Science (Shiota, 2017)◈ ⌺ Main Takeaways: Fame is about visibility – who is seen. Ample evidence documents the influence of heuristics in determining who is visible, and whose contribution is considered important. Explicit and implicit beliefs about competence influences peer review when methodological quality or potential impact is ambiguous. The author is sceptical about the extent that fame is shaped by the quality of one’s work instead of confidence, dominance, persistence and demographics. The pace of academic life accelerates, the pressure to depend on shortcuts in gatekeeping and evaluation will continue to grow. The scientific community cannot remove implicit biases, there are ways to deflect the impact of these implicit biases. Reviews of submitted work should be blind to identity and demographics, letting the quality of the product stand on its own. Quote “We specify criteria for good science flexibly but explicitly and in detail, including thorough and accurate contextualisation in relevant previous work, methodological rigour; innovation and problem solving and implications for theory, future research and/or intervention. We should insist on diversity in career stage, gender, ethnicity and perspective instead of inviting first people who come to mind for invited opportunities such as conference talks, contribution to edited volumes, awards, and participation in committees that determine the direction of our field. We can resist temptation to track women and minorities into high profile, high-demand services roles, thinking that this solves problems of diversity in science. When, in fact, it does not.” (p.7) Abstract To be famous is to be widely known, and honored for one’s achievements. The process by which researchers achieve fame or eminence is skewed by heuristics that influence visibility; implications of these heuristics are magnified by a snowball effect, in which current fame leads to bias in ostensibly objective metrics of merit, including the distribution of resources that support future excellence. This effect may disproportionately hurt women and minorities, who struggle with both external and internalized implicit biases regarding competence and worth. While some solutions to this problem are available, they will not address the deeper problems of defining what it means for research to “make a difference” in our field and in society, and consistently holding our work to that criterion. APA Style Reference Shiota, M. N. (2017) “Fame” is the Problem: Conflation of Visibility With Potential for Long-Term Impact in Psychological Science. https://psyarxiv.com/4kwuq\nYou may also be interested in The Focus on Fame distorts Science (Innes-Ker, 2017) Fame: I’m Skeptical (Ferreira, 2017) Let’s Look at the Big Picture: A System-Level Approach to Assessing Scholarly Merit (Pickett, 2017) Why a Focus on Eminence is Misguided: A Call to Return to Basic Scientific Values (Corker, 2017) Am I Famous Yet? Judging Scholarly Merit in Psychological Science: An Introduction (Sternberg, 2016) Against Eminence (Vazire, 2017) Giving Credit Where Credit’s Due: Why It’s So Hard to Do in Psychological Science (Simonton, 2016) Eminence and Omniscience: Statistical and Clinical Prediction of Merit (Foss, 2016) Taking Advantage of Citation Measures of Scholarly Impact: Hip Hip h Index! (Ruscio, 2016) Improving Departments of Psychology (Diener, 2016) Varieties of Fame in Psychology (Roediger III, 2016) Scientific Eminence: Where Are the Women? (Eagly \u0026amp; Miller, 2016) Intrinsic and Extrinsic Science: A Dialectic of Scientific Fame (Feist, 2016) Why a Focus on Eminence is Misguided: A Call to Return to Basic Scientific Values (Corker, 2017) ◈ ⌺ Main Takeaways: The author argues that our current methods of scientific rewards are based on identifying research eminence. This reward system is not in line with scientific values of transparency and universalism and undermines scientific quality. Why do we accord knowledge derived from scientific method a privileged position relative to common sense, appeals to authority figures, or other forms of rhetoric? If scientists depend on their own expertise as justification to prioritise their claims, we are not better to make truth-claims than religious, political and other leaders. Instead, science’s claim on truth comes not from its practitioners’ training and expertise, but rather from its strong adherence to norms of transparency and universalism. Universalism means scientists reject claims of special authority. It matters far less who did the research than how it was done. How do we square scientific ideals of universalism with scientific culture that fetishizes lone scientific genius? We need to recognise the methods used to produce a scientific claim are more important than eminence of a person who produced it. Focusing primarily on the individual researcher excellence hurts psychological science, as eminence reflects values that are counterproductive to maximise scientific knowledge. The current system privileges quantity over quality, outcome of research instead of the process itself. Systematic biases (e.g., structural sexism, racism, and status bias) affect how we identify who qualifies as eminent under status quo. Gender, nationality, race or institution should not matter to measure research quality. Structural changes should be initiated to help researchers reward and evaluate quality research (i.e., work that is reproducible, transparent and open, and likely to be high in validity). We can do a much better job to recognise and reward many activities researchers do that support scientific discovery beyond publishing peer reviewed articles (e.g. develop scientific software, generate large datasets, write data analytic code and construct tutorials to teach others to use it). We need to re-evaluate ways to measure researchers’ excellence in light of value and promise of team-driven research. After all, science is a communal endeavour. To combat structural and systematic problems linked to recognising eminence, double blind peer reviews need to be considered as standard practice for journal publication, grant funding and awards committee. Technological solutions could even be developed to allow departments to blind in early stages of faculty hiring, as blinding is associated with higher levels of diversity. Abstract The scientific method has been used to eradicate polio, send humans to the moon, and enrich understanding of human cognition and behavior. It produced these accomplishments not through magic or appeals to authority, but through open, detailed, and reproducible methods. To call something “science” means there are clear ways to independently and empirically evaluate research claims. There is no need to simply trust an information source. Scientific values thus prioritize transparency and universalism, emphasizing that it matters less who has made a discovery than how it was done. Yet, scientific reward systems are based on identifying individual eminence. The current paper contrasts this focus on individual eminence with reforms to scientific rewards systems that help these systems better align with scientific values. APA Style Reference Corker, K. S. (2017). Why a Focus on Eminence is Misguided: A Call to Return to Basic Scientific Values. https://psyarxiv.com/yqfrd\nYou may also be interested in The Focus on Fame distorts Science (Innes-Ker, 2017) Fame: I’m Skeptical (Ferreira, 2017) Let’s Look at the Big Picture: A System-Level Approach to Assessing Scholarly Merit (Pickett, 2017) “Fame” is the Problem: Conflation of Visibility With Potential for Long-Term Impact in Psychological Science (Shiota, 2017) Am I Famous Yet? Judging Scholarly Merit in Psychological Science: An Introduction (Sternberg, 2016) Against Eminence (Vazire, 2017) Giving Credit Where Credit’s Due: Why It’s So Hard to Do in Psychological Science (Simonton, 2016) Eminence and Omniscience: Statistical and Clinical Prediction of Merit (Foss, 2016) Taking Advantage of Citation Measures of Scholarly Impact: Hip Hip h Index! (Ruscio, 2016) Improving Departments of Psychology (Diener, 2016) Varieties of Fame in Psychology (Roediger III, 2016) Scientific Eminence: Where Are the Women? (Eagly \u0026amp; Miller, 2016) Intrinsic and Extrinsic Science: A Dialectic of Scientific Fame (Feist, 2016) Don’t let transparency damage science (Lewandowsky \u0026amp; Bishop, 2016) Main Takeaways: Scientific communities have launched initiatives to increase transparency, open critique, and data sharing. Good researchers include all perspectives but their openness can be abused by opponents who aim to stall inconvenient research. Science is prone to attacks but rigour and transparency helps researchers and their universities respond to valid criticism. Open data practices should be adopted and scientists should not regard all requests for data as harassment. Researchers should explain why they cannot share their data. Confidentiality issues need to be considered, also researchers need control over how data is going to be used if the participant agrees to the sharing of this data. Engagement with critics is a fundamental part of scientific practice. Researchers may feel obliged to respond even to trolls but can ignore abusive or illogical critics that make the same points. Minor corrections and clarifications after publications should not be seen as a stigma against fellow researchers. Thus, Publications should be seen as living documents with corrigenda being accepted (even if unwelcome) as part of scientific progress. Self-censorship affects academic freedom and discussion. Publication retractions should be reserved for fraud or grave errors, but often are demanded by people who do not like a paper’s conclusions. Complaints may undervalue researchers for legal but contentious science. Harassed scientists feel alone. They should not tolerate harassment dependent on race or gender nor if it is based on controversial science. Training and support should be used to aid researchers in the ability to cope with harassment. Abstract Professor Stephan Lewandowsky and Professor Dorothy Bishop explain how the research community should protect its members from harassment, while encouraging openness and transparency as it is essential for science. APA Style Reference Lewandowsky, S., \u0026amp; Bishop, D. (2016). Research integrity: Don\u0026#39;t let transparency damage science. Nature, 529(7587), 459-461.http://dx.doi.org/10.1038/529459a\nYou may also be interested in Only Human: Scientists, Systems, and Suspect Statistics A review of: Improving Scientific Practice: Dealing With The Human Factors, University of Amsterdam, Amsterdam, September 11, 2014 (Hardwicke et al., 2014) Rein in the four horsemen of irreproducibility (Bishop, 2019) Fallibility in Science: Responding to Errors in the Work of Oneself and Others (Bishop, 2018) Unequal effects of the COVID-19 pandemic on scientists (Myers et al., 2019) ⌺ Main Takeaways: COVID-19 pandemic disrupted scientific enterprise. Policymakers and institutional leaders have started to respond to reduce influences of pandemic on researchers. For this study, authors reached out to US- and Europe-based scientists across institutions, career stages and demographic backgrounds. The present paper solicited information about working hours and how time allocations changed since the onset of pandemic and asked scientists to report the range of individual and family properties, as these feature moderate effects of pandemic. The sample was self-selected and it is likely that those who feel strongly about sharing situations, whether they experienced large positive or negative changes due to the pandemic, were the ones who chose to participate. They found a decline in total working hours with the average dropping from 61 hours per week pre-pandemic to 54 hours at time of survey. Only 5% of scientists report they worked 42 hours or less before the pandemic. This share increased to 30% of scientists during the pandemic. Time devoted to research has changed most during pandemic. Total working hours decreased by 11% on average, but research declined by 24%. Scientists working in fields that rely on physical laboratories and on time sensitive experiments report largest declines in research time (in the range of 30-40% below pre-pandemic levels). Fields that are less equipment intensive (e.g., mathematics, statistics, computer science and economics) report lowest declines in research time. The difference to other fields can be as large as fourfold. There are differences between male and female respondents in how the pandemic influenced their work. Female scientists and scientists with young dependents report ability to devote time to their research has been influenced and effects are additive - most impact was for female scientists with young dependents. Individual circumstances of researchers best explain changes in time devoted to research during pandemic. Career stage and facility closures did not contribute to changes in time allocated to research when everything else is held constant. Gender and young dependents contributed major roles. Female scientists reported a 5% larger decline in research time than male scientists, but scientists with at least one child 5 years old or younger experienced a 17% larger decline in research time. Having multiple dependents was linked to a further 3% reduction in time spent on research. Scientists with dependents aged 6-11 years were less affected. This indicates gender discrepancy can be due to female scientists being more likely to have young children as dependents. Results indicate that the pandemic influences members of the scientific community differently. Shelter at home is not the same as work from home, when dependents are also at home and need care. Unless adequate childcare services are available, researchers with young children continue to be affected irrespective of reopening plans of institutions. Pandemic will likely have longer-term impacts that are important to monitor. Further efforts to track effects of pandemic on the scientific workforce need to consider household circumstances. Uniform policies do not consider individual circumstances and may have unintended consequences and worsen pre-existing inequalities. The disparities may worsen as institutions begin the process of reopening given that different priorities for bench sciences versus work with human subjects or field-work travel may lead to new disparities across scientists. Funders seeking to support high-impact programs adopt a similar approach, favouring proposals that are more resilient to uncertain future scenarios. Senior researchers have incentives to avoid in-person interactions facilitating mentoring and hands-on training of junior researchers. Impact of changes on individuals and groups of scientists could be large in short- and long-term, worsening negative impacts among those at a disadvantage. We need to consider consequences of policies adopted to respond to pandemic, as they may disadvantage under-represented minorities and worsen existing disparities. Quote “The disparities we observe and the likely surfacing of new impacts in the coming months and years argue for targeted and nuanced approaches as the world-wide research enterprise rebuilds.” (p.882) Abstract COVID-19 has not affected all scientists equally. A survey of principal investigators indicates that female scientists, those in the ‘bench sciences’ and, especially, scientists with young children experienced a substantial decline in time devoted to research. This could have important short- and longer-term effects on their careers, which institution leaders and funders need to address carefully. APA Style Reference Myers, K. R., Tham, W. Y., Yin, Y., Cohodes, N., Thursby, J. G., Thursby, M. C., ... \u0026amp; Wang, D. (2020). Unequal effects of the COVID-19 pandemic on scientists. Nature Human Behaviour, 4, 880-883. https://doi.org/10.1038/s41562-020-0921-y\nYou may also be interested in The Gender Gap: Who Is (and Is Not) Included on Graduate-Level Syllabi in Social/Personality Psychology (Skitka et al., 2020) Leveraging a collaborative consortium model of mentee/mentor training to foster career progression of under-represented post-doctoral researchers and promote institutional diversity and inclusion (Risner et al., 2020) ⌺ The mental health of PhD researchers demands urgent attention (Nature, 2019) Postdocs in crisis: science risks losing the next generation (Nature, 2020) Boosting research without supporting universities is wrong-headed (Nature, 2020b) Against Eminence (Vazire, 2017) Am I Famous Yet? Judging Scholarly Merit in Psychological Science: An Introduction (Sternberg, 2016) Main Takeaways: Quality, productivity, visibility and impact are judged by the department in terms of scientific merit. These evaluations are subjective. Letters from distinguished referees provide a detailed and qualitative assessment of the referee’s future potential and the nature of the individual’s scientific contribution. Letter writers are prone to fads, biases and personal idiosyncrasies that can positively or negatively affect the chances of tenure or promotion. Quantity of publication is a reasonable measure of the researcher’s productivity. If one does not worry about the prestige of the journal, most articles get published. However, it does not inform us about the quality of the researcher’s work. Quantity of publications controlling for impact factors of journals tells us how much, on average, articles in that specific journal get cited. However, the problem is that it focuses on the average, some articles get highly cited and some do not get cited at all. Also, prestigious journals are conservative in what they do publish. Number of citations is a good measure and provides us information about how often is the researcher cited over their career. If you are in a controversial topic, or in an area that appeals to the broad audience or work in hot areas, it can provide a unique advantage to those researchers. H index is the number of publications cited at least h times and takes into account how quality and quantity affect impact. I10 is the number of publications cited at least 10 times. Grants and contracts show scholars have systematically and valued proposed programs of research. Editorship shows scholar’s work is recognised in their field. Invited service on a grant panel is another recognition of success in one’s professional endeavours. Awards are a useful measure of recognition by peers and measure quality of work instead of citation to work. Honorary doctorates are recognitions by broader academic audiences of merit of a scholar’s work. There are not many big psychological theorists left. Some would say the shift represents a natural progression as the field becomes more and more of a natural science. The big thinkers of yesterday might be taken aback by the amount of work done in modern times. The use of neuroimaging, behavioural experiments importance shrinks towards small-scale psychology without theory but with a large theory, they contribute to larger theory. Big thinking pays off. Quote “Most of us in academia go through a series of increasingly more challenging evaluations—first to get the PhD, next at the time of hiring, then at the time of reappointment, subsequently at the time of tenure, and finally at the time of promotion to full professor. And when we go through these evaluations, we almost inevitably wonder whether the criteria by which we will be judged are fair and whether the criteria, whatever they are, will be applied fairly.” (p.877) Abstract The purpose of this symposium is to consider new ways of judging merit in academia, especially with respect to research in psychological science. First, I discuss the importance of merit-based evaluation and the purpose of this symposium. Next, I review some previous ideas about judging merit—especially creative merit—and I describe some of the main criteria used by institutions today for judging the quality of research in psychological science. Finally, I suggest a new criterion that institutions and individuals might use and draw some conclusions. APA Style Reference Sternberg, R. J. (2016). “Am I famous yet?” Judging scholarly merit in psychological science: An introduction. Perspectives on Psychological Science, 11(6), 877-881. https://doi.org/10.1177/1745691616661777\nYou may also be interested in The Focus on Fame distorts Science (Innes-Ker, 2017) Fame: I’m Skeptical (Ferreira, 2017) Let’s Look at the Big Picture: A System-Level Approach to Assessing Scholarly Merit (Pickett, 2017) “Fame” is the Problem: Conflation of Visibility With Potential for Long-Term Impact in Psychological Science (Shiota, 2017) Why a Focus on Eminence is Misguided: A Call to Return to Basic Scientific Values (Corker, 2017) Against Eminence (Vazire, 2017) Giving Credit Where Credit’s Due: Why It’s So Hard to Do in Psychological Science (Simonton, 2016) Eminence and Omniscience: Statistical and Clinical Prediction of Merit (Foss, 2016) Taking Advantage of Citation Measures of Scholarly Impact: Hip Hip h Index! (Ruscio, 2016) Improving Departments of Psychology (Diener, 2016) Varieties of Fame in Psychology (Roediger III, 2016) Scientific Eminence: Where Are the Women? (Eagly \u0026amp; Miller, 2016) Intrinsic and Extrinsic Science: A Dialectic of Scientific Fame (Feist, 2016) Against Eminence (Vazire, 2017) ◈ ⌺ Main Takeaways: The author argues that the drive for eminence is inherently at odds with scientific values and that insufficient attention to this problem is partly responsible for the recent crisis of confidence in psychology and other sciences. Transparency makes it possible for scientists to discriminate robust from shaky findings. The Replicability crisis shows a system without transparency does not work. Those in charge of setting scientific norms and standards should strive to increase transparency, bolster our confidence that we trust published research. However many high level decisions in science are made with a different goal in mind: to increase impact. Professional societies and journals prioritise publishing attention-grabbing findings to boost visibility and prestige. Seeking eminence is at odds with scientific value and affects scientific gatekeepers’ decisions. Editors influenced by the status of submitting authors or prestige of institutions violate the basic premise of science. Science work should be evaluated on its own merit, irrespective of the source. Lack of transparency in science is a direct consequence of the corrupting influence of eminence seeking. Gatekeepers control incentive structures that shape individual researchers’ behaviour. Therefore they have a bigger responsibility to uphold scientific values and most power to erode those values. Individual researchers’ desire for eminence threatens the integrity of the research process. All researchers are human and desire recognition for their work. However, there is no good reason to amplify this human drive and encourage scientists to seek fame. The glorification of eminence also reinforces inequalities in science. If scientists are evaluated based on ability to attract attention, those with the most prestige will be heard the loudest. Certain groups are overrepresented at a high level of status. Eminence propagates privilege and raises barriers to entry for others. How should scientific merit be evaluated? What does this mean for committees to select one or few winners? First, it is important to admit that a larger number of scientists meet the objective criteria for these recognitions (i.e., do sound science). It is also important to admit that selection of one or few individuals is not based on merit but on preference or partiality. It is fine to select or recognise members who exemplify their values, but this should not be confused with exceptional scientific merit. Whenever possible (for tenure, promotion and when journal space or grant fund permits), we should attempt to reward scientists whose work reaches a more objective threshold of scientific rigour or soundness instead of selecting scientists based on fame. Abstract The drive for eminence is inherently at odds with scientific values, and insufficient attention to this problem is partly responsible for the recent crisis of confidence in psychology and other sciences. The replicability crisis has shown that a system without transparency doesn’t work. The lack of transparency in science is a direct consequence of the corrupting influence of eminence-seeking. If journals and societies are primarily motivated by boosting their impact, their most effective strategy will be to publish the sexiest findings by the most famous authors. Humans will always care about eminence. Scientific institutions and gatekeepers should be a bulwark against the corrupting influence of the drive for eminence, and help researchers maintain integrity and uphold scientific values in the face of internal and external pressures to compromise. One implication for evaluating scientific merit is that gatekeepers should attempt to reward all scientists whose work reaches a more objective threshold of scientific rigor or soundness, rather than attempting to select the cream of the crop (i.e., identify the most “eminent”). APA Style Reference Vazire, S. (2017). Against eminence. https://doi.org/10.31234/osf.io/djbcw\nYou may also be interested in The Gender Gap: Who Is (and Is Not) Included on Graduate-Level Syllabi in Social/Personality Psychology (Skitka et al., 2020) The Focus on Fame distorts Science (Innes-Ker, 2017) Fame: I’m Skeptical (Ferreira, 2017) Let’s Look at the Big Picture: A System-Level Approach to Assessing Scholarly Merit (Pickett, 2017) “Fame” is the Problem: Conflation of Visibility With Potential for Long-Term Impact in Psychological Science (Shiota, 2017) Why a Focus on Eminence is Misguided: A Call to Return to Basic Scientific Values (Corker, 2017) Am I Famous Yet? Judging Scholarly Merit in Psychological Science: An Introduction (Sternberg, 2016) Unequal effects of the COVID-19 pandemic on scientists (Myers et al., 2019) Giving Credit Where Credit’s Due: Why It’s So Hard to Do in Psychological Science (Simonton, 2016) Eminence and Omniscience: Statistical and Clinical Prediction of Merit (Foss, 2016) Taking Advantage of Citation Measures of Scholarly Impact: Hip Hip h Index! (Ruscio, 2016) Improving Departments of Psychology (Diener, 2016) Varieties of Fame in Psychology (Roediger III, 2016) Scientific Eminence: Where Are the Women? (Eagly \u0026amp; Miller, 2016) Intrinsic and Extrinsic Science: A Dialectic of Scientific Fame (Feist, 2016) Giving Credit Where Credit’s Due: Why It’s So Hard to Do in Psychological Science (Simonton, 2016) Main Takeaways: The article begins with indicators of scientific achievement before discussing some important precautions about the implications of these measures. One measure is the lifetime career award that is based on the scientist’s cumulative record that spans over their career lifetime. This indicator is perceived as more reliable than early career award, as an early career award is founded on a much smaller and likely less representative sample of that career. This limitation also applies to an award for a single publication such as best article recognition, as it may not predict the citations that the article later receives in the literature. The article argues that it makes most sense to concentrate on assessing lifetime contributions to psychological science. Another measure to assess scientific achievement is an invitation to write a definitive handbook chapter, as it indicates that the scientist is a widely recognised expert on this specific topic. A final measure to indicate that a scientist is well known is the simple count of total citations. However, this may be assessed by several alternative citation measures (e.g. h-index and i10). However, these measures suffer from poor predictive validity, as the relationship between various predictors and criterion variables tend to be small to moderate, not large enough to make fine discriminations among scientists. In turn, these predictive utilities are contaminated with other potentially biasing factors (e.g. gender, ethnicity, specialty, methodology, ideology, affiliation, and publication type). In addition, these measures suffer from interjudge reliability, as a psychologist receives mixed reviews after submitting a manuscript to a high-impact journal. For instance, one referee recommends the author to publish the manuscript with minor revision, while another advises an outright rejection. This frustrates the author and the editor, but proves the discipline lacks a strong consensus on what contributes to science or their specific area. This lack of agreement should be reduced if evaluators operate with a larger sample of contributions such as lifetime career awards. However, the same problem from interjudge reliability applies to lifetime awards, as one committee member would argue that the scientist deserves this award, while a minority of the committee may disagree with the final decision and argue another scientist deserves this award. In the end, “the committee chair can then only assure the dissenters that their preferred candidate will most definitely emerge the winner in the next award cycle.” (p.890). Quote “Eminence in any scientific discipline will therefore be directly proportional to actual contributions. This expectation would be especially strong given that scientists purport to make inferences based on empirical fact and logical reasoning. Not only would peer assessments prove highly objective, but scientists’ self-assessments of their own contributions should depart relatively little from colleagues in the best position to evaluate their work. In short, a strong consensus should permeate all evaluations. One specific manifestation of this consensus would appear in the awards and honors bestowed on those scientists who have devoted a whole career to producing high-impact work. That is what would happen ideally, but does that happen in fact? And even if the ideal is closely approximated in most sciences, is it also reasonably attained in psychological science?” (p.888) Abstract More than a century of scientific research has shed considerable light on how a scientist’s contributions to psychological science might be best assessed and duly recognized. This brief overview of that empirical evidence concentrates on recognition for lifetime career achievements in psychological science. After discussing both productivity and citation indicators, the treatment turns to critical precautions in the application of these indicators to psychologists. These issues concern both predictive validity and interjudge reliability. In the former case, not only are the predictive validities for standard indicators relatively small, but the indicators can exhibit important non-merit-based biases that undermine validity. In the latter case, peer consensus in the evaluation of scientific contributions is appreciably lower in psychology than in the natural sciences, a fact that has consequences for citation measures as well. Psychologists must therefore exercise considerable care in judging achievements in psychological science—both their own and those of others. APA Style Reference Simonton, D. K. (2016). Giving credit where credit’s due: Why it’s so hard to do in psychological science. Perspectives on Psychological Science, 11(6), 888-892. https://doi.org/10.1177/1745691616660155\nYou may also be interested in The Focus on Fame distorts Science (Innes-Ker, 2017) Fame: I’m Skeptical (Ferreira, 2017) Let’s Look at the Big Picture: A System-Level Approach to Assessing Scholarly Merit (Pickett, 2017) “Fame” is the Problem: Conflation of Visibility With Potential for Long-Term Impact in Psychological Science (Shiota, 2017) Why a Focus on Eminence is Misguided: A Call to Return to Basic Scientific Values (Corker, 2017) Am I Famous Yet? Judging Scholarly Merit in Psychological Science: An Introduction (Sternberg, 2016) Against Eminence (Vazire, 2017) Eminence and Omniscience: Statistical and Clinical Prediction of Merit (Foss, 2016) Taking Advantage of Citation Measures of Scholarly Impact: Hip Hip h Index! (Ruscio, 2016) Improving Departments of Psychology (Diener, 2016) Varieties of Fame in Psychology (Roediger III, 2016) Scientific Eminence: Where Are the Women? (Eagly \u0026amp; Miller, 2016) Intrinsic and Extrinsic Science: A Dialectic of Scientific Fame (Feist, 2016) Eminence and Omniscience: Statistical and Clinical Prediction of Merit (Foss, 2016) Main Takeaways: The author asks the reader to take the perspective of the individual who has the final say in making a tenure, promotion, or hiring decision. The author also asks that you imagine the difference between the fallible human state we are in on such an occasion and what it would be like to be omniscient when making such decisions. The author argues that there are two types of eminence: Deep eminence and surface eminence. The former refers to you as omniscient, you know, and future generations will know, if the candidate is doing work moving some part of the discipline toward “capital-T Truth”, while the latter is the basis for our mere-mortal judgment of ‘tenurability’ in a candidate. Citation data predicts early prominence at least at extremes of citation distribution. However, longitudinal studies with large sample sizes are required to investigate this question. Quote “Diener suggests that the discipline will progress more rapidly if the most highly productive individuals are allowed to be even more productive, which will occur by further unburdening these worthies from their teaching responsibilities. I have three quick reactions to this point. One is that, to a considerable extent, his proposal has already been adopted. The typical teaching assignments in research universities are very substantially lower than they were in the days when modern experimental psychology took off. And it is not unusual to see less productive scholars with teaching assignments that involve, say, larger sections of undergraduates, as well as carrying out other service activities. Second, in many places it is still possible for successful grantees to “buy out” some of their teaching time. By definition, these are members of the publishing crew or they would not have the grant money that allows this exchange. And third, and most importantly, let’s revisit what a university is for. One of its primary goals is to develop the human capital of society. In order to keep faith with the funders of (at least the public) universities, we should be leery of allowing that mission to slip too low in our goal hierarchy.” (p.914) Abstract In this article, I review, comment upon, and assess some of the suggestions for evaluating scientific merit as suggested by contributors to this symposium. I ask the reader to take the perspective of the individual who has the final say in making a tenure, promotion, or hiring decision. I also ask that one imagine the difference between the fallible human state we are in on such an occasion and what it would be like to be omniscient when making such decisions. After adopting the terminology of “deep” and “surface” eminence, I consider what an omniscient being would take into account to determine eminence and to guide decision-making. After discussing how some proposed improvements in assessing merit might move us closer to wise decisions, I conclude by noting that both data and judgment are, and will continue to be, necessary. A clerk cannot determine eminence. APA Style Reference Foss, D. J. (2016). Eminence and omniscience: Statistical and clinical prediction of merit. Perspectives on Psychological Science, 11(6), 913-916. https://doi.org/10.1177/1745691616662440\nYou may also be interested in The Focus on Fame distorts Science (Innes-Ker, 2017) Fame: I’m Skeptical (Ferreira, 2017) Let’s Look at the Big Picture: A System-Level Approach to Assessing Scholarly Merit (Pickett, 2017) “Fame” is the Problem: Conflation of Visibility With Potential for Long-Term Impact in Psychological Science (Shiota, 2017) Why a Focus on Eminence is Misguided: A Call to Return to Basic Scientific Values (Corker, 2017) Am I Famous Yet? Judging Scholarly Merit in Psychological Science: An Introduction (Sternberg, 2016) Against Eminence (Vazire, 2017) Giving Credit Where Credit’s Due: Why It’s So Hard to Do in Psychological Science (Simonton, 2016) Taking Advantage of Citation Measures of Scholarly Impact: Hip Hip h Index! (Ruscio, 2016) Improving Departments of Psychology (Diener, 2016) Varieties of Fame in Psychology (Roediger III, 2016) Scientific Eminence: Where Are the Women? (Eagly \u0026amp; Miller, 2016) Intrinsic and Extrinsic Science: A Dialectic of Scientific Fame (Feist, 2016) Improving Departments of Psychology (Diener, 2016) Main Takeaways: Although we have excellent universities, our selection-based approach to talent and productivity is incomplete for creating the very best departments. What can we do to improve our department? Current approach to excellence in scholarship rests largely on hiring the right individuals who have the right talent and motivation. Abstract Our procedures for creating excellent departments of psychology are based largely on selection—hiring and promoting the best people. I argue that these procedures have been successful, but I suggest the implementation of policies that I believe will further improve departments in the behavioral and brain sciences. I recommend that we institute more faculty development programs attached to incentives to guarantee continuing education and scholarly activities after the Ph.D. degree. I also argue that we would do a much better job if we more strongly stream our faculty into research, education, or service and not expect all faculty members to carry equal responsibility for each of these. Finally, I argue that more hiring should occur at advanced levels, where scholars have a proven track record of independent scholarship. Although these practices will be a challenge to implement, institutions do ossify over time and thus searching for ways to improve our departments should be a key element of faculty governance. APA Style Reference Diener, E. (2016). Improving departments of psychology. Perspectives on Psychological Science, 11(6), 909-912. https://doi.org/10.1177/1745691616662865\nYou may also be interested in The Focus on Fame distorts Science (Innes-Ker, 2017) Fame: I’m Skeptical (Ferreira, 2017) Let’s Look at the Big Picture: A System-Level Approach to Assessing Scholarly Merit (Pickett, 2017) “Fame” is the Problem: Conflation of Visibility With Potential for Long-Term Impact in Psychological Science (Shiota, 2017) Why a Focus on Eminence is Misguided: A Call to Return to Basic Scientific Values (Corker, 2017) Am I Famous Yet? Judging Scholarly Merit in Psychological Science: An Introduction (Sternberg, 2016) Against Eminence (Vazire, 2017) Giving Credit Where Credit’s Due: Why It’s So Hard to Do in Psychological Science (Simonton, 2016) Eminence and Omniscience: Statistical and Clinical Prediction of Merit (Foss, 2016) Taking Advantage of Citation Measures of Scholarly Impact: Hip Hip h Index! (Ruscio, 2016) Varieties of Fame in Psychology (Roediger III, 2016) Scientific Eminence: Where Are the Women? (Eagly \u0026amp; Miller, 2016) Intrinsic and Extrinsic Science: A Dialectic of Scientific Fame (Feist, 2016) Varieties of Fame in Psychology (Roediger III, 2016) Main Takeaways: How do we determine the quality and impact of an individual and their research in psychological science? We use progression of fame to indicate how the system works. The author goes on to discuss routes to fame, but emphasizes that more likely than not, it is ‘local’ fame we are achieving that is not long lasting across time. Once a researcher succeeds in graduate school, obtains a job in academia, industry or research institute, then it is time to move up in one’s career. Below are different actions one could take: Quote “Fame is local, both by area and by time. This point has been made by scholars in other contexts (usually politics or other historical figures), but it is as true of psychology as of any other field. As with other writers in this series, the best advice is to do the research, the writing, and the teaching that you are passionate about. Fame may or may not come for a time, but should not be an all-consuming concern. Even if it comes, it will soon fade away” (p.887) Abstract Fame in psychology, as in all arenas, is a local phenomenon. Psychologists (and probably academics in all fields) often first become well known for studying a subfield of an area (say, the study of attention in cognitive psychology, or even certain tasks used to study attention). Later, the researcher may become famous within cognitive psychology. In a few cases, researchers break out of a discipline to become famous across psychology and (more rarely still) even outside the confines of academe. The progression is slow and uneven. Fame is also temporally constricted. The most famous psychologists today will be forgotten in less than a century, just as the greats from the era of World War I are rarely read or remembered today. Freud and a few others represent exceptions to the rule, but generally fame is fleeting and each generation seems to dispense with the lessons learned by previous ones to claim their place in the sun. APA Style Reference Roediger III, H. L. (2016). Varieties of fame in psychology. Perspectives on Psychological Science, 11(6), 882-887. https://doi.org/10.1177/1745691616662457\nYou may also be interested in The Focus on Fame distorts Science (Innes-Ker, 2017) Fame: I’m Skeptical (Ferreira, 2017) Let’s Look at the Big Picture: A System-Level Approach to Assessing Scholarly Merit (Pickett, 2017) “Fame” is the Problem: Conflation of Visibility With Potential for Long-Term Impact in Psychological Science (Shiota, 2017) Why a Focus on Eminence is Misguided: A Call to Return to Basic Scientific Values (Corker, 2017) Am I Famous Yet? Judging Scholarly Merit in Psychological Science: An Introduction (Sternberg, 2016) Against Eminence (Vazire, 2017) Giving Credit Where Credit’s Due: Why It’s So Hard to Do in Psychological Science (Simonton, 2016) Eminence and Omniscience: Statistical and Clinical Prediction of Merit (Foss, 2016) Taking Advantage of Citation Measures of Scholarly Impact: Hip Hip h Index! (Ruscio, 2016) Improving Departments of Psychology (Diener, 2016) Scientific Eminence: Where Are the Women? (Eagly \u0026amp; Miller, 2016) Intrinsic and Extrinsic Science: A Dialectic of Scientific Fame (Feist, 2016) Scientific Eminence: Where Are the Women? (Eagly \u0026amp; Miller, 2016) ⌺ Main Takeaways: Women’s scientific contributions in psychology may not be as numerous or influential as those of men. What is the magnitude of the current eminence gender gap? Women’s modest inroads into this list of eminent psychologists deserve respect, given this lag between obtaining a doctorate and attaining eminence and formidable barriers that women once faced in pursuing scientific careers. Psychologists judge eminence by observing signs such as memberships in selective societies, career scientific achievement awards and honorary degrees. Do men exceed women on both quantity and impact of their publication underlies h index? Are these metrics tainted by unfair bias against women? Does the h-index identify potential socio-cultural and individual causes of the eminence gap? Women’s publications are cited less than men. This gap was larger in psychology. Women received 20% fewer in psychology varying across subfields. Gender gap on h-index and similar metrics has two sources: women publish less than men and articles receive fewer citations. Metrics assessing scientific eminence may be tainted by prejudicial bias against female scientists in obtaining grant support, publishing papers, or gaining citations of published papers. If psychologists are disadvantaged in publishing their work, bias may be limited to culturally masculine topics or male-dominated research areas. Such topics and are no doubt becoming rarer in psychology, given women receive most US doctorates. Men’s greater overall citations reflect higher rates of self-citation, women self-cite less often. This reflects men’s larger corpus of their own citable papers. Prejudicial gender bias is limited and presents ambiguity given most studies are correlational instead of experimental. Little is known about possible gender bias in awards for scientific eminence such as science prizes and honorary degrees, which are imperfect indicators of the importance of scientists’ contributions. Female scientists’ lesser rates of publication and citation reflect causes other than biases. Broader socio-cultural factors shape individual identities and motivations. Nature and nurture affects role occupancies so men and women are differently distributed into social roles. Women excel in communal qualities of warmth and concern for others and for men to excel in agentic qualities of assertiveness and mastery. Women are over-represented in less research intensive but more in teaching-intensive ranks and part-time positions. Gender norms discourage female agency may disadvantage to gain status in departmental and disciplinary networks and garner resources. Stereotypes erode women’s confidence in ability to become highly successful scientists. Eminence gender gaps in psychology and other sciences shrink further over time as new cohorts of scientists advance in their careers. Women’s representation among PhD earners has increased dramatically over recent decades. Abstract Women are sparsely represented among psychologists honored for scientific eminence. However, most currently eminent psychologists started their careers when far fewer women pursued training in psychological science. Now that women earn the majority of psychology Ph.D.’s, will they predominate in the next generation’s cadre of eminent psychologists? Comparing currently active female and male psychology professors on publication metrics such as the h index provides clues for answering this question. Men outperform women on the h index and its two components: scientific productivity and citations of contributions. To interpret these gender gaps, we first evaluate whether publication metrics are affected by gender bias in obtaining grant support, publishing papers, or gaining citations of published papers. We also consider whether women’s chances of attaining eminence are compromised by two intertwined sets of influences: (a) gender bias stemming from social norms pertaining to gender and to science and (b) the choices that individual psychologists make in pursuing their careers. APA Style Reference Eagly, A. H., \u0026amp; Miller, D. I. (2016). Scientific eminence: Where are the women?. Perspectives on Psychological Science, 11(6), 899-904. https://doi.org/10.1177/1745691616663918\nYou may also be interested in The Gender Gap: Who Is (and Is Not) Included on Graduate-Level Syllabi in Social/Personality Psychology (Skitka et al., 2020) Leveraging a collaborative consortium model of mentee/mentor training to foster career progression of under-represented post-doctoral researchers and promote institutional diversity and inclusion (Risner et al., 2020) The Focus on Fame distorts Science (Innes-Ker, 2017) Fame: I’m Skeptical (Ferreira, 2017) Let’s Look at the Big Picture: A System-Level Approach to Assessing Scholarly Merit (Pickett, 2017) “Fame” is the Problem: Conflation of Visibility With Potential for Long-Term Impact in Psychological Science (Shiota, 2017) Why a Focus on Eminence is Misguided: A Call to Return to Basic Scientific Values (Corker, 2017) Am I Famous Yet? Judging Scholarly Merit in Psychological Science: An Introduction (Sternberg, 2016) Against Eminence (Vazire, 2017) Varieties of Fame in Psychology (Roediger III, 2016) Eminence and Omniscience: Statistical and Clinical Prediction of Merit (Foss, 2016) Taking Advantage of Citation Measures of Scholarly Impact: Hip Hip h Index! (Ruscio, 2016) Improving Departments of Psychology (Diener, 2016) Giving Credit Where Credit’s Due: Why It’s So Hard to Do in Psychological Science (Simonton, 2016) Intrinsic and Extrinsic Science: A Dialectic of Scientific Fame (Feist, 2016) Intrinsic and Extrinsic Science: A Dialectic of Scientific Fame (Feist, 2016) Main Takeaways: Fame and desire for a legacy provides meaning in one’s existence. Scientists are not the only group driven by a desire to be famous. What does it mean to be famous in science and how do we measure scientific fame? There is intrinsic motivation to follow one’s interest, curiosity, gut and intuition for important and undiscovered topics, while there is extrinsic motivation to follow money, grants and/or what is being published in top-tier journals. There is a continuum of fame: On one end, there is ‘mundane or imitative’ science - someone conducts a replication or slight advance of already published research. Its impact is little more than personal, influencing the person conducting it but impacts few other people. ‘Normal’ science is when one takes an idea or theory from within an existing theoretical paradigm and tests it. Most scientific research falls in the normal category. Its impact is regional and/or narrowly national. We have ‘creative science’ - this is moderate to high impact science, heavily cited by other scholars in the field and sometimes garner regional, national, or even international awards. Finally, there is ‘rare transformational/revolutionary’ science that changes the entire field and whose impact is both internal and historic. If a peer-reviewed article is the currency of scientific career, funding is its bread and butter. Research is not possible without finding increasing amounts of money (for most scientists). Generative publications are not only highly cited themselves but also generate other works that are highly cited. If they generate enough new works of high impact, the original publication can be transformative. Once published, articles are either ignored or exert some kind of influence on the field. Publication and citation counts are reliable and robust measures of creative output in science. Scientists could cite any and all work that affects their current research,but this appears to not be the case. Papers with several authors are more likely to be cited due to greater exposure. Other more integrated measures of productivity have been developed to correct some problems. H index: when an author of ‘N’ articles has a number of publications cited at least X number of times and the rest of articles receive few citations. Traditional and citation-based metrics are impacted by time lag between when an article is published and when citation indexes catch up. Altmetrics measures impact derived from online and social media data. Altmetrics assesses article outcomes such as: the number of times an article is viewed, liked, downloaded, discussed, saved, cited, tweeted, blogged, or recommended. Altmetric data is faster than traditional citation count and h-index because it is counted immediately upon publication with real-time updates at any given time. Publications are necessary but not sufficient conditions for citations, those who publish the most are cited the most. It is important to remember there are individuals who publish a lot but not get cited, and those who publish not much but are heavily cited. One can do very good work but the field may or may not pay much attention to it. Many heavily cited papers make a methodological or statistical advance and are of practical, not theoretical, importance. Psychologists would better understand the difference between individual success and disciplinary success. What is good for one’s career is not always what is good for science. Researchers have begun to make recommendations to authors, editors, and instructors of research methodology to increase replicability such as pre-registering predictions by increasing transparency and clearly justify sample size and publish raw data. Most psychological scientists find a way to marry their intrinsic interests with its extrinsic reward and impact. Quote “Finding that sweet spot between the two extremes of joy and recognition may be the best definition of success in science that we can come up with. So if I were to recommend a strategy for up and coming scientists it might be this: develop a research program that combines intrinsic fascination and interest with extrinsic recognition and career advancement. Follow your heart and your head. Explore and develop the riskier, more potentially transformative and creative lines of research at the same time that you develop the safer, more fundable ideas. This might occur by developing two separate lines of research, or better yet, by finding one research program that is both intrinsically motivated and then other people also recognize, appreciate, and reward you for it. If you can do both of these, you stand the best chance of surviving, succeeding, and maybe even becoming famous in the competitive world of academic psychological science” (p.897) Abstract In this article, I argue that scientific fame and impact exists on a continuum from the mundane to the transformative/ revolutionary. Ideally, one achieves fame and impact in science by synthesizing two extreme career prototypes: intrinsic and extrinsic research. The former is guided by interest, curiosity, passion, gut, and intuition for important untapped topics. The latter is guided by money, grants, and/or what is being published in top-tier journals. Assessment of fame and impact in science ultimately rests on productivity (publication) and some variation of its impact (citations). In addition to those traditional measures of impact, there are some relatively new metrics (e.g., the h index and altmetrics). If psychology is to achieve consensual cumulative progress and better rates of replication, I propose that upcoming psychologists would do well to understand that success is not equal to fame and that individual career success is not necessarily the same as disciplinary success. Finally, if one is to have a successful and perhaps even famous career in psychological science, a good strategy would be to synthesize intrinsic and extrinsic motives for one’s research. APA Style Reference Feist, G. J. (2016). Intrinsic and extrinsic science: A dialectic of scientific fame. Perspectives on Psychological Science, 11(6), 893-898. https://doi.org/10.1177/1745691616660535\nYou may also be interested in The Focus on Fame distorts Science (Innes-Ker, 2017) Fame: I’m Skeptical (Ferreira, 2017) Let’s Look at the Big Picture: A System-Level Approach to Assessing Scholarly Merit (Pickett, 2017) “Fame” is the Problem: Conflation of Visibility With Potential for Long-Term Impact in Psychological Science (Shiota, 2017) Why a Focus on Eminence is Misguided: A Call to Return to Basic Scientific Values (Corker, 2017) Am I Famous Yet? Judging Scholarly Merit in Psychological Science: An Introduction (Sternberg, 2016) Against Eminence (Vazire, 2017) Giving Credit Where Credit’s Due: Why It’s So Hard to Do in Psychological Science (Simonton, 2016) Scientific Eminence: Where Are the Women? (Eagly \u0026amp; Miller, 2016) Eminence and Omniscience: Statistical and Clinical Prediction of Merit (Foss, 2016) Taking Advantage of Citation Measures of Scholarly Impact: Hip Hip h Index! (Ruscio, 2016) Varieties of Fame in Psychology (Roediger III, 2016) Improving Departments of Psychology (Diener, 2016) Scientific inbreeding and same-team replication: Type D personality as an example (Ioannidis, 2012) Main Takeaways: Replication is fundamental for the scientific process. However, this is fraught with difficulties, making researchers avoid any form of replication. However, there is one specific type of replication that researchers need to be wary of: same-team replications. This type of replication is more likely to lead to spurious confirmation as a result of allegiance and confirmation bias. Consistent results are seen as a sign of being a good scientist, leaving no room for objection. As a result, proponents of original theories can shape the literature and control the publication venues that they largely select. In turn, this moulds the results, wording and interpretation of studies that are eventually published. Psychological science may be prone to implicit bias and a desire for a ‘clean’ narrative, as opposed to reflecting the messy reality. There is large flexibility in definitions, uses of cut-offs, modelling and statistical handling of the data, leading to large room for exploratory analyses and vibration of effects (cf. Simmons et al., 2011). Researchers in psychological science perform research in single teams, as opposed to larger collaborative ones. Hence, it’s common practice for these researchers to keep their data, protocols and analyses private. “Nevertheless, in fields that have high levels of inbreeding and one team has the lion\u0026#39;s share of the major papers, it is likely that submitted papers will hit either one of the team members or an affiliate or devoted follower in the peer-review stage.” (p.409). Conscious, subconscious and unconscious bias are very common and related to the scientific discovery process. Independent replication is necessary to alleviate bias and understand true effects of associations and interventions. Failure to replicate is not a bad outcome if replications are conducted appropriately and with proper attention to study design and conduct. The scientific community (i.e. investigators, funders, reviewers and editors) should facilitate and endorse independent replications. Also, the scientific community should be protecting and promoting this independence, while discouraging and preventing obedient and obliged replications. Quote “In several scientific fields, funders, journals, and peers may create disincentives towards replication efforts, considering them second-rate, me-too efforts unworthy of funding and prestigious publication.” (p. 408) Abstract Replication is essential for validating correct results, sorting out false-positive early discoveries, and improving the accuracy and precision of estimated effects. However, some types of seemingly successful replication may foster a spurious notion of increased credibility, if they are performed by the same team and propagate or extend the same errors made by the original discoveries. Besides same-team replication, replication by other teams may also succumb to inbreeding, if it cannot fiercely maintain its independence. These patterns include obedient replication and obliged replication. I discuss these replication patterns in the context of associations and effects in the psychological sciences, drawing from the criticism of Coyne and de Voogd of the proposed association between type D personality and cardiovascular mortality and other empirical examples. APA Style Reference Ioannidis, J. P. (2012). Scientific inbreeding and same-team replication: type D personality as an example. Journal of psychosomatic research, 73(6), 408-410. https://doi.org/10.1016/j.jpsychores.2012.09.014\nYou may also be interested in How scientists can stop fooling themselves (Bishop, 2020b) The Statistical Crisis in Science (Gelman \u0026amp; Loken, 2014) Only Human: Scientists, Systems, and Suspect Statistics A review of: Improving Scientific Practice: Dealing With The Human Factors, University of Amsterdam, Amsterdam, September 11, 2014 (Hardwicke et al., 2014) Rein in the four horsemen of irreproducibility (Bishop, 2019) Seven Steps Toward Transparency and Replicability in Psychological Science (Lindsay, 2020) False-Positive Psychology: Undisclosed Flexibility in Data Collection and Analysis Allows Presenting Anything as Significant (Simmons et al., 2011) The Nine Circles of Scientific Hell (Neuroskeptic, 2012) Main Takeaways: Neuroskeptic provides a humorous take on Dante’s nine circles of hell, where each circle corresponds to a different questionable research practice that becomes increasingly problematic for scientific integrity. The first circle is Limbo, which is reserved for people who turn a blind eye on scientific sins or reward others who engage in them (e.g., by giving them grants). The second circle is Overselling, which is reserved for people who overstate the importance of their work to get grants or write better papers. The third circle is Post-hoc Storytelling- the scientist fires arrows at random; if a finding is noticed, a demon will explain at length or ramble that it aimed for this precise finding all along. The fourth circle is P-value Fishing, which is reserved for those who “try every statistical test in the book” until they get a p-value of less than .05. The fifth circle is Creative Use of Outliers, which is reserved for those who exclude “inconvenient” data points. The sixth circle is Plagiarism- or presenting another individual’s work as their own work. The seventh circle is the Non-publication of Data- scientists can free themselves from this circle if they write an article about it; however, the drawers containing these articles are locked. The eighth circle is the Partial Publication of Data, where sinners are chased at random and prodded by demons, in analogy of the selective reporting and massaging of data. The ninth circle is Inventing Data, which is reserved for Satan himself (i.e., people who make up their data). Abstract In the spirit of Dante Alighieri’s Inferno, this paper takes a humorous look at the fate that awaits scientists who sin against best practice. APA Style Reference Neuroskeptic. (2012). The nine circles of scientific hell. Perspectives on Psychological Science, 7(6), 643-644. https://doi.org/10.1177/1745691612459519\nYou may also be interested in Psychologists Are Open to Change, yet Wary of Rules (Fuchs et al., 2012) Six principles for assessing scientists for hiring, promotion, and tenure (Naudet et al, 2018) Only Human: Scientists, Systems, and Suspect Statistics A review of: Improving Scientific Practice: Dealing With The Human Factors, University of Amsterdam, Amsterdam, September 11, 2014 (Hardwicke et al., 2014) Rein in the four horsemen of irreproducibility (Bishop, 2019) Check for publication integrity before misconduct (Grey et al., 2020) Main Takeaways: The integrity of a publication can be compromised via accidental errors such as typos, transcription errors or incorrect analyses, or via intentional errors such as image manipulation, data falsification and plagiarism. “How publication integrity was compromised is secondary to whether the paper is reliable. Unreliable data or conclusions are problems irrespective of the cause” (p. 167) “Resources for editors ... focus on how to manage communications, rather than on how to evaluate reliability and validity. The net effect is inaction: readers remain uninformed about potential problems with a paper, and that can lead to wasted time and resources, and sometimes put patients at risk” (p. 167). The authors believe that a major obstacle to evaluating the integrity of publications is a lack of tools. Accordingly, they present the REAPPRAISED checklist to help readers, journal editors and anyone else assess a papers publication integrity. This was originally designed for clinical and animal studies but can also be used more broadly. Irrespective of whether misconduct is suspected, this checklist should be used to facilitate the identification and correction of flawed papers, thus preventing wasted time/resources and protecting patients. The REAPPRAISED checklist facilitates evaluation through 11 categories, covering: ethical oversight and funding, research productivity and investigator workload, validity of randomisation, plausibility of results and duplicate data reporting. The authors would like to see the checklist used during both manuscript review and post-publication evaluation. They believe that since the checklist separates the assessment of publication integrity from the investigation of research misconduct, it will speed up evaluations. It could also be published alongside retractions and corrections. “If multiple concerns are identified [by the checklist], or the concerns identified are often associated with misconduct, the entire body of an author’s work should be systematically assessed” (p. 169). Quote “The use of REAPPRAISED will lead to more detailed, efficient, consistent and transparent evaluations of publication integrity, thus faster and more accurate reporting of corrections and retractions...People using the tool will be able to help refine it as they gain experience, and it will help them to develop standards to assess the integrity of publications and act accordingly” (p.169). Abstract A tool that focuses on papers — not researcher behaviour — can help readers, editors and institutions assess which publications to trust. APA Style Reference Grey, A., Bolland, M. J., Avenell, A., Klein, A. A., \u0026amp; Gunsalus, C. K. (2020). Check for Publication Integrity before Misconduct. Nature, 577, 167–169. doi:10.1038/d41586-019-03959-6\nYou may also be interested in Signalling the trustworthiness of science should not be a substitute for direct action against research misconduct (Kornfeld \u0026amp; Titus, 2020) Stop ignoring misconduct (Kornfeld \u0026amp; Titus, 2016) Fallibility in Science: Responding to Errors in the Work of Oneself and Others (Bishop, 2018) Publication Pressure and Scientific Misconduct in Medical Scientists (Tijdink et al., 2014) Signalling the trustworthiness of science (Jamieson et al., 2020) Reply to Kornfeld and Titus: No distraction from misconduct (Jamieson et al., 2020) Credibility of preprints: an interdisciplinary survey (Soderberg et al., 2020) Main Takeaways: The present study conducted a survey to collect data about cues that could be displayed on preprints to help researchers assess their credibility. Preprints are not part of the long peer-review journal process, but the findings can be made available sooner, thus encouraging new work and discoveries by others. This can only happen if preprints are judged as credible. There is some skepticism about the credibility of preprints, particularly in fields for which the concept is new. Researchers need to keep up with scholarly work with the emerging evidence in the field and explore new ideas that might inform their research. However, time is a valuable commodity to researchers and effective filters are required to decide whether the researcher should continue to review the research further or stop and move on. There has no research conducted to assess the credibility cues on preprints. The authors investigated to which cues are considered important for credibility judgments about preprints and how this varied across disciplines and career stages. Method: 3759 researchers from several disciplines (e.g. medicine and psychology) were given a final survey that included questions in four categories: engagement information (e.g. familiarity and favourability of preprints), importance of cues for credibility (e.g. links to available data and independent verification of findings), credibility of service characteristics (e.g. how the user engages with service), and demographics (e.g. age and discipline). Results: Most disciplines, especially the social sciences and psychology, favoured preprints. Only 51% of people in the field of medicine favoured preprints. Graduate students and post-doctoral students favoured preprints the most, while full professors favoured preprints the least. Results: Individuals who favour and use preprints tended to judge the credibility of preprints as highly important based on cues related to information about open science content and independent verification of author claims, whereas they viewed peer review information and author information as less important. The opposite pattern was observed for individuals who do not favour or use preprints. Results: In early 2020, very few preprint services mention these highly important cues. The authors concluded that cue related to openness and independent verification of author assertions were rated more highly than cues related to author identities and peer review and usage indicators. The opposite pattern of findings were observed for researchers more skeptical of pre-prints. There was a broad agreement that transparency of research content (e.g. pre-registration) and evidence of independent verification of content and research claims were the most important to assess credibility of preprints. This pattern was common across all disciplines. These shared sets of cues can be applied across scholarly preprint communities to improve the assessment of research credibility. Preprint services could improve support of preprint readers’ assessment of research credibility by implementing some of the highly relevant cues with each preprint. Abstract Preprints increase accessibility and can speed scholarly communication if researchers view them as credible enough to read and use. Preprint services do not provide the heuristic cues of a journal’s reputation, selection, and peer-review processes that, regardless of their flaws, are often used as a guide for deciding what to read. We conducted a survey of 3759 researchers across a wide range of disciplines to determine the importance of different cues for assessing the credibility of individual preprints and preprint services. We found that cues related to information about open science content and independent verification of author claims were rated as highly important for judging preprint credibility, and peer views and author information were rated as less important. As of early 2020, very few preprint services display any of the most important cues. By adding such cues, services may be able to help researchers better assess the credibility of preprints, enabling scholars to more confidently use preprints, thereby accelerating scientific communication and discovery. APA Style Reference Soderberg, C.K., Errington, T.M., \u0026amp; Nosek, B.A. (2020). Credibility of preprints: an interdisciplinary survey of researchers. Royal Society of Open Science, 7, 201520. http://doi.org/10.1098/rsos.201520\nYou may also be interested in Add references soon The digital Archaeologists (Perkel, 2018) Main Takeaways: Computation plays a key and larger part in science, scientific articles rarely include their underlying code. Even when codes are included, it can be difficult for others and the original author to execute it. Programming languages evolve, together with the computing environment and codes that work flawlessly one day can fail the next. Researchers need to maximise code reusability in the future. Reproducibility-minded scientists need to up their documentation games. There can be deficiencies in code organisation and code fragments can be run out of order. These can be taken care of by breaking code into modules and implementing code tests and using version control to track changes to the code and note which version produced each set of results. New versions of software language are created that are not backwards compatible, making it difficult to reproduce the results. Finding the code does not mean it is obvious on how to use the code. It is important to document key details of data normalisation, to make it easier to reproduce the findings. Developing resources takes time to clean and document code, create test suites, archive data sets and reproduce computational environments. In addition, there are few incentives to conduct these behaviours and there is a lack of consensus on what a reproducible article should look like. Finally, computational systems continue to evolve and it is getting harder to predict which strategies will endure. Reproducibility ranges from scientists repeating their own analyses to peer reviewers showing that the code works and applying algorithms to fresh data. The best thing to do is release your source code, so others can browse it and rewrite it as needed. Quote “Software is a living thing. And if it’s living it will eventually decay, and you will have to repair it.” (p.658). Abstract A computational challenge dares scientists to revive and run their own decades-old code. By Jeffrey M. Perkel. APA Style Reference Perkel, J. M. (2018). The digital Archaeologists. Nature, 584 (7822), 656-658. https://media.nature.com/original/magazine-assets/d41586-020-02462-7/d41586-020-02462-7.pdf\nYou may also be interested in Trust Your Science? Open Your Data and Code (Stodden, 2011)◈ Attitudes Toward Open Science and Public Data Sharing: A Survey Among Members of the German Psychological Society (Abele-Brehm et al., 2019) Willingness to Share Research Data Is Related to the Strength of the Evidence and the Quality of Reporting of Statistical Results (Wicherts et al., 2011) Open Data in Qualitative Research (Chauvette et al., 2019) CJEP Will Offer Open Science Badges (Pexman, 2017) Badges to Acknowledge Open Practices: A Simple, Low-Cost, Effective Method for Increasing Transparency (Kidwell et al., 2016) Only Human: Scientists, Systems, and Suspect Statistics A review of: Improving Scientific Practice: Dealing With The Human Factors, University of Amsterdam, Amsterdam, September 11, 2014 (Hardwicke et al., 2014) Rein in the four horsemen of irreproducibility (Bishop, 2019) Seven Easy Steps to Open Science: An Annotated Reading List (Crüwell et al., 2019) Seven Steps Toward Transparency and Replicability in Psychological Science (Lindsay, 2020) Using OSF to Share Data: A Step-by-Step Guide (Soderberg, 2018) How much Free Labor? Estimating Reviewers’ Contribution to Publishers (Aczel \u0026amp; Szaszi, 2020) ◈ Main Takeaways: This manuscript aimed to estimate the reviewers’ contribution to the publication system in terms of time and salary-based monetised value. The journal article is the main product of the academic publishing system. This product is a co-production of scientists and publishers. Scientists provide value by their professional work producing some new knowledge about the world and they provide value by being peer reviewers to validate and improve other scientists’ manuscripts. The peer reviewer is rarely recognised and never compensated within this system, although they provide their time and professional knowledge to assess the scientific value and validity of submissions to improve the quality of the manuscript. Peer reviewers are, simply put, the gatekeepers of the publishing system, so they work on several manuscripts that go through the review system but are finally rejected. However, the work conducted by the peer reviewer of academic publishing is important but the magnitude of this work is unknown. The paper aims to investigate this issue. To estimate the salary-based monetised value of the time that reviewers annually work for publishers need three main parameters: number of peer reviews per year, time spent on one review by one reviewer and hourly wage of reviewers. Using Publon, the number of peer reviews accepted was estimated to be 55%, while 45% of the manuscripts were rejected after review. The global sum of citable documents (i.e. accepted and published articles, reviews, and conference papers published in journals) is 3,900,066. The authors assume these values reflect a 55% acceptance rate, the number of rejected manuscripts are estimated to be 3,190,963. The total number of reviews done is 18,082,124 and the authors estimate the number of hours spent on one review as 6 hours. Results: The total time reviewers worked on peer-review in 2019 is over 100 million hours,similar to 12,385 years on working review requests, thus the monetary value is above 200 million US dollars in the UK, 600 million US dollars in China and 1.1 billion US dollars in the USA. Abstract In this paper, we estimated the time, as well as the salary-based monetary value, of the work scientific journal publishers received in 2019 from reviewers in the peer-review process. In case of uncertainty, we used conservative estimates for our parameters, therefore, the true values are likely to be above our results. The total time reviewers worked on peer-reviews in 2019 is over 100 million hours, equivalent to over 12 thousand years. The estimated monetary value of the time reviewers spend on reviews can be calculated for individual countries. In 2019, for the USA it was over 1.1 billion, for China over 600 million, and for the UK over 200 million USD. APA Style Reference Aczel, B., \u0026amp; Szaszi, B. (2020, October 9). How Much Free Labor? Estimating Reviewers’ Contribution to Publishers. https://doi.org/10.31222/osf.io/5h9z4\nYou may also be interested in See references later Current Incentives for Scientists Lead to Underpowered Studies with Erroneous Conclusions (Higginson \u0026amp; Munafo, 2016) Main Takeaways: Scientists are trained to be objective and pursue the discovery of knowledge via exploratory and confirmatory research. However, scientists are human and work within incentive structures that may consciously or unconsciously shape their behaviours. The incentive structure is an ecosystem within which scientists strive to maximise their fitness, via publication record, and might predict that individual scientists would strategically adapt consciously or unconsciously to these pressures and module their research strategy in order to boost their career success. The first model, conducted by authors, used optimality theory to predict the rational strategy of a scientist possessing finite resources who seek to maximise the career value of their publications. The authors observed that as the weight given to novel findings increase, the total number of publications declines. This is because most exploratory studies are not published, as they have low statistical power, thus leading to non-significant findings. From an individual career perspective, when statistical power is low, it is better to run many exploratory studies, thus increasing the probability of false positives than to run a smaller number of well-powered studies. As the weight given to novel findings increases, so does the investment in exploratory studies, leading to more papers drawing erroneous conclusions to over 50%. A second model was created to predict how characteristics of the current scientific ecosystem influence the total scientific value of research. Current incentive structures (e.g. recruitment processes) place weight on findings published in journals with a high Impact Factor and may consider only the best few publications of any individual. The model showed that scientific value of research is not maximised, when scientists try to maximise their own success within this ecosystem. If a small number of novel findings count heavily towards career progression, this encourages scientists to focus all research efforts on underpowered exploratory work to maximise the number of publications. The model indicates that incentive structures could be redesigned so that the optimal strategy for individual scientists align with the optimal conditions for the advancement of knowledge. Put simply, a small reduction in weight being given to novel findings and how quickly the value of total number of publications diminishes would shift individual incentives away from a focus on exploratory work, meaning confirmatory work is more likely to be conducted, increasing the total scientific value of research. As journal editors are more stringent on sample size,the more likely studies will be correct increases towards 100. More confirmatory studies are carried out, so the number of studies published increases. When the sample size is very large, the number of exploratory studies approach 0, leading to a decline in the total scientific value of research. This means that journals should be more stringent about required statistical power and sample size. Current incentive structures are appropriate if editorial and peer review practices were more stringent regarding sample size and statistical power and strength of statistical evidence required of studies. By considering more of a researcher’s output and giving less weight to novel findings, when making appointment and promotion decisions, encourage a change in researcher’s behaviour improving scientific value of research. Journals and journal editors may strive to increase the stringency of the editorial and peer review process, by requiring large sample sizes and greater statistical stringency. Quote “Current incentive structures in science, combined with existing conventions such as a significance level of 5%, encourage rational scientists to adopt a research strategy that is to the detriment of the advancement of scientific knowledge. Given finite resources, the importance placed on novel findings, and the emphasis on a relatively small number of publications, scientists wishing to accelerate their career progression should conduct a large number of exploratory studies, each of which will have low statistical power. Since the conclusions of underpowered studies are highly likely to be erroneous [2], this means that most published findings are likely to be false [5]. The results of our model support this conclusion. Indeed, given evidence that with sufficient analytical flexibility (known as p-hacking) almost any dataset can produce a statistically significant (and therefore publishable) finding [16], our results are likely to be conservative. There is therefore evidence from both simulations and empirical studies that current research practices may not be optimal for the advancement of knowledge, at least in the biomedical sciences.” (p.8). Abstract We can regard the wider incentive structures that operate across science, such as the priority given to novel findings, as an ecosystem within which scientists strive to maximise their fitness (i.e., publication record and career success). Here, we develop an optimality model that predicts the most rational research strategy, in terms of the proportion of research effort spent on seeking novel results rather than on confirmatory studies, and the amount of research effort per exploratory study. We show that, for parameter values derived from the scientific literature, researchers acting to maximise their fitness should spend most of their effort seeking novel results and conduct small studies that have only 10%±40% statistical power. As a result, half of the studies they publish will report erroneous conclusions. Current incentive structures are in conflict with maximising the scientific value of research; we suggest ways that the scientific ecosystem could be improved. APA Style Reference Higginson, A. D., \u0026amp; Munafò, M. R. (2016). Current incentives for scientists lead to underpowered studies with erroneous conclusions. PLoS Biology, 14(11), e2000995, https://doi.org/10.1371/journal.pbio.2000995.\nYou may also be interested in How scientists can stop fooling themselves (Bishop, 2020b) The Statistical Crisis in Science (Gelman \u0026amp; Loken, 2014) Only Human: Scientists, Systems, and Suspect Statistics A review of: Improving Scientific Practice: Dealing With The Human Factors, University of Amsterdam, Amsterdam, September 11, 2014 (Hardwicke et al., 2014) A 21 Word Solution (Simmons et al., 2012) Rein in the four horsemen of irreproducibility (Bishop, 2019) False-Positive Psychology: Undisclosed Flexibility in Data Collection and Analysis Allows Presenting Anything as Significant (Simmons et al., 2011) Many Analysts, One Data Set: Making Transparent How Variations in Analytical Choices Affect Results (Silberzahn et al., 2019) Why small low-powered studies are worse than large high-powered studies and how to protect against “trivial” findings in research: Comment on Friston (2012) (Ingre, 2013) Ironing out the statistical wrinkles in “ten ironic rules” (Lindquist et al., 2013) Ten ironic rules for non-statistical reviewers (Friston, 2012) Small sample size is not the real problem (Bacchetti, 2013) Measurement error and the replication crisis (Loken \u0026amp; Gelman, 2017) Misuse of power: in defence of small-scale science (Quinlan, 2013) Bite-Size Science and Its Undesired Side Effects (Bertamini \u0026amp; Munafo, 2012) Confidence and precision increase with high statistical power (Button et al., 2013) Testing an active intervention to deter researchers’ use of questionable research practices (Bruton et al., 2019) Main Takeaways: Questionable Research Practices produce harm tangibly and directly that affects prescribed medical care or waste research funds. Reforming current practices will be a gradual process at best. It is not entirely clear how best to improve current practices and scientists are often resistant to change. The present study used an intervention that assesses a direct psychological means to encourage research integrity to reduce the number of Questionable Research Practices. Method: 201 participants had to complete a brief writing task. The participants were split into the consistency condition (i.e. the participants had to write how research integrity is modelled in their work and how it is consistent with their core ethical standards) or control condition (i.e. participants had to write about why fabrication, falsification and plagiarism are ethically objectionable). Participants were asked to complete two questionnaires: 1) the extent to which they endorsed 15 questionable research practices and indicate the extent to which each questionable research practice was ethically defensible and willingness to engage in each practice. 2) the impact on others of engaging in questionable research practices. Results: The consistency intervention had no significant effect on respondents’ reactions regarding the defensibility of the Questionable Research Practices or their willingness to engage in them. Results: Participants in the control condition expressed lower perception of Questionable Research Practice defensibility and willingness. The authors concluded that the consistency intervention did not differ from the control intervention on the respondents’ reactions to Questionable Research Practices but may have had the unwanted effect of inducing increased rationalisation about these effects. Abstract Introduction: In this study, we tested a simple, active “ethical consistency” intervention aimed at reducing researchers’ endorsement of questionable research practices (QRPs).Methods: We developed a simple, active ethical consistency intervention and tested it against a control using an established QRP survey instrument. Before responding to a survey that asked about attitudes towards each of fifteen QRPs, participants were randomly assigned to either a consistency or control 3–5-min writing task. A total of 201 participants completed the survey: 121 participants were recruited from a database of currently funded NSF/ NIH scientists, and 80 participants were recruited from a pool of active researchers at a large university medical center in the southeastern US. Narrative responses to the writing prompts were coded and analyzed to assist post hoc interpretation of the quantitative data.Results: We hypothesized that participants in the consistency condition would find ethically ambiguous QRPs less defensible and would indicate less willingness to engage in them than participants in the control condition. The results showed that the consistency intervention had no significant effect on respondents’ reactions regarding the defensibility of the QRPs or their willingness to engage in them. Exploratory analyses considering the narrative themes of participants’ responses indicated that participants in the control condition expressed lower perceptions of QRP defensibility and willingness.Conclusion: The results did not support the main hypothesis, and the consistency intervention may have had the unwanted effect of inducing increased rationalization. These results may partially explain why RCR courses often seem to have little positive effect. APA Style Reference Bruton, S. V., Brown, M., Sacco, D. F., \u0026amp; Didlake, R. (2019). Testing an active intervention to deter researchers’ use of questionable research practices. Research integrity and peer review, 4(1), 1-9. https://doi.org/10.1186/s41073-019-0085-3\nYou may also be interested in How scientists can stop fooling themselves (Bishop, 2020b) The Statistical Crisis in Science (Gelman \u0026amp; Loken, 2014) Only Human: Scientists, Systems, and Suspect Statistics A review of: Improving Scientific Practice: Dealing With The Human Factors, University of Amsterdam, Amsterdam, September 11, 2014 (Hardwicke et al., 2014) A 21 Word Solution (Simmons et al., 2012) Rein in the four horsemen of irreproducibility (Bishop, 2019) Seven Steps Toward Transparency and Replicability in Psychological Science (Lindsay, 2020) The life of p: “Just significant” results are on the rise (Leggett et al., 2013) Many Analysts, One Data Set: Making Transparent How Variations in Analytical Choices Affect Results (Silberzahn et al., 2019) False-Positive Psychology: Undisclosed Flexibility in Data Collection and Analysis Allows Presenting Anything as Significant (Simmons et al., 2011) Raising research quality will require collective action (Munafo, 2019) Main Takeaways: Individual achievement is highlighted as being important for funding, appointments, promotions, tenure, prizes etc. but ignores the deeds that benefit the community, which should be valued explicitly, which should produce usable tools or shared code. If the scientific community wants to move towards a transparent model of research, open-research practices need to be rewarded. If the scientific community wants researchers to work well in large collaborations, researchers need to train them in communication skills and collective self-scrutiny. Researchers must reflect on how and why questionable research practices and undesirable behaviours arise and persist. What are the flaws that the cultures and practices that are being spread by the institutions’ cultures? Has solid and cumulative work, replication studies and null findings been dis-incentivised due to a focus on groundbreaking findings? Institutions are working together to determine how their cultural practices undermine the value of replication, verification and transparency. Figuring out which system-level changes are needed and how to make them happen will be someone’s primary responsibility, not a volunteer activity. What changes might ensue? Institutions should make the use of data sharing and other-research practices an explicit criterion for promotion. Universities in general need to act collectively. Changes to incentives at a single institution is not enough to make new behaviours stick, as these practices can be seen as a form of tax on the scientist’s individual career. Only if changes occur across many institutions will the impacts permeate scientific culture. The same is true for training, universities must agree all graduates must reach common standards. When it comes to changing the culture of science, numerous initiatives link members of the research community to support robust transparent research such as the UK Reproducibility Network, Center for Open Science in the United States, the QUEST Center in Germany, the Research on Research Institute with eight participating countries and other grassroots networks of researchers in many countries. Quote “But these cultural changes might falter. Culture eats strategy for breakfast — grand plans founder on the rocks of implicit values, beliefs and ways of working. Top-down initiatives from funders and publishers will fizzle out if they are not implemented by researchers, who review papers and grant proposals. Grass-roots efforts will flourish only if institutions recognize and reward researchers’ efforts. Funders, publishers and bottom-up networks of researchers have all made strides. Institutions are, in many ways, the final piece of the jigsaw. Universities are already investing in cutting-edge technology and embarking on ambitious infrastructure programmes. Cultural change is just as essential to long-term success.” (p. 183). Abstract Institutions must act together to reform research culture, says Marcus Munafò. APA Style Reference Munafò, M. (2019). Raising research quality will require collective action. Nature, 576(7786), 183. DOI: 10.1038/d41586-019-03750-7\nYou may also be interested in Promoting an open research culture (Nosek et al., 2015) Quality Uncertainty Erodes Trust in Science (Vazire, 2017) Rein in the four horsemen of irreproducibility (Bishop, 2019) Seven Easy Steps to Open Science: An Annotated Reading List (Crüwell et al., 2019) Promoting Transparency in Social Science Research (Miguel et al., 2014) Fallibility in Science: Responding to Errors in the Work of Oneself and Others (Bishop, 2018) Seven Steps Toward Transparency and Replicability in Psychological Science (Lindsay, 2020) A journal club to fix science (Orben, 2019) Small sample size is not the real problem (Bacchetti, 2013) Main Takeaways: Widespread poor research practices raise difficult questions about how to bring about improvements. Bacchetti argued that the positive predictive value of p \u0026lt; 0.05 is an unacceptably poor measure of the evidence that a study provides. It ignores the distinction between p = .049 and p \u0026lt; .0001, thus wasting information. Estimated effects, confidence intervals and exact p values should be considered when interpreting a study’s results, and these make power irrelevant to interpret completed studies. The author argues that any specific result is not weaker evidence because of a small sample size per se than the same p value would be with a larger sample size. Each additional subject produces a smaller increment in projected scientific or practical value than the previous one, indicating efficiency is defined by projected value per animal sacrifice, thus it will be worse with a larger planned sample size Quote “Power calculations therefore should not overrule cost–efficiency and feasibility, and this is impossible in real research practice anyway. Manipulation of the design, conduct analysis and interpretation of studies towards producing more ‘interesting’ results is a serious problem, as is selective dissemination of studies’ results, but these are not caused by small sample size.” (p.585). Abstract Dr Peter Bacchetti provides a commentary on Power failure: why small sample size undermines the reliability of neuroscience and discusses that small sample size does not undermine the reliability of neuroscience. APA Style Reference Bacchetti, P. (2013). Small sample size is not the real problem. Nature Reviews Neuroscience, 14(8), 585-585. https://doi.org/10.1038/nrn3475-c3\nYou may also be interested in How scientists can stop fooling themselves (Bishop, 2020b) The Statistical Crisis in Science (Gelman \u0026amp; Loken, 2014) Only Human: Scientists, Systems, and Suspect Statistics A review of: Improving Scientific Practice: Dealing With The Human Factors, University of Amsterdam, Amsterdam, September 11, 2014 (Hardwicke et al., 2014) A 21 Word Solution (Simmons et al., 2012) Rein in the four horsemen of irreproducibility (Bishop, 2019) False-Positive Psychology: Undisclosed Flexibility in Data Collection and Analysis Allows Presenting Anything as Significant (Simmons et al., 2011) Many Analysts, One Data Set: Making Transparent How Variations in Analytical Choices Affect Results (Silberzahn et al., 2019) Why small low-powered studies are worse than large high-powered studies and how to protect against “trivial” findings in research: Comment on Friston (2012) (Ingre, 2013) Ironing out the statistical wrinkles in “ten ironic rules” (Lindquist et al., 2013) Ten ironic rules for non-statistical reviewers (Friston, 2012) Current Incentives for Scientists Lead to Underpowered Studies with Erroneous Conclusions (Higginson \u0026amp; Munafo, 2016) Measurement error and the replication crisis (Loken \u0026amp; Gelman, 2017) Misuse of power: in defence of small-scale science (Quinlan, 2013) Bite-Size Science and Its Undesired Side Effects (Bertamini \u0026amp; Munafo, 2012) Confidence and precision increase with high statistical power (Button et al., 2013) A journal club to fix science (Orben, 2019) Main Takeaways: Early career researchers are taught about replication issues and failures to challenge results of the previous decade. Connecting through social media lets us sidestep conventional hierarchies and scrutinise current research practices. Science needs researchers who care about better research to stay invested and this will not happen by telling the next generation of scientists to sit back and hoe. Early-career researchers should not wait passively for coveted improvements, they can create communities and push for bottom-up changes. it back and hope. The ReproducibiliTea was started by Sophia Cruwell, Amy Orben and Sam Parsons in 2018 at the experimental psychology department at the University Oxford to promote a stronger open-science community and converse with others about reproducibility. This initiative is now active at more than 27 universities in 8 countries. In each meeting, a scientific paper lays the groundwork for a conversation. Concerns vary from field to field and institution to institution, so each club focuses on specific aspects of scientific methods (e.g. open access, pre-registration and data sharing if their supervisors state these practices will undermine their careers) and systems that concern them most. ReproducibiliTea has helped trainees to return to their lab groups and advocate for change. Sometimes this approach works and sometimes it does not. ReproducibiliTea members state how valuable this journal club is and how they want to see science is practised. Creating Reproducibilitea is easy and visible, does not require jumping over bureaucratic hurdles, and does not require senior support or funding. These groups consist of about a dozen psychology researchers but people from other departments. ReproducibiliTea puts open science on radar of other academics and senior staff. Quote “In practice, I have found our meetings underscore the idea that open science is a process, not a one-time achievement or a claim to virtue...One attendee told me, “Before, I thought everything was black and white in open science, and now I see there are caveats and difficulties and things to overcome.” ReproducibiliTea’s low-key grass-roots meetings will encourage a new generation of scientists to feel motivated to master these challenges.” (p.465). Abstract ReproducibiliTea can build up open science without top-down initiatives, says Amy Orben. APA Style Reference Orben, A. (2019). A journal club to fix science. Nature, 573(7775), 465. https://doi.org/10.1038/d41586-019-02842-8\nYou may also be interested in Raising research quality will require collective action (Munafo, 2019) Preregistered Direct Replications in Psychological Science (Lindsay, 2017) Main Takeaways: Authors of pre-registered direct replications need to provide a compelling case why a replication will make a valuable contribution to understand a phenomenon or theory to psychologists in general. The main criterion to be published in psychological science should be related to theoretical significance. Direct replications should reproduce the original methods and procedures as closely as possible, with the goal to assess the same effect as the original study. The aim of a direct replication is to create conditions that experts agree test the same hypotheses in the same way as the original study. Researchers conducting a pre-registered direct replication should consult the author or authors of the original article. These direct replications will be subject to external review, which will consist of an author of the target piece, together with two independent experts. Pre-registered direct replications should include justification for sample sizes and outcome-independent quality control checks to maximise the credibility of reported findings. Researchers are strongly encouraged to submit proposals for pre-registered direct replications for review before data collection. After the data is collected, the manuscript reporting the replication will be reviewed with the expectation that it will be accepted if the agreed-upon criteria are met. Pre-registered direct replications are limited to 1500 words, excluding Methods and Results section. Authors should provide online Supplemental Materials that would be of interest to experts. Quote “And, as Walter Mischel (2009) noted in an APS Observer Presidential column, replications sometimes yield more nuanced results that spark new hypotheses and contribute to the elaboration of psychological theories.” (p.1192). Abstract A commentary by Professor Stephen D. Lindsay on implementing pre-registered direct replications in the journal of Psychological Science. APA Style Reference Lindsay, D. S. (2017). Preregistered Direct Replications in Psychological Science. Psychological Science, 28(9), 1191-1192. https://doi.org/10.1177/0956797617718802\nYou may also be interested in Easy preregistration will benefit any research (Mellor \u0026amp; Nosek, 2018) Is pre-registration worthwhile? (Szollosi et al., 2020) From pre-registration to publication: a non-technical primer for conducting meta-analysis to synthesize correlation data (Quintana, 2015) Pre-registration is Hard, And Worthwhile (Nosek et al., 2019) Preregistration of Modeling Exercises May Not Be Useful (MacEachern \u0026amp; Van Zandt, 2019) Attitudes Toward Open Science and Public Data Sharing: A Survey Among Members of the German Psychological Society (Abele-Brehm et al., 2019) Main Takeaways: Public data sharing is the only topic not discussed in open science. We should make data accessible for re-analyses in a secure, reliable and competently managed repository. Although there is a positive attitude towards open science, some researchers argue whether data sharing will benefit the careers of early career researchers. The present study investigated the attitude towards open science and public data sharing in general, as attitudes not only contribute to the research practice of the individual, but also the undergraduate student, the postgraduate student, the post-doctoral student, colleagues and the wider scientific community. Method: 337 people were given scales and open-ended questions with 14 items that measured attitudes toward open science and public data sharing (e.g. what are the long-term consequences if a researcher shares raw data as part of a publication?). Method: Attitudes toward open science were separated into hopes and fears. Results: More hopes were related to open science and data sharing attitudes than fears. Hopes and fears were highest for ECRs, whereas for professors, hopes and fears were the lowest. Attitudes towards open science and public data sharing were positive but there were fears that sharing data may have negative consequences for an individual’s career (e.g. data scooping). Professors exhibited the least hopes and fears concerning the consequences of open science and data sharing. Quote “This is, of course, true, but the idea of OS is transparency, and the question whether transparency and a higher commitment to data sharing and OS practices will eventually decrease QRPs and, thus, increase the robustness and replicability of psychological effects remains to be determined empirically.” (p.259). Abstract Central values of science are, among others, transparency, verifiability, replicability, and openness. The currently very prominent Open Science (OS) movement supports these values. Among its most important principles are open methodology (comprehensive and useful documentation of methods and materials used), open access to published research output, and open data (making collected data available for re-analyses). We here present a survey conducted among members of the German Psychological Society (N = 337), in which we applied a mixed-methods approach (quantitative and qualitative data) to assess attitudes toward OS in general and toward data sharing more specifically. Attitudes toward OS were distinguished into positive expectations (“hopes”) and negative expectations (“fears”). These were uncorrelated. There were generally more hopes associated with OS and data sharing than fears. Both hopes and fears were highest among early career researchers and lowest among professors. The analysis of the open answers revealed that generally positive attitudes toward data sharing (especially sharing of data related to a published article) are somewhat diminished by cost/benefit considerations. The results are discussed with respect to individual researchers’ behavior and with respect to structural changes in the research system. APA Style Reference Abele-Brehm, A. E., Gollwitzer, M., Steinberg, U., \u0026amp; Schönbrodt, F. D. (2019). Attitudes toward open science and public data sharing. Social Psychology, 50, 252-260. https://doi.org/10.1027/1864-9335/a000384\nYou may also be interested in Trust Your Science? Open Your Data and Code (Stodden, 2011) Open Data in Qualitative Research (Chauvette et al., 2019) Only Human: Scientists, Systems, and Suspect Statistics A review of: Improving Scientific Practice: Dealing With The Human Factors, University of Amsterdam, Amsterdam, September 11, 2014 (Hardwicke et al., 2014) Seven Steps Toward Transparency and Replicability in Psychological Science (Lindsay, 2020) Using OSF to Share Data: A Step-by-Step Guide (Soderberg, 2018) The digital Archaeologists (Perkel, 2020) Sharing Data and Materials in Psychological Science (Lindsay, 2017) Main Takeaways: Psychological science requests data and material to be sent from author to reviewers and the public. Anonymity should be preserved. However, this may not be possible when the manuscript is not ethically permitted or practically feasible, thus the corresponding author needs to offer a brief explanation. The reported analyses should be made available to reviewers when doing so is ethically permitted and practically feasible. Easy access to the data enables reviewers to assess the substantive claims of the manuscript. Psychological Science asks authors who submit a manuscript to explain how reviewers access data for review purposes or, if reviewers cannot access the data, to explain why. Researchers are encouraged to specify data-sharing plans in their Institutional Review Board applications and informed consent forms. It can be argued that if the consent form did not mention data sharing, it may be permissible to share non-identifiable data with reviewers. Researchers owe it to people who participated to make the data available to improve the extent to which data contributes to science. It can take a lot of work to prepare an archive of non-identifiable data, together with coding schemes, analysis scripts, etc. is sufficiently clear that other researchers can understand the dataset. This is part of being a scientist and the burden reduces with further practice and good workflow processes. Authors must make their best judgments as to the granularity of the shared data best balancing costs and benefits. If multiple measures are collected and report only a subset, it needs to be made clear that the dropped measures will be provided to reviewers. The reviewers do not have to look at the data linked to the submission, it is their choice if they want to examine the data. The reviewers are asked to report on whether or not they looked at the data and if it affected their evaluation of the manuscript. Data sharing allows the reviewers access to stimulus materials, measures, computer code for running experiments, simulations or data analyses etc. Having access to materials helps reviewers to evaluate claims made in the manuscript. It may not be ethically appropriate to give reviewers easy access to the materials and there are challenges with sharing materials for research conducted in languages other than English. Authors must make their best judgments to balance the cost and benefits of giving reviewers access to materials. Data and materials should be shared, this improves the impact and interpretation of a manuscript. Some materials are copyrighted, such that they cannot be shared. Authors who have invested heavily in developing these materials may not want to give them away to other scientists. Put simply, materials must not always be shared but authors must be encouraged to share materials with other scientists. This sharing of materials will not only benefit the field but also the author themselves. Quote “In at least some areas of psychology, it is getting harder to publish. Expectations for methodological rigor and statistical sophistication have risen sharply in the past 6 years. Preregistering one’s research plans, testing sufficient numbers of subjects to attain respectable power or precision, avoiding p-hacking and HARKing (“hypothesizing after the results are known”; Kerr, 1998), replicating one’s findings— these new norms make it harder to produce a primary research report that tells a good story. The upside is that more of our stories will turn out to be true” (p.702). Abstract A commentary by Professor Stephen D. Lindsay on sharing data and materials in the journal of Psychological Science. APA Style Reference Lindsay, D. S. (2017). Sharing Data and Materials in Psychological Science. Psychological Science, 28(6), 699-702. https://doi.org/10.1177/0956797617704015\nYou may also be interested in Trust Your Science? Open Your Data and Code (Stodden, 2011)◈ Attitudes Toward Open Science and Public Data Sharing: A Survey Among Members of the German Psychological Society (Abele-Brehm et al., 2019) Willingness to Share Research Data Is Related to the Strength of the Evidence and the Quality of Reporting of Statistical Results (Wicherts et al., 2011) Open Data in Qualitative Research (Chauvette et al., 2019) CJEP Will Offer Open Science Badges (Pexman, 2017) Badges to Acknowledge Open Practices: A Simple, Low-Cost, Effective Method for Increasing Transparency (Kidwell et al., 2016) Only Human: Scientists, Systems, and Suspect Statistics A review of: Improving Scientific Practice: Dealing With The Human Factors, University of Amsterdam, Amsterdam, September 11, 2014 (Hardwicke et al., 2014) Rein in the four horsemen of irreproducibility (Bishop, 2019) Seven Easy Steps to Open Science: An Annotated Reading List (Crüwell et al., 2019) Seven Steps Toward Transparency and Replicability in Psychological Science (Lindsay, 2020) Using OSF to Share Data: A Step-by-Step Guide (Soderberg, 2018) The digital Archaeologists (Perkel, 2020) We Have to Break Up (Cialdini, 2009) Main Takeaways: Mediation is used by psychologists to locate causality and sophisticated psychometric techniques allow mediational accounts of our major findings through the analysis of ancillary data. Human conduct is broadly and distinctly cognitive. If psychology failed to focus on systematic scrutiny on cognitive variables and their roles in behaviour. It makes sense psychological science value submissions combining several related studies. Data collection and recruitment in the field takes longer than in the laboratory. The package of multiple-study research reports takes numerous years and getting permission to conduct experiments in a naturally occurring environment can take as long as completing several laboratory investigations. The author wants to leave psychology, as psychology has become too lax in their responsibilities to the public in this regard. They deserve to know the pertinence of research to their lives, as they paid for that research and are entitled to know what we have learned about them with their money. To improve academic psychology, we would need to reassign more value to field research than has been the case in recent times. It should be taught regularly in graduate methods classes and there should be awards for them and given more grace and space in the loftiest of our journals. Quote “Finally, truly natural human activities don’t lend themselves to the collection of the kinds of secondary data on which to base mediational analyses; participants in many of the contexts I have employed (e.g., automobile dealerships, hospital parking garages, amusement parks, recycling centers, hotel guestrooms) do not feel bound or inclined to offer such data in order to help some researcher distinguish among theoretical models.” (p. 5). Abstract Three mostly positive developments in academic psychology—the cognitive revolution, the virtual requirement for multiple study reports in our top journals, and the prioritization of mediational evidence in our data—have had the unintended effect of making field research on naturally occurring behavior less suited to publication in the leading outlets of the discipline. Two regrettable consequences have ensued. The first is a reduction in the willingness of researchers, especially those young investigators confronting hiring and promotion issues, to undertake such field work. The second is a reduction in the clarity with which nonacademic audiences (e.g., citizens and legislators) can see the relevance of academic psychology to their lives and self-interest, which has contributed to a concomitant reduction in the availability of federal funds for basic behavioral science. Suggestions are offered for countering this problem. APA Style Reference Cialdini, R. B. (2009). We have to break up. Perspectives on psychological science, 4(1), 5-6. https://doi.org/10.1111/j.1745-6924.2009.01091.x\nYou may also be interested in References will be included soon Swan Song Editorial (Lindsay, 2019) Main Takeaways: Eich instituted the use of badges that designate articles with open data, open materials and pre-registration. They wanted authors to disclose all data exclusions, manipulations and measures how they determined sample size. In addition word-count limits from Method and Results were removed to allow authors to report key details of their studies, analyses, and findings. A few pre-registered direction replications have been published in Psychological Science to date. More are in the pipeline. Psychological Science published several articles stressing evidence for the absence of a non-trivial effect (i.e. evidence supporting the null). Statisticians agree Bayes factor equivalence tests are used to assess the strength of evidence for the null. This is an important step forward for life science. Retractions have been issued at the behest of authors who stepped forward and requested the Corrigendum or Retraction themselves. Researchers should be saluted for owning up to the mistakes when appropriate. Errors came to light because authors posted their data and other scientists examined them and found evidence of problems. This is progress. Badges are not an end in themselves. The aim is to make it easy for scientists to access data and materials that support reproducibility, robustness, and replicability to encourage detailed pre-registrations allow researchers and readers to discriminate between confirmatory and exploratory analyses. Critics argue that commercial publishers\u0026#39; profits are high and that the goals of scientists and commercial publication systems block many people from accessing reports of publicly funded research. Early career researchers rely on and show the virtues of preprints. Academic Twitter is against journals that publish articles behind a paywall. Journals can set policies that can quickly and dramatically increase transparency. Professional societies use revenue from journals to support their proscience agendas. Professional societies must think about how and why they publish journals and how they can continue to do so in the future. Abstract A commentary by Professor Stephen D. Lindsay about his time as the editor of the journal of psychological science. APA Style Reference Lindsay, D. S. (2019). Swan Song Editorial. Psychological Science, 30(12), 1669-1673. https://doi.org/10.1177/0956797619893653\nYou may also be interested in The Statistical Crisis in Science (Gelman \u0026amp; Loken, 2014) Only Human: Scientists, Systems, and Suspect Statistics A review of: Improving Scientific Practice: Dealing With The Human Factors, University of Amsterdam, Amsterdam, September 11, 2014 (Hardwicke et al., 2014) False-Positive Psychology: Undisclosed Flexibility in Data Collection and Analysis Allows Presenting Anything as Significant (Simmons et al., 2011) A 21 Word Solution (Simmons et al., 2012)◈ Seven Easy Steps to Open Science: An Annotated Reading List (Crüwell et al., 2019) Promoting an open research culture (Nosek et al., 2015) Promoting Transparency in Social Science Research (Miguel et al., 2014) Psychologists Are Open to Change, yet Wary of Rules (Fuchs et al., 2012) Registered Reports: Realigning incentives in scientific publishing (Chambers et al., 2015) Registered Reports: A new publishing initiative at Cortex (Chambers, 2013) Registered Reports: A step change in scientific publishing (Chambers, 2014) Registered reports: a method to increase the credibility of published results (Nosek \u0026amp; Lakens, 2014) Registered reports (Jamieson et al., 2019) Attitudes Toward Open Science and Public Data Sharing: A Survey Among Members of the German Psychological Society (Abele-Brehm et al., 2019) Willingness to Share Research Data Is Related to the Strength of the Evidence and the Quality of Reporting of Statistical Results (Wicherts et al., 2011) How scientists can stop fooling themselves (Bishop, 2020b) CJEP Will Offer Open Science Badges (Pexman, 2017) Badges to Acknowledge Open Practices: A Simple, Low-Cost, Effective Method for Increasing Transparency (Kidwell et al., 2016) Trust Your Science? Open Your Data and Code (Stodden, 2011)◈ Publishing Research With Undergraduate Students via Replication Work: The Collaborative Replications and Education Project (CREP; Wagge et al., 2019) Is science really facing a reproducibility crisis, and do we need it to? (Fanelli, 2018) Fallibility in Science: Responding to Errors in the Work of Oneself and Others (Bishop, 2018) A consensus-based transparency checklist (Aczel et al., 2020) Tell it like it is (Anon, 2020) Is pre-registration worthwhile? (Szollosi et al., 2020) Signalling the trustworthiness of science (Jamieson et al., 2020) Many Analysts, One Data Set: Making Transparent How Variations in Analytical Choices Affect Results (Silberzahn et al., 2019) Seven Steps Toward Transparency and Replicability in Psychological Science (Lindsay, 2020) Editorial of Psychological Bulletin (Albarracin, 2015) Main Takeaways: Readers of Psychological Bulletin are accustomed to accurate, balanced treatments of a subject. Whenever quantitative synthesis is possible, precision and accuracy take the form of well executed meta-analysis, a method used over the editorial periods. More significant than whether a review is quantitative or qualitative is whether it contributes a cohesive, useful theory. Many of the reviews from recent decades involve integrations of data that highlight fundamental variables and their structure, causal processes, and factors that initiate or disrupt those processes. Qualitative reviews can accelerate theoretical advancement, regularly knit connections among subfields of psychology, and may produce the fully theory testing meta-analysis appearing in the journal. Psychological Review is the outlet for theory development and specification, Psychological Bulletin can be the outlet for theory testing achieved through systematic research synthesis. Authors should be mindful of the need to write for a general psychology audience. The prose must be intelligible, the topic interesting, and interdisciplinary and applied implications explicit. Psychological Bulletin generates a cohesive, authoritative, theory-based, and complete synthesis of scientific evidence in the field of psychology. Reviewers must present a problem and offer an intellectual solution. Focused questions about the phenomenon are important to move the field of psychology forward. Authors should make all possible attempts at estimating and reducing review biases, including translating reports from foreign languages, examining publication biases and survey the grey literature. The methods of review and effect size calculation should be properly reported, including details about the coding process, report indexes of inter-coder reliability and verify that the reported methods could be replicated by readers of an article. Quote “Critical reviews of psychological problems offer an opportunity for the discipline’s self-study and self-actualization. To continue to support this mission, I am interested in submissions of reviews answering questions the discipline must regularly confront: What is psychology? What questions have psychologists not fully answered yet in a given area or set of areas? Or, what are the fundamental, indispensable constructs of our discipline? Psychological Bulletin is ideally suited to answer those questions through systematic review articles. Psychological Bulletin is also an optimal forum for scientific debates about the magnitude and replicability of psychological phenomena. Scientific error as well as voluntary and accidental misreporting, not to mention the occasional case of fraud, undoubtedly reduce the contribution of virtually any primary study considered in isolation. In recent years, concerns with error and scientific misconduct have received a great deal of attention within and outside of the discipline, but pointing fingers at individual researchers or idealizing the contribution of a particular form of replication is unlikely to alter the cumulative mandate of science. Instead, well-conducted research syntheses will continue to gamer advantage from our collective contributions to excellence in psychological science. I foresee Psychological Bulletin at the center of those endeavors.” (p.5). Abstract An editorial by Professor Dolores Albarracin about how to make a robust systematic review and how to get it published in Psychological Bulletin. APA Style Reference Albarracín, D. (2015). Editorial. Psychological bulletin, 141(1), 1-5. https://doi.org/10.1037/bul0000007\nYou may also be interested in Editorial of Psychological Bulletin (Albarracin et al., 2018) Business as Usual (Eich, 2014) Main Takeaways: Research Articles and Research Reports were limited to 4000 and 2500 words and included all of the main text (i.e. introduction, method, results and discussion), along with notes, acknowledgement and appendices. The new limits on Research Articles and Research Reports are 2000 and 1000 words, respectively. The word count will include the introduction, discussion, notes, acknowledgement and appendices but not the method and results. The authors will have to state why is this knowledge important for the field? How are the claims made in the article justified by the methods used? Editors and external referees will evaluate submissions with three questions: what will the reader learn about psychology that they did not know before? The authors are asked to preview their answers as part of the manuscript submission. The aim is to make the preview exercise manageable for all parties, while helping everyone be on the same page. Authors are asked to report the total number of observations that were excluded and the criterion for exclusion, all tested experimental conditions, including failed manipulations; all administered measures and items and how they determined their sample sizes. The manuscript submission portal will have a new section for Disclosure Statement items. Submitting authors check each item in order for their manuscript to proceed to editorial evaluation. Authors declare that they have disclosed all of the required information for each study in the submitted manuscript. Psychological Science will promote open scientific practices. Present norms do not provide strong incentives for individual researchers to share data, materials or the research process. Journals could provide incentives for scientists to adopt open practices by acknowledging them in publication. The challenge is to establish which open practices should be acknowledged, what criteria must be met to earn acknowledgement, and how acknowledgement would be displayed within the journal. Quote “Null-hypothesis significance testing (NHST) has long been the mainstay method of analyzing data and drawing inferences in psychology and many other disciplines. This is despite the fact that, for nearly as long, researchers have recognized essential problems with NHST in general, and with the dichotomous (“significant” vs. “nonsignificant”) thinking it engenders in particular. The problems that pervade NHST are avoided by the new statistics—effect sizes, confidence intervals, and meta analysis.” (p.5). Abstract An editorial by Professor Eric Eich about submitting a manuscript in Psychological Science. APA Style Reference Eich, E. (2014). Business not as usual. Psychological science, 25(1), 3.https://doi.org/10.1177/0956797613512465\nYou may also be interested in False-Positive Psychology: Undisclosed Flexibility in Data Collection and Analysis Allows Presenting Anything as Significant (Simmons et al., 2011) Trade-Offs in the Design of Experiments (Wiley, 2009) Main Takeaways: Experiments should always include rigorous attempts to identify and reduce unintended influences of one subject on another and make appropriate use of any multi-level statistical design. There can be no argument that treatments must be assigned randomly to experimental units. This note focuses on the pervasiveness of trade-offs in the design of experiments, the inappropriate and appropriate uses of multiple tests of a hypothesis and the dangers that counteract the benefits of large samples. The decision to include blocking in an experimental design requires repetition of a treatment on any one experimental unit. It may involve measuring the response of each individual more than once or obtaining measurements in each location or in each block of time more than once. The advantage of this blocking is the information it produces about variation among blocks. This information can be used to test what might be called the secondary hypotheses of the experiment and the sources of variation that influence the responses to the treatment. In addition, by obtaining repeated measures of each experimental unit or subject. If the units were not predicted to differ intrinsically in response to treatments, there remains the possibility of errors of measurement. If the accuracy or precision of measurements were low, it could happen that variation in repeated measurements of any one unit were greater than variations in means between units under any one treatment or between treatment. Standardisation involves a trade-off with the generality of the results. The results become constrained by those conditions. The advantage of reducing variation in the responses of subjects is counterbalanced by a disadvantage in the generality of the results. This is problematic for multiple measurements, since it raises the question: Is the experimenter interested in each number being separate responses to the treatment? Or are they interested in any number of possible responses to the treatment? Investigators should detect all possible biases and detect the smallest effects of the treatment possible. Large samples allow detection of small effects because of the influence of sample size on the estimation of sample means. Small samples have more variation among samples, and this variation is a source of bias. Bias is some unsuspected systematic difference between experimental units for different treatments. Randomisation of treatment among subjects reduces bias. In the real world, it does not remove all systematic differences between treatment, because randomisation does not assure that all features of experimental units are equally distributed among different treatment groups. If the sample size is small, an experiment can only detect a large effect of treatment. Only a large difference in a confounding variable can produce an apparent effect of treatment. A large bias is likely to be noticed by the investigator or by reviewers. A large experiment is subject to bias remaining after randomisation of a finite sample. Large random samples are less likely to have large differences. Small differences remain likely. Small differences from bias in large samples can reach a criterion for statistical significance, just as small differences from treatments can. Abstract This comment supplements and clarifies issues raised by J. C. Shank and T. C. Koehnle (2009) in their critique of experimental design. First, the pervasiveness of trade-offs in the design of experiments is emphasized (Wiley, 2003). Particularly germane to Shank and Koehnle’s discussion are the inevitable trade-offs in any decisions to include blocking or to standardize conditions in experiments. Second, the interpretation of multiple tests of a hypothesis is clarified. Only when interest focuses on any, rather than each, of N possible responses is it appropriate to adjust criteria for statistical significance of the results. Finally, a misunderstanding is corrected about a disadvantage of large experiments (Wiley, 2003). Experiments with large samples raise the possibility of small, but statistically significant, biases even after randomization of treatments. Because these small biases are difficult for experimenters and readers to notice, large experiments demonstrating small effects require special scrutiny. Such experiments are justified only when they involve minimal human intervention and maximal standardization. Justifications for the inevitable trade-offs in experimental design require careful attention when reporting any experiment. APA Style Reference Wiley, R. (2009). Trade-Offs in the Design of Experiments. Journal of Comparative Psychology, 123(4), 447-449. doi: 10.1037/a0016094. [ungated]\nYou may also be interested in Add references soon A guideline for whom? (Furukawa, 2016) Main Takeaways: Systematic reviews and meta-analyses need critical evaluation of each included and excluded trial and a critical overview of the totality of thus selected evidence. They have conducted a number of influential randomised controlled trials of psychotherapies themselves. The first group who would use this guideline would be people who developed their own programme of psychotherapy, especially for those who had strong allegiance to all therapies that are examined in their own randomised control trials. This would introduce the bias to instill expectation to the same effect among participants that were recruited into the trials. The second group who would use this guideline would be people who conduct systematic reviews and critical appraisal of psychotherapy literature, especially when they pay good attention to risks of bias pertaining to proper randomisation, blinding or intention-to-treat principle and publication bias. More trials are at high risk of bias for research allegiance recently than before can now recommend. Quote “How should the ultimate consumers of medical literature (i.e. patients, families and policy makers) use this guideline? Do they remain at the mercy of the bulk of literature consisting of the original randomised control trials that follow this guideline, of systematic reviews that ignored this guideline, of systematic reviews that ignored this guideline in their evidence synthesis, and of practicing psychotherapists who may be all too easily convinced of the effectiveness of the therapies that they practice?” (p.2). Abstract A commentary by Professor T.A. Furukawa about How to prove that your therapy is effective, even when it is not: A guideline. APA Style Reference Furukawa, T. A. (2016). A guideline for whom?. Epidemiology and psychiatric sciences, 25(5), 439. doi: 10.1017/S2045796015000955 You may also be interested in Most psychotherapies do not really work, but those that might work should be assessed in biased studies (Ioannidis, 2016) How to prove that your therapy is effective, even when it is not: a guideline (Cuijpers \u0026amp; Cristea, 2016) Most psychotherapies do not really work, but those that might work should be assessed in biased studies (Ioannidis, 2016) Main Takeaways: Psychotherapies may need to be tested under biased conditions, but bias should be of the right type. Using the weak spots of randomised trials, not concealing treatment allocation to assessors of outcome, analysing only the participants who completed the intervention and ignoring dropouts, using multiple outcome instruments and selectively reporting only the significant ones and not publishing results unless positive represent clear cheating. Treatment dropouts and losses to follow-up are frequent even in short-term studies and, indeed, they often reflect lack of effectiveness or poor tolerability. Imputation methods are better than ignoring missing observations, but still leave substantial uncertainty. All this means is that at least the other improper and easy-to-handle biases should be eliminated. There is absolutely no reason nowadays for a trial not to be performed with robust randomisation, allocation concealment and pre-specified outcomes and not to get published as pre-specified. The author leaves room to modify the analysis plan if something exploratory could not be expected a priori. However, this still needs to be transparently acknowledged, the modified analysis plan justified and results interpreted with caution. Conversely, psychotherapies emerge from theoretical speculation and currently hit patients with little pre-screening. The odds of success are probably weaker for psychotherapies than for drugs. However, raising the expectations of the participants by boosting the placebo effect is also not a bad idea. Most psychotherapies that are effective, probably work primarily through the placebo effect. An additional bias is small sample size, as it is more susceptible to the five ‘improper biases’ than larger ones. It is beneficial to use a small sample size to assess whether we would be wasting resources to run large trials on low-yield experimental therapies. Large studies at an early stage make sense only if there is a reasonable chance to see a clinically meaningful effect and that clinically meaningful effect is small, thus requiring a large study to detect it. Quote “However most psychotherapies that do not work even against nothing will be quickly screened out with small trials, failing even this favourably biased test. Again, incentives should reward publishing such ‘negative’ results and save the field from wasting effort chasing spurious claims.” (p.2). Abstract A commentary by Professor John Ioannidis about How to prove that your therapy is effective, even when it is not: A guideline. APA Style Reference Ioannidis, J. P. A. (2016). Most psychotherapies do not really work, but those that might work should be assessed in biased studies. Epidemiology and psychiatric sciences, 25(5), 436. doi: 10.1017/S2045796015000888\nYou may also be interested in A guideline for whom? (Furukawa, 2016) How to prove that your therapy is effective, even when it is not: a guideline (Cuijpers \u0026amp; Cristea, 2016) Misuse of power: in defence of small-scale science (Quinlan, 2013) Main Takeaways: One unfortunate conclusion is that the results of any small sample study are probably misleading and possibly worthless. It can be perfectly acceptable to publish research based on a sample size that is as small (N = 16). The author does not want researchers to ignore statistical power but it is troubling to think that an unresolved scientific controversy exists because, fundamentally, the issues reside in studies of low statistical power. Quote “Indeed, by exploiting established statistical tests together with computation of the Bayes factor, it is relatively easy to expose the strength of evidence for an experimental hypothesis relative to that of the null hypothesis even with small sample.” (p.585). Abstract Dr Philip Quinlan provides a commentary on Power failure: why small sample size undermines the reliability of neuroscience and discusses that small sample size does not undermine the reliability of neuroscience. APA Style Reference Quinlan, P. T. (2013). Misuse of power: in defence of small-scale science. Nature Reviews Neuroscience, 14(8), 585-585. https://doi.org/10.1038/nrn3475-c1\nYou may also be interested in How scientists can stop fooling themselves (Bishop, 2020b) The Statistical Crisis in Science (Gelman \u0026amp; Loken, 2014) Only Human: Scientists, Systems, and Suspect Statistics A review of: Improving Scientific Practice: Dealing With The Human Factors, University of Amsterdam, Amsterdam, September 11, 2014 (Hardwicke et al., 2014) A 21 Word Solution (Simmons et al., 2012) Rein in the four horsemen of irreproducibility (Bishop, 2019) False-Positive Psychology: Undisclosed Flexibility in Data Collection and Analysis Allows Presenting Anything as Significant (Simmons et al., 2011) Many Analysts, One Data Set: Making Transparent How Variations in Analytical Choices Affect Results (Silberzahn et al., 2019) Why small low-powered studies are worse than large high-powered studies and how to protect against “trivial” findings in research: Comment on Friston (2012) (Ingre, 2013) Ironing out the statistical wrinkles in “ten ironic rules” (Lindquist et al., 2013) Ten ironic rules for non-statistical reviewers (Friston, 2012) Current Incentives for Scientists Lead to Underpowered Studies with Erroneous Conclusions (Higginson \u0026amp; Munafo, 2016) Small sample size is not the real problem (Bacchetti, 2013) Bite-Size Science and Its Undesired Side Effects (Bertamini \u0026amp; Munafo, 2012) Confidence and precision increase with high statistical power (Button et al., 2013) Measurement error and the replication crisis (Loken \u0026amp; Gelman, 2017) Bite-Size Science and Its Undesired Side Effects (Bertamini \u0026amp; Munafo, 2012) Main Takeaways: Psychological science have claimed that short reports are faster communication of results, easy to assimilate with the literature, easy to understand for people outside the field, easy to process for editors and reviewers and allow more dynamic exchange of fresh ideas even if they may turn out to be wrong. In addition, short reports lead to an increased pressure on researchers to produce a quantifiable output. Citation impact should not be seen as a superior measure of impact, especially once it is adjusted length. For instance, if the same findings were written in a short or long format and if both articles get cited equally, the impact per page would be higher for the short article. However, this would be misleading to state the short article has achieved any greater impact than the long article. Short articles in journals means that more articles can be published, they allow the editor to decide quickly about acceptance or rejection of the manuscript, thus speeding up the processing of a manuscript. If replication is fundamental to the scientific method, then an advantage of multiexperiment papers is that replication is inherent and usually rather stringently defined. Also, short articles are more prone to citation amnesia, especially when a tight word count criteria has to be met. The findings are more newsworthy when the discussion of previous relevant work is less detailed, thus, there is pressure on authors not to go into great depth when researching and discussing previous work. Put simply, ignorance allows researchers to discover “new” things. Bite-size articles make false positives or flukes worse. We are all aware of the need for results to be replicated. Long articles with multiple experiments show whether an effect can be replicated and supported by converging evidence. Quote “By far the most popular and influential measure of quality for journals is their Impact Factor. This is computed on the basis of citations, and therefore should reflect influence in the field. As critics have often pointed out, it does not distinguish between citations that confirm and extend the original findings and citations that criticize and debunk them. In this sense, journals have little formal incentive to minimize bias in the effects reported and to minimize false alarms, although we have no doubt that every good editor is trying to do that...By combining different measures, such as number of articles, Impact Factor, and publication bias, we could arrive at better measures of the quality of a journal in reporting interesting but replicable and valid results. At the moment, there are sophisticated tools to count articles published and number of citations…Despite increased interest in bibliometrics, there is also a growing consciousness of the limitations of any individual index, especially because as soon as one index achieves the status of the measure of choice, with practical implications, authors and institutions will start to adapt and play the system.” (p.70). Abstract Short and rapid publication of research findings has many advantages. However, there is another side of the coin that needs careful consideration. We argue that the most dangerous aspect of a shift toward “bite-size” publishing is the relationship between study size and publication bias. Findings based on a single study or a study based on a limited sample size are more likely to be false positive, because the false positive rate remains constant, whereas the true positive rate (the power) declines as sample size declines. Pressure on productivity and on novelty value further exacerbates the problem. APA Style Reference Bertamini, M., \u0026amp; Munafò, M. R. (2012). Bite-size science and its undesired side effects. Perspectives on Psychological Science, 7(1), 67-71. https://doi.org/10.1177/1745691611429353\nYou may also be interested in How scientists can stop fooling themselves (Bishop, 2020b) The Statistical Crisis in Science (Gelman \u0026amp; Loken, 2014) Only Human: Scientists, Systems, and Suspect Statistics A review of: Improving Scientific Practice: Dealing With The Human Factors, University of Amsterdam, Amsterdam, September 11, 2014 (Hardwicke et al., 2014) A 21 Word Solution (Simmons et al., 2012) Rein in the four horsemen of irreproducibility (Bishop, 2019) False-Positive Psychology: Undisclosed Flexibility in Data Collection and Analysis Allows Presenting Anything as Significant (Simmons et al., 2011) Many Analysts, One Data Set: Making Transparent How Variations in Analytical Choices Affect Results (Silberzahn et al., 2019) Why small low-powered studies are worse than large high-powered studies and how to protect against “trivial” findings in research: Comment on Friston (2012) (Ingre, 2013) Ironing out the statistical wrinkles in “ten ironic rules” (Lindquist et al., 2013) Ten ironic rules for non-statistical reviewers (Friston, 2012) Current Incentives for Scientists Lead to Underpowered Studies with Erroneous Conclusions (Higginson \u0026amp; Munafo, 2016) Misuse of power: in defence of small-scale science (Quinlan, 2013) Small sample size is not the real problem (Bacchetti, 2013) Confidence and precision increase with high statistical power (Button et al., 2013) Measurement error and the replication crisis (Loken \u0026amp; Gelman, 2017) Willingness to Share Research Data Is Related to the Strength of the Evidence and the Quality of Reporting of Statistical Results (Wicherts et al., 2011) Main Takeaways: The American Psychological Association asks authors to sign a contract that data is available for individuals who wish to re-analyse the data to verify claims put forth in the paper. There has been no published research to assess this scenario in reality. The present study examined the willingness to share data for re-analysis linked to strength of evidence and quality of reporting of statistical results. Method: Wicherts et al. contacted corresponding authors of 141 papers published in the second half of 2004 in one of four high-ranking journals published by the American Psychological Association and determined whether the effects of outliers contributed to statistical outcomes. Method: They included studies from the Journal of Personality and Social Psychology, and the Journal of Experimental Psychology: learning, memory and cognition, as authors were more willing to share data than other journals. Method: They included tests results that were complete (i.e. test statistic, degrees of freedom, and p-value reported) and reported as significant effects. Results: Higher p-values were more likely in papers from which no data were shared. Conclusions: Reluctance to share was linked with weaker evidence and higher prevalence of apparent errors to report results. An unwillingness to share data was linked to reporting errors that affected statistical significance. The authors seem to suggest that a reluctance to share data was linked to more errors in reporting of results and with weaker evidence. The unwillingness to share data was more pronounced when errors concerned significance. Statistically rigorous researchers archive data better and are more attentive to statistical power than less statistically rigorous researchers. Quote “Best practices in conducting analyses and reporting statistical results involve, for instance, that all co-authors hold copies of the data, and that at least two of the authors independently run all the analyses (as we did in this study). Such double-checks and the possibility for others to independently verify results later should go a long way in dealing with human factors in the conduct of statistical analyses and the reporting of results” (pp.6-7). Abstract The widespread reluctance to share published research data is often hypothesized to be due to the authors’ fear that reanalysis may expose errors in their work or may produce conclusions that contradict their own. However, these hypotheses have not previously been studied systematically. We related the reluctance to share research data for reanalysis to 1148 statistically significant results reported in 49 papers published in two major psychology journals. We found the reluctance to share data to be associated with weaker evidence (against the null hypothesis of no effect) and a higher prevalence of apparent errors in the reporting of statistical results. The unwillingness to share data was particularly clear when reporting errors had a bearing on statistical significance.Our findings on the basis of psychological papers suggest that statistical results are particularly hard to verify when reanalysis is more likely to lead to contrasting conclusions. This highlights the importance of establishing mandatory data archiving policies. APA Style Reference Wicherts, J. M., Bakker, M., \u0026amp; Molenaar, D. (2011). Willingness to share research data is related to the strength of the evidence and the quality of reporting of statistical results. PloS one, 6(11), e26828. https://doi.org/10.1371/journal.pone.0026828\nYou may also be interested in Trust Your Science? Open Your Data and Code (Stodden, 2011) Attitudes Toward Open Science and Public Data Sharing: A Survey Among Members of the German Psychological Society (Abele-Brehm et al., 2019) Open Data in Qualitative Research (Chauvette et al., 2019) CJEP Will Offer Open Science Badges (Pexman, 2017) Badges to Acknowledge Open Practices: A Simple, Low-Cost, Effective Method for Increasing Transparency (Kidwell et al., 2016) Only Human: Scientists, Systems, and Suspect Statistics A review of: Improving Scientific Practice: Dealing With The Human Factors, University of Amsterdam, Amsterdam, September 11, 2014 (Hardwicke et al., 2014) Rein in the four horsemen of irreproducibility (Bishop, 2019) Seven Easy Steps to Open Science: An Annotated Reading List (Crüwell et al., 2019) Seven Steps Toward Transparency and Replicability in Psychological Science (Lindsay, 2020) Using OSF to Share Data: A Step-by-Step Guide (Soderberg, 2018) Confidence and precision increase with high statistical power (Button et al., 2013) Main Takeaways: High-powered studies will generate formally statistically significant differences for a ‘trivial’ effect. However, in studies with low statistical power, an observed effect will necessarily be large if it is to pass p \u0026lt; .05, but this does not mean that the true effect will be large, or even exist at all. Also, the concern only applies if a p-value \u0026lt; .05 to reject or implicitly accept the null hypothesis. Larger studies protect against inferences from trivial effect sizes by enabling a better estimation of the magnitude of the true effect. Researchers would need to move significance testing to effect sizes and confidence intervals to improve matters. In addition, the true effect size is not known a priori; what is considered trivial can only be determined when the effect size is known. High power provides greater precision in the estimation of the actual effect size so that researchers can assess their importance or triviality with confidence. There needs to be greater emphasis on effect size and confidence intervals than on significance testing. The use of significance testing in the absence of any mention of effect size, confidence intervals or prospective power remains the norm. Some may argue that effect size is not relevant to the theoretical models they wish to test. That may be true if the models are imprecise about effect sizes. However, data from low-powered studies are not useful for testing a theoretical model because they provide little opportunity to find conclusive evidence for or against a model and therefore provide limited scope for model refinement. It would be wonderful if small studies and their research environment were devoid of biases and if all small studies on a particular question of interest could be perfectly integrated. This has not happened. To achieve this would require a major restructuring of the incentives for publishing papers, and especially for publishing novel and positive findings. Simply changing to another p-value threshold does not solve the problem. Abstract Dr Katie Button and colleagues respond to all commentaries on Power failure: why small sample size undermines the reliability of neuroscience and discusses that small sample size does undermine the reliability of neuroscience. APA Style Reference Button, K. S., Ioannidis, J. P., Mokrysz, C., Nosek, B. A., Flint, J., Robinson, E. S., \u0026amp; Munafò, M. R. (2013). Confidence and precision increase with high statistical power. Nature Reviews Neuroscience, 14(8), 585-585. https://doi.org/10.1038/nrn3475-c4\nYou may also be interested in How scientists can stop fooling themselves (Bishop, 2020b) The Statistical Crisis in Science (Gelman \u0026amp; Loken, 2014) Only Human: Scientists, Systems, and Suspect Statistics A review of: Improving Scientific Practice: Dealing With The Human Factors, University of Amsterdam, Amsterdam, September 11, 2014 (Hardwicke et al., 2014) A 21 Word Solution (Simmons et al., 2012) Rein in the four horsemen of irreproducibility (Bishop, 2019) False-Positive Psychology: Undisclosed Flexibility in Data Collection and Analysis Allows Presenting Anything as Significant (Simmons et al., 2011) Many Analysts, One Data Set: Making Transparent How Variations in Analytical Choices Affect Results (Silberzahn et al., 2019) Why small low-powered studies are worse than large high-powered studies and how to protect against “trivial” findings in research: Comment on Friston (2012) (Ingre, 2013) Ironing out the statistical wrinkles in “ten ironic rules” (Lindquist et al., 2013) Ten ironic rules for non-statistical reviewers (Friston, 2012) Current Incentives for Scientists Lead to Underpowered Studies with Erroneous Conclusions (Higginson \u0026amp; Munafo, 2016) Misuse of power: in defence of small-scale science (Quinlan, 2013) Small sample size is not the real problem (Bacchetti, 2013) Measurement error and the replication crisis (Loken \u0026amp; Gelman, 2017) Bite-Size Science and Its Undesired Side Effects (Bertamini \u0026amp; Munafo, 2012) The Leiden Manifesto for research metrics (Hicks et al., 2015) Main Takeaways: Data are increasingly used to govern science. Evaluation is now led by the data rather than by judgement. Metrics have proliferated: usually well intentioned, not always well informed, often ill applied. As scientometricians, social scientists and research administrators, we have watched with increasing alarm the pervasive misapplication of indicators to the evaluation of scientific performance. Everywhere, supervisors ask PhD students to publish in high-impact journals and acquire external funding before they are ready. Researchers and evaluators still exert balanced judgement. Yet the abuse of research metrics has become too widespread to ignore. Scientists searching for literature with which to contest an evaluation find the material scattered in what are, to them, obscure journals to which they lack access. There are ten principles: Quote “Abiding by these ten principles, research evaluation can play an important part in the development of science and its interactions with society. Research metrics can provide crucial information that would be difficult to gather or understand by means of individual expertise. But this quantitative information must not be allowed to morph from an instrument into the goal. The best decisions are taken by combining robust statistics with sensitivity to the aim and nature of the research that is evaluated. Both quantitative and qualitative evidence are needed; each is objective in its own way. Decision-making about science must be based on high-quality processes that are informed by the highest quality data.” (p.431). Abstract Use these ten principles to guide research evaluation, urge Diana Hicks, Paul Wouters and colleagues. APA Style Reference Hicks, D., Wouters, P., Waltman, L., De Rijcke, S., \u0026amp; Rafols, I. (2015). Bibliometrics: the Leiden Manifesto for research metrics. Nature, 520(7548), 429-431. doi:10.1038/520429a\nYou may also be interested in High Impact =High Statistical Standards? Not Necessarily So (Tressoldi et al., 2013) An index to quantify an individual’s scientific research output (Hirsch, 2005) Publication bias in the social sciences: Unlocking the file drawer (Franco et al., 2014) Main Takeaways: Editors and reviewers may prefer statistically significant results and reject sound studies that fail to reject the null hypothesis (i.e. publication bias). As a result, authors may not write up and submit papers that have null findings or have their own preferences to not pursue the publication of null results. We leveraged Time-sharing Experiments in the Social Sciences (TESS),in which researchers propose survey-based experiments to be run on nationally representative samples. The paper aims to compare the statistical results of TESS experiments in published manuscripts and unpublished results. Method: The paper analysed the entire online archive of TESS studies conducted between 2002 and 2012. The analysis was restricted to 221 studies. The outcome of interest is the publication status of each TESS experiment. Results: Although around half of the total studies in our sample were published, only 20% of those with null results were published. In contrast, ~60% of studies with strong results and 50% of those with mixed results were published. Although more than 20% of the studies in our sample had null findings, less than 10% of published articles based on TESS experiments report such results. Although the direction of these results may not be surprising, the observed magnitude is remarkably large. Results: 15 authors reported that they abandoned the project because they believed that null results have no publication potential even if they found the results interesting personally. 9 authors reacted to null findings by reducing the priority of writing up the TESS study and focusing on other projects. 2 authors whose studies “didn’t work out” eventually published papers supporting their initial hypotheses using findings obtained from smaller convenience samples. Researchers might be wasting effort and resources in conducting studies that have already been executed in which the treatments were not efficacious. If future researchers conduct similar studies and obtain statistically significant results by chance, then the published literature will incorrectly suggest stronger effects. Hence, even if null results are characterized by treatments that “did not work” and strong results are characterized by efficacious treatments, authors’ failures to write up null findings still adversely affects the universe of knowledge. A vital part of developing institutional solutions to improve scientific transparency would be to better understand the motivations of researchers who choose to pursue projects as a function of results. Quote “Creating high-status publication outlets for these studies could provide such incentives. The movement toward open-access journals may provide space for such articles. Further, the pre-analysis plans and registries themselves will increase researcher access to null results. Alternatively, funding agencies could impose costs on investigators who do not write up the results of funded studies. Last, resources should be deployed for replications of published studies if they are unrepresentative of conducted studies and more likely to report large effects.” (p.1504). Abstract We studied publication bias in the social sciences by analyzing a known population of conducted studies—221 in total—in which there is a full accounting of what is published and unpublished. We leveraged Time-sharing Experiments in the Social Sciences (TESS), a National Science Foundation–sponsored program in which researchers propose survey-based experiments to be run on representative samples of American adults. Because TESS proposals undergo rigorous peer review, the studies in the sample all exceed a substantial quality threshold. Strong results are 40 percentage points more likely to be published than are null results and 60 percentage points more likely to be written up. We provide direct evidence of publication bias and identify the stage of research production at which publication bias occurs: Authors do not write up and submit null findings. APA Style Reference Franco, A., Malhotra, N., \u0026amp; Simonovits, G. (2014). Publication bias in the social sciences: Unlocking the file drawer. Science, 345(6203), 1502-1505. DOI: 10.1126/science.1255484\nYou may also be interested in The File-drawer problem revisited: A general weighted method for calculating fail-safe numbers in meta analysis (Rosenberg, 2005) The “File Drawer Problem” and Tolerance for Null Results (Rosenthal, 1979) GRADE: an emerging consensus on rating quality of evidence and strength of recommendations (Guyatt et al., 2008) Main Takeaways: Guideline developers around the world are inconsistent in how they rate quality of evidence and grade strength of recommendations. The British Medical Journal has requested in its “Instructions to Authors” on bmj.com that authors should preferably use the Grading of Recommendations Assessment, Development and Evaluation (GRADE) system for grading evidence when submitting a clinical guidelines article. This article will explain why many organisations use formal systems to grade evidence and recommendations and why this is important for clinicians. The authors will focus on the GRADE approach to recommendations. A rigorous system of rating the quality of the evidence is required to evaluate the evidence. Observational studies that show inconsistent results, indicates the evidence is of very low quality. Insufficient attention to quality of evidence risks inappropriate guidelines and recommendations may lead to the detriment of patients wellbeing. Recognising the quality of evidence will help to prevent these errors. Guidelines and recommendations must therefore indicate whether (a) the evidence is high quality and the desirable effects clearly outweigh the undesirable effects, or (b) there is a close or uncertain balance. A simple, transparent grading of the recommendation can effectively convey this key information. There are limitations to formal grading of recommendations. Like the quality of evidence, the balance between desirable and undesirable effects reflects a continuum. Some arbitrariness will therefore be associated with placing particular recommendations in categories such as “strong” and “weak.” Grading systems that are simple with respect to judgments both about the quality of the evidence and the strength of recommendations facilitate use by patients, clinicians, and policy makers. Detailed and explicit criteria for ratings of quality and grading of strength will make judgments more transparent to those using guidelines and recommendations. To achieve transparency and simplicity, the GRADE system classifies the quality of evidence in one of four levels—high, moderate, low, and very low. Some of the organisations using the GRADE system have chosen to combine the low and very low categories. Evidence based on randomised controlled trials begins as high quality evidence, but our confidence in the evidence may be decreased for several reasons, including: Study limitations, Inconsistency of results, Indirectness of evidence, Imprecision and Reporting bias. Although observational studies (e.g. case-control and cohort studies) start with a “low quality” rating, grading upwards may be warranted if the magnitude of the treatment effect is very large (e.g. hip replacement), if there is evidence of a dose-response relation or if all plausible biases would decrease the magnitude of an apparent treatment effect. Quote “When the desirable effects of an intervention clearly outweigh the undesirable effects, or clearly do not, guideline panels offer strong recommendations. On the other hand, when the trade-offs are less certain—either because of low quality evidence or because evidence suggests that desirable and undesirable effects are closely balanced—weak recommendations become mandatory. In addition to the quality of the evidence, several other factors [e.g. uncertainty about the balance between desirable and undesirable effects, Uncertainty or variability in values and Uncertainty about whether the intervention represents a wise use of resources] preferences and affect whether recommendations are strong or weak” (p. 336). Abstract Guidelines are inconsistent in how they rate the quality of evidence and the strength of recommendations. This article explores the advantages of the GRADE system, which is increasingly being adopted by organisations worldwide. APA Style Reference Guyatt, G. H., Oxman, A. D., Vist, G. E., Kunz, R., Falck-Ytter, Y., Alonso-Coello, P., \u0026amp; Schünemann, H. J. (2008). GRADE: an emerging consensus on rating quality of evidence and strength of recommendations. Bmj, 336(7650), 924-926. https://doi.org/10.1136/bmj.39489.470347.AD\nYou may also be interested in References will be included soon Seeking Congruity Between Goals and Roles: A New Look at Why Women Opt Out of Science, Technology, Engineering, and Mathematics Careers (Diekman et al., 2010) ⌺ Main Takeaways: We present a new perspective on this issue by proposing that interest in some careers and disinterest in others results from the intersection of people’s goals and their preconceptions of the goals afforded by different careers. We hypothesize that people perceive Science, Technology, Engineering and Mathematics (STEM) careers as being especially incompatible with an orientation to care about other people (i.e. communion). Because women in particular tend to endorse communal goals, they may be more likely than men to opt out of STEM careers in favor of careers that seem to afford communion. These trends suggest that to explain women’s absence in STEM fields, research should focus on factors that differentiate careers in STEM from other careers. We hypothesize that a critical but relatively unexplored factor may be that many non-STEM careers are perceived as fulfilling communal goals. We thus examined (a) whether communal-goal affordances are perceived to differ between STEM and other careers, and (b) whether communal-goal endorsement inhibits STEM interest, given consensual beliefs about the goals these careers afford. Method: 333 introductory psychology students provided goal-affordance ratings and information about their mathematics and science experience. Our goal was to determine predictors of differential interest in STEM, male-stereotypic/non-STEM (MST), and female-stereotypic (FST) careers. To create scales reflecting these different stereotypic categories, we used archival and primary data. Method: For each core career, participants rated how much they considered the career to fulfill agentic goals (power, achievement, and seeking new experiences or excitement) and communal goals (intimacy, affiliation, and altruism). Method: Because career interest was our critical dependent measure, participants rated their interest in the core careers, as well as additional careers. Method: Participants rated several goals according to “how important each of the following kinds of goals is to you personally,” on scales ranging from 1 (not at all important) to 7 (extremely important). Method: Self-efficacy and experience. Measures of self-efficacy included the scientific, mechanical, and computational subscales of the Kuder Task Self-Efficacy Scale as well as participants’ estimated grades in STEM classes. Results: The authors found that STEM careers, relative to other careers, were perceived to impede communal goals. Moreover, communal-goal endorsement negatively predicted interest in STEM careers, even when controlling for past experience and self-efficacy in science and mathematics. STEM careers are perceived as inhibiting communal goals: When individuals highly endorse communal goals, they are less interested in STEM. If women perceive STEM as antithetical to highly valued goals, it is not surprising that even women talented in these areas might choose alternative career paths. Certainly, traditionally studied predictors of STEM interest, such as agentic motivations or self-efficacy, continue to be critical factors. Our argument is not that the study of communal motivations should replace agentic motivations or self-efficacy, but that this traditional approach overlooks critically important information. Quote “It is ironic that STEM fields hold the key to helping many people, but are commonly regarded as antithetical (or, at best, irrelevant) to such communal goals. However, the first step toward change is increasing knowledge about this belief and its consequences. Interventions could not only provide opportunities for girls and young women to succeed in mathematics and science but also demonstrate how STEM fields involve helping and collaborating with other people. For example, our current research investigates how portraying science or engineering careers as more other-oriented fosters positivity.” (p.1056). Abstract Although women have nearly attained equality with men in several formerly male-dominated fields, they remain underrepresented in the fields of science, technology, engineering, and mathematics (STEM). We argue that one important reason for this discrepancy is that STEM careers are perceived as less likely than careers in other fields to fulfill communal goals (e.g., working with or helping other people). Such perceptions might disproportionately affect women’s career decisions, because women tend to endorse communal goals more than men. As predicted, we found that STEM careers, relative to other careers, were perceived to impede communal goals. Moreover, communal-goal endorsement negatively predicted interest in STEM careers, even when controlling for past experience and self-efficacy in science and mathematics. Understanding how communal goals influence people’s interest in STEM fields thus provides a new perspective on the issue of women’s representation in STEM careers. APA Style Reference Diekman, A. B., Brown, E. R., Johnston, A. M., \u0026amp; Clark, E. K. (2010). Seeking congruity between goals and roles: A new look at why women opt out of science, technology, engineering, and mathematics careers. Psychological science, 21(8), 1051-1057. https://doi.org/10.1177/0956797610377342\nYou may also be interested in Unequal effects of the COVID-19 pandemic on scientists (Myers et al., 2019) Against Eminence (Vazire, 2017) Scientific Eminence: Where Are the Women? (Eagly \u0026amp; Miller, 2016) The Gender Gap: Who Is (and Is Not) Included on Graduate-Level Syllabi in Social/Personality Psychology (Skitka et al., 2020) Leveraging a collaborative consortium model of mentee/mentor training to foster career progression of under-represented post-doctoral researchers and promote institutional diversity and inclusion (Risner et al., 2020) Communism, Universalism and Disinterestedness: Re-examining Contemporary Support among Academics for Merton’s Scientific Norms (Macfarlane \u0026amp; Cheng, 2008) Main Takeaways: This paper will re-examine these influential accounts of academic values and develop a contemporary interpretation of the values that underpin commitment to the academic ‘vocation’. The four norms of Mertonian science are: Communism (i.e. the results of their research should be the common property of the whole scientific community), disinterestedness (i.e. scientists should have no emotional or financial attachments to their work), universalism (i.e. Academic knowledge should transcend race, class, political and/or religious barriers) and Organised skepticism (i.e. the expectation that academics will continually challenge conventional wisdom in their discipline). The paper tests the contemporary relevance of Merton’s institutional norms through a web-based survey of academic values. In questioning the contemporary sway of Merton’s norms they will be contrasted with an alternative set of academic values: capitalism (or individualism), interestedness and particularism. Method: A web-based survey instrument was designed to test out the extent to which academics agree with three of Merton’s four institutional norms. Four value statements were designed to test out the extent to which respondents either agreed or disagreed with communism, universalism and disinterestedness. Results: The authors observed that there is strong support for communism followed by universalism and weak support for disinterestedness. The results of this limited survey indicate that there is still substantial support for at least one of Merton’s norms, namely communism. However, it is also clear that they are being re-shaped by a number of contemporary trends and influences. While most academics in this survey were supportive of communism as a norm there are more capitalist and individualistic forces at work. These policies are designed to maximise the commercial benefits derived from academic work for individual universities rather than share the results of research for the benefit of all. Academics are encouraged to seek a public profile rather than shy away from self promotion opportunities. The pressures of performativity mean that academics can no longer afford to be as committed as they might like to be to disinterestedness as a norm. Indeed, the survey indicates more opposition than support for this norm. ‘Interestedness’ appears to have displaced disinterestedness. Most respondents indicated that they were comfortable with the idea of contributing to public debate in areas that fall outside their expertise.Contemporary academics may regard such interventions as a right as a citizen and as a legitimate extension of academic freedom. Many academics are also prepared to direct or re-direct their efforts toward available funding opportunities. Gaining funding for research and directing one’s research agenda in the direction of the contemporary concerns of public policymakers is a fact of life that has re-shaped attitudes to ‘disinterested’ research. Quote “In contrast with communism, capitalism as a norm would imply a belief in maximising individual financial return on academic endeavour in a market economy. Here, conceptualising the role of research as an income generation activity affirms this alternative norm. Particularism implies a belief that knowledge is individually constructed on the basis of social experiences and political forces. It implies a rejection of absolute social, cultural or religious values in favour of moral relativism. A particularist stance might further be characterised by opposition to the cultural hegemony of Western products, systems and modes of thought. Finally, interestedness is closely related to the belief that academic enquiry can never be a value-free, dispassionate analysis of the observed ‘facts’. It is a norm that essentially rejects the positivist paradigm.” (p.77). Abstract This paper re-examines the relevance of three academic norms to contemporary academic life – communism, universalism and disinterestedness – based on the work of Robert Merton. The results of a web-based survey elicited responses to a series of value statements and were analysed using the weighted average method and through crosstabulation. Results indicate strong support for communism as an academic norm defined in relation to sharing research results and teaching materials as opposed to protecting intellectual copyright and withholding access. There is more limited support for universalism based on the belief that academic knowledge should transcend national, political, or religious boundaries. Disinterestedness, defined in terms of personal detachment from truth claims, is the least popular contemporary academic norm. Here, the impact of a performative culture is linked to the need for a large number of academics to align their research interests with funding opportunities. The paper concludes by considering the claims of an alternate set of contemporary academic norms including capitalism, particularism and interestedness. APA Style Reference Macfarlane, B., \u0026amp; Cheng, M. (2008). Communism, universalism and disinterestedness: Re-examining contemporary support among academics for Merton’s scientific norms. Journal of Academic Ethics, 6(1), 67-78. https://doi.org/10.1007/s10805-008-9055-y\nYou may also be interested in The bases for generalisation in scientific methods (Rousmaniere, 1909) The bases for generalisation in scientific methods (Rousmaniere, 1909) Main Takeaways: In that all true induction involves generalizing on the basis of particulars, the question of the conditions under which various numbers of particulars are required for a generalization stands as a fundamental question in discussing inductive methods. The most marked contrast within the field of generalization is between those cases where we draw our conclusions with apparently reckless trust in a few examples and those where we build our foundations very nearly as wide as the superstructure. Roughly, the difference is between the more exact sciences and the use of statistical methods. One part of scientific investigation which calls most for judgment and wide knowledge centers around the question of the place of uniformity, the parts to be sampled, if one is to win a fair idea of the quality of a boatload of grain. Sometimes this method of using a carefully selected group is called into play where the investigator\u0026#39;s interest is in pointing out or studying the very variation itself. Where such uniformity as is found belongs to the field as a whole, it is true that we also sometimes call for a small group instead of a single example in testing for the nature of that field, but such a group is very different, from a logical point of view, from the selected group. There is no selection here, variation is generally and vaguely accepted as possible, not specifically placed. Quote “So long as the conditions that determine variation are believed standard, that is, in play in all the groups compared, the use of averages to represent the different groups is legitimate, but under the suggestion that difference in race varied those conditions in part, belief in the genuineness of that representation withers away.” (p. 205). Abstract Rousmaniere discusses the bases for generalisations concerning scientific evidence. APA Style Reference Rousmaniere, F. H. (1909). The bases for generalization in scientific methods. The Journal of Philosophy, Psychology and Scientific Methods, 6(8), 202–205 https://doi.org/10.2307/2011346\nYou may also be interested in Communism, Universalism and Disinterestedness: Re-examining Contemporary Support among Academics for Merton’s Scientific Norms (Macfarlane \u0026amp; Cheng, 2008) False-Positive Citations (Simmons et al., 2017) Main Takeaways: When results in the scientific literature disagree with our intuition, researchers should be able to trust the literature enough to question our beliefs rather than to question the findings. The authors were questioning the findings. Something was broken. The authors knew many researchers who readily admitted to dropping dependent variables, conditions, or participants to achieve significance. Everyone knew it was wrong, but they thought it was wrong the way it is wrong to jaywalk. The authors decided to write “False-Positive Psychology” when simulations revealed that it was wrong the way it is wrong to rob a bank. An article cannot be influential if it is not read, and no one likes to read boring or hard-to-understand articles. So we tried to make sure that our article was accessible and at least a little bit entertaining. To help accomplish this, the authors ran two experiments demonstrating how p-hacking could allow us to find significant evidence for an obviously false hypothesis. It was not hard to generate a false hypothesis to test, but in a field that seemed ready to believe in lots of things, it was hard to generate one that was obviously false. The authors knew that their article could not lead to real change if we just complained about the problem. So they spent a long time thinking about solutions, seeking out one that would require the least of researchers and journals while achieving the most for knowledge and truth. The authors wanted to ask authors to simply describe what they did in their studies. Specifically, the authors proposed that researchers should be required to disclose how they arrived at their sample sizes and to report all of their measures, manipulations, and exclusions. If the authors went back in time to 2010, they would not recommend that authors be required to have more than 20 observations per cell, because that led people to focus on the wrong aspect of disclosure and emphasize that you cannot consistently get underpowered studies to work without p-hacking (or an implausible amount of luck). In addition, the authors would modify the n \u0026gt; 20 rule in two ways: choose a larger reference point and not advocate for a strict sample-size cutoff. Since 2010, the authors believe in preregistration as it gives researchers the freedom to conduct analyses that could, if disclosed afterward, seem suspicious, such as excluding participants who failed an attention check or running an unusual statistical test. Second, it is simply a more verifiable form of disclosure. Indeed, preregistration is the only way for authors to irrefutably demonstrate that their key analyses were not p-hacked. Preregistration makes you immune to suspicions of p-hacking. Preregistration is now routine in our own labs, and, if you are in the business of collecting and analyzing new data, we see no counterargument to doing it. Preregistration does not restrict the ability to conduct exploratory analyses; it merely allows the researcher and the reader to properly distinguish between analyses that were planned and exploratory. In addition, it does not prevent researchers from publishing results that do not confirm their hypothesis; the critical aspect of preregistration is not the prediction that the researcher makes but, rather, the methodological and analytical plan that the researcher specifies. It is perfectly acceptable to simply pose a research question and describe exactly how you intend to answer it, and it is perfectly acceptable to publish a finding that did not conform to your original prediction. Quote “How could any half serious scientist actively oppose a rule requiring authors to accurately describe their research?...In 2010, approximately 0% of researchers were disclosing all of their methodological details, posting their data and materials, and preregistering their studies. Today, disclosure, data posting, and preregistration are slowly becoming the norm, particularly among the younger generation of researchers. We would like to think that our article had something to do with all of this, but honestly, it is impossible to say, because hundreds of psychologists have worked incredibly hard to improve our science. Without them, our article would have had no influence whatsoever. And without our article, these changes may have happened anyway. It was time. It is time.” (p.256). Abstract We describe why we wrote “False-Positive Psychology,” analyze how it has been cited, and explain why the integrity of experimental psychology hinges on the full disclosure of methods, the sharing of materials and data, and, especially, the preregistration of analyses. APA Style Reference Simmons, J. P., Nelson, L. D., \u0026amp; Simonsohn, U. (2018). False-positive citations. Perspectives on Psychological Science, 13(2), 255-259. https://doi.org/10.1177/1745691617698146 . [ungated]\nYou may also be interested in How scientists can stop fooling themselves (Bishop, 2020b) The Statistical Crisis in Science (Gelman \u0026amp; Loken, 2014) Only Human: Scientists, Systems, and Suspect Statistics A review of: Improving Scientific Practice: Dealing With The Human Factors, University of Amsterdam, Amsterdam, September 11, 2014 (Hardwicke et al., 2014) A 21 Word Solution (Simmons et al., 2012) Rein in the four horsemen of irreproducibility (Bishop, 2019) Seven Steps Toward Transparency and Replicability in Psychological Science (Lindsay, 2020) The life of p: “Just significant” results are on the rise (Leggett et al., 2013) Many Analysts, One Data Set: Making Transparent How Variations in Analytical Choices Affect Results (Silberzahn et al., 2019) Scientific inbreeding and same-team replication: Type D personality as an example (Ioannidis, 2012) Pandemic Darkens Postdocs’ work and careers hopes (Woolson, 2020) Main Takeaways: The pandemic has shuttered or reduced the output of academic labs globally, slashed institutional budgets and threatened the availability of grants, fellowships and other postdoctoral funding sources. The fallout adds up to a major challenge for a group of junior researchers who were already grappling with limited funds, intense job competition and career uncertainties. 1% of respondents say that they have been diagnosed with COVID-19, and another 9% suspect that they have had the infection but were never tested. But concerns go far beyond the presence or absence of the virus. Some 61% of respondents say that the pandemic has negatively affected their career prospects, and another 25% say that its cumulative effects on their career remain uncertain. Worries about one’s professional future are especially widespread in South America, where 70% of respondents say their careers have already suffered since the start of the pandemic. Belief that the pandemic had already negatively affected career prospects were also common in North and Central America (68%), Australasia (68%), Asia (61%), Africa (59%) and Europe (54%). In China, where the virus was first detected, 54% of respondents said their career had already suffered and 25% said they weren’t sure. Perceived impacts varied by area of study. Slightly less than half of researchers in computer science and mathematics thought that their career prospects had suffered, compared with 68% of researchers in chemistry, 67% in ecology and evolution, and 60% in biomedicine. The impact of the pandemic has now joined the list of the top concerns in the minds of postdocs. Asked to name the three primary challenges to their career progression, 40% of respondents point to the economic impact of COVID-19, nearly two-thirds (64%) note the competition for funding, and 45% point to the lack of jobs in their field. 13% of respondents say they have already lost a postdoc job or an offer of one as a result of the pandemic, and 21% suspected the virus had wiped out a job but weren’t sure. More than one-third of researchers in South America report already losing a job, compared with 11% in Europe and 12% in North and Central America. 60%of respondents are currently working abroad, a circumstance that only amplifies the pandemic’s potential impact. On top of everything else, many worry about the pandemic’s effect on their visas and their ability to stay in their new country. Experiments aren’t the only scientific activities that can suffer during a pandemic. 59% of respondents said that they had more trouble discussing ideas with their supervisor or colleagues, and 57% said that the pandemic had made it harder to share their research findings. The survey included questions about supervisors, a role that takes on extra importance during a crisis. 54% of respondents said that their supervisor had provided clear guidance on managing their work during the pandemic, but 32% said that they weren’t receiving that sort of support from above. 29% of respondents strongly or somewhat disagreed that their adviser has done everything they can to support them during the pandemic. Female respondents (28%) were more likely than male respondents (25%) to think that their supervisors fell short. The free-comment section of the survey underscores how the pandemic has strained some supervisor–postdoc relationships. Some postdocs have found small consolations in the pandemic. Although 26% of respondents say that the pandemic has somewhat or significantly impaired their ability to write papers, 43% say that writing has become easier. Abstract Nature’s survey of this key segment of the scientific workforce paints a gloomy picture of interrupted research and anxiety about the future by Chris Woolston. APA Style Reference Woolston, C. (2020). Pandemic darkens postdocs\u0026#39; work and career hopes. Nature, 585(7824), 309-312. https://doi.org/10.1038/d41586-020-02548-2\nYou may also be interested in Seeking an exit plan (Woolston, 2020) Unequal effects of the COVID-19 pandemic on scientists (Myers et al., 2019) Leveraging a collaborative consortium model of mentee/mentor training to foster career progression of under-represented post-doctoral researchers and promote institutional diversity and inclusion (Risner et al., 2020) ⌺ The mental health of PhD researchers demands urgent attention (Nature, 2019) Postdocs in crisis: science risks losing the next generation (Nature, 2020) Boosting research without supporting universities is wrong-headed (Nature, 2020b) Against Eminence (Vazire, 2017) The importance of stupidity in scientific research (Schwartz, 2008) Main Takeaways: Science makes me feel stupid too. It’s just that I’ve gotten used to it. So used to it, in fact, that I actively seek out new opportunities to feel stupid. I wouldn’t know what to do without that feeling. For almost all of us, one of the reasons that researchers liked science in high school and college is that they were good at it. That can’t be the only reason – fascination with understanding the physical world and an emotional need to discover new things has to enter into it too. But high-school and college science means taking courses, and doing well in courses means getting the right answers on tests. If you know those answers, you do well and get to feel smart. Once I faced that fact, nobody knew the answer to my research problem, only that it was up to me to solve my research problem. I solved the problem in a couple of days. (It wasn’t really very hard; I just had to try a few things.) The crucial lesson was that the scope of things I didn’t know wasn’t merely vast; it was, for all practical purposes, infinite. That realization, instead of being discouraging, was liberating. First, I don’t think students are made to understand how hard it is to do research. And how very, very hard it is to do important research. It’s a lot harder than taking even very demanding courses. What makes it difficult is that research is immersion in the unknown. We just don’t know what we’re doing. We can’t be sure whether we’re asking the right question or doing the right experiment until we get the answer or the result. Admittedly, science is made harder by competition for grants and space in top journals. Productive stupidity means being ignorant by choice. Focusing on important questions puts us in the awkward position of being ignorant. One of the beautiful things about science is that it allows us to bumble along, getting it wrong time after time, and feel perfectly fine as long as we learn something each time. Quote “We don’t do a good enough job of teaching our students how to be productively stupid – that is, if we don’t feel stupid it means we’re not really trying…Science involves confronting our ‘absolute stupidity’. That kind of stupidity is an existential fact, inherent in our efforts to push our way into the unknown... I think scientific education might do more to ease what is a very big transition: from learning what other people once discovered to making your own discoveries. The more comfortable we become with being stupid, the deeper we will wade into the unknown and the more likely we are to make big discoveries.” (p.1771). Abstract This is an essay written by Dr Martin Schwartz about the importance of stupidity in scientific research in helping researchers to understand and answer their research problems. APA Style Reference Schwartz, M. A. (2008). The importance of stupidity in scientific research. Journal of Cell Science, 121(11), 1771-1771. doi: 10.1242/jcs.033340\nYou may also be interested in Add references soon Constraints on Generality (COG): A Proposed Addition to All Empirical Papers (Simons et al., 2017) Main Takeaways: When a paper identifies a target population and specifies constraints on generality (COG) of findings, researchers conduct direct replications that sample from the target population, leading to more appropriate tests of reliability of the original claim. A COG statement indicates why the sample and target population is representative, justifying why subjects, materials, and procedures are representative of broader populations. A COG statement does not limit the claim but leads the reader to correctly infer these findings are limited to the groups of populations being tested such as undergraduate students. A COG statement establishes boundaries on generality rather than “replication failure.” A COG statement inspires follow-up studies building on results by testing generality populations not originally tested. A COG statement encourages reviewers and editors more receptive to next-step studies to test constraints identified. A COG statement should be included in all papers, so editors support manuscripts with well-justified constraint on generality statements explicitly ground claims of generality. Editors can evaluate whether claims are sufficiently important to justify publication. A COG statement incentivises cumulative follow-up research, leading to greater reliability, influence and increased citations. This COG statement values rigor, honesty, accuracy and supports the conclusion justified by evidence and theory, allowing readers to understand the limits of generalisability. If science was more cumulative and self-correcting, broad generalisation might be justifiable. A COG statement describes known or anticipated limits on finding and not mediation by unknown factors. It asks how our sample is representative of a broader population. Abstract Psychological scientists draw inferences about populations based on samples—of people, situations, and stimuli—from those populations. Yet, few papers identify their target populations, and even fewer justify how or why the tested samples are representative of broader populations. A cumulative science depends on accurately characterizing the generality of findings, but current publishing standards do not require authors to constrain their inferences, leaving readers to assume the broadest possible generalizations. We propose that the discussion section of all primary research articles specify Constraints on Generality (i.e., a “COG” statement) that identify and justify target populations for the reported findings. Explicitly defining the target populations will help other researchers to sample from the same populations when conducting a direct replication, and it could encourage follow-up studies that test the boundary conditions of the original finding. Universal adoption of COG statements would change publishing incentives to favor a more cumulative science. APA Style Reference Simons, D. J., Shoda, Y., \u0026amp; Lindsay, D. S. (2017). Constraints on generality (COG): A proposed addition to all empirical papers. Perspectives on Psychological Science, 12(6), 1123-1128. https://doi.org/10.1177/1745691617708630 [ungated]\nYou may also be interested in Seven Steps Toward Transparency and Replicability in Psychological Science (Lindsay, 2020) Most people are not WEIRD (Henrich et al., 2010) #bropenscience is broken science (Whitaker \u0026amp; Guest, 2020) ⌺ Main Takeaways: [Scientists have] been spurred into action by a variety of disappointing stories about irreplicable research – due to both purposeful misconduct and variable guidance for transparent reporting standards – as well as inspired by the pre-existing ideals of ‘open science’. It is a very narrow demographic of researchers who have the institutional support to spend time on such projects as well as the fortune to be publicly acknowledged for their hard work. However, #bropenscience has also been misunderstood and misrepresented. Not all men are bros, and not all bros are men. A bro will often be condescending, forthright, aggressive, overpowering, and lacking kindness and self-awareness. Although bros solicit debate on important issues, they tend to resist descriptions of the complexities, nuances, and multiple perspectives on their argument. Bros find it hard to understand – or accept – that others will have a different lived experience. At its worst, #bropenscience is the same closed system as before. Broscience creates new breaks within science such as excluding people from participating in open science generally due to the behaviour of a vocal, powerful and privileged minority. It’s a type of exclusionary, monolithic, inflexible rhetoric that ignores or even builds on structural power imbalances. Most researchers don’t fit neatly into many of the broposed solutions, and science is not a monolith. The authors have both dealt with published findings that cannot be reproduced, been driven by frustration at the inefficiency of current research practices and have different work ethics and philosophies. This is a feature, not a bug. A diverse and inclusive definition of open science is necessary to truly reform academic practice. At its core, open scholarship reminds researchers why they wanted to conduct research in the first place: to learn and to educate. Regardless of individual intentions, groups can easily develop and perpetuate elitist, yet informal social structures, recreating the same biases inherent in society at large. Bro-y culture dominates at the leadership level in science and technology because it always has and there aren’t enough explicit processes to deconstruct these biases. To avoid perpetuating ‘bropen’ practices, the authors recommend following three core principles: Understanding: You make the work accessible and clear; Sharing: You make the work easy to adapt, reproduce, and spread; and Participation \u0026amp; inclusion: You build shared ownership and agency with contributors through accountability, equity, and transparency to make the work inviting, relevant, safe, and sustainable for all. Inclusive actions that you can take to make science more open to underrepresented minorities include: using a microphone at in person events or providing live transcription and sign language translation for online events so that hard of hearing and autistic colleagues (among others) can engage more effectively. Editors and tenured faculty members can and should do the most to improve equity and inclusion in academia. What are the actions you can take that will improve scholarship for all? Ultimately, the only way to dismantle structural and systemic biases is to listen to those who experience them. Quote “It’s likely infeasible to include all the possible open scholarship elements mentioned above in the readers’ work. Therefore, and to change metaphors, the authors encourage the reader to take a healthy and balanced portion from the open science buffet...Binging from the many different topics that fall under open scholarship will leave you feeling overwhelmed and exhausted...take what you can and what benefits you now, and then come back for more when you have the time and mental space to develop a new skill.” (p.2) Abstract Kirstie Whitaker and Olivia Guest ask how open ‘open science’ really is. APA Style Reference Whitaker, K., \u0026amp; Guest, O. (2020). #bropenscience is broken science. The Psychologist, 33, 34-37. You may also be interested in Seeking an exit plan (Woolston, 2020) The mental health of PhD researchers demands urgent attention (Nature, 2019) Postdocs in crisis: science risks losing the next generation (Nature, 2020) Boosting research without supporting universities is wrong-headed (Nature, 2020b) Unequal effects of the COVID-19 pandemic on scientists (Myers et al., 2019) Against Eminence (Vazire, 2017) Scientific Eminence: Where Are the Women? (Eagly \u0026amp; Miller, 2016) The Gender Gap: Who Is (and Is Not) Included on Graduate-Level Syllabi in Social/Personality Psychology (Skitka et al., 2020) Leveraging a collaborative consortium model of mentee/mentor training to foster career progression of under-represented post-doctoral researchers and promote institutional diversity and inclusion (Risner et al., 2020) Prestige drives epistemic inequality in the diffusion of scientific ideas (Morgan et al., 2018) On supporting early-career black scholars (Roberson, 2020) Open Science Isn’t Always Open to All Scientists (Bahlai et al., 2019) The Matthew effect in science funding (Bol et al., 2018) ls There a Positive Correlation between Socioeconomic Status and Academic Achievement? (Quagliata, 2008) Education and Socio-economic status (APA, 2017b) Ethnic and Racial minorities and socio-economic status (APA, 2017) Women and Socio-economic status (APA, 2010) Disability and Socio-economic status (APA, 2010) Lesbian, Gay, Bisexual and Transgender Persons \u0026amp; Socioeconomic Status (APA, 2010) Participant Attentiveness to Consent Forms (Baker \u0026amp; Chartier, 2018) Main Takeaways: Consent forms rarely receive participants’ full attention. To improve attention in this area, it is important to identify and understand what causes an individual to be inattentive. It is also important to understand why individuals decide to consent without being informed, as well as their level of competence. The reasons that many participants admittedly do not understand a document but still sign may include the formal and official style of the document, a feeling of time pressure, and an inadequate style of presentation of the materials included on a consent form document. It is necessary to be able to identify careless responses that skew results and cause inconsistencies, in order to draw valid and reproducible conclusions. The authors predicted that participants would be unlikely to accurately recall the code word prompt, while our secondary prediction was that participants would be equally inattentive to the code word at the middle of the form and the end of the form. Method: 136 gave participants a pencil and the sheets necessary to complete the attentiveness task. A set of eight copies of the consent form was created with the code word being placed above a different section of the form. The consent forms were identical except for the placement of the code word. Each code word was placed directly above the beginning of the new section. Method: When participants arrived, we gave them a consent form to sign. We then told participants to read through the consent form and sign it when they had finished reading. After participants signed the consent form, we read the instructions of the filler task orally as we gave them materials to complete the filler experiment. Following the completion of the filler task, participants were given a third sheet of paper, which we described as a demographic sheet. This sheet included the prompt “what is the code word?” asking for the consent form code word. Results: The vast majority of participants did not respond to the prompt asking for a code word; some provided an incorrect guess using words such as pizza, baseball, and password. Code word location and the frequency of correct responses were not significantly associated. The results did not support that there would be a difference in correct response rate depending on the location of the code word. There was no significant difference across groups in attentiveness. There was a low rate of correct response and an overall low rate of attentiveness to the consent form. This shows that there is a need for ways to improve this inefficacy. It is assumed that participants are aware of what they have consented to. By giving consent, participants are essentially saying that they understand and accept responsibility for what is expected of them as well as any distress, harm, or otherwise unexpected outcome that may occur. If participants do not know what they are giving consent to, a number of negative outcomes could occur: participants may not perform the procedures of the experiment correctly, leaving their results invalid or difficult to interpret; participants could endanger themselves due to a health complication involved in the experiment that they remain unaware of; participants could endanger others for their lack of understanding of what is expected of them; and other possibilities not specified. When participants give consent, an experimenter should not be expected to provide a repetition of what they have just given consent to. Although the current study demonstrated the problem with attentiveness in paper and pencil consent forms, research has shown that online studies may suffer even more in attention levels. Abstract In the present study, we tested to see if participants were attentive to details in the consent form for a psychological experiment before signing it. Our initial hypothesis was that participants might not read attentively, due to perceiving the information to be mundane. Depending on condition, the code word was placed in an early, middle, or late section of the consent form. This experiment allowed us to analyze whether participants read through the consent form, and if they paid more attention to a specific part of the form than others. We asked participants to read through the consent form and sign at the bottom when they were finished. Following their signed consent, we orally gave instructionson how to complete the filler task. At the conclusion of the study, participants were given a prompt to recall the code word. The results of this preregistered study show that, of the 136 participants, only 20 participants correctly recalled the code word. A χ2 test of independence revealed that successfully noticing the code word did not depend on the location on the consent form, χ2(2, N = 136) = 0.67, p = 0.72, φ = 0.07. The results of this study show that students did not differentially respond to different parts of the consent form. APA Style Reference Baker, D., \u0026amp; Chartier, C. R. (2018). Participant attentiveness to consent forms. Psi Chi Journal of Psychological Research, 23(2), 142–146. https://doi.org/10.24839/2325-7342.JN23.2.141.\nYou may also be interested in See references for later Protect students and staff in precarious jobs (Anon, 2020b) Main Takeaways: Coronavirus lockdowns have precipitated a crisis in university funding and academic morale. When lockdowns were announced, universities all over the world closed their doors. They moved classes and some research activity online. But staff were given little or no time to prepare and few resources or training to help them. Fewer students are expected to enrol in the coming academic year, instead waiting until institutions open fully. That means that young people will lose a year of their education, and universities will lose out financially. Some governments have plans to boost post-lockdown research, but these will be undermined if universities decide to make job cuts and end up with staff shortages. Universities need support at this crucial time. Low- and middle-income countries face extra challenges from the sudden transition to online learning. The main concern is for students who are unable to access digital classrooms. This is especially the case for those who live in areas without fast, reliable and affordable broadband, or where students have no access to laptops, tablets, smartphones and other essential hardware. Teachers in many countries have been reporting that students in such situations have been struggling to keep up since lockdowns began. In some cases, students from poorer households in remote regions are having to travel to their nearest city to access the Internet, and to pay commercial Internet cafes to download course materials. There is a way to get the technology to under-served areas, and to avert redundancies. But it requires governments and funding bodies to accept that students and universities should be eligible for the same kinds of temporary emergency funding as other industries are asking for. What governments in these countries and other countries need to realize is that the impact of such decisions will fall disproportionately on the poorest students and on more vulnerable members of staff. Job cuts are more likely to affect people whose employment is less secure, such as those on fixed-term contracts. And such staff will, in turn, include people from minority groups, who are often over-represented in contract staff. There are smaller actions that institutions and academics can take. Students, and staff on short-term contracts, would welcome more support from academic colleagues in senior positions and from others with permanent positions, for example. Quote “These colleagues should make the case to their managers that failing to provide more help to low-income students, or cutting the number of postdoctoral staff and teaching fellows will harm the next generation of researchers and teachers. It will also drastically reduce departments’ capacity to teach and increase the load on those who remain, who are often forced to taking on the teaching responsibilities of their former colleagues. Senior colleagues can also request assessments of how any planned redundancies will affect equality and diversity. Cutting back on scholarly capacity is always unwise, but to do so while increasing spending on R\u0026amp;D is wrong-headed. It will slow down economic recovery and jeopardize plans to make research more inclusive. Yet again, the academic precariat finds itself at a disadvantage. Governments, research managers and senior colleagues have a duty to help so that universities can keep these essential and valuable employees.” (p.314). Abstract Universities face a severe financial crisis, and some contract staff are hanging by a thread. Senior colleagues need to speak up now. APA Style Reference Anon (2020) . Protect students and staff in precarious job. Nature, 582, 313-314. https://media.nature.com/original/magazine-assets/d41586-020-01788-6/d41586-020-01788-6.pdf\nYou may also be interested in Seeking an exit plan (Woolston, 2020) The mental health of PhD researchers demands urgent attention (Nature, 2019) Postdocs in crisis: science risks losing the next generation (Nature, 2020) Boosting research without supporting universities is wrong-headed (Nature, 2020b Unequal effects of the COVID-19 pandemic on scientists (Myers et al., 2019) Don’t erase undergrad researchers and technicians from author lists (Fogarty, 2020) Semiretirement is treating me well—and it made room for a younger scientist (Larson, 2018) Main Takeaways: The official asked the author to investigate how eliminating mandatory retirement has affected the availability of positions for new assistant professors. The question struck with the author as important but not personally relevant—until the author and their colleagues got their results. Their initial intuition was that there would be no substantial long-term effect. The authors expected to find that the number of open positions dipped just after the law’s two changes. After all, the number of available tenure-track faculty slots is essentially fixed—at MIT, there are approximately 1000. To create room for a new faculty member, an existing one has to leave. But after a brief dip, we thought, retirements should return to normal, creating room for new recruits. The authors discovered that eliminating the retirement age had reduced the number of new slots for MIT assistant professors by 19%, from 57 to 46 per year. Put simply, without a mandatory retirement age, senior faculty members are much slower to leave. When their paper was published, the author viewed it as just another finding. But eventually, the author had serious reflections about what the results really meant. There are simply too many applicants seeking too few positions. The author and other professors older than 65 were blocking the way of many young scholars who seek academic careers. The author started to wonder whether it was time for me to step aside, but the idea of leaving the job the author had been tied to for so long was hard to swallow. The dean of the engineering school heard about our paper and asked the author to go over the details with him. It must have resonated with him, because he briefed the department heads about the need for a flexible after-tenure option that would vacate a position and open the way for a new hire. They soon invented “professor, post-tenure,” tossing out an earlier option with the horrendous name “professor without tenure, retired,” (i.e. PWOTR). Once “professor, post-tenure” was announced in 2016, the author found it increasingly attractive. It wasn’t the same as “emeritus”—not full retirement. The author could retain their office, teach and supervise students, and be a principal investigator on research grants—all with great flexibility. The author would get to choose which projects they wanted to do and be paid accordingly, up to 49% of my previous salary. They could also access retirement and pension funds. Quote “At 74, I in essence removed 9 years from someone else’s career. I should have stepped aside sooner” (p. 3). Abstract An article by Professor Richard Larson about semi-retirement to allow older scientists to retire and allow young scientists to enter academia. APA Style Reference Larson, R. C. (2018). Semiretirement is treating me well—and it made room for a younger scientist. Science. doi:10.1126/science.caredit.aav8986\nYou may also be interested in Seeking an exit plan (Woolston, 2020) The mental health of PhD researchers demands urgent attention (Nature, 2019) Postdocs in crisis: science risks losing the next generation (Nature, 2020) The Relation Between Family Socioeconomic Status and Academic Achievement in China: A Meta-analysis (Liu et al., 2020) Main Takeaways: In the current study, we conducted a meta-analysis on the relation between SES and academic achievement within the context of Mainland China. The current meta-analysis aimed to investigate the relation between SES and academic achievement among students in the basic education stage in mainland China and possible potential moderators on this relation including year, grade level, type of SES measures, and subjects of academic achievement. Method: A study was included if it met the following criteria: Abstract Academic achievement is one of the most important indicators for assessing students’ performance and educational attainment. Family socioeconomic status (SES) is the main factor influencing academic achievement, but the relation between SES and academic achievement may vary across different sociocultural contexts. China is the most populous developing country with a large number of schooling students in the basic education stage. Chinese schools are unified and managed by the Ministry of Education, but the central and local governments in accordance with their responsibilities share the investment of educational funds. However, the strength of the relation between SES and academic achievement and possible moderators of this relation remain unclear. Therefore, we conducted a meta-analysis on the relation between SES and academic achievement based on 215,649 students from 78 independent samples in the basic education stage from mainland China. The results indicated a moderate relation between SES and academic achievement (r = 0.243) in general. Moderation analyses indicated that the relation between SES and academic achievement gradually decreased in the past several decades; SES has a stronger correlation with language achievement (i.e., Chinese and English) than science/math achievement and general achievement. These findings were discussed from the perspective of governmental policies on education. APA Style Reference Liu, J., Peng, P., \u0026amp; Luo, L. (2020). The relation between family socioeconomic status and academic achievement in China: a meta-analysis. Educational Psychology Review, 32(1), 49-76. https://doi.org/10.1007/s10648-019-09494-0\nYou may also be interested in #bropenscience is broken science (Whitaker \u0026amp; Guest, 2020) ⌺ ls There a Positive Correlation between Socioeconomic Status and Academic Achievement? (Quagliata, 2008) Education and Socio-economic status (APA, 2017b) Ethnic and Racial minorities and socio-economic status (APA, 2017) Women and Socio-economic status (APA, 2010) Disability and Socio-economic status (APA, 2010) Lesbian, Gay, Bisexual and Transgender Persons \u0026amp; Socioeconomic Status (APA, 2010) The Relation Between Family Socioeconomic Status and Academic Achievement in China: A Meta-analysis (Liu et al., 2020) Is science only for the rich? (Lee, 2016) ⌺ Socioeconomic Status and Academic Outcomes in Developing Countries: A Meta-Analysis (Kim et al., 2019) Funders must mandate and reward open research records (Madsen, 2020) ◈ Main Takeaways: Transparent and responsible record-keeping is a pillar of high-quality research. Institutions reward scientists by focusing on quantity, not quality of publications. Ralitsa Madsen and Chris Chambers drafted a Universal Funders Policy that mandates and rewards the open deposition of all records linked with a publication. The proposal does not apply to all materials generated in the course of a project. This is not a requirement that would not be beneficial or pragmatic in biomedical sciences. This could result in a data dump of limited value. The standard of biomedical publication is based on smaller datasets that are often available only from the relevant author upon reasonable request, a practice that hampers transparency. For such a policy to be accepted and work long-term, its implementation route might find inspiration in Plan S developments: an initial phase of consultation with diverse stakeholders, followed by a transition period during which researchers and institutions prepare for the ‘new normal’. Finally, funders will need to enforce the mandate. Quote “To change a game, its rules must change. Funders can make open science the norm and improve research culture in the process.” (p.200). Abstract Dr Ralitsa Madsen discusses how funders must reward open science norms in order to improve research culture. APA Style Reference Madsen, R. (2020). Funders must mandate and reward open research records. Nature, 586, 200. https://doi.org/10.1038/d41586-020-02395-1.\nYou may also be interested in References will be included soon The pressure to publish pushes down quality (Sarewitz, 2016) Main Takeaways: Indeed, the widespread availability of bibliometric data from sources such as Elsevier, Google Scholar and Thomson Reuters ISI makes it easy for scientists to obsess about their productivity and impact, and to compare their numbers with those of other scientists. And if more is good, then the trends for science are favourable. The number of publications continues to grow exponentially; it was already approaching two million per year by 2012. More importantly, and contrary to common mythology, most papers do get cited. Indeed, more papers, from more journals, over longer periods of time, are being cited more often. Mainstream scientific leaders increasingly accept that large bodies of published research are unreliable. But what seems to have escaped general notice is a destructive feedback between the production of poor-quality science, the responsibility to cite previous work and the compulsion to publish. Pervasive quality problems have been exposed for rodent studies of neurological diseases, biomarkers for cancer and other diseases and experimental psychology, amid the publication of thousands of papers. So yes, the web makes it much more efficient to identify relevant published studies, but it also makes it that much easier to troll for supporting papers, whether or not they are any good. No wonder citation rates are going up. Quote “the enterprise of science is evolving towards something different and as yet only dimly seen. Current trajectories threaten science with drowning in the noise of its own rising productivity...Avoiding this destiny will, in part, require much more selective publication. Rising quality can thus emerge from declining scientific efficiency and productivity. We can start by publishing less, and less often, whatever the promotional e-mails promise us.” (p.147). Abstract Scientists must publish less, says Daniel Sarewitz, or good research will be swamped by the ever-increasing volume of poor work. APA Style Reference Sarewitz, D. (2016). The pressure to publish pushes down quality. Nature, 533(7602), 147-147. doi:10.1038/533147a\nYou may also be interested in Fast Lane to Slow Science (Frith, 2020) Let’s Publish Fewer Papers (Nelson et al., 2012) Citizen Science: A Developing Tool for Expanding Science Knowledge and Scientific Literacy (Bonney et al.,, 2009) Main Takeaways: Large-scale projects can engage participants in continental or even global data-gathering networks. Pooled data can be analyzed to illuminate population trends, range changes, and shifts in phenologies. Results can be published in the scientific literature and used to inform population management decisions. Developing and implementing public participation projects that yield both scientific and educational outcomes requires careful planning. This article describes the model for building and operating citizen science projects that has evolved at Cornell Laboratory of Ornithology (CLO) over the past two decades. The authors hope that their model will inform the fields of biodiversity monitoring, biological research, and science education while providing a window into the culture of citizen science. All of the data contributed to CLO citizen science databases are provided by the public and are available at no charge to anyone, amateur or professional, for any noncommercial use. Maintenance and security are provided by database managers housed within the CLO’s information science department. There are 9 steps to develop a citizen science project: Quote “Innovative and rigorous statistical analysis methods will be required to handle the massive amounts of monitoring data that will be collected across vast geographic scales. Systems for ensuring high-quality data through interactive technological and educational techniques will have to be developed. Research on the best ways for people to learn through the citizen science process, and on how that process may differ among different cultures and languages, also will be needed. To fulfill these requirements, expertise from a diversity of science, education, engineering, and other fields must be harnessed in a collaborative, integrated research effort.” (p.983). Abstract Citizen science enlists the public in collecting large quantities of data across an array of habitats and locations over long spans of time. Citizen science projects have been remarkably successful in advancing scientific knowledge, and contributions from citizen scientists now provide a vast quantity of data about species occurrence and distribution around the world. Most citizen science projects also strive to help participants learn about the organisms they are observing and to experience the process by which scientific investigations are conducted. Developing and implementing public data-collection projects that yield both scientific and educational outcomes requires significant effort. This article describes the model for building and operating citizen science projects that has evolved at the Cornell Lab of Ornithology over the past two decades. We hope that our model will inform the fields of biodiversity monitoring, biological research, and science education while providing a window into the culture of citizen science. APA Style Reference Bonney, R., Cooper, C. B., Dickinson, J., Kelling, S., Phillips, T., Rosenberg, K. V., \u0026amp; Shirk, J. (2009). Citizen science: a developing tool for expanding science knowledge and scientific literacy. BioScience, 59(11), 977-984. https://doi.org/10.1525/bio.2009.59.11.9\nYou may also be interested in Next Steps for Citizen Science (Bonney et al., 2014) Citizen Science: Can Volunteers Do Real Research? (Cohn, 2008) The Increasing Dominance of Teams In Production of Knowledge (Wuchty et al., 2007) A Manifesto for Team Science (Forscher et al., 2020) Many hands make tight work(Silberzahn \u0026amp; Uhlmann, 2015) Publishing Research With Undergraduate Students via Replication Work: The Collaborative Replications and Education Project (CREP; Wagge et al., 2019) Many Analysts, One Data Set: Making Transparent How Variations in Analytical Choices Affect Results (Silberzahn et al., 2019) Next Steps for Citizen Science (Bonney et al., 2014) Main Takeaways: Despite the wealth of information emerging from citizen science projects, the practice is not universally accepted as a valid method of scientific investigation. Scientific papers presenting volunteer-collected data sometimes have trouble getting reviewed and are often placed in outreach sections of journals or education tracks of scientific meetings. At the same time, opportunities to use citizen science to achieve positive outcomes for science and society are going unrealized. The authors offer suggestions for strategic thinking by citizen science practitioners and their scientific peers—and for tactical investment by private funders and government agencies—to help the field reach its full potential. During the past two decades, the number of citizen science projects, along with scientific reports and peer-reviewed articles resulting from their data, has expanded tremendously. Much of this growth results from integration of the Internet into everyday life, which has substantially increased project visibility, functionality, and accessibility. People who are passionate about a subject can quickly locate a relevant citizen science project, follow its instructions, submit data directly to online databases, and join a community of peers. Some people question the practice of citizen science citing concerns about data quality. With appropriate protocols, training, and oversight, volunteers can collect data of quality equal to those collected by experts. For large projects where training volunteers and assessing their skills can be challenging, new statistical and high-performance computing tools have addressed data-quality issues (e.g. sampling bias). Creating projects to achieve social and scientific objectives requires deliberate design that is attentive to diverse interests, including why and how members of the public would even want to be involved. Investments in infrastructure and partnerships that help to create more local projects with both science and social components could leverage under appreciated knowledge sources, including local and traditional knowledge. Such efforts could also inform the questions and issues pursued through citizen science, leading to new research and a stronger science-society relationship. Project developers could also look for opportunities to gather truly important information in ways that are currently going unrealized. For example, citizen science could play a stronger role when natural or human caused disasters or other unique data-collection opportunities occur. Many existing citizen science projects could be enhanced by preparing protocols and volunteer infrastructure to enable scientifically sound data collection during and after recurring disaster situations (e.g., oil spills). Quote “Centers for citizen science could create, organize, and synthesize centralized repositories of volunteer-collected data on topics such as water quality, phenology, biodiversity, astronomy, precipitation, and human health. Centers also could help to coordinate questions being asked of citizen science data, methods of answering those questions, and techniques for achieving educational and community-development goals for participants. As such, centers for citizen science would be excellent strategic investments for both private and government foundations..” (p.1437). Abstract Strategic investments and coordination are needed for citizen science to reach its full potential. APA Style Reference Bonney, R., Shirk, J. L., Phillips, T. B., Wiggins, A., Ballard, H. L., Miller-Rushing, A. J., \u0026amp; Parrish, J. K. (2014). Next steps for citizen science. Science, 343(6178), 1436-1437. DOI: 10.1126/science.1251554\nYou may also be interested in Citizen Science: A Developing Tool for Expanding Science Knowledge and Scientific Literacy (Bonney et al.,, 2009) Citizen Science: Can Volunteers Do Real Research? (Cohn, 2008) The Increasing Dominance of Teams In Production of Knowledge (Wuchty et al., 2007) A Manifesto for Team Science (Forscher et al., 2020) Many hands make tight work(Silberzahn \u0026amp; Uhlmann, 2015) Publishing Research With Undergraduate Students via Replication Work: The Collaborative Replications and Education Project (CREP; Wagge et al., 2019) Many Analysts, One Data Set: Making Transparent How Variations in Analytical Choices Affect Results (Silberzahn et al., 2019) Citizen Science: Can Volunteers Do Real Research? (Cohn, 2008) Main Takeaways: Working with citizen scientists is hardly new. The practice goes back at least to the National Audubon Society’s annual Christmas bird count, which began in 1900. About 60,000 to 80,000 volunteers now participate in that survey. What is new is the number of studies that use citizen scientists, the number of volunteers enlisted in the studies, and the scope of data they are asked to collect. But why citizen scientists? Why depend on amateurs who may make mistakes, may not fully understand the context of the study, or may produce data that might be unreliable? Why not hire scientists, graduate students, and field technicians? One obvious reason is money. But can citizen scientists learn to use equipment, read results, and collect data that are as accurate, reliable, and usable as those generated by professional researchers? Yes if they are explained the procedures and how to use them. Nothing we’re doing is so difficult that volunteers can’t do it if they are properly trained. Citizen scientists are typically people who care about the wild, feel at home in nature, and have at least some awareness of the scientific process. In the end, what have citizen scientists achieved? Has their labor actually helped advance scientific knowledge? Yes. Quote “Citizen scientists have also collected data that helped scientists develop guidelines for land managers to preserve habitat. Nevertheless, despite years of practice, the use of citizen science is still an evolving art. “We’re playing it by ear,” says NPS’s Mitchell. “We are designing studies and involving citizen scientists as we go along.” Stevens anticipates that the ATC’s volunteers and other citizen scientists will help provide information that policymakers need to understand ecological changes on the public lands they manage. “The environment belongs to all of us,” Stevens says, adding: “We want to give people a chance to get involved in its preservation in a whole new way.” (p.197). Abstract Collaborations between scientists and volunteers have the potential to broaden the scope of research and enhance the ability to collect scientific data. Interested members of the public may contribute valuable information as they learn about wildlife in their local communities. APA Style Reference Cohn, J. P. (2008). Citizen science: Can volunteers do real research?. BioScience, 58(3), 192-197. https://doi.org/10.1641/B580303\nYou may also be interested in Citizen Science: A Developing Tool for Expanding Science Knowledge and Scientific Literacy (Bonney et al.,, 2009) Next Steps for Citizen Science (Bonney et al., 2014) The Increasing Dominance of Teams In Production of Knowledge (Wuchty et al., 2007) A Manifesto for Team Science (Forscher et al., 2020) Many hands make tight work(Silberzahn \u0026amp; Uhlmann, 2015) Publishing Research With Undergraduate Students via Replication Work: The Collaborative Replications and Education Project (CREP; Wagge et al., 2019) Many Analysts, One Data Set: Making Transparent How Variations in Analytical Choices Affect Results (Silberzahn et al., 2019) Most people are not WEIRD (Henrich et al., 2010) Main Takeaways: Much research on human behaviour is based on Western, Educated, Industrialized, Rich, and Democratic people (WEIRD). They are the most unusual and psychological distinct individuals in the world. Most research ignores the importance of generalizability and researchers tend to assume cognition and behaviours will be the same across all cultures. However, across cultures, there are differences in terms of perceptual illusions, cultural biases and stereotypes. There is a need for cross-cultural evidence in order to have a better understanding of cognition and behaviour. Quote “Recognizing the full extent of human diversity does not mean giving up on the quest to understand human nature. To the contrary, this recognition illuminates a journey into human nature that is more exciting, more complex, and ultimately more consequential than has previously been suspected” (p.29) Abstract To understand human psychology, behavioural scientists must stop doing most of their experiments on Westerners, argue Joseph Henrich, Steven J. Heine and Ara Norenzayan. APA Style Reference Henrich, J., Heine, S. \u0026amp; Norenzayan, A. (2010) Most people are not WEIRD. Nature 466, 29. https://doi.org/10.1038/466029a\nYou may also be interested in Seven Steps Toward Transparency and Replicability in Psychological Science (Lindsay, 2020) Constraints on Generality (COG): A Proposed Addition to All Empirical Papers (Simons et al., 2017) Don’t erase undergrad researchers and technicians from author lists (Fogarty, 2020) Main Takeaways: The author was an intern in a laboratory and was told they would be made an author but their work when submitted was relegated to the acknowledgement section. This was their wake-up call about the need to speak up for myself regarding authorship and to speak out against the unfair convention to reduce the contribution of undergraduates and technicians to scientific research. As an undergraduate, the author was excited to be involved in science and make discovery but is not aware of publications being the true currency of science. The undergraduate was told they would be first or second author when they published the work but the specifics were not discussed. After the author left the lab, they were in contact with the post-doctoral student and graduate student who took over the project, eager to be a co-author on their first paper, they contacted the graduate student and postdoctoral student to help write the manuscript, especially during graduate school. When the author emailed them to check in, they found out that the paper had been written, submitted and accepted for publication but the author was excluded from the process and author list. They were acknowledged as a technician. The author had fulfilled the criteria of being an author according to the guidelines from International Committee of Medical Journal Editors. However, the work was trivialised, as it was done when the author was an undergraduate technician. The author argues they should have initiated explicit conversations about authorship from the beginning, with all members of the project but they should have written up the results and method in a format that could be readily included in the paper. The community needs to address the fact that undergraduate students and technicians should not be excluded from authorship lists. A researcher’s title does not make their contribution any less significant. Junior researchers are critical to a project’s success and researchers need to make sure that everyone who contributes gets the credit that they deserve. Abstract An editorial by Dr Emily Fogarty who discussed that technicians and undergraduates should be included as an author and involved in the process of writing a manuscript. APA Style Reference Fogarty, E. (2020). Don’t erase undergrad researchers and technicians from author lists. Science. doi:10.1126/science.caredit.abf8865.\nYou may also be interested in Boosting research without supporting universities is wrong-headed (Nature, 2020b) Protect students and staff in precarious jobs (Anon, 2020b) Contributorship, Not Authorship: Use CRediT to Indicate Who Did What (Holcombe, 2019) The Increasing Dominance of Teams In Production of Knowledge (Wuchty et al., 2007) Main Takeaways: A shift toward teams also raises new questions of whether teams produce better science. Teams may bring greater collective knowledge and effort, but they are known to experience social network and coordination losses that make them underperform individuals even in highly complex tasks. From this viewpoint, a shift to teamwork may be a costly phenomenon or one that promotes low impact science, whereas the highest-impact ideas remain the domain of great minds working alone. Method: The authors studied 19.9 million research articles in the Institute for Scientific Information (ISI) Web of Science database and an additional 2.1 million patent records covering research publications in science and engineering since 1955, social sciences since 1956, and arts and humanities since 1975. Results: For science and engineering, social sciences, and patents, there has been a substantial shift toward collective research. In the sciences, team size has grown steadily each year and nearly doubled, from 1.9 to 3.5 authors per paper, over 45 years. Results: Shifts toward teamwork in science and engineering have been suggested to follow from the increasing scale, complexity, and costs of big science. Surprisingly then, we find an equally strong trend toward teamwork in the social sciences, where these drivers are much less notable. Results: The citation advantage of teams has also been increasing with time when teams of fixed size are compared with solo authors. In science and engineering (e.g. papers with two authors received 1.30 times more citations than solo authors in the 1950s but 1.74 times more citations in the 1990s). In general, this pattern prevails for comparisons between teams of any fixed size versus solo authors. Results: The authors found that removing self-citations can produce modest decreases in the relative team impact measure in some fields; for example, RTIs fell from 3.10 to 2.87 in medicine and 2.30 to 2.13 in biology. Removing self-citations can reduce the relative team impact by 5 to 10%, but the relative citation advantage of teams remains essentially intact. Teams now dominate the top of the citation distribution in all four research domains. In the early years, a solo author in science and engineering or the social sciences was more likely than a team to receive no citations, but a solo author was also more likely to garner the highest number of citations. A team-authored paper has a higher probability of being extremely highly cited (e.g. a team-authored paper in science and engineering is currently 6.3 times more likely than a solo authored paper to receive at least 1000 citations). Lastly, in arts and humanities and in patents, individuals were never more likely than teams to produce more-influential work. It never appeared to be the domain of solo authors in arts and humanities and in patents. Second, solo authors did produce the papers of singular distinction in science and engineering and social science in the 1950s, but the mantle of extraordinarily cited work has passed to teams by 2000. Since the 1950s, the number of researchers has grown as well, which could promote finer divisions of labor and more collaboration. Similarly, steady growth in knowledge may have driven scholars toward more specialization, prompting larger and more diverse teams. However, we found that teamwork is growing nearly as fast in fields where the number of researchers has grown relatively slowly. Declines in communication costs could make teamwork less costly as well. Shifting authorship norms may have influenced coauthorship trends in fields with extremely large teams (e.g. biomedicine). However, our results hold across diverse fields in which norms for order of authorship, existence of postdoctorates, and prevalence of grant-based research differ substantially. Abstract We have used 19.9 million papers over 5 decades and 2.1 million patents to demonstrate that teams increasingly dominate solo authors in the production of knowledge. Research is increasingly done in teams across nearly all fields. Teams typically produce more frequently cited research than individuals do, and this advantage has been increasing over time. Teams now also produce the exceptionally high impact research, even where that distinction was once the domain of solo authors. These results are detailed for sciences and engineering, social sciences, arts and humanities, and patents, suggesting that the process of knowledge creation has fundamentally changed. APA Style Reference Wuchty, S., Jones, B. F., \u0026amp; Uzzi, B. (2007). The increasing dominance of teams in production of knowledge. Science, 316(5827), 1036-1039. DOI: 10.1126/science.1136099\nYou may also be interested in Citizen Science: A Developing Tool for Expanding Science Knowledge and Scientific Literacy (Bonney et al.,, 2009) Next Steps for Citizen Science (Bonney et al., 2014) A Manifesto for Team Science (Forscher et al., 2020) A Manifesto for Team Science (Forscher et al., 2020) Main Takeaways: The lack of progress is empirically supported by the results of replication studies whether in the same or new setting, when separate teams develop research strategies and analysis plans to address the same research question and same dataset respectively, and when separate teams write code to execute the same analysis. The authors argue that there is one cause for these problems: insufficient resource investment in the typical psychology study and that team science could help address this cause by efficiently scaling up the resources that can be invested in a given study. An individual researcher bears the bulk of the cost of each remedy, but discussions of problems in psychology tend to assume that implementations of these remedies are zero-sum. If implementations for remedies to psychology’s problems are indeed zero-sum, then emphasizing one aspect of rigor necessarily means sacrificing other aspects, as resources are finite. The authors suggest that the views that prioritize one aspect of rigor over the others all share an important, mistaken assumption: that the pool of resources available for investment in a given study is, on average, fixed. This is because scientists within that research ecosystem face a collective action problem: as long as scientists are rewarded for publishing more studies, any single scientist who decides to invest more resources in fewer studies will be outcompeted by scientists who invest less resources in more studies. There are two ways to solve this reward ecosystem: directly change the institutional incentives that prioritize quantity of publication or devise new institutions (team science institutions) that allow blocs of scientists to increase resource investment in concert. Team science institutions solve the collective action problem by coordinating the actions of large groups of scientists. In return for their efforts and resources, the institution gives each individual scientist a publication. Once a group of scientists have signed onto a team science project, the institution serves a coordinating role, pooling the resources across all the individual scientists and focusing them on a common project. This is becoming more common in psychology and involves the smaller teams to develop proposals based on their ideas and submit them for consideration by the larger consortium. The larger team can even explicitly build in mechanisms to solicit proposals from teams whose perspectives may differ from the scientific mainstream – such as those from non-Western regions. However, there are three obstacles to team science: incentivising labour within the collaboration; developing and maintaining infrastructure to coordinate team science activities; and dealing with institutions designed around research conducted by smaller teams. These can be solved by: Quote “We believe that the biggest problems in science require many minds. By leveraging the combined talents of many minds, the combined efforts of many labs, and the combined resources of many institutions, we believe that team science will be instrumental in the movement to build more reliable, informative, and rigorous science” (p.12). Abstract Progress in psychology has been frustrated by challenges concerning replicability, generalizability, strategy selection, inferential reproducibility, and computational reproducibility. Although often discussed separately, we argue that these five challenges share a common cause: insufficient investment of resources in the typical psychology study. We suggest that the emerging emphasis on team science can help address these challenges by allowing researchers to pool their resources to efficiently and drastically increase the amount of resources available for a single study. However, we also anticipate that team science will create new challenges for the field to manage, such as the potential for team science institutions to monopolize power, become overly conservative, and make mistakes at a grand scale. If researchers can overcome these new challenges, we believe team science has the potential to spur enormous progress in psychology and beyond. APA Style Reference Forscher, P. S., Wagenmakers, E., Coles, N. A., Silan, M. A., \u0026amp; IJzerman, H. (2020, May 20). A Manifesto for Team Science. https://doi.org/10.31234/osf.io/2mdxh\nYou may also be interested in Citizen Science: A Developing Tool for Expanding Science Knowledge and Scientific Literacy (Bonney et al.,, 2009) Next Steps for Citizen Science (Bonney et al., 2014) The Increasing Dominance of Teams In Production of Knowledge (Wuchty et al., 2007) Citizen Science: Can Volunteers Do Real Research? (Cohn, 2008) Many hands make tight work(Silberzahn \u0026amp; Uhlmann, 2015) Publishing Research With Undergraduate Students via Replication Work: The Collaborative Replications and Education Project (CREP; Wagge et al., 2019) Many Analysts, One Data Set: Making Transparent How Variations in Analytical Choices Affect Results (Silberzahn et al., 2019) Science faculty’s subtle gender biases favour male students (Moss-Racusin et al., 2012) ⌺ Main Takeaways: The present research investigates whether faculty gender bias exists within academic biological and physical sciences, whether it might exert an independent effect on the gender disparity as students progress through the pipeline to careers in science, and finally, whether, given an equally qualified male and female student, science faculty members would show preferential evaluation and treatment of the male student to work in their laboratory. Also, the authors investigated whether faculty members’ perceptions of student competence would help to explain why they would be less likely to hire a female (relative to an identical male) student for a laboratory manager position. Science faculty’s perceptions and treatment of students would reveal a gender bias favoring male students in perceptions of competence and hireability, salary conferral, and willingness to mentor; Faculty gender would not influence this gender bias. Hiring discrimination against the female student would be mediated (i.e., explained) by faculty perceptions that a female student is less competent than an identical male student. Finally, participants’ preexisting subtle bias against women would moderate (i.e., impact) results, such that subtle bias against women would be negatively related to evaluations of the female student, but unrelated to evaluations of the male student. Method: 127 faculty participants from Biology, Chemistry and Physics provided feedback on materials of an undergraduate science student who stated their intention to go on to graduate school and those who applied for a science laboratory manager position and evaluated a real student who received faculty participants’ ratings as feedback to help their career development. Method: Participants were randomly assigned to one of two student gender conditions: male or female. Using validated scales, participants rated student competence, their own likelihood of hiring the student, selecting an annual starting salary for the student, indicated how much career mentoring they would provide and also had to fill in the Modern Sexism Scale. Results: Faculty participants rated the male applicant as significantly more competent and hireable than the (identical) female applicant. These participants also selected a higher starting salary and offered more career mentoring to the male applicant. The gender of the faculty participants did not affect responses, such that female and male faculty were equally likely to exhibit bias against the female student. Mediation analyses indicated that the female student was less likely to be hired because she was viewed as less competent. The authors found that preexisting subtle bias against women played a moderating role, such that subtle bias against women was associated with less support for the female student, but was unrelated to reactions to the male student. A female student was seen as less competent and less worthy of being hired than an identical male student with a smaller starting salary and less career mentoring. The subtle gender bias is important to address as it could translate into large real-world disadvantages in judgment and treatment of female science students. The female student was less likely to be hired than male student because the former was seen as less competent. Faculty participants’ pre-existing subtle bias against women undermined perceptions and treatment of the female, not male, student, indicating chronic subtle biases may harm women within academic science. Female faculty members were just as likely as their male colleagues to favour the male student. Faculty members’ bias was independent of their gender, scientific discipline, age and tenure status indicating this bias is not consciously done and is unintentionally generated from widespread cultural stereotypes. Faculty participants reported liking the female more than male students indicates that faculty members’ are not overtly hostile toward women. Faculty members of both genders are affected by enduring cultural stereotypes about a women’s lack of science competence, which translate into biases in student evaluation and mentoring. Not only do women encounter biased judgments regarding their competence and hireability but receive less faculty encouragement and financial rewards than identical male counterparts. Abstract Despite efforts to recruit and retain more women, a stark gender disparity persists within academic science. Abundant research has demonstrated gender bias in many demographic groups, but has yet to experimentally investigate whether science faculty exhibit a bias against female students that could contribute to the gender disparity in academic science. In a randomized double-blind study (n = 127), science faculty from research-intensive universities rated the application materials of a student—who was randomly assigned either a male or female name—for a laboratory manager position. Faculty participants rated the male applicant as significantly more competent and hireable than the (identical) female applicant. These participants also selected a higher starting salary and offered more career mentoring to the male applicant. The gender of the faculty participants did not affect responses, such that female and male faculty were equally likely to exhibit bias against the female student. Mediation analyses indicated that the female student was less likely to be hired because she was viewed as less competent. We also assessed faculty participants’ preexisting subtle bias against women using a standard instrument and found that preexisting subtle bias against women played a moderating APA Style Reference Moss-Racusin, C. A., Dovidio, J. F., Brescoll, V. L., Graham, M. J., \u0026amp; Handelsman, J. (2012). Science faculty’s subtle gender biases favor male students. Proceedings of the national academy of sciences, 109(41), 16474-16479. https://doi.org/10.1073/pnas.1211286109\nYou may also be interested in Unequal effects of the COVID-19 pandemic on scientists (Myers et al., 2019) Against Eminence (Vazire, 2017) Scientific Eminence: Where Are the Women? (Eagly \u0026amp; Miller, 2016) #bropenscience is broken science (Whitaker \u0026amp; Guest, 2020) ⌺ Examining Gender Bias in Student Evaluations of Teaching for Graduate Teaching Assistants (Khazan et al., 2019) ⌺ Contributorship, Not Authorship: Use CRediT to Indicate Who Did What (Holcombe, 2019) Main Takeaways: Science today is highly collaborative, and results usually reflect the work of multiple people. Many of those who do this work are evaluated on their contributions to science when they apply for a job, a promotion, or a grant. One might expect this evaluation to be based simply on one’s contributions to science. In many fields, evaluation is formally based on the list of publications that include the researcher as an author. But there can be a disconnect between those who are authors and those who make contributions. While author typically means “writer”, funders and promotion committee members are interested in various types of substantive contributions—who actually provided the words in the paper is, often, not the paramount concern. Many journals and societies specifically enshrine manuscript drafting or revising as a requirement for co-authorship. This is a problem for funders and others interested in both writing related contributions and also contributions that are not writing related. More broadly, the concept of authorship itself may no longer be appropriate. With standard practice today, the ambiguity regarding the contribution(s) of a co-author to a paper extends well beyond whether there was a contribution to the writing. Surveys of researchers show strong disagreement both with the most widely used authorship guidelines and also high levels of inconsistency between researchers regarding the categories of contributions that merit authorship. To resolve the ambiguity in the meaning of author lists, one would have to ask the authors what the contributions were of each author and what, if any, were the significant contributions of others. Under a contributorship model, authors are required to state this information. And in a world where a number of researchers often contribute one or more of a variety of things to a research project, it seems illogical for researchers not to say something about who did what in a way that is visible and systematically enables credit for those contributions. The transparency and removal of ambiguity stemming from a declaration of contributions will have multiple benefits. Universities, laboratories, and institutions make hiring and funding decisions based on what they can determine about a person’s past scientific contributions. These hiring organizations can have very different interests in specific kinds of contributions. When hiring a staff scientist in a large laboratory, or a statistician in a research group, one is looking for different types of contributions than if one is hiring a faculty member expected to run a laboratory. Thus, to best serve the various stakeholders involved in science, what is needed is to know what the significant contributions were to a project and who made them. There are benefits if a standardised contributorship system were adopted by large numbers of journals such as: Quote “Authorship criteria today in medicine and many other fields are not simple, as they typically involve the combination of a mandatory writing requirement with other criteria, the nature of which (such as being optional or required) vary from journal to journal. The idea of indicating who did what is simpler. There is still the difficult issue of how substantive a contribution needs to be for a researcher to be credited, but this is an issue that any system faces. There seems to be little reason to not move toward a contributorship model; it provides a more inclusive and realistic recognition of their research contributions” (p.9). Abstract Participation in the writing or revising of a manuscript is, according to many journal guidelines, necessary to be listed as an author of the resulting article. This is the traditional concept of authorship. But there are good reasons to shift to a contributorship model, under which it is not necessary to contribute to the writing or revision of a manuscript, and all those who make substantial contributions to a project are credited. Many journals and publishers have already taken steps in this direction, and further adoption will have several benefits. This article makes the case for continuing to move down that path. Use of a contributorship model should improve the ability of universities and funders to identify effective individual researchers and improving their ability to identify the right mix of researchers needed to advance modern science. Other benefits should include facilitating the formation of productive collaborations and the creation of important scientific tools and software. The CRediT (Contributor Roles Taxonomy) taxonomy is a machine-readable standard already incorporated into some journal management systems and it allows incremental transition toward contributorship. APA Style Reference Holcombe, A. O. (2019). Contributorship, not authorship: Use credit to indicate who did what. Publications, 7(3), 48. https://doi.org/10.3390/publications7030048\nYou may also be interested in Boosting research without supporting universities is wrong-headed (Nature, 2020b) Protect students and staff in precarious jobs (Anon, 2020b) Don’t erase undergrad researchers and technicians from author lists (Fogarty, 2020) Show the dots in plots (Anon, 2017) Main Takeaways: The type of graph, its dimensions and layout, colour palettes and gradients, the data intervals displayed in the axes, specific data comparisons, and above all, the presence or absence of individual data points, error bars and information on statistical significance, can strongly affect how the graphed dataset is interpreted. However, they are also commonly used to present small samples of continuous data, especially in biomedical fields. There are reasons for this: because of their shape and area, bars are easy to see at a glance; therefore, they are effective when comparing data and visualizing trends; and they make it easy to see the relative position of the data along the axes. However, bar graphs can be misleading. Moreover, providing only statistical parameters (e.g. mean ± s.d.) can suggest that the data underlying any particular bar are normally distributed and contains no outliers, when this may not be the case. Graphing error bars with the s.e.m. indicates the precision of the mean is commonly done because they are shorter than error bars representing the s.d. (which instead quantifies variability). The authors discourage this practice. All these issues can be avoided by displaying every data point. This journal strongly suggests that the individual data points (in addition to error bars and other statistical information) be graphed, in particular for relatively small samples and for bar graphs, and when statistical significance is claimed. Data deposition is recommended, as it can provide easy access to colleagues who wish to further analyse or make use of the data, increases reporting transparency, encourages the eventual reproducibility of the findings, ensures data preservation, increases the overall usability of datasets, especially when they are large, and enables convenient citation to the data (with a doi). Data presentation should not be an afterthought; the visuals affect how the story is told and perceived. Display items in papers should highlight the relevant data and make their interpretation easy. Bar graphs can make comparisons easier to see at a glance, even for continuous variables when categorized. However, the individual data points should be displayed. Quote “Figure captions should be clear, provide all the necessary statistical information, and guide the reader through the story. In fact, an engaging narrative is structured, showcases the protagonists, and provides relevant context. In bar graphs, the data points are the context. Show them.” (p.1). Abstract We encourage our authors to display data points in graphs, and to deposit the data in repositories. APA Style Reference Anon (2017). Show the dots in plots. Nature Biomedical Engineering, 1 (0079). https://doi.org/10.1038/s41551-017-0079\nYou may also be interested in Bar graphs depicting averages are perceptually misinterpreted: The within-the-bar bias (Newman \u0026amp; Scholl, 2012) Bar graphs depicting averages are perceptually misinterpreted: The within-the-bar bias (Newman \u0026amp; Scholl, 2012) Main Takeaways: People who view bar graphs to reflexively attend to the bars, and so to mistakenly prioritize regions within the bars over equivalent regions outside the bars, even when this is not justified. Thus, when viewers are shown a bar graph that depicts a mean and are then asked to judge the likelihood that a particular value was part of its underlying distribution, they will judge points that fall within the bar as being more likely than points equidistant from the mean, but outside the bar—as if the bar somehow “contained” the relevant data. The authors investigated this bias. Method: Abstract Perhaps the most common method of depicting data, in both scientific communication and popular media, is the bar graph. Bar graphs often depict measures of central tendency, but they do so asymmetrically: A mean, for example, is depicted not by a point, but by the edge of a bar that originates from a single axis. Here we show that this graphical asymmetry gives rise to a corresponding cognitive asymmetry. When viewers are shown a bar depicting a mean value and are then asked to judge the likelihood of a particular data point being part of its underlying distribution, viewers judge points that fall within the bar as being more likely than points equidistant from the mean, but outside the bar—as if the bar somehow “contained” the relevant data. This “within-the-bar bias” occurred (a) for graphs with and without error bars, (b) for bars that originated from both lower and upper axes, (c) for test points with equally extreme numeric labels, (d) both from memory (when the bar was no longer visible) and in online perception (while the bar was visible during the judgment), (e) both within and between subjects, and (f) in populations including college students, adults from the broader community, and online samples. We posit that this bias may arise due to principles of object perception, and we show how it has downstream implications for decision making. APA Style Reference Newman, G. E., \u0026amp; Scholl, B. J. (2012). Bar graphs depicting averages are perceptually misinterpreted: The within-the-bar bias. Psychonomic bulletin \u0026amp; review, 19(4), 601-607. https://doi.org/10.3758/s13423-012-0247-5\nYou may also be interested in Show the dots in plots (Anon, 2017) Use caution when applying behavioural science to policy (Ijzerman et al., 2020) Main Takeaways: Researchers in the social and behavioural sciences periodically debate whether their research should be used to address pressing issues in society. Confident applications of social and behavioural science findings, then, require first and foremost an assessment of the evidence quality and weighing heterogeneity and the trade-offs and opportunity costs that follow. The scientific community must identify reliable findings that can be applied, have been investigated in the nations for which the application is intended and are derived from investigations using diverse stimuli. In the social and behavioural sciences researchers start with defining problem(s) in collaboration with the stakeholders most likely to implement the interventions evidence readiness level 1 (ERL1). These concepts can then be further developed in consultation with people in the target settings to gather preliminary information about how settings or context might alter processes (ERL2). From there, researchers can conduct systematic reviews and other meta-syntheses to select evidence that could potentially be applied (ERL3). These systematic reviews require a number of bias-detection techniques. Some findings may be reliable, but the onus is on the scientific community to identify which are and which are not and which generalize or don’t. Yet, these systematic reviews must still be done with an awareness that the currently available statistical techniques do not completely correct for bias and that the resultant findings are at most at ERL3. Following this, one can gather information about stimulus and measurement validity and equivalence for application in the target setting (ERL4). Next, researchers and local experts—should consider the potential benefits and harms associated with applying potential solutions (ERL5) and generate estimates of effects in a pilot sample (ERL6). With preliminary effects in hand, the team can then begin to test for heterogeneity in low-stakes (ERL7) and higher-stakes (ERL8)samples and settings, which would build the confidence necessary to apply the findings in the real target setting or crisis situation (ERL9). Even at ERL9, evidence evaluation continues; applications of social and behavioural work, particularly in a crisis, should be iterative, so high-quality evidence is fed back to evaluate the effectiveness of the intervention and to develop critical and flexible improvements. Feedback should be grounded in collaboration between basic and applied researchers, as well as with stakeholders, to ensure that the resulting evidence is relevant and actionable. Failure to continually re-evaluate interventions in light of new data could lead to unnecessary harm, where even the best evidence was inadequate to predict the intervention’s real-world effects. A benchmarking system such as the ERL requires us to think carefully about the nature of our research that can be applied credibly and guides where research investments should be made. Behavioural scientists from different cultures then discuss how interventions may need to differ in nature across context and cultures. The multidisciplinary and multi-stakeholder nature of ERLs requires us to fundamentally rethink how we produce, and communicate confidence in, application-ready findings. The current crisis provides a chance for social and behavioural scientists to question how we understand and communicate the value of our scientific models in terms of ERLs. Even if findings are at ERL3 after assessing evidence quality of primary studies, researchers have little way of knowing how much positive, or unintended negative, consequences an intervention might have when applied to a new situation. Researchers are concerned to see social and behavioural scientists making confident claims about the utility of scientific findings for solving COVID-19 problems without regard for whether those findings are based on the kind of scientific methods that would move them up the ERL ladder. The absence of recognised benchmarking systems makes this challenging. While it is tempting to instead qualify uncertainty by using non-committal language about the possible utility of existing findings (for example, ‘may’, ‘could’), this approach is fundamentally flawed because public conversations generally ignore these rhetorical caveats. Scientists should actively communicate uncertainty, particularly when speaking to crises. Quote “And now, in 2020, psychologists and other social and behavioural scientists are arguing that our research should inform the response to the new coronavirus disease (henceforth COVID-19)1,2. We are a team mostly consisting of empirical psychologists who conduct research on basic, applied and meta-scientific processes. We believe that scientists should apply their creativity, efforts and talents to serve our society, especially during crises. However, the way that social and behavioural science research is often conducted makes it difficult to know whether our efforts will do more good than harm.” (p.1). Abstract Social and behavioural scientists have attempted to speak to the COVID-19 crisis. But is behavioural research on COVID-19 suitable for making policy decisions? We offer a taxonomy that lets our science advance in ‘evidence readiness levels’ to be suitable for policy. We caution practitioners to take extreme care translating our findings to applications. APA Style Reference IJzerman, H., Lewis, N. A., Przybylski, A. K., Weinstein, N., DeBruine, L., Ritchie, S. J., ... \u0026amp; Anvari, F. (2020). Use caution when applying behavioural science to policy. Nature Human Behaviour, 4(11), 1092-1094. https://doi.org/10.1038/s41562-020-00990-w\nYou may also be interested in See references for later Why Has the Number of Scientific Retractions Increased? (Steen et al., 2013) Main Takeaways: Substantial fraction of all retractions are due to research misconduct and there has been an estimated 10-fold increase in retractions for scientific fraud (e.g., data fabrication or falsification) since 1975. An explanation for the apparent increase in the rate of fraud is not immediately obvious. If the literature truly does self-correct, then research fraud should ultimately be futile. Yet there is reasonable evidence that scientific misconduct is both common and under-reported. Therefore, it is not clear whether the increase in retractions is a result of an increase in the rate of publication of flawed articles or an increase in the rate at which flawed articles are recognized and withdrawn. The goal of this study is to gain a deeper understanding of the increase in retracted scientific publications by analyzing trends in the time interval from publication to retraction. Method: The PubMed database of the National Center for Biotechnology was used. Information was searched on 3 May 2012, using the limits of ‘‘retracted publication, English language.’’ A total of 2,047 articles were identified. Each article was classified according to the cause of retraction, using published retraction notices, proceedings from the Office of Research Integrity (ORI), Retraction Watch (http://retractionwatch.wordpress.com), and other sources (e.g., the New York Times). Retractions were classified as resulting from fraud (e.g., data fabrication or falsification), suspected fraud, scientific error, plagiarism, duplicate publication, other cause (e.g., publisher error, authorship disputes, copyright infringement), or unknown. Method: An apparent increase in recent retractions might result: (1) if the time to retract has increased in recent years, so that editors are reaching further back in time to retract (e.g., if the introduction of plagiarism-detection software has lead to the detection of long published articles that need to be retracted for plagiarism); (2) if peer scrutiny has increased, so that flawed work is detected more quickly; or (3) if there are reduced barriers to retraction, such that retraction occurs more swiftly (or for different reasons) now than in the past. The time required to retract an article was calculated as the number of months from when a hard-copy version of the article was published in a journal (i.e., as opposed to an online electronic version) to when the retraction notice was published we sorted first authors by name, to determine how many retractions were associated with each first author. The authors then compared first authors with a single retraction to first authors with multiple retractions. Results: Among 714 retracted articles published in or before 2002, retraction required 49.82 months; among 1,333 retracted articles published after 2002, retraction required 23.82 months. This suggests that journals are retracting papers more quickly than in the past, although recent articles requiring retraction may not have been recognized yet. To test the hypothesis that time-to-retraction is shorter for articles that receive careful scrutiny, time-to-retraction was correlated with journal impact factor (IF). Time-to-retraction was significantly shorter for high-IF journals, but only ,1% of the variance in time-to-retraction was explained by increased scrutiny. The first article retracted for plagiarism was published in 1979 and the first for duplicate publication in 1990, showing that articles are now retracted for reasons not cited in the past. The proportional impact of authors with multiple retractions was greater in 1972–1992 than in the current era. From 1972–1992, 46.0% of retracted papers were written by authors with a single retraction; from 1993 to 2012, 63.1% of retracted papers were written by single-retraction authors. The rate of publication has increased, with a concomitant increase in the rate of retraction. Editors are retracting articles significantly faster now than in the past. The reasons for retraction have expanded to include plagiarism and duplicate publication. Journals are reaching further back in time to retract flawed work. There has been an increase in the number and proportion of retractions by authors with a single retraction. Discovery of fraud by an author prompts reevaluation of an author’s entire body of work. Greater scrutiny of high-profile publications has had a modest impact on retractions The recent spike in retractions thus appears to be a consequence of changes both in institutional policy and in the behavior of individual authors. Similarly, the first articles retracted for error or plagiarism were published in 1979, and the first article retracted for duplicate publication was published in 1990. Retraction is more widely recognized as a remedy for a flawed publication in the modern era, and the reasons for retraction have expanded over time. Recognition of serial misconduct has increased in recent years, although retractions by authors with only one retraction are more common and proportionally more important, these individual authors have had a grossly disproportionate impact on retractions from the literature. Quote “Data fabrication and falsification are not new phenomena in science. Gregor Mendel, the father of genetics, may have modified or selectively used data to support his conclusions [40] and statistical analysis suggests that Mendel’s ‘‘data… [are] biased strongly in the direction of agreement with expectation…. This bias seems to pervade the whole data [set]’’ [26]. However, there now appear to be lower barriers to retraction as a remedy to correct the scientific literature. Our results (Fig. 5) suggest that the overall rate of retraction may decrease in the future as editors continue to process a glut of articles requiring retraction” (p.8). Abstract Background: The number of retracted scientific publications has risen sharply, but it is unclear whether this reflects an increase in publication of flawed articles or an increase in the rate at which flawed articles are withdrawn. Methods and Findings: We examined the interval between publication and retraction for 2,047 retracted articles indexed in PubMed. Time-to-retraction (from publication of article to publication of retraction) averaged 32.91 months. Among 714 retracted articles published in or before 2002, retraction required 49.82 months; among 1,333 retracted articles published after 2002, retraction required 23.82 months (p \u0026lt; 0.0001). This suggests that journals are retracting papers more quickly than in the past, although recent articles requiring retraction may not have been recognized yet. To test the hypothesis that time-to-retraction is shorter for articles that receive careful scrutiny, time-to-retraction was correlated with journal impact factor (IF). Time-to-retraction was significantly shorter for high-IF journals, but only .1% of the variance in time-to-retraction was explained by increased scrutiny. The first article retracted for plagiarism was published in 1979 and the first for duplicate publication in 1990, showing that articles are now retracted for reasons not cited in the past. The proportional impact of authors with multiple retractions was greater in 1972–1992 than in the current era (p \u0026lt; 0.001). From 1972–1992, 46.0% of retracted papers were written by authors with a single retraction; from 1993 to 2012, 63.1% of retracted papers were written by single-retraction authors (p \u0026lt; 0.001). Conclusions: The increase in retracted articles appears to reflect changes in the behavior of both authors and institutions. Lower barriers to publication of flawed articles are seen in the increase in number and proportion of retractions by authors with a single retraction. Lower barriers to retraction are apparent in an increase in retraction for ‘‘new’’ offenses such as plagiarism and a decrease in the time-to-retraction of flawed work. APA Style Reference Steen, R. G., Casadevall, A., \u0026amp; Fang, F. C. (2013). Why has the number of scientific retractions increased?. PloS one, 8(7), e68397. https://doi.org/10.1371/journal.pone.0068397\nYou may also be interested in Signalling the trustworthiness of science should not be a substitute for direct action against research misconduct (Kornfeld \u0026amp; Titus, 2020) Reply to Kornfeld and Titus: No distraction from misconduct (Jamieson et al., 2020) Stop ignoring misconduct (Kornfeld \u0026amp; Titus, 2016) Fallibility in Science: Responding to Errors in the Work of Oneself and Others (Bishop, 2018) Publication Pressure and Scientific Misconduct in Medical Scientists (Tijdink et al., 2014) Signalling the trustworthiness of science (Jamieson et al., 2020) Check for publication integrity before misconduct (Grey et al., 2020) The association between early career informal mentorship in academic collaborations and junior author performance (AlShebli et al., 2020)⌺ Main Takeaways: This paper has been retracted: https://retractionwatch.com/2020/12/21/nature-communications-retracts-much-criticized-paper-on-mentorship/ By mentoring novices, senior members pass on the organizational culture, best practices, and the inner workings of a profession. In this way, the mentor–protégé relationship provides the social glue that links generations within a field. The authors study mentorship in scientific collaboration, where a junior scientist is supported by potentially multiple senior collaborators, without them necessarily having formal supervisory roles.The authors also identify 3 million mentor-protege pairs and survey a random sample, verifying that their relationship involved some form of mentorship. Method: This dataset includes records of scientific publications specifying the date of the publication, the authors’ names and affiliations, and the publication venue. It also contains a citation network in which every node represents a paper and every directed edge represents a citation. While the number of citations of any given paper is not provided explicitly, it can be calculated from the citation network in any given year. Additionally, every paper is positioned in a field-of-study hierarchy, the highest level of which comprises 19 scientific disciplines. The authors derive two key measures: the discipline of scientists and their impact and additional measures such as the scientists’ gender. Whenever a junior scientist (with academic age = 7) publishes a paper with a senior scientist (academic age \u0026gt; 7), the former is defined as a protégé, and the latter is delineated as a mentor. The author analyze every mentor–protégé dyad that satisfies all of the following conditions: (i) the protégé has at least one publication during their senior years without a mentor; (ii) the affiliation of the protégé is in the US throughout their mentorship years; (iii) the main discipline of the mentor is the same as that of the protégé; (iv) the mentor and the protégé share an affiliation on at least one publication; (v) during the mentorship period, the mentor and the protégé worked together on a paper whose number of authors is 20 or less; and (vi) the protégé does not have a gap of 5-years or more in their publication history. Results: The author finds that mentorship quality predicts the scientific impact of the papers written by protégés post mentorship without their mentors. The author observed that increasing the proportion of female mentors is associated not only with a reduction in post-mentorship impact of female protégés, but also a reduction in the gain of female mentors. The authors found that both have an independent association with the protégé’s impact post mentorship without their mentors. Interestingly, the big-shot experience seems to matter more than the hub experience, implying that the scientific impact of mentors matters more than the number of their collaborators. The association between the big-shot experience and the post-mentorship outcome persists regardless of the discipline, the affiliation rank, the number of mentors, the average age of the mentors, the protégé’s gender, and the protégé’s first year of publication. The present study suggests that female protégés who remain in academia reap more benefits when mentored by males rather than equally-impactful females. The specific drivers underlying this empirical fact could be multifold, such as female mentors serving on more committees, thereby reducing the time they are able to invest in their protégés or women taking onless recognized topics that their protégés emulate. Additionally, findings also suggest that mentors benefit more when working with male protégés rather than working with comparable female protégés, especially if the mentor is female. These conclusions are all deduced from careful comparisons between protégés who published their first mentored paper in the same discipline, in the same cohort, and at the very same institution. One potential explanation could be that, historically, male scientists had enjoyed more privileges and access to resources than their female counterparts, and thus were able to provide more support to their protégés. Alternatively, these findings may be attributed to sorting mechanisms within programs based on the quality of protégés and the gender of mentors. The gender-related findings suggest that current diversity policies promoting female–female mentorships, as well-intended as they may be, could hinder the careers of women who remain in academia in unexpected ways. Female scientists, in fact, may benefit from opposite-gender mentorships in terms of their publication potential and impact throughout their post-mentorship careers. Abstract We study mentorship in scientific collaborations, where a junior scientist is supported by potentially multiple senior collaborators, without them necessarily having formal supervisory roles. We identify 3 million mentor–protégé pairs and survey a random sample, verifying that their relationship involved some form of mentorship. We find that mentorship quality predicts the scientific impact of the papers written by protégés post mentorship without their mentors. We also find that increasing the proportion of female mentors is associated not only with a reduction in post-mentorship impact of female protégés, but also a reduction in the gain of female mentors. While current diversity policies encourage same-gender mentorships to retain women in academia, our findings raise the possibility that opposite-gender mentorship may actually increase the impact of women who pursue a scientific career. These findings add a new perspective to the policy debate on how to best elevate the status of women in science. APA Style Reference AlShebli, B., Makovi, K., \u0026amp; Rahwan, T. (2020). The association between early career informal mentorship in academic collaborations and junior author performance. Nature communications, 11(1), 1-8. https://doi.org/10.1038/s41467-020-19723-8\nYou may also be interested in Unequal effects of the COVID-19 pandemic on scientists (Myers et al., 2019) Against Eminence (Vazire, 2017) Scientific Eminence: Where Are the Women? (Eagly \u0026amp; Miller, 2016) #bropenscience is broken science (Whitaker \u0026amp; Guest, 2020) ⌺ Science faculty’s subtle gender biases favour male students (Moss-Racusin et al., 2012) ⌺ Examining Gender Bias in Student Evaluations of Teaching for Graduate Teaching Assistants (Khazan et al., 2019) ⌺ Open Data in Qualitative Research (Chauvette et al., 2019) Main Takeaways: This article argues that as a result of epistemological, methodological, legal and ethical issues, not all qualitative data is appropriate for open access. Open data allows researchers to test or refute new theories by validating research findings. Although data is becoming more available, we need to consider hotly debated issues concerning open data and that not all data is created equally, especially data from qualitative research. Qualitative research is not equally useful when decontextualized and requires contextualisation. Secondary analyses occur in teams or between collaborators when insider knowledge is shared. Qualitative research design is not beneficial to secondary analysis. Researchers become part of the research and may bias the data. Also, preconceptions should not be removed from the analyses. Personal knowledge is important for phenomenological research. Open data is not captured in transcripts and participants may conduct research to become active contributors to the research process. Field notes are written by researchers. Blanket consent forms have been recommended by some researchers in order to keep the participants’ data indefinitely and to make it potentially reusable by anyone. However, confidentiality and anonymity becomes an issue for participants with open data, especially in small sample sizes. In addition, there are other issues pertaining to open data such as sensitive issues, nature of questions and disclosure of information that may be harmful to the individual and researcher. Quote “Requirements for data access must consider the uniqueness and context of the data in each qualitative study. Consideration should be given to policies that grant the original research team adequate opportunities for involvement in publication of secondary analyses, perhaps with the rights to authorship to future publications if circumstances warrant. Alternatively, opportunities to comment on the new analysis and interpretation, considering the investigators’ understanding of the unique context of the study, would provide some additional accountability” (p.4). Abstract There is a growing movement for research data to be accessed, used, and shared by multiple stakeholders for various purposes. The changing technological landscape makes it possible to digitally store data, creating opportunity to both share and reuse data anywhere in the world for later use. This movement is growing rapidly and becoming widely accepted as publicly funded agencies are mandating that researchers open their research data for sharing and reuse. While there are numerous advantages to use of open data, such as facilitating accountability and transparency, not all data are created equally. Accordingly, reusing data in qualitative research present some epistemological, methodological, legal, and ethical issues that must be addressed in the movement toward open data. We examine some of these challenges and make a case that some qualitative research data should not be reused in secondary analysis. APA Style Reference Chauvette, A., Schick-Makaroff, K., \u0026amp; Molzahn, A. E. (2019). Open data in qualitative research. International Journal of Qualitative Methods, 18, 1609406918823863. https://doi.org/10.1177/1609406918823863\nYou may also be interested in Trust Your Science? Open Your Data and Code (Stodden, 2011) Attitudes Toward Open Science and Public Data Sharing: A Survey Among Members of the German Psychological Society (Abele-Brehm et al., 2019) Willingness to Share Research Data Is Related to the Strength of the Evidence and the Quality of Reporting of Statistical Results (Wicherts et al., 2011) Only Human: Scientists, Systems, and Suspect Statistics A review of: Improving Scientific Practice: Dealing With The Human Factors, University of Amsterdam, Amsterdam, September 11, 2014 (Hardwicke et al., 2014 Reply to Nuijten et al.: Reanalyses actually confirm that US studies overestimate effects in softer research (Fanelli \u0026amp; Ioannidis, 2014) Main Takeaways: Instead, Nuitjen et al. synthesized unscaled metaregression coefficients. Lack of scaling is problematic because the topics considered in the meta-analyses are very different and distributions of unscaled effects have several outliers. Nuitjen et al. back their claims of no “US effect” by plotting datapoints in histograms, but this violates the very essence of metaanalysis, which is to weight datapoints by an inverse function of their Standard error. Moreover, in the authors’ study, they only observed the US effect among behavioral studies, so graphs should be partitioned by method. The authors used the same code as Nuitjen et al. and found the same results as what the authors observed in their original articles such that the US effect was observed in behavioural studies. Nuitjen et al.’s reanalyses actually verify our conclusions. Nevertheless, additional studies should be encouraged to probe for the potential presence and magnitude of US effects also in other disciplines and larger datasets. Abstract A reply by Professor Daniele Fanelli and Professor John Ioannidis to Naudet and colleagues, the authors observed that the findings from the original article remain true even with a different analytical approach. APA Style Reference Fanelli, D., \u0026amp; Ioannidis, J. P. (2014). Reply to Nuijten et al.: Reanalyses actually confirm that US studies overestimate effects in softer research. Proceedings of the National Academy of Sciences, 111(7), E714-E715. https://doi.org/10.1073/pnas.1322565111\nYou may also be interested in Standard analyses fail to show that US studies overestimate effect sizes in softer research (Nuitjen et al., 2014) Is there a Reproducibility Crisis? (Baker, 2016) Main Takeaways: More than 70% of researchers have tried and failed to reproduce another scientist’s experiments, and more than half have failed to reproduce their own experiments. Those are some of the telling figures that emerged from Nature’s survey of 1,576 researchers who took a brief online questionnaire on reproducibility in research. The data reveal sometimes-contradictory attitudes towards reproducibility. Although 52% of those surveyed agree that there is a significant ‘crisis’ of reproducibility, less than 31% think that failure to reproduce published results means that the result is probably wrong, and most say that they still trust the published literature. But sorting discoveries from false leads can be discomfiting. Although the vast majority of researchers in our survey had failed to reproduce an experiment, less than 20% of respondents said that they had ever been contacted by another researcher unable to reproduce their work. When work does not reproduce, researchers often assume there is a perfectly valid (and probably boring) reason. What’s more, incentives to publish positive replications are low and journals can be reluctant to publish negative findings. In fact, several respondents who had published a failed replication said that editors and reviewers demanded that they play down comparisons with the original study. Nevertheless, 24% said that they had been able to publish a successful replication and 13% had published a failed replication. Acceptance was more common than persistent rejection: only 12% reported being unable to publish successful attempts to reproduce others’ work; 10% reported being unable to publish unsuccessful attempts. One-third of respondents said that their labs had taken concrete steps to improve reproducibility within the past five years. Rates ranged from a high of 41% in medicine to a low of 24% in physics and engineering. Free-text responses suggested that redoing the work or asking someone else within a lab to repeat the work is the most common practice. Also common are efforts to beef up the documentation and standardization of experimental methods. One of the best-publicized approaches to boosting reproducibility is pre-registration, where scientists submit hypotheses and plans for data analysis to a third party before performing experiments, to prevent cherry-picking statistically significant results later. Respondents were asked to rate 11 different approaches to improving reproducibility in science, and all got ringing endorsements. Nearly 90% — more than 1,000 people — ticked “More robust experimental design” “better statistics” and “better mentorship”. Those ranked higher than the option of providing incentives (such as funding or credit towards tenure) for reproducibility-enhancing practices. But even the lowest-ranked item — journal checklists — won a whopping 69% endorsement. About 80% of respondents thought that funders and publishers should do more to improve reproducibility. Quote ““It’s healthy that people are aware of the issues and open to a range of straightforward ways to improve them,” says Munafo. And given that these ideas are being widely discussed, even in mainstream media, tackling the initiative now may be crucial. “If we don’t act on this, then the moment will pass, and people will get tired of being told that they need to do something.” (p.454). Abstract A Nature survey lifts the lid on how researchers view the ‘crisis’ rocking science and what they think will help. APA Style Reference Baker, M. (2016). Reproducibility crisis. Nature, 533(26), 353-66. doi:10.1038/533452a\nYou may also be interested in Publishing Research With Undergraduate Students via Replication Work: The Collaborative Replications and Education Project (Wagge et al., 2019) Only Human: Scientists, Systems, and Suspect Statistics A review of: Improving Scientific Practice: Dealing With The Human Factors, University of Amsterdam, Amsterdam, September 11, 2014 (Hardwicke et al., 2014) Rein in the four horsemen of irreproducibility (Bishop, 2019) Seven Steps Toward Transparency and Replicability in Psychological Science (Lindsay, 2020) On the persistence of low power in psychological science (Vankov et al., 2014) Research Culture and Reproducibility (Munafo et al., 2020) Behaviour and the standardization fallacy (Wurbel, 2000) Main Takeaways: Because specific strains of mice, husbandry practice and test protocol may influence behavioural effects of mutations in unpredictable ways, scientists strive for consensus on rigorous standards to maximize reproducibility of results across laboratories. Increasing reproducibility of results through standardization, however, accentuates and obscures the very problem— that of reporting artefacts that are idiosyncratic to particular circumstances. Standardization serves to reduce individual differences within study populations (within-experiment variation) in order to facilitate detection of treatment effects, and to reduce differences between studies (between-experiment variation) in order to maximize the reproducibility of results. External validity stands for “how applicable your results are to other situations (environmental contexts), populations or species”. External validity is an inherent feature of a result and will not be affected by standardization. However, standardization increases the risk of detecting effects with low external validity (or of missing effects with high external validity). In contrast, reproducibility can be increased simply by equating situations more carefully and, hence, tells us nothing about external validity. A result that is highly reproducible under highly standardized conditions may therefore poorly generalize to other conditions, whereas high external validity necessarily goes together with high reproducibility, even when conditions are poorly equated between replicate studies. It is important to note that using a single standardized genetic or environmental background or test situation for the characterization of mutants makes it impossible to distinguish artefacts from informative effects. Systematic variation of situations is the only means to determine the nature, and demonstrate external validity, of genetic effects on behaviour. Abstract An editorial about the standardisation fallacy by Dr Hanno Wurbel. APA Style Reference Würbel, H. (2000). Behaviour and the standardization fallacy. Nature genetics, 26(3), 263-263. https://doi.org/10.1038/81541\nYou may also be interested in See references for later Research Culture and Reproducibility (Munafo et al., 2020) Main Takeaways: There is ongoing debate regarding the extent to which research claims are robust and credible. Modern research-intensive universities present a paradox. On the one hand, they are dynamic, vibrant institutions where researchers use cutting-edge methods to advance knowledge. On the other, their traditions, structures, and ways of working remain rooted in the 19th century model of the independent scientist. A growing realization of this, and the impact it might have on the performance of research intensive institutions, has led to growing interest in examining and understanding research culture. The vast majority of scientists choose their career because they are passionate about their subject and are excited by the possibility of advancing human knowledge. However, this passion can serve as a double-edged sword. When the authors are personally invested in our own research, then their ability to objectively analyze data may be negatively affected. The authors may see patterns in noise, suffer from confirmation bias, and so on. The authors have argued that open research practices – protocol preregistration, data and material sharing, the use of preprints and so on – can protect against these kinds of cognitive biases. Promoting transparency in methods and data sharing should encourage greater self- and peer-appraisal of research methods. Although the conventional journal article format, with restrictions on word count and display items may not encourage this, exciting innovations are emerging that offer new approaches to scientific communication – preprint servers, post-publication peer review (e.g., F1000), the \u0026#39;Registered Reports\u0026#39; article format, and data repositories. Given these innovations, there is really no reason to provide only a partial account of one’s research. Open research also highlights the extent to which our current scientific culture relies heavily on trust. This may have been appropriate in the 19th century era of the independent scientist (although even that is debatable), but it does not provide a strong basis for robust science in the highly charged and competitive environment of modern science. At present, it is difficult for research consumers to know whether what is reported in an article is a complete and honest account of what was actually done and found. This desire for narrative is reflected in something that many early-career researchers are told – that their data needs to \u0026#39;tell a story\u0026#39; (i.e. scientists should write in a clear and compelling way). However, the focus on narrative has come to dominate to such an extent that perhaps the story matters more than the truth. Scientists are rarely incentivized by the system for being right – they are rewarded for papers, grants, and so on, but not (directly) for getting the right answer – and their success in writing papers and winning grants often reflects their storytelling rather than their science. Is this the fault of the journals? There is a place for high-risk, high-return findings – those which may well be wrong but which if right would turn out to be transformative (which essentially is what groundbreaking research is). It is our institutions – their hiring and promotion practices – and to an extent the authors ourselves – the community of scientists – that fetishize publication in specific journals. By disproportionately lauding and rewarding high-risk, high-return activity, we risk incentivizing science in a manner similar to the way in which the banking system was incentivized before 2008 – the focus on high-return investment vehicles that looked reliable and robust but were in fact built on sand. And that did not end well. Quote “Fundamentally, the authors need to better align our research culture with the demands of 21st century research. The authors need to move away from a model that relies on trust in individual researchers towards one where the system is inherently trustworthy. This will require a focus on realigning incentives such that what is good for scientists’ careers is good for scientists, as well as on recognizing that excellence in research is not generated by individuals but by teams, departments, institutions, and international collaborations. These teams require a diverse range of skills, each of which is crucial to the success of the wider effort.” (p.92) Abstract There is ongoing debate regarding the robustness and credibility of published scientific research. We argue that these issues stem from two broad causal mechanisms: the cognitive biases of researchers and the incentive structures within which researchers operate. The UK Reproducibility Network (UKRN) is working with researchers, institutions, funders, publishers, and other stakeholders to address these issues. APA Style Reference Munafò, M. R., Chambers, C. D., Collins, A. M., Fortunato, L., \u0026amp; Macleod, M. R. (2020). Research culture and reproducibility. Trends in Cognitive Sciences, 24(2), 91-93. https://doi.org/10.1016/j.tics.2019.12.002\nYou may also be interested in Publishing Research With Undergraduate Students via Replication Work: The Collaborative Replications and Education Project (Wagge et al., 2019) Only Human: Scientists, Systems, and Suspect Statistics A review of: Improving Scientific Practice: Dealing With The Human Factors, University of Amsterdam, Amsterdam, September 11, 2014 (Hardwicke et al., 2014) Rein in the four horsemen of irreproducibility (Bishop, 2019) Seven Steps Toward Transparency and Replicability in Psychological Science (Lindsay, 2020) On the persistence of low power in psychological science (Vankov et al., 2014) Is there a Reproducibility Crisis? (Baker, 2016) Measurement error and the replication crisis (Loken \u0026amp; Gelman, 2017) Main Takeaways: Measurement error adds noise to predictions, increases uncertainty in parameter estimates, and makes it more difficult to discover new phenomena or to distinguish among competing theories. A common view is that any study finding an effect under noisy conditions provides evidence that the underlying effect is particularly strong and robust. Yet, statistical significance conveys very little information when measurements are noisy. In noisy research settings, poor measurement can contribute to exaggerated estimates of effect size. Should we assume that if statistical significance is achieved in the presence of measurement error, the associated effects would have been stronger without noise? The author caution against the fallacy of assuming that that which does not kill statistical significance makes it stronger. Measurement error and other sources of uncontrolled variation in scientific research therefore add noise. It is understandable, then, that many researchers have the intuition that if they manage to achieve statistical significance under noisy conditions, the observed effect would have been even larger in the absence of noise. In settings with uncontrolled researcher degrees of freedom, the attainment of statistical significance in the presence of noise is not an impressive feat. For the largest samples, the observed effect is always smaller than the original. But for smaller N, a fraction of the observed effects exceeds the original. The authors are concerned researchers are sometimes tempted to use the “iron law” reasoning to defend or justify surprisingly large statistically significant effects from small studies. If it really were true that effect sizes were always attenuated by measurement error, then it would be all the more impressive to have achieved significance. Quote “A key point for practitioners is that surprising results from small studies should not be defended by saying that they would have been even better with improved measurement. Furthermore, the signal-to-noise ratio cannot in general be estimated merely from internal evidence. It is a common mistake to take a t-ratio as a measure of strength of evidence and conclude that just because an estimate is statistically significant, the signal-to-noise level is high. It is also a mistake to assume that the observed effect size would have been even larger if not for the burden of measurement error...measurement error (or other uncontrolled variation) should not be invoked automatically to suggest that effects are even larger”. (p.585). Abstract The assumption that measurement error always reduces effect sizes is false. APA Style Reference Loken, E., \u0026amp; Gelman, A. (2017). Measurement error and the replication crisis. Science, 355(6325), 584-585.\nYou may also be interested in How scientists can stop fooling themselves (Bishop, 2020b) The Statistical Crisis in Science (Gelman \u0026amp; Loken, 2014) Only Human: Scientists, Systems, and Suspect Statistics A review of: Improving Scientific Practice: Dealing With The Human Factors, University of Amsterdam, Amsterdam, September 11, 2014 (Hardwicke et al., 2014) A 21 Word Solution (Simmons et al., 2012) Rein in the four horsemen of irreproducibility (Bishop, 2019) False-Positive Psychology: Undisclosed Flexibility in Data Collection and Analysis Allows Presenting Anything as Significant (Simmons et al., 2011) Many Analysts, One Data Set: Making Transparent How Variations in Analytical Choices Affect Results (Silberzahn et al., 2019) Why small low-powered studies are worse than large high-powered studies and how to protect against “trivial” findings in research: Comment on Friston (2012) (Ingre, 2013) Ironing out the statistical wrinkles in “ten ironic rules” (Lindquist et al., 2013) Ten ironic rules for non-statistical reviewers (Friston, 2012) Current Incentives for Scientists Lead to Underpowered Studies with Erroneous Conclusions (Higginson \u0026amp; Munafo, 2016) Small sample size is not the real problem (Bacchetti, 2013) Bite-Size Science and Its Undesired Side Effects (Bertamini \u0026amp; Munafo, 2012) Confidence and precision increase with high statistical power (Button et al., 2013) Markets for replication (Brandon \u0026amp; List, 2015) Main Takeaways: Dreber et al. take the innovative approach of considering whether markets can play a crucial role. The Dreber et al. study focuses on the replicability of recent publications in top psychology journals. Brandon and List argued that the crux of just about every empirical study is the P value. Researchers pose a null hypothesis meant to capture the status quo line of thinking. Data are then analyzed and if the P value \u0026lt; .05, then the researcher rejects the null hypothesis and an alternative hypothesis is evident. However, the mechanics of the inference problem call into question this simple approach. Inference not only relies on reported P values, but also priors and the power of the test. The scientific community wants to identify true findings then they need replications. Even in those cases when they allow only a small false-positive rate (a P value of 0.01), the author needs three successful replications before they can be very confident that the observed relationship is a true relationship. Furthermore, as the scientific community allows a larger false-positive rate, more replications are necessary. Quote “Although these ideas can work on the margin, unless there is a sweeping change in academic culture, the returns to publishing original work will always dominate work on replications. Prediction markets, such as those used in Dreber et al. (5), offer a different type of incentive for replications: financial returns. Imagine a market wherein academics can trade on the outcome of replications and a small cut on transactions funds the work of actually conducting the replications. Such a market may suffer the liquidity problems that have doomed other prediction markets (e.g., Intrade), but in light of the ideas currently on the table, this one is worth strong consideration.” (p.15268). Abstract A response by Professors Alec Brandon and John A. List who reply to a paper by Professor Anna Dreber and colleagues. APA Style Reference Brandon, A., \u0026amp; List, J. A. (2015). Markets for replication. Proceedings of the National Academy of Sciences, 112(50), 15267-15268. https://doi.org/10.1073/pnas.1521417112\nYou may also be interested in See references for later Ten common statistical mistakes to watch out for when writing or reviewing a manuscript (Ijzerman et al., 2020) Main Takeaways: In the authors’ view, the most appropriate checkpoint to prevent erroneous results from being published is the peer-review process at journals, or the online discussions that can follow the publication of preprints. The primary purpose of this commentary is to provide reviewers with a tool to help identify and manage these common issues. To promote further discussion of these issues, and to consolidate advice on how to best solve them, the authors encourage readers to offer alternative solutions to ours by annotating the online version of this article (by clicking on the ’annotations’ icon). This will allow other readers to benefit from a diversity of ideas and perspectives. The authors hope that greater awareness of these common mistakes will help make authors and reviewers more vigilant in the future so that the mistakes become less common. The 10 mistakes are: Abstract Inspired by broader efforts to make the conclusions of scientific research more robust, we have compiled a list of some of the most common statistical mistakes that appear in the scientific literature. The mistakes have their origins in ineffective experimental designs, inappropriate analyses and/or flawed reasoning. We provide advice on how authors, reviewers and readers can identify and resolve these mistakes and, we hope, avoid them in the future. APA Style Reference Makin, T. R., \u0026amp; de Xivry, J. J. O. (2019). Science Forum: Ten common statistical mistakes to watch out for when writing or reviewing a manuscript. Elife, 8, e48175. DOI: 10.7554/eLife.48175\nYou may also be interested in See references for later Comparing journal-independent review services (ASAPbio, 2020) ◈ Main Takeaways: Preprinting not only accelerates the dissemination of science, but also enables early feedback from a broad community. Therefore, it’s no surprise that there are many innovative projects offering feedback, commentary, and peer reviews on preprints. Such feedback can range from the informal (tweets, comments, annotations, or a simple endorsement) to the formal (an editor-organized process that can provide an in-depth assessment of the manuscript leading to a formal acceptance/endorsement like in a journal). This organized, journal-independent peer review might accomplish several goals: it can provide readers with context to evaluate the paper and foster constructive review that is focused on improving the science rather than gatekeeping for a particular journal. It can also be used as a way to validate the scientific content of a preprint, supporting its value as a citable reference for the scientific literature. When preprints are submitted to a journal, journal-independent peer review can be used by editors to speed up their editorial decisions. Additionally, since 15 million hours of reviewers’ time is wasted every year in re-reviewing revised manuscripts, transparent peer review on preprints could be one way to make the entire publishing process more efficient for reviewers, authors, and editors alike. Preprint Review does not currently have formal journal participation outside of eLife, but it can be said to provide a journal recommendation because one of the outcomes is acceptance at eLife. Peerage of Science provides authors with recommendations on which journal to submit to; Peer Community In and Review Commons do not. Peerage of Science is the only service in our comparison that sends all submitted manuscripts for peer review. Preprint Review attempts to send all manuscripts for review but is sometimes limited by workload and availability of editors. Peer Community In only performs review if an associate editor accepts to handle the manuscript within 20 days, and Review Commons selects papers for review in consultation with an editorial board, looking for significant advancements for the field. The services take a variety of approaches to transparency. At Preprint Review, posting the preprint and reviews is mandatory. Peer Community In posts reviews publicly and publishes a recommendation text with a DOI only if the paper is accepted by the service; otherwise they are transferred confidentially to the author. Both Peerage of Science and Review Commons allow authors to opt-in to post reviews publicly. Reviewers are named by default (though they can opt-out) when reviewing for Peer Community In, whereas the identity of reviewers is not communicated to the authors by default in the case of Review Commons, Peerage of Science, and Preprint Review. Abstract An editorial about comparing journal-independent peer reviews. APA Style Reference ASAPbio (2020, July). Comparing journal-independent review services [Blog post]. Retrieved from https://asapbio.org/comparing-review-services\nYou may also be interested in Publication Prejudices: An Experimental Study of Confirmatory Bias in the Peer Review System (Mahoney, 1977) Effect of open peer review on quality of reviews and on reviewers’ recommendations: a randomised trial (van Rooyen et al., 1999) The Peer Reviewers’ Openness Initiative: incentivising open research practices through peer review (Morey et al., 2016) Problems with p values: why p values do not tell you if your treatment is likely to work (Price et al., 2020) Main Takeaways: In null hypothesis significance test, p value is a conditional probability, conditioned on the null hypothesis being true. Authors calculate p values because it is easy, not because they are the probability that they want. P values cannot be used to indicate if the null hypothesis is true or false. It is incorrectly assumed that if p = .05 there is a 1 in 20 probability that the data arose by chance alone. P value exaggerates on weight of evidence against null hypothesis. The scientific community needs to discuss the false discovery rate, which is the proportion of reported discoveries that are false positives. The false discovery rate can be calculated from the power, type I error and an estimate of the prevalence of effects among the many ideas that the scientists may test. Type I error rate is long-term probability of false positive for a single experiment repeated with exact replication. It is not the same as false discovery rate, which applies to a single run of each experiment. Even when experiments are perfectly designed and executed without bias, missing data or multiple statistical testing, the false discovery rate in null hypothesis significance testing using a p \u0026lt; .05 threshold has been estimated to be in the range 30%-60%, depending on the field of research and the power of the study. In biomedical research, researchers often do not know the effect size as it is frequently small, sampling is difficult and variance is often large and poorly known. Researchers only do the experiment once and have only a one-point estimate of the p value. Additionally, scientific theories are only weakly predictive and do not generate precise numerical quantities that can be checked in quantitative experiments as is possible in the physical sciences. Null hypothesis significance testing does not perform well under these circumstances. The p value is strictly the probability of obtaining data as extreme or more so, given the null hypothesis is true. The most common error was to claim that the p value is the probability that the data was generated by chance alone. Beyond misinterpretations of p values, there are also widespread problems with multiple testing, sometimes inadvertent, which grossly inflates the proportion of false positive results (i.e. p-hacking). However, confidence intervals are also misinterpreted and often used to produce identical results to a p\u0026lt;0.05 significance test. Reducing the p value threshold will reduce the false positive rate at the cost of an increase in the false negative rate, particularly in under-powered studies. A more intuitive methodology to use is Bayesian statistics; this calculates the probability that a hypothesis is true given the data; this is mostly what researches and readers actually assume the p value to be. This is much more meaningful than the frequentist CI, which is again based on performance over many repetitions but is measured only once. In the past, the two major objections to Bayesian methods have been the difficulty of calculating intractable integrals and the use of prior probabilities. We urge authors and editors to demote the prominence of p values in journal articles, have the actual null hypothesis formally stated at the beginning of the article and promote the use of the more intuitive (but harder to calculate) Bayesian statistics. The p value in null hypothesis significance testing is conditioned on the null hypothesis being true. This means that a p value of 0.05 does not mean that the probability our data arose by chance alone is 1 in 20.. In fact, the chance of us mistakenly rejecting the null hypothesis and concluding we have a successful treatment is more in the region of 30%–60%. Scientific journals and textbooks need to be explicit on how p values are used and defined. Use of the more intuitive Bayesian statistics should become more widespread. The main point of the article is that frequentism only works in repeated testing scenarios, while it does not work in one time experiments. Finally, researchers should move towards Bayesian statistics and forego frequentist statistics. Abstract Dr Robert Price and colleagues argue about the problems of p values and support the idea that researchers should adopt Bayesian statistics. APA Style Reference Price, R., Bethune, R., \u0026amp; Massey, L. (2020). Problem with p values: why p values do not tell you if your treatment is likely to work. Postgraduate medical journal, 96(1131), 1. http://dx.doi.org/10.1136/postgradmedj-2019-137079\nYou may also be interested in How scientists can stop fooling themselves (Bishop, 2020b) The Statistical Crisis in Science (Gelman \u0026amp; Loken, 2014) Only Human: Scientists, Systems, and Suspect Statistics A review of: Improving Scientific Practice: Dealing With The Human Factors, University of Amsterdam, Amsterdam, September 11, 2014 (Hardwicke et al., 2014) A 21 Word Solution (Simmons et al., 2012) Rein in the four horsemen of irreproducibility (Bishop, 2019) False-Positive Psychology: Undisclosed Flexibility in Data Collection and Analysis Allows Presenting Anything as Significant (Simmons et al., 2011) Many Analysts, One Data Set: Making Transparent How Variations in Analytical Choices Affect Results (Silberzahn et al., 2019) Why small low-powered studies are worse than large high-powered studies and how to protect against “trivial” findings in research: Comment on Friston (2012) (Ingre, 2013) Ironing out the statistical wrinkles in “ten ironic rules” (Lindquist et al., 2013) Ten ironic rules for non-statistical reviewers (Friston, 2012) Confidence and precision increase with high statistical power (Button et al., 2013) Misuse of power: in defence of small-scale science (Quinlan, 2013) Small sample size is not the real problem (Bacchetti, 2013) Measurement error and the replication crisis (Loken \u0026amp; Gelman, 2017) Bite-Size Science and Its Undesired Side Effects (Bertamini \u0026amp; Munafo, 2012) Socioeconomic Status and Academic Outcomes in Developing Countries: A Meta-Analysis (Kim et al., 2019) Main Takeaways: This study is the first meta-analytic effort to focus on socio-economic status (SES) and academic outcomes in developing countries. As a growing number of countries reach minimum thresholds of school expansion and quality, and as they move toward privatization, families play an increasingly large role in the social stratification process, resulting in a stronger relation between SES and academic outcomes Meta-analyses up to date have tended to exclude developing countries, despite the potential theoretical contributions such studies can make. The paper aims to assess the overall association between SES and student achievement in developing countries? How does the strength of the association differ by countries’ economic levels of development, type of SES measure, grade level, and gender? Also, What is the overall association between SES and student attainment in developing countries? How does the strength of the association differ by countries’ economic levels of development, type of SES measure, grade level, and gender? How does study quality influence the ES of studies, and are there any differences in study quality by country studied, year of publication, and publication type? Since there have not been any meta-analyses of SES and educational outcomes of developing countries up to date, there is little information about how the effect size might be different for achievement and attainment. Studies collectively point to a positive association between SES and educational attainment but do not shed light on the overall strength of the relation between SES and attainment and how this compares to the relation between SES and achievement. Method: This meta-analysis included 49 empirical studies representing 294 correlations reporting the relation between SES and academic outcomes. All studies used students as the unit of analysis, and represented a total sample of 2,828,216 students, with samples ranging from 70 to 2,248,598. To be included in the meta analysis: Abstract Despite the multiple meta-analyses documenting the association between socioeconomic status (SES) and achievement, none have examined this question outside of English-speaking industrialized countries. This study is the first meta-analytic effort, to the best of our knowledge, to focus on developing countries. Based on 49 empirical studies representing 38 countries, and a sample of 2,828,216 school-age students (grades K–12) published between 1990 and 2017, we found an overall weak relation between SES and academic outcomes. Results for attainment outcomes were stronger than achievement outcomes, and the effect size was stronger in more economically developed countries. The SES-academic outcome relation was further moderated by grade level and gender. There were no differences in the strength of the relation by specific SES measures of income/consumption, education, and wealth/home resources. Our results provide evidence that educational inequalities are wider in higher income countries, creating a serious challenge for developing countries as they expand school access. APA Style Reference Kim, S. W., Cho, H., \u0026amp; Kim, L. Y. (2019). Socioeconomic Status and Academic Outcomes in Developing Countries: A Meta-Analysis. Review of Educational Research, 89(6), 875-916. https://doi.org/10.3102/0034654319877155 [ungated]\nYou may also be interested in Is science only for the rich? (Lee, 2016) ⌺ #bropenscience is broken science (Whitaker \u0026amp; Guest, 2020) ⌺ ls There a Positive Correlation between Socioeconomic Status and Academic Achievement? (Quagliata, 2008) Education and Socio-economic status (APA, 2017b) Ethnic and Racial minorities and socio-economic status (APA, 2017) Women and Socio-economic status (APA, 2010) Disability and Socio-economic status (APA, 2010) Lesbian, Gay, Bisexual and Transgender Persons \u0026amp; Socioeconomic Status (APA, 2010) The Relation Between Family Socioeconomic Status and Academic Achievement in China: A Meta-analysis (Liu et al., 2020) How scientists can stop fooling themselves over statistics (Bishop, 2020b) Main Takeaways: Lab scientists should not be allowed to handle dangerous substances without safety training, researchers should not be allowed to be near a p value or similar measures of probability until researchers can demonstrate they understand their meaning. Preconceived notions allow us to see a structure that is not there, whereas contradictory views provided from new data tend to be ignored. People under-estimate how noisy small samples can be and conduct studies that lack the necessary power to detect an effect. The more variables are investigated, the more likely a p value is to be significant due to type I error. Basic statistical training is insufficient or counterproductive, providing misplaced confidence. Simulated data allows students to discover how easy it is to find false results that are significant. Students learn with simulation that small sample sizes are useless to show a moderate difference. Researchers need to build lifelong habits to avoid being led astray by specific confirmation bias. It is easy to forget papers that counter our own instacts, albeit the papers had no flaws. Keeping tracks of these papers enables us to understand the blind spots and how to avoid them. Abstract Sampling simulated data can reveal common ways in which our cognitive biases mislead us. APA Style Reference Bishop, D. (2020). How scientists can stop fooling themselves over statistics. Nature, 584(7819), 9. https://doi.org/10.1038/d41586-020-02275-8\nYou may also be interested in The Statistical Crisis in Science (Gelman \u0026amp; Loken, 2014) Only Human: Scientists, Systems, and Suspect Statistics A review of: Improving Scientific Practice: Dealing With The Human Factors, University of Amsterdam, Amsterdam, September 11, 2014 (Hardwicke et al., 2014) False-Positive Psychology: Undisclosed Flexibility in Data Collection and Analysis Allows Presenting Anything as Significant (Simmons et al., 2011) Publication Prejudices: An Experimental Study of Confirmatory Bias in the Peer Review System (Mahoney, 1977) Quality Uncertainty Erodes Trust in Science (Vazire, 2017) Rein in the four horsemen of irreproducibility (Bishop, 2019) Seven Easy Steps to Open Science: An Annotated Reading List (Crüwell et al., 2019) Seven Steps Toward Transparency and Replicability in Psychological Science (Lindsay, 2020) A consensus-based transparency checklist (Aczel et al., 2020) Tell it like it is (Anon, 2020) Is pre-registration worthwhile? (Szollosi et al., 2020) The life of p: “Just significant” results are on the rise (Leggett et al., 2013) Many Analysts, One Data Set: Making Transparent How Variations in Analytical Choices Affect Results (Silberzahn et al., 2019) Scientific inbreeding and same-team replication: Type D personality as an example (Ioannidis, 2012) Variability in the analysis of a single neuroimaging dataset by many teams (Botvinik-Nezer et al., 2020) Main Takeaways: The present study investigated the degree and effect of analytical flexibility on functional magnetic resonance imaging (fMRI) results in practice. The authors estimated the beliefs of researchers in the field concerning degree of variability in analysis outcomes using prediction markets to test whether peers in the field could predict the findings. Method: 70 teams were provided raw data, an optional preprocessed version of the dataset and were asked to analyse data to test 9 ex-ante hypotheses. They were given up to 100 days to report whether each hypothesis was supported based on whole-brain-corrected analysis. Method: The research group were instructed to perform analysis as they would in their own laboratory and report binary decisions based on their own criteria for whole-brain corrected results for specific regions. Results: The analytical flexibility resulted in sizable variation in the results of hypothesis tests, even for teams whose statistical maps were highly correlated at intermediate stages of the analysis pipeline. Variation in reported results was related to several aspects of analysis methodology. Notably, a meta-analytical approach that aggregated information across teams yielded a significant consensus in activated regions. Furthermore, prediction markets of researchers in the field revealed an overestimation of the likelihood of significant findings, even by researchers with direct knowledge of the dataset. It is hard to estimate the reproducibility of single studies performed using a single analysis pipeline. Teams with highly correlated underlying unthresholded statistical maps showed different hypothesis outcomes. Prediction markets performed on the outcome of analyses showed general over-estimation by researchers of likelihood of significant findings across hypotheses reflecting marked optimism bias by researchers in the field. Complex datasets should be analysed using several analysis pipelines, and by more than one research team. fMRI can provide reliable answers to scientific questions, as strongly shown in meta-analytical results across teams along with several large-scale studies and replication of many findings using fMRI. Abstract Data analysis workflows in many scientific domains have become increasingly complex and flexible. Here we assess the effect of this flexibility on the results of functional magnetic resonance imaging by asking 70 independent teams to analyse the same dataset, testing the same 9 ex-ante hypotheses. The flexibility of analytical approaches is exemplified by the fact that no two teams chose identical workflows to analyse the data. This flexibility resulted in sizeable variation in the results of hypothesis tests, even for teams whose statistical maps were highly correlated at intermediate stages of the analysis pipeline. Variation in reported results was related to several aspects of analysis methodology. Notably, a meta-analytical approach that aggregated information across teams yielded a significant consensus in activated regions. Furthermore, prediction markets of researchers in the field revealed an overestimation of the likelihood of significant findings, even by researchers with direct knowledge of the dataset. Our findings show that analytical flexibility can have substantial effects on scientific conclusions, and identify factors that may be related to variability in the analysis of functional magnetic resonance imaging. The results emphasize the importance of validating and sharing complex analysis workflows, and demonstrate the need for performing and reporting multiple analyses of the same data. Potential approaches that could be used to mitigate issues related to analytical variability are discussed. APA Style Reference Botvinik-Nezer, R., Holzmeister, F., Camerer, C. F., Dreber, A., Huber, J., Johannesson, M., ... \u0026amp; Avesani, P. (2020). Variability in the analysis of a single neuroimaging dataset by many teams. Nature, 582, 84-88. https://doi.org/10.1038/s41586-020-2314-9\nYou may also be interested in Next Steps for Citizen Science (Bonney et al., 2014) Citizen Science: Can Volunteers Do Real Research? (Cohn, 2008) The Increasing Dominance of Teams In Production of Knowledge (Wuchty et al., 2007) A Manifesto for Team Science (Forscher et al., 2020) Many hands make tight work(Silberzahn \u0026amp; Uhlmann, 2015) Publishing Research With Undergraduate Students via Replication Work: The Collaborative Replications and Education Project (CREP; Wagge et al., 2019) Many Analysts, One Data Set: Making Transparent How Variations in Analytical Choices Affect Results (Silberzahn et al., 2019) Fifty psychological and psychiatric terms to avoid:a list of inaccurate, misleading, misused, ambiguous, and logically confused words and phrases (Lilienfeld et al., 2015) Main Takeaways: Scientific thinking necessitates clarity, including clarity in writing. In turn clarity hinges on accuracy in the use of specialized terminology. Researchers, students and allied health researchers should be as explicit as possible about what they are saying andare not saying,as terms in these disciplines readily lend themselves to confusion and misinterpretation. If students are allowed,or worse,encouraged,to be imprecise in their language concerning psychological concepts,their thinking about these concepts is likely to follow suit. First,some psychological terms are inaccurate or misleading. Quote “We modestly hope that our admittedly selective list of 50 terms to avoid will become recommended, if not required, reading for students, instructors, and researchers in psychology, psychiatry, and similar disciplines. Although jargon has a crucial place in these fields, it must be used with care, as the imprecise use of terminology can engender conceptual confusion. At the very least, we hope that our article encourages further discussion regarding the vital importance of clear writing and clear thinking in science, and underscores the point that clarity in writing and thinking are intimately linked. Clear writing fosters clear thinking, and confused writing fosters confused thinking. In the words of author McCullough (2002), “Writing is thinking. To write well is to think clearly. That’s why it’s so hard.”” (p.11). Abstract The goal of this article is to promote clear thinking and clear writing among students and teachers of psychological science by curbing terminological misinformation and confusion. To this end, we present a provisional list of 50 commonly used terms in psychology, psychiatry, and allied fields that should be avoided, or at most used sparingly and with explicit caveats. We provide corrective information for students, instructors, and researchers regarding these terms, which we organize for expository purposes into five categories: inaccurate or misleading terms, frequently misused terms, ambiguous terms, oxymorons, and pleonasms. For each term, we (a) explain why it is problematic, (b) delineate one or more examples of its misuse, and (c) when pertinent, offer recommendations for preferable terms. By being more judicious in their use of terminology, psychologists and psychiatrists can foster clearer thinking in their students and the field at large regarding mental phenomena. APA Style Reference Lilienfeld, S. O., Sauvigné, K. C., Lynn, S. J., Cautin, R. L., Latzman, R. D., \u0026amp; Waldman, I. D. (2015). Fifty psychological and psychiatric terms to avoid: a list of inaccurate, misleading, misused, ambiguous, and logically confused words and phrases. Frontiers in Psychology, 6, 1100. https://doi.org/10.3389/fpsyg.2015.01100\nYou may also be interested in References will be included soon Is science only for the rich? (Lee, 2016) ⌺ Main Takeaways: Few countries collect detailed data on socioeconomic status, but the available numbers consistently show that nations are wasting the talents of underprivileged youth who might otherwise be tackling challenges in health, energy, pollution, climate change and a host of other societal issues. And it’s clear that the universal issue of class is far from universal in the way it plays out. Here, Nature looks at eight countries around the world, and their efforts to battle the many problems of class in science. Students from poor districts therefore end up being less prepared for university-level science than are their wealthier peers, many of whom attended well-appointed private schools. That also puts the students at a disadvantage in the fiercely competitive applications process: only about 40% of high-school graduates in the lowest-income bracket enrolled in a university in 2013, versus about 68% of those born to families with the highest incomes. The students who do get in then have to find a way to pay the increasingly steep cost of university. Between 2003 and 2013, undergraduate tuition, fees, room and board rose by an average of 34% at state-supported institutions, and by 25% at private institutions, after adjusting for inflation. The bill at a top university can easily surpass US$60,000 per year. Many students are at least partly supported by their parents, and can also take advantage of scholarships, grants and federal financial aid. Many, like Quasney, work part time. But if graduate students have to worry about repaying student loans, that can dissuade them from continuing with their scientific training. In China: Abstract Around the world, poverty and social background remain huge barriers in scientific careers. APA Style Reference Lee, J. J. (2016). Is science only for the rich?. Nature, 537(7621), 466-467.\nYou may also be interested in #bropenscience is broken science (Whitaker \u0026amp; Guest, 2020) ⌺ ls There a Positive Correlation between Socioeconomic Status and Academic Achievement? (Quagliata, 2008) Education and Socio-economic status (APA, 2017b) Ethnic and Racial minorities and socio-economic status (APA, 2017) Women and Socio-economic status (APA, 2010) Disability and Socio-economic status (APA, 2010) Lesbian, Gay, Bisexual and Transgender Persons \u0026amp; Socioeconomic Status (APA, 2010) The Relation Between Family Socioeconomic Status and Academic Achievement in China: A Meta-analysis (Liu et al., 2020) Socioeconomic Status and Academic Outcomes in Developing Countries: A Meta-Analysis (Kim et al., 2019) Psychology’s Renaissance (Nelson et al., 2018) Main Takeaways: Researchers now understand that the old ways of collecting and analyzing data produce results that are not diagnostic of truth and that a new, more enlightened approach is needed. Thousands of psychologists have embraced this notion. The improvements to our field have been dramatic. This is psychology’s renaissance. The authors believe that this “file-drawer explanation” is incorrect. Most failed studies are not missing. They are published in our journals, masquerading as successes. The file-drawer explanation becomes transparently implausible once its assumptions are made explicit. It assumes that researchers conduct a study and perform one (predetermined) statistical analysis. If the analysis is significant, then they publish it. P-hacking provides the real solution to the paradox. P-hacking is the only honest and practical way to consistently get underpowered studies to be statistically significant. P-hacking makes it dramatically easier to generate false-positive findings, so much so that, for decades, p-hacking enabled researchers to achieve the otherwise mathematically impossible feat of getting most of their underpowered studies to be significant. P-hacking has long been the biggest threat to the integrity of our discipline. False positives are bad. Publishing them can cause scientists to spend precious resources chasing down false leads, policy makers to enact potentially harmful or ineffective policies, and funding agencies to allocate their resources away from hypotheses that are actually true. When false positives populate the scientific literature, we can no longer distinguish between what is true and what is false, undermining the very goal of science. P-hacking is a pervasive problem precisely because researchers usually do not realize that they are doing it or appreciate that what they are doing is consequential. The most straightforward way to prevent researchers from selectively reporting their methods and analyses is to require them to report less selectively. At the bare minimum, this means requiring authors to disclose all of their measures, manipulations, and exclusions, as well as how they determined their sample sizes. Overcoming this concern requires realizing that preregistrations do not tie researchers’ hands, but merely uncover readers’ eyes. Preregistering does not preclude exploration, but it does communicate to readers that it occurred. Preregistering allows readers to discriminate between confirmatory analyses, which provide valid p-values and trustworthy results, and exploratory analyses, which provide invalid p-values and tentative results. Replications have traditionally been deemed failures when the effect described by the original study is not statistically significant in the replication. This approach has two obvious flaws. First, a replication attempt could be nonsignificant simply because its sample size is too small. Second, a replication attempt could be significant even if the effect size is categorically smaller than in the original. To correct selective reporting is p-curve analysis, P-curve is the distribution of statistically significant p-values from a set of studies. P-curve’s shape can be used to diagnose whether a literature contains replicable effects. Discussions of fraud typically focus on two questions: How common is it and how can we stop it? Estimating the frequency of fraud is very difficult. Some blatantly detectable fraud is prevented by vigilant coauthors, reviewers, or editors and, thus, not typically observed by the rest of the field. The fraud that gets through those filters might be noticed by a very small share of readers. Of those readers, a very small number might ever make their concerns known. Meta-analytic thinking has its benefits. It allows inferences to be based on larger and potentially more diverse samples, promotes collaboration among scientists, and incentivizes more systematic research programs. Nevertheless, meta-analytic thinking not only fails to solve the problems of p-hacking, reporting errors, and fraud, it dramatically exacerbates them. Why is p values heavily relied upon? The authors think it is because there is actually no compelling reason to abandon the use of p-values. It is true that p-values are imperfect, but, for the types of questions that most psychologists are interested in answering, they are no more imperfect than confidence intervals, effect sizes, or Bayesian approaches. The biggest problem with p-values is that they can be mindlessly relied upon; however, when effect size estimates, confidence intervals, or Bayesian results are mindlessly relied upon, the results are at least as problematic. It is not the statistic that causes the problem, it is the mindlessness. Quote “It might make sense for new graduate students to erroneously think 12 participants per cell will be a sufficiently large sample size to test a counterintuitive attenuated interaction hypothesis, but it would not make sense for a full professor to maintain this belief after running hundreds of experiments that should have failed. It is one thing for a very young child to believe that 12 peas are enough for dinner and quite another for a chronically starving adult to do so.” (p.515) Abstract In 2010–2012, a few largely coincidental events led experimental psychologists to realize that their approach to collecting, analyzing, and reporting data made it too easy to publish false-positive findings. This sparked a period of methodological reflection that we review here and call Psychology’s Renaissance. We begin by describing how psychologists’ concerns with publication bias shifted from worrying about file-drawered studies to worrying about p-hacked analyses. We then review the methodological changes that psychologists have proposed and, in some cases, embraced. In describing how the renaissance has unfolded, we attempt to describe different points of view fairly but not neutrally, so as to identify the most promising paths forward. In so doing, we champion disclosure and preregistration, express skepticism about most statistical solutions to publication bias, take positions on the analysis and interpretation of replication failures, and contend that meta-analytical thinking increases the prevalence of false positives. Our general thesis is that the scientific practices of experimental psychologists have improved dramatically. APA Style Reference Nelson, L. D., Simmons, J., \u0026amp; Simonsohn, U. (2018). Psychology\u0026#39;s renaissance. Annual review of psychology, 69, 511-534. https://doi.org/10.1146/annurev-psych-122216-011836\nYou may also be interested in False-Positive Psychology: Undisclosed Flexibility in Data Collection and Analysis Allows Presenting Anything as Significant (Simmons et al., 2011) Seven Steps Toward Transparency and Replicability in Psychological Science (Lindsay, 2020) The life of p: “Just significant” results are on the rise (Leggett et al., 2013) Many Analysts, One Data Set: Making Transparent How Variations in Analytical Choices Affect Results (Silberzahn et al., 2019) The Statistical Crisis in Science (Gelman \u0026amp; Loken, 2014) Only Human: Scientists, Systems, and Suspect Statistics A review of: Improving Scientific Practice: Dealing With The Human Factors, University of Amsterdam, Amsterdam, September 11, 2014 (Hardwicke et al., 2014) A 21 Word Solution (Simmons et al., 2012)◈ Quality Uncertainty Erodes Trust in Science (Vazire, 2017) Rein in the four horsemen of irreproducibility (Bishop, 2019) Seven Easy Steps to Open Science: An Annotated Reading List (Crüwell et al., 2019) Many hands make tight work (Silberzahn \u0026amp; Uhlmann, 2015) Promoting an open research culture (Nosek et al., 2015) Promoting Transparency in Social Science Research (Miguel et al., 2014) Psychologists Are Open to Change, yet Wary of Rules (Fuchs et al., 2012) Registered Reports: Realigning incentives in scientific publishing (Chambers et al., 2015) Registered Reports: A new publishing initiative at Cortex (Chambers, 2013) Registered Reports: A step change in scientific publishing (Chambers, 2014) Registered reports: a method to increase the credibility of published results (Nosek \u0026amp; Lakens, 2014) Registered reports (Jamieson et al., 2019) Attitudes Toward Open Science and Public Data Sharing: A Survey Among Members of the German Psychological Society (Abele-Brehm et al., 2019) Willingness to Share Research Data Is Related to the Strength of the Evidence and the Quality of Reporting of Statistical Results (Wicherts et al., 2011) Constraints on Generality (COG): A Proposed Addition to All Empirical Papers (Simons et al., 2017) Most people are not WEIRD (Henrich et al., 2010) How scientists can stop fooling themselves (Bishop, 2020b) CJEP Will Offer Open Science Badges (Pexman, 2017) Badges to Acknowledge Open Practices: A Simple, Low-Cost, Effective Method for Increasing Transparency (Kidwell et al., 2016) Trust Your Science? Open Your Data and Code (Stodden, 2011)◈ Publishing Research With Undergraduate Students via Replication Work: The Collaborative Replications and Education Project (CREP; Wagge et al., 2019) Is science really facing a reproducibility crisis, and do we need it to? (Fanelli, 2018) Fallibility in Science: Responding to Errors in the Work of Oneself and Others (Bishop, 2018) A consensus-based transparency checklist (Aczel et al., 2020) Tell it like it is (Anon, 2020) Is pre-registration worthwhile? (Szollosi et al., 2020) Signalling the trustworthiness of science (Jamieson et al., 2020) Scientific inbreeding and same-team replication: Type D personality as an example (Ioannidis, 2012) Speaker Introductions at Internal Medicine Grand Rounds: Forms of Address Reveal Gender Bias (Files et al., 2017)⌺ Main Takeaways: The authors hypothesize that female speakers in this professional setting are more often addressed by first name or equivalent than their male counterparts during speaker introductions. The authors examined the association between gender and address practices used during formal introductions of speakers in Internal Medicine Grand Rounds (IMGR). Method: 134 unique grand rounds presentations listed in a video archive library were accessed and reviewed. Of the 124 grand rounds reviewed, 83 had more than 1 introduction (introducer introducing the speaker) with each introduction representing an opportunity for the introducer to utilize the appropriate professional title. Each grand round had between one and five introducers. The authors analyzed the form of address used in up to 5 speaker introductions in each grand round. Results: Female introducers were more likely to use professional titles when introducing any speaker during the first form of address compared with male introducers. Female dyads utilized formal titles during the first form of address compared with male dyads who utilized a formal title 72.4% of the time. In mixed-gender dyads, where the introducer was female and speaker male, formal titles were used 95.0% of the time. Male introducers of female speakers utilized professional titles 49.2% of the time. In this study, women introduced by men at IMGR were less likely to be addressed by their professional title than were men introduced by men. In contrast, women introducers were more formal in both same- and mixed-gender interactions. The findings demonstrate that female introducers compared with male introducers were more likely to use professional titles when introducing any speaker, male or female, during the first form of address. However, there were striking differences in how males utilized their informal introduction style depending on whether the speaker was a man or woman. While women consistently and nearly universally introduced both male and female speakers by their formal titles during first form of address, men used male’s formal title during introductions 72% of the time, whereas acknowledging female speakers with their professional title only 49.2% (31/63) of the time. Female introducers with their high utilization of formal title during the first form of address exhibited no change in their utilization of formal title. When all introductions by men were included, the rate of utilization of professional titles increased slightly, but a gender difference remained. Despite multiple opportunities to acknowledge the speakers’ credentials, the title of Dr. was withheld by male introducers from 41.3% of female speakers compared with only 24.3% of male speakers. This study supports what many female physicians have experienced and discussed informally; the withholding of their professional titles when they are referenced or addressed by their male colleagues. Perhaps this is made more noticeable by their finding that women use formal titles close to 100% of the time for both the men and the women they introduce. This formal practice by women may engender an expectation of reciprocity, thus, further amplifying the disparity. While the did find that men are less formal overall and do withhold the professional title of Dr. during the first form of address from over one quarter of male speakers, it is important to view the experience from the perspective of the female speaker. As she prepares to assume the podium for her formal presentation, she will hear her formal title from almost all of the female introducers; however, she has less than a 50% likelihood that a male introducer will set the tone in the first form of address by calling her ‘‘Doctor.’ The significance of these linguistic biases lies in the fact that they implicitly communicate stereotypes to the individual, in this case women in medicine, and thereby contribute to the transmission and maintenance of socially shared stereotypes which ultimately have the potential to affect both the recipient and the audience. Overt discrimination is usually obvious and well recognized by those experiencing it, whereas more subtle forms of gender bias are difficult to describe, explain, and to address especially when inflicted upon an individual who may feel unsafe to address the practice as it occurs. Furthermore, unrecognized aspects of an organization’s culture may have different effects on men and women. It is the authors’ hope that objective documentation of the gender disparity identified in speaker introductions at IMGR will provide validation to women who have experienced it. Abstract Background: Gender bias has been identified as one of the drivers of gender disparity in academic medicine. Bias may be reinforced by gender subordinating language or differential use of formality in forms of address. Professional titles may influence the perceived expertise and authority of the referenced individual. The objective of this study is to examine how professional titles were used in the same and mixed-gender speaker introductions at Internal Medicine Grand Rounds (IMGR).Methods: A retrospective observational study of video-archived speaker introductions at consecutive IMGR was conducted at two different locations (Arizona, Minnesota) of an academic medical center. Introducers and speakers at IMGR were physician and scientist peers holding MD, PhD, or MD/PhD degrees. The primary outcome was whether or not a speaker’s professional title was used during the first form of address during speaker introductions at IMGR. As secondary outcomes, we evaluated whether or not the speakers professional title was used in any form of address during the introduction.Results: Three hundred twenty-one forms of address were analyzed. Female introducers were more likely to use professional titles when introducing any speaker during the first form of address compared with male introducers (96.2% [102/106] vs. 65.6% [141/215]; p \u0026lt; 0.001). Female dyads utilized formal titles during the first form of address 97.8% (45/46) compared with male dyads who utilized a formal title 72.4% (110/152) of the time ( p = 0.007). In mixed-gender dyads, where the introducer was female and speaker male, formal titles were used 95.0% (57/60) of the time. Male introducers of female speakers utilized professional titles 49.2% (31/63) of the time ( p \u0026lt; 0.001).Conclusion: In this study, women introduced by men at IMGR were less likely to be addressed by professional title than were men introduced by men. Differential formality in speaker introductions may amplify isolation, marginalization, and professional discomfiture expressed by women faculty in academic medicine. APA Style Reference Files, J. A., Mayer, A. P., Ko, M. G., Friedrich, P., Jenkins, M., Bryan, M. J., ... \u0026amp; Duston, T. (2017). Speaker introductions at internal medicine grand rounds: forms of address reveal gender bias. Journal of women\u0026#39;s health, 26(5), 413-419. https://doi.org/10.1089/jwh.2016.6044\nYou may also be interested in Unequal effects of the COVID-19 pandemic on scientists (Myers et al., 2019) Against Eminence (Vazire, 2017) Scientific Eminence: Where Are the Women? (Eagly \u0026amp; Miller, 2016) #bropenscience is broken science (Whitaker \u0026amp; Guest, 2020) ⌺ Examining Gender Bias in Student Evaluations of Teaching for Graduate Teaching Assistants (Khazan et al., 2019) ⌺ Science faculty’s subtle gender biases favour male students (Moss-Racusin et al., 2012) ⌺ Redesign open science for Asia, Africa and Latin America (Onie, 2020)⌺ Main Takeaways: Research is relatively new in many countries in Asia, Africa and Latin America. Across these regions, young scientists are working to build practices for open science from the ground up. The aim is that scientific communities will incorporate these principles as they grow. But these communities’ needs differ from those that are part of mature research systems. So, rather than shifting and shaping established systems, scientists are endeavouring to design new ones. Financial and career incentives to publish (or disadvantages from not publishing) are common government policies in countries such as Indonesia, China and Brazil where research culture is still being shaped. They aim to increase publication quantity to ‘catch up’ with other countries, but inadvertently encourage poor research practices. Lower-income countries cannot waste resources on funding untrustworthy research. Policies should therefore be designed to improve transparency, relevance and scientific rigour, rather than just to increase output — especially if governments want to use research to inform decision-making. Governments must also provide the training, resources and motivation needed for people to take these changes to heart. Crucially, the team will include researchers from many different types of university, not just the largest ones. Going forward, the institute will monitor whether the repository improves research quality. Other countries will face different issues. But a commonality will be that all stakeholders — not just the rich or prestigious ones — should be involved in finding a solution. Most universities in Asia, Africa and Latin America were set up for education. Many are ill-equipped to perform research and lack the proper infrastructure. Sustainable changes require education. Universities should train researchers not just in field-specific theories, but also in how to improve scientific practice. This training should cover the pitfalls of modern academia (e.g. prestige and academic metrics) have contributed to publication bias. It must address the consequences of succumbing to these pressures for the quality, replicability and trustworthiness of research. And it should honestly highlight disagreements about whether and when these practices actually work — debates about when pre-registration of research is and is not useful, for instance. And researchers must learn about these topics as they begin their research careers, even as undergraduates, rather having to modify existing practices later. Training in good scientific practices will set scientists up to think more critically and to adopt practices that increase the credibility of their work. Training will also enable researchers to add their diverse voices to continuing debates about open science, including active consideration of how science can benefit society, locally and globally. This shift towards open research might require a reworking of the overall training package, reducing the number of field-specific courses to avoid an overwhelming workload. Journals should take an active role in reducing under-representation, without compromising rigour. Ask authors to explicitly describe the populations they study upfront, and not to generalize their findings beyond this sample without good reason. Open reviews could reduce potential bias against samples from outside Western countries. Established journals should make efforts to communicate their standards to scientists in developing research cultures, and could also host special issues focused on understanding under-represented populations. A lack of funding and travel restrictions in many parts of Asia, Africa and Latin America reduce these opportunities for international collaboration, networking and travel, leading to researchers becoming more isolated. Such problems need to be acknowledged explicitly and confronted. Metrics and policies should be in place only if they are useful to science’s goal: knowledge accumulation for the greater societal good. Constant monitoring and introspection are therefore essential. Some of the best initiatives to improve science today might not be relevant in the future. Quote “If young research cultures can guard against harmful practices becoming ingrained, they have the opportunity to lay down a new type of strategy for open research. This could avoid the pressures that can sometimes warp research in the Western world and ultimately produce work that is credible and beneficial to society. The goal is not to replicate what is done in North America, Europe and Australia — rather, it is to do better.” (p.37) Abstract Researchers in many countries need custom-built systems to do robust and transparent science. APA Style Reference Onie, S.(2020). Redesign open science for Asia, Africa and Latin America. Nature, 587, 35-37. https://doi.org/10.1038/d41586-020-03052-3\nYou may also be interested in #bropenscience is broken science (Whitaker \u0026amp; Guest, 2020) ⌺ ls There a Positive Correlation between Socioeconomic Status and Academic Achievement? (Quagliata, 2008) Education and Socio-economic status (APA, 2017b) Ethnic and Racial minorities and socio-economic status (APA, 2017) Women and Socio-economic status (APA, 2010) Disability and Socio-economic status (APA, 2010) Lesbian, Gay, Bisexual and Transgender Persons \u0026amp; Socioeconomic Status (APA, 2010) The Relation Between Family Socioeconomic Status and Academic Achievement in China: A Meta-analysis (Liu et al., 2020) Socioeconomic Status and Academic Outcomes in Developing Countries: A Meta-Analysis (Kim et al., 2019) Is science only for the rich? (Lee, 2016) ⌺ Main Takeaways: Abstract APA Style Reference You may also be interested in Publishing Research With Undergraduate Students via Replication Work: The Collaborative Replications and Education Project (CREP; Wagge et al., 2019) Main Takeaways: The Collaborative Replications and Education Project (CREP) allows undergraduates to participate in high-quality direct replication, using existing resources and by providing structure for research projects. CREP samples seminal papers in 9 sub-disciplines published 3 years prior to the current year. Alumni students can then rate papers based on time and level of interest. CREP teaches good scientific practices utilizing direct replications and open science methods. CREP informs the original authors of the study selections and asks for materials and guidance for replication. The skills acquired from CREP can be applied to non-academic careers. For instance, teaching students the ability to evaluate scientific claims. CREP provides a forum and a community: for replication results to be presented and the institutionalization of replications, thereby contributing to science. Students are invited to contribute to authorship, even if they do not involve lead authorship roles. CREP deems that unaided, most student projects are not adequately powered for publication, thus do not lead to publication. Working with CREP allows students to replicate/not replicate a seminal finding but also to provide them a publication. Quote “CREP offers a supportive entry point for faculty…new to open science and large-scale collaboration…helps with fidelity and quality checks…eliminates need for instructors to vet every hypothesis and design for student research projects…not be experts in a topic…do not need to learn new programs…documentable experience blending teaching, scholarship, and close mentoring.” (p. 4). Abstract The Collaborative Replications and Education Project (CREP; http://osf.io/wfc6u) is a framework for undergraduate students to participate in the production of high-quality direct replications. Staffed by volunteers (including the seven authors of this paper) and incorporated into coursework, CREP helps produce high-quality data using existing resources and provides structure for research projects from conceptualization to dissemination. Most notably, student research generated through CREP make an impact: data from these projects are available for meta-analyses, some of which are published with student authors. APA Style Reference Wagge, J. R., Brandt, M. J., Lazarevic, L. B., Legate, N., Christopherson, C., Wiggins, B., \u0026amp; Grahe, J. E. (2019). Publishing research with undergraduate students via replication work: The collaborative replications and education project. Frontiers in psychology, 10, 247. https://doi.org/10.3389/fpsyg.2019.00247\nYou may also be interested in Is science really facing a reproducibility crisis, and do we need it to? (Fanelli, 2018) Many hands make tight work(Silberzahn \u0026amp; Uhlmann, 2015) Seven Steps Toward Transparency and Replicability in Psychological Science (Lindsay, 2020) CJEP Will Offer Open Science Badges (Pexman, 2017) Main Takeaways: This article introduces three badges: open data, open material, and pre-registration badges to the Canadian Journal of Experimental Psychology. Open data badge: the data is digitally shareable and made publicly available to reproduce results. Open materials badge: all materials that are necessary to reproduce reported results are digitally shareable with descriptions of non-digital materials being provided in order to replicate the study. Pre-registration badge: researchers provide an analysis plan that includes a planned sample size, motivated research questions or hypotheses, outcome and predictor variables, including controls, covariates and independent variables. Results must be fully disclosed and distinguished from other results that were not pre-registered. Pre-register \u0026#43; analysis: design a pre-register study with an analysis plan for research and the results are recorded according to the pre-registered plan. Quote “Indeed, in most cases, authors who wish to apply for badges will do so only after the editorial decision has been made. I understand that there are many reasons why it may not be possible to share data or materials, or to preregister a study, and so I certainly do not expect all authors to apply for badges. Nonetheless, I hope that many authors will devote the time required to make their data, materials, or research plans publicly available; these efforts are an important step toward improving our science.” (p.1). Abstract This is a view on open science badges in the Canadian Journal of Psychology by Professor Penny Pexman. It describes the badges and their importance to open science. The badges are used as a mechanism to state that the author is following good research practices. APA Style Reference Pexman, P. M. (2017). CJEP will offer open science badges. Canadian Journal of Experimental Psychology= Revue Canadienne de Psychologie Experimentale, 71(1), 1-1. https://doi.org/10.1037/cep0000128\nYou may also be interested in Trust Your Science? Open Your Data and Code (Stodden, 2011) Willingness to Share Research Data Is Related to the Strength of the Evidence and the Quality of Reporting of Statistical Results (Wicherts et al., 2011) Psychologists Are Open to Change, yet Wary of Rules (Fuchs et al., 2012) Badges to Acknowledge Open Practices: A Simple, Low-Cost, Effective Method for Increasing Transparency (Kidwell et al., 2016) Quality Uncertainty Erodes Trust in Science (Vazire, 2017) Seven Steps Toward Transparency and Replicability in Psychological Science (Lindsay, 2020) Signalling the trustworthiness of science (Jamieson et al., 2020) Badges to Acknowledge Open Practices: A Simple, Low-Cost, Effective Method for Increasing Transparency (Kidwell et al., 2016) Main Takeaways: Researchers are not likely to share data and materials unless there are incentives such as badges. The inclusion of badges states that the journal values transparency and that the author has met the transparency standards for research by signalling to the reader that they have provided accessible data, materials, or pre-registered their study. The present study investigated the influence of adopting badges by comparing data and material sharing rates before badges were adopted (i.e. 2012-2013) and after badges were adopted (2014-May 2015) in Psychological Science. Method: “We used the population of empirical articles with studies based on experiment or observation (N = 2,478) published in 2012, 2013, 2014, and January through May 2015 issues of one journal that started awarding badges” (p.3). Variables included were open data or open material badge, availability statement of data and material, and whether data or materials are available at a publicly accessible location. Results: There was an increase in the reporting of open data after badges were introduced. However, reporting openness does not guarantee openness. When badges are earned, available data is provided, correct, usable and complete than when it was not earned. Results: Open materials increased but not to the same extent. Psychological science adopts badges, report sharing rates increases 10-fold to 40%. Without badges – a small percentage of reported sharing is a gross exaggeration of sharing. Sharing data was larger when a badge was earned than when it was not earned. Effects on sharing research materials were similar to sharing data but weaker with badges producing only three times more sharing. Quote “However, actual evidence suggests that this very simple intervention is sufficient to overcome some barriers to sharing data and materials. Badges signal a valued behavior, and the specifications for earning the badges offer simple guides for enacting that behavior. Moreover, the mere fact that the journal engages authors with the possibility of promoting transparency by earning a badge may spur authors to act on their scientific values. Whatever the mechanism, the present results suggest that offering badges can increase sharing by up to an order of magnitude or more. With high return coupled with comparatively little cost, risk, or bureaucratic requirements, what’s not to like?” (p.13). Abstract Beginning January 2014, Psychological Science gave authors the opportunity to signal open data and materials if they qualified for badges that accompanied published articles. Before badges, less than 3% of Psychological Science articles reported open data. After badges, 23% reported open data, with an accelerating trend; 39% reported open data in the first half of 2015, an increase of more than an order of magnitude from baseline. There was no change over time in the low rates of data sharing among comparison journals. Moreover, reporting openness does not guarantee openness. When badges were earned, reportedly available data were more likely to be actually available, correct, usable, and complete than when badges were not earned. Open materials also increased to a weaker degree, and there was more variability among comparison journals. Badges are simple, effective signals to promote open practices and improve preservation of data and materials by using independent repositories. APA Style Reference Kidwell, M. C., Lazarević, L. B., Baranski, E., Hardwicke, T. E., Piechowski, S., Falkenberg, L. S., ... \u0026amp; Errington, T. M. (2016). Badges to acknowledge open practices: A simple, low-cost, effective method for increasing transparency. PLoS biology, 14(5), e1002456. https://doi.org/10.1371/journal.pbio.1002456\nYou may also be interested in Trust Your Science? Open Your Data and Code (Stodden, 2011) Willingness to Share Research Data Is Related to the Strength of the Evidence and the Quality of Reporting of Statistical Results (Wicherts et al., 2011) Psychologists Are Open to Change, yet Wary of Rules (Fuchs et al., 2012) CJEP Will Offer Open Science Badges (Pexman, 2017) Only Human: Scientists, Systems, and Suspect Statistics A review of: Improving Scientific Practice: Dealing With The Human Factors, University of Amsterdam, Amsterdam, September 11, 2014 (Hardwicke et al., 2014) Seven Steps Toward Transparency and Replicability in Psychological Science (Lindsay, 2020) Signalling the trustworthiness of science (Jamieson et al., 2020) Signalling the trustworthiness of science should not be a substitute for direct action against research misconduct (Kornfeld \u0026amp; Titus, 2020) Main Takeaways: Truth is undermined by misconduct, fraud, failure to replicate, rise in the number of retractions, and the public media. Fraudulent behaviour does not decrease the trust in science. Fraudulent behaviour is a result of the fraudulent scientist, not untrustworthy science. Reports indicate failure to publish will prevent academic appointment, tenure and ensuring funding of laboratories as the main concerns. Educating the public about the high standards of science and scientists will not reduce the outrage concerning fraudulent research. Quote “When then will these leaders of the scientific community finally direct their talents and energy to the culprit per se, research misconduct, and its perpetrators” (p.41). Abstract This is a response to the paper by Jamieson et al. (2019) on signalling trustworthiness in science. It contains information that the trust in science from the public and scientific community contributes to misconduct and fraudulent behaviour. APA Style Reference Kornfeld, D. S., \u0026amp; Titus, S. L. (2020). Signaling the trustworthiness of science should not be a substitute for direct action against research misconduct. Proceedings of the National Academy of Sciences of the United States of America, 117(1), 41. https://doi.org/10.1073/pnas.1917490116\nYou may also be interested in Reply to Kornfeld and Titus: No distraction from misconduct (Jamieson et al., 2020) Stop ignoring misconduct (Kornfeld \u0026amp; Titus, 2016) Fallibility in Science: Responding to Errors in the Work of Oneself and Others (Bishop, 2018) Publication Pressure and Scientific Misconduct in Medical Scientists (Tijdink et al., 2014) Signalling the trustworthiness of science (Jamieson et al., 2020) Check for publication integrity before misconduct (Grey et al., 2020) Reply to Kornfeld and Titus: No distraction from misconduct (Jamieson et al., 2020) Main Takeaways: Funders should make research ethics a condition of support. Institutions should provide education and investigate misconduct fairly, quickly and transparently, while protecting whistle-blowers. Journals should act quickly to correct the record. Scientists and outlets that publish their work should not only honor science’s integrity-protecting norms but also clearly signal when, and how, they have done so (e.g. statistical checks, plagiarism checks, badges, checklists). These aforementioned methods should uncover and increase awareness of biases that undermine the ability to fairly interpret the authors’ findings. “These indicators of trustworthiness clearly signal that the scientific community is safeguarding science’s norms and institutionalizing practices that protect its integrity as a way of knowing.” (p.42). Abstract This is a response to the commentary by Kornfeld and Titus (2020). It contains information about the importance of research ethics for funders, how institutions should protect whistleblowers and provide education to prevent misconduct and how scientists and outlets can provide evidence they honour scientific integrity. APA Style Reference Jamieson, K. H., McNutt, M., Kiermer, V., \u0026amp; Sever, R. (2020). Reply to Kornfeld and Titus: No distraction from misconduct. Proceedings of the National Academy of Sciences of the United States of America, 117(1), 42. https://doi.org/10.1073/pnas.1918001116\nYou may also be interested in Signalling the trustworthiness of science should not be a substitute for direct action against research misconduct (Kornfeld \u0026amp; Titus, 2020) Stop ignoring misconduct (Kornfeld \u0026amp; Titus, 2016) Fallibility in Science: Responding to Errors in the Work of Oneself and Others (Bishop, 2018) Publication Pressure and Scientific Misconduct in Medical Scientists (Tijdink et al., 2014) Signalling the trustworthiness of science (Jamieson et al., 2020) Check for publication integrity before misconduct (Grey et al., 2020) Stop ignoring misconduct (Kornfeld \u0026amp; Titus, 2016) Main Takeaways: The history of science shows irreproducibility is not a product of our times. These problems result from inadequate research practices and fraud. Current initiatives to improve science ignores fraudulent behaviour. Reducing irreproducibility is a wasted opportunity, if dishonesty is not given much attention. These unethical practices occurred long before people entered science. We need to consider reasons for misconduct: some researchers are perfectionists and are unable to cope with failure. Funders should craft policies to ensure mentors are advisers, teachers, and role models, while limiting the number of trainees per mentor by discipline. Established scientists are less likely to commit misconduct if they were more concerned about being detected and punished. Whistle-blowers need to come forward and be protected. One method is to provide research integrity officers in the university who will protect them from retaliation. Research funds should be given only when current certification about research integrity and honesty is provided by the institution. If misconduct occurs, institutions that fail to establish and execute these policies to assure integrity, will be held accountable. Quote “Government officials should be prepared to pursue repayments. The threat of such penalties should have a chilling effect on investigators contemplating research misconduct, and motivate institutions to establish and implement policies that reflect their commitment to institutional integrity.” (p.30) Abstract This is an editorial by Kornfeld and Titus (2016) who discusses that misconduct needs to be taken seriously and discussed. It contains solutions to resolve matters concerning research integrity for both the scientist and research institute. APA Style Reference Kornfeld, D. S., \u0026amp; Titus, S. L. (2016). Stop ignoring misconduct. Nature, 537(7618), 29-30.https://doi.org/10.1038/537029a\nYou may also be interested in Signalling the trustworthiness of science should not be a substitute for direct action against research misconduct (Kornfeld \u0026amp; Titus, 2020) Reply to Kornfeld and Titus: No distraction from misconduct (Jamieson et al., 2020) Fallibility in Science: Responding to Errors in the Work of Oneself and Others (Bishop, 2018) Publication Pressure and Scientific Misconduct in Medical Scientists (Tijdink et al., 2014) Check for publication integrity before misconduct (Grey et al., 2020) The Statistical Crisis in Science (Gelman \u0026amp; Loken, 2014) Main Takeaways: Authors argue that there is a statistical crisis in science because results are data-dependent, meaning that analytical decisions — a \u0026#34;garden of forking paths\u0026#34; — are so impactful that it can potentially produce different results depending on researchers\u0026#39; decisions. Authors explain that p-value is short for probability value. It is the probability of obtaining an effect that is at least as extreme as the one you found in your sample - assuming the null hypothesis is true. Gelman and Loken define it as: “a way of measuring the extent to which a data set provides evidence against a so-called null hypothesis.” Some ‘common’ practices (e.g., creating rules for data exclusion, for example) can flip analyses from non-significant to significant (and vice-versa). Such practices are particularly problematic when effect sizes or sample sizes are small, or when there are substantial measurement errors and variability. The ‘garden of forking paths’, or researcher degrees of freedom, or data-dependent results, highlights that multiple comparisons can be a problem, even when there is no “fishing expedition” or “p-hacking” and the research hypothesis was posited ahead of time. This is because different choices about combining variables, inclusion and exclusion of cases, transformations of variables, tests for interactions in absence of main effects, and other steps, could occur with different data (depending on these decisions, which are often implicit, and unreported). The Way Forward? Researchers should be made aware of choices involved in data analysis (pre-registration is practical but cannot be a general solution). Make a sharper distinction between exploratory and confirmatory data analysis, recognizing the benefits and limitations of each. Hence, researchers can perform two experiments: exploratory and confirmatory with its own pre-registered protocol. Authors also argue that statistically significant p-values shouldn’t be taken at face value even if linked to comparison consistent with existing theory. Researchers need to be aware of data dredging (the misuse of data analysis to find patterns in data that can be presented as statistically significant which dramatically increase the risk of false positives) and using both confidence intervals and p-values to avoid getting fooled by noise. At the aggregate level, as the vast majority of papers are not published in high-impact journals without a significant p \u0026lt; .05 result (i.e., most journals, and the academic system in general, value ‘novel’ positive results rather than replications or pointing out mistakes in published literature), data-dependency results may be widespread. This is also known as perverse incentives. Abstract Data-dependent analysis— a \u0026#34;garden of forking paths\u0026#34; — explains why many statistically significant comparisons don\u0026#39;t hold up. APA Style Reference Gelman, A., \u0026amp; Loken, E. (2014). The statistical crisis in science: data-dependent analysis--a\u0026#34; garden of forking paths\u0026#34;--explains why many statistically significant comparisons don\u0026#39;t hold up. American scientist, 102(6), 460-466. [gated, ungated]\nYou may also be interested in How scientists can stop fooling themselves over statistics (Bishop, 2020b) Only Human: Scientists, Systems, and Suspect Statistics A review of: Improving Scientific Practice: Dealing With The Human Factors, University of Amsterdam, Amsterdam, September 11, 2014 (Hardwicke et al., 2014) False-Positive Psychology: Undisclosed Flexibility in Data Collection and Analysis Allows Presenting Anything as Significant (Simmons et al., 2011) Seven Steps Toward Transparency and Replicability in Psychological Science (Lindsay, 2020) The life of p: “Just significant” results are on the rise (Leggett et al., 2013) Many Analysts, One Data Set: Making Transparent How Variations in Analytical Choices Affect Results (Silberzahn et al., 2019) Scientific inbreeding and same-team replication: Type D personality as an example (Ioannidis, 2012) Only Human: Scientists, Systems, and Suspect Statistics A review of: Improving Scientific Practice: Dealing With The Human Factors, University of Amsterdam, Amsterdam, September 11, 2014 (Hardwicke et al., 2014) Main Takeaways: The h-index is used to evaluate scientists for hiring, promoting and funding decisions. This metric is affected by the number of publications, citations, and productivity. But papers can be cited in critiques, due to faults in methodology or failures to replicate. Does this mean citations are a good measure? No! (cf. Goodhart’s Law). Academia provides short-term contracts to exploit without wasting resources. There is fierce competition for limited funding. Publications aim for newsworthy results, leading to false positives and less integration with the literature. The pressure for positive findings can lead to unethical behaviour. Questionable research practices are seen as unethical as it distorts data to support the researchers’ hypotheses, either intentionally or unintentionally. There is too much faith that scientists will self-correct. Scientists need to be open about their results. Many scientists subscribe to the norm of communality (common ownership of scientific results and methods). There is some data sharing but, most scientists don’t. Scientists are assumed to self-regulate, but this assumption is erroneous. Incentives need to change and focus on quality, reproducibility, data sharing, and impact on society. Pre-registration can help with publication biases and questionable research practice. A study should be published irrespective of findings. Criticism of pre-registration is that workload will increase; evaluation of methodology and data collection to evaluate adherence to pre-registration plan. It is argued that pre-registration would save time in preventing manuscripts being rejected based on methodological issues or null results. Pre-registration could backfire, as editors may require revisions to protocols, study is complete and changes may be impossible. Pre-registration may address integrity issues before and during data collection. Need to change the culture so scientists don’t need to prioritise their own research over scientific inquiry or credibility. Quote “The success of science is often attributed to its objectivity: surely science is an impartial, transparent, and dispassionate method for obtaining the truth? In fact, there is growing concern that several aspects of typical scientific practice conflict with these principles and that the integrity of the scientific enterprise has been deeply compromised.” (p.1) Abstract It is becoming increasingly clear that science has sailed into troubled waters. Recent revelations about cases of serious research fraud and widespread ‘questionable research practices’ have initiated a period of critical self-reflection in the scientific community and there is growing concern that several common research practices fall far short of the principles of robust scientific inquiry. At a recent symposium, ‘Improving Scientific Practice: Dealing with the Human Factors’ held at The University of Amsterdam, the notion of the objective, infallible, and dispassionate scientist was firmly challenged. The symposium was guided by the acknowledgement that scientists are only human, and thus subject to the desires, needs, biases, and limitations inherent to the human condition. In this article, five post-graduate students from University College London describe the issues addressed at the symposium and evaluate proposed solutions to the scientific integrity crisis. APA Style Reference Hardwicke, T E et al 2014 Only Human: Scientists, Systems, and Suspect\nYou may also be interested in Trust Your Science? Open Your Data and Code (Stodden, 2011) Is science really facing a reproducibility crisis, and do we need it to? (Fanelli, 2018) Six principles for assessing scientists for hiring, promotion, and tenure (Naudet et al, 2018) The Nine Circles of Scientific Hell (Neuroskeptic, 2012) Don’t let transparency damage science (Lewandowsky \u0026amp; Bishop, 2016) Signalling the trustworthiness of science should not be a substitute for direct action against research misconduct (Kornfeld \u0026amp; Titus, 2020) Attitudes Toward Open Science and Public Data Sharing: A Survey Among Members of the German Psychological Society (Abele-Brehm et al., 2019) Willingness to Share Research Data Is Related to the Strength of the Evidence and the Quality of Reporting of Statistical Results (Wicherts et al., 2011) Open Data in Qualitative Research (Chauvette et al., 2019) How scientists can stop fooling themselves (Bishop, 2020b) Reply to Kornfeld and Titus: No distraction from misconduct (Jamieson et al., 2020) Stop ignoring misconduct (Kornfeld \u0026amp; Titus, 2016) The Statistical Crisis in Science (Gelman \u0026amp; Loken, 2014) False-Positive Psychology: Undisclosed Flexibility in Data Collection and Analysis Allows Presenting Anything as Significant (Simmons et al., 2011) Publication Prejudices: An Experimental Study of Confirmatory Bias in the Peer Review System (Mahoney, 1977) Rein in the four horsemen of irreproducibility (Bishop, 2019) Seven Easy Steps to Open Science: An Annotated Reading List (Crüwell et al., 2019) Fallibility in Science: Responding to Errors in the Work of Oneself and Others (Bishop, 2018) Seven Steps Toward Transparency and Replicability in Psychological Science (Lindsay, 2020) The life of p: “Just significant” results are on the rise (Leggett et al., 2013) Signalling the trustworthiness of science (Jamieson et al., 2020) Many Analysts, One Data Set: Making Transparent How Variations in Analytical Choices Affect Results (Silberzahn et al., 2019) Scientific inbreeding and same-team replication: Type D personality as an example (Ioannidis, 2012) False-Positive Psychology: Undisclosed Flexibility in Data Collection and Analysis Allows Presenting Anything as Significant (Simmons et al., 2011) Main Takeaways: Researchers are faced with many different decisions when analysing their data, such as whether to test more participants, how to exclude outliers, whether to use covariates or not. These “researcher degrees of freedom” can be exploited to run different variations of the analysis until a statistically significant result is found. The combination of different researcher degrees of freedom makes it increasingly more likely to find a false positive result (i.e., a statistical fluke). The authors use two real experiments and computer simulations to show how undisclosed flexibility in data analysis and the selective reporting of results makes it “unacceptably easy” to find significant results, even for hypotheses that are unlikely, or necessarily incorrect. It is recommended that authors should: 1) determine data collection rules in advance of running the experiment; 2) collect at least 20 observations per cell; 3) report all variables and experimental conditions (including failed manipulations); and 4) if outliers are removed or covariates are included, authors should show how these actions change the results. Reviewers should: 1) ensure that the rules for authors above are followed; 2) be tolerant towards imperfections of the study; 3) ask authors to show that the results don’t depend on specific analytical decisions; and 4) ask for direct replications when the authors’ justification is not compelling enough. While other solutions are possible, such as correcting the alpha level, using Bayesian statistics, doing conceptual replications, and posting data and materials online, the authors consider them to be less practical and effective. Abstract In this article, we accomplish two things. First, we show that despite empirical psychologists’ nominal endorsement of a low rate of false-positive findings (≤ .05), flexibility in data collection, analysis, and reporting dramatically increases actual false-positive rates. In many cases, a researcher is more likely to falsely find evidence that an effect exists than to correctly find evidence that it does not. We present computer simulations and a pair of actual experiments that demonstrate how unacceptably easy it is to accumulate (and report) statistically significant evidence for a false hypothesis. Second, we suggest a simple, low-cost, and straightforwardly effective disclosure-based solution to this problem. The solution involves six concrete requirements for authors and four guidelines for reviewers, all of which impose a minimal burden on the publication process. APA Style Reference Simmons, J. P., Nelson, L. D., \u0026amp; Simonsohn, U. (2011). False-positive psychology: Undisclosed flexibility in data collection and analysis allows presenting anything as significant. Psychological science, 22(11), 1359-1366. https://doi.org/10.1177/0956797611417632 [ungated]\nYou may also be interested in How scientists can stop fooling themselves (Bishop, 2020b) The Statistical Crisis in Science (Gelman \u0026amp; Loken, 2014) Only Human: Scientists, Systems, and Suspect Statistics A review of: Improving Scientific Practice: Dealing With The Human Factors, University of Amsterdam, Amsterdam, September 11, 2014 (Hardwicke et al., 2014) A 21 Word Solution (Simmons et al., 2012) Rein in the four horsemen of irreproducibility (Bishop, 2019) Seven Steps Toward Transparency and Replicability in Psychological Science (Lindsay, 2020) The life of p: “Just significant” results are on the rise (Leggett et al., 2013) Many Analysts, One Data Set: Making Transparent How Variations in Analytical Choices Affect Results (Silberzahn et al., 2019) Scientific inbreeding and same-team replication: Type D personality as an example (Ioannidis, 2012) Publication Prejudices: An Experimental Study of Confirmatory Bias in the Peer Review System (Mahoney, 1977) Main Takeaways: Cognitive bias (e.g. confirmation bias) is more prevalent in scientific publication. A piece of research is threatened by human decision making (i.e. the journal editor and reviewer). The present study investigated whether confirmation bias is a problem for current review practices and how we can reduce confirmatory bias To what extent do editors and referees weigh various components in evaluation? The ideal publication review system should focus on methodological quality and relevance, over data outcome and interpretation. Writing styles and conclusions can impact the decision made by the editor and peer reviewer. Method: five groups of referees read manuscripts that had data that was consistent or inconsistent with the reviewer’s theoretical perspective. Method con’t: Reviewers had to evaluate manuscripts based on relevance and methodology. Method con’t: two final groups of reviewers received mixed findings, supporting one perspective of the reviewer and one was contradictory to the reviewer’s perspective. Results: There was poor inter-rater reliability. Reviewers were more likely to show confirmation bias for manuscripts in favour of their theoretical perspective and were more severe for manuscripts that contradict their perspective. Referees should be asked to evaluate relevance and methodology of an experiment without seeing its results or interpretations (cf. registered reports). Referees show little agreement on topics, thus they should be trained to produce better and unprejudiced consensus. Peer review is perceived as an objective measure but ironically is prone to human biases. Abstract Confirmatory bias is the tendency to emphasize and believe experiences which support one\u0026#39;s views and to ignore or discredit those which do not. The effects of this tendency have been repeatedly documented in clinical research. However, its ramifications for the behavior of scientists have yet to be adequately explored. For example, although publication is a critical element in determining the contribution and impact of scientific findings~ little research attention has been devoted to the variables operative in journal review policies. In the present study, 75 journal reviewers were asked to referee manuscripts which described identical experimental procedures but which reported positive, negative, mixed, or no results. In addition to showing poor interrater agreement, reviewers were strongly biased against manuscripts which reported results contrary to their theoretical perspective. The implications of these findings for epistemology and the peer review system are briefly addressed. APA Style Reference Mahoney, M. J. (1977). Publication prejudices: An experimental study of confirmatory bias in the peer review system. Cognitive therapy and research, 1(2), 161-175. https://doi.org/10.1007/BF01173636\nYou may also be interested in How scientists can stop fooling themselves (Bishop, 2020b) Only Human: Scientists, Systems, and Suspect Statistics A review of: Improving Scientific Practice: Dealing With The Human Factors, University of Amsterdam, Amsterdam, September 11, 2014 (Hardwicke et al., 2014) Effect of open peer review on quality of reviews and on reviewers’ recommendations: a randomised trial (van Rooyen et al., 1999) The Peer Reviewers’ Openness Initiative: incentivising open research practices through peer review (Morey et al., 2016) Effect of open peer review on quality of reviews and on reviewers’ recommendations: a randomised trial (van Rooyen et al., 1999) Main Takeaways: The British Medical Journal wants to improve peer review. There was no evidence that investigated whether anonymous peer review is better than other forms of peer review. Open peer review (i.e. reviewer signing their review) was argued to lead to better reviews, thus increasing credibility and accountability. The article investigated whether the quality of reviews in open review was the same as traditional review. Method: When both reviews were received, the reviews and the manuscript were passed to a responsible editor who was asked to rate the quality of reviews, using a validated review quality instrument. A second editor was randomly chosen from the other 12 editors to measure review quality independently. Method con’t: The corresponding author of each manuscript was sent anonymous copies of the two reviews and was told a decision on the manuscript had not been reached. Method con’t: The corresponding author was asked to assess the quality of the review, using a review quality instrument. Results: Twelve percent of reviewers were more likely to decline to review, if they were to be identified, as opposed to being anonymous. Results: There was no difference between anonymous and identified reviewers in terms of quality of reviews, in recommendation of reviewers, or time taken to review the papers. Results: The editors’ quality scores for reviews was higher than that of the authors. There was no difference observed in terms of the quality of and time taken to produce the review of the manuscript. Authors rated reviews that recommended the publication of the manuscript higher than those reviews that recommended rejection of the manuscript. “Editors...did not seem to be influenced by a reviewer\u0026#39;s opinion of the merit of a paper when they assessed the quality of the review” (p.26) Abstract To examine the effect on peer review of asking reviewers to have their identity revealed to the authors of the paper. Randomised trial. Consecutive eligible papers were sent to two reviewers who were randomised to have their identity revealed to the authors or to remain anonymous. Editors and authors were blind to the intervention. The quality of the reviews was independently rated by two editors and the corresponding author using a validated instrument. Additional outcomes were the time taken to complete the review and the recommendation regarding publication. A questionnaire survey was undertaken of the authors of a cohort of manuscripts submitted for publication to find out their views on open peer review. Two editors\u0026#39; assessments were obtained for 113 out of 125 manuscripts, and the corresponding author\u0026#39;s assessment was obtained for 105. Reviewers randomised to be asked to be identified were 12% (95% confidence interval 0.2% to 24%) more likely to decline to review than reviewers randomised to remain anonymous (35% v 23%). There was no significant difference in quality (scored on a scale of 1 to 5) between anonymous reviewers (3.06 (SD 0.72)) and identified reviewers (3.09 (0.68)) (P = 0.68, 95% confidence interval for difference - 0.19 to 0.12), and no significant difference in the recommendation regarding publication or time taken to review the paper. The editors\u0026#39; quality score for reviews (3.05 (SD 0.70)) was significantly higher than that of authors (2.90 (0.87)) (P \u0026lt; 0.005, 95%confidence interval for difference - 0.26 to - 0.03). Most authors were in favour of open peer review. Asking reviewers to consent to being identified to the author had no important effect on the quality of the review, the recommendation regarding publication, or the time taken to review, but it significantly increased the likelihood of reviewers declining to review. APA Style Reference Van Rooyen, S., Godlee, F., Evans, S., Black, N., \u0026amp; Smith, R. (1999). Effect of open peer review on quality of reviews and on reviewers\u0026#39; recommendations: a randomised trial. Bmj, 318(7175), 23-27. https://doi.org/10.1136/bmj.318.7175.23\nYou may also be interested in Publication Prejudices: An Experimental Study of Confirmatory Bias in the Peer Review System (Mahoney, 1977) The Peer Reviewers’ Openness Initiative: incentivising open research practices through peer review (Morey et al., 2016) Early co-authorship with top scientist predicts success in academic careers (Li et al., 2019) Main Takeaways: Academic impact is complex and linked to citation number. The output of junior researchers can gain a competitive advantage based on visibility. The present study asks whether a single event of interaction with ‘top scientists’ may alter junior researchers futures in academia. Hypothesis: the more co-authorship with ‘top scientists’, the more junior researchers have competitive advantage. Method: publication and citation data for four disciplines was indexed, since 1970 of selected journals for specific authors and institutions. A paper’s prestige score is: the average prestige score of its authors’ institution (\u0026#43;) the average prestige score of the researchers’ papers. Results: co-author with top scientists provide competitive advantage compared to peers of comparable early career profiles without top co-authors. Authors seem to suggest that students from less prestigious institutions would benefit junior researchers most. Discussion: Abstract We examined the long-term impact of coauthorship with established, highly-cited scientists on the careers of junior researchers in four scientific disciplines. Here, using matched pair analysis, we find that junior researchers who coauthor work with top scientists enjoy a persistent competitive advantage throughout the rest of their careers, compared to peers with similar early career profiles but without top coauthors. Such early coauthorship predicts a higher probability of repeatedly coauthoring work with top-cited scientists, and, ultimately, a higher probability of becoming one. Junior researchers affiliated with less prestigious institutions show the most benefits from coauthorship with a top scientist. As a consequence, we argue that such institutions may hold vast amounts of untapped potential, which may be realised by improving access to top scientists. APA Style Reference Li, W., Aste, T., Caccioli, F., \u0026amp; Livan, G. (2019). Early coauthorship with top scientists predicts success in academic careers. Nature communications, 10(1), 1-9. https://doi.org/10.1038/s41467-019-13130-4 [ungated]\nYou may also be interested in Prestige drives epistemic inequality in the diffusion of scientific ideas (Morgan et al., 2018) Open Science Isn’t Always Open to All Scientists (Bahlai et al., 2019) The Matthew effect in science funding (Bol et al., 2018) The Peer Reviewers’ Openness Initiative: incentivising open research practices through peer review (Morey et al., 2016) Main Takeaways: Openness and transparency is crucial to science. “Scientific progress is accelerated as more data are available for verification, theory-building and meta-analysis, and experimental materials are available for easier replications and extension studies.” (p.2) Openness is an ethical obligation that provides further advantages and is being introduced as a policy change. It is not difficult to learn to be open, but implementing them may delay publications by a few days. The relationship between reviewers and authors is important for the scientific process, especially when there is a missing figure or statistical results which requires the author to clarify. The author must justify to the reviewer if the following is not included: link to the data, materials, a document with details on how to interpret any files, code, or explanation of how to run the software in the manuscript. If no real reason is given (e.g. legal, ethical or impracticality), reviewers should provide a short review for the lack of openness and failure to justify. This Peer Reviewers’ Openness Initiative will ease the review load, as the reviewer can reject the manuscript, if the openness requirement is not met. Open research is not a matter of policy, but a matter of scientific value and quality of product. Open practices are not standardised and are driven by practice. Authors that lack training in open practices and scientists need to learn new skills and knowledge. Senior researchers can help students curate data and research materials. Open data allows the reviewer the option to check the analysis. The initiative is targeted at reviewers, not action editors. Researchers who value open research practices should join the Initiative to help promote open research. Quote “As a group, reviewers share the power to ensure that articles meet minimum scientific quality standards.What is needed is an affirmation that those minimum scientific quality standards include open practices. By acknowledging that open practices should be considered by reviewers alongside other research norms, reviewers can collectively bring about a radical positive change in the culture of science.” (p.3) Abstract Openness is one of the central values of science. Open scientific practices such as sharing data, materials and analysis scripts alongside published articles have many benefits, including easier replication and extension studies, increased availability of data for theory-building and metaanalysis, and increased possibility of review and collaboration even after a paper has been published. Although modern information technology makes sharing easier than ever before, uptake of open practices had been slow. We suggest this might be in part due to a social dilemma arising from misaligned incentives and propose a specific, concrete mechanism—reviewers withholding comprehensive review—to achieve the goal of creating the expectation of open practices as a matter of scientific principle. APA Style Reference Morey, R. D., Chambers, C. D., Etchells, P. J., Harris, C. R., Hoekstra, R., Lakens, D., ... \u0026amp; Vanpaemel, W. (2016). The Peer Reviewers\u0026#39; Openness Initiative: incentivizing open research practices through peer review. Royal Society Open Science, 3(1), 150547. https://doi.org/10.1098/rsos.150547\nYou may also be interested in Publication Prejudices: An Experimental Study of Confirmatory Bias in the Peer Review System (Mahoney, 1977) Effect of open peer review on quality of reviews and on reviewers’ recommendations: a randomised trial (van Rooyen et al., 1999) A 21 Word Solution (Simmons et al., 2012)◈ Main Takeaways: The authors suggest a “21-word solution” to encourage researchers to disclose their analytical choices. If researchers did not engage in questionable research practices (e.g., dropping conditions/ variables, p-hacking, optional data stopping), they should explicitly say so in their paper. Researchers should say what their sample size was in advance, be transparent and disclose information that they did not drop any variables or conditions. We cannot trust our colleagues to run and report studies properly if some people believe it is okay to drop conditions and variables and others do not believe this is good scientific practice. Researchers shouldn’t wait for journals and other colleagues to catch up if they want to achieve transparency in science. Rather, they should take the initiative themselves. Scientific journals should ask researchers to disclose data collection and analysis decisions truthfully, but this doesn’t mean that they are responsible for policing researchers. The 21-word solution can be easily included in your manuscript, even if this is done in the Supplemental materials to reduce word count. The ‘red tape’ of this transparency statement is arguably negligible compared to the thousands of idiosyncratic rules in APA’s Publication Manual. False positives need to be scrutinised, as many p-hacking choices are encouraged. Reviewers and the readers should ask if this study is a 1 or 2 dependent variable study. Disclosure does not reduce p-hacking and does not reduce probability of false positives. Papers should include this proposed 21 words to improve its credibility. Quote “We report how we determined our sample size, all data exclusions (if any), all manipulations, and all measures in the study.” (p.1) Abstract APA Style Reference Simmons, Joseph P. and Nelson, Leif D. and Simonsohn, Uri, A 21 Word Solution (October 14, 2012). Available at SSRN: https://ssrn.com/abstract=2160588 or http://dx.doi.org/10.2139/ssrn.2160588\nYou may also be interested in False-Positive Psychology: Undisclosed Flexibility in Data Collection and Analysis Allows Presenting Anything as Significant (Simmons et al., 2011) Seven Steps Toward Transparency and Replicability in Psychological Science (Lindsay, 2020) The life of p: “Just significant” results are on the rise (Leggett et al., 2013) Many Analysts, One Data Set: Making Transparent How Variations in Analytical Choices Affect Results (Silberzahn et al., 2019) Quality Uncertainty Erodes Trust in Science (Vazire, 2017) Main Takeaways: Quality uncertainty threatens the confidence people have in the findings and to build on them. Vazire argues that the lack of transparency in science has led to quality uncertainty and threatened to erode trust in science. They argue that greater transparency in scientific reporting will reduce quality uncertainty and restore trust in science. In the scientific market, the source of quality uncertainty is that the authors know much more about what went into the articles than the potential consumers (e.g. the raw data, the original design and analysis plan, the exploratory analyses and final analysis). Put simply, the more information that is hidden, the larger the quality uncertainty. As a result of low levels of transparency in scientific publication, journal editors, reviewers and readers cannot differentiate between lemons and high-quality findings. Some, but not all, the information is presented in the method and results section. However, vital information is kept private (e.g. the raw data, the original design and analysis plan, the exploratory analyses and final analysis), preventing consumers of their manuscript being certain about its quality. The cost of lack of transparency will end up in building a science based on low-quality finding and shaky foundations, thus driving out rigorous science. When the low-quality findings do not stand up, it is too late, as high-quality research has been driven out. The motto of the Royal Society is to “take no one’s word”, as we cannot rely on a few experts to evaluate the findings of specific research and then ask everyone to completely trust the author. To reduce quality uncertainty, we must be transparent in order to make a judgment about the quality of the manuscript. Increased transparency will provide the consumers the information needed to detect many errors in the article, while making authors more accountable for their mistakes, thus encouraging further care for how their study is designed, analysed and written. This will not help consumers and researchers catch researchers who are willing to use fraudulent behaviours, but it will solve unintentional misrepresentations, thus rebuilding trust in science. When journals choose to maximise citation impacts, instead of producing reliable, robust and reproducible science, the consumer is being given shoddy, as opposed to reliable product, thus making journals neglect their duties. To make journals more accountable, we must tie their reputation not to impact factor, but the quality of their articles instead and their policies on open science and transparency. Quote “In any market, consumers must evaluate the quality of products and decide their willingness to pay based on their evaluation. In science, consumers of new scientific findings must likewise evaluate the strength of the findings and decide their willingness to put stock in them. In both kinds of markets, the inability to make informed and accurate evaluations of quality (i.e., quality uncertainty) leads to a lower and lower willingness to put stock in any product – a lack of trust in the market itself. When there are asymmetries in the information that the seller and the buyer have, the buyers cannot be certain about the quality of the products, leading to quality uncertainty.” (p.1). Abstract When consumers of science (readers and reviewers) lack relevant details about the study design, data, and analyses, they cannot adequately evaluate the strength of a scientific study. Lack of transparency is common in science, and is encouraged by journals that place more emphasis on the aesthetic appeal of a manuscript than the robustness of its scientific claims. In doing this, journals are implicitly encouraging authors to do whatever it takes to obtain eye-catching results. To achieve this, researchers can use common research practices that beautify results at the expense of the robustness of those results (e.g., p-hacking). The problem is not engaging in these practices, but failing to disclose them. A car whose carburetor is duct-taped to the rest of the car might work perfectly fine, but the buyer has a right to know about the duct-taping. Without high levels of transparency in scientific publications, consumers of scientific manuscripts are in a similar position as buyers of used cars – they cannot reliably tell the difference between lemons and high quality findings. This phenomenon – quality uncertainty – has been shown to erode trust in economic markets, such as the used car market. The same problem threatens to erode trust in science. The solution is to increase transparency and give consumers of scientific research the information they need to accurately evaluate research. Transparency would also encourage researchers to be more careful in how they conduct their studies and write up their results. To make this happen, we must tie journals’ reputations to their practices regarding transparency. Reviewers hold a great deal of power to make this happen, by demanding the transparency needed to rigorously evaluate scientific manuscripts. The public expects transparency from science, and appropriately so – we should be held to a higher standard than used car salespeople. APA Style Reference Vazire, S. (2017). Quality Uncertainty Erodes Trust in Science. Collabra: Psychology, 3(1), 1. DOI: http://doi.org/10.1525/collabra.74\nYou may also be interested in Trust Your Science? Open Your Data and Code (Stodden, 2011) Willingness to Share Research Data Is Related to the Strength of the Evidence and the Quality of Reporting of Statistical Results (Wicherts et al., 2011) Is science really facing a reproducibility crisis, and do we need it to? (Fanelli, 2018) Psychologists Are Open to Change, yet Wary of Rules (Fuchs et al., 2012) The Nine Circles of Scientific Hell (Neuroskeptic, 2012) Don’t let transparency damage science (Lewandowsky \u0026amp; Bishop, 2016) How scientists can stop fooling themselves (Bishop, 2020b) CJEP Will Offer Open Science Badges (Pexman, 2017) Badges to Acknowledge Open Practices: A Simple, Low-Cost, Effective Method for Increasing Transparency (Kidwell et al., 2016) Only Human: Scientists, Systems, and Suspect Statistics A review of: Improving Scientific Practice: Dealing With The Human Factors, University of Amsterdam, Amsterdam, September 11, 2014 (Hardwicke et al., 2014) False-Positive Psychology: Undisclosed Flexibility in Data Collection and Analysis Allows Presenting Anything as Significant (Simmons et al., 2011) Rein in the four horsemen of irreproducibility (Bishop, 2019) Seven Easy Steps to Open Science: An Annotated Reading List (Crüwell et al., 2019) Promoting an open research culture (Nosek et al., 2015) Promoting Transparency in Social Science Research (Miguel et al., 2014) Fallibility in Science: Responding to Errors in the Work of Oneself and Others (Bishop, 2018) Seven Steps Toward Transparency and Replicability in Psychological Science (Lindsay, 2020) Signalling the trustworthiness of science (Jamieson et al., 2020) Rein in the four horsemen of irreproducibility (Bishop, 2019) Main Takeaways: Publication bias, low statistical power, p-hacking, and HARKing (hypothesising after the results are known) are threats to research reproducibility that make it difficult to find meaningful results. Publication bias harms patients. The tendency to not publish negative results misleads readers and biases meta-analyses. Low statistical power also misleads readers- when the sample size is small, there is a low probability that one will detect an effect even if it exists. Time and resources are wasted on such underpowered studies. P-hacking occurs when researchers conduct many analyses, but report only those that are significant. HARKing is so wide-spread, that researchers may come to accept it as a good practice. Authors should be free to do exploratory analyses, but not when p-values are used outside of the context that was used to calculate them. These four problems are older than most of the junior researchers working on them. New developments may help combat these issues: Abstract Dorothy Bishop describes how threats to reproducibility, recognized but unaddressed for decades, might finally be brought under control. APA Style Reference Bishop, D. (2019). Rein in the four horsemen of irreproducibility. Nature, 568(7753), 435-436. http://doi.org/10.1038/d41586-019-01307-2\nYou may also be interested in Trust Your Science? Open Your Data and Code (Stodden, 2011) Willingness to Share Research Data Is Related to the Strength of the Evidence and the Quality of Reporting of Statistical Results (Wicherts et al., 2011) Is science really facing a reproducibility crisis, and do we need it to? (Fanelli, 2018) Psychologists Are Open to Change, yet Wary of Rules (Fuchs et al., 2012) The Nine Circles of Scientific Hell (Neuroskeptic, 2012) Don’t let transparency damage science (Lewandowsky \u0026amp; Bishop, 2016) How scientists can stop fooling themselves (Bishop, 2020b) Only Human: Scientists, Systems, and Suspect Statistics A review of: Improving Scientific Practice: Dealing With The Human Factors, University of Amsterdam, Amsterdam, September 11, 2014 (Hardwicke et al., 2014) False-Positive Psychology: Undisclosed Flexibility in Data Collection and Analysis Allows Presenting Anything as Significant (Simmons et al., 2011) Registered Reports: Realigning incentives in scientific publishing (Chambers et al., 2015) Registered Reports: A new publishing initiative at Cortex (Chambers, 2013) Registered Reports: A step change in scientific publishing (Chambers, 2014) Registered reports: a method to increase the credibility of published results (Nosek \u0026amp; Lakens, 2014) Registered reports (Jamieson et al., 2019) Quality Uncertainty Erodes Trust in Science (Vazire, 2017) Seven Easy Steps to Open Science: An Annotated Reading List (Crüwell et al., 2019) Promoting an open research culture (Nosek et al., 2015) Promoting Transparency in Social Science Research (Miguel et al., 2014) Fallibility in Science: Responding to Errors in the Work of Oneself and Others (Bishop, 2018) Seven Steps Toward Transparency and Replicability in Psychological Science (Lindsay, 2020) On the persistence of low power in psychological science (Vankov et al., 2014) The life of p: “Just significant” results are on the rise (Leggett et al., 2013) Many Analysts, One Data Set: Making Transparent How Variations in Analytical Choices Affect Results (Silberzahn et al., 2019) Scientific inbreeding and same-team replication: Type D personality as an example (Ioannidis, 2012) Seven Easy Steps to Open Science: An Annotated Reading List (Crüwell et al., 2019) Main Takeaways: Students and academics with little knowledge of open science may not easily find and make use of resources. Transparency and robustness may not guarantee increased rigour. Researchers should plan data collection and analysis, be aware of assumptions of statistical models and understanding of statistical tools. Credibility of scientific claims depend on replicability. Open access removes barriers to access and distributes research. The gold route refers to publicly available articles, while the green route relates to self-archiving or the works are made publicly available by people who created the manuscripts (e.g. preprints). Open access articles are cited between 36%-600% more than non-open access work. It is given more coverage and discussed more in non-scientific settings. Researchers need to consider how they share their data. Is it findable, accessible, interoperable and reusable (FAIR)? All steps of data analysis should be recorded in open source programs (e.g. R or Python) or placed in a reproducible syntax file. Pre-registration is an open science practice that: protects people from biases; encourages transparency about analytic decision-making; supports rigorous scientific research; enables more replicable and reproducible work. Open science increases confidence and replicability of scientific results. Direct replication duplicates the necessary elements in order to assess whether the original findings are reproducible, whereas conceptual replication changes one component of the original procedure such as sample or measure to measure whether the original results are reproducible. Quote “We hope that this paper will provide researchers interested in open science an accessible entry point to the practices most applicable to their needs. For all of the steps presented in this annotated reading list, any time taken by researchers to understand the issues and develop better practices will be rewarded in orders of magnitude. On an individual level, time and effort are ultimately saved, errors are reduced, and one’s own research is improved through a greater adherence to openness and transparency. On a field-wide level, the more researchers invest in adopting these practices, the closer the field will come toward adhering to scientific norms and the values it claims to espouse.” (p.245) Abstract The open science movement is rapidly changing the scientific landscape. Because exact definitions are often lacking and reforms are constantly evolving, accessible guides to open science are needed. This paper provides an introduction to open science and related reforms in the form of an annotated reading list of seven peer-reviewed articles, following the format of Etz, Gronau, Dablander, Edelsbrunner, and Baribault (2018). Written for researchers and students – particularly in psychological science – it highlights and introduces seven topics: understanding open science; open access; open data, materials, and code; reproducible analyses; preregistration and registered reports; replication research; and teaching open science. For each topic, we provide a detailed summary of one particularly informative and actionable article and suggest several further resources. Supporting a broader understanding of open science issues, this overview should enable researchers to engage with, improve, and implement current open, transparent, reproducible, replicable, and cumulative scientific practices APA Style Reference Crüwell, S., van Doorn, J., Etz, A., Makel, M. C., Moshontz, H., Niebaum, J. C., ... \u0026amp; Schulte-Mecklenbeck, M. (2019). Seven Easy Steps to Open Science. Zeitschrift für Psychologie. https://doi.org/10.1027/2151-2604/a000387\nYou may also be interested in Trust Your Science? Open Your Data and Code (Stodden, 2011) Willingness to Share Research Data Is Related to the Strength of the Evidence and the Quality of Reporting of Statistical Results (Wicherts et al., 2011) Psychologists Are Open to Change, yet Wary of Rules (Fuchs et al., 2012) How scientists can stop fooling themselves (Bishop, 2020b) Only Human: Scientists, Systems, and Suspect Statistics A review of: Improving Scientific Practice: Dealing With The Human Factors, University of Amsterdam, Amsterdam, September 11, 2014 (Hardwicke et al., 2014) Registered Reports: Realigning incentives in scientific publishing (Chambers et al., 2015) Registered Reports: A new publishing initiative at Cortex (Chambers, 2013) Registered Reports: A step change in scientific publishing (Chambers, 2014) Registered reports: a method to increase the credibility of published results (Nosek \u0026amp; Lakens, 2014) Registered reports (Jamieson et al., 2019) Quality Uncertainty Erodes Trust in Science (Vazire, 2017) Rein in the four horsemen of irreproducibility (Bishop, 2019) Promoting an open research culture (Nosek et al., 2015) Promoting Transparency in Social Science Research (Miguel et al., 2014) Fallibility in Science: Responding to Errors in the Work of Oneself and Others (Bishop, 2018) Seven Steps Toward Transparency and Replicability in Psychological Science (Lindsay, 2020) Signalling the trustworthiness of science (Jamieson et al., 2020) Many Analysts, One Data Set: Making Transparent How Variations in Analytical Choices Affect Results (Silberzahn et al., 2019) Many hands make tight work (Silberzahn \u0026amp; Uhlmann, 2015) Main Takeaways: It is argued that re-running the analysis produces the same outcome. An analysis run by single team-researchers takes on several roles: inventor: creates ideas and hypotheses; analysts: scrutinise data to support hypotheses; devil’s advocate: use different approaches to show weaknesses in the findings. A crowdsourcing approach can be a useful addition to research. Several teams work with the same dataset, where the hypotheses and results are held closed. All researchers discuss results via email exchanges and researchers add notes to their individual reports in light of others’ work; to express doubt or confidence about their approach. Teams present findings in a draft manuscript, in which the participants are invited to comment and modify. Researchers should not take any single analysis too seriously, as different analyses can produce a broad range of effect sizes. Crowdsourcing analyses will not be the optimal solution for several research problems e.g. resource intensive. Decision making should be made with care regarding: which hypothesis to test; data collection; and which variables to collect. Researchers will disagree about findings, making it difficult to present a manuscript with a clear conclusion. Crowdsourcing research can allow us to evaluate whether analytical approaches and decisions drive findings. This would allow us to discuss the analytical approaches before we commit to a specific strategy. Crowdsourcing reduces the incentives for novel and groundbreaking results and can reveal several scientific possibilities. Quote “Under the current system, strong storylines win out over messy results. Worse, once a finding has been published in a journal, it becomes difficult to challenge. Ideas become entrenched too quickly, and uprooting them is more disruptive than it ought to be. The crowdsourcing approach gives space to dissenting opinions. Scientists around the world are hungry for more-reliable ways to discover knowledge and eager to forge new kinds of collaborations to do so.” (p.191). Abstract Crowdsourcing research can balance discussions, validate findings and better inform policy, say Raphael Silberzahn and Eric L. Uhlmann. APA Style Reference Silberzahn, R., \u0026amp; Uhlmann, E. L. (2015). Crowdsourced research: Many hands make tight work. Nature News, 526(7572), 189. https://doi.org/10.1038/526189a\nYou may also be interested in Publishing Research With Undergraduate Students via Replication Work: The Collaborative Replications and Education Project (CREP; Wagge et al., 2019) Seven Steps Toward Transparency and Replicability in Psychological Science (Lindsay, 2020) A user’s guide to inflated and manipulated impact factor (Ioannidis \u0026amp; Thombs, 2019) Main Takeaways: A widely misused metric is impact factor: reflecting the importance of publications in a specific journal. The promotion and funding of an individual depends on the impact factor (cf. Goodhart’s Law). Based on the Declaration on Research Assessment, over 14000 researchers agree: let’s remove impact factor as the measure of an individual article’s quality. There is a belief that a higher impact factor leads to more and better articles being submitted and published. If this is the case, a journal’s ratings may improve, as its impact factor increases. Volume of submissions may increase, as many scientists naively decide where to send their paper based on journal impact factor. Volume may dissociate from quality. An inappropriate use of impact factor is unlikely to stop (e.g. self citations and including citations to other recent articles without justification), especially with a large number of papers being cited without being counted. In addition, certain manuscripts (e.g. review articles and papers with questionable scientific value) will get more citations than others research articles. Papers should be submitted to target journals based on the quality, scientific rigour, and the relevance of the journal, not impact factor. Quote “Authors should pick target journals based on relevance and scientific rigour and quality, not spurious impact factors. Inspecting inflation measures is more informative for choosing a journal than JIF, because prominent inflation may herald spurious editorial practices and thus poor quality. Authors who submit to journals with high‐impact inflation may become members of bubbles. They even run the risk of having their work published in journals that are eventually formally discredited if Clarivate decides to make a more serious effort to curtail spurious gaming.” (p.5). Abstract This is a view on impact factor by Professor John P.A. Ioannidis and Dr Brett D. Thombs. It contains a discussion of the impact factor being misused, how it is misused by journals and reviewers but provides solutions to overcome the use of this metric. In addition, we should base journals not on the impact factor but the relevance, scientific rigour and quality of the journal. APA Style Reference Ioannidis, J. P., \u0026amp; Thombs, B. D. (2019). A user’s guide to inflated and manipulated impact factors. European journal of clinical investigation, 49(9), e13151. https://doi.org/10.1111/eci.13151\nYou may also be interested in Six principles for assessing scientists for hiring, promotion, and tenure (Naudet et al, 2018) The Matthew effect in science funding (Bol et al., 2018) Promoting an open research culture (Nosek et al., 2015) Main Takeaways: The incentive system focuses on innovation, as opposed to replication, openness, transparency, and reproducibility. There is no means to align individual and communal incentives via universal scientific policies and procedures. We should reward researchers for the time and effort spent in open practices. Citations should also extend to data, code, and research materials. Regular and rigorous citation of these materials should be cited as an original intellectual contribution. Reproducibility increases confidence in the results and allows scholars to learn more about data interpretation. The transparency guidelines are used to improve explicitness about the research process, while reducing vague or incomplete reporting of methodology. Pre-registration of studies facilitates the discovery of research, allowing the study to be recorded in a public registry. Four levels are used to encourage open science policy: Level 1 is designed to have no barrier or incentive to adopting open science practices (e.g. code sharing). This reduces the effort on the efficiency and workflow of the journal. Level 2 has stronger authorial expectations than Level 1. It avoids adding resource cost to editors or publishers who adopt this standard. In Level 2, journals would require codes to be deposited in a trusted repository (e.g. osf), also reviewers would need to check the link appears in the manuscript and access the code in the repository. Level 3 is the strongest standard but provides some barriers to implementations in the journal. For instance, authors must provide their code for the review process and editors must be able to reproduce the reported analyses publication. These higher level guidelines should reduce the time spent on communication with the authors and reviewers, improve standards of reporting, increase detectability of errors prior to publication and ensure that publication-related data is accessible for a long time. Quote “The journal article is central to the research communication process. Guidelines for authors define what aspects of the research process should be made available to the community to evaluate, critique, reuse, and extend. Scientists recognize the value of transparency, openness, and reproducibility. Improvement of journal policies can help those values become more evident in daily practice and ultimately improve the public trust in science, and science itself.” (p.1425). Abstract Author guidelines for journals could help to promote transparency, openness, and reproducibility. APA Style Reference Nosek, B. A., Alter, G., Banks, G. C., Borsboom, D., Bowman, S. D., Breckler, S. J., ... \u0026amp; Contestabile, M. (2015). Promoting an open research culture. Science, 348(6242), 1422-1425. http://doi.org/10.1126/science.aab2374\nYou may also be interested in Quality Uncertainty Erodes Trust in Science (Vazire, 2017) Rein in the four horsemen of irreproducibility (Bishop, 2019) Seven Easy Steps to Open Science: An Annotated Reading List (Crüwell et al., 2019) Promoting Transparency in Social Science Research (Miguel et al., 2014) Fallibility in Science: Responding to Errors in the Work of Oneself and Others (Bishop, 2018) Seven Steps Toward Transparency and Replicability in Psychological Science (Lindsay, 2020) Promoting Transparency in Social Science Research (Miguel et al., 2014) Main Takeaways: The incentives, norms, institutions, and a dysfunctional reward structure make it difficult to improve research design in the social sciences. Since social science journals do not instruct adherence to reporting standards (e.g. data sharing), researchers feel motivated to analyse and present data in a more publishable way, e.g. by selecting a subset of positive results. Such practices result in a distorted body of evidence with too few null results having direct consequences on policies and eventually citizens. This article surveys recent progress towards research transparency in the social sciences and provides standards and rules to realign scholarly incentives with good scientific practices based on three pillars: Disclosure, Preregistration, and Open data and materials. Disclosure is about the systematic reporting of all measures, manipulations, data exclusions, and sample sizes. Preregistration helps to reduce bias and increase credibility by pre-specifying, e.g., statistical models, dependent variables, and covariates. Open data and materials allows researchers to test alternative approaches on the data, reproduce results, identify misreported or fraudulent results; reuse or adapt materials for replication or their own research. Limitation: One might argue that preregistration counteracts exploratory analysis. Counterargument: Preregistration should just free an analysis from being reported as formal hypothesis testing. Further work is needed, e.g., it is unclear how to preregister studies based on existing data which is a common approach in the social sciences. Quote “Scientific inquiry requires imaginative exploration. Many important findings originated as unexpected discoveries. But findings from such inductive analysis are necessarily more tentative because of the greater flexibility of methods and tests and, hence, the greater opportunity for the outcome to obtain by chance. The purpose of prespecification is not to disparage exploratory analysis but to free it from the tradition of being portrayed as formal hypothesis testing. New practices need to be implemented in a way that does not stifle creativity or create excess burden. Yet we believe that such concerns are outweighed by the benefits that a shift in transparency norms will have for overall scientific progress, the credibility of the social science research enterprise, and the quality of evidence that we as a community provide to policy-makers” (p.31). Abstract Social scientists should adopt higher transparency standards to improve the quality and credibility of research. APA Style Reference Miguel, E., Camerer, C., Casey, K., Cohen, J., Esterling, K. M., Gerber, A., ... \u0026amp; Laitin, D. (2014). Promoting transparency in social science research. Science, 343(6166), 30-31. http://doi.org/10.1126/science.1245317\nYou may also be interested in Quality Uncertainty Erodes Trust in Science (Vazire, 2017) Rein in the four horsemen of irreproducibility (Bishop, 2019) Seven Easy Steps to Open Science: An Annotated Reading List (Crüwell et al., 2019) Promoting an open research culture (Nosek et al., 2015) Fallibility in Science: Responding to Errors in the Work of Oneself and Others (Bishop, 2018) Seven Steps Toward Transparency and Replicability in Psychological Science (Lindsay, 2020) Rebuilding Ivory Tower: A bottom-up experiment in aligning research with societal needs (Hart \u0026amp; Silka, 2020) ◈ Main Takeaways: Scientists are trained to conduct good science, develop interesting research questions, be impartial to data, sceptical about conclusions and open to criticisms from our peers. We are taught good science is a reward in itself for improving our world. We need strong collaborations with diverse stakeholders in the public and private sectors, non-governmental organisations and civil society in order to identify and solve (sustainability/wicked) problems. “So it turned out that social scientists as well as natural scientists had keen interest in a project aimed at bringing together their expertise and forming bonds with individuals and groups outside academia to solve local problems...also to identify best practices for interdisciplinarity and stakeholder engagement”. (pp. 80-81). A shared culture, comprising a common set of beliefs and values and supported by organizational strategy and structure, is needed to streamline the commitment to excellence innate to many academics. We try to create an atmosphere of learning from successes and failures. There is no sure-fire formula to match research with societal needs. Older faculty are retiring but are being replaced by younger students who are able to move the initiative forward as a result of their skills to be interdisciplinary researchers. Universities should use bottom-up (inner interest of academics to improve the world) and top-down (university programs; ideas from senior leaders) strategies to become more useful partners to society. Put simply, “Although no single recipe will work in all contexts, it is our hope that the ingredients we’ve identified may prove useful to other universities in their own quests to help solve society’s greatest problems\u0026#34;. (p.85). Quote “Two fundamental commitments [have emerged]: 1) In addition to the traditional focus on the biophysical components underpinning a problem, a much greater emphasis is needed on the human dimensions, including the complex interactions between society and nature; and 2) productive collaborations must be built between the university and diverse stakeholders to develop a sufficient understanding of sustainability problems and viable strategies for solving them.” Abstract Academic scientists can transcend publish-or-perish incentives to help produce real-world solutions. Here’s how one group did it. APA Style Reference Hart, D. D., \u0026amp; Silka, L. (2020). Rebuilding the ivory tower: bottom-up experiment in aligning research with societal needs. Issues Sci Technol, 36(3), 64-70. https://issues.org/aligning-research-with-societal-needs/ [accessed 14/08/2020]\nYou may also be interested in Promoting an open research culture (Nosek et al., 2015) Promoting Transparency in Social Science Research (Miguel et al., 2014) Open Knowledge Institutions (Montgomery et al. 2018) Professors, We Need You! (Kristof, 2014) Is science really facing a reproducibility crisis, and do we need it to? (Fanelli, 2018) Main Takeaways: Science is said to be in a crisis due to unreliable findings, poor research quality and integrity, low statistical power, and questionable publication practices caused by the pressure to publish. Fanelli questions this “science in crisis” narrative by critically examining the evidence for the existence of these problems. Fraud and questionable research practices exist, but they are likely not common enough to seriously distort the scientific literature. The pressure to publish has not been conclusively linked to scientific bias or misconduct. Low power and replicability may differ between subfields and methodologies, and may be influenced by the magnitude of the true effect size, and the prior probability of the hypothesis being true. There is little evidence to suggest that misconduct or questionable research practices have increased in recent years. The “science in crisis” narrative is not well supported by the evidence and may be counterproductive, as it encourages values that can be used to discredit science. A narrative of “new opportunities” or “revolution” may be more empowering to scientists. Quote “Science always was and always will be a struggle to produce knowledge for the benefit of all of humanity against the cognitive and moral limitations of individual human beings, including the limitations of scientists themselves.” (p.2630) Abstract Efforts to improve the reproducibility and integrity of science are typically justified by a narrative of crisis, according to which most published results are unreliable due to growing problems with research and publication practices. This article provides an overview of recent evidence suggesting that this narrative is mistaken, and argues that a narrative of epochal changes and empowerment of scientists would be more accurate, inspiring, and compelling. APA Style Reference Fanelli, D. (2018). Opinion: Is science really facing a reproducibility crisis, and do we need it to?. Proceedings of the National Academy of Sciences, 115(11), 2628-2631. https://doi.org/10.1073/pnas.1708272114\nYou may also be interested in Publishing Research With Undergraduate Students via Replication Work: The Collaborative Replications and Education Project (Wagge et al., 2019) Only Human: Scientists, Systems, and Suspect Statistics A review of: Improving Scientific Practice: Dealing With The Human Factors, University of Amsterdam, Amsterdam, September 11, 2014 (Hardwicke et al., 2014) Rein in the four horsemen of irreproducibility (Bishop, 2019) Seven Steps Toward Transparency and Replicability in Psychological Science (Lindsay, 2020) On the persistence of low power in psychological science (Vankov et al., 2014) Fallibility in Science: Responding to Errors in the Work of Oneself and Others (Bishop, 2018) Main Takeaways: Researchers may be under pressure, once they find a error, to not reveal it due to pressure from senior scientist and the institution; Retraction produces fear in a scientist, as it is associated with shame. Errors can be reduced with open science practices. Raw data can never be made completely open due to confidentiality but we can modify it to remove identifiable information, so that other researchers reproduce what was done. Stigma needs to be removed concerning error detection. Making an analysis program open does not mean they are error-free. A reproducible result simply indicates that when the same data is analysed, the same result is obtained, even if incorrect. Researchers whose error is noticed and respond with denial, anger or silence tend to damage their reputation for integrity. Resolving such issues via the journal that published the original article may be a better approach, though this process seldom proceeds smoothly. Findings may be due to methodological concern, as opposed to errors in calculation or scripts, such as conducting a study without a control group, underpowered, using unreliable measures or that has a major confound. Methodological errors may be due to ignorance instead of bad faith, including honest errors in the data, analysis or method which compromise conclusions inferred. Replication is important, as confidence in the robustness of a finding cannot depend on a single study. When there is a failure to replicate, we should uncover why this happened (e.g. contextual factors or research expertise). We should not say original researchers are incompetent, frauds, etc., but we should also not say that critics had malevolent motives and lack expertise. We need to be impartial. We should avoid bias and identify publications that are ignored, as positive findings produce more citations than null findings. Investigating misconduct is important but challenging. It is a difficult endeavour and requires evidence that takes time to accumulate. Academic institutions take an accusation of misconduct against a staff member seriously but it takes a long time. We should consider whether people could have vested interests against this academic. We should not mock or abuse other scientists who make honest errors, as this would encourage poor research practices and people may be less likely to be open about these errors. Quote “Criticism is the bedrock of the scientific method. It should not be personal: If one has to point to problems with someone’s data, methods, or conclusions, this should be done without implying that the person is stupid or dishonest. This is important, because the alternative is that many people will avoid engaging in robust debate because of fears of interpersonal conflict—a recipe for scientific stasis. If wrong ideas or results are not challenged, we let down future generations who will try to build on a research base that is not a solid foundation. Worse still, when the research findings have practical applications in clinical or policy areas, we may allow wrongheaded interventions or policies to damage the well-being of individuals or society. As open science becomes increasingly the norm, we will find that everyone is fallible. The reputations of scientists will depend not on whether there are flaws in their research, but on how they respond when those flaws are noted.” (p.6) Abstract This is a view on the fallibility of science, response to self-errors and errors made by others by Professor Dorothy Bishop. It contains a discussion on how open science should be the norm but being open and honest about oneself is not. It informs us that we should not mock or be hurtful to others concerning honest mistakes and that misconduct is a serious issue but we need to be supportive of both the researcher who is being accused and the individual who is accusing them. APA Style Reference Bishop, D. V. M. (2018). Fallibility in science: responding to errors in the work of oneself and others. Advances in Methods and Practices in Psychological Science, 1(3), 432-438. https://doi.org/10.1177/2515245918776632\nYou may also be interested in Don’t let transparency damage science (Lewandowsky \u0026amp; Bishop, 2016) Only Human: Scientists, Systems, and Suspect Statistics A review of: Improving Scientific Practice: Dealing With The Human Factors, University of Amsterdam, Amsterdam, September 11, 2014 (Hardwicke et al., 2014) Quality Uncertainty Erodes Trust in Science (Vazire, 2017) Rein in the four horsemen of irreproducibility (Bishop, 2019) Seven Easy Steps to Open Science: An Annotated Reading List (Crüwell et al., 2019) Promoting an open research culture (Nosek et al., 2015) Promoting Transparency in Social Science Research (Miguel et al., 2014) Signalling the trustworthiness of science should not be a substitute for direct action against research misconduct (Kornfeld \u0026amp; Titus, 2020) Reply to Kornfeld and Titus: No distraction from misconduct (Jamieson et al., 2020) Stop ignoring misconduct (Kornfeld \u0026amp; Titus, 2016) Seven Steps Toward Transparency and Replicability in Psychological Science (Lindsay, 2020) Publication Pressure and Scientific Misconduct in Medical Scientists (Tijdink et al., 2014) Scientists’ Reputations are Based on Getting it Right, not being Right (Ebersole et al., 2016) Check for publication integrity before misconduct (Grey et al., 2020) Seven Steps Toward Transparency and Replicability in Psychological Science (Lindsay, 2020) Main Takeaways: Publication bias, small sample sizes, and p-hacking exaggerate effect sizes in the literature, contributing to the replication crisis. Lindsay proposes seven steps to improve transparency and replicability: 1. Tell the truth. Be honest and advocate research-if the idea was inspired by data, state so. Report effect size with 95% confidence intervals around them. 2. Assess your understanding of inferential statistical tools. We need improved statistical sophistication for researchers to test hypotheses about populations based on their samples - reward quality and accuracy of methods, not quantity and flashiness of results. 3. Consider standardizing aspects of your approach to conducting hypothesis testing research. Create a detailed research plan providing a priori hypotheses, sample size planning, data exclusion rules, analyses, transformations, covariates etc. Be transparent and register a research plan (cf. Pre-registration and Registered reports). 4. Consider developing a lab manual. Include standardised procedures in data exclusion, data transformations, data-cleaning, authorship, file naming conventions, etc. 5. Make your materials, data, and analysis scripts transparent. They should be Findable, Accessible, Interoperable and Reusable (FAIR). 6. Address constraints on the generality of your findings. Under what conditions should your results replicate, and not replicate? Failure to replicate could be due to differences in procedures, albeit original work did not indicate such differences modulate effect. 7. Consider collaborative approaches to conducting research. Quote “The aim of the methodological reform movement is not to restrict psychological research to procedures that meet some fixed criterion of replicability. Replicability is not in itself the goal of science. Rather, the central aim of methodological reform is to make research reports more transparent, so that readers can gain an accurate understanding of how the data were obtained and analyzed and can therefore better gauge how much confidence to place in the findings. A second aim is to discourage practices that contribute to effect-size exaggeration and false discoveries of non-existent phenomena. As per Vazire’s analogy, the call is not for car dealerships to sell nothing but new Ferraris, but rather for dealers to be forthcoming about the weaknesses of what they have on the lot. The grand aim of science is to develop better, more accurate, and more useful understandings of reality. Methodological reform cannot in and of itself deliver on that goal, but it can help.” (p.19). Abstract Psychological scientists strive to advance understanding of how and why we animals do and think and feel as we do. This is difficult, in part because flukes of chance and measurement error obscure researchers’ perceptions. Many psychologists use inferential statistical tests to peer through the murk of chance and discern relationships between variables. Those tests are powerful tools, but they must be wielded with skill. Moreover, research reports must convey to readers a detailed and accurate understanding of how the data were obtained and analyzed. Research psychologists often fall short in those regards. This paper attempts to motivate and explain ways to enhance the transparency and replicability of psychological science. Specifically, I speak to how publication bias and p hacking contribute to effect-size exaggeration in the published literature, and how effect-size exaggeration contributes, in turn, to replication failures. Then I present seven steps toward addressing these problems: Telling the truth; upgrading statistical knowledge; standardizing aspects of research practices; documenting lab procedures in a lab manual; making materials, data, and analysis scripts transparent; addressing constraints on generality; and collaborating. APA Style Reference Lindsay, D. S. (2020). Seven steps toward transparency and replicability in psychological science. Canadian Psychology/Psychologie canadienne. Advance online publication. https://doi.org/10.1037/cap0000222 [ungated]\nYou may also be interested in The Statistical Crisis in Science (Gelman \u0026amp; Loken, 2014) Only Human: Scientists, Systems, and Suspect Statistics A review of: Improving Scientific Practice: Dealing With The Human Factors, University of Amsterdam, Amsterdam, September 11, 2014 (Hardwicke et al., 2014) False-Positive Psychology: Undisclosed Flexibility in Data Collection and Analysis Allows Presenting Anything as Significant (Simmons et al., 2011) A 21 Word Solution (Simmons et al., 2012)◈ Quality Uncertainty Erodes Trust in Science (Vazire, 2017) Rein in the four horsemen of irreproducibility (Bishop, 2019) Seven Easy Steps to Open Science: An Annotated Reading List (Crüwell et al., 2019) Many hands make tight work (Silberzahn \u0026amp; Uhlmann, 2015) Promoting an open research culture (Nosek et al., 2015) Promoting Transparency in Social Science Research (Miguel et al., 2014) Psychologists Are Open to Change, yet Wary of Rules (Fuchs et al., 2012) Registered Reports: Realigning incentives in scientific publishing (Chambers et al., 2015) Registered Reports: A new publishing initiative at Cortex (Chambers, 2013) Registered Reports: A step change in scientific publishing (Chambers, 2014) Registered reports: a method to increase the credibility of published results (Nosek \u0026amp; Lakens, 2014) Registered reports (Jamieson et al., 2019) Attitudes Toward Open Science and Public Data Sharing: A Survey Among Members of the German Psychological Society (Abele-Brehm et al., 2019) Willingness to Share Research Data Is Related to the Strength of the Evidence and the Quality of Reporting of Statistical Results (Wicherts et al., 2011) Constraints on Generality (COG): A Proposed Addition to All Empirical Papers (Simons et al., 2017) Most people are not WEIRD (Henrich et al., 2010) How scientists can stop fooling themselves (Bishop, 2020b) CJEP Will Offer Open Science Badges (Pexman, 2017) Badges to Acknowledge Open Practices: A Simple, Low-Cost, Effective Method for Increasing Transparency (Kidwell et al., 2016) Trust Your Science? Open Your Data and Code (Stodden, 2011)◈ Publishing Research With Undergraduate Students via Replication Work: The Collaborative Replications and Education Project (CREP; Wagge et al., 2019) Is science really facing a reproducibility crisis, and do we need it to? (Fanelli, 2018) Fallibility in Science: Responding to Errors in the Work of Oneself and Others (Bishop, 2018) A consensus-based transparency checklist (Aczel et al., 2020) Tell it like it is (Anon, 2020) Is pre-registration worthwhile? (Szollosi et al., 2020) Signalling the trustworthiness of science (Jamieson et al., 2020) Many Analysts, One Data Set: Making Transparent How Variations in Analytical Choices Affect Results (Silberzahn et al., 2019) Scientific inbreeding and same-team replication: Type D personality as an example (Ioannidis, 2012) Measurement Schmeasurement: Questionable Measurement Practices and How to Avoid Them (Flake \u0026amp; Fried, 2019)◈ Main Takeaways: Questionable measurement practices (QMPs) undermine internal and external validity, both of the statistical conclusions and construct of interest. The authors focus on lack of transparency in reporting how measures are developed, used, and adapted, as well as reporting key psychometric information. Often, information about the decisions researchers make is lacking, particularly when measures are created/adapted on the fly (e.g. changing response type, changing response style or options, changing item wording or content). Increasing transparency of measurement development and use facilitates thorough and accurate evaluation of validity of results. The authors offer the following guidance: Quote “The increased awareness and emphasis on QRPs, such as p-hacking, have been an important contribution to improving psychological science. We echo those concerns, but also see a grave need for broadening our scrutiny of current practices to include QMPs (Fried \u0026amp; Flake, 2018). Recalling our example of depression at the outset, even if we increase the sample size of our depression trials, adequately power our studies, pre-register our analytic strategies, and stop p-hacking — we can still be left wondering if we were ever measuring depression at all.” (p.22) Abstract In this paper, we define questionable measurement practices (QMPs) as decisions researchers make that raise doubts about the validity of the measures, and ultimately the validity of study conclusions. Doubts arise for a host of reasons including a lack of transparency, ignorance, negligence, or misrepresentation of the evidence. We describe the scope of the problem and focus on how transparency is a part of the solution. A lack of measurement transparency makes it impossible to evaluate potential threats to internal, external, statistical conclusion, and construct validity. We demonstrate that psychology is plagued by a measurement schmeasurement attitude: QMPs are common, hide a stunning source of researcher degrees of freedom, pose a serious threat to cumulative psychological science, but are largely ignored. We address these challenges by providing a set of questions that researchers and consumers of scientific research can consider to identify and avoid QMPs. Transparent answers to these measurement questions promote rigorous research, allow for thorough evaluations of a study’s inferences, and are necessary for meaningful replication studies. APA Style Reference Flake, J. K., \u0026amp; Fried, E. I. (2019). Measurement schmeasurement: Questionable measurement practices and how to avoid them. https://psyarxiv.com/hs7wm/\nYou may also be interested in The Statistical Crisis in Science (Gelman \u0026amp; Loken, 2014) A consensus-based transparency checklist (Aczel et al., 2020) Main Takeaways: This manuscript provides a checklist of research transparency practices researchers can use to evaluate the transparency in their study, or to help improve transparency at each stage of the research process. Transparency is required to evaluate and reproduce findings, and also for research synthesis and meta analysis from the raw data. There is a lack of transparency in the literature, but we should not assume an intention to be deceptive or misleading. Rather, human reasoning is prone to biases (e.g. confirmation bias and motivated reasoning) and few journals ask about statistical and methodological practices and transparency. Journals can support open practices by offering badges, using the transparency and openness promotion guidelines, promote the availability of all research items, including data, materials and codes. The consensus-based transparency checklist can be submitted with the manuscript to provide critical information about the process to evaluate the robustness of a finding. The checklist can be modified by deleting, adding and rewording items with a high level of acceptability and consensus with no strong counter argument for single items. Researchers can explain the choices at the end of each 36 section. There is a shortened 12-item version to reduce demands on the researchers’ time and facilitate broader adoption that fosters transparency and asks authors to complete a 36-item list. Abstract We present a consensus-based checklist to improve and document the transparency of research reports in social and behavioural research. An accompanying online application allows users to complete the form and generate a report that they can submit with their manuscript or post to a public repository. APA Style Reference Aczel, B., Szaszi, B., Sarafoglou, A., Kekecs, Z., Kucharský, Š., Benjamin, D., ... \u0026amp; Ioannidis, J. P. (2020). A consensus-based transparency checklist. Nature human behaviour, 4(1), 4-6. https://doi.org/10.1038/s41562-019-0772-6\nYou may also be interested in How scientists can stop fooling themselves (Bishop, 2020b) Seven Steps Toward Transparency and Replicability in Psychological Science (Lindsay, 2020) Tell it like it is (Anon, 2020) Is pre-registration worthwhile? (Szollosi et al., 2020) Tell it like it is (Anon, 2020) Main Takeaways: A manuscript provides an account of how a research question(s) is/are addressed, reports findings, and explains how findings support or contradict hypotheses. Current research culture is defined by a pressure to present research projects as conclusive narratives that leave no room for ambiguity. The pressure to produce clean narratives represents a threat to validity and counter reality of what science looks like. Clean narratives often report only outcomes to confirm original predictions or exclude research findings that provide messy results. These questionable research practices create a distorted picture of research that prevents cumulative knowledge. Pre-registration has little value if not heeded or transparently reported. It sometimes becomes evident during peer review that a pre-registered analysis is inappropriate or suboptimal. Authors should indicate deviations from their original plan, and explain why they did these deviations. To ensure transparency, unless a preregistered analysis plan is unquestionably flawed, authors should also report the results of their preregistered analyses. In multi-study research papers authors should report all work they executed, irrespective of outcomes. All research papers must include a limitation section that explains study shortcomings and explicitly acknowledges alternative interpretations of the findings. Quote “No research project is perfect; there are always limitations that also need to be transparently reported. In 2019, we made it a requirement that all our research papers include a limitations section, in which authors explain methodological and other shortcomings and explicitly acknowledge alternative interpretations of their findings… Science is messy, and the results of research rarely conform fully to plan or expectation. ‘Clean’ narratives are an artefact of inappropriate pressures and the culture they have generated. We strongly support authors in their efforts to be transparent about what they did and what they found, and we commit to publishing work that is robust, transparent and appropriately presented, even if it does not yield ‘clean’ narratives” p.1 Abstract Every research paper tells a story, but the pressure to provide ‘clean’ narratives is harmful for the scientific endeavour. APA Style Reference Anon (2020). Tell it like it is. Nat Hum Behav 4, 1. https://doi.org/10.1038/s41562-020-0818-9 You may also be interested in How scientists can stop fooling themselves (Bishop, 2020b) Seven Steps Toward Transparency and Replicability in Psychological Science (Lindsay, 2020) A consensus-based transparency checklist (Aczel et al., 2020) Tell it like it is (Anon, 2020) Is pre-registration worthwhile? (Szollosi et al., 2020) Is pre-registration worthwhile? (Szollosi et al., 2020) Main Takeaways: Pre-registration should be an option to improve research. Pre-registration intends to solve statistical problems and forces people to think more deeply about theories, methods, and analyses. But, needing, rewarding, or promoting it is not worthwhile. Requiring pre-registration could harm the progress in our field. Scientific inference is the process to develop better theories. Statistical models are simplified mathematical abstractions of scientific problems, simplifications to aid scientific inference but to allow abstraction. Diagnosticity of statistical tests depends on how well statistical models map onto theories and improved statistical techniques does little to improve theories when mapping is weak. Models are useful depending on how accurately the theory is matched to the model. Many statistical models (e.g. general linear model) in psychology are poor estimates of the theory. Bad theories can be pre-registered with predictions barely better than randomly picking an outcome. Pre-registration does not improve theories but should allow researchers to think more deeply on how to improve theories through better planning, more precise operationalisation of constructs, and clear motivation for statistical planning. We should improve theories when encountering difficulties with pre-registration or when pre-registered predictions are wrong. There is no problem with post-hoc scientific inference when the theories are strong. Any improvement depends on a good understanding of how to improve a theory, and pre-registration provides no understanding. Pre-registration encourages thinking, but it is unclear whether the thinking is better or worse. Poor operationalisation, imprecise measurement, weak connection between theory and statistical method should take precedence over problems of statistical inference. Abstract Proponents of preregistration argue that, among other benefits, it improves the diagnosticity of statistical tests. In the strong version of this argument, preregistration does this by solving statistical problems, such as family-wise error rates. In the weak version, it nudges people to think more deeply about their theories, methods, and analyses. We argue against both: the diagnosticity of statistical tests depends entirely on how well statistical models map onto underlying theories, and so improving statistical techniques does little to improve theories when the mapping is weak. There is also little reason to expect that preregistration will spontaneously help researchers to develop better theories (and, hence, better methods and analyses). APA Style Reference Szollosi, A., Kellen, D., Navarro, D. J., Shiffrin, R., van Rooij, I., Van Zandt, T., \u0026amp; Donkin, C. (2020). Is Preregistration Worthwhile?. Trends in cognitive sciences, 24(2), 94.https://doi.org/10.1016/j.tics.2019.11.009\nYou may also be interested in How scientists can stop fooling themselves (Bishop, 2020b) Seven Steps Toward Transparency and Replicability in Psychological Science (Lindsay, 2020) A consensus-based transparency checklist (Aczel et al., 2020) Tell it like it is (Anon, 2020) Arrested theory development: The misguided distinction between exploratory and confirmatory research (Szollosi \u0026amp; Donkin, 2019) From pre-registration to publication: a non-technical primer for conducting meta-analysis to synthesize correlation data (Quintana, 2015) Pre-registration is Hard, And Worthwhile (Nosek et al., 2019) Easy preregistration will benefit any research (Mellor \u0026amp; Nosek, 2018) Preregistration of Modeling Exercises May Not Be Useful (MacEachern \u0026amp; Van Zandt, 2019) Arrested theory development: The misguided distinction between exploratory and confirmatory research (Szollosi \u0026amp; Donkin, 2019)◈ Main Takeaways: The article describes the current philosophy of science and explain how theory development and theory assessment should work under this framework. This will be followed by why proposed methodological solutions (e.g. direct replications and pre-registration) to the “replication crisis” are unlikely to eliminate factors that interrupt theory development. They conclude in how we can move towards the development of good theories and explanations. The aim of science is to develop good explanations. To bring about good explanations is to detect and correct flaws in our existing theories. A theory can be criticised by argument, meaning the rejection of explanations that are bad according to the criteria of a good theory: “(1) explain what they are supposed to explain, (2) consistent with other good theories, and (3) cannot easily be adapted to explain anything” (p.4). This takes the form of an argument that a theory cannot account for some existing observation. Also, the theory can be criticised based on how easily it can be adapted to explain several unobserved data patterns. The theory can also be criticised by experimental testing, making a theory problematic by increasing the set of observations that a theory is meant to explain but is unable to do so. For science to progress, accountability in theory change is important, we should predict that each adaptation of a theory makes it more inflexible, thus increasing the potential to be made problematic. In turn, this makes the current theory problematic, thus requiring new theories. The distinction between exploratory and confirmatory research is meaningless. It does not matter when theories are changed but how easy it was or would have been to make these changes. Focusing on the distinction between exploratory and confirmatory research focus on the inflexibility of the predictions of a theory as important, while ignoring the flexibility of the theory. Hypotheses only matter for experimental studies when a theory is invariant, “experimental testing of a flexible theory will not move scientific progress, even if the predictions of the theory were pre-registered or directly replicated.” (p.10). Quote “The key property of a good explanation is that it is hard to vary (Deutsch, 2011). More specifically, a theory can be regarded as good if it satisfies the following criteria, proposed by Deutsch (2016): good theories (1) explain what they are supposed to explain, (2) are consistent with other good theories, and (3) cannot easily be adapted to explain anything. These criteria aim to ensure that a theory is constrained by all of our existing knowledge (existing observations and other good theories), without the benefit of flexibility to tailor the explanation to any possible pattern of observation. In other words, the conjectures that comprise a theory must be inflexible while still allowing the theory to account for its explicanda. This property of good theories constrains the way in which that theory can be changed. A good theory will resist most changes, because the explanation for any change must be consistent with the retained inflexible conjectures of that theory without making the theory inconsistent with existing observations.” (p.4). Abstract Science progresses by finding and correcting problems in theories. Good theories are those that help facilitate this process by being hard-to-vary: they explain what they are supposed to explain, they are consistent with other good theories, and they are not easily adaptable to explain anything. Here we argue that, rather than a lack of distinction between exploratory and confirmatory research, an abundance of flexible theories is a better explanation for current replicability problems of psychology. We also explain why popular methods-oriented solutions fail to address the real problem of flexibility. Instead, we propose that a greater emphasis on theory criticism by argument would improve replicability. APA Style Reference Szollosi, A., \u0026amp; Donkin, C. (2019). Arrested theory development: The misguided distinction between exploratory and confirmatory research. https://doi.org/10.31234/osf.io/suzej\nYou may also be interested in Is pre-registration worthwhile? (Szollosi et al., 2020) From pre-registration to publication: a non-technical primer for conducting meta-analysis to synthesize correlation data (Quintana, 2015) Main Takeaways: This review will discuss how to conduct a meta-analysis following PRISMA guidelines. Pre-register the meta-analysis protocol, as it allows the researchers to formulate the study rationale for a specific research question. In addition, pre-registration avoids bias by providing evidence of a priori analysis intentions, thus in turn, reducing p-hacking. Although few journals need to consider meta-analysis registration, pre-registration is important for submission. As meta-analyses are often used to guide treatment for practice and health policy, its pre-registration is possibly even more important than the pre-registration of clinical trials. Most journals do not explicitly state that pre-registration is a requirement, the submission of a PRISMA checklist is required, which includes a protocol and a study registration. Although there are many databases available, it is the researcher\u0026#39;s responsibility to choose the most suitable sources for their research areas. Numerous scientists use duplicate search terms within two or more databases to cover numerous sources. Researchers can also search reference lists of eligible studies for other eligible studies (i.e. snowballing). It is important to note the number of studies returned and after using the specified search term, how many of these studies were discarded and the motivation behind discarding these studies. The search teams and strategies should be specific enough for a reader to reproduce the search, which includes the date range of studies, together with the date that the search was conducted. Traditionally, it has been difficult to access the gray literature (i.e. research that has not been formally published), now it is becoming more accessible as libraries are posting dissertations in their online repository. Regardless of whether gray literature studies should be included, explicit and detailed search strategies need to be included in the study protocol and method section. There are two effect models generally used in a meta-analysis, fixed and random. The way to select one of these models is centered around \u0026#34;how much of the variation of studies can be attributed to variation in the true effect sizes\u0026#34; (p. 5) – assumptions of study homogeneity. A variation is from random error and true study heterogeneity. Forest plots visualise effect sizes and confidence intervals from included studies, together with summary effect size. A funnel plot is a visual tool to investigate potential publication bias (i.e. significant findings are published, while non-significant results are not published) in meta-analyses. Funnel plots offer a useful visualisation for potential publication bias, it is important to consider that asymmetry may represent other types of bias like study quality, location bias and study size. However, funnel plots suffer from subjective measures of potential publication bias. Two tests used to calculate objective measures of potential bias: trim and fill method and moderating variables. The final step of the meta analysis is data interpretation and write-up. The PRISMA guidelines provide a checklist that includes all the items that should be included when reporting a meta-analysis. Quote Up to 63% of psychological scientists anonymously admit to questionable research practices(John etal.,2012). These practices include removing data points and analysing data, failing to report all measures analyzed, and HARKing.Such behavior has likely contributed to the lowrates of successful replication observed in psychology (Open Science Collaboration, 2015). The pre-registration of clinical trial protocols has become standard. In contrast,lessthan10%of meta-analysis refers to a study protocol,let alone make the protocol publically available (Moheretal.,2007).Thus,meta-analyses pre-registration would markedly improve the transparency of meta-analyses and the confidence of reported findings.” (p.8) Abstract Starting from the view that progress in science consists of the improvement of our theories, in the current paper we ask two questions: what makes a theory good, and how much do the current method-oriented solutions to the replication crisis contribute to the development of good theories? Based on contemporary philosophy of science, we argue that good theories are hard-to-vary: they (1) explain what they are supposed to explain, (2) are consistent with other good theories, and (3) cannot easily be adapted to explain anything. Theories can be improved by identifying problems in them either by argument or by experimental test, and then correcting these problems by changing the theory. Importantly, such changes and the resultant theory should only be assessed based on whether they are hard-to-vary. An assessment of the current state of the behavioral sciences reveals that theory development is arrested by the lack of consideration for how easy it is to change theories to account for unexpected observations. Further, most of the current method-oriented solutions are unlikely to contribute much to the development of good theories, because they do not work towards eliminating this problem. Instead, they reward only temporary inflexibility in theories, and promote the assessment of theory change based on whether the theory was changed before (confirmatory) or after (exploratory) an experimental test, but not whether that change yields a hard-to-vary theory. Finally, we argue that these methodological solutions would become irrelevant if we turned our focus to the explicit aim of developing theories that are hard-to-vary. APA Style Reference Quintana, D. S. (2015). From pre-registration to publication: a non-technical primer for conducting a meta-analysis to synthesize correlational data. Frontiers in psychology, 6, 1549. https://doi.org/10.3389/fpsyg.2015.01549\nYou may also be interested in Is pre-registration worthwhile? (Szollosi et al., 2020) Pre-registration is Hard, And Worthwhile (Nosek et al., 2019) Easy preregistration will benefit any research (Mellor \u0026amp; Nosek, 2018) Preregistration of Modeling Exercises May Not Be Useful (MacEachern \u0026amp; Van Zandt, 2019) On the reproducibility of meta-analyses: six practical recommendations (Lakens et al., 2014) Pre-registration is Hard, And Worthwhile (Nosek et al., 2019) Main Takeaways: Pre-registration allows us to make exploratory and confirmatory analyses. Pre-registration allows us to make the transparent uncertainty more certain, how many statistical tests were conducted and familywise error rate to be corrected. Pre-registration reduces influence of publication bias and pre-registration is a skill that needs experience to be improved. Pre-registration promotes intellectual humility and better calibration of scientific claims. It allows us to provide information on how methodology is implemented, how hypotheses are tested, the exclusion rules, how variables are combined and what to use concerning the statistical model, covariates and characteristics. Pre-registration converts general sense into precise and explicit plans that predict what has not yet occurred and decide what will be done. It allows us to stop data collection. What are the steps required to assess questions of interest? What are the outcomes? Having a plan is better than no plan, sharing plans to advance is better than not sharing them. Planning will improve and benefits will increase for oneself and consumers of research. Deviations make it harder to interpret with confidence what occurred to what was planned. Transparency is important and all deviations should be reported, this is difficult due to narrative coherence, reviewer expectations and word limits. We need to maximise credibility of reporting findings when possible, update pre-registration, deviations before observing data, mention all planned analyses to explain why a planned analysis was not reported. Use supplements to share in full not hide inconvenient information and during analysis. Abstract Preregistration clarifies the distinction between planned and unplanned research by reducing unnoticed flexibility. This improves credibility of findings and calibration of uncertainty. However, making decisions before conducting analyses requires practice. During report writing, respecting both what was planned and what actually happened requires good judgment and humility in making claims. APA Style Reference Nosek, B. A., Beck, E. D., Campbell, L., Flake, J. K., Hardwicke, T. E., Mellor, D. T., ... \u0026amp; Vazire, S. (2019). Preregistration is hard, and worthwhile. Trends in cognitive sciences, 23(10), 815-818.https://doi.org/10.1016/j.tics.2019.07.009 [ungated]\nYou may also be interested in Is pre-registration worthwhile? (Szollosi et al., 2020) From pre-registration to publication: a non-technical primer for conducting meta-analysis to synthesize correlation data (Quintana, 2015) Easy preregistration will benefit any research (Mellor \u0026amp; Nosek, 2018) Preregistration of Modeling Exercises May Not Be Useful (MacEachern \u0026amp; Van Zandt, 2019) Preregistration of Modeling Exercises May Not Be Useful (MacEachern \u0026amp; Van Zandt, 2019) Main Takeaways: The present study focuses on modelling and data analysis and how each round improves analysis that builds richer understanding of data and processes that give rise to data. Powerful software and improved graphical capabilities allows us to explore many more features of data. The ease with which data is transformed and cleaned, with which a model can be fit may lead to overfitting. Model development is intrinsically exploratory and creative. The present article disagrees with pre- and post-registration of models. In highly exploratory settings, there is greater difficulty to pre-register a model and analysis. Modelling depends on the modeller’s perspective and data collected. Each author performs exploratory analysis and may settle on the same transformation for the response variable. When the model is combined with the Bayesian model averaging, the overall model provides a better description of the entire dataset than any single model on its own. Reality is too complicated and covariates are sparse enough that it would be a challenge to identify the right model. Models are tools. Different models are used differently for distinct ends. Model construction and development depend on analysing and re-analysing a dataset to determine which of its properties are crucial to understand a phenomenon and/or make predictions. Confirmatory model implies truth to be discovered among models in competition but there tends to be model favouritism,which tends to be determined by which models the researcher has invested time in developing; how the modeller views the world; ease of implementation and so on. One model is not true in the strictest sense as some data will be captured, but other data will not. Bayesian methods need to be used, as datasets grow. If preregistration of analyses is required, Bayesian analysts may need to pay particular attention to the impact of the prior distribution on features of the analysis such as the Bayes factor, and the analyst must adopt techniques that can automatically provide robustness to the analysis. Underfitting of the data is as problematic as overfitting. Pre-registration of model development may lessen the engagement of analysts with the data, contributing to less creative and fewer exploratory analyses. Psychology departments should devote more resources to training in quantitative areas and training which include explicit content on under- and over-modelling. Also, we should partner with the statistics department to improve our modelling skills. Abstract This is a commentary on Lee et al.’s (2019) article encouraging preregistration of model development, fitting, and evaluation. While we are in general agreement with Lee et al.’s characterization of the modeling process, we disagree on whether preregistration of this process will move the scientific enterprise forward. We emphasize the subjective and exploratory nature of model development, and point out that “under-modeling” of data (relying on black-box approaches applied to data without data exploration) is as big a problem as “over-modeling” (fitting noise, resulting in models that generalize poorly). We also note the potential long-run negative impact of preregistration on future generations of cognitive scientists. It is our opinion that preregistration of model development will lead to less, and to less creative, exploratory analysis (i.e., to more under-modeling), and that Lee at al.’s primary goals can be achieved by requiring publication of raw data and code. We conclude our commentary with suggestions on how to move forward. APA Style Reference MacEachern, S. N., \u0026amp; Van Zandt, T. (2019). Preregistration of modeling exercises may not be useful. Computational Brain \u0026amp; Behavior, 2(3-4), 179-182. https://doi.org/10.1007/s42113-019-00038-x\nYou may also be interested in Is pre-registration worthwhile? (Szollosi et al., 2020) From pre-registration to publication: a non-technical primer for conducting meta-analysis to synthesize correlation data (Quintana, 2015) Easy preregistration will benefit any research (Mellor \u0026amp; Nosek, 2018) Pre-registration is Hard, And Worthwhile (Nosek et al., 2019) Six principles for assessing scientists for hiring, promotion, and tenure (Naudet et al, 2018) Main Takeaways: Academic work is usually quantified by the quantity of publications. However, this is not a reliable measure. An alternative measure is impact factor: the average number of citations to research articles over the preceding two years. This is an imperfect measure that does not capture the ethos of an academic institution. Impact factor provides information about citation influence for a few papers but is less informative about an individual publication and the authors involved in the publication (cf. Goodhart’s Law - a valid measurement becomes useless when it becomes an optimisation target). Promotions are based on questionable research practices that promote the quantity of publications, but reproducible research does not receive such support. The incentive structure in academia is problematic, as the high impact factor is taken to be similar to high societal impact; this is not the case! High impact factor leads to more funding, more citations and further funding (cf. Matthew’s Effect), whereas the opposite is observed for papers with low impact factor. Papers with high societal impact seem to fit papers with low impact factor. We need to provide a more inclusive evaluation scheme that allows researchers and research to focus more on open science practices. We need to consider societal and broader impact for promotions. Abstract The negative consequences of relying too heavily on metrics to assess research quality are well known, potentially fostering practices harmful to scientific research such as p-hacking, salami science, or selective reporting. The \u0026#34;flourish or perish\u0026#34; culture defined by these metrics in turn drives the system of career advancement in academia, a system that empirical evidence has shown to be problematic and which fails to adequately take societal and broader impact into account. To address this systemic problem, APA Style Reference Naudet, F., Ioannidis, J., Miedema, F., Cristea, I. A., Goodman, S. N., \u0026amp; Moher, D. (2018). Six principles for assessing scientists for hiring, promotion, and tenure. Impact of Social Sciences Blog. http://eprints.lse.ac.uk/90753/\nYou may also be interested in The Nine Circles of Scientific Hell (Neuroskeptic, 2012) Only Human: Scientists, Systems, and Suspect Statistics A review of: Improving Scientific Practice: Dealing With The Human Factors, University of Amsterdam, Amsterdam, September 11, 2014 (Hardwicke et al., 2014) A user’s guide to inflated and manipulated impact factor (Ioannidis \u0026amp; Thombs, 2019) Publication metrics and success on the academic job market (Van Dijk et al., 2014) Rewarding Research Transparency (Gernsbacher, 2018) Faculty promotion must assess reproducibility (Flier, 2017) ⌺ Sample size and the fallacies of classical inference (Friston, 2013) Main Takeaways: The purpose of this target article was to make authors and reviewers think about their response to questions on sample size and effect size for their data. It is important to get as much data as possible (i.e. to have a large sample) to reduce false positives. Trivial effect sizes can be resolved by reporting confidence intervals. The best studies use a large number of subjects and report the results in terms of confidence intervals or protected inference. Large sample sizes will increase the efficiency of model comparison. Increasing sample size will allow us to do more model comparisons, that are otherwise not possible in small sample sizes. “a trivial (standardised) effect size does not mean a very small effect — it is only small in relation to the random fluctuations that attend its expression or measurement. In other words, a miniscule effect is not trivial if it is expressed reliably.” (p.504). Quote “the proportion of true positives, in relation to the total number of significant tests, increases with sensitivity (i.e., the positive predictive value increases with sensitivity). This simply reflects the fact that the number of false negatives is fixed and the number of true positives increases with sensitivity...if we now assume that increasing sample size will increase sensitivity, increases in sample size should therefore increase the portion of true positives. However...if trivial effect sizes predominate, the PPV measures the proportion of significant results that are trivial. This means that increasing sample size is a bad thing and will increase the probability of declaring trivial effects significant (on average).” (p. 504). Abstract I would like to thank Michael Ingre, Martin Lindquist and their co-authors for their thoughtful responses to my ironic Comments and Controversies piece. I was of two minds about whether to accept the invitation to reply — largely because I was convinced by most of their observations. I concluded that I should say this explicitly, taking the opportunity to consolidate points of consensus and highlight outstanding issues. APA Style Reference Friston, K. (2013). Sample size and the fallacies of classical inference. Neuroimage, 81, 503-504.https://doi.org/10.1016/j.neuroimage.2013.02.057\nYou may also be interested in Why small low-powered studies are worse than large high-powered studies and how to protect against “trivial” findings in research: Comment on Friston (2012) (Ingre, 2013) Ironing out the statistical wrinkles in “ten ironic rules” (Lindquist et al., 2013) Ten ironic rules for non-statistical reviewers (Friston, 2012) Why small low-powered studies are worse than large high-powered studies and how to protect against “trivial” findings in research: Comment on Friston (2012) (Ingre, 2013) Main Takeaways: It is often argued that small underpowered studies provide better evidence due to a lack of small and trivial effect sizes yet underpowered studies are less likely to find a true effect in the data and failure does not come without consequences. Failure to detect true effects may indicate that when significant findings are reported they may be due to type I error. Findings from small low-powered studies are weaker than high-powered studies due to the fact that poor statistical power increases false positive rates, even with large effect sizes. Researchers should make use of at least one additional statistic value (e.g. t value, effect size or confidence intervals) alongside p-values as this would protect against analysing meaningless effects; Quote “From a strictly scientific point of view, you can never have too much precision, and consequently, never too many subjects or too much statistical power (unless a researcher is doing something wrong when reporting and interpreting data). The limiting factors are cost (time, resources and money) and potential harm for the subjects involved in the study. The real question you need to ask is how much cost and harm you can afford to get as good answer as possible.” (p.498) Abstract It is sometimes argued that small studies provide better evidence for reported effects because they are less likely to report findings with small and trivial effect sizes (Friston, 2012). But larger studies are actually better at protecting against inferences from trivial effect sizes, if researchers just make use of effect sizes and confidence intervals. Poor statistical power also comes at a cost of inflated proportion of false positive findings, less power to “confirm” true effects and bias in reported (inflated) effect sizes. Small studies (n = 16) lack the precision to reliably distinguish small and medium to large effect sizes (r \u0026lt; .50) from random noise (α = .05) that larger studies (n = 100) do with high level of confidence (r = .50, p = .00000012). The present paper introduces the arguments needed for researchers to refute the claim that small low-powered studies have a higher degree of scientific evidence than large high-powered studies. APA Style Reference Ingre, M. (2013). Why small low-powered studies are worse than large high-powered studies and how to protect against “trivial” findings in research: Comment on Friston (2012). Neuroimage, 81, 496-498.https://doi.org/10.1016/j.neuroimage.2013.03.030\nYou may also be interested in Sample size and the fallacies of classical inference (Friston, 2013) Ironing out the statistical wrinkles in “ten ironic rules” (Lindquist et al., 2013) Ten ironic rules for non-statistical reviewers (Friston, 2012) Ironing out the statistical wrinkles in “ten ironic rules” (Lindquist et al., 2013) Main Takeaways: This commentary discusses the concerns and premise of the Ten ironic rules for non-statistical reviewers by Professor Karl Friston (2012). “working in a highly collaborative environment has taught us that both experts and non-experts alike can have good and bad ideas about statistics (as well as every other field) and that the idea of sharp boundaries between domains is inaccurate and counterproductive.” (p.499). It is more difficult to interpret significant results in small samples, as sample sizes prevent sensitivity analyses to be conducted and specific assumptions to be checked. Under-sampled studies do not allow us to ask questions about confounding variables such as age and gender. Increasing sample size allows us to detect small effects. One argument in favour of small sample sizes is that when we need to consider important non-statistical/ethical issues such as the lives of animals or side effects. Hypothesis testing cannot discriminate between important, but subtle, effects and trivial effects. Quote “In summary, sample size discussions, both prior to conducting a study and post-hoc in peer review, should depend on a number of contextual factors and especially specifics of the hypotheses under question. A small sample size is perfectly capable of differentiating gross brain morphometry between, say, children and adults. However, thousands of participants may be necessary to detect subtle longitudinal trends associated with human brain activation patterns in disease. That is, it is information content that is important, of which number of study participants is only a proxy” (p.501). Abstract The article “Ten ironic rules for non-statistical reviewers” (Friston, 2012) shares some commonly heard frustrations about the peer-review process that all researchers can identify with. Though we found the article amusing, we have some concerns about its description of a number of statistical issues. In this commentary we address these issues, as well as the premise of the article. APA Style Reference Lindquist, M. A., Caffo, B., \u0026amp; Crainiceanu, C. (2013). Ironing out the statistical wrinkles in “ten ironic rules”. Neuroimage, 81, 499-502. https://doi.org/10.1016/j.neuroimage.2013.02.056\nYou may also be interested in Sample size and the fallacies of classical inference (Friston, 2013) Why small low-powered studies are worse than large high-powered studies and how to protect against “trivial” findings in research: Comment on Friston (2012) (Ingre, 2013) Ten ironic rules for non-statistical reviewers (Friston, 2012) Ten ironic rules for non-statistical reviewers (Friston, 2012) Main Takeaways: Reviewers may not have adequate statistical expertise to provide a critique during peer review in order to reject a manuscript. Handling editors are happy to decline a paper and are placed under pressure to maintain a high rejection rate. All journals need to maximise rejection rates in order to increase the quality of submission and impact factor. There are ten rules to follow. Quote “We have reviewed some general and pragmatic approaches to critiquing the scientific work of others. The emphasis here has been on how to ensure a paper is rejected and enable editors to maintain an appropriately high standard, in terms of papers that are accepted for publication. Remember, as a reviewer, you are the only instrument of selective pressure that ensures scientific reports are as good as they can be. This is particularly true of prestige publications like Science and Nature, where special efforts to subvert a paper are sometimes called for.” (p.1303) Abstract As an expert reviewer, it is sometimes necessary to ensure a paper is rejected. This can sometimes be achieved by highlighting improper statistical practice. This technical note provides guidance on how to critique the statistical analysis of neuroimaging studies to maximise the chance that the paper will be declined. We will review a series of critiques that can be applied universally to any neuroimaging paper and consider responses to potential rebuttals that reviewers might encounter from authors or editors. APA Style Reference Friston, K. (2012). Ten ironic rules for non-statistical reviewers. Neuroimage, 61(4), 1300-1310. https://doi.org/10.1016/j.neuroimage.2012.04.018\nYou may also be interested in Sample size and the fallacies of classical inference (Friston, 2013) Why small low-powered studies are worse than large high-powered studies and how to protect against “trivial” findings in research: Comment on Friston (2012) (Ingre, 2013) Ironing out the statistical wrinkles in “ten ironic rules” (Lindquist et al., 2013) Using OSF to Share Data: A Step-by-Step Guide (Soderberg, 2018) Main Takeaways: Materials should be findable, accessible, interoperable and reusable (FAIR) forms. Researchers should look for repositories to decide where and how to share their data. A repository should contain unique and persistent identifiers, so data can be cited. The data is publicly searchable with licenses clarifying how data is reused. Rich meta-data descriptions are provided to allow data to be understandable and reusable. Open science framework is a free and open-source Web tool to help researchers collaboratively manage, store and share the research process and the files related to their research. Step 1: create an account on https://osf.io Step 2: Sign-in your account. Enter name and password or login through your institution. Step 3: Create a project. Press the green button to create a new project. Step 4: Add Collaborators to the project. Click on Contributors and press \u0026#43;Add green button. Search for contributors by name and click on the green \u0026#43; button. If a collaborator does not come up in search, add them to the project by clicking add as an unregistered contributor link. Step 5: upload files that are below the maximum storage of 5GB. To upload files to OSF storage, go to your main project page and click on “OSF Storage” in the Files section. Click on the green “Upload” button and then select the files you wish to upload. Step 6: Add a description of the project. In order to allow you and other users to know what files relate to the project. Step 7: Add a License. reuse is one of the main purposes of data sharing. Other researchers need to know how they are allowed to reuse your work. Step 8: Add component. data, analysis script and study materials should be placed in the project. Step 9: Share your project with reviewers. The project is set up that you may want or need to give reviewers access to the contents of your project before you make it public. Step 10: Make a project public. To make a project public, press the “make public” button in the top right corner of the project page. Anyone will be able to view and download all files. Step 11: Reference open science files in your work. Include the links in the manuscript, lab website or the published article to make the data accessible and useful. Abstract Sharing data, materials, and analysis scripts with reviewers and readers is valued in psychological science. To facilitate this sharing, files should be stored in a stable location, referenced with unique identifiers, and cited in published work associated with them. This Tutorial provides a step-by-step guide to using OSF to meet the needs for sharing psychological data. APA Style Reference Soderberg, C. K. (2018). Using OSF to share data: A step-by-step guide. Advances in methods and practices in psychological science, 1(1), 115-120. https://doi.org/10.1177/2515245918757689\nYou may also be interested in Trust Your Science? Open Your Data and Code (Stodden, 2011) Attitudes Toward Open Science and Public Data Sharing: A Survey Among Members of the German Psychological Society (Abele-Brehm et al., 2019) Willingness to Share Research Data Is Related to the Strength of the Evidence and the Quality of Reporting of Statistical Results (Wicherts et al., 2011) On supporting early-career black scholars (Roberson, 2020) ⌺ Main Takeaways: Non-Black researchers need to take immediate support for early-career Black scholars. “Maybe you were in a seminar where a Black doctoral student pushed back against a racist disciplinary norm, and you silently agreed and followed up with them afterwards to let them know that you support them.” This silence in public signals to Black scholars that they are not welcome in these spaces We must challenge white supremacy in academia. Speaking up about this is much more costly for Black scholars, who face an onslaught of racist micro- and macro-aggressions on a daily basis. The burden should not fall on their shoulders. We should be proactive in our outreach. We should invite early-career Black scholars, if they have expertise to improve a research project. Our careers and science will benefit from this help. “Do not just encourage [Black scholars] to apply, provide material support to promote our successful applications; share funded grants with [Black scholars], work with [Black scholars] on developing compelling aims pages, and write [Black scholars] a persuasive letter of support. Supporting [Black scholars] on manuscripts and funding opportunities can mitigate some of the barriers in science that often stunt Black success.” Inviting Black scholars will increase their credibility as experts and expand the audience’s familiarity with their scholarship. Manels are now being prohibited but we need to eliminate all-white speaker panels. Educate yourself on rising Black scholars in your field, learn from early-career Black researchers, investigate journals that publish their scholarships, be familiar with the Black community’s professional societies, affinity groups and diversify your following list on Twitter. Incorporate Black scholar’s work into your syllabi. This is necessary to eliminate structural racism. However, it requires individuals with the most amount of power. These steps will promote Black people to thrive among trainees and early-career scholars. This will remove barriers to promote a more inclusive environment! Quote “Do not just encourage [Black scholars] to apply, provide material support to promote our successful applications; share funded grants with [Black scholars], work with [Black scholars] on developing compelling aims pages, and write [Black scholars] a persuasive letter of support. Supporting [Black scholars] on manuscripts and funding opportunities can mitigate some of the barriers in science that often stunt Black success.” Abstract Professor Mya Roberson provides a detailed commentary about the struggles that Black people encounter in academia and starting steps to eliminate structural racism. APA Style Reference Roberson, M. L. (2020). On supporting early-career Black scholars. Nature Human Behaviour, 1-1. https://doi.org/10.1038/s41562-020-0926-6\nYou may also be interested in Open Science Isn’t Always Open to All Scientists (Bahlai et al., 2019) On the persistence of low power in psychological science (Vankov et al., 2014) Main Takeaways: Surveys of the literature have consistently shown that psychological studies have low statistical power and this problem has seen little, if any, improvement over the last few decades. Vankov et al. examine two arguments of why this may be the case. The first possible reason is that researchers may fail to appreciate the importance of statistical power, since null hypothesis significance testing is a hybrid of two statistical theories- Fisher’s and Neyman and Pearson’s. While researchers readily adhere to the 5% Type I error rate, they pay little attention to the Type II error rate. Both need to be considered when evaluating whether a result is true. A second possible reason is that scientists are humans and respond to incentives, such as the prestige of publishing a transformative study in a highly-regarded journal. However, producing such works is a high-risk strategy; a safer option may be to “salami-slice” works into multiple publications to increase the chance of producing publishable outputs. To examine the merit of the first reason, Vankov et al. contacted authors of published papers and asked them for their sample size rationale. One third of the contacted authors were found to hold beliefs that would typically act to reduce statistical power. There is a need for structural change, where editors and journals enforce rigorous requirements for statistical power. Journals introducing registered reports may also place greater emphasis on statistical power and robust designs. Abstract A comment by Dr Ivan Vankov, Professors Jeffrey Bowers and Marcus Munafo on the persistence of low power in psychological sciences. They discuss issues concerning false negatives, the importance of highly-regarded journals and that power is an issue to be discussed. They state that we need structural changes in journals in order to avoid the replicability crisis. APA Style Reference Vankov, I., Bowers, J., \u0026amp; Munafò, M. R. (2014). Article commentary: On the persistence of low power in psychological science. Quarterly journal of experimental psychology, 67(5), 1037-1040. https://doi.org/10.1080/17470218.2014.885986\nYou may also be interested in Is science really facing a reproducibility crisis, and do we need it to? (Fanelli, 2018) Registered Reports: Realigning incentives in scientific publishing (Chambers et al., 2015) Registered Reports: A new publishing initiative at Cortex (Chambers, 2013) Registered Reports: A step change in scientific publishing (Chambers, 2014) Rein in the four horsemen of irreproducibility (Bishop, 2019) Registered reports: a method to increase the credibility of published results (Nosek \u0026amp; Lakens, 2014) Registered reports (Jamieson et al., 2019) Publication Decisions and their possible effects on inferences drawn from tests of significance or vice versa (Sterling, 1959) Main Takeaways: There is a risk to reject the null hypothesis, as it can lead to type I error. “The experimenter who uses...tests of significance to evaluate observed differences usually reports that he has tested Ho by finding the probability of the experimental results on the assumption that Ho is true, and he does (or does not) ascribe some effect to experimental treatments”. (p.30). Depending on the confidence of methodology and data collection, readers can reject or accept the null hypothesis. Acceptance and rejection of null hypothesis is taken at p \u0026lt; .05. When a fixed level of significance is used as a criterion for publishing in professional journals, it may result in embarrassing and surprising results. Quote “What credence can then be given to inferences drawn from statistical tests of Ho if the reader is not aware of all experimental outcomes of a kind? Perhaps even more pertinent is the question: Can the reader justify adopting the same level of significance as does the author of a published study?” (p.33) Abstract There is some evidence that in fields where statistical tests of significance are commonly used, research which yields non-significant results is not published. Such research being unknown to other investigators may be repeated independently until eventually by chance a significant result occurs - an \u0026#34;error of the first kind\u0026#34; - and is published. Significant results published in these fields are seldom verified by independent replication. The possibility thus arises that the literature of such a field consists in substantial part of false conclusions resulting from errors of the first kind in statistical tests of significance. APA Style Reference Sterling, T. D. (1959). Publication decisions and their possible effects on inferences drawn from tests of significance—or vice versa. Journal of the American statistical association, 54(285), 30-34. https://doi.org/10.1080/01621459.1959.10501497\nYou may also be interested in Negative results are disappearing from most disciplines and countries (Fanelli, 2011) Negativity towards negative results: a discussion of the disconnect between scientific worth and scientific culture (Matosin et al., 2014) Replicability as a Publication Criterion (Lubin, 1957) Main Takeaways: How can publication lag be reduced? Researchers should perform replications to ensure the results are repeated. Replicability and generalisability should be used as a criteria to judge the rigour for these articles. If results are replicated, there is no need to discuss other trivial factors. However, if it is not replicated, you should discuss contextual factors such as the time of day. Abstract A commentary by Dr Ardie Lubin on replicability being perceived as a criterion of publication. Replications are perceived as fundamental but not enough to publish. However, replication studies are important to conduct in order to remove any trivial variables that may explain the findings. APA Style Reference Lubin, A. (1957). Replicability as a publication criterion. American Psychologist, 12(8), 519-520. https://doi.org/10.1037/h0039746\nYou may also be interested in Negative results are disappearing from most disciplines and countries (Fanelli, 2011) Negativity towards negative results: a discussion of the disconnect between scientific worth and scientific culture (Matosin et al., 2014) The life of p: “Just significant” results are on the rise (Leggett et al., 2013) Main Takeaways: Computers make it easy to analyse data. Modern software allows simple calculations, enabling them to monitor data, while collecting it. The ease of data analysis has made issues such as optional stopping and exclusion of outliers (i.e. p-hacking) easier to engage in. The current study measured whether there is an over-representation of p \u0026lt; .05 for 2005 than 1965. Method: P values were collected from all articles published between 1965 and 2005. Method: P values that were categorised as p \u0026lt; .05 and .01 were recalculated to provide a more exact p-value. Method: If there was a lack of information to determine the exact p value, the data for this specific p value was excluded from the analysis. Method: The distributions of p values used for the manuscript were between .01 and .10, any values outside of this range were excluded. Results: The frequency of p values at or below .05 was greater compared to p frequencies in other ranges. Results: Although there is an over-representation for p values below .05 between 1965 and 2005, there was a greater spike for p \u0026lt; .05 in 2005 than 1965. Results: In addition, p values close to but over .05 were more likely to be rounded down (e.g. p = .053 becomes p \u0026lt; .05) or incorrectly reported as significant in 2005 than in 1965. As a result of shifting research climates, there are changes in how statistical analyses are executed. Any values above .05 should be interpreted as non-significant, including trends such as .051. Suboptimal research practices are easier to engage in, as calculations have become easier to compute. Quote “The use of confidence intervals, along with effect sizes, as well as registered reporting and mandatory methods disclosure, might decrease the emphasis placed on p values. This would, in turn, also encourage the use of optimal research practices. In the absence of additional, complementary statistics or registered reports, the use of p values as an isolated method for determining statistical significance remains vulnerable to human fallibility.” (p.2309) Abstract Null hypothesis significance testing uses the seemingly arbitrary probability of .05 as a means of objectively determining whether a tested effect is reliable. Within recent psychological articles, research has found an overrepresentation of p values around this cut-off. The present study examined whether this overrepresentation is a product of recent pressure to publish or whether it has existed throughout psychological research. Articles published in 1965 and 2005 from two prominent psychology journals were examined. Like previous research, the frequency of p values at and just below .05 was greater than expected compared to p frequencies in other ranges. While this overrepresentation was found for values published in both 1965 and 2005, it was much greater in 2005. Additionally, p values close to but over .05 were more likely to be rounded down to, or incorrectly reported as, significant in 2005 than in 1965. Modern statistical software and an increased pressure to publish may explain this pattern. The problem may be alleviated by reduced reliance on p values and increased reporting of confidence intervals and effect sizes. APA Style Reference Leggett, N. C., Thomas, N. A., Loetscher, T., \u0026amp; Nicholls, M. E. (2013). The life of p:\u0026#34; just significant\u0026#34; results are on the rise. Quarterly journal of experimental psychology (2006), 66(12), 2303. https://doi.org/10.1080/17470218.2013.863371\nYou may also be interested in How scientists can stop fooling themselves (Bishop, 2020b) The Statistical Crisis in Science (Gelman \u0026amp; Loken, 2014) Only Human: Scientists, Systems, and Suspect Statistics A review of: Improving Scientific Practice: Dealing With The Human Factors, University of Amsterdam, Amsterdam, September 11, 2014 (Hardwicke et al., 2014) A 21 Word Solution (Simmons et al., 2012) Rein in the four horsemen of irreproducibility (Bishop, 2019) False-Positive Psychology: Undisclosed Flexibility in Data Collection and Analysis Allows Presenting Anything as Significant (Simmons et al., 2011) Many Analysts, One Data Set: Making Transparent How Variations in Analytical Choices Affect Results (Silberzahn et al., 2019) Why small low-powered studies are worse than large high-powered studies and how to protect against “trivial” findings in research: Comment on Friston (2012) (Ingre, 2013) Ironing out the statistical wrinkles in “ten ironic rules” (Lindquist et al., 2013) Ten ironic rules for non-statistical reviewers (Friston, 2012) Psychologists Are Open to Change, yet Wary of Rules (Fuchs et al., 2012) Main Takeaways: The present study investigated whether psychologists support concrete changes to data collection, reporting and publication processes. If not, what are their reasons? Method: 1292 psychologists from 42 countries were surveyed to assess whether each of Simmons et al.’s (2011) requirements and guidelines should be followed as a measure of good practice and whether these guidelines should be placed as mandatory conditions for publication in psychological journals. Results: 98% of psychologists are open to change and agreed at least one requirement should be placed as a condition for publication, especially that “researchers must report all experimental conditions run in a study, including failed manipulations”(p.641). Results: The reasons for not including a condition were: it was too rigorous; the condition did not agree with the argument; or the condition was not appropriate for all studies. Psychologists are open to change for reporting and conducting research, and agree with the guidelines. However, some requirements are too rigid and questionable. Quote “Researchers and editorial staff alike must also ensure that standards are enforceable so as to avoid punishing honest researchers. The psychological community should capitalize on the current openness to change in order to develop and implement appropriate changes and thus improve the quality of published psychological research.” (p. 641). Abstract Psychologists must change the way they conduct and report their research—this notion has been the topic of much debate in recent years. One article recently published in Psychological Science proposing six requirements for researchers concerning data collection and reporting practices as well as four guidelines for reviewers aimed at improving the publication process has recently received much attention (Simmons, Nelson, \u0026amp; Simonsohn, 2011). We surveyed 1,292 psychologists to address two questions: Do psychologists support these concrete changes to data collection, reporting, and publication practices, and if not, what are their reasons? Respondents also indicated the percentage of print and online journal space that should be dedicated to novel studies and direct replications as well as the percentage of published psychological research that they believed would be confirmed if direct replications were conducted. We found that psychologists are generally open to change. Five requirements for researchers and three guidelines for reviewers were supported as standards of good practice, whereas one requirement was even supported as a publication condition. Psychologists appear to be less in favor of mandatory conditions of publication than standards of good practice. We conclude that the proposal made by Simmons, Nelson \u0026amp; Simonsohn (2011) is a starting point for such standards. APA Style Reference Fuchs, H. M., Jenny, M., \u0026amp; Fiedler, S. (2012). Psychologists are open to change, yet wary of rules. Perspectives on Psychological Science, 7(6), 639-642.\nYou may also be interested in The Nine Circles of Scientific Hell (Neuroskeptic, 2012) Six principles for assessing scientists for hiring, promotion, and tenure (Naudet et al, 2018) CJEP Will Offer Open Science Badges (Pexman, 2017) Badges to Acknowledge Open Practices: A Simple, Low-Cost, Effective Method for Increasing Transparency (Kidwell et al., 2016) Only Human: Scientists, Systems, and Suspect Statistics A review of: Improving Scientific Practice: Dealing With The Human Factors, University of Amsterdam, Amsterdam, September 11, 2014 (Hardwicke et al., 2014) Quality Uncertainty Erodes Trust in Science (Vazire, 2017) Rein in the four horsemen of irreproducibility (Bishop, 2019) Seven Easy Steps to Open Science: An Annotated Reading List (Crüwell et al., 2019) Seven Steps Toward Transparency and Replicability in Psychological Science (Lindsay, 2020) Signalling the trustworthiness of science (Jamieson et al., 2020) Experimental power comes from powerful theories – the real problem in null hypothesis testing (Ashton, 2013) Main Takeaways: Although null hypothesis testing is a powerful tool for decision making, null hypothesis is no longer performed in how it was originally conceived. A power analysis of a specific desired effect size must be carried out prior to the experiment being conducted, wherein a null hypothesis is compared against an alternative hypothesis based on this effect size. This may not be possible with an effect size that was estimated from the data with no standard to compare with. The advice to increase sample size and statistical power is sound, this may make any hypothesis in neuroscience become virtually untestable. For instance, if we fail to find an effect at 10%, we need to increase power to detect a 1% or 0.1% effect. In turn, poorly testable and hard-to-refute hypotheses become difficult to restrain, thus making these hypotheses more prevalent in the literature. “The only way to resolve this dilemma while retaining the advantages of traditional null hypothesis testing is to be specific about the theoretical predictions that our experiments are designed to test” (p.1). Quote “The solution to the problem is to increase discipline not only in analysis and experimental design but also in relating experiments to explanatory theory. Much current practice instead seems to be an open-ended search for associations, reminiscent of old-style inductionism while superficially following the conventions of hypothetico-deductivism.” (p.1). Abstract A commentary by John C. Ashton who discusses the paper written by Professor Kate Button on small sample sizes. Ashton argues that power analyses and effects sizes should be used to estimate the alternative hypothesis. APA Style Reference Ashton, J. C. (2013). Experimental power comes from powerful theories—the real problem in null hypothesis testing. Nature Reviews Neuroscience, 14(8), 585-585. https://doi.org/10.1038/nrn3475-c2\nYou may also be interested in Negative results are disappearing from most disciplines and countries (Fanelli, 2011) Negativity towards negative results: a discussion of the disconnect between scientific worth and scientific culture (Matosin et al., 2014) Negative results are disappearing from most disciplines and countries (Fanelli, 2011) Main Takeaways: There are concerns that distort science. One concern is that the system disfavours negative findings, which gives a poor impression of science. The study investigated whether positive results have increased in the recent scientific literature. Method: 4600 papers in all disciplines between 1990 and 2007 were used, including variables such as frequency of papers to test a hypothesis and a report to support it. Method: Country of location was included, information on year of publication and country was coded. Whether the evidence was positive or negative. Results: Frequency of positive findings increased between 1990 and 2007 by 22%. This increase was larger in social and some biomedical disciplines. Results: There were fewer positive results published by American than Asian countries. More positive results in American than in European countries. Negative results decreased in frequency across disciplines due to publication bias. The authors seem to suggest that science is now closer to truth today than 20 years ago. There is an editorial bias that favours the United States that enables them to publish as many or more negative results than any other country, not fewer. The United States has a stronger bias against negative findings than Europe. Quote “However, even if in the long run truth will prevail, in the short term resources go wasted in pursuing exaggerated or completely false findings (Ioannidis 2006). Moreover, this self-correcting principle will not work efficiently in fields where theoretical predictions are less accurate, methodologies less codified, and true replications rare. Such conditions increase the rate of both false positives and false negatives, and a research system that suppresses the latter will suffer the most severe distortions.” (p.900) Abstract Concerns that the growing competition for funding and citations might distort science are frequently discussed, but have not been verified directly. Of the hypothesized problems, perhaps the most worrying is a worsening of positive-outcome bias. A system that disfavours negative results not only distorts the scientific literature directly, but might also discourage high-risk projects and pressure scientists to fabricate and falsify their data. This study analysed over 4,600 papers published in all disciplines between 1990 and 2007, measuring the frequency of papers that, having declared to have ‘‘tested’’ a hypothesis, reported a positive support for it. The overall frequency of positive supports has grown by over 22% between 1990 and 2007, with significant differences between disciplines and countries. The increase was stronger in the social and some biomedical disciplines. The United States had published, over the years, significantly fewer positive results than Asian countries (and particularly Japan) but more than European countries (and in particular the United Kingdom). Methodological artefacts cannot explain away these patterns, which support the hypotheses that research is becoming less pioneering and/or that the objectivity with which results are produced and published is decreasing. APA Style Reference Fanelli, D. (2012). Negative results are disappearing from most disciplines and countries. Scientometrics, 90(3), 891-904. https://doi.org/10.1007/s11192-011-0494-7\nYou may also be interested in A farewell to Bonferroni: the problems of low statistical power and publication bias (Nakagawa, 2004) Experimental power comes from powerful theories – the real problem in null hypothesis testing (Ashton, 2013) Negativity towards negative results: a discussion of the disconnect between scientific worth and scientific culture (Matosin et al., 2014) Negativity towards negative results: a discussion of the disconnect between scientific worth and scientific culture (Matosin et al., 2014) Main Takeaways: There is pressure on scientists to choose investigative avenues that result in high-impact knowledge (as opposed to hypothesis-driven). Negative results are not valued as positive results (positive results are valued, negative results are undervalued). Scientific principles are under reconsideration and there are events, in which new evidence refutes old hypotheses (cf. paradigm shift). Negative findings are seen as an inconvenient truth. Science is a collaborative endeavour and we should value and report negative findings When time is money, the current heuristic of judging research output on impact and citations can lead to waste of funds \u0026amp; time. It is commonplace for researchers to face resistance when presenting their work at scientific conferences. Then why is a negative finding viewed as a bad thing? What’s more, a negative result is often seen as philosophical rather than practical (voir real). If negative questions are rephrased as positive questions, does that mean a negative finding is a positive finding? Negative findings are seen as taboo and unworthy of publication in social sciences, but, for example, for clinical research, negative results are of absolute relevance and importance. Negative results are not worthy of attention, thus placed in a file drawer and seen as less important. Quote “It means that the direction of scientific research should not be determined by the pressure to win the ‘significance lottery’, but rather systematic, hypothesis-driven attempts to fill holes in our knowledge. At the core, it is our duty as scientists to both: (1) publish all data, no matter what the outcome, because a negative finding is still an important finding; and (2) have a hypothesis to explain the finding. If the experiment has been performed to plan, the data has not been manipulated or pulled out of context and there is compiled evidence of a negative result, then it is our duty to provide an explanation as to why we are seeing what we are seeing. Only by truly rethinking the current scientific culture, which clearly favours positive findings, will negative results be esteemed for their entire value. Only then can we work towards an improved scientific paradigm.” (p.173) Abstract “What gets us into trouble is not what we don’t know, it’s what we know for sure that just ain’t so.” – Mark Twain. Science is often romanticised as a flawless system of knowledge building, where scientists work together to systematically find answers. In reality, this is not always the case. Dissemination of results are straightforward when the findings are positive, but what happens when you obtain results that support the null hypothesis, or do not fit with the current scientific thinking? In this Editorial, we discuss the issues surrounding publication bias and the difficulty in communicating negative results. Negative findings are a valuable component of the scientific literature because they force us to critically evaluate and validate our current thinking, and fundamentally move us towards unabridged science. APA Style Reference Matosin, N., Frank, E., Engel, M., Lum, J. S., \u0026amp; Newell, K. A. (2014). Negativity towards negative results: a discussion of the disconnect between scientific worth and scientific culture. Disease Models \u0026amp; Mechanisms, 7(2), 171. https://doi.org/10.1242/dmm.015123\nYou may also be interested in Experimental power comes from powerful theories – the real problem in null hypothesis testing (Ashton, 2013) Negative results are disappearing from most disciplines and countries (Fanelli, 2011) A farewell to Bonferroni: the problems of low statistical power and publication bias (Nakagawa, 2004) Main Takeaways: There are several effect size measures: Cohen’s d and Pearson’s r. The former assesses the mean difference and the latter evaluates the strength of the relationship. Bonferroni correction tries to reduce false positives when multiple tests or comparisons are performed. Reviewers may demand a Bonferroni correction to remove irrelevant variables and reduce the number of false positives but it can still lead to publication bias. The scientific community should discourage Bonferroni or the idea that reviewers should demand a Bonferroni correction. These problems stem from a focus on statistical significance (i.e. p values) in journals instead of practical or biological significance (i.e. effect sizes). Researchers should be reporting effect sizes and the confidence intervals around these effect sizes. Abstract Professor Shinichi Nakagawa provides a commentary on low statistical power and the need to discourage Bonferroni corrections. In addition, we should rely on effect sizes and their confidence intervals to determine the value of science findings. APA Style Reference Nakagawa, S. (2004). A farewell to Bonferroni: the problems of low statistical power and publication bias. Behavioral ecology, 15(6), 1044-1045. https://doi.org/10.1093/beheco/arh107\nYou may also be interested in Experimental power comes from powerful theories – the real problem in null hypothesis testing (Ashton, 2013) Negativity towards negative results: a discussion of the disconnect between scientific worth and scientific culture (Matosin et al., 2014) The File-drawer problem revisited: A general weighted method for calculating fail-safe numbers in meta analysis (Rosenberg, 2005) Main Takeaways: There is a file-drawer problem in which studies are not published if they observe no significant effects. One measure to assess the number of non-significant findings is a fail-safe number. A fail-safe number is the number of non-significant, unpublished or missing studies would be needed to reduce the overall significant results to non-significant effect. If the fail-safe number is large compared to the number of observed studies, one can be confident in the summary conclusions. The method to calculate a fail-safe number is to calculate the significance of multiple studies by calculating the significance of the mean Z-score (Rosenthal, 1979), whereas Orwin (1983) calculate a fail-safe number based on an effect size that measures the standardised mean difference between intervention and control. It also calculates the number of additional studies to reduce an observed mean effect size to a desired minimal effect size. However, these approaches have several problems: “The first is that they are both explicitly unweighted. One of the primary attributes of contemporary meta-analysis is weighting; studies with large sample size or small variance are given higher weight than those with small sample sizes or large variance. Neither method accounts for the weight of the observed or the hypothesized unpublished studies. A second problem with Rosenthal’s method is that the method of adding Z-scores is not normally the method by which one combines studies in a meta-analysis; most modern meta-analyses are based on the combination of effect sizes, not simply significance values (Rosenberg et al. 2000). Rosenthal’s calculation is therefore not precisely applicable to the actual significance obtained from a meta-analysis. Orwin’s method is not based on significance testing; the choice of a desired minimal effect size to test the observed mean against seems unstable without a corresponding measure of variance” (pp.464-465). The article proposes a general, weighted fail-safe calculation framework applicable to both fixed- and random-effects models. The original fail-safe calculations are based on the fixed-effect model, but the authors estimate a fail-safe number for random-effects model meta-analysis. Although the number of studies of null effect are perceived as important to change a significant outcome in the fixed-effect calculations, these studies for the random-effects model involve a sum-of-squares calculation that we need to assume have effects that are precisely zero. This assumption could be partially avoided by simulating missing studies with a desired variance. Quote “One needs to remember that a fail-safe calculation is neither a method of identifying publication bias nor a method of accounting for publication bias that does exist. It is simply a procedure by which one can estimate whether publication biases (if they exist) may be safely ignored...While perhaps not as elegant as some of these methods, a fail-safe number is much simpler to calculate. Hopefully, the approach presented here will allow us to better estimate the potential for unpublished or missing studies to alter our conclusions; a low fail-safe number should certainly encourage researchers to pursue the more complicated publication bias methodologies.” (p.467). Abstract Quantitative literature reviews such as meta-analysis are becoming common in evolutionary biology but may be strongly affected by publication biases. Using fail-safe numbers is a quick way to estimate whether publication bias is likely to be a problem for a specific study. However, previously suggested fail-safe calculations are unweighted and are not based on the framework in which most meta-analyses are performed. A general, weighted fail-safe calculation, grounded in the meta-analysis framework, applicable to both fixed- and random-effects models, is proposed. Recent meta-analyses published in Evolution are used for illustration. APA Style Reference Rosenberg, M. S. (2005). The file‐drawer problem revisited: a general weighted method for calculating fail‐safe numbers in meta‐analysis. Evolution, 59(2), 464-468.https://doi.org/10.1111/j.0014-3820.2005.tb01004.x\nYou may also be interested in The “File Drawer Problem” and Tolerance for Null Results (Rosenthal, 1979) The “File Drawer Problem” and Tolerance for Null Results (Rosenthal, 1979) Main Takeaways: The file drawer problem is that 5% of articles are false positives, while file drawers have 95% non-significant results. Researchers need to calculate the number of studies with null findings before the overall false positives are made. A conservative alternative is to set Z = .00 when the exact p levels are not present for any non-significant findings, while setting Z = 1.645 when p \u0026lt; .05. “A small number of studies that are not very significant, even when their combined p is significant, may well be misleading in that only a few studies filed away could change the combined significant result to a nonsignificant one.” (p.640). Currently, there are no firm guidelines that can be given as to what constitute an unlikely number of unretrieved or unpublished studies. Quote “[...] more and more reviewers of research literature are estimating average effect sizes and combined ps of the studies they summarize. It would be very helpful to readers if for each combined p they presented, reviewers also gave the tolerance for future null results associated with their overall significance level.” (p.640) Abstract For any given research area, one cannot tell how many studies have been conducted but never reported. The extreme view of the \u0026#34;file drawer problem\u0026#34; is that journals are filled with the 5% of the studies that show Type I errors, while the file drawers are filled with the 95% of the studies that show nonsignificant results. Quantitative procedures for computing the tolerance for filed and future null results are reported and illustrated, and the implications are discussed. APA Style Reference Rosenthal, R. (1979). The file drawer problem and tolerance for null results. Psychological bulletin, 86(3), 638–641. https://doi.org/10.1037/0033-2909.86.3.638. [ungated]\nYou may also be interested in The File-drawer problem revisited: A general weighted method for calculating fail-safe numbers in meta analysis (Rosenberg, 2005) At random: Sense and Nonsense (McNemar, 1960) Main Takeaways: 1) To what extent have biologists entered into psychology? A random sample of 100 American Psychological Association (APA) members indicated that only 1% can be defined as fifth columnists for biology and from a random sample of 100 titles in Psychological Abstracts indicated that only 4% have a biological slant. This indicates that psychology has not risen to the level of biology. 2) To what extent have statisticians taken over of psychology? Using the aforementioned samples, 1% of the APA members call themselves statisticians and 5% of abstracts deal primarily with statistics. The development of a science depends largely on the invention of measuring instruments (e.g. The Thurstone and the Likert scaling techniques). However, “it does not require much imagination to predict that other instruments will lead to more and more unimaginative research. It is so easy to say: For your dissertation, why don\u0026#39;t you apply the ZANY to such and such groups?” (p.296). It is also important to look at the arsenal of statistics. The application of chi-square did not spread into psychology until the 1930s. However, the chi-square test was being misused, and its frequent misuse has contributed to some astoundingly leading to fallacious significance levels. In the late 1930s, the analysis of variance (ANOVA) invaded psychology and supporters of the ANOVA argued that the ANOVA would “rescue involve analysis of variance. Also, during the late 1930s, researchers seem to think that the more complex the design the better, although this complexity will introduce additional complexity of data interpretation. Too many users of the ANOVA argue that the “reaching of a mediocre level of significance as more important than any descriptive specification of the underlying averages” (p.297). In addition, it was argued that significance testing is necessary but not sufficient for the development of a science. In order to critically evaluate the literature and to plan their own research, it is important psychologists have a sound understanding of all commonly used statistical techniques. The teaching task is partly that we should maintain enthusiasm to sell but not oversell statistics. The research problem should come first, then at the design, the “available tools should be scrutinised but with the ever present though that there is merit in simplicity.” (p.299). Anonymous reviewer’s comment: This paper may not be the most informative in terms of how much readers would get out of it. Much of it is very idiosyncratic, written almost in a \u0026#34;stream of consciousness\u0026#34; kind of way with little organisation. While there are definitely some good arguments in there, they get a bit lost in the rest of the comments that were mostly motivated by issues in the 1930s- 1960s. A similar article from the same era with more clear messaging is: Bozarth, J. D., \u0026amp; Roberts, R. R. (1972). Signifying significant significance. American Psychologist, 27(8), 774–775. https://doi.org/10.1037/h0038034 and, with respect to misusing statistical tests: Gigerenzer, G. (2004). Mindless statistics. The Journal of Socio-Economics, 33(5), 587-606. https://doi.org/10.1016/j.socec.2004.09.033 Quote “Our reviewer, after noting that 9 of the 23 authors of Foundations were not psychologists, said that the problem of learning seemed to be the only one that a psychologist could call his own. He went on to point out that learning was too much concerned with statistical interpretation of empirical data. Then he concluded that \u0026#34;psychology as a science is now bankrupt\u0026#34; and should be turned over to two groups of receivers: the biologists (broadly defined) and the statisticians.” (p.295) Abstract A commentary by Dr Quinn McNemar who discusses sense and nonsense data, the difficulties of statistical teaching and the advancements of research design and statistics. APA Style Reference McNemar, Q. (1960). At random: Sense and nonsense. American Psychologist, 15(5), 295–300. https://doi.org/10.1037/h0049193\nYou may also be interested in Relevant references will be added soon Replication Report: Interpretation of levels of significance by psychological researchers (Beauchamp \u0026amp; May, 1964) Main Takeaways: Importance: one of the first papers dealing with replication in psychology. Study: Psychology lecturers were more cautious than psychology graduate students when making confidence judgments about research findings for the p value. Also, psychology lecturers and students were more confident of p values based on a sample of 100 than on a sample of 10. Methods: Subjects had to state the degree of belief in research findings as a function of associated p levels based on a sample sizes of 100 and 10. Subjects rated each of the 12 p levels (.001 to .90) for each sample size on a six point scale from 0 (i.e. complete absence of confidence or belief) to 5 (i.e. extreme confidence or belief). Results: “Effects due to sample size (S) and p levels (P) were significant in both studies (p \u0026lt; .005) and the groups effect was significant in the replication (p \u0026lt; ,025). In addition, the S X P interaction was significant in the replication (p \u0026lt; .005), indicating that differences in confidence related to sample sizes varied across p levels.” (p.272). Discussion: There was no significant cliff effect found in intervals following p \u0026lt; .05, .01, or any other p value. Abstract A commentary by Drs Kenneth Beauchamp and Richard May who investigated confidence judgments about research findings for p value. APA Style Reference Beauchamp, K. L., \u0026amp; May, R. B. (1964). Replication Report: Interpretation of Levels of Significance by Psychological Researchers. Psychological Reports, 14(1), 272-272. https://doi.org/10.2466/pr0.1964.14.1.272 [ungated]\nYou may also be interested in Further evidence for the Cliff Effect in the Interpretation of Levels of Significance (Rosenthal \u0026amp; Gaito, 1964) Further evidence for the Cliff Effect in the Interpretation of Levels of Significance (Rosenthal \u0026amp; Gaito, 1964) Main Takeaways: Importance: one of the first papers dealing with replication in psychology. There was a non-monotonicity decrease of confidence as p values increased. 11 graduate student subjects showed a greater degree of confidence in .05 than that at .03 level. This indicates that p \u0026lt; .05 level has rather special characteristics. Abstract A commentary by Drs Robert Rosenthal and John Gaito who investigated confidence judgments about research findings and confidence in the levels of significance. APA Style Reference Rosenthal, R., \u0026amp; Gaito, J. (1964). Further Evidence for the Cliff Effect in the Interpretation of Levels of Significance. Psychological Reports, 15(2), 570-570. https://doi.org/10.2466/pr0.1964.15.2.570\nYou may also be interested in Replication Report: Interpretation of levels of significance by psychological researchers (Beauchamp \u0026amp; May, 1964) Ten Simple Rules for Effective Statistical Practice (Kass et al., 2016) Main Takeaways: The 10 simple rules are: Abstract A commentary by Dr Robert Kass providing 10 rules about effective statistical practices and how to improve statistical practices. APA Style Reference Kass, R. E., Caffo, B. S., Davidian, M., Meng, X. L., Yu, B., \u0026amp; Reid, N. (2016). Ten Simple Rules for Effective Statistical Practice. Plos Computational Biology, 12(6), e1004961-e1004961. https://doi.org/10.1371/journal.pcbi.1004961\nYou may also be interested in Relevant references will be added soon Registered Reports: Realigning incentives in scientific publishing (Chambers et al., 2015) Main Takeaways: Registered Reports allows peer reviews to focus on the quality and rigour of the experimental design instead of ground-breaking results. This should reduce questionable research practices such as selective reporting, post-hoc hypothesising, and low statistical power. Registered reports are reviewed and revised prior to data collection. A cortex editorial sub-team triages submissions within one week: to reject manuscripts; to invite for revision to meet the necessary standards; or to send out for Stage 1 in-depth review. It takes approximately 8-10 weeks for a Stage 1 Registered Report to move from “initial review” to “in-principle acceptance”. This also includes 1-3 rounds of peer reviews. Once the study is completed, it takes 4 weeks for a paper to move from Stage 2 review to final editorial decision. Registered reports are not a one-shot cure for reproducibility problems in science and pose no threat to exploratory analyses. Abstract This is a view on registered reports in Cortex by Professor Chris Chambers and colleagues. It contains information on Registered Reports and the length of duration for submission and review. They discuss the editorial process and that a registered report is not a threat to exploratory research and is not a panacea to cure reproducibility problems. APA Style Reference Chambers, C. D., Dienes, Z., McIntosh, R. D., Rotshtein, P., \u0026amp; Willmes, K. (2015). Registered reports: realigning incentives in scientific publishing. Cortex, 66, A1-A2.\nYou may also be interested in Registered Reports: A new publishing initiative at Cortex (Chambers, 2013) Registered Reports: A step change in scientific publishing (Chambers, 2014) Registered reports : a method to increase the credibility of published results (Nosek \u0026amp; Lakens, 2014) Registered reports (Jamieson et al., 2019) Rein in the four horsemen of irreproducibility (Bishop, 2019) Seven Easy Steps to Open Science: An Annotated Reading List (Crüwell et al., 2019) Seven Steps Toward Transparency and Replicability in Psychological Science (Lindsay, 2020) On the persistence of low power in psychological science (Vankov et al., 2014) Raise standards for preclinical cancer research (Begley \u0026amp; Ellis, 2012) Main Takeaways: Clinical trials in oncology have the highest failure rates. Low success rate is not sustainable or acceptable. Drug development heavily depends on literature. Clinical endpoints are defined in terms of patient survival focus instead of intermediate endpoints e.g. cholesterol levels for statins. It takes years before clinical applicability of the preclinical observation is known. Preclinical observation needs to withstand the challenges and rigorous nature of a clinical trial (e.g. blinding). Claims in a preclinical study needs to be taken at face value. Issue of irreproducible data has been discussed and received greater attention at costs of drug development. Researchers need to contact original authors for mixed findings, exchange reagents and repeat experiments under authors’ direction. In studies for which findings could be reproduced, authors pay close attention to controls, reagents, investigator bias and describing complete dataset. Researchers need commitment and change of prevalent cultures to increase the robustness of published preclinical cancer research. Researchers need to consider negative preclinical data and report all findings, irrespective of the outcome. Funding agencies, reviewers and journal editors should agree negative data is as informative as positive data. There are transparent opportunities for trainees, technicians and colleagues to discuss and report troubling or unethical behaviours without fearing adverse consequences. These should be reinforced and made easier and more general. There needs to be a greater dialogue between physicians, scientists, patient advocates and patients: scientists need to learn about clinical reality, whereas physicians need better knowledge of challenges and limitations of preclinical studies. And both groups would benefit from improved understanding of patients’ concerns. Institutions and committees should give more credit for teaching and mentoring, relying solely on publications for promotion or grant funding can be misleading and does not recognise the valuable contribution of greater mentors, educators and administrators. The Academic system and peer review process encourages erroneous, selective or irreproducible data. Abstract Glenn Begley and Lee M. Ellis propose how methods, publications and incentives must change if patients are to benefit. APA Style Reference Begley, C. G., \u0026amp; Ellis, L. M. (2012). Raise standards for preclinical cancer research. Nature, 483(7391), 531-533. https://doi.org/10.1038/483531a\nYou may also be interested in Relevant references will be added soon The cumulative effect of reporting and citation biases on the apparent efficacy of treatment: the case of depression (deVries et al., 2018) Main Takeaways: The authors analysed the cumulative influence of biases on efficacy and discussed remedies, using the evidence base for two effective treatments for depression: antidepressants and psychotherapy. “Trials that faithfully report non-significant results will yield accurate effect size estimates, but results interpretation can still be positively biased, which may affect apparent efficacy.” (p.2453) A spin occurs when a treatment is concluded to be effective, in spite of the fact that the results on the primary outcome was non-significant (e.g. concluding that treatment X was more effective than placebo, when it should be treatment X was not more effective than the placebo). Positive trials are more likely to be published (cf. publication bias) and significant outcomes are more likely to be included in a published trial, while negative outcomes are changed or removed. Put simply, negative outcomes are reported but in an overly positive manner that makes the negative outcome into a positive outcome (i.e. spin). Negative trials with either positive or mixed abstracts (e.g. concluding that the treatment was effective for one outcome but not another) were cited more often than those with negative abstracts. These findings indicate that the effects of different biases accumulate to hide non-significant results from view. Peer reviewers have an important role to ensure that important negative studies are cited and that the abstract accurately reports trial results. The peer reviewer can assess the study’s actual results, as opposed to their conclusions, and can conduct independent literature searches, since the authors’ reference list may have studies that disproportionately produce a number of positive findings. Quote “Close examination of registries by independent researchers may be necessary for registration to be a truly effective deterrent to study publication and outcome reporting bias. An alternative (or addition) to registration could be publication of study protocols or ‘registered reports’, in which journals accept a study for publication based on the introduction and methods, before the results are known. Widespread adoption of this format might also help to prevent spin, by reducing the pressure that researchers might feel to ‘oversell’ their results to get published. Hence, adoption of registered reports might also reduce citation bias by reducing the tendency for positive studies to be published in higher impact journals.” (p.2455) Abstract Dr deVries and colleagues discuss the importance of a spin on clinical trials, citation biases for positive trials and the benefits of registered reports and pre-registration. APA Style Reference De Vries, Y. A., Roest, A. M., de Jonge, P., Cuijpers, P., Munafò, M. R., \u0026amp; Bastiaansen, J. A. (2018). The cumulative effect of reporting and citation biases on the apparent efficacy of treatments: the case of depression. Psychological medicine, 48(15), 2453-2455. https://doi.org/10.1017/S0033291718001873\nYou may also be interested in Likelihood of Null Effects of Large NHLBI Clinical Trials Has Increased over Time (Kaplan \u0026amp; Irvin, 2015) Likelihood of Null Effects of Large NHLBI Clinical Trials Has Increased over Time (Kaplan \u0026amp; Irvin, 2015) Main Takeaways: The article investigates whether null results have increased over time in the National Heart, Lung, and Blood Institute. Method: All large randomised controlled trials between 1970-2012 were identified. Method: Two independent searches to improve probability to accurately capture all related trials- one by study author and second by grant databases from 1970-2012. Results: 57% of papers were published prior to 2000 that showed benefit of intervention on primary outcome in comparison to only 2 among 25 (8%) trials published after 2000. Results: Industry co-sponsorship was not linked to benefit but pre-registration linked to null findings. Results: Pre-registration in clinical trials.gov was strongly related with the trend toward null findings. The probability of finding a treatment benefit decreased, as opposed to increased, as studies became more precise. Following the year 2000, file drawer problems became more prominent leading to over-reported positive findings. There is a need to have stricter reporting standards for biases and greater rigour to suppress positive outcomes. Quote “All post 2000 trials reported total mortality while total mortality was only reported in about 80% of the pre-2000 trials and many of the early trials were not powered to detect changes in mortality. The effects on total mortality were null for both pooled analyses of trials that were registered or not registered prior to publication (see data in online supplement) In addition, prior to 2000 and the implementation of Clinicaltrials.gov, investigators had the opportunity to change the p level or the directionality of their hypothesis post hoc. Further, they could create composite variables by adding variables together in a way that favored their hypothesis. Preregistration in ClinicalTrials.gov essentially eliminated this possibility.” (p.9). Abstract We explore whether the number of null results in large National Heart Lung, and Blood Institute (NHLBI) funded trials has increased over time. We identified all large NHLBI supported RCTs between 1970 and 2012 evaluating drugs or dietary supplements for the treatment or prevention of cardiovascular disease. Trials were included if direct costs \u0026gt;$500,000/year, participants were adult humans, and the primary outcome was cardiovascular risk, disease or death. The 55 trials meeting these criteria were coded for whether they were published prior to or after the year 2000, whether they registered in clinicaltrials.gov prior to publication, used active or placebo comparator, and whether or not the trial had industry co-sponsorship. We tabulated whether the study reported a positive, negative, or null result on the primary outcome variable and for total mortality.17 of 30 studies (57%) published prior to 2000 showed a significant benefit of intervention on the primary outcome in comparison to only 2 among the 25 (8%) trials published after 2000 (χ2=12.2,df= 1, p=0.0005). There has been no change in the proportion of trials that compared treatment to placebo versus active comparator. Industry co-sponsorship was unrelated to the probability of reporting a significant benefit. Pre-registration in clinical trials.gov was strongly associated with the trend toward null findings.The number of NHLBI trials reporting positive results declined after the year 2000. Prospective declaration of outcomes in RCTs, and the adoption of transparent reporting standards, as required by clinicaltrials.gov, may have contributed to the trend toward null findings. APA Style Reference Kaplan, R. M., \u0026amp; Irvin, V. L. (2015). Likelihood of null effects of large NHLBI clinical trials has increased over time. PloS one, 10(8), e0132382. https://doi.org/10.1371/journal.pone.0132382\nYou may also be interested in The cumulative effect of reporting and citation biases on the apparent efficacy of treatment: the case of depression (deVries et al., 2018) Publication Pressure and Scientific Misconduct in Medical Scientists (Tijdink et al., 2014) Main Takeaways: There is increasing evidence that scientific misconduct compromises the credibility of science. There are different definitions and classifications for scientific misconduct: fabrication, falsification, plagiarism. Each of them is seen as fraud. Publication pressure is a risk factor for scientific misconduct but has not been studied. The present study addresses the relationship among publication pressure, self-reported fraud and questionable research practices. Method: All researchers received a survey and a publication pressure questionnaire that assessed scientific misconduct. Method: 315 Respondents provided demographic information on gender, age, type of specialty; years working as a scientist; appointment status; main professional activity and Hirsch index. Results: 15% of respondents admitted that they had fabricated, falsified and plagiarised or manipulated data. Results: Fraud was more common among younger scientists working in a university hospital. Results: 72% rated publication pressure as too high. Publication pressure was related to scientific misconduct severity score. Discussion: Publication pressure is a psychological stress. The pressure generated by this stress affects the amount of errors made in scientific research. The data is more suited for the identification of potential determinants for self-reported misconduct than as a measure of the prevalence of misconduct. Abstract There is increasing evidence that scientific misconduct is more common than previously thought. Strong emphasis on scientific productivity may increase the sense of publication pressure. We administered a nationwide survey to Flemish biomedical scientists on whether they had engaged in scientific misconduct and whether they had experienced publication pressure. A total of 315 scientists participated in the survey; 15% of the respondents admitted they had fabricated, falsified, plagiarized, or manipulated data in the past 3 years. Fraud was more common among younger scientists working in a university hospital. Furthermore, 72% rated publication pressure as “too high.” Publication pressure was strongly and significantly associated with a composite scientific misconduct severity score. APA Style Reference Tijdink, J. K., Verbeke, R., \u0026amp; Smulders, Y. M. (2014). Publication pressure and scientific misconduct in medical scientists. Journal of Empirical Research on Human Research Ethics, 9(5), 64-71. https://doi.org/10.1177/1556264614552421 [ungated]\nYou may also be interested in Fallibility in Science: Responding to Errors in the Work of Oneself and Others (Bishop, 2018) Signalling the trustworthiness of science should not be a substitute for direct action against research misconduct (Kornfeld \u0026amp; Titus, 2020) Reply to Kornfeld and Titus: No distraction from misconduct (Jamieson et al., 2020) Stop ignoring misconduct (Kornfeld \u0026amp; Titus, 2016) Scientists’ Reputations are Based on Getting it Right, not being Right (Ebersole et al., 2016) Check for publication integrity before misconduct (Grey et al., 2020) Using science and psychology to improve the dissemination and evaluation of scientific work (Buttliere, 2014) Main Takeaways: Buttliere advocates that the best way to optimise open science tools would be increasing their utility and lowering its costs and risks by centralizing existing individual and group efforts. This centralized platform should be easy to use, have a sophisticated public discussion space and impact metrics using the associated data. In order to be competitive, we have to publish in high impact journals. Being competitive can drive science and human progress but when we have questionable research practices, lack of open data and the file drawer problem, it is ineffective. Researchers invest hours to set up their profile, learn the interface and build up their network. Individuals post a paper, dataset, general comment, new protocol and shows up in the newsfeed of the system. Other researchers interact with the post and the system notifies the original poster and displays the content from the same source. If a question a researcher proposes is not found in the discussion of a paper or the subfield, the system could provide a list of experts to answer the question. To increase impact and reduce questionable research practices, we need individuals to engage with prosocial activities. Reviews should be done pre-publication and should privately provide feedback or reviews are made public and serve as a discussion of a certain number of comments. To help science, people should adopt the new system. Abstract Here I outline some of what science can tell us about the problems in psychological publishing and how to best address those problems. First, the motivation behind questionable research practices is examined (the desire to get ahead or, at least, not fall behind). Next, behavior modification strategies are discussed, pointing out that reward works better than punishment. Humans are utility seekers and the implementation of current change initiatives is hindered by high initial buy-in costs and insufficient expected utility. Open science tools interested in improving science should team up, to increase utility while lowering the cost and risk associated with engagement. The best way to realign individual and group motives will probably be to create one, centralized, easy to use, platform, with a profile, a feed of targeted science stories based upon previous system interaction, a sophisticated (public) discussion section, and impact metrics which use the associated data. These measures encourage high quality review and other prosocial activities while inhibiting self-serving behavior. Some advantages of centrally digitizing communications are outlined, including ways the data could be used to improve the peer review process. Most generally, it seems that decisions about change design and implementation should be theory and data driven. APA Style Reference Buttliere, B. T. (2014). Using science and psychology to improve the dissemination and evaluation of scientific work. Frontiers in computational neuroscience, 8, 82. https://doi.org/10.3389/fncom.2014.00082\nYou may also be interested in Relevant references will be added soon Bias against research on gender bias (Cislak et al., 2018) ⌺ Main Takeaways: Scientific inquiries often disregard the moderating roles of sex or gender. Moreover, some finding applies only to male participants, producing biased knowledge. Findings related to men may be irrelevant and harmful to women. Studies on gender bias are often met with lower appreciation in the scientific community compared to studies on race bias. The present study investigated whether research on gender bias is prone to biased evaluation resulting in fewer and less prestigious publications and fewer funding opportunities. The present study compared articles for gender bias and race bias in impact factor and grant support. Method: 1485 articles published in 520 journals were assigned a numerical value based on type of bias. Two peer review criteria were used: Impact factor and whether the article was supported by finding or not. Results: Articles on gender bias are funded less often and published in journals with lower Impact factor than articles on similar instances of social discrimination. Discussion: Results suggest that bias against gender bias research is not merit based but reflects the topic\u0026#39;s lower prestige and appreciation due to a generalised gender bias. Another potential explanation for the observed difference in grant funding is the relative difference in availability of participant samples. Recruiting racially diverse samples may be more difficult, time-consuming and costly, while recruiting gender-diverse samples does not have similar issues. It is less plausible, however, that differences in participant samples affect researcher’s decisions of the outlet for their work. It may be that researchers are aware of bias against gender bias research and consider their own work less suitable for more prestigious journals. Research on gender bias is more often reviewed by male researchers than research on race bias. Rejection by more prestigious journals show subtle bias in perceived quality of studies evidencing gender discrimination. Quote “This discussion is primarily important in order for gender bias to be properly acknowledged within the scientific community and to pursue further examination of this powerful source of inequality that severely affects many women in the world.” (p. 200) Abstract The bias against women in academia is a documented phenomenon that has had detrimental consequences, not only for women, but also for the quality of science. First, gender bias in academia affects female scientists, resulting in their underrepresentation in academic institutions, particularly in higher ranks. The second type of gender bias in science relates to some findings applying only to male participants, which produces biased knowledge. Here, we identify a third potentially powerful source of gender bias in academia: the bias against research on gender bias. In a bibliometric investigation covering a broad range of social sciences, we analyzed published articles on gender bias and race bias and established that articles on gender bias are funded less often and published in journals with a lower Impact Factor than articles on comparable instances of social discrimination. This result suggests the possibility of an underappreciation of the phenomenon of gender bias and related research within the academic community. Addressing this meta-bias is crucial for the further examination of gender inequality, which severely affects many women across the world. APA Style Reference Cislak, A., Formanowicz, M., \u0026amp; Saguy, T. (2018). Bias against research on gender bias. Scientometrics, 115(1), 189-200. https://doi.org/10.1007/s11192-018-2667-0\nYou may also be interested in Surviving (thriving) in academia: feminist support networks and women ECRs (Macoun \u0026amp; Miller, 2014) Global gender disparities in science (Lariviere et al., 2013) The Pandemic and Gender Inequality in Academia (Kim \u0026amp; Patterson, Jr, 2020)◈ Gender in the editorial boards of scientific journals: A study on the current state of the art (Ghasemi et al., 2020) ◈ Something’s Got to Give (Flaherty, 2020) ◈ When it is fine to fail/ Irreproducibility is not a sign of failure, but an inspiration for fresh ideas (Anon, 2020) Main Takeaways: The past decade has seen a growing recognition that results must be independently replicated before they can be accepted as true. It is argued that a focus on reproducibility is necessary in the physical sciences as well, although it should be viewed through slightly different lenses. Questions in biomedicine and in the social sciences do not reduce as cleanly to the determination of a fundamental constant of nature as questions in physical sciences. As a result, attempts to reproduce results may include many sources of variability, which are hard to control for. Experimental results of replications may question long-held theories or point to the existence of another theory altogether. It is important to be cautious about assuming something is inherently wrong when researchers cannot reproduce a result when adhering to the best agreed standards. When attempting to reproduce previous results, it helps to build trust and confidence in the research process. Researchers from different domains must talk and share the experiences of reproducibility. Quote “Irreproducibility should not automatically be seen as a sign of failure. It can also be an indication that it’s time to rethink our assumptions.” (p.192) Abstract The history of metrology holds valuable lessons for initiatives to reproduce results. APA Style Reference Anon (2020). It is fine to fail/Irreproducibility is not a sign of failure, but an inspiration for fresh ideas. Nature, 578, 191-192. https://doi.org/10.1038/d41586-020-00380-2\nYou may also be interested in Relevant references will be added soon Signalling the trustworthiness of science (Jamieson et al., 2020) Main Takeaways: Authors argue that trust in science increases when scientists abide by the scientific norms. Scientists reinforce trust in science when they promote the use and value of evidence, transparent reporting, self-correction, replication, a culture of critique, and controls for bias”. There are already a number of practical ways scientists and scientific outlets have at their disposal to signal the trustworthiness of science: article badging, checklists, a more extensive withdrawal ontology, identity verification, better forward linking, and greater transparency. The research community has started to thwart human biases and increase trustworthiness of scholarly work. Scientists, policy makers and public base their decisions on inappropriate grounds such as irrational biases, non-scientific beliefs and misdirections by conflicted stakeholders and malicious actors. It is important to communicate the value of scientific practices more explicitly and transparent to clarify misconceptions of science. Scientific advances are built on previous work with new technological revolutions, new areas of research. As a result of these new approaches, interpretations can be corrected and advanced. Central to this progress of science is a culture of critique, replication and independent validation of results, and self correction. Science discourages group-think, countermands, human biases and rewards a dispassionate stance to the subject and institutionalised organised scepticism but fosters competition for scientists to replicate and challenge each other’s work. To validate and build on the results of others, it is important to archive data and analysis plans in publicly available repositories. Retraction statements to allow the issues that led to the retraction to be known and who was responsible for the paper’s shortcomings. If an official investigation commences, it can help the blame be narrowed as opposed to generalised to all authors (cf. CRediT, as it allows us to look and identify the contributor who caused this issue, without blaming all the authors). We should also use a neutral term that encourages vigilance without disincentivizing disclosure such as relevant interest or relevant relationships, as opposed to conflict of interest, to indicate that not all ties are necessarily corrupt. To complement peer review, badges, checklist, plagiarism and image manipulations, independent statistics and verification that authors comply with community endorsed reporting and archiving standards checks are used to signal trustworthiness of findings. Authors organize their thinking in their helpful Table 1, where they describe 3 dimensions (competence, integrity, and benevolence) that communicate the level of trust warranted by an individual study, as well as their associated norms and examples of violation, while fleshing out the role of stakeholders. Quote “Science enjoys a relatively high level of public trust. To sustain this valued commodity, in our increasingly polarized age, scientists and the custodians of science would do well to signal to other researchers and to the public and policy makers the ways in which they are safeguarding science’s norms and improving the practices that protect its integrity as a way of knowing...beyond this peer-to-peer communication, the research community and its institutions also can signal to the public and policy makers that the scientific community itself actively protects the trustworthiness of its work.” (p.19235) Abstract Trust in science increases when scientists and the outlets certifying their work honor science’s norms. Scientists often fail to signal to other scientists and, perhaps more importantly, the public that these norms are being upheld. They could do so as they generate, certify, and react to each other’s findings: for example, by promoting the use and value of evidence, transparent reporting, self-correction, replication, a culture of critique, and controls for bias. A number of approaches for authors and journals would lead to more effective signals of trustworthiness at the article level. These include article badging, checklists, a more extensive withdrawal ontology, identity verification, better forward linking, and greater transparency. APA Style Reference Jamieson, K. H., McNutt, M., Kiermer, V., \u0026amp; Sever, R. (2019). Signaling the trustworthiness of science. Proceedings of the National Academy of Sciences, 116(39), 19231-19236.https://doi.org/10.1073/pnas.1913039116\nYou may also be interested in Seven Steps Toward Transparency and Replicability in Psychological Science (Lindsay, 2020) Psychologists Are Open to Change, yet Wary of Rules (Fuchs et al., 2012) CJEP Will Offer Open Science Badges (Pexman, 2017) Only Human: Scientists, Systems, and Suspect Statistics A review of: Improving Scientific Practice: Dealing With The Human Factors, University of Amsterdam, Amsterdam, September 11, 2014 (Hardwicke et al., 2014) Badges to Acknowledge Open Practices: A Simple, Low-Cost, Effective Method for Increasing Transparency (Kidwell et al., 2016) Quality Uncertainty Erodes Trust in Science (Vazire, 2017) Signalling the trustworthiness of science should not be a substitute for direct action against research misconduct (Kornfeld \u0026amp; Titus, 2020) Reply to Kornfeld and Titus: No distraction from misconduct (Jamieson et al., 2020) Seven Easy Steps to Open Science: An Annotated Reading List (Crüwell et al., 2019) On the reproducibility of meta-analyses: six practical recommendations (Lakens et al., 2014) Main Takeaways: Researchers on different sides of a scientific argument reach different conclusions in their meta-analyses of the same literature. The article recommends six recommendations that will increase the openness and reproducibility of meta-analyses. Quote APA Style Reference Abstract Meta-analyses play an important role in cumulative science by combining information across multiple studies and attempting to provide effect size estimates corrected for publication bias. Research on the reproducibility of meta-analyses reveals that errors are common, and the percentage of effect size calculations that cannot be reproduced is much higher than is desirable. Furthermore, the flexibility in inclusion criteria when performing a meta-analysis, combined with the many conflicting conclusions drawn by meta-analyses of the same set of studies performed by different researchers, has led some people to doubt whether meta-analyses can provide objective conclusions. The present article highlights the need to improve the reproducibility of meta-analyses to facilitate the identification of errors, allow researchers to examine the impact of subjective choices such as inclusion criteria, and update the meta-analysis after several years. Reproducibility can be improved by applying standardized reporting guidelines and sharing all meta-analytic data underlying the meta-analysis, including Quote from articles to specify how effect sizes were calculated. Pre-registration of the research protocol (which can be peer-reviewed using novel ‘registered report’ formats) can be used to distinguish a-priori analysis plans from data-driven choices, and reduce the amount of criticism after the results are known. The recommendations put forward in this article aim to improve the reproducibility of meta-analyses. In addition, they have the benefit of “future-proofing” meta-analyses by allowing the shared data to be re-analyzed as new theoretical viewpoints emerge or as novel statistical techniques are developed. Adoption of these practices will lead to increased credibility of meta-analytic conclusions, and facilitate cumulative scientific knowledge. APA Style Reference Lakens, D., Hilgard, J., \u0026amp; Staaks, J. (2016). On the reproducibility of meta-analyses: Six practical recommendations. BMC psychology, 4(1), 24. https://doi.org/10.1186/s40359-016-0126-3\nYou may also be interested in From pre-registration to publication: a non-technical primer for conducting meta-analysis to synthesize correlation data (Quintana, 2015) Specification Curve: Descriptive and Inferential Statistics on All Reasonable Specifications (Simonsohn et al., 2015) ◈ Main Takeaways: To convert a scientific hypothesis into a testable prediction, researchers make several decisions for data analysis. However, these decisions are affected by implicit decisions such as conflict of interest or trying to publish a result that tells a publishable story. This article introduces the Specification-Curve Analysis to reduce these problems. The steps include reporting results for “that (1) are consistent with the underlying theory, (2) are expected to be statistically valid, and (3) are not redundant with other specifications in the set.” (p.2). Without specification-curve analysis, researchers selectively report a few specifications in their papers. However, the decisions that are conducted are based on arbitrary analytical decisions, thus specification-curve analysis aims to reduce the influence of arbitrary analytical decisions, while preserving the influence of non-arbitrary analytical decisions. Competent researchers will disagree whether a data analysis is an appropriate test of the hypothesis of interest and/or statistically valid for the data at hand, specification-curve analysis will end debates about what data analysis to be conducted, but facilitate them further (cf. crowdsourcing; Tierney et al. ,2020; in press). There are three main steps for Specification-Curve Analysis. 1. Define the set of reasonable specifications to estimate. Estimate all specifications and report the results in a descriptive specification curve. Finally, conduct joint statistical tests using an inferential specification. A set of specifications can be produced by enumerating all data analytic decisions that are important to map the scientific hypothesis or construct of interest onto a statistical hypothesis, enumerating all reasonable alternative ways a researcher may make those decisions and generate the combination of decisions, to remove invalid and redundant combinations. Different conclusions from the same data can be interpreted by different researchers based on theoretically justified or statistically valid analyses or may reflect on arbitrary decisions on how the shared views of the researchers are operationalised. Specification allows us to help reach consensus on the latter. To solve this issue, we need to do more or different theory or training, not data analysis. Quote “The Specification-Curve Analysis, (i) provides a step-by-step guide to generate the set or reasonable specifications, (ii) aids in the identification of the source of variation in results across specifications via a descriptive specification curve... (iii) and provides a formal joint significance test for the family of alternative specifications, derived from expected distributions under the null...If different valid analyses lead to different conclusions, traditional pre-analysis plans lead researchers to blindly pre-commit to one vs the other conclusion by pre-committing to one vs another valid analysis, while Specification-Curve allows learning what the conclusion hinges on.” (pp.5-6). Abstract Empirical results often hinge on data analytic decisions that are simultaneously defensible, arbitrary, and motivated. To mitigate this problem we introduce Specification-Curve Analysis, which consists of three steps: (i) identifying the set of theoretically justified, statistically valid, and non-redundant analytic specifications, (ii) displaying alternative results graphically, allowing the identification of decisions producing different results, and (iii) conducting statistical tests to determine whether as a whole results are inconsistent with the null hypothesis. We illustrate its use by applying it to three published findings. One proves robust, one weak, one not robust at all. APA Style Reference Simonsohn, U., Simmons, J. P., \u0026amp; Nelson, L. D. (2020). Specification curve analysis. Nature Human Behaviour, 1-7.https://doi.org/10.1038/s41562-020-0912-z [ungated]\nYou may also be interested in False-Positive Psychology: Undisclosed Flexibility in Data Collection and Analysis Allows Presenting Anything as Significant (Simmons et al., 2011) Registered Reports: A new publishing initiative at Cortex (Chambers, 2013) Main Takeaways: We value novel and eye-catching findings over genuine findings, thus increasing questionable research practices. Editorial decisions are one cause of questionable research practices, as they make decisions based on results. Science undergraduates are taught about data analysis and hypothesis generation before the data is collected, ensuring the observer is independent of observation. Cortex provides registered reports to allow null results and encourage replication. Registered reports are manuscripts submitted before the experiment begins. This includes the introduction, hypotheses, procedures, analysis pipeline, power analysis, and pilot data, if possible. Following peer review, the article is rejected or accepted in principle for publication, irrespective of the obtained results. Authors have to submit a finalised manuscript for re-review, share raw data, and laboratory logs. Pending quality checks and a sensible interpretation of findings, the manuscript is, in essence, accepted. Registered reports are immune to publication bias and need authors to adhere to pre-approved methodology and analysis pipeline to prevent questionable research practices from being used. A priori power analysis is required and the criteria for a registered report is seen as providing the highest truth value. Registered reports do not exclude exploratory analyses but must be distinguished from the planned analyses. Not all modes of scientific investigation fit registered reports but most will. Abstract This is an editorial by Chris Chambers who encouraged Registered Reports in Cortex as a viable initiative to reduce questionable research practices, its benefits, limitations and what information to include in a registered report. APA Style Reference Chambers, C. D. (2013). Registered reports: a new publishing initiative at Cortex. Cortex, 49(3), 609-610. https://doi.org/10.1016/j.cortex.2012.12.016 [ungated]\nYou may also be interested in Registered Reports: A step change in scientific publishing (Chambers, 2014) Registered Reports: Realigning incentives in scientific publishing (Chambers et al., 2015) Registered reports : a method to increase the credibility of published results (Nosek \u0026amp; Lakens, 2014) Registered reports (Jamieson et al., 2019) Rein in the four horsemen of irreproducibility (Bishop, 2019) Seven Easy Steps to Open Science: An Annotated Reading List (Crüwell et al., 2019) Seven Steps Toward Transparency and Replicability in Psychological Science (Lindsay, 2020) On the persistence of low power in psychological science (Vankov et al., 2014) Prestige drives epistemic inequality in the diffusion of scientific ideas (Morgan et al., 2018) ⌺ Main Takeaways: There is no clear evidence that epistemic inequality is driven by non-meritocratic social mechanisms. It remains unknown how an idea spreads in the scientific community. If the origin does shape its scientific discourse, what is the relationship between the intrinsic fitness of the idea and its structural advantage by the prestige of origin? The present study takes a different approach to define how faculty hiring drives epistemic inequality and can determine which researchers are situated in which institutions and the origin of the idea. Method: 5032 tenured or tenure-track faculty data were collected. Data was collected from faculty hiring networks, nodes reflect university and the connections if a PhD was acquired at that university and if they held a tenure-track position. Networks with a self-loop contained individuals who received their PhD at the same institution and held a faculty position. Small departments have high placement power, while large departments have power. Elite institutions have a structural advantage. Faculty hiring may not contribute to the spread of every research idea. Hiring contributes to others. Faculty hiring is a possible mechanism for the diffusion of ideas in academia. The spread of information from a varying level of prestige for universities was investigated. Results: Research from prestigious institutions spreads more quickly and completely than work of similar quality originating from less prestigious institutions. Higher quality research from less prestigious universities has similar success as lower-quality research in more prestigious universities. Even when the assessment of an idea’s quality is objective, idea dissemination in academia is not meritocratic. Researchers at prestigious institutions benefit from structural advantage allowing ideas to be more easily spread throughout the network of institutions and impact discourse of science. Lower quality ideas are overshadowed by comparable ideas from more prestigious institutions, high-quality ideas circulate widely, irrespective of origin. Abstract The spread of ideas in the scientific community is often viewed as a competition, in which good ideas spread further because of greater intrinsic fitness, and publication venue and citation counts correlate with importance and impact. However, relatively little is known about how structural factors influence the spread of ideas, and specifically how where an idea originates might influence how it spreads. Here, we investigate the role of faculty hiring networks, which embody the set of researcher transitions from doctoral to faculty institutions, in shaping the spread of ideas in computer science, and the importance of where in the network an idea originates. We consider comprehensive data on the hiring events of 5032 faculty at all 205 Phd.-granting departments of computer science in the U.S. and Canada, and on the timing and titles of 200,476 associated publications. Analyzing five popular research topics, we show empirically that faculty hiring can and does facilitate the spread of ideas in science. Having established such a mechanism, we then analyze its potential consequences using epidemic models to simulate the generic spread of research ideas and quantify the impact of where an idea originates on its long-term diffusion across the network. We find that research from prestigious institutions spreads more quickly and completely than work of similar quality originating from less prestigious institutions. Our analyses establish the theoretical trade-offs between university prestige and the quality of ideas necessary for efficient circulation. Our results establish faculty hiring as an underlying mechanism that drives the persistent epistemic advantage observed for elite institutions, and provide a theoretical lower bound for the impact of structural inequality in shaping the spread of ideas in science. APA Style Reference Morgan, A. C., Economou, D. J., Way, S. F., \u0026amp; Clauset, A. (2018). Prestige drives epistemic inequality in the diffusion of scientific ideas. EPJ Data Science, 7(1), 40. https://doi.org/10.1140/epjds/s13688-018-0166-4\nYou may also be interested in Early co-authorship with top scientist predicts success in academic careers (Li et al., 2019) Open Science Isn’t Always Open to All Scientists (Bahlai et al., 2019) Scientists’ Reputations are Based on Getting it Right, not being Right (Ebersole et al., 2016) The Matthew effect in science funding (Bol et al., 2018) Open Science Isn’t Always Open to All Scientists (Bahlai et al., 2019) ⌺ Main Takeaways: Open science focuses on accountability and transparency, invites anyone to observe, contribute and create. Open science focuses on conviction that research performs in dialogue with society. Science is a mainstreamed but increasing sense of competition rewards scientists who discover ideas and publish findings. “... science traditionally has rewarded only scientists who are the first to discover ideas and publish findings, there is resistance to move from “closed” practices…” The broad term of Open Science and resulting vague scope is stalling the progress of the open science movement. We are now often caught up in detailed checklists about whether a project is “open” or not, rather than “focusing on the core goal of accountability and transparency.” All or nothing checklists reduce “the accessibility of science and may reify existing inequalities within this profession.” Open science makes science accessible to everyone but there are systemic barriers (e.g. financial and social) that make open science more accessible to some not others such as career stage, power imbalance, employment stability, financial circumstance, country of origin and cultural context. These barriers prevent scientists from pursuing further and should not be used to deny further participation, including receiving grant funding or job applications. “To truly achieve open science’s transformative vision, it must be universally accessible, so that all people have access to the dialogue of science. Accessible in this context means usable by all, with particular emphasis on communities often not served by scientific products.” Open science practices are not equally accessible to all scientists. aywalls make research inaccessible but Open Access processing fees may prevent scientists from sharing their work, as not all institutions/individuals have the resources to overcome these barriers. If open access is paid out of our personal funds, instead of grant or institution funding sources, it is an unsustainable solution for many scholars that do not have access to these funds. “Yet open tools, code, or data sets are often not valued the same as “normal” academic products, and therefore those who spend their limited time and resources on these products suffer a cost in how they are evaluated for current and future jobs.” Preprints and signed peer reviews may exacerbate inherent biases. “.. forcing transparency in practices that have traditionally operated in a “black box” may exacerbate inherent biases against women and people of color, especially women of color.” Making data available is seen as high risk as someone can publish analyses with your data before you can. Even “a small risk particularly affects members of the scientific community with fewer resources…” Quote “Power imbalance can play a large role in an individual’s ability to convince their research group to use openscience practices and as a result may cause them to not engage in these practices until they have stable employment or are in a senior position.” Abstract Current efforts to make research more accessible and transparent can reinforce inequality within STEM professions. APA Style Reference Bahlai, C., Bartlett, L. J., Burgio, K. R., Fournier, A., Keiser, C. N., Poisot, T., \u0026amp; Whitney, K. S. (2019). Open science isn’t always open to all scientists. American Scientist, 107(2), 78-82. https://doi.org/10.1511/2019.107.2.78\nYou may also be interested in Early co-authorship with top scientist predicts success in academic careers (Li et al., 2019) Prestige drives epistemic inequality in the diffusion of scientific ideas (Morgan et al., 2018) On supporting early-career black scholars (Roberson, 2020) The Matthew effect in science funding (Bol et al., 2018) Surviving (thriving) in academia: feminist support networks and women ECRs (Macoun \u0026amp; Miller, 2014) ⌺ Main Takeaways: This paper argues about how peer support networks may affect the experience of early-career scholars Women who participated in the Feminist Reading Group (FRG) are actively intellectually engaged in theorising their own experiences. The group perform functions linked to reading groups, create an informal space concerned with furthering disciplinary knowledge and developing academic skills. FRG members created a community of belonging among themselves, in which personal support, knowledge, and cultural and social capital were provided. Participants share resources and information about institutional processes and gain the confidence to navigate complex and hostile spaces of the University. School’s official spaces are seen as gendered and not reflective of our research interests or intellectual backgrounds. Participants state that FRG allowed them to continue their studies in times of difficulty. FRG provides opportunities to broaden exposure to other fields and improve critical thinking skills. FRG promotes the learn essential academic skills, since women are able to learn from experience with writing and publishing, and also developing presentation and analytical skills without fear of seeming to be an inadequate researcher. Academic work can be isolating and early career researchers frequently report feeling unsettled, anxious and experiencing self-doubt. FRG re-dresses this opacity and operates as an information sharing network for participants to learn about how things work at the University and in the department. Women graduates receive less mentoring, less involvement in professional and social networking than their male peers. Participation in the FRG also stimulated other academic activities, with members encouraging each other to attend conferences and present paper. Most participants were white, straight, cis-gendered and middle class. The group was whiter than our department as a whole. FRG provides participants with an opportunity to understand individual experiences of exclusion, exploitation, self-doubt, discrimination as shared and fundamentally political in character. Our backgrounds and experiences are not homogeneous, most participants in the reading group are racially and socio-economically privileged. Abstract In this paper, we reflect upon our experiences and those of our peers as doctoral students and early career researchers in an Australian Political Science department. We seek to explain and understand the diverse ways that participating in an unofficial Feminist Reading Group in our department affected our experiences. We contend that informal peer support networks like reading groups do more than is conventionally assumed, and may provide important avenues for sustaining feminist research in times of austerity, as well as supporting and enabling women and emerging feminist scholars in academia. Participating in the group created a community of belonging and resistance, providing women with personal validation, information and material support, as well as intellectual and political resources to understand and resist our position within the often hostile spaces of the University. While these experiences are specific to our context, time and location, they signal that peer networks may offer critical political resources for responding to the ways that women’s bodies and concerns are marginalised in increasingly competitive and corporatised university environments. APA Style Reference Macoun, A., \u0026amp; Miller, D. (2014). Surviving (thriving) in academia: Feminist support networks and women ECRs. Journal of Gender Studies, 23(3), 287-301. https://doi.org/10.1080/09589236.2014.909718\nYou may also be interested in Global gender disparities in science (Lariviere et al., 2013) The Pandemic and Gender Inequality in Academia (Kim \u0026amp; Patterson, Jr, 2020) Gender in the editorial boards of scientific journals: A study on the current state of the art (Ghasemi et al., 2020) Bias against research on gender bias (Cislak et al., 2018) Something’s Got to Give (Flaherty, 2020) Global gender disparities in science (Lariviere et al., 2013) ⌺ Main Takeaways: Gender inequality is still rife in science. There are gender inequalities in hiring, earnings, funding, satisfactions and patenting. Men publish more papers than women. There is no consensus whether gender differences are a result of bias, childbearing or other variables. The present state of quantitative knowledge of gender disparities in science was shaped by anecdotal reports and studies which are localized, monodisciplinary and dated. These studies take little account of changes in scholarly practices. The present study presents a cross-disciplinary bibliographic research to investigate (i) the relationship between gender and academic output, (ii) the extent of collaboration and (iii) the scientific impact of all articles published between 2008 and 2012 and indexed in the Thomson Reuters Web of Science databases Citation disadvantage is highlighted by the fact that women’s publication portfolios are more domestic than male colleagues and profit less from extra citations that international collaborations accrue. Men dominate scientific production in nearly every country (the extent of this domination varies by region). Women account for fewer than 30% fractionalised authorships, while men representation in such publications was more than 70%. Women are underrepresented when it comes to first authorships. For every article with a female first author, there are nearly two (1.93) articles first-authored by men. Female authorship is more prevalent in countries with lower scientific output. Female collaborations are more domestically oriented than collaborations of males from the same country. The present study analysed prominent author positions (sole, first- and last-authorship). When a woman was in any of these roles, paper attracted fewer citations than in cases wherein a man was in one of these roles. Academic pipeline from junior to senior faculty leaks female scientists. Thus it is likely that many of the trends we observed can be explained by the under-representation of women among the elders of science. Barriers to women in science remain widespread worldwide, despite more than a decade of policies aimed at levelling the playing field. For a country to be scientifically competitive, it needs to maximise its human intellectual capital. Collaboration is one of the main drivers of research output and scientific impact. Programmes fostering international collaboration for female researchers might help to level the playing field. No country can afford to neglect the intellectual contributions of half of its population. Abstract Cassidy R. Sugimoto and colleagues present a bibliometric analysis confirming that gender imbalances persist in research output worldwide. APA Style Reference Larivière, V., Ni, C., Gingras, Y., Cronin, B., \u0026amp; Sugimoto, C. R. (2013). Bibliometrics: Global gender disparities in science. Nature News, 504(7479), 211. https://doi.org/10.1038/504211a\nYou may also be interested in Surviving (thriving) in academia: feminist support networks and women ECRs (Macoun \u0026amp; Miller, 2014) The Pandemic and Gender Inequality in Academia (Kim \u0026amp; Patterson, Jr, 2020)◈ Gender in the editorial boards of scientific journals: A study on the current state of the art (Ghasemi et al., 2020) Bias against research on gender bias (Cislak et al., 2018) Something’s Got to Give (Flaherty, 2020) The Pandemic and Gender Inequality in Academia (Kim \u0026amp; Patterson, Jr, 2020)◈ ⌺ Main Takeaways: The COVID-19 pandemic worsened existing gender inequalities across society. The present study investigated the influence of the current pandemic requires addressing an academic publication pipeline best measured in months, if not years. If the pandemic disproportionately influences the productivity of female faculty, the effects on research productivity may not fully materialise for years and evaluation and promotion of female scholars could adversely be affected by gender-related inequalities woven into the system years before. The present study determined the proportion of work- and family-related tweets sent by male and female academics using subject-specific keywords. The pandemic caused the gender-related differences in professional tweeting to increase by 239%. The lockdown increased the gap between male and female faculty member’s propensity to tweet about family and care-giving. Women bear all care-giving activities- both men and women experienced an increase in family-related tweets- patterns we uncover reveal that female careers are more severely taxed by these commitments. Method: Our sample was narrowed to tenure-track or tenured faculty based in the United States, producing approximately 3000 handles. Method: We first identified all tweets related to career-promoting and family-related activities, and began with terms (e.g. publication, new paper, child care and home school). Each tweet was coded as work- and family-related or not. A more extensive set of keywords classified the entire corpus. Most papers and articles are shared on Twitter via URL, tweet was classified as work-related, if shared, URL address indicates file type, publication venue or data repository services. Results: Faculty members of both genders were affected by the pandemic, the gap in work-related tweets between male and female academics roughly tripled following the work-from-home. Variation in effects between junior and senior faculty indicates this relationship is not driven by an intrinsic gender difference. This effect is produced by gendered differences in adapting a work/life balance to the pandemic. Female academics who reach full professor have overcome existing barriers to gender equality in academia. Parenting obligations overshadow all other factors in limiting research productivity, indicating the influence of parenting on productivity. Increased efforts to address these deep-rooted inequalities, the cracks in the pipeline continue to loom large. Gender imbalances are less pronounced among the ranks of junior faculty, efforts to explain biases in early career trajectories would have the greatest long-term influence on the pipeline of female academics. Quote “With gender imbalances less pronounced among the ranks of junior faculty, efforts to account for biases in early career trajectories would have the greatest long-term impact on the pipeline of female academics. Moreover, as female role-models can positively influence young women’s propensities to enter male-dominated fields (Bonneau and Kanthak, 2018; Breda et al., 2020), administrators’ success or failure here could have downstream impacts on female representation in the academy for the next generation.” (p.15) Abstract Does the pandemic exacerbate gender inequality in academia? The temporal lag in publication pipeline complicates the effort to determine the extent to which women’s productivity is disproportionately affected by the COVID-19 crisis. We provide real-time evidence by analyzing 1.8 million tweets from approximately 3,000 political scientists, leveraging their use of social media for career advancement. Using automated text analysis and difference-in-differences estimation, we find that while faculty members of both genders were affected by the pandemic, the gap in work-related tweets between male and female academics roughly tripled following work-from-home. We further argue that these effects are likely driven by the increased familial obligations placed on women, as demonstrated by the increase in family-related tweets and the more pronounced effects among junior academics. Our causal evidence on work-family trade-off provides an opportunity for proactive efforts to address gender disparities that may otherwise take years to manifest. APA Style Reference Kim, E., \u0026amp; Patterson, S. (2020). The Pandemic and Gender Inequality in Academia. Available at SSRN 3666587. http://dx.doi.org/10.2139/ssrn.3666587\nYou may also be interested in Surviving (thriving) in academia: feminist support networks and women ECRs (Macoun \u0026amp; Miller, 2014) Global gender disparities in science (Lariviere et al., 2013) Gender in the editorial boards of scientific journals: A study on the current state of the art (Ghasemi et al., 2020) ◈ Bias against research on gender bias (Cislak et al., 2018) Something’s Got to Give (Flaherty, 2020) ◈ Gender in the editorial boards of scientific journals: A study on the current state of the art (Ghasemi et al., 2020) ◈ ⌺ Main Takeaways: There is a large number of studies on gender in academia, gender in membership of editorial boards of scientific journals garner attention of research and little literature. They make the policies and determine what is accepted for publication and what is not. Admission or rejection of articles influences the academic careers of authors: full professors or PhD students. Gender in editorial boards attracted attention from several researchers, albeit studies focus on journals of a specific field of knowledge. Works dealing with women and academia are addressed, those works focusing on editorial boards are reviewed. Male professors, male authors in journals and male dominance is higher than female counterparts. Women’s receipt of professional awards, prizes and funding increased in the past two decades. Men continue to win a higher proportion of awards and funding for scholarly research than expected based on the nomination pool. Stereotypes about women’s abilities, harsh self-assessment of scientific ability by women than by men; academic and professional climates dissatisfying to women and unconscious bias contribute to achieving fewer awards and funds. Female board representations have improved over time, is consistent across countries, and gendered subdisciplines attract higher female board representations. Inequities persist at the highest level: women are under-represented as editors and on boards of higher ranked journals. Three factors for women under-representation in editorial board: discipline, journal\u0026#39;s prestige and editor’s gender. The last 15 years hinders women’s ability to attain scholarly recognition and advancement and carries risk to the narrow nature and scope of research in the field. They all show a worrying trend of under-representation of women and agree on negative consequences for advancement of science. Abstract Gender issues have been studied in a broad range of fields and in many areas of society, including social relations, politics, labour, and also academia. However, gender in the membership of editorial boards of scientific journals is a topic that only recently has started to attract the attention of researchers, and there is little literature on this subject as of today. The objective of this work is to present a study of the current state of editorial boards with regard to gender. The methodology is based on a literature review of gender issues in academia, and more specifically in the incipient field of gender in editorial boards. The main findings of this work, according to the reviewed bibliography, are that women are underrepresented in academic institutions, that this underrepresentation is increasingly marked in higher rank positions in academia and in editorial boards, and that this carries the risk of narrowing the nature and scope of the research in some fields of knowledge. APA Style Reference Ghasemi, N. M., Perramon Tornil, X., \u0026amp; Simó Guzmán, P. (2019, March). Gender in the editorial boards of scientific journals: a study on the current state of the art. In Congrés Dones Ciència i Tecnologia 2019: Terrassa, 6 i 7 de març de 2019. http://hdl.handle.net/2117/134267\nYou may also be interested in Surviving (thriving) in academia: feminist support networks and women ECRs (Macoun \u0026amp; Miller, 2014) Global gender disparities in science (Lariviere et al., 2013) The Pandemic and Gender Inequality in Academia (Kim \u0026amp; Patterson, Jr, 2020)◈ Bias against research on gender bias (Cislak et al., 2018) Something’s Got to Give (Flaherty, 2020) ◈ Something’s Got to Give (Flaherty, 2020) ◈ ⌺ Main Takeaways: Women\u0026#39;s journal submission rates fell as their caring responsibilities increased due to COVID-19 (see also) based on data from ongoing study of article submissions to preprint databases, whose preliminary results were published in Nature’s Index. Submissions were up since COVID-19, but the share of submissions made by women was down. Submissions by women as first authors (often junior scholars) were especially down, with some indication that they were shifting to middle authors. Female first-author submissions to medRxiv, for example, dropped from 36% in December to 20% in April 2020. Senior and author submissions by women decreased 6% over the same period, while male senior author submissions rose 5%. Other researchers have found COVID-19 related papers in medicine and economics have fewer female authors than expected. At one journal, male authors outnumbered female authors by more than three to one. It was recommended by Melina R. Kibbe, editor of JAMA Surgery, that we should pause the tenure clock during the pandemic. However, critics of this approach have argued this can actually hurt, not help, women and under-represented minorities, as it can delay career progression and decrease lifetime earnings. The status quo is such that men win the COVID-19 game, whereas women, in general, lose. We need to allow part-time work. Different work shifts should be available to those who need them. And agencies should extend grant end dates and allow for increased funding carryover from year to year. Quote “In any case, Power said, the challenge “needs more thinking about and a bigger public conversation, because this situation is not going away fast.” That conversation is long overdue, she added, in that “women and carers are supposed to just fit into a system designed for people without caring responsibilities. There is a saying working mothers have: ‘You have to work like you don’t have children and parents like you don’t have a job.’ And that was before COVID-19.”” (p.10). Abstract Women\u0026#39;s journal submission rates fell as their caring responsibilities jumped due to COVID-19. Without meaningful interventions, the trend is likely to continue. APA Style Reference Flaherty, C. (2020, August, 20). Something\u0026#39;s Got to Give. Inside Higher Ed. Retrieved fromhttps://www.insidehighered.com/news/2020/08/20/womens-journal-submission-rates-continue-fall\nYou may also be interested in Surviving (thriving) in academia: feminist support networks and women ECRs (Macoun \u0026amp; Miller, 2014) Global gender disparities in science (Lariviere et al., 2013) The Pandemic and Gender Inequality in Academia (Kim \u0026amp; Patterson, Jr, 2020)◈ Bias against research on gender bias (Cislak et al., 2018) Gender in the editorial boards of scientific journals: A study on the current state of the art (Ghasemi et al., 2020) ◈ Publication metrics and success on the academic job market (Van Dijk et al., 2014) Main Takeaways: The number of applicants seeking academic positions vastly outnumber the available faculty positions. To date, there has not been a quantitative analysis of which characteristics lead researchers towards becoming a principal investigator (PI). Authors, based on their empirical results, defend that ‘success in academia’ is predictable, depending on number of publications, the journal’s impact factors (IF) of these publications and ratio between citations and IF. In addition, scientist’s gender and the rank of their university are important predictors suggesting that non-publication features play a statistically significant role in the academic hiring process. Method: The authors qualified more than 200 different metrics of publication output for authors who became Principal Investors and those who did not. Method: Whether or not a scientist becomes a scientist depends on the publication record, considering the first few years of publication and effect of each publication feature independent of other confounding variables. Results: Authors with more first-author publications and more papers in high impact journals are more likely to have higher h index and take less time to become principal investigators. Results: The actual number of citations is less predictive than journal impact factor.of becoming a Principal Investigator. Results: Authors with more first or second author publications are more likely to become Principal Investigators. However, if you have a lot of co-authors, less credit is given to this publication. Results: More middle author publications add little value in becoming a Principal Investigator, unless they are published in high impact journals. Results: Authors who take longer than seven years to become a Principal Investigator have more citations per paper than authors who become Principal Investigators more quickly. Results: Men are over-represented as Principal Investigator after correcting for all other publication and non-publication derived features, being male is positively predictive of becoming a Principal Investigator. Quality of publication is given more weight than its actual quality. The number of citations a publication receives is correlated with the impact factor of the journal. The authors found that citations/impact factor is the fourth most predictive feature after impact factor, number of publications and gender. These authors have a two-fold increase in their first-author publication rate relative to authors who do not become Principal Investigator, indicating that more first-author publications per year can compensate for lack of high impact factor publications. The Set of Principal investigator is enriched for scientists who attend higher-ranked universities, linked to many other features. It predicts becoming Principal Investigator independent of other publication features. Scientists from higher-ranked institutions become Principal Investigators before those from lower-ranked institutions. The author suggests that better universities attract better people and produce more Principal investigators. Quote “Our results suggest that currently, journal impact factor and academic pedigree are rewarded over the quality of publications, which may dis-incentivize rapid communication of findings, collaboration and interdisciplinary science.” (p.517) Abstract The number of applicants vastly outnumbers the available academic faculty positions. What makes a successful academic job market candidate is the subject of much current discussion 1, 2, 3, 4. Yet, so far there has been no quantitative analysis of who becomes a principal investigator (PI). We here use a machine-learning approach to predict who becomes a PI, based on data from over 25,000 scientists in PubMed. We show that success in academia is predictable. It depends on the number of publications, the impact factor (IF) of the journals in which those papers are published, and the number of papers that receive more citations than average for the journal in which they were published (citations/IF). However, both the scientist’s gender and the rank of their university are also of importance, suggesting that non-publication features play a statistically significant role in the academic hiring process. Our model (www.pipredictor.com) allows anyone to calculate their likelihood of becoming a PI. APA Style Reference Van Dijk, D., Manor, O., \u0026amp; Carey, L. B. (2014). Publication metrics and success on the academic job market. Current Biology, 24(11), R516-R517. https://doi.org/10.1016/j.cub.2014.04.039\nYou may also be interested in Six principles for assessing scientists for hiring, promotion, and tenure (Naudet et al, 2018) Faculty promotion must assess reproducibility (Flier, 2017) ⌺ Rewarding Research Transparency (Gernsbacher, 2018) The Matthew effect in science funding (Bol et al., 2018) Scientists’ Reputations are Based on Getting it Right, not being Right (Ebersole et al., 2016) Main Takeaways: What happens if my finding does not replicate? The success of replications depends on the methodology used. Many researchers argue that scientists should be evaluated only for things that they control (e.g. hypotheses, design, implementation, analysis and reporting). Scientists produce ideas and insights that drive the discovery of the results, but results are determined by reality. Exciting, innovative results are perceived as better than boring, incremental results. However, certain and reproducible results are better than uncertain and irreproducible results. It is ideal to have innovative and certain results. However, people prefer reproducible and boring findings than exciting but not reproducible results. The authors ask whether we should chase the next exciting findings or should we work to achieve greater certainty via replication and other strategies? How the scientist responds to other individuals’ replications or whether they pursue their own replication are closely tied to the reputation of the scientist. If self-replication failure was reported or if their research failed to be replicated but was pursued with follow-up research, the reputation of the scientist whose work was being replicated increases. A second survey was conducted between researchers and the general population. The same pattern of findings was observed. Abstract Replication is vital for increasing precision and accuracy of scientific claims. However, when replications “succeed” or “fail,” they could have reputational consequences for the claim’s originators. Surveys of United States adults (N = 4,786), undergraduates (N = 428), and researchers (N = 313) showed that reputational assessments of scientists were based more on how they pursue knowledge and respond to replication evidence, not whether the initial results were true. When comparing one scientist that produced boring but certain results with another that produced exciting but uncertain results, opinion favored the former despite researchers’ belief in more rewards for the latter. Considering idealized views of scientific practices offers an opportunity to address incentives to reward both innovation and verification. APA Style Reference Ebersole, C. R., Axt, J. R., \u0026amp; Nosek, B. A. (2016). Scientists’ reputations are based on getting it right, not being right. PLoS biology, 14(5), e1002460.\nYou may also be interested in Publication Pressure and Scientific Misconduct in Medical Scientists (Tijdink et al., 2014) Prestige drives epistemic inequality in the diffusion of scientific ideas (Morgan et al., 2018) Fallibility in Science: Responding to Errors in the Work of Oneself and Others (Bishop, 2018) Rewarding Research Transparency (Gernsbacher, 2018) Main Takeaways: Reproducibility of results is the active ingredient of any science, including cognitive science. To ensure reproducibility, cognitive scientists are increasingly taking steps such as pre-registering their studies’ goals and analytic plans toward research transparency. Taking steps to research transparency takes time and steps might not be rewarded. The author suggests ways to better reward research transparency in three phases: when hiring researchers for academic jobs, when evaluating researchers for academic promotion and tenure, and when selecting researchers for society and national awards: Abstract Cognitive scientists are increasingly enthusiastic about research transparency. However, their enthusiasm could be tempered if the research reward system fails to acknowledge and compensate these efforts. This article suggests ways to reward greater research transparency during academic job searches, academic promotion and tenure evaluations, and society and national award selections. APA Style Reference Gernsbacher, M. A. (2018). Rewarding research transparency. Trends in cognitive sciences, 22(11), 953-956. https://doi.org/10.1016/j.tics.2018.07.002\nYou may also be interested in Six principles for assessing scientists for hiring, promotion, and tenure (Naudet et al, 2018) Publication metrics and success on the academic job market (Van Dijk et al., 2014) Registered Reports: A step change in scientific publishing (Chambers, 2014) Main Takeaways: Registered reports foster clarity and replication before the experiment is conducted. Study protocols are reviewed before the experiments are conducted. Readers feel more confident that work is replicable with initial study predictions and analysis plans that were independently reviewed. Registered reports are a departure from traditional peer review. Low power, high rate of cherry picking, post-hoc hypothesising, lack of data sharing, journal culture marked by publication bias, and few replication studies, have contributed to the reproducibility crisis. Allows us to publish positive, negative, or null findings, thus producing a true picture of the literature. We will not suffer from publication bias, when a manuscript is worthy of publication, editors and reviewers are driven by the quality of the methods, as opposed to results. Registered reports are not an innovation but closer to restoration-reinvention of publication and peer review mechanisms. Registered reports allow creativity, flexibility and reporting of unexpected findings. Quote “Ultimately, it is up to all of us to determine the future of any reform, and if the community continues to support Registered Reports then that future looks promising. Each field that adopts this initiative will be helping to create a scientific literature that is free from publication bias, that celebrates transparency, that welcomes replication as well as novelty, and in which the reported science will be more reproducible.” (p. 3) Abstract Professor Chris Chambers, Registered Reports Editor of the Elsevier journal Cortex and one of the concept’s founders, on how the initiative combats publication bias. APA Style Reference Chambers, C. (2014). Registered reports: A step change in scientific publishing. Reviewers’ Update. November, 13, 2014. https://www.elsevier.com/reviewers-update/story/innovation-in-publishing/registered-reports-a-step-change-in-scientific-publishing\nYou may also be interested in Registered Reports: A new publishing initiative at Cortex (Chambers, 2013) Registered Reports: Realigning incentives in scientific publishing (Chambers et al., 2015) Registered reports : a method to increase the credibility of published results (Nosek \u0026amp; Lakens, 2014) Registered reports (Jamieson et al., 2019) Rein in the four horsemen of irreproducibility (Bishop, 2019) Seven Easy Steps to Open Science: An Annotated Reading List (Crüwell et al., 2019) Seven Steps Toward Transparency and Replicability in Psychological Science (Lindsay, 2020) On the persistence of low power in psychological science (Vankov et al., 2014) Fast Lane to Slow Science (Frith, 2020) Main Takeaways: Fast Science is bad for scientists and bad for science. Slow science may help us to make faster progress, how can we slow down? People hardly have time to read the original studies. There is little chance to cultivate broader interests: impairing mental health and well-being of a researcher. We lost talented people resulting in decreased diversity. Fast science cuts corners and contributes to the reproducibility crisis. We could set up a working group-a small conference where practical ideas could be discussed. We must look differently at timescales and consider bigger aims of science. Researchers need to be reminded that we contribute to a human effort that transcends an individual’s lifetime. We work for the sake of truth and for the benefit of society, as they have reason to believe science continuously improves our models of the world. A farsighted vision is important to create and test big theories, irrespective of obstacles. How funders view lengths of grant proposals and intervals for evaluations. Early career researchers believe they need to amass publications and grants. Established researchers assume that grants need to be maintained for their teams and facilities. Researchers need to be encouraged and rewarded for long-term projects that depend on collaborations and may not have a short-term pay-off. We must teach students about the history of science, its noble goals, how it moves forward through failure and success through collaboration and competition. Researchers should actively model thinking pauses. We need to inform researchers about regret and make them aware that in time they may feel similarly. Quality, as opposed to quantity, should be grounds for giving grants, for hiring people and promotion and awards. Quality feels too subjective and tainted by bias stems from being part or wishing to be part of high-status networks. How then do we assess quality, authors can be good judges of their work? Best papers have something new and fascinating to say in a well-argued theoretical framework, are concise and use simple languages. Collaborations are visible and replace the lone genius stereotype. New solutions to big problems can be found more readily when researchers of diverse skills and different viewpoints interact. This is not difficult, first, we need to achieve common ground and language. Need for vigilance to measure reliability and discriminate fact from fake. Engaging with those who bring different perspectives and make us aware of flaws in our theories and experiments. Why not develop a system that allows a listing in the manner of film credits? We need to restrict the number of grants anyone holds at any one time and limit the number of papers published per year. Funders, institutions and publishers regulate an initially voluntary triage to a prearranged number. New models of science communication overcome some problems of traditional journal articles and provide answers to tricky problems of credits. Doing less is better but we need to develop tools to measure quality. It would be exciting to set a goal and have content between those who continue in the fast lane and those who decide to switch lanes. Abstract Fast Science is bad for scientists and bad for science. Slow Science may actually help us to make faster progress, but how can we slow down? Here, I offer preliminary suggestions for how we can transition to a healthier and more sustainable research culture. APA Style Reference Frith, U. (2020). Fast lane to slow science. Trends in cognitive sciences, 24(1), 1-2. https://doi.org/10.1016/j.tics.2019.10.007\nYou may also be interested in Let’s Publish Fewer Papers (Nelson et al., 2012) Lessons for psychology laboratories from industrial laboratories (Gomez et al., 2017) Main Takeaways: This proposal does not discuss outright fraud and reaches well-intentioned researchers to produce the best possible scientific work. How can we increase the quality of the data in psychology and cognitive neuroscience laboratories? Behavioural and social scientists are not less ethical than scientists from other disciplines but noisiness of data obtained from human behaviour contributes to these fields’ problems. It is a research ethics imperative to reduce sources of noise in our data by implementing data quality systems. Academic laboratories do not have external controls and scientists rarely get trained in quality systems. Industrial laboratories have a very different culture as quality systems are widely used. High-quality standards are imperative for industrial activities, as there are external forces that cannot be ignored. Junior graduate students mess up times before they adopt their own quality habits and development of formal, explicit and enforceable quality policies would be beneficial for everyone involved, benefits would quickly outweigh costs of developing and enforcing these systems. Reduce waste of resources on failed studies, facilitate the adoption of open science practices and improve the signal to noise ratio in the data. Quality system needs of a group do survey-based studies might be different than the needs of a group collecting neurophysiological data. Quality Assessment should be the responsibility of senior members of the team, as this process is strategic, pre-planned and has a long-term time frame. A more stringent quality system would be to have an external group perform a quality verification audit. A laboratory could be audited by a buddy laboratory from either the same or different institution. Research could have verification badges the same way that some of the open science initiatives provide forms of certification for different levels of openness. Abstract In the past decade there has been a lot of attention to the quality of the evidence in experimental psychology and in other social and medical sciences. Some have described the current climate as a ‘crisis of confidence’. We focus on a specific question: how can we increase the quality of the data in psychology and cognitive neuroscience laboratories. Again, the challenges of the field are related to many different issues, but we believe that increasing the quality of the data collection process and the quality of the data per se will be a significant step in the right direction. We suggest that the adoption of quality control systems which parallel the methods used in industrial laboratories might be a way to improve the quality of data. We recommend that administrators incentivize the use of quality systems in academic laboratories. APA Style Reference Gomez, P., Anderson, A. R., \u0026amp; Baciero, A. (2017). Lessons for psychology laboratories from industrial laboratories. Research Ethics, 13(3-4), 155-160. https://doi.org/10.1177/1747016117693827\nYou may also be interested in Minimising Mistakes in Psychological Science (Rouder et al., 2018) Let’s Publish Fewer Papers (Nelson et al., 2012) Main Takeaways: Authors agree with Nosek and Bar-Anan’s (2012) “Scientific Utopia: I. Opening Scientific Communication” that there is no longer a need for page limits, long lags between acceptance and publication, and prohibitive journal subscription fees, but worry that when all findings are made available, (a) it is harder to discriminate the true findings from the false findings; and (b) there will be more false findings. We know authors file away less successful papers (file drawer problem and effect), leading to publication bias but we also need to focus on the cluttered office effect. In an office full of papers, it is hard to tell good manuscripts from bad papers. Not all researchers should receive equal consideration: “What is less often pictured is the paper that landed in the file drawer, not because of the vagaries of the publication process but because it reports a study that was ill-conceived, poorly run, or generally uninteresting.” The consequence is that the less established researcher is unlikely to be noticed and praised. Researchers seeking top jobs would better to comment on a paper by a famous and eminent researcher. Authors write: “When every paper is available, it becomes increasingly burdensome to find the good papers, and even harder to find the diamond in the rough—the paper that is not by a famous person, not from a famous school, and not in a popular research area.” Advancement, as described by the proposal by Nosek and Bar-Anan’s (2012) depends on the value of papers being rescued from the file drawer. For every good idea we have, we need to consider many bad ideas. It is a good idea to drop bad ideas before they mature into bad papers. Bad papers are easy to write but difficult to publish. However, it is easy to publish papers now, making it easier to introduce more bad manuscripts Some published papers are false-positives. An occasional false-positive manuscript is bad but lots of false positives are catastrophic to science and the field. False positives are hard to identify and correct. False positives produce severe costs on the scientific community, which is felt more by the field than the individual researcher. We reward researchers heavily for having new and exciting ideas, and less so for being accurate (cf. Ebersole et al., 2016). Researchers are trained to defeat the review process and conquer the publisher. Researchers are rewarded for the quantity of papers and less for the truth value of our shared knowledge. In a system that focuses on one paper per year, researchers can publish a paper on an effect that can be reliably obtained. The researcher would be able to pursue their own work with improved clarity and focus, as there is only one paper to write per year. It would also be easier to evaluate two candidates who differ in quality but are matched in quantity. Abstract This commentary is written by Professor Leif Nelson, Professor Jon Simons and Professor Uri Simonsohn about the importance of publishing fewer papers. APA Style Reference Nelson, L. D., Simmons, J. P., \u0026amp; Simonsohn, U. (2012). Let\u0026#39;s publish fewer papers. Psychological Inquiry, 23(3), 291-293. https://doi.org/10.1080/1047840X.2012.705245\nYou may also be interested in Fast Lane to Slow Science (Frith, 2020) Let’s Publish Fewer Papers (Nelson et al., 2012) Informal Teaching Advice (Bloom, 2020)◈ Main Takeaways: To be an excellent teacher. You need 12 points: 1. Enthusiasm. Act enthusiastic like there’s no place in the world you would rather be. You should enjoy the material more. This will make your audience interested and want to learn more. 2. Be confident. Even if you have practiced this talk 100 times, always act as if the talk has gone smashingly. 3. Mix it up. Throw in some movies, demos and so on to cure boredom and make the talk more interesting. 4. Bring in other people (e.g. guest lectures, interviews and debates). This will introduce variety to your course. 5. Be modest in goals for each class. Do not cram too much material in any single session. 6. Be yourself. Everyone has strengths. Use your strength to your advance and align it with the way you teach. 7. Teaching prep can leech away all the time. Don’t let it. 8. If you say well-timed “Great question. I don’t know but I’ll find out for next class”, it is perceived as charming and makes everyone feel good. 9. Use specific students as examples in arbitrary ways. 10. If a student asks a stupid question, don’t say they are stupid. Always respond with how interesting at a minimum level, no matter how off-topic. 11. Use concrete examples from your own life. They do not necessarily have to be true. 12. If you suffer from anxiety, self-medicate before teaching but do not get addicted. Abstract This commentary is written by Professor Paul Bloom about how to make your teaching more engaging with your students. APA Style Reference Bloom, P. (2020). Informal Teaching advice. https://www.dropbox.com/s/glm1agnxtz5tbww/informal-teaching-advice.pdf?dl=0\nYou may also be interested in Relevant references will be added soon The Matthew effect in science funding (Bol et al., 2018) Main Takeaways: Why is academic success so unequally distributed across scientists? One explanation is the Matthew effect (i.e. a scientist’s past success positively influences success in the future). For example, if one of two equally bright scientists is given an award, the award winning scholar will have a more successful career than the other equally bright scientist who did not receive an award. Put simply, the Matthew effect undermines meritocracy, by allowing an initially fortunate scientist to self-perpetuate, whereas an equally talented but less fortunate counterpart remains underappreciated. “First, we address the causal inference problem using a regression-discontinuity approach. Second, we systematically study the Matthew effect in science funding... third, we identify a participation mechanism driving the Matthew effect whereby early stage failure inhibits participation in further competition through discouragement and lack of resources.” (p.4887). Method: A single granting program, the Innovation Research Incentives Scheme, is the primary funding source for young Dutch scientists. This was used to assess the Matthew effect, as it provided a dataset containing all review scores and funding decisions of grant proposals. Method: “We isolate the effects of recent PhDs winning an early career “Veni” grant by comparing the subsequent funding success of nonwinners with evaluation scores just below the threshold to winners with scores just above it.” (p.4888). Results: A scientist with an early career award is 2.5 times more likely to win a mid-career award than those who did not obtain an early-career award. This effect was not due to superior proposal quality or scientific ability but early funding itself. Results: Winning an early-career grant explains 40% of differences in earning between the best and worst applicant and raises long-term prospects of becoming a professor by 47%. The funding of early-career researchers show a Matthew effect, as candidates who won prior awards are evaluated more positively than non-winners, whereas scientists who were successful in obtaining grants select themselves into applicant pools for the following grants at higher rates than unsuccessful researchers. Quote “Recent studies have documented rising inequality among scientists across the academic world (38, 39). Not only do our findings suggest that positive feedback in funding may be a key mechanism through which money is increasingly concentrated in the hands of a few extremely successful scholars, but also that the origins of emergent distinction in scientists’ careers may be of an arbitrary nature.” (p.4880) Abstract A classic thesis is that scientific achievement exhibits a “Matthew effect”: Scientists who have previously been successful are more likely to succeed again, producing increasing distinction. We investigate to what extent the Matthew effect drives the allocation of research funds. To this end, we assembled a dataset containing all review scores and funding decisions of grant proposals submitted by recent PhDs in a V2 billion granting program. Analyses of review scores reveal that early funding success introduces a growing rift, with winners just above the funding threshold accumulating more than twice as much research funding (€180,000) during the following eight years as nonwinners just below it. We find no evidence that winners’ improved funding chances in subsequent competitions are due to achievements enabled by the preceding grant, which suggests that early funding itself is an asset for acquiring later funding. Surprisingly, however, the emergent funding gap is partly created by applicants, who, after failing to win one grant, apply for another grant less often. APA Style Reference Bol, T., de Vaan, M., \u0026amp; van de Rijt, A. (2018). The Matthew effect in science funding. Proceedings of the National Academy of Sciences, 115(19), 4887-4890. https://doi.org/10.1073/pnas.1719557115\nYou may also be interested in Early co-authorship with top scientist predicts success in academic careers (Li et al., 2019) Prestige drives epistemic inequality in the diffusion of scientific ideas (Morgan et al., 2018) Open Science Isn’t Always Open to All Scientists (Bahlai et al., 2019) A user’s guide to inflated and manipulated impact factor (Ioannidis \u0026amp; Thombs, 2019) Publication metrics and success on the academic job market (Van Dijk et al., 2014) Minimising Mistakes in Psychological Science (Rouder et al., 2018) Main Takeaways: The article discusses a few practices to improve the reliability of scientific labs by focusing on what technologies and elements and reduce common and ordinary errors. Common, everyday and ordinary mistakes (e.g. reporting a figure based on incorrect data) can be detrimental to science and everyone has made these mistakes. We need to consider practices in high risk fields where mistakes can have devastating consequences (e.g. healthcare and military). We have organisations that research this type of management and how to reduce these risks through high reliability organisations and the high reliability organisation principles. Should our lab be a high reliability organisation? Yes. Although mistakes in the labs do not have life-or-death consequences, they can produce knowledge waste and can threaten our reputations. The principles of a high reliable organisation can transfer well to the academic lab setting. The principles of a high reliable organisation are: Quote “We have been practicing open science for about two years. It is our view that there are some not-so-obvious benefits that have improved our work as follows: There are many little decisions that people must make in performing research. To the extent that these little decisions tend to go in a preferred direction, they may be thought of as subtle biases. These decisions are often made quickly, sometimes without much thought, and sometimes without awareness that a decision has been made. Being open has changed our awareness of these little decisions. Lab members bring them to the forefront early in the research process where they may be critically examined. One example is that a student brought up outlier detection very early in the process knowing that not only would she have to report her approach, but that others could try different approaches with the same data. Addressing these decisions head on, transparently, and early in the process is an example of how practicing open science improves our own science.” (p.9). Abstract Developing and implementing best practices in organizing a lab is challenging, especially in the face of new cultural norms such as the open-science movement. Part of this challenge in today’s landscape is using new technologies such as cloud storage and computer automation. Here we discuss a few practices designed to increase the reliability of scientific labs by focusing on what technologies and elements minimize common, ordinary mistakes. We borrow principles from the Theory of High-Reliability Organizations which has been used to characterize operational practices in high-risk environments such as aviation and healthcare. From these principles, we focus on five elements: 1. implementing a lab culture focused on learning from mistakes; 2. using computer automation in data and meta-data collection wherever possible; 3. standardizing organization strategies; 4. using coded rather than menu-driven analyses; 5. developing expanded documents that record how analyses were performed. APA Style Reference Rouder, J. N., Haaf, J. M., \u0026amp; Snyder, H. K. (2019). Minimizing mistakes in psychological science. Advances in Methods and Practices in Psychological Science, 2(1), 3-11. https://doi.org/10.1177/2515245918801915\nYou may also be interested in Lessons for psychology laboratories from industrial laboratories (Gomez et al., 2017) Open Science at Liberal Arts Colleges (Lane et al., 2020)◈ Main Takeaways: Authors offer suggestions on how open science can be fertile when promoted among the faculty that works with undergraduates in classrooms and in labs – i.e., faculty at Small Liberal Arts Colleges (SLACS). Authors also discuss how to use open science to encourage a transformation of the institutional culture and the development of professionals around open science practices. SLACs\u0026#39; primary focus is the exceptionality of its undergraduate education (i.e., the integration of teaching and research defines the SLAC experience by meaningfully incorporating students into research, which is central to the institutional mission). Authors discuss that faculty engaging in open science practices may be hesitant to discuss the replicability crisis because of the worry students will lose trust in the field or are not knowledgeable or qualified enough about open science to teach it. However, authors argue, SLAC has small class sizes and an interactive and educational approach that facilitates productive and robust discussion about open science, thus encouraging critical thinking and well-rounded education. For example, open science can be included in statistics and advanced methods, as pre-registration can be used for students to plan their research question, hypothesis, methods and data analytic procedures before data collection begins. Open science should be studied as part of the liberal arts experience or as general education requirements, as transferable skills can be taught (e.g. framing questions, thinking critically, working collaboratively, grappling with data and communicating clearly). Students can be taught an explicit and transparent account of how discoveries are made, focusing primarily on the process itself, as opposed to the outcome. Pre-registration can be used as a checkpoint during the research process to ensure the students understand their project prior to data collection, especially when SLACS have limited participant pools. Open science will help the faculty at SLACs, as it allows the faculty to not compete in terms of quantity but also focus primarily on the research process, thus allowing for well-designed and robust studies and lines of research. Sharing data and material encourages more productive collaborations with other researchers, current and future generations of undergraduate students, as it allows us to systematically track, organise and share materials and data. This will encourage good practices for future students and will have ready access to materials that were used in a study that was conducted several years ago, saving the research mentor’s time and energy that otherwise would be used to track down analyses, datasets and questionnaires. SIPS: Inclusion is a primary focus on the Society of Improving Psychological Science: to make sure non-PhD granting institutions have a strong voice in its governance. Most projects at SIPS are reviewed with regard to diversity, including type and size of the institution. Quote “Sharing materials and data increases the trustworthiness of a research project and makes it possible for others to replicate our work. Sharing data requires greater accountability by researchers, who must demonstrate that they have handled the data properly, used data analysis tools adeptly, and did not overlook potential alternative explanations for their findings. The scrutiny that accompanies open science can begin to feel a little like inviting other researchers to look at how you’ve organized your bedroom closet.” (p.10). Abstract Adopting and sustaining open science practices is accompanied by particular opportunities and challenges for faculty at small liberal arts colleges (SLACs). Their predominantly undergraduate student body, small size, limited resources, substantial teaching responsibilities, and focus on intensive faculty-student interactions make it difficult to normalize open science at SLACs. However, given the unique synergy between teaching and research at SLACs, many of these practices are well-suited for work with undergraduate psychology students. In addition, the opportunities for collaboration afforded by the open science community may be especially attractive for those doing research at SLACs. In this paper, we offer suggestions for how open science can further grow and flourish among faculty who work closely with undergraduates, both in classrooms and in labs. We also discuss how to encourage professional development and transform institutional culture around open science practices. Most importantly, this paper serves as an invitation to SLAC psychology faculty to participate in the open science community. APA Style Reference Lane, K. A., Le, B., Woodzicka, J. A., Detweiler-Bedell, J., \u0026amp; Detweiler-Bedell, B. (2020, August 23). Open Science at Liberal Arts Colleges. https://doi.org/10.31234/osf.io/437c8\nYou may also be interested in Relevant references will be added soon How to prove that your therapy is effective, even when it is not: a guideline (Cuijpers \u0026amp; Cristea, 2016) Main Takeaways: Treatment guidelines use randomised trials to advise professionals to use specific interventions and not others, policymakers and health insurance companies use evidence to indicate whether or not a specific intervention should be adopted and implemented. “If this were your starting position, how could you make sure that the randomised trial you do actually results in positive outcomes that your therapy is indeed effective?” (p.1). In fact, if you attained one important method to optimise the chance, results of a trial are favourable. The placebo effect may lead to an expectation that a therapy works. Advertise your trial in the media as innovative, unique and the best among the available interventions. “Another thing that you have to learn when you want to optimise the effects found for your therapy is that randomised trials have ‘weak spots’, also called ‘risk of bias’.” (p.3) Consider the randomisation of participants – randomisation contributes to the trial – if participants are not randomised in groups, effects could be due to baseline differences between groups, not the intervention. There are two important factors of randomisation: random numbers should be generated – use coin toss for instance or allocation concealment – researchers conduct trial or assistant to assign participants to respond well to intervention to intervention group instead of to control group. Use non-blinded raters of clinical assessment of outcomes to influence outcomes of the trial. Also, there is the issue of attrition for individuals who do not respond to intervention or experience side effects. It does not help them, harm them, so why continue? Ignore attrition in analyses of outcomes and look exclusively at completers, participants who continued should be analysed. Therapy had better outcomes for patients who completed the therapy than individuals who dropped out of the therapy. The correct alternative is to include all participants who are randomised in the final analyses. Include multiple outcomes and analyse results so you can look at which outcome produced the best result and only report them, ignoring the other measures. Published articles can fail to mention trial registration number, not prompting readers to dig up available protocol and check for selective outcome reporting. You can say during presentations – the therapy works better than the existing one and user reports are positive but this is not examined in your manuscript. If you find null effects for this specific intervention, do not publish the findings. If you think this is unethical to the participant and funder, just remind yourself, many other researchers do it, so it is an acceptable strategy! Quote “Research on the effects on therapies is no exception to this predicament. Many published research findings were found not to be true when other researchers tried to replicate these finding. When you want to show that your therapy is effective, you can simply wait until a trial is conducted and published that does find positive outcomes. And then you can still claim that your therapy is effective and evidence-based.” (p.6) Abstract Suppose you are the developer of a new therapy for a mental health problem or you have several years of experience working with such a therapy, and you would like to prove that it is effective. Randomised trials have become the gold standard to prove that interventions are effective, and they are used by treatment guidelines and policy makers to decide whether or not to adopt, implement or fund a therapy. You would want to do such a randomised trial to get your therapy disseminated, but in reality your clinical experience already showed you that the therapy works. How could you do a trial in order to optimise the chance of finding a positive effect? Methods that can help include a strong allegiance towards the therapy, anything that increases expectations and hope in participants, making use of the weak spots of randomised trials (risk of bias), small sample sizes and waiting list control groups (but not comparisons with existing interventions). And if all that fails one can always not publish the outcomes and wait for positive trials. Several methods are available to help you show that your therapy is effective, even when it is not. APA Style Reference Cuijpers, P., \u0026amp; Cristea, I. A. (2016). How to prove that your therapy is effective, even when it is not: a guideline. Epidemiology and Psychiatric Sciences, 25(5), 428-435. https://doi.org/10.1017/S2045796015000864\nYou may also be interested in Most psychotherapies do not really work, but those that might work should be assessed in biased studies (Ioannidis, 2016) A guideline for whom? (Furukawa, 2016) Many Analysts, One Data Set: Making Transparent How Variations in Analytical Choices Affect Results (Silberzahn et al., 2019) Main Takeaways: The article investigates ‘what if scientific results are highly dependent on subjective decisions at the analysis stage’? It also addresses the current lack of knowledge on how much diversity in analytic choice there can be when different researchers analyse the same data, as well as whether these variations lead to different conclusions. The study reports the influence of analytical decisions on research findings obtained by 29 teams that analysed the same dataset to answer the same research question. The project had several key stages: Quote “The observed results from analyzing a complex data set can be highly contingent on justifiable, but subjective, analytic decisions. Uncertainty in interpreting research results is therefore not just a function of statistical power or the use of questionable research practices; it is also a function of the many reasonable decisions that researchers must make in order to conduct the research. This does not mean that analyzing data and drawing research conclusions is a subjective enterprise with no connection to reality. It does mean that many subjective decisions are part of the research process and can affect the outcomes. The best defense against subjectivity in science is to expose it. Transparency in data, methods, and process gives the rest of the community the opportunity to see the decisions, question them, offer alternatives, and test these alternatives in further research.” (p.354) Abstract Twenty-nine teams involving 61 analysts used the same data set to address the same research question: whether soccer referees are more likely to give red cards to dark-skin-toned players than to light-skin-toned players. Analytic approaches varied widely across the teams, and the estimated effect sizes ranged from 0.89 to 2.93 (Mdn = 1.31) in odds-ratio units. Twenty teams (69%) found a statistically significant positive effect, and 9 teams (31%) did not observe a significant relationship. Overall, the 29 different analyses used 21 unique combinations of covariates. Neither analysts’ prior beliefs about the effect of interest nor their level of expertise readily explained the variation in the outcomes of the analyses. Peer ratings of the quality of the analyses also did not account for the variability. These findings suggest that significant variation in the results of analyses of complex data may be difficult to avoid, even by experts with honest intentions. Crowdsourcing data analysis, a strategy in which numerous research teams are recruited to simultaneously investigate the same research question, makes transparent how defensible, yet subjective, analytic choices influence research results. APA Style Reference Silberzahn, R., Uhlmann, E. L., Martin, D. P., Anselmi, P., Aust, F., Awtrey, E., ... \u0026amp; Carlsson, R. (2018). Many analysts, one data set: Making transparent how variations in analytic choices affect results. Advances in Methods and Practices in Psychological Science, 1(3), 337-356.https://doi.org/10.1177/2515245917747646\nYou may also be interested in How scientists can stop fooling themselves (Bishop, 2020b) The Statistical Crisis in Science (Gelman \u0026amp; Loken, 2014) Only Human: Scientists, Systems, and Suspect Statistics A review of: Improving Scientific Practice: Dealing With The Human Factors, University of Amsterdam, Amsterdam, September 11, 2014 (Hardwicke et al., 2014) A 21 Word Solution (Simmons et al., 2012) Rein in the four horsemen of irreproducibility (Bishop, 2019) Seven Steps Toward Transparency and Replicability in Psychological Science (Lindsay, 2020) The life of p: “Just significant” results are on the rise (Leggett et al., 2013) Only Human: Scientists, Systems, and Suspect Statistics A review of: Improving Scientific Practice: Dealing With The Human Factors, University of Amsterdam, Amsterdam, September 11, 2014 (Hardwicke et al., 2014) False-Positive Psychology: Undisclosed Flexibility in Data Collection and Analysis Allows Presenting Anything as Significant (Simmons et al., 2011) Seven Easy Steps to Open Science: An Annotated Reading List (Crüwell et al., 2019) ls There a Positive Correlation between Socioeconomic Status and Academic Achievement? (Quagliata, 2008) ◈ ⌺ Main Takeaways: Poverty rates have been increasing together with a debate on socio-economic status. Parental income is an indicator of socio-economic status reflecting a potential for social and economic resources. Parental education is a component of socio-economic status. Learning in a meaningful context so at-risk students can immediately apply when they have learned and connect it to their own lives and individual experiences. Many dropouts are not only from low SES backgrounds but also from mismatched learning styles. SES affects children’s academic achievement. It is beneficial to determine the type of home environment, how educators will best support them at school. Learning environment must be structured to achieve the highest level of internal motivation from all students. School success is greatly determined by a family\u0026#39;s socio-economic status. American society may be failing to provide educational opportunities for every student and citizen irrespective of socio-economic background. Many poor students come to school without social and economic benefits available to most middle and high SES students. Sufficient resources for optimal academic achievement irrespective of socio-economic status. The educational system produces an intergenerational cycle of school failures and short change an entire future American society as a result of family socio-economic status. Method: 31 surveys were handed out and 13 were returned. Some of the answers include health/nutrition; level of IQ; motivation or lack of motivation of teacher; amount of parental support; class size; quality of instruction/teaching resources; support available in home; school; student disabilities; language; education in culture; style of learning exposure to style; gender; peer influence; natural ability; attendance; family loss of tragic event; pregnancy full term; expectations and teacher/student relationship were also considered. Method: Every teacher felt that the environment contributed most when considering academic achievement. Method: Additional variables for socio-economic status were included: attitude; self-confidence; need to please; desire to do better; love of learning; acceptance; economics in the home; stability of family; siblings; age of parent(s); age of student maturity; family involvement; importance placed on learning; cognitive level; family history; neighbourhood; modelling of good work; ethics; pride; choices made; resources available; parental achievement; attending pre-k; home literacy; received early intervention; good nutrition; health; high IQ; oral language development; self-care skills; family life; class dynamics; personality and mood on any given day tells a specific teacher what they can or cannot do on a given day. Results: The higher the socio-economic status, the higher the academic achievement. The current literature is not available as specific students in low socio-economic status homes have high academic achievement. Income, education and occupation are responsible for low academic achievement in many low SES families. Socio-economic status causes less time with children and a result of lower education level of a parent, students from families of higher economic status tend to have parents who read to and with them, parents more apt to talk to them about the world and offer them more cultural experiences, many of the students\u0026#39; struggle with reading comes from low SES and parents that struggle with reading. If a family does not have a good educational background or materials to use to work with their child, the child may suffer as a result of their environment. If education is not valued in the home, students will not value education, more expectation for higher education in higher classes. Abstract In this literature review, family environments of low socioeconomic status (SES) students were examined and a comparison made in learning styles between low and high achievers Socioeconomic factors such as family income, education, and occupation play major role in the academic achievement of all students. There is a positive correlation between SES and academic achievement. The conclusions of this review have implications for all educators as well as the entire future of American society. APA Style Reference Quagliata, T. (2008). Is there a positive correlation between socioeconomic status and academic achievement?. Paper: Education masters (p. 78). https://fisherpub.sjfc.edu/cgi/viewcontent.cgi?article=1077\u0026amp;context=education_ETD_masters\nYou may also be interested in Education and Socio-economic status (APA, 2017b) Ethnic and Racial minorities and socio-economic status (APA, 2017) Women and Socio-economic status (APA, 2010) Disability and Socio-economic status (APA, 2010) Lesbian, Gay, Bisexual and Transgender Persons \u0026amp; Socioeconomic Status (APA, 2010) #bropenscience is broken science (Whitaker \u0026amp; Guest, 2020) ⌺ ","date":1557010800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1600704203,"objectID":"1dc213bdd8099f75aae965f52024df0d","permalink":"https://forrt.org/summaries/open-reproducible/","publishdate":"2019-05-05T00:00:00+01:00","relpermalink":"/summaries/open-reproducible/","section":"summaries","summary":"The symbol ◈ stands for non-peer-reviewed work.\nThe symbol ⌺ stands for summaries on the topic of Diversity, Equity, and Inclusion.\nTrust Your Science? Open Your Data and Code (Stodden, 2011)◈ Main Takeaways: Computational results suffer from problems of errors in final published conclusions. In order to allow independent replication and reproducible work, release the scripts and data files, and if the researcher uses MATLAB for graphs etc, please provide the graphical user interface.","tags":null,"title":"Open and Reproducible Science Summaries","type":"docs"},{"authors":null,"categories":null,"content":"The symbol ◈ stands for non-peer-reviewed work.\nThe symbol ⌺ stands for summaries on the topic of Diversity, Equity, and Inclusion.\nEducation and Socio-economic status (APA, 2017b) ◈⌺ Main Takeaways: Children from low socio-economic status take longer to develop academic skills than children from higher socio-economic status groups (e.g. poor cognitive development), leading to poorer academic achievement. Children from low socio-economic status are less likely to attain experiences for the development of reading acquisition and reading competence. As a result of fewer learning materials and experiences at home, children from low socio-economic status enter high school with literacy skills 5 years behind their affluent age-matched peers. Children from lower socio-economic status households are twice as likely as those from high SES households to show learning related behaviour problems. High school dropout rate was evident in low-income families compared to high-income families. Placing low-socio-economic status students in higher-quality classrooms will help them earn more disposable income, more likely to attend college, live in affluent neighbourhoods and save more income for retirement. Students from low socio-economic status are less likely to have access to resources about colleges (e.g. career offices and familial experience with higher/further education) and are more at-risk of being in debt to student loans than their affluent peers. Low income students are less likely to succeed in STEM disciplines, 8 times less likely to obtain a bachelor’s degree by the age of 24 and have less career-related self-efficacy when it came to vocational aspirations than high income students. These problems are worsened for people of colour, women, people who are disabled and LGBTIQ-identified individuals. Abstract This fact sheet explains the impact socioeconomic status on educational outcomes. APA Style Reference APA (2017, July). Education and Socioeconomic Status [Blog post]. Retrieved from https://www.apa.org/pi/ses/resources/publications/education\nYou may also be interested in ls There a Positive Correlation between Socioeconomic Status and Academic Achievement? (Quagliata, 2008) Ethnic and Racial minorities and socio-economic status (APA, 2017) Women and Socio-economic status (APA, 2010) Disability and Socio-economic status (APA, 2010) Lesbian, Gay, Bisexual and Transgender Persons \u0026amp; Socioeconomic Status (APA, 2010) Ethnic and Racial minorities and socio-economic status (APA, 2017) ◈ ⌺ Main Takeaways: The relationship between SES, race and ethnicity is intimately intertwined. Communities are segregated by socio-economic status, race and ethnicity. Low economic development, poor health conditions and low levels of educational attainment are often comorbidities shared in these communities. Discrimination hinders social mobility of ethnic and racial minorities. In the US, 39% of African American children and adolescents, and 33% of Latino children and adolescents are living in poverty, which is more than double than the 14% poverty rate for non-Latino, White and Asian children and adolescents. Minority racial groups are more likely to experience multidimensional poverty than their White counterparts. American Indian/Alaska Native, Hispanic, Pacific Islander, and Native Hawaiian families are more likely than Caucasian and Asian families to live in poverty. “African Americans (53%) and Latinos (43%) are more likely to receive high-cost mortgages than Caucasians (18%; Logan, 2008).” (p.9). African American unemployment rates are double of Caucasian Americans. African American men working full time earn only 72% of Caucasian men\u0026#39;s average earnings, and 85% of earnings of Caucasian women. African Americans and Latinos are more likely to attend high-poverty schools than Asian Americans and Caucasians. From 2000 to 2013, dropout rates between racial groups narrowed significantly. High school dropouts were highest for Latinos, followed by African Americans and Whites. High achieving African American students may be exposed to less rigorous curriculums, attend schools with fewer resources, and have teachers who expect less of them academically than similarly situated Caucasian students. 12% of African American college graduates were unemployed, which is more than double the rate of unemployment among all college graduates in the same age range. Racial and ethnic minorities have worse health than that of White Americans. Health disparities stem from economic determinants, education, geography, neighbourhood, environment, lower-quality care, inadequate access to care, inability to navigate the system, provider ignorance or bias, and stress. “At each level of income or education, African American have worse outcomes than Whites. This could be due to adverse health effects of more concentrated disadvantage or a range of experiences related to racial bias (Braveman, Cubbin, Egerter, Williams, \u0026amp; Pamuk, 2010).” (p.10). In pre-retirement years, Hispanics and American Indians are much less likely than Whites, African Americans, and Asians to have any health insurance. Negative net worth, zero net worth, and not owning a home in young adulthood are linked to depressive symptoms independent of other socio-economic indicators. Hispanics and African Americans report a lower risk of psychiatric disorder relative to White counterparts, but those who become ill tend to have more persistent disorders. African Americans, Hispanics, Asians, American Indians, and Native Hawaiians have higher rates of post-traumatic stress disorders than Whites, which is not explained by Socio-economic status and a history of psychiatric disorders. However discrimination is factor that contributes to increasing mental health disorders among the Asian and African American communities (i.e., compared to the White community, African American communities are more frequently diagnosed with schizophrenia, a low prevalence but serious condition). Abstract Learn how socioeconomic status affects the lives of many racial and ethnic minorities. APA Style Reference APA (2017, July). Ethnic and Racial Minorities \u0026amp; Socioeconomic Status [Blog post]. Retrieved from https://www.apa.org/pi/ses/resources/publications/minorities\nYou may also be interested in ls There a Positive Correlation between Socioeconomic Status and Academic Achievement? (Quagliata, 2008) Education and Socio-economic status (APA, 2017b) Women and Socio-economic status (APA, 2010) Disability and Socio-economic status (APA, 2010) Lesbian, Gay, Bisexual and Transgender Persons \u0026amp; Socioeconomic Status (APA, 2010) Faculty promotion must assess reproducibility (Flier, 2017) ⌺ Main Takeaways: Inadequate training, increased competition, problems in peer review and publishing, and occasionally scientific misconduct are some of the variables behind irreproducible research in the biomedical field. Diverse causes make finding solutions for the problem of irreproducibility difficult, especially, as they must be implemented by independent constituencies including funders and publishers. Academic institutions can and must do better to make science more reliable. One of the most effective (but least discussed) measures is to change how we appoint and promote our faculty members. Promotion criteria has changed over time. Committees now consider how well a candidate participates in team science, but we still depend on imperfect metrics for judging research publications and our ability to assess reliability and accuracy is underdeveloped. Reproducibility and robustness are under-emphasised when job applicants are evaluated and when faculty members are promoted. Currently, reviewers of committees are asked to assess how a field would be different without a candidate’s contributions, and to survey a candidate’s accomplishment, scholarship, and recognition. The promotion process should also encourage evaluators to say whether they feel candidates’ work is problematic or over-stated and whether it has been reproduced and broadly accepted. If not, they should say whether they believe widespread reproducibility is likely or whether work will advance the field. Applicants should also be asked to critically evaluate their research, including unanswered questions, controversies and uncertainties. This signals the importance of assessment and creates a mechanism to judge a candidate’s capacity for critical self-reflection. Evaluators should be asked to consider how technical and statistical issues were handled by candidates. Research and discovery is not simple and unidirectional, and evaluators should be sceptical of candidates who oversimplify. Institutions need to incentivise data sharing and transparency. Efforts are more urgent as increasingly interdisciplinary projects extend beyond individual investigators’ expertise. Success will need creativity, pragmatism and diplomacy, because investigators bristle at any perceived imposition on their academic freedom. Quote “Over time, efforts to increase the ratio of self-reflection to self-promotion may be the best way to improve science. It will be a slog, but if we don’t take this on, formally and explicitly, nothing will change.” (p.133) Abstract Research institutions should explicitly seek job candidates who can be frankly self-critical of their work, says Jeffrey Flier. APA Style Reference Flier J. (2017) Faculty promotion must assess reproducibility. Nature, 549(7671),133. https://doi.org/10.1038/549133a\nYou may also be interested in Publication metrics and success on the academic job market (Van Dijk et al., 2014) Six principles for assessing scientists for hiring, promotion, and tenure (Naudet et al, 2018) Women and Socio-economic status (APA, 2010)◈ ⌺ Main Takeaways: Socioeconomic status encompasses quality of life attributes and opportunities and privileges afforded to people in society. Socio-economic status is a consistent and reliable predictor of outcomes across lifespan. Low socio-economic status and its correlates (e.g., lower educational achievement, poverty and poor health) affect society. Inequities in health distribution, resource distribution and quality of life are increasing in the US and globally. Socio-economic status is a key factor in determining the quality of life for women and, by extension, strongly affects the lives of children and families. Inequities in wealth and quality of life for women are long-standing and exist both locally and globally. Women are more likely to live in poverty than men. Men are paid more than women despite similar levels of education and fields of occupation. Reduced income for women coupled with longer life expectancy and increased responsibility to raise children, increase probabilities that women face economic disadvantages. Pay gap has narrowed over time but recently the progress has plateaued. Women with a high school diploma are paid 80% of what men with the same qualifications are paid. Single mother families are more than 5 times as likely to live in poverty as married-couples families. Pregnancy affects work and educational opportunities for women and costs associated with pregnancy are higher for women than men. 46% of women believed they have experienced gender discrimination. Pregnant women with low socio-economic status report more depressive symptoms, suggesting the third trimester may be more stressful for low-income women. At 2 and 3 months postpartum, women with low income have been found to experience more depressive symptoms than women with high-income. Women with insecure and low-status jobs with little to no decision-making authority experience higher-levels of negative life events, insecure housing tenure, more chronic stressors, and reduced social support. Depression and anxiety have increased significantly for poor women in developing countries undergoing restructuring. Women with low income develop alcoholism and drug addiction influenced by social stressors linked to poverty. Improved balance in gender roles, obligations, pay equity, poverty reduction and renewed attention to maintenance of social capital redress the gender disparities in mental health. SES also affects physical health, with women living with breast cancer being11% more likely to die if they live in lower SES communities. Low-income women who have no insurance have lowest rates of mammography screening among women aged 40-64, increasing risk of death from breast cancer. Obesity and staying obese from adolescence to young adulthood is linked to poverty among women. Relative to HIV-positive men, women with HIV have disproportionately low-income in the US. Abstract Learn how socioeconomic status affects the lives of women. APA Style Reference APA. (2017, July). Women \u0026amp; Socioeconomic Status [Blog post]. Retrieved from https://www.apa.org/pi/ses/resources/publications/women You may also be interested in ls There a Positive Correlation between Socioeconomic Status and Academic Achievement? (Quagliata, 2008) Education and Socio-economic status (APA, 2017b) Disability and Socio-economic status (APA, 2010) Ethnic and Racial minorities and socio-economic status (APA, 2017) Lesbian, Gay, Bisexual and Transgender Persons \u0026amp; Socioeconomic Status (APA, 2010) The Gender Gap: Who Is (and Is Not) Included on Graduate-Level Syllabi in Social/Personality Psychology (Skitka et al., 2020) ⌺ Main Takeaways: This article investigates whether there is a gender gap in Social/Personality Psychology syllabi. One factor contributing to gender gaps is whose work we choose to teach in graduate seminars. It is hypothesised that one link in the broad chain of factors contributing to the eminent gender gap is that female authors are likely to be under-represented on graduate course syllabi compared to their male peers (gender gap hypothesis). Reasons why female authors might be under-represented on course syllabi could be varied. Instructors may internalise cultural prejudices and biases favouring men over women. This might result in a greater preference for male over female-authored papers (i.e., bias hypothesis). Another possibility is that instructors might prefer older over contemporary papers (i.e., classic hypothesis). Yet another possibility is that there are more male-authored papers available to include in syllabi than female-authored papers (i.e., availability hypothesis). The present study investigates whether there is a gender gap in representation on graduate level syllabi and whether it is explained by preference for classic over contemporary papers or relative availability of male- versus female-authored manuscripts. Method: The authors identified every social and/or personality PhD program in the US using the Social Psychology Network’s PhD ranking list and Graduate Programs GeoSearch. 120 programs were identified and a list of social/personality faculty names and email addresses for each program were put together by going to psychology department websites. Main interest was in courses for first-year graduate students. Inclusion criteria for syllabi were: (1) course name includes words: social or personality, (2) course was at the graduate level. Papers cited in the syllabi were coded for the following characteristics: gender of all authors, each author’s h-index, total number of authors, journal where the article was published, number of citations the article received since publication and topic in social/personality psychology. To understand whether the gender representation on graduate syllabi is (or is not) consistent with the number of high-quality papers from which instructors can select, the present study obtained all names of authors, authorship order and year of publication for all papers published in the Journal of Social and Personality Psychology from 1965 to 2017 and published in the Personality and Social Psychology Bulletin from 1974 until April 2018. These journals accounted for 33% of reading on sample course syllabi and formed benchmarks. Results: Less than 30% of papers referenced on syllabi were written by female first authors. The gender gap on syllabi, differed as a function of instructor gender and decade papers were published: female instructors assigned more recently published papers (post-1990) and female first-authored papers at levels significantly higher than their male counterparts. Difference in inclusion rates of female first-authored paper could not be explained by preference for classic over contemporary papers in syllabi or relative availability of female first-authored papers in the published literature. The gender gap differed depending on the content of the course. Male and female authors were approximately equally represented on graduate-level syllabi of topics as prejudice, close relationships, culture and health. The gender gap was much larger in syllabi of topics as best practices, replicability, attitude change and persuasion. Male and female-authored papers included on syllabi had similar citation rates, although they had different h-index scores. Increasing representation of female scholars’ work on graduate course syllabi would have beneficial consequences, moving toward greater gender inclusiveness in social/personality psychology. Abstract We contacted a random sample of social/personality psychologists in the United States and asked for copies of their graduate syllabi. We coded more than 3,400 papers referenced on these syllabi for gender of authors as well as other characteristics. Less than 30% of the papers referenced on these syllabi were written by female first authors, with no evidence of a trend toward greater inclusion of papers published by female first authors since the 1980s. The difference in inclusion rates of female first-authored papers could not be explained by a preference for including classic over contemporary papers in syllabi (there was evidence of a recency bias instead) or the relative availability of female first-authored papers in the published literature. Implications are discussed. APA Style Reference Skitka, L. J., Melton, Z. J., Mueller, A. B., \u0026amp; Wei, K. Y. (2020). The Gender Gap: Who Is (and Is Not) Included on Graduate-Level Syllabi in Social/Personality Psychology. Personality and Social Psychology Bulletin, 0146167220947326. https://doi.org/10.1177/0146167220947326\nYou may also be interested in Unequal effects of the COVID-19 pandemic on scientists (Myers et al., 2019) Against Eminence (Vazire, 2017) Scientific Eminence: Where Are the Women? (Eagly \u0026amp; Miller, 2016) Leveraging a collaborative consortium model of mentee/mentor training to foster career progression of under-represented post-doctoral researchers and promote institutional diversity and inclusion (Risner et al., 2020) ⌺ Main Takeaways: The goal of the study is to empower post-doctoral students and make them active participants in the mentoring relationships by emphasising the mentees’ contributions in shaping more productive interactions to be built upon to develop their own skills as a future mentor. The study used several metrics by which they assessed the success of this collaborative, multi-institutional approach, using National Research Mentoring Network (NRMN), the Committee on Institutional Cooperation Academic Network (CAN) approach to provide mentor facilitator training for faculty and senior administrators and mentoring-Up training for post-doctoral students. Background: “Establishing a functioning consortium needs buy-in and high-level cooperation from all partners. Prior to initiating programming, all potential institutional representatives set initial goals to address campus needs for mentor-up skill development for post-docs and mentor facilitator training for staff and faculty, establish sustainable communities of practice for mentor training and develop mechanisms for central coordination, outreach to campus constituents, templates for recruitment of participants and strategies to sustain collaboration and develop mechanisms for central coordination, outreach to campus constituents, templates for recruitment of participants, and strategies to sustain collaboration.” (p.4) Method: “The seven Core Principles [of “Mentoring-UP”] are: 1. Two-way communication, 2. Aligning expectations, 3. Assessing understanding, 4. Fostering independence, 5. Ethics, 6. Addressing equity and inclusion, 7. Promoting professional development. This curriculum provided postdocs opportunities for: i.) self-evaluation and reflection to become aware of their personal biases, attitudes, and behaviors; ii.) exploring strengths, weaknesses, and challenges in their interpersonal and professional relationships; iii.) understanding and learning how to use the mentor principles; and iv.) focusing on cognitive processes that may lead to behavioral changes and strategies to facilitate those changes in a process-based approach over 1.5–2 day workshops.” (p.5) Method: “The 1.5–2 day workshops included case studies and activities that: i.) engage mentors in peer discussion of a mentor framework; ii.) explore strategies to improve mentoring relationships; iii.) address mentoring problems; iv.) reflect on mentoring philosophies; v.) and create mentoring action plans to model the interactive, collaborative, and problem-solving ways to develop and implement this set of trainings in the future. The training goals provided tools and mechanisms to implement mentor training venues at the participating institutions, thereby establishing sustainable Mentor-training programs for undergrads, graduate students, postdocs and faculty” (p.5). “A specific NRMN-CAN survey was developed for all four postdoc cohorts to ascertain whether mentor training: i.) influenced career progression; ii.) impacted the postdocs’ relationship with their PIs; and iii.) components of the mentor training that were implemented by the postdoc mentees... A dedicated NRMN-CAN survey for faculty and senior administrators was also developed to ascertain whether participation in Mentor Facilitator training led to: i.) implementation of training workshops on their campuses; ii.) the level and number of participants; iii.) and whether facilitated sessions were carried out in partnership with others.” (p.5) Results: Post-doctoral students reported improvements in their mentoring proficiency and improved relationships with the Principal Investigators. 29% of post-doc respondents transitioned to faculty positions, and 85% of these respondents were under-represented and 75% were female. 59 out of 120 faculty and administrators provided mentor training to over 3000 undergraduate, graduate and postdoctoral students and faculty on their campus for the duration of this project. The findings showed that the majority of post-doctoral students indicate that mentor training positively influenced their relationship with their Mentors in several domains (e.g. confidence building). In addition, this curriculum has guided most post-doctoral students to better understand their mentoring needs, develop strategies to manage their mentoring relationships and empower them to make critical career decisions to pursue an academic career. In addition, early-career scientists stated they had more confidence to pursue an academic career with increased self-efficiency and advocacy. Impressively, 29% of the responding postdocs, predominantly females (75%) and underrepresented postdocs (85%) have successfully migrated to faculty. Some postdocs also indicated that their mentor training and experiences were valuable skills when applying for academic positions and definitely aided in adapting to responsibilities as a faculty mentor. Abstract Changing institutional culture to be more diverse and inclusive within the biomedical academic community is difficult for many reasons. Herein we present evidence that a collaborative model involving multiple institutions of higher education can initiate and execute individual institutional change directed at enhancing diversity and inclusion at the postdoctoral researcher (postdoc) and junior faculty level by implementing evidence-based mentoring practices. A higher education consortium, the Big Ten Academic Alliance, invited individual member institutions to send participants to one of two types of annual mentor training: 1) “Mentoring-Up” training for postdocs, a majority of whom were from underrepresented groups; 2) Mentor Facilitator training—a train-the-trainer model—for faculty and senior leadership. From 2016 to 2019, 102 postdocs and 160 senior faculty and administrative leaders participated. Postdocs reported improvements in their mentoring proficiency (87%) and improved relationships with their PIs (71%). 29% of postdoc respondents transitioned to faculty positions, and 85% of these were underrepresented and 75% were female. 59 out of the 120 faculty and administrators (49%) trained in the first three years provided mentor training on their campuses to over 3000 undergraduate and graduate students, postdocs and faculty within the project period. We conclude that early stage biomedical professionals as well as individual institutions of higher education benefited significantly from this collaborative mentee/mentor training model APA Style Reference Risner, L. E., Morin, X. K., Erenrich, E. S., Clifford, P. S., Franke, J., Hurley, I., \u0026amp; Schwartz, N. B. (2020). Leveraging a collaborative consortium model of mentee/mentor training to foster career progression of underrepresented postdoctoral researchers and promote institutional diversity and inclusion. PloS one, 15(9), e0238518. https://doi.org/10.1371/journal.pone.0238518\nYou may also be interested in The mental of PhD cry for help (Nature, 2019) Postdocs in crisis: science risks losing the next generation (Nature, 2020) Unequal effects of the COVID-19 pandemic on scientists (Myers et al., 2019) Disability and Socio-economic status (APA, 2010) ◈ ⌺ Main Takeaways: The Disabilities Act assures equal opportunities in education and employment for people with disabilities and prohibits discrimination based on disability. Despite the Disabilities Act, people with disabilities remain over-represented among America’s poor and under-educated. The federal government has two major programs to assist individuals with disabilities: the Social Security Disability Insurance and the Supplemental Security Income. The Social Security Disability Insurance is a program for workers who have become disabled and unable to work after paying Social Security taxes for at least 40 quarters. In this program, a higher income yields higher SSDI earnings. The Supplemental Security Income is a welfare program for individuals with low income, fewer overall resources and no or an abbreviated work history. Current federal benefit for a single person using Supplemental Security Income is $735 a month. Despite these programs, people with disabilities are more likely to be unemployed and live in poverty. For individuals who are blind and visually impaired, unemployment rates exceed 70 percent while for people with intellectual and developmental disabilities, the unemployment rate exceeds 80 percent. Also, one in ten veterans with disabilities are unemployed. The American Association of People with Disabilities estimates that two thirds of people with disabilities are of working age and want to work. There are disparities in median incomes for people with and without disabilities, such that individuals with disabilities often earn lower incomes. A study surveyed human resources and project managers about perceptions of hiring persons with disabilities. Results show professionals held negative perceptions related to productivity, social maturity, interpersonal skills and psychological adjustment of persons with disabilities. Disparities in education have been ongoing for generations. 20.9% of individuals 65 years and older without a disability failed to complete high school, relative to 25.1% and 38.6% of elder individuals with a non-severe or severe disability. Great disparities exist when comparing attainment of higher degrees. 15.1% of the population aged 25 and over with disability obtain a bachelor’s degree, whereas 33% of individuals in the same age category with no disability attain the same educational status. Individuals with a disability experience increased barriers to obtaining health care as a result of accessibility concerns, such as transportation, problems with communication and insurance. Family members who provide care to individuals with chronic or disabling conditions are themselves at risk of developing emotional, mental and physical health problems due to complex caregiving situations. Abstract Learn how socioeconomic status affects individuals with disabilities. APA Style Reference APA (2010). Disability \u0026amp; Socioeconomic Status [Blog post]. Retrieved from https://www.apa.org/pi/ses/resources/publications/disability\nYou may also be interested in ls There a Positive Correlation between Socioeconomic Status and Academic Achievement? (Quagliata, 2008) Education and Socio-economic status (APA, 2017b) Ethnic and Racial minorities and socio-economic status (APA, 2017) Women and Socio-economic status (APA, 2010) Lesbian, Gay, Bisexual and Transgender Persons \u0026amp; Socioeconomic Status (APA, 2010) Boosting research without supporting universities is wrong-headed (Nature, 2020b) ⌺ Main Takeaways: Coronavirus lockdowns have precipitated a crisis in university funding and academic morale. Universities all over the world closed their doors. Classes and some research activities were moved online. Staff were given little or no time to prepare and few resources or training to help them with online teaching. Fewer students are expected to enrol in the coming academic year, instead waiting until institutions open fully. This means young people will lose a year of their education and universities will lose out financially. Governments have plans to boost post-lockdown research but these plans will be undermined if universities make job cuts and end up with staff shortages. Universities need support at this crucial time. Low- and middle-income countries face extra challenges from sudden transition to online learning. The main concern is for students unable to access digital classrooms (those who live in areas without fast, reliable and affordable broadband or where students have no access to laptops, tablets, smartphones and other essential hardware). Teachers report students struggle to keep up since lockdown began. Students from poorer households in remote regions travel to the nearest city to access the Internet and pay commercial internet cafes to download course materials. To solve this issue, governments and funding bodies need to accept that students and universities should be eligible for the same kinds of temporary emergency funding as other industries are asking for. Governments have denied requests to negotiate with universities or delayed decisions. In high-income countries, this is partly because universities are functioning and might be seen as less deserving of government help than businesses and professions that had no choice but to close. In poorer countries, public funding for universities is under threat because economies have crashed during lockdowns. Cuts in universities’ budgets will disproportionately affect poorest students and more vulnerable members of staff (those with fixed-term contracts). Students and staff on short-term contracts would welcome more support from academic colleagues in senior positions and from others with permanent positions. Colleagues should make the case for managers that failing to provide more help to low-income students or cutting the number of post-doctoral staff and teaching fellows presents a harm to the next generation of researchers and teachers. It will reduce departments’ capacity to teach and increase load on those who remain. Cutting back on scholarly capacity while increasing spending on research and development is wrong-headed, slowing down economic recovery and jeopardising plans to make research more inclusive. Abstract Universities face a severe financial crisis, and some contract staff are hanging by a thread. Senior colleagues need to speak up now. APA Style Reference Nature. (2020). Boosting research without supporting universities is wrong-headed. Nature, 582, 313-314. https://www.nature.com/articles/d41586-020-01788-6\nYou may also be interested in The mental health of PhD researchers demands urgent attention (Nature, 2019) Postdocs in crisis: science risks losing the next generation (Nature, 2020) Seeking an exit plan (Woolston, 2020) Unequal effects of the COVID-19 pandemic on scientists (Myers et al., 2019) Lesbian, Gay, Bisexual and Transgender Persons \u0026amp; Socioeconomic Status (APA, 2010) ◈ ⌺ Main Takeaways: Individuals who identify as Lesbian, gay, bisexual and/or transgender are specially susceptible to socio-economic disadvantages. Socioeconomic status is inherently linked to rights, quality of life, and general well-being of Lesbian, Gay, Bisexual and/or transgender persons. Low income LGBT individuals and same-sex/gender couples have been found to be more likely to receive cash assistance and food stamps benefits compared to heterosexual individuals or couples. Transgender adults were nearly 4 times more likely to have household income of less than $10,000 per year relative to the general population. Raising the federal minimum wage benefits LGBT individuals and couples in the United States. An increase in minimum wage should reduce poverty rates by 25% for same-sex/gender female couples and 30% for same-sex/gender male couples. Due to an increase in minimum wage, poverty rates would be projected to fall for the most vulnerable individuals in same-sex/gender couples, including African American, couples with children, people with disabilities, individuals under 24 years of age, people without high school diplomas or the equivalent, and those living in rural areas. The socio-economic position may be linked to experiences of discrimination. Gay and bisexual men who earned higher income were less likely to report discrimination relative to those in lower socio-economic positions. Discrimination against and unfair treatment of LGBT persons remains legally permitted. 47% of transgender individuals report being discriminated against in hiring, firing and promotion, over 25% had lost a job due to discrimination based on gender identity. A lack of acceptance and fear of persecution lead many LGBT youth to leave their homes and live in transitional housing or on the street. Many LGBT youth may be rejected by their family of origin or caregivers and forced to leave home as minors. LGBT youth experience homeless at a disproportionate rate. LGBT homeless youth are more likely than their homeless heterosexual counterparts to have poorer mental and physical health outcomes. Although since 2015 states must issue marriage licenses to same-sex couples and recognise same-sex unions, legal barriers continue to exist. Workplace and housing discrimination contribute to increasing socio-economic status disparities for LGBT persons and families. 20 states and District of Columbia prohibit discrimination in workplace based on sexual orientation and gender identity, while 18 states have no laws prohibiting workplace discrimination against LGBT people. 19% of transgender individuals report in a previous study that they were refused a home or apartment and 11% report being evicted because of their gender identity or expression. Abstract Evidence indicates individuals who identify as lesbian, gay, bisexual and/or transgender (LGBT) are especially susceptible to socioeconomic disadvantages. Thus, SES is inherently related to the rights, quality of life and general well-being of LGBT persons. APA Style Reference APA (2010). Lesbian, Gay, Bisexual and Transgender Persons \u0026amp; Socioeconomic Status. [Blog post]. Retrieved from https://www.apa.org/pi/ses/resources/publications/lgbt\nYou may also be interested in ls There a Positive Correlation between Socioeconomic Status and Academic Achievement? (Quagliata, 2008) Education and Socio-economic status (APA, 2017b) Ethnic and Racial minorities and socio-economic status (APA, 2017) Women and Socio-economic status (APA, 2010) Disability and Socio-economic status (APA, 2010) The Focus on Fame distorts Science (Innes-Ker, 2017) ◈ ⌺ Main Takeaways: The author argues that instead of focusing on individual merit it is important that science is focused on scientific ideas and collaborative groups. Asking if you are famous is a wrong question. It focuses on the individual scientist, as if science is a lonely enterprise of hopeful geniuses. We should focus on ideas and knowledge and refining those ideas. H-index is not an objective measure. It presupposes that peer-review papers are solid and that citations are a proxy for quality. Science is argued to advance in an evolutionary manner. A wealth of ideas is produced, but only some are selected and survive depending on scientific merit and social process (production of papers, citations and engagement of groups of scientists). Ideas that engage groups of scientists will grow and change and bring knowledge closer to the truth. Ideas that are not interacted with, on the other hand, will likely die. This is far from focus on eminence and individual fame prevalent in science. Competition is a factor but cooperation is vital. For ideas to survive, multiple labs need to engage with them as champions or severe adversarial testers. If we focus on who may become eminent, we lose some power of the scientific process. Eminent scientists would be nowhere without collaborators and adversaries willing to engage with the ideas. The tendency to overwhelmingly publish only positive results with no clear avenue for publishing failures to confirm, means scientists are not grappling with the real field. Recent work to improve methods, statistics and publishing practices is an example of collaboration. In science, scientific ideas are the ones that need to be stress-tested, not scientists. We need to move away from the cultural market model of science focusing on individuals rather than on robustness of ideas. Science is a low yield, high risk business. Assigning individual merit based on productivity and citation encourages poor scientific practices and discourages collaboration and argumentative engagement with ideas. It results in a waste of talent. Objectivity in Science is not a characteristic of individual researchers, but a characteristic of scientific communities. Abstract The 2016 symposium on Scholarly Merit focused on individual eminence and fame. I argue, with some evidence, that the focus on individual merit distorts science. Instead we need to focus on the scientific ideas, and the creation of collaborative groups. APA Style Reference Innes-Ker, Å. (2017). The Focus on Fame Distorts Science. https://psyarxiv.com/vyr3e/\nYou may also be interested in Fame: I’m Skeptical (Ferreira, 2017) Let’s Look at the Big Picture: A System-Level Approach to Assessing Scholarly Merit (Pickett, 2017) “Fame” is the Problem: Conflation of Visibility With Potential for Long-Term Impact in Psychological Science (Shiota, 2017) Why a Focus on Eminence is Misguided: A Call to Return to Basic Scientific Values (Corker, 2017) Am I Famous Yet? Judging Scholarly Merit in Psychological Science: An Introduction (Sternberg, 2016) Against Eminence (Vazire, 2017) Giving Credit Where Credit’s Due: Why It’s So Hard to Do in Psychological Science (Simonton, 2016) Eminence and Omniscience: Statistical and Clinical Prediction of Merit (Foss, 2016) Taking Advantage of Citation Measures of Scholarly Impact: Hip Hip h Index! (Ruscio, 2016) Improving Departments of Psychology (Diener, 2016) Varieties of Fame in Psychology (Roediger III, 2016) Scientific Eminence: Where Are the Women? (Eagly \u0026amp; Miller, 2016) Intrinsic and Extrinsic Science: A Dialectic of Scientific Fame (Feist, 2016) Fame: I’m Skeptical (Ferreira, 2017) ◈ ⌺ Main Takeaways: The author argues that fame and quality sometimes diverge and that reliance on fame helps to perpetuate stereotypes that keep women and underrepresented groups from participating in science. Most of us believe we have the respect of our peers and acknowledge we wish to be admired and viewed as successful and important. No psychologist and no rational person would deny that evaluating people and the quality of their work is necessary and inevitable in any field. We like to admit most promising candidates to graduate programs, hire the best faculty, tenure only those who have long productive careers and reward scientists with prizes if they contributed more than most to uncover the nature of psychological processes. We must not conflate fame and scientific quality, integrity and impact. All of us point to colleagues who completed excellent work but are barely known or who are not famous until long after their research careers have ended. Some scientists are well known because they have been called out for unethical practices, including data fabrication and other forms of cheating. We need to discriminate between two questions: (i) what one must do to become famous and (ii) what leads a person to end up famous. While the second question is merely an attempt to reconstruct someone’s path to fame, the motivations of the first question need to be challenged. Fame should not be a goal in science and valuing people or ideas because they are famous comes at a risk. Fame should be viewed with caution and scepticism to avoid temptation to assume that if someone is famous, their work is significant. Fame perpetuates discrimination and overlook excellent people and work. Science is based on critical thinking. As such, we should never hesitate to question the ideas of someone who is famous. We should not refuse to view the work of famous people positively or refuse to give it its due, but we must be careful to think an idea is useful due to the person being famous. Abstract Fame is often deserved, emerging from a person’s significant and timely contributions to science. It is also true that fame and quality clearly sometimes diverge: many people who do excellent work are barely known, and some people are famous even though their work is mediocre. Reliance on fame and name recognition when identifying psychologists as candidates for honors and awards helps to perpetuate a range of stereotypes and prevents us from broadening participation in our field, particularly from women and underrepresented groups. The pursuit of fame may also be contributing to the current crisis in psychology concerning research integrity, because it incentivizes quantity and speed in publishing. The right attitude towards fame is to use it wisely if it happens to come, but to focus our efforts on conducting excellent research and nurturing talent in others. APA Style Reference Ferreira, F. (2017). Fame: I\u0026#39;m Skeptical (2017). https://psyarxiv.com/6zb4f/\nYou may also be interested in The Focus on Fame distorts Science (Innes-Ker, 2017) Let’s Look at the Big Picture: A System-Level Approach to Assessing Scholarly Merit (Pickett, 2017) “Fame” is the Problem: Conflation of Visibility With Potential for Long-Term Impact in Psychological Science (Shiota, 2017) Why a Focus on Eminence is Misguided: A Call to Return to Basic Scientific Values (Corker, 2017) Am I Famous Yet? Judging Scholarly Merit in Psychological Science: An Introduction (Sternberg, 2016) Against Eminence (Vazire, 2017) Giving Credit Where Credit’s Due: Why It’s So Hard to Do in Psychological Science (Simonton, 2016) Eminence and Omniscience: Statistical and Clinical Prediction of Merit (Foss, 2016) Taking Advantage of Citation Measures of Scholarly Impact: Hip Hip h Index! (Ruscio, 2016) Improving Departments of Psychology (Diener, 2016) Varieties of Fame in Psychology (Roediger III, 2016) Scientific Eminence: Where Are the Women? (Eagly \u0026amp; Miller, 2016) Intrinsic and Extrinsic Science: A Dialectic of Scientific Fame (Feist, 2016) Let’s Look at the Big Picture: A System-Level Approach to Assessing Scholarly Merit (Pickett, 2017) ◈ ⌺ Main Takeaways: Why do we care about judging scientific merit? There is a need to have a system to determine whether to award tenure and promotion to faculty members, leading to a development of criteria in order to judge and measure the scholarly merit of individuals. Science is a collective enterprise whose goal is to explain and understand the natural world and to build knowledge. Science cares about advancements and discoveries, not about individuals. Individual scientists are valued to the extent that they further the goals of the collective system. However, science comprises lab workers, scientists, institutions, agencies, and broader society. At organisation level, features facilitate scientific discovery-organisational autonomy, organisational flexibility, moderate scientific diversity and frequent and intense interaction among scientists with different viewpoints. An individual scientist contributes to scientific discovery directly through their own scientific products or indirectly by positively affecting other aspects of the system. More senior graduate students train incoming graduate students- when good at this the output of an entire lab can skyrocket as a result. Graduate students not only conduct their own personal research but their presence in the lab facilitates scientific progress of others. Scientists promote productivity of other scientists by reviewing manuscripts, sharing data, creating and serving scientific organisations, and developing scientific tools and paradigms used by others. Individual research scientists do not have resources to create large research centres, but can organise conferences and symposia, create and contribute to scientific discussion platforms, and make their research protocols and data easily shareable. Scholarly merit should include an individual’s system-level contributions, not only their productivity. Abstract When judging scientific merit, the traditional method has been to use measures that assess the quality and/or quantity of an individual’s research program. In today’s academic world, a meritorious scholar is one who publishes high quality work that is frequently cited, who receives plentiful funding and scientific awards, and who is well regarded among his or her peers. In other words, merit is defined by how successful the scholar has been in terms of promoting his or her own career. In this commentary, I argue that there has been an overemphasis on measuring individual career outcomes and that we should be more concerned with the effect that scholars have on the scientific system in which they are embedded. Put simply, the question we should be asking is whether and to what extent a scholar has advanced the scientific discipline and moved the field forward collectively. APA Style Reference Pickett, C. (2017). Let\u0026#39;s Look at the Big Picture: A System-Level Approach to Assessing Scholarly Merit. https://psyarxiv.com/tv6nb/\nYou may also be interested in The Focus on Fame distorts Science (Innes-Ker, 2017) Fame: I’m Skeptical (Ferreira, 2017) “Fame” is the Problem: Conflation of Visibility With Potential for Long-Term Impact in Psychological Science (Shiota, 2017) Why a Focus on Eminence is Misguided: A Call to Return to Basic Scientific Values (Corker, 2017) Am I Famous Yet? Judging Scholarly Merit in Psychological Science: An Introduction (Sternberg, 2016) Against Eminence (Vazire, 2017) Giving Credit Where Credit’s Due: Why It’s So Hard to Do in Psychological Science (Simonton, 2016) Eminence and Omniscience: Statistical and Clinical Prediction of Merit (Foss, 2016) Taking Advantage of Citation Measures of Scholarly Impact: Hip Hip h Index! (Ruscio, 2016) Improving Departments of Psychology (Diener, 2016) Varieties of Fame in Psychology (Roediger III, 2016) Scientific Eminence: Where Are the Women? (Eagly \u0026amp; Miller, 2016) Intrinsic and Extrinsic Science: A Dialectic of Scientific Fame (Feist, 2016) “Fame” is the Problem: Conflation of Visibility With Potential for Long-Term Impact in Psychological Science (Shiota, 2017)◈ ⌺ Main Takeaways: Fame is about visibility – who is seen. Ample evidence documents the influence of heuristics in determining who is visible, and whose contribution is considered important. Explicit and implicit beliefs about competence influences peer review when methodological quality or potential impact is ambiguous. The author is sceptical about the extent that fame is shaped by the quality of one’s work instead of confidence, dominance, persistence and demographics. The pace of academic life accelerates, the pressure to depend on shortcuts in gatekeeping and evaluation will continue to grow. The scientific community cannot remove implicit biases, there are ways to deflect the impact of these implicit biases. Reviews of submitted work should be blind to identity and demographics, letting the quality of the product stand on its own. Quote “We specify criteria for good science flexibly but explicitly and in detail, including thorough and accurate contextualisation in relevant previous work, methodological rigour; innovation and problem solving and implications for theory, future research and/or intervention. We should insist on diversity in career stage, gender, ethnicity and perspective instead of inviting first people who come to mind for invited opportunities such as conference talks, contribution to edited volumes, awards, and participation in committees that determine the direction of our field. We can resist temptation to track women and minorities into high profile, high-demand services roles, thinking that this solves problems of diversity in science. When, in fact, it does not.” (p.7) Abstract To be famous is to be widely known, and honored for one’s achievements. The process by which researchers achieve fame or eminence is skewed by heuristics that influence visibility; implications of these heuristics are magnified by a snowball effect, in which current fame leads to bias in ostensibly objective metrics of merit, including the distribution of resources that support future excellence. This effect may disproportionately hurt women and minorities, who struggle with both external and internalized implicit biases regarding competence and worth. While some solutions to this problem are available, they will not address the deeper problems of defining what it means for research to “make a difference” in our field and in society, and consistently holding our work to that criterion. APA Style Reference Shiota, M. N. (2017) “Fame” is the Problem: Conflation of Visibility With Potential for Long-Term Impact in Psychological Science. https://psyarxiv.com/4kwuq\nYou may also be interested in The Focus on Fame distorts Science (Innes-Ker, 2017) Fame: I’m Skeptical (Ferreira, 2017) Let’s Look at the Big Picture: A System-Level Approach to Assessing Scholarly Merit (Pickett, 2017) Why a Focus on Eminence is Misguided: A Call to Return to Basic Scientific Values (Corker, 2017) Am I Famous Yet? Judging Scholarly Merit in Psychological Science: An Introduction (Sternberg, 2016) Against Eminence (Vazire, 2017) Giving Credit Where Credit’s Due: Why It’s So Hard to Do in Psychological Science (Simonton, 2016) Eminence and Omniscience: Statistical and Clinical Prediction of Merit (Foss, 2016) Taking Advantage of Citation Measures of Scholarly Impact: Hip Hip h Index! (Ruscio, 2016) Improving Departments of Psychology (Diener, 2016) Varieties of Fame in Psychology (Roediger III, 2016) Scientific Eminence: Where Are the Women? (Eagly \u0026amp; Miller, 2016) Intrinsic and Extrinsic Science: A Dialectic of Scientific Fame (Feist, 2016) Why a Focus on Eminence is Misguided: A Call to Return to Basic Scientific Values (Corker, 2017) ◈ ⌺ Main Takeaways: The author argues that our current methods of scientific rewards are based on identifying research eminence. This reward system is not in line with scientific values of transparency and universalism and undermines scientific quality. Why do we accord knowledge derived from scientific method a privileged position relative to common sense, appeals to authority figures, or other forms of rhetoric? If scientists depend on their own expertise as justification to prioritise their claims, we are not better to make truth-claims than religious, political and other leaders. Instead, science’s claim on truth comes not from its practitioners’ training and expertise, but rather from its strong adherence to norms of transparency and universalism. Universalism means scientists reject claims of special authority. It matters far less who did the research than how it was done. How do we square scientific ideals of universalism with scientific culture that fetishizes lone scientific genius? We need to recognise the methods used to produce a scientific claim are more important than eminence of a person who produced it. Focusing primarily on the individual researcher excellence hurts psychological science, as eminence reflects values that are counterproductive to maximise scientific knowledge. The current system privileges quantity over quality, outcome of research instead of the process itself. Systematic biases (e.g., structural sexism, racism, and status bias) affect how we identify who qualifies as eminent under status quo. Gender, nationality, race or institution should not matter to measure research quality. Structural changes should be initiated to help researchers reward and evaluate quality research (i.e., work that is reproducible, transparent and open, and likely to be high in validity). We can do a much better job to recognise and reward many activities researchers do that support scientific discovery beyond publishing peer reviewed articles (e.g. develop scientific software, generate large datasets, write data analytic code and construct tutorials to teach others to use it). We need to re-evaluate ways to measure researchers’ excellence in light of value and promise of team-driven research. After all, science is a communal endeavour. To combat structural and systematic problems linked to recognising eminence, double blind peer reviews need to be considered as standard practice for journal publication, grant funding and awards committee. Technological solutions could even be developed to allow departments to blind in early stages of faculty hiring, as blinding is associated with higher levels of diversity. Abstract The scientific method has been used to eradicate polio, send humans to the moon, and enrich understanding of human cognition and behavior. It produced these accomplishments not through magic or appeals to authority, but through open, detailed, and reproducible methods. To call something “science” means there are clear ways to independently and empirically evaluate research claims. There is no need to simply trust an information source. Scientific values thus prioritize transparency and universalism, emphasizing that it matters less who has made a discovery than how it was done. Yet, scientific reward systems are based on identifying individual eminence. The current paper contrasts this focus on individual eminence with reforms to scientific rewards systems that help these systems better align with scientific values. APA Style Reference Corker, K. S. (2017). Why a Focus on Eminence is Misguided: A Call to Return to Basic Scientific Values. https://psyarxiv.com/yqfrd\nYou may also be interested in The Focus on Fame distorts Science (Innes-Ker, 2017) Fame: I’m Skeptical (Ferreira, 2017) Let’s Look at the Big Picture: A System-Level Approach to Assessing Scholarly Merit (Pickett, 2017) “Fame” is the Problem: Conflation of Visibility With Potential for Long-Term Impact in Psychological Science (Shiota, 2017) Am I Famous Yet? Judging Scholarly Merit in Psychological Science: An Introduction (Sternberg, 2016) Against Eminence (Vazire, 2017) Giving Credit Where Credit’s Due: Why It’s So Hard to Do in Psychological Science (Simonton, 2016) Eminence and Omniscience: Statistical and Clinical Prediction of Merit (Foss, 2016) Taking Advantage of Citation Measures of Scholarly Impact: Hip Hip h Index! (Ruscio, 2016) Improving Departments of Psychology (Diener, 2016) Varieties of Fame in Psychology (Roediger III, 2016) Scientific Eminence: Where Are the Women? (Eagly \u0026amp; Miller, 2016) Intrinsic and Extrinsic Science: A Dialectic of Scientific Fame (Feist, 2016) Unequal effects of the COVID-19 pandemic on scientists (Myers et al., 2019) ⌺ Main Takeaways: COVID-19 pandemic disrupted scientific enterprise. Policymakers and institutional leaders have started to respond to reduce influences of pandemic on researchers. For this study, authors reached out to US- and Europe-based scientists across institutions, career stages and demographic backgrounds. The present paper solicited information about working hours and how time allocations changed since the onset of pandemic and asked scientists to report the range of individual and family properties, as these feature moderate effects of pandemic. The sample was self-selected and it is likely that those who feel strongly about sharing situations, whether they experienced large positive or negative changes due to the pandemic, were the ones who chose to participate. They found a decline in total working hours with the average dropping from 61 hours per week pre-pandemic to 54 hours at time of survey. Only 5% of scientists report they worked 42 hours or less before the pandemic. This share increased to 30% of scientists during the pandemic. Time devoted to research has changed most during pandemic. Total working hours decreased by 11% on average, but research declined by 24%. Scientists working in fields that rely on physical laboratories and on time sensitive experiments report largest declines in research time (in the range of 30-40% below pre-pandemic levels). Fields that are less equipment intensive (e.g., mathematics, statistics, computer science and economics) report lowest declines in research time. The difference to other fields can be as large as fourfold. There are differences between male and female respondents in how the pandemic influenced their work. Female scientists and scientists with young dependents report ability to devote time to their research has been influenced and effects are additive - most impact was for female scientists with young dependents. Individual circumstances of researchers best explain changes in time devoted to research during pandemic. Career stage and facility closures did not contribute to changes in time allocated to research when everything else is held constant. Gender and young dependents contributed major roles. Female scientists reported a 5% larger decline in research time than male scientists, but scientists with at least one child 5 years old or younger experienced a 17% larger decline in research time. Having multiple dependents was linked to a further 3% reduction in time spent on research. Scientists with dependents aged 6-11 years were less affected. This indicates gender discrepancy can be due to female scientists being more likely to have young children as dependents. Results indicate that the pandemic influences members of the scientific community differently. Shelter at home is not the same as work from home, when dependents are also at home and need care. Unless adequate childcare services are available, researchers with young children continue to be affected irrespective of reopening plans of institutions. Pandemic will likely have longer-term impacts that are important to monitor. Further efforts to track effects of pandemic on the scientific workforce need to consider household circumstances. Uniform policies do not consider individual circumstances and may have unintended consequences and worsen pre-existing inequalities. The disparities may worsen as institutions begin the process of reopening given that different priorities for bench sciences versus work with human subjects or field-work travel may lead to new disparities across scientists. Funders seeking to support high-impact programs adopt a similar approach, favouring proposals that are more resilient to uncertain future scenarios. Senior researchers have incentives to avoid in-person interactions facilitating mentoring and hands-on training of junior researchers. Impact of changes on individuals and groups of scientists could be large in short- and long-term, worsening negative impacts among those at a disadvantage. We need to consider consequences of policies adopted to respond to pandemic, as they may disadvantage under-represented minorities and worsen existing disparities. Quote “The disparities we observe and the likely surfacing of new impacts in the coming months and years argue for targeted and nuanced approaches as the world-wide research enterprise rebuilds.” (p.882) Abstract COVID-19 has not affected all scientists equally. A survey of principal investigators indicates that female scientists, those in the ‘bench sciences’ and, especially, scientists with young children experienced a substantial decline in time devoted to research. This could have important short- and longer-term effects on their careers, which institution leaders and funders need to address carefully. APA Style Reference Myers, K. R., Tham, W. Y., Yin, Y., Cohodes, N., Thursby, J. G., Thursby, M. C., ... \u0026amp; Wang, D. (2020). Unequal effects of the COVID-19 pandemic on scientists. Nature Human Behaviour, 4, 880-883. https://doi.org/10.1038/s41562-020-0921-y\nYou may also be interested in The Gender Gap: Who Is (and Is Not) Included on Graduate-Level Syllabi in Social/Personality Psychology (Skitka et al., 2020) Leveraging a collaborative consortium model of mentee/mentor training to foster career progression of under-represented post-doctoral researchers and promote institutional diversity and inclusion (Risner et al., 2020) ⌺ The mental health of PhD researchers demands urgent attention (Nature, 2019) Postdocs in crisis: science risks losing the next generation (Nature, 2020) Boosting research without supporting universities is wrong-headed (Nature, 2020b) Against Eminence (Vazire, 2017) Against Eminence (Vazire, 2017) ◈ ⌺ Main Takeaways: The author argues that the drive for eminence is inherently at odds with scientific values and that insufficient attention to this problem is partly responsible for the recent crisis of confidence in psychology and other sciences. Transparency makes it possible for scientists to discriminate robust from shaky findings. The Replicability crisis shows a system without transparency does not work. Those in charge of setting scientific norms and standards should strive to increase transparency, bolster our confidence that we trust published research. However many high level decisions in science are made with a different goal in mind: to increase impact. Professional societies and journals prioritise publishing attention-grabbing findings to boost visibility and prestige. Seeking eminence is at odds with scientific value and affects scientific gatekeepers’ decisions. Editors influenced by the status of submitting authors or prestige of institutions violate the basic premise of science. Science work should be evaluated on its own merit, irrespective of the source. Lack of transparency in science is a direct consequence of the corrupting influence of eminence seeking. Gatekeepers control incentive structures that shape individual researchers’ behaviour. Therefore they have a bigger responsibility to uphold scientific values and most power to erode those values. Individual researchers’ desire for eminence threatens the integrity of the research process. All researchers are human and desire recognition for their work. However, there is no good reason to amplify this human drive and encourage scientists to seek fame. The glorification of eminence also reinforces inequalities in science. If scientists are evaluated based on ability to attract attention, those with the most prestige will be heard the loudest. Certain groups are overrepresented at a high level of status. Eminence propagates privilege and raises barriers to entry for others. How should scientific merit be evaluated? What does this mean for committees to select one or few winners? First, it is important to admit that a larger number of scientists meet the objective criteria for these recognitions (i.e., do sound science). It is also important to admit that selection of one or few individuals is not based on merit but on preference or partiality. It is fine to select or recognise members who exemplify their values, but this should not be confused with exceptional scientific merit. Whenever possible (for tenure, promotion and when journal space or grant fund permits), we should attempt to reward scientists whose work reaches a more objective threshold of scientific rigour or soundness instead of selecting scientists based on fame. Abstract The drive for eminence is inherently at odds with scientific values, and insufficient attention to this problem is partly responsible for the recent crisis of confidence in psychology and other sciences. The replicability crisis has shown that a system without transparency doesn’t work. The lack of transparency in science is a direct consequence of the corrupting influence of eminence-seeking. If journals and societies are primarily motivated by boosting their impact, their most effective strategy will be to publish the sexiest findings by the most famous authors. Humans will always care about eminence. Scientific institutions and gatekeepers should be a bulwark against the corrupting influence of the drive for eminence, and help researchers maintain integrity and uphold scientific values in the face of internal and external pressures to compromise. One implication for evaluating scientific merit is that gatekeepers should attempt to reward all scientists whose work reaches a more objective threshold of scientific rigor or soundness, rather than attempting to select the cream of the crop (i.e., identify the most “eminent”). APA Style Reference Vazire, S. (2017). Against eminence. https://doi.org/10.31234/osf.io/djbcw\nYou may also be interested in The Gender Gap: Who Is (and Is Not) Included on Graduate-Level Syllabi in Social/Personality Psychology (Skitka et al., 2020) The Focus on Fame distorts Science (Innes-Ker, 2017) Fame: I’m Skeptical (Ferreira, 2017) Let’s Look at the Big Picture: A System-Level Approach to Assessing Scholarly Merit (Pickett, 2017) “Fame” is the Problem: Conflation of Visibility With Potential for Long-Term Impact in Psychological Science (Shiota, 2017) Why a Focus on Eminence is Misguided: A Call to Return to Basic Scientific Values (Corker, 2017) Am I Famous Yet? Judging Scholarly Merit in Psychological Science: An Introduction (Sternberg, 2016) Unequal effects of the COVID-19 pandemic on scientists (Myers et al., 2019) Giving Credit Where Credit’s Due: Why It’s So Hard to Do in Psychological Science (Simonton, 2016) Eminence and Omniscience: Statistical and Clinical Prediction of Merit (Foss, 2016) Taking Advantage of Citation Measures of Scholarly Impact: Hip Hip h Index! (Ruscio, 2016) Improving Departments of Psychology (Diener, 2016) Varieties of Fame in Psychology (Roediger III, 2016) Scientific Eminence: Where Are the Women? (Eagly \u0026amp; Miller, 2016) Intrinsic and Extrinsic Science: A Dialectic of Scientific Fame (Feist, 2016) Scientific Eminence: Where Are the Women? (Eagly \u0026amp; Miller, 2016) ⌺ Main Takeaways: Women’s scientific contributions in psychology may not be as numerous or influential as those of men. What is the magnitude of the current eminence gender gap? Women’s modest inroads into this list of eminent psychologists deserve respect, given this lag between obtaining a doctorate and attaining eminence and formidable barriers that women once faced in pursuing scientific careers. Psychologists judge eminence by observing signs such as memberships in selective societies, career scientific achievement awards and honorary degrees. Do men exceed women on both quantity and impact of their publication underlies h index? Are these metrics tainted by unfair bias against women? Does the h-index identify potential socio-cultural and individual causes of the eminence gap? Women’s publications are cited less than men. This gap was larger in psychology. Women received 20% fewer in psychology varying across subfields. Gender gap on h-index and similar metrics has two sources: women publish less than men and articles receive fewer citations. Metrics assessing scientific eminence may be tainted by prejudicial bias against female scientists in obtaining grant support, publishing papers, or gaining citations of published papers. If psychologists are disadvantaged in publishing their work, bias may be limited to culturally masculine topics or male-dominated research areas. Such topics and are no doubt becoming rarer in psychology, given women receive most US doctorates. Men’s greater overall citations reflect higher rates of self-citation, women self-cite less often. This reflects men’s larger corpus of their own citable papers. Prejudicial gender bias is limited and presents ambiguity given most studies are correlational instead of experimental. Little is known about possible gender bias in awards for scientific eminence such as science prizes and honorary degrees, which are imperfect indicators of the importance of scientists’ contributions. Female scientists’ lesser rates of publication and citation reflect causes other than biases. Broader socio-cultural factors shape individual identities and motivations. Nature and nurture affects role occupancies so men and women are differently distributed into social roles. Women excel in communal qualities of warmth and concern for others and for men to excel in agentic qualities of assertiveness and mastery. Women are over-represented in less research intensive but more in teaching-intensive ranks and part-time positions. Gender norms discourage female agency may disadvantage to gain status in departmental and disciplinary networks and garner resources. Stereotypes erode women’s confidence in ability to become highly successful scientists. Eminence gender gaps in psychology and other sciences shrink further over time as new cohorts of scientists advance in their careers. Women’s representation among PhD earners has increased dramatically over recent decades. Abstract Women are sparsely represented among psychologists honored for scientific eminence. However, most currently eminent psychologists started their careers when far fewer women pursued training in psychological science. Now that women earn the majority of psychology Ph.D.’s, will they predominate in the next generation’s cadre of eminent psychologists? Comparing currently active female and male psychology professors on publication metrics such as the h index provides clues for answering this question. Men outperform women on the h index and its two components: scientific productivity and citations of contributions. To interpret these gender gaps, we first evaluate whether publication metrics are affected by gender bias in obtaining grant support, publishing papers, or gaining citations of published papers. We also consider whether women’s chances of attaining eminence are compromised by two intertwined sets of influences: (a) gender bias stemming from social norms pertaining to gender and to science and (b) the choices that individual psychologists make in pursuing their careers. APA Style Reference Eagly, A. H., \u0026amp; Miller, D. I. (2016). Scientific eminence: Where are the women?. Perspectives on Psychological Science, 11(6), 899-904. https://doi.org/10.1177/1745691616663918\nYou may also be interested in The Gender Gap: Who Is (and Is Not) Included on Graduate-Level Syllabi in Social/Personality Psychology (Skitka et al., 2020) Leveraging a collaborative consortium model of mentee/mentor training to foster career progression of under-represented post-doctoral researchers and promote institutional diversity and inclusion (Risner et al., 2020) The Focus on Fame distorts Science (Innes-Ker, 2017) Fame: I’m Skeptical (Ferreira, 2017) Let’s Look at the Big Picture: A System-Level Approach to Assessing Scholarly Merit (Pickett, 2017) “Fame” is the Problem: Conflation of Visibility With Potential for Long-Term Impact in Psychological Science (Shiota, 2017) Why a Focus on Eminence is Misguided: A Call to Return to Basic Scientific Values (Corker, 2017) Am I Famous Yet? Judging Scholarly Merit in Psychological Science: An Introduction (Sternberg, 2016) Against Eminence (Vazire, 2017) Varieties of Fame in Psychology (Roediger III, 2016) Eminence and Omniscience: Statistical and Clinical Prediction of Merit (Foss, 2016) Taking Advantage of Citation Measures of Scholarly Impact: Hip Hip h Index! (Ruscio, 2016) Improving Departments of Psychology (Diener, 2016) Giving Credit Where Credit’s Due: Why It’s So Hard to Do in Psychological Science (Simonton, 2016) Intrinsic and Extrinsic Science: A Dialectic of Scientific Fame (Feist, 2016) Seeking Congruity Between Goals and Roles: A New Look at Why Women Opt Out of Science, Technology, Engineering, and Mathematics Careers (Diekman et al., 2010) ⌺ Main Takeaways: We present a new perspective on this issue by proposing that interest in some careers and disinterest in others results from the intersection of people’s goals and their preconceptions of the goals afforded by different careers. We hypothesize that people perceive Science, Technology, Engineering and Mathematics (STEM) careers as being especially incompatible with an orientation to care about other people (i.e. communion). Because women in particular tend to endorse communal goals, they may be more likely than men to opt out of STEM careers in favor of careers that seem to afford communion. These trends suggest that to explain women’s absence in STEM fields, research should focus on factors that differentiate careers in STEM from other careers. We hypothesize that a critical but relatively unexplored factor may be that many non-STEM careers are perceived as fulfilling communal goals. We thus examined (a) whether communal-goal affordances are perceived to differ between STEM and other careers, and (b) whether communal-goal endorsement inhibits STEM interest, given consensual beliefs about the goals these careers afford. Method: 333 introductory psychology students provided goal-affordance ratings and information about their mathematics and science experience. Our goal was to determine predictors of differential interest in STEM, male-stereotypic/non-STEM (MST), and female-stereotypic (FST) careers. To create scales reflecting these different stereotypic categories, we used archival and primary data. Method: For each core career, participants rated how much they considered the career to fulfill agentic goals (power, achievement, and seeking new experiences or excitement) and communal goals (intimacy, affiliation, and altruism). Method: Because career interest was our critical dependent measure, participants rated their interest in the core careers, as well as additional careers. Method: Participants rated several goals according to “how important each of the following kinds of goals is to you personally,” on scales ranging from 1 (not at all important) to 7 (extremely important). Method: Self-efficacy and experience. Measures of self-efficacy included the scientific, mechanical, and computational subscales of the Kuder Task Self-Efficacy Scale as well as participants’ estimated grades in STEM classes. Results: The authors found that STEM careers, relative to other careers, were perceived to impede communal goals. Moreover, communal-goal endorsement negatively predicted interest in STEM careers, even when controlling for past experience and self-efficacy in science and mathematics. STEM careers are perceived as inhibiting communal goals: When individuals highly endorse communal goals, they are less interested in STEM. If women perceive STEM as antithetical to highly valued goals, it is not surprising that even women talented in these areas might choose alternative career paths. Certainly, traditionally studied predictors of STEM interest, such as agentic motivations or self-efficacy, continue to be critical factors. Our argument is not that the study of communal motivations should replace agentic motivations or self-efficacy, but that this traditional approach overlooks critically important information. Quote “It is ironic that STEM fields hold the key to helping many people, but are commonly regarded as antithetical (or, at best, irrelevant) to such communal goals. However, the first step toward change is increasing knowledge about this belief and its consequences. Interventions could not only provide opportunities for girls and young women to succeed in mathematics and science but also demonstrate how STEM fields involve helping and collaborating with other people. For example, our current research investigates how portraying science or engineering careers as more other-oriented fosters positivity.” (p.1056). Abstract Although women have nearly attained equality with men in several formerly male-dominated fields, they remain underrepresented in the fields of science, technology, engineering, and mathematics (STEM). We argue that one important reason for this discrepancy is that STEM careers are perceived as less likely than careers in other fields to fulfill communal goals (e.g., working with or helping other people). Such perceptions might disproportionately affect women’s career decisions, because women tend to endorse communal goals more than men. As predicted, we found that STEM careers, relative to other careers, were perceived to impede communal goals. Moreover, communal-goal endorsement negatively predicted interest in STEM careers, even when controlling for past experience and self-efficacy in science and mathematics. Understanding how communal goals influence people’s interest in STEM fields thus provides a new perspective on the issue of women’s representation in STEM careers. APA Style Reference Diekman, A. B., Brown, E. R., Johnston, A. M., \u0026amp; Clark, E. K. (2010). Seeking congruity between goals and roles: A new look at why women opt out of science, technology, engineering, and mathematics careers. Psychological science, 21(8), 1051-1057. https://doi.org/10.1177/0956797610377342\nYou may also be interested in Unequal effects of the COVID-19 pandemic on scientists (Myers et al., 2019) Against Eminence (Vazire, 2017) Scientific Eminence: Where Are the Women? (Eagly \u0026amp; Miller, 2016) The Gender Gap: Who Is (and Is Not) Included on Graduate-Level Syllabi in Social/Personality Psychology (Skitka et al., 2020) Leveraging a collaborative consortium model of mentee/mentor training to foster career progression of under-represented post-doctoral researchers and promote institutional diversity and inclusion (Risner et al., 2020) #bropenscience is broken science (Whitaker \u0026amp; Guest, 2020) ⌺ Main Takeaways: [Scientists have] been spurred into action by a variety of disappointing stories about irreplicable research – due to both purposeful misconduct and variable guidance for transparent reporting standards – as well as inspired by the pre-existing ideals of ‘open science’. It is a very narrow demographic of researchers who have the institutional support to spend time on such projects as well as the fortune to be publicly acknowledged for their hard work. However, #bropenscience has also been misunderstood and misrepresented. Not all men are bros, and not all bros are men. A bro will often be condescending, forthright, aggressive, overpowering, and lacking kindness and self-awareness. Although bros solicit debate on important issues, they tend to resist descriptions of the complexities, nuances, and multiple perspectives on their argument. Bros find it hard to understand – or accept – that others will have a different lived experience. At its worst, #bropenscience is the same closed system as before. Broscience creates new breaks within science such as excluding people from participating in open science generally due to the behaviour of a vocal, powerful and privileged minority. It’s a type of exclusionary, monolithic, inflexible rhetoric that ignores or even builds on structural power imbalances. Most researchers don’t fit neatly into many of the broposed solutions, and science is not a monolith. The authors have both dealt with published findings that cannot be reproduced, been driven by frustration at the inefficiency of current research practices and have different work ethics and philosophies. This is a feature, not a bug. A diverse and inclusive definition of open science is necessary to truly reform academic practice. At its core, open scholarship reminds researchers why they wanted to conduct research in the first place: to learn and to educate. Regardless of individual intentions, groups can easily develop and perpetuate elitist, yet informal social structures, recreating the same biases inherent in society at large. Bro-y culture dominates at the leadership level in science and technology because it always has and there aren’t enough explicit processes to deconstruct these biases. To avoid perpetuating ‘bropen’ practices, the authors recommend following three core principles: Understanding: You make the work accessible and clear; Sharing: You make the work easy to adapt, reproduce, and spread; and Participation \u0026amp; inclusion: You build shared ownership and agency with contributors through accountability, equity, and transparency to make the work inviting, relevant, safe, and sustainable for all. Inclusive actions that you can take to make science more open to underrepresented minorities include: using a microphone at in person events or providing live transcription and sign language translation for online events so that hard of hearing and autistic colleagues (among others) can engage more effectively. Editors and tenured faculty members can and should do the most to improve equity and inclusion in academia. What are the actions you can take that will improve scholarship for all? Ultimately, the only way to dismantle structural and systemic biases is to listen to those who experience them. Quote “It’s likely infeasible to include all the possible open scholarship elements mentioned above in the readers’ work. Therefore, and to change metaphors, the authors encourage the reader to take a healthy and balanced portion from the open science buffet...Binging from the many different topics that fall under open scholarship will leave you feeling overwhelmed and exhausted...take what you can and what benefits you now, and then come back for more when you have the time and mental space to develop a new skill.” (p.2) Abstract Kirstie Whitaker and Olivia Guest ask how open ‘open science’ really is. APA Style Reference Whitaker, K., \u0026amp; Guest, O. (2020). #bropenscience is broken science. The Psychologist, 33, 34-37. You may also be interested in Seeking an exit plan (Woolston, 2020) The mental health of PhD researchers demands urgent attention (Nature, 2019) Postdocs in crisis: science risks losing the next generation (Nature, 2020) Boosting research without supporting universities is wrong-headed (Nature, 2020b) Unequal effects of the COVID-19 pandemic on scientists (Myers et al., 2019) Against Eminence (Vazire, 2017) Scientific Eminence: Where Are the Women? (Eagly \u0026amp; Miller, 2016) The Gender Gap: Who Is (and Is Not) Included on Graduate-Level Syllabi in Social/Personality Psychology (Skitka et al., 2020) Leveraging a collaborative consortium model of mentee/mentor training to foster career progression of under-represented post-doctoral researchers and promote institutional diversity and inclusion (Risner et al., 2020) Prestige drives epistemic inequality in the diffusion of scientific ideas (Morgan et al., 2018) On supporting early-career black scholars (Roberson, 2020) Open Science Isn’t Always Open to All Scientists (Bahlai et al., 2019) The Matthew effect in science funding (Bol et al., 2018) ls There a Positive Correlation between Socioeconomic Status and Academic Achievement? (Quagliata, 2008) Education and Socio-economic status (APA, 2017b) Ethnic and Racial minorities and socio-economic status (APA, 2017) Women and Socio-economic status (APA, 2010) Disability and Socio-economic status (APA, 2010) Lesbian, Gay, Bisexual and Transgender Persons \u0026amp; Socioeconomic Status (APA, 2010) Science faculty’s subtle gender biases favour male students (Moss-Racusin et al., 2012) ⌺ Main Takeaways: The present research investigates whether faculty gender bias exists within academic biological and physical sciences, whether it might exert an independent effect on the gender disparity as students progress through the pipeline to careers in science, and finally, whether, given an equally qualified male and female student, science faculty members would show preferential evaluation and treatment of the male student to work in their laboratory. Also, the authors investigated whether faculty members’ perceptions of student competence would help to explain why they would be less likely to hire a female (relative to an identical male) student for a laboratory manager position. Science faculty’s perceptions and treatment of students would reveal a gender bias favoring male students in perceptions of competence and hireability, salary conferral, and willingness to mentor; Faculty gender would not influence this gender bias. Hiring discrimination against the female student would be mediated (i.e., explained) by faculty perceptions that a female student is less competent than an identical male student. Finally, participants’ preexisting subtle bias against women would moderate (i.e., impact) results, such that subtle bias against women would be negatively related to evaluations of the female student, but unrelated to evaluations of the male student. Method: 127 faculty participants from Biology, Chemistry and Physics provided feedback on materials of an undergraduate science student who stated their intention to go on to graduate school and those who applied for a science laboratory manager position and evaluated a real student who received faculty participants’ ratings as feedback to help their career development. Method: Participants were randomly assigned to one of two student gender conditions: male or female. Using validated scales, participants rated student competence, their own likelihood of hiring the student, selecting an annual starting salary for the student, indicated how much career mentoring they would provide and also had to fill in the Modern Sexism Scale. Results: Faculty participants rated the male applicant as significantly more competent and hireable than the (identical) female applicant. These participants also selected a higher starting salary and offered more career mentoring to the male applicant. The gender of the faculty participants did not affect responses, such that female and male faculty were equally likely to exhibit bias against the female student. Mediation analyses indicated that the female student was less likely to be hired because she was viewed as less competent. The authors found that preexisting subtle bias against women played a moderating role, such that subtle bias against women was associated with less support for the female student, but was unrelated to reactions to the male student. A female student was seen as less competent and less worthy of being hired than an identical male student with a smaller starting salary and less career mentoring. The subtle gender bias is important to address as it could translate into large real-world disadvantages in judgment and treatment of female science students. The female student was less likely to be hired than male student because the former was seen as less competent. Faculty participants’ pre-existing subtle bias against women undermined perceptions and treatment of the female, not male, student, indicating chronic subtle biases may harm women within academic science. Female faculty members were just as likely as their male colleagues to favour the male student. Faculty members’ bias was independent of their gender, scientific discipline, age and tenure status indicating this bias is not consciously done and is unintentionally generated from widespread cultural stereotypes. Faculty participants reported liking the female more than male students indicates that faculty members’ are not overtly hostile toward women. Faculty members of both genders are affected by enduring cultural stereotypes about a women’s lack of science competence, which translate into biases in student evaluation and mentoring. Not only do women encounter biased judgments regarding their competence and hireability but receive less faculty encouragement and financial rewards than identical male counterparts. Abstract Despite efforts to recruit and retain more women, a stark gender disparity persists within academic science. Abundant research has demonstrated gender bias in many demographic groups, but has yet to experimentally investigate whether science faculty exhibit a bias against female students that could contribute to the gender disparity in academic science. In a randomized double-blind study (n = 127), science faculty from research-intensive universities rated the application materials of a student—who was randomly assigned either a male or female name—for a laboratory manager position. Faculty participants rated the male applicant as significantly more competent and hireable than the (identical) female applicant. These participants also selected a higher starting salary and offered more career mentoring to the male applicant. The gender of the faculty participants did not affect responses, such that female and male faculty were equally likely to exhibit bias against the female student. Mediation analyses indicated that the female student was less likely to be hired because she was viewed as less competent. We also assessed faculty participants’ preexisting subtle bias against women using a standard instrument and found that preexisting subtle bias against women played a moderating APA Style Reference Moss-Racusin, C. A., Dovidio, J. F., Brescoll, V. L., Graham, M. J., \u0026amp; Handelsman, J. (2012). Science faculty’s subtle gender biases favor male students. Proceedings of the national academy of sciences, 109(41), 16474-16479. https://doi.org/10.1073/pnas.1211286109\nYou may also be interested in Unequal effects of the COVID-19 pandemic on scientists (Myers et al., 2019) Against Eminence (Vazire, 2017) Scientific Eminence: Where Are the Women? (Eagly \u0026amp; Miller, 2016) #bropenscience is broken science (Whitaker \u0026amp; Guest, 2020) ⌺ Examining Gender Bias in Student Evaluations of Teaching for Graduate Teaching Assistants (Khazan et al., 2019) ⌺ The association between early career informal mentorship in academic collaborations and junior author performance (AlShebli et al., 2020)⌺ Main Takeaways: This paper has been retracted: https://retractionwatch.com/2020/12/21/nature-communications-retracts-much-criticized-paper-on-mentorship/ By mentoring novices, senior members pass on the organizational culture, best practices, and the inner workings of a profession. In this way, the mentor–protégé relationship provides the social glue that links generations within a field. The authors study mentorship in scientific collaboration, where a junior scientist is supported by potentially multiple senior collaborators, without them necessarily having formal supervisory roles.The authors also identify 3 million mentor-protege pairs and survey a random sample, verifying that their relationship involved some form of mentorship. Method: This dataset includes records of scientific publications specifying the date of the publication, the authors’ names and affiliations, and the publication venue. It also contains a citation network in which every node represents a paper and every directed edge represents a citation. While the number of citations of any given paper is not provided explicitly, it can be calculated from the citation network in any given year. Additionally, every paper is positioned in a field-of-study hierarchy, the highest level of which comprises 19 scientific disciplines. The authors derive two key measures: the discipline of scientists and their impact and additional measures such as the scientists’ gender. Whenever a junior scientist (with academic age = 7) publishes a paper with a senior scientist (academic age \u0026gt; 7), the former is defined as a protégé, and the latter is delineated as a mentor. The author analyze every mentor–protégé dyad that satisfies all of the following conditions: (i) the protégé has at least one publication during their senior years without a mentor; (ii) the affiliation of the protégé is in the US throughout their mentorship years; (iii) the main discipline of the mentor is the same as that of the protégé; (iv) the mentor and the protégé share an affiliation on at least one publication; (v) during the mentorship period, the mentor and the protégé worked together on a paper whose number of authors is 20 or less; and (vi) the protégé does not have a gap of 5-years or more in their publication history. Results: The author finds that mentorship quality predicts the scientific impact of the papers written by protégés post mentorship without their mentors. The author observed that increasing the proportion of female mentors is associated not only with a reduction in post-mentorship impact of female protégés, but also a reduction in the gain of female mentors. The authors found that both have an independent association with the protégé’s impact post mentorship without their mentors. Interestingly, the big-shot experience seems to matter more than the hub experience, implying that the scientific impact of mentors matters more than the number of their collaborators. The association between the big-shot experience and the post-mentorship outcome persists regardless of the discipline, the affiliation rank, the number of mentors, the average age of the mentors, the protégé’s gender, and the protégé’s first year of publication. The present study suggests that female protégés who remain in academia reap more benefits when mentored by males rather than equally-impactful females. The specific drivers underlying this empirical fact could be multifold, such as female mentors serving on more committees, thereby reducing the time they are able to invest in their protégés or women taking onless recognized topics that their protégés emulate. Additionally, findings also suggest that mentors benefit more when working with male protégés rather than working with comparable female protégés, especially if the mentor is female. These conclusions are all deduced from careful comparisons between protégés who published their first mentored paper in the same discipline, in the same cohort, and at the very same institution. One potential explanation could be that, historically, male scientists had enjoyed more privileges and access to resources than their female counterparts, and thus were able to provide more support to their protégés. Alternatively, these findings may be attributed to sorting mechanisms within programs based on the quality of protégés and the gender of mentors. The gender-related findings suggest that current diversity policies promoting female–female mentorships, as well-intended as they may be, could hinder the careers of women who remain in academia in unexpected ways. Female scientists, in fact, may benefit from opposite-gender mentorships in terms of their publication potential and impact throughout their post-mentorship careers. Abstract We study mentorship in scientific collaborations, where a junior scientist is supported by potentially multiple senior collaborators, without them necessarily having formal supervisory roles. We identify 3 million mentor–protégé pairs and survey a random sample, verifying that their relationship involved some form of mentorship. We find that mentorship quality predicts the scientific impact of the papers written by protégés post mentorship without their mentors. We also find that increasing the proportion of female mentors is associated not only with a reduction in post-mentorship impact of female protégés, but also a reduction in the gain of female mentors. While current diversity policies encourage same-gender mentorships to retain women in academia, our findings raise the possibility that opposite-gender mentorship may actually increase the impact of women who pursue a scientific career. These findings add a new perspective to the policy debate on how to best elevate the status of women in science. APA Style Reference AlShebli, B., Makovi, K., \u0026amp; Rahwan, T. (2020). The association between early career informal mentorship in academic collaborations and junior author performance. Nature communications, 11(1), 1-8. https://doi.org/10.1038/s41467-020-19723-8\nYou may also be interested in Unequal effects of the COVID-19 pandemic on scientists (Myers et al., 2019) Against Eminence (Vazire, 2017) Scientific Eminence: Where Are the Women? (Eagly \u0026amp; Miller, 2016) #bropenscience is broken science (Whitaker \u0026amp; Guest, 2020) ⌺ Science faculty’s subtle gender biases favour male students (Moss-Racusin et al., 2012) ⌺ Examining Gender Bias in Student Evaluations of Teaching for Graduate Teaching Assistants (Khazan et al., 2019) ⌺ Is science only for the rich? (Lee, 2016) ⌺ Main Takeaways: Few countries collect detailed data on socioeconomic status, but the available numbers consistently show that nations are wasting the talents of underprivileged youth who might otherwise be tackling challenges in health, energy, pollution, climate change and a host of other societal issues. And it’s clear that the universal issue of class is far from universal in the way it plays out. Here, Nature looks at eight countries around the world, and their efforts to battle the many problems of class in science. Students from poor districts therefore end up being less prepared for university-level science than are their wealthier peers, many of whom attended well-appointed private schools. That also puts the students at a disadvantage in the fiercely competitive applications process: only about 40% of high-school graduates in the lowest-income bracket enrolled in a university in 2013, versus about 68% of those born to families with the highest incomes. The students who do get in then have to find a way to pay the increasingly steep cost of university. Between 2003 and 2013, undergraduate tuition, fees, room and board rose by an average of 34% at state-supported institutions, and by 25% at private institutions, after adjusting for inflation. The bill at a top university can easily surpass US$60,000 per year. Many students are at least partly supported by their parents, and can also take advantage of scholarships, grants and federal financial aid. Many, like Quasney, work part time. But if graduate students have to worry about repaying student loans, that can dissuade them from continuing with their scientific training. In China: Abstract Around the world, poverty and social background remain huge barriers in scientific careers. APA Style Reference Lee, J. J. (2016). Is science only for the rich?. Nature, 537(7621), 466-467.\nYou may also be interested in #bropenscience is broken science (Whitaker \u0026amp; Guest, 2020) ⌺ ls There a Positive Correlation between Socioeconomic Status and Academic Achievement? (Quagliata, 2008) Education and Socio-economic status (APA, 2017b) Ethnic and Racial minorities and socio-economic status (APA, 2017) Women and Socio-economic status (APA, 2010) Disability and Socio-economic status (APA, 2010) Lesbian, Gay, Bisexual and Transgender Persons \u0026amp; Socioeconomic Status (APA, 2010) The Relation Between Family Socioeconomic Status and Academic Achievement in China: A Meta-analysis (Liu et al., 2020) Socioeconomic Status and Academic Outcomes in Developing Countries: A Meta-Analysis (Kim et al., 2019) Speaker Introductions at Internal Medicine Grand Rounds: Forms of Address Reveal Gender Bias (Files et al., 2017)⌺ Main Takeaways: The authors hypothesize that female speakers in this professional setting are more often addressed by first name or equivalent than their male counterparts during speaker introductions. The authors examined the association between gender and address practices used during formal introductions of speakers in Internal Medicine Grand Rounds (IMGR). Method: 134 unique grand rounds presentations listed in a video archive library were accessed and reviewed. Of the 124 grand rounds reviewed, 83 had more than 1 introduction (introducer introducing the speaker) with each introduction representing an opportunity for the introducer to utilize the appropriate professional title. Each grand round had between one and five introducers. The authors analyzed the form of address used in up to 5 speaker introductions in each grand round. Results: Female introducers were more likely to use professional titles when introducing any speaker during the first form of address compared with male introducers. Female dyads utilized formal titles during the first form of address compared with male dyads who utilized a formal title 72.4% of the time. In mixed-gender dyads, where the introducer was female and speaker male, formal titles were used 95.0% of the time. Male introducers of female speakers utilized professional titles 49.2% of the time. In this study, women introduced by men at IMGR were less likely to be addressed by their professional title than were men introduced by men. In contrast, women introducers were more formal in both same- and mixed-gender interactions. The findings demonstrate that female introducers compared with male introducers were more likely to use professional titles when introducing any speaker, male or female, during the first form of address. However, there were striking differences in how males utilized their informal introduction style depending on whether the speaker was a man or woman. While women consistently and nearly universally introduced both male and female speakers by their formal titles during first form of address, men used male’s formal title during introductions 72% of the time, whereas acknowledging female speakers with their professional title only 49.2% (31/63) of the time. Female introducers with their high utilization of formal title during the first form of address exhibited no change in their utilization of formal title. When all introductions by men were included, the rate of utilization of professional titles increased slightly, but a gender difference remained. Despite multiple opportunities to acknowledge the speakers’ credentials, the title of Dr. was withheld by male introducers from 41.3% of female speakers compared with only 24.3% of male speakers. This study supports what many female physicians have experienced and discussed informally; the withholding of their professional titles when they are referenced or addressed by their male colleagues. Perhaps this is made more noticeable by their finding that women use formal titles close to 100% of the time for both the men and the women they introduce. This formal practice by women may engender an expectation of reciprocity, thus, further amplifying the disparity. While the did find that men are less formal overall and do withhold the professional title of Dr. during the first form of address from over one quarter of male speakers, it is important to view the experience from the perspective of the female speaker. As she prepares to assume the podium for her formal presentation, she will hear her formal title from almost all of the female introducers; however, she has less than a 50% likelihood that a male introducer will set the tone in the first form of address by calling her ‘‘Doctor.’ The significance of these linguistic biases lies in the fact that they implicitly communicate stereotypes to the individual, in this case women in medicine, and thereby contribute to the transmission and maintenance of socially shared stereotypes which ultimately have the potential to affect both the recipient and the audience. Overt discrimination is usually obvious and well recognized by those experiencing it, whereas more subtle forms of gender bias are difficult to describe, explain, and to address especially when inflicted upon an individual who may feel unsafe to address the practice as it occurs. Furthermore, unrecognized aspects of an organization’s culture may have different effects on men and women. It is the authors’ hope that objective documentation of the gender disparity identified in speaker introductions at IMGR will provide validation to women who have experienced it. Abstract Background: Gender bias has been identified as one of the drivers of gender disparity in academic medicine. Bias may be reinforced by gender subordinating language or differential use of formality in forms of address. Professional titles may influence the perceived expertise and authority of the referenced individual. The objective of this study is to examine how professional titles were used in the same and mixed-gender speaker introductions at Internal Medicine Grand Rounds (IMGR).Methods: A retrospective observational study of video-archived speaker introductions at consecutive IMGR was conducted at two different locations (Arizona, Minnesota) of an academic medical center. Introducers and speakers at IMGR were physician and scientist peers holding MD, PhD, or MD/PhD degrees. The primary outcome was whether or not a speaker’s professional title was used during the first form of address during speaker introductions at IMGR. As secondary outcomes, we evaluated whether or not the speakers professional title was used in any form of address during the introduction.Results: Three hundred twenty-one forms of address were analyzed. Female introducers were more likely to use professional titles when introducing any speaker during the first form of address compared with male introducers (96.2% [102/106] vs. 65.6% [141/215]; p \u0026lt; 0.001). Female dyads utilized formal titles during the first form of address 97.8% (45/46) compared with male dyads who utilized a formal title 72.4% (110/152) of the time ( p = 0.007). In mixed-gender dyads, where the introducer was female and speaker male, formal titles were used 95.0% (57/60) of the time. Male introducers of female speakers utilized professional titles 49.2% (31/63) of the time ( p \u0026lt; 0.001).Conclusion: In this study, women introduced by men at IMGR were less likely to be addressed by professional title than were men introduced by men. Differential formality in speaker introductions may amplify isolation, marginalization, and professional discomfiture expressed by women faculty in academic medicine. APA Style Reference Files, J. A., Mayer, A. P., Ko, M. G., Friedrich, P., Jenkins, M., Bryan, M. J., ... \u0026amp; Duston, T. (2017). Speaker introductions at internal medicine grand rounds: forms of address reveal gender bias. Journal of women\u0026#39;s health, 26(5), 413-419. https://doi.org/10.1089/jwh.2016.6044\nYou may also be interested in Unequal effects of the COVID-19 pandemic on scientists (Myers et al., 2019) Against Eminence (Vazire, 2017) Scientific Eminence: Where Are the Women? (Eagly \u0026amp; Miller, 2016) #bropenscience is broken science (Whitaker \u0026amp; Guest, 2020) ⌺ Examining Gender Bias in Student Evaluations of Teaching for Graduate Teaching Assistants (Khazan et al., 2019) ⌺ Science faculty’s subtle gender biases favour male students (Moss-Racusin et al., 2012) ⌺ Redesign open science for Asia, Africa and Latin America (Onie, 2020)⌺ Main Takeaways: Research is relatively new in many countries in Asia, Africa and Latin America. Across these regions, young scientists are working to build practices for open science from the ground up. The aim is that scientific communities will incorporate these principles as they grow. But these communities’ needs differ from those that are part of mature research systems. So, rather than shifting and shaping established systems, scientists are endeavouring to design new ones. Financial and career incentives to publish (or disadvantages from not publishing) are common government policies in countries such as Indonesia, China and Brazil where research culture is still being shaped. They aim to increase publication quantity to ‘catch up’ with other countries, but inadvertently encourage poor research practices. Lower-income countries cannot waste resources on funding untrustworthy research. Policies should therefore be designed to improve transparency, relevance and scientific rigour, rather than just to increase output — especially if governments want to use research to inform decision-making. Governments must also provide the training, resources and motivation needed for people to take these changes to heart. Crucially, the team will include researchers from many different types of university, not just the largest ones. Going forward, the institute will monitor whether the repository improves research quality. Other countries will face different issues. But a commonality will be that all stakeholders — not just the rich or prestigious ones — should be involved in finding a solution. Most universities in Asia, Africa and Latin America were set up for education. Many are ill-equipped to perform research and lack the proper infrastructure. Sustainable changes require education. Universities should train researchers not just in field-specific theories, but also in how to improve scientific practice. This training should cover the pitfalls of modern academia (e.g. prestige and academic metrics) have contributed to publication bias. It must address the consequences of succumbing to these pressures for the quality, replicability and trustworthiness of research. And it should honestly highlight disagreements about whether and when these practices actually work — debates about when pre-registration of research is and is not useful, for instance. And researchers must learn about these topics as they begin their research careers, even as undergraduates, rather having to modify existing practices later. Training in good scientific practices will set scientists up to think more critically and to adopt practices that increase the credibility of their work. Training will also enable researchers to add their diverse voices to continuing debates about open science, including active consideration of how science can benefit society, locally and globally. This shift towards open research might require a reworking of the overall training package, reducing the number of field-specific courses to avoid an overwhelming workload. Journals should take an active role in reducing under-representation, without compromising rigour. Ask authors to explicitly describe the populations they study upfront, and not to generalize their findings beyond this sample without good reason. Open reviews could reduce potential bias against samples from outside Western countries. Established journals should make efforts to communicate their standards to scientists in developing research cultures, and could also host special issues focused on understanding under-represented populations. A lack of funding and travel restrictions in many parts of Asia, Africa and Latin America reduce these opportunities for international collaboration, networking and travel, leading to researchers becoming more isolated. Such problems need to be acknowledged explicitly and confronted. Metrics and policies should be in place only if they are useful to science’s goal: knowledge accumulation for the greater societal good. Constant monitoring and introspection are therefore essential. Some of the best initiatives to improve science today might not be relevant in the future. Quote “If young research cultures can guard against harmful practices becoming ingrained, they have the opportunity to lay down a new type of strategy for open research. This could avoid the pressures that can sometimes warp research in the Western world and ultimately produce work that is credible and beneficial to society. The goal is not to replicate what is done in North America, Europe and Australia — rather, it is to do better.” (p.37) Abstract Researchers in many countries need custom-built systems to do robust and transparent science. APA Style Reference Onie, S.(2020). Redesign open science for Asia, Africa and Latin America. Nature, 587, 35-37. https://doi.org/10.1038/d41586-020-03052-3\nYou may also be interested in #bropenscience is broken science (Whitaker \u0026amp; Guest, 2020) ⌺ ls There a Positive Correlation between Socioeconomic Status and Academic Achievement? (Quagliata, 2008) Education and Socio-economic status (APA, 2017b) Ethnic and Racial minorities and socio-economic status (APA, 2017) Women and Socio-economic status (APA, 2010) Disability and Socio-economic status (APA, 2010) Lesbian, Gay, Bisexual and Transgender Persons \u0026amp; Socioeconomic Status (APA, 2010) The Relation Between Family Socioeconomic Status and Academic Achievement in China: A Meta-analysis (Liu et al., 2020) Socioeconomic Status and Academic Outcomes in Developing Countries: A Meta-Analysis (Kim et al., 2019) Is science only for the rich? (Lee, 2016) ⌺ On supporting early-career black scholars (Roberson, 2020) ⌺ Main Takeaways: Non-Black researchers need to take immediate support for early-career Black scholars. “Maybe you were in a seminar where a Black doctoral student pushed back against a racist disciplinary norm, and you silently agreed and followed up with them afterwards to let them know that you support them.” This silence in public signals to Black scholars that they are not welcome in these spaces We must challenge white supremacy in academia. Speaking up about this is much more costly for Black scholars, who face an onslaught of racist micro- and macro-aggressions on a daily basis. The burden should not fall on their shoulders. We should be proactive in our outreach. We should invite early-career Black scholars, if they have expertise to improve a research project. Our careers and science will benefit from this help. “Do not just encourage [Black scholars] to apply, provide material support to promote our successful applications; share funded grants with [Black scholars], work with [Black scholars] on developing compelling aims pages, and write [Black scholars] a persuasive letter of support. Supporting [Black scholars] on manuscripts and funding opportunities can mitigate some of the barriers in science that often stunt Black success.” Inviting Black scholars will increase their credibility as experts and expand the audience’s familiarity with their scholarship. Manels are now being prohibited but we need to eliminate all-white speaker panels. Educate yourself on rising Black scholars in your field, learn from early-career Black researchers, investigate journals that publish their scholarships, be familiar with the Black community’s professional societies, affinity groups and diversify your following list on Twitter. Incorporate Black scholar’s work into your syllabi. This is necessary to eliminate structural racism. However, it requires individuals with the most amount of power. These steps will promote Black people to thrive among trainees and early-career scholars. This will remove barriers to promote a more inclusive environment! Quote “Do not just encourage [Black scholars] to apply, provide material support to promote our successful applications; share funded grants with [Black scholars], work with [Black scholars] on developing compelling aims pages, and write [Black scholars] a persuasive letter of support. Supporting [Black scholars] on manuscripts and funding opportunities can mitigate some of the barriers in science that often stunt Black success.” Abstract Professor Mya Roberson provides a detailed commentary about the struggles that Black people encounter in academia and starting steps to eliminate structural racism. APA Style Reference Roberson, M. L. (2020). On supporting early-career Black scholars. Nature Human Behaviour, 1-1. https://doi.org/10.1038/s41562-020-0926-6\nYou may also be interested in Open Science Isn’t Always Open to All Scientists (Bahlai et al., 2019) Bias against research on gender bias (Cislak et al., 2018) ⌺ Main Takeaways: Scientific inquiries often disregard the moderating roles of sex or gender. Moreover, some finding applies only to male participants, producing biased knowledge. Findings related to men may be irrelevant and harmful to women. Studies on gender bias are often met with lower appreciation in the scientific community compared to studies on race bias. The present study investigated whether research on gender bias is prone to biased evaluation resulting in fewer and less prestigious publications and fewer funding opportunities. The present study compared articles for gender bias and race bias in impact factor and grant support. Method: 1485 articles published in 520 journals were assigned a numerical value based on type of bias. Two peer review criteria were used: Impact factor and whether the article was supported by finding or not. Results: Articles on gender bias are funded less often and published in journals with lower Impact factor than articles on similar instances of social discrimination. Discussion: Results suggest that bias against gender bias research is not merit based but reflects the topic\u0026#39;s lower prestige and appreciation due to a generalised gender bias. Another potential explanation for the observed difference in grant funding is the relative difference in availability of participant samples. Recruiting racially diverse samples may be more difficult, time-consuming and costly, while recruiting gender-diverse samples does not have similar issues. It is less plausible, however, that differences in participant samples affect researcher’s decisions of the outlet for their work. It may be that researchers are aware of bias against gender bias research and consider their own work less suitable for more prestigious journals. Research on gender bias is more often reviewed by male researchers than research on race bias. Rejection by more prestigious journals show subtle bias in perceived quality of studies evidencing gender discrimination. Quote “This discussion is primarily important in order for gender bias to be properly acknowledged within the scientific community and to pursue further examination of this powerful source of inequality that severely affects many women in the world.” (p. 200) Abstract The bias against women in academia is a documented phenomenon that has had detrimental consequences, not only for women, but also for the quality of science. First, gender bias in academia affects female scientists, resulting in their underrepresentation in academic institutions, particularly in higher ranks. The second type of gender bias in science relates to some findings applying only to male participants, which produces biased knowledge. Here, we identify a third potentially powerful source of gender bias in academia: the bias against research on gender bias. In a bibliometric investigation covering a broad range of social sciences, we analyzed published articles on gender bias and race bias and established that articles on gender bias are funded less often and published in journals with a lower Impact Factor than articles on comparable instances of social discrimination. This result suggests the possibility of an underappreciation of the phenomenon of gender bias and related research within the academic community. Addressing this meta-bias is crucial for the further examination of gender inequality, which severely affects many women across the world. APA Style Reference Cislak, A., Formanowicz, M., \u0026amp; Saguy, T. (2018). Bias against research on gender bias. Scientometrics, 115(1), 189-200. https://doi.org/10.1007/s11192-018-2667-0\nYou may also be interested in Surviving (thriving) in academia: feminist support networks and women ECRs (Macoun \u0026amp; Miller, 2014) Global gender disparities in science (Lariviere et al., 2013) The Pandemic and Gender Inequality in Academia (Kim \u0026amp; Patterson, Jr, 2020)◈ Gender in the editorial boards of scientific journals: A study on the current state of the art (Ghasemi et al., 2020) ◈ Something’s Got to Give (Flaherty, 2020) ◈ Prestige drives epistemic inequality in the diffusion of scientific ideas (Morgan et al., 2018) ⌺ Main Takeaways: There is no clear evidence that epistemic inequality is driven by non-meritocratic social mechanisms. It remains unknown how an idea spreads in the scientific community. If the origin does shape its scientific discourse, what is the relationship between the intrinsic fitness of the idea and its structural advantage by the prestige of origin? The present study takes a different approach to define how faculty hiring drives epistemic inequality and can determine which researchers are situated in which institutions and the origin of the idea. Method: 5032 tenured or tenure-track faculty data were collected. Data was collected from faculty hiring networks, nodes reflect university and the connections if a PhD was acquired at that university and if they held a tenure-track position. Networks with a self-loop contained individuals who received their PhD at the same institution and held a faculty position. Small departments have high placement power, while large departments have power. Elite institutions have a structural advantage. Faculty hiring may not contribute to the spread of every research idea. Hiring contributes to others. Faculty hiring is a possible mechanism for the diffusion of ideas in academia. The spread of information from a varying level of prestige for universities was investigated. Results: Research from prestigious institutions spreads more quickly and completely than work of similar quality originating from less prestigious institutions. Higher quality research from less prestigious universities has similar success as lower-quality research in more prestigious universities. Even when the assessment of an idea’s quality is objective, idea dissemination in academia is not meritocratic. Researchers at prestigious institutions benefit from structural advantage allowing ideas to be more easily spread throughout the network of institutions and impact discourse of science. Lower quality ideas are overshadowed by comparable ideas from more prestigious institutions, high-quality ideas circulate widely, irrespective of origin. Abstract The spread of ideas in the scientific community is often viewed as a competition, in which good ideas spread further because of greater intrinsic fitness, and publication venue and citation counts correlate with importance and impact. However, relatively little is known about how structural factors influence the spread of ideas, and specifically how where an idea originates might influence how it spreads. Here, we investigate the role of faculty hiring networks, which embody the set of researcher transitions from doctoral to faculty institutions, in shaping the spread of ideas in computer science, and the importance of where in the network an idea originates. We consider comprehensive data on the hiring events of 5032 faculty at all 205 Phd.-granting departments of computer science in the U.S. and Canada, and on the timing and titles of 200,476 associated publications. Analyzing five popular research topics, we show empirically that faculty hiring can and does facilitate the spread of ideas in science. Having established such a mechanism, we then analyze its potential consequences using epidemic models to simulate the generic spread of research ideas and quantify the impact of where an idea originates on its long-term diffusion across the network. We find that research from prestigious institutions spreads more quickly and completely than work of similar quality originating from less prestigious institutions. Our analyses establish the theoretical trade-offs between university prestige and the quality of ideas necessary for efficient circulation. Our results establish faculty hiring as an underlying mechanism that drives the persistent epistemic advantage observed for elite institutions, and provide a theoretical lower bound for the impact of structural inequality in shaping the spread of ideas in science. APA Style Reference Morgan, A. C., Economou, D. J., Way, S. F., \u0026amp; Clauset, A. (2018). Prestige drives epistemic inequality in the diffusion of scientific ideas. EPJ Data Science, 7(1), 40. https://doi.org/10.1140/epjds/s13688-018-0166-4\nYou may also be interested in Early co-authorship with top scientist predicts success in academic careers (Li et al., 2019) Open Science Isn’t Always Open to All Scientists (Bahlai et al., 2019) Scientists’ Reputations are Based on Getting it Right, not being Right (Ebersole et al., 2016) The Matthew effect in science funding (Bol et al., 2018) Open Science Isn’t Always Open to All Scientists (Bahlai et al., 2019) ⌺ Main Takeaways: Open science focuses on accountability and transparency, invites anyone to observe, contribute and create. Open science focuses on conviction that research performs in dialogue with society. Science is a mainstreamed but increasing sense of competition rewards scientists who discover ideas and publish findings. “... science traditionally has rewarded only scientists who are the first to discover ideas and publish findings, there is resistance to move from “closed” practices…” The broad term of Open Science and resulting vague scope is stalling the progress of the open science movement. We are now often caught up in detailed checklists about whether a project is “open” or not, rather than “focusing on the core goal of accountability and transparency.” All or nothing checklists reduce “the accessibility of science and may reify existing inequalities within this profession.” Open science makes science accessible to everyone but there are systemic barriers (e.g. financial and social) that make open science more accessible to some not others such as career stage, power imbalance, employment stability, financial circumstance, country of origin and cultural context. These barriers prevent scientists from pursuing further and should not be used to deny further participation, including receiving grant funding or job applications. “To truly achieve open science’s transformative vision, it must be universally accessible, so that all people have access to the dialogue of science. Accessible in this context means usable by all, with particular emphasis on communities often not served by scientific products.” Open science practices are not equally accessible to all scientists. aywalls make research inaccessible but Open Access processing fees may prevent scientists from sharing their work, as not all institutions/individuals have the resources to overcome these barriers. If open access is paid out of our personal funds, instead of grant or institution funding sources, it is an unsustainable solution for many scholars that do not have access to these funds. “Yet open tools, code, or data sets are often not valued the same as “normal” academic products, and therefore those who spend their limited time and resources on these products suffer a cost in how they are evaluated for current and future jobs.” Preprints and signed peer reviews may exacerbate inherent biases. “.. forcing transparency in practices that have traditionally operated in a “black box” may exacerbate inherent biases against women and people of color, especially women of color.” Making data available is seen as high risk as someone can publish analyses with your data before you can. Even “a small risk particularly affects members of the scientific community with fewer resources…” Quote “Power imbalance can play a large role in an individual’s ability to convince their research group to use openscience practices and as a result may cause them to not engage in these practices until they have stable employment or are in a senior position.” Abstract Current efforts to make research more accessible and transparent can reinforce inequality within STEM professions. APA Style Reference Bahlai, C., Bartlett, L. J., Burgio, K. R., Fournier, A., Keiser, C. N., Poisot, T., \u0026amp; Whitney, K. S. (2019). Open science isn’t always open to all scientists. American Scientist, 107(2), 78-82. https://doi.org/10.1511/2019.107.2.78\nYou may also be interested in Early co-authorship with top scientist predicts success in academic careers (Li et al., 2019) Prestige drives epistemic inequality in the diffusion of scientific ideas (Morgan et al., 2018) On supporting early-career black scholars (Roberson, 2020) The Matthew effect in science funding (Bol et al., 2018) Surviving (thriving) in academia: feminist support networks and women ECRs (Macoun \u0026amp; Miller, 2014) ⌺ Main Takeaways: This paper argues about how peer support networks may affect the experience of early-career scholars Women who participated in the Feminist Reading Group (FRG) are actively intellectually engaged in theorising their own experiences. The group perform functions linked to reading groups, create an informal space concerned with furthering disciplinary knowledge and developing academic skills. FRG members created a community of belonging among themselves, in which personal support, knowledge, and cultural and social capital were provided. Participants share resources and information about institutional processes and gain the confidence to navigate complex and hostile spaces of the University. School’s official spaces are seen as gendered and not reflective of our research interests or intellectual backgrounds. Participants state that FRG allowed them to continue their studies in times of difficulty. FRG provides opportunities to broaden exposure to other fields and improve critical thinking skills. FRG promotes the learn essential academic skills, since women are able to learn from experience with writing and publishing, and also developing presentation and analytical skills without fear of seeming to be an inadequate researcher. Academic work can be isolating and early career researchers frequently report feeling unsettled, anxious and experiencing self-doubt. FRG re-dresses this opacity and operates as an information sharing network for participants to learn about how things work at the University and in the department. Women graduates receive less mentoring, less involvement in professional and social networking than their male peers. Participation in the FRG also stimulated other academic activities, with members encouraging each other to attend conferences and present paper. Most participants were white, straight, cis-gendered and middle class. The group was whiter than our department as a whole. FRG provides participants with an opportunity to understand individual experiences of exclusion, exploitation, self-doubt, discrimination as shared and fundamentally political in character. Our backgrounds and experiences are not homogeneous, most participants in the reading group are racially and socio-economically privileged. Abstract In this paper, we reflect upon our experiences and those of our peers as doctoral students and early career researchers in an Australian Political Science department. We seek to explain and understand the diverse ways that participating in an unofficial Feminist Reading Group in our department affected our experiences. We contend that informal peer support networks like reading groups do more than is conventionally assumed, and may provide important avenues for sustaining feminist research in times of austerity, as well as supporting and enabling women and emerging feminist scholars in academia. Participating in the group created a community of belonging and resistance, providing women with personal validation, information and material support, as well as intellectual and political resources to understand and resist our position within the often hostile spaces of the University. While these experiences are specific to our context, time and location, they signal that peer networks may offer critical political resources for responding to the ways that women’s bodies and concerns are marginalised in increasingly competitive and corporatised university environments. APA Style Reference Macoun, A., \u0026amp; Miller, D. (2014). Surviving (thriving) in academia: Feminist support networks and women ECRs. Journal of Gender Studies, 23(3), 287-301. https://doi.org/10.1080/09589236.2014.909718\nYou may also be interested in Global gender disparities in science (Lariviere et al., 2013) The Pandemic and Gender Inequality in Academia (Kim \u0026amp; Patterson, Jr, 2020) Gender in the editorial boards of scientific journals: A study on the current state of the art (Ghasemi et al., 2020) Bias against research on gender bias (Cislak et al., 2018) Something’s Got to Give (Flaherty, 2020) Global gender disparities in science (Lariviere et al., 2013) ⌺ Main Takeaways: Gender inequality is still rife in science. There are gender inequalities in hiring, earnings, funding, satisfactions and patenting. Men publish more papers than women. There is no consensus whether gender differences are a result of bias, childbearing or other variables. The present state of quantitative knowledge of gender disparities in science was shaped by anecdotal reports and studies which are localized, monodisciplinary and dated. These studies take little account of changes in scholarly practices. The present study presents a cross-disciplinary bibliographic research to investigate (i) the relationship between gender and academic output, (ii) the extent of collaboration and (iii) the scientific impact of all articles published between 2008 and 2012 and indexed in the Thomson Reuters Web of Science databases Citation disadvantage is highlighted by the fact that women’s publication portfolios are more domestic than male colleagues and profit less from extra citations that international collaborations accrue. Men dominate scientific production in nearly every country (the extent of this domination varies by region). Women account for fewer than 30% fractionalised authorships, while men representation in such publications was more than 70%. Women are underrepresented when it comes to first authorships. For every article with a female first author, there are nearly two (1.93) articles first-authored by men. Female authorship is more prevalent in countries with lower scientific output. Female collaborations are more domestically oriented than collaborations of males from the same country. The present study analysed prominent author positions (sole, first- and last-authorship). When a woman was in any of these roles, paper attracted fewer citations than in cases wherein a man was in one of these roles. Academic pipeline from junior to senior faculty leaks female scientists. Thus it is likely that many of the trends we observed can be explained by the under-representation of women among the elders of science. Barriers to women in science remain widespread worldwide, despite more than a decade of policies aimed at levelling the playing field. For a country to be scientifically competitive, it needs to maximise its human intellectual capital. Collaboration is one of the main drivers of research output and scientific impact. Programmes fostering international collaboration for female researchers might help to level the playing field. No country can afford to neglect the intellectual contributions of half of its population. Abstract Cassidy R. Sugimoto and colleagues present a bibliometric analysis confirming that gender imbalances persist in research output worldwide. APA Style Reference Larivière, V., Ni, C., Gingras, Y., Cronin, B., \u0026amp; Sugimoto, C. R. (2013). Bibliometrics: Global gender disparities in science. Nature News, 504(7479), 211. https://doi.org/10.1038/504211a\nYou may also be interested in Surviving (thriving) in academia: feminist support networks and women ECRs (Macoun \u0026amp; Miller, 2014) The Pandemic and Gender Inequality in Academia (Kim \u0026amp; Patterson, Jr, 2020)◈ Gender in the editorial boards of scientific journals: A study on the current state of the art (Ghasemi et al., 2020) Bias against research on gender bias (Cislak et al., 2018) Something’s Got to Give (Flaherty, 2020) The Pandemic and Gender Inequality in Academia (Kim \u0026amp; Patterson, Jr, 2020)◈ ⌺ Main Takeaways: The COVID-19 pandemic worsened existing gender inequalities across society. The present study investigated the influence of the current pandemic requires addressing an academic publication pipeline best measured in months, if not years. If the pandemic disproportionately influences the productivity of female faculty, the effects on research productivity may not fully materialise for years and evaluation and promotion of female scholars could adversely be affected by gender-related inequalities woven into the system years before. The present study determined the proportion of work- and family-related tweets sent by male and female academics using subject-specific keywords. The pandemic caused the gender-related differences in professional tweeting to increase by 239%. The lockdown increased the gap between male and female faculty member’s propensity to tweet about family and care-giving. Women bear all care-giving activities- both men and women experienced an increase in family-related tweets- patterns we uncover reveal that female careers are more severely taxed by these commitments. Method: Our sample was narrowed to tenure-track or tenured faculty based in the United States, producing approximately 3000 handles. Method: We first identified all tweets related to career-promoting and family-related activities, and began with terms (e.g. publication, new paper, child care and home school). Each tweet was coded as work- and family-related or not. A more extensive set of keywords classified the entire corpus. Most papers and articles are shared on Twitter via URL, tweet was classified as work-related, if shared, URL address indicates file type, publication venue or data repository services. Results: Faculty members of both genders were affected by the pandemic, the gap in work-related tweets between male and female academics roughly tripled following the work-from-home. Variation in effects between junior and senior faculty indicates this relationship is not driven by an intrinsic gender difference. This effect is produced by gendered differences in adapting a work/life balance to the pandemic. Female academics who reach full professor have overcome existing barriers to gender equality in academia. Parenting obligations overshadow all other factors in limiting research productivity, indicating the influence of parenting on productivity. Increased efforts to address these deep-rooted inequalities, the cracks in the pipeline continue to loom large. Gender imbalances are less pronounced among the ranks of junior faculty, efforts to explain biases in early career trajectories would have the greatest long-term influence on the pipeline of female academics. Quote “With gender imbalances less pronounced among the ranks of junior faculty, efforts to account for biases in early career trajectories would have the greatest long-term impact on the pipeline of female academics. Moreover, as female role-models can positively influence young women’s propensities to enter male-dominated fields (Bonneau and Kanthak, 2018; Breda et al., 2020), administrators’ success or failure here could have downstream impacts on female representation in the academy for the next generation.” (p.15) Abstract Does the pandemic exacerbate gender inequality in academia? The temporal lag in publication pipeline complicates the effort to determine the extent to which women’s productivity is disproportionately affected by the COVID-19 crisis. We provide real-time evidence by analyzing 1.8 million tweets from approximately 3,000 political scientists, leveraging their use of social media for career advancement. Using automated text analysis and difference-in-differences estimation, we find that while faculty members of both genders were affected by the pandemic, the gap in work-related tweets between male and female academics roughly tripled following work-from-home. We further argue that these effects are likely driven by the increased familial obligations placed on women, as demonstrated by the increase in family-related tweets and the more pronounced effects among junior academics. Our causal evidence on work-family trade-off provides an opportunity for proactive efforts to address gender disparities that may otherwise take years to manifest. APA Style Reference Kim, E., \u0026amp; Patterson, S. (2020). The Pandemic and Gender Inequality in Academia. Available at SSRN 3666587. http://dx.doi.org/10.2139/ssrn.3666587\nYou may also be interested in Surviving (thriving) in academia: feminist support networks and women ECRs (Macoun \u0026amp; Miller, 2014) Global gender disparities in science (Lariviere et al., 2013) Gender in the editorial boards of scientific journals: A study on the current state of the art (Ghasemi et al., 2020) ◈ Bias against research on gender bias (Cislak et al., 2018) Something’s Got to Give (Flaherty, 2020) ◈ Gender in the editorial boards of scientific journals: A study on the current state of the art (Ghasemi et al., 2020) ◈ ⌺ Main Takeaways: There is a large number of studies on gender in academia, gender in membership of editorial boards of scientific journals garner attention of research and little literature. They make the policies and determine what is accepted for publication and what is not. Admission or rejection of articles influences the academic careers of authors: full professors or PhD students. Gender in editorial boards attracted attention from several researchers, albeit studies focus on journals of a specific field of knowledge. Works dealing with women and academia are addressed, those works focusing on editorial boards are reviewed. Male professors, male authors in journals and male dominance is higher than female counterparts. Women’s receipt of professional awards, prizes and funding increased in the past two decades. Men continue to win a higher proportion of awards and funding for scholarly research than expected based on the nomination pool. Stereotypes about women’s abilities, harsh self-assessment of scientific ability by women than by men; academic and professional climates dissatisfying to women and unconscious bias contribute to achieving fewer awards and funds. Female board representations have improved over time, is consistent across countries, and gendered subdisciplines attract higher female board representations. Inequities persist at the highest level: women are under-represented as editors and on boards of higher ranked journals. Three factors for women under-representation in editorial board: discipline, journal\u0026#39;s prestige and editor’s gender. The last 15 years hinders women’s ability to attain scholarly recognition and advancement and carries risk to the narrow nature and scope of research in the field. They all show a worrying trend of under-representation of women and agree on negative consequences for advancement of science. Abstract Gender issues have been studied in a broad range of fields and in many areas of society, including social relations, politics, labour, and also academia. However, gender in the membership of editorial boards of scientific journals is a topic that only recently has started to attract the attention of researchers, and there is little literature on this subject as of today. The objective of this work is to present a study of the current state of editorial boards with regard to gender. The methodology is based on a literature review of gender issues in academia, and more specifically in the incipient field of gender in editorial boards. The main findings of this work, according to the reviewed bibliography, are that women are underrepresented in academic institutions, that this underrepresentation is increasingly marked in higher rank positions in academia and in editorial boards, and that this carries the risk of narrowing the nature and scope of the research in some fields of knowledge. APA Style Reference Ghasemi, N. M., Perramon Tornil, X., \u0026amp; Simó Guzmán, P. (2019, March). Gender in the editorial boards of scientific journals: a study on the current state of the art. In Congrés Dones Ciència i Tecnologia 2019: Terrassa, 6 i 7 de març de 2019. http://hdl.handle.net/2117/134267\nYou may also be interested in Surviving (thriving) in academia: feminist support networks and women ECRs (Macoun \u0026amp; Miller, 2014) Global gender disparities in science (Lariviere et al., 2013) The Pandemic and Gender Inequality in Academia (Kim \u0026amp; Patterson, Jr, 2020)◈ Bias against research on gender bias (Cislak et al., 2018) Something’s Got to Give (Flaherty, 2020) ◈ Something’s Got to Give (Flaherty, 2020) ◈ ⌺ Main Takeaways: Women\u0026#39;s journal submission rates fell as their caring responsibilities increased due to COVID-19 (see also) based on data from ongoing study of article submissions to preprint databases, whose preliminary results were published in Nature’s Index. Submissions were up since COVID-19, but the share of submissions made by women was down. Submissions by women as first authors (often junior scholars) were especially down, with some indication that they were shifting to middle authors. Female first-author submissions to medRxiv, for example, dropped from 36% in December to 20% in April 2020. Senior and author submissions by women decreased 6% over the same period, while male senior author submissions rose 5%. Other researchers have found COVID-19 related papers in medicine and economics have fewer female authors than expected. At one journal, male authors outnumbered female authors by more than three to one. It was recommended by Melina R. Kibbe, editor of JAMA Surgery, that we should pause the tenure clock during the pandemic. However, critics of this approach have argued this can actually hurt, not help, women and under-represented minorities, as it can delay career progression and decrease lifetime earnings. The status quo is such that men win the COVID-19 game, whereas women, in general, lose. We need to allow part-time work. Different work shifts should be available to those who need them. And agencies should extend grant end dates and allow for increased funding carryover from year to year. Quote “In any case, Power said, the challenge “needs more thinking about and a bigger public conversation, because this situation is not going away fast.” That conversation is long overdue, she added, in that “women and carers are supposed to just fit into a system designed for people without caring responsibilities. There is a saying working mothers have: ‘You have to work like you don’t have children and parents like you don’t have a job.’ And that was before COVID-19.”” (p.10). Abstract Women\u0026#39;s journal submission rates fell as their caring responsibilities jumped due to COVID-19. Without meaningful interventions, the trend is likely to continue. APA Style Reference Flaherty, C. (2020, August, 20). Something\u0026#39;s Got to Give. Inside Higher Ed. Retrieved fromhttps://www.insidehighered.com/news/2020/08/20/womens-journal-submission-rates-continue-fall\nYou may also be interested in Surviving (thriving) in academia: feminist support networks and women ECRs (Macoun \u0026amp; Miller, 2014) Global gender disparities in science (Lariviere et al., 2013) The Pandemic and Gender Inequality in Academia (Kim \u0026amp; Patterson, Jr, 2020)◈ Bias against research on gender bias (Cislak et al., 2018) Gender in the editorial boards of scientific journals: A study on the current state of the art (Ghasemi et al., 2020) ◈ ls There a Positive Correlation between Socioeconomic Status and Academic Achievement? (Quagliata, 2008) ◈ ⌺ Main Takeaways: Poverty rates have been increasing together with a debate on socio-economic status. Parental income is an indicator of socio-economic status reflecting a potential for social and economic resources. Parental education is a component of socio-economic status. Learning in a meaningful context so at-risk students can immediately apply when they have learned and connect it to their own lives and individual experiences. Many dropouts are not only from low SES backgrounds but also from mismatched learning styles. SES affects children’s academic achievement. It is beneficial to determine the type of home environment, how educators will best support them at school. Learning environment must be structured to achieve the highest level of internal motivation from all students. School success is greatly determined by a family\u0026#39;s socio-economic status. American society may be failing to provide educational opportunities for every student and citizen irrespective of socio-economic background. Many poor students come to school without social and economic benefits available to most middle and high SES students. Sufficient resources for optimal academic achievement irrespective of socio-economic status. The educational system produces an intergenerational cycle of school failures and short change an entire future American society as a result of family socio-economic status. Method: 31 surveys were handed out and 13 were returned. Some of the answers include health/nutrition; level of IQ; motivation or lack of motivation of teacher; amount of parental support; class size; quality of instruction/teaching resources; support available in home; school; student disabilities; language; education in culture; style of learning exposure to style; gender; peer influence; natural ability; attendance; family loss of tragic event; pregnancy full term; expectations and teacher/student relationship were also considered. Method: Every teacher felt that the environment contributed most when considering academic achievement. Method: Additional variables for socio-economic status were included: attitude; self-confidence; need to please; desire to do better; love of learning; acceptance; economics in the home; stability of family; siblings; age of parent(s); age of student maturity; family involvement; importance placed on learning; cognitive level; family history; neighbourhood; modelling of good work; ethics; pride; choices made; resources available; parental achievement; attending pre-k; home literacy; received early intervention; good nutrition; health; high IQ; oral language development; self-care skills; family life; class dynamics; personality and mood on any given day tells a specific teacher what they can or cannot do on a given day. Results: The higher the socio-economic status, the higher the academic achievement. The current literature is not available as specific students in low socio-economic status homes have high academic achievement. Income, education and occupation are responsible for low academic achievement in many low SES families. Socio-economic status causes less time with children and a result of lower education level of a parent, students from families of higher economic status tend to have parents who read to and with them, parents more apt to talk to them about the world and offer them more cultural experiences, many of the students\u0026#39; struggle with reading comes from low SES and parents that struggle with reading. If a family does not have a good educational background or materials to use to work with their child, the child may suffer as a result of their environment. If education is not valued in the home, students will not value education, more expectation for higher education in higher classes. Abstract In this literature review, family environments of low socioeconomic status (SES) students were examined and a comparison made in learning styles between low and high achievers Socioeconomic factors such as family income, education, and occupation play major role in the academic achievement of all students. There is a positive correlation between SES and academic achievement. The conclusions of this review have implications for all educators as well as the entire future of American society. APA Style Reference Quagliata, T. (2008). Is there a positive correlation between socioeconomic status and academic achievement?. Paper: Education masters (p. 78). https://fisherpub.sjfc.edu/cgi/viewcontent.cgi?article=1077\u0026amp;context=education_ETD_masters\nYou may also be interested in Education and Socio-economic status (APA, 2017b) Ethnic and Racial minorities and socio-economic status (APA, 2017) Women and Socio-economic status (APA, 2010) Disability and Socio-economic status (APA, 2010) Lesbian, Gay, Bisexual and Transgender Persons \u0026amp; Socioeconomic Status (APA, 2010) #bropenscience is broken science (Whitaker \u0026amp; Guest, 2020) ⌺ ","date":1557010800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1601940269,"objectID":"1a466a95d0582b639ea119f3c26bd3ec","permalink":"https://forrt.org/summaries/diversity-equity-inclusion/","publishdate":"2019-05-05T00:00:00+01:00","relpermalink":"/summaries/diversity-equity-inclusion/","section":"summaries","summary":"The symbol ◈ stands for non-peer-reviewed work.\nThe symbol ⌺ stands for summaries on the topic of Diversity, Equity, and Inclusion.\nEducation and Socio-economic status (APA, 2017b) ◈⌺ Main Takeaways: Children from low socio-economic status take longer to develop academic skills than children from higher socio-economic status groups (e.g. poor cognitive development), leading to poorer academic achievement.","tags":null,"title":"Diversity, Equity, \u0026 Inclusion","type":"docs"},{"authors":["Dr. Thomas Rhys Evans"],"categories":[],"content":"We are thrilled to present our new Pedagogies, which features Dr. Thomas Rhys Evans. Thomas is Associate Professor of Occupational Psychology (from September 2024 he’ll be Professor in Organizational Psychology and Open Scholarship) at the University of Greenwich and a very active member of FORRT! His research focuses, among other things, on using meta-psychology and Open Science practices to understand the quality of evidence in Psychology. In this FORRT’s Pedagogies, Thomas shares insights on the 12-week module on evidence-based practice he developed and taught for the first time in 2023. We discussed the philosophy behind the course, how it relates to Open Science, his students’ reactions, and so much more. You can watch the interview in the video or read a summary of their main points below. Make sure to also check the 12-week module, which is fully available (including teaching materials, students’ reports etc) on the Open Science Framework (link below).\nHow did you come up with the idea and identified the need for this course? Thomas: We all are involved in research and teaching because we want to do good and have a positive influence, but it can be really hard to know how you can help or what you can do. Especially when knowledge is so compartmentalized into different fields. The core thing for me was to try and break down what is shared across the ways in which different fields do research and the ways in which we teach people to think more about the broader skills that underlie, for example, open scholarship. It\u0026rsquo;s this need for more critical thinking, to look at the evidence and say, “is that right? Are there other alternative interpretations?” Critical thinking skills underpin so much of what we do, even in our general day-to-day life, not just our research life. For example, I\u0026rsquo;ve become increasingly vocal about things like climate change and the need to start creating better bodies of evidence. And to do that, we need to teach that better. We need to teach people, support people, and give them the confidence to be able to question things and not just accept a paper written by somebody from a prestigious university. And so this course was based upon the idea that we can do better in providing students and the general public with those critical skills that can be used in day to day life as well as in research.\nCan you tell us a bit more about the course set up and organization? Thomas: I set up a new module for our occupational psychology MSc at the University of Greenwich, which lasts twelve weeks. It consists of a one-hour pre-recorded lecture and a two-hour face-to-face practical seminar each week. The module broadly fits four themes from start to finish, which are to identify, evaluate, synthesize and apply evidence. In terms of my teaching philosophy, I think it is really important that students can label the skills that they develop. Basically, the modules cover these four dimensions from start to finish: How do we identify and generate evidence? How do we push it through the process to make sure that people, policymakers, and practitioners use that evidence? There was an interesting paper demonstrating that it can takes about ten years for research to actually evidence influence. That’s a long time. So it\u0026rsquo;s really important that we get the evidence right in the first case, developing high quality, robust, and rigorous evidence. And so that\u0026rsquo;s where the kind of Open Science content tends to trickle in.\nHow would you describe the teaching philosophy behind the course? Thomas: I try to always think about inclusivity in the broadest sense and I also think about it in terms of evidence. When people think about evidence-based practice in action, they often think about academic sources of evidence. But practitioners have loads of experience too. It is often not written down, but that does not make it any less legitimate. And so we are changing the way of thinking, becoming more inclusive in how we integrate all these different forms of evidence. When we think about inclusion, we also do what is known as the class charter, which is a co-agreed statement between everybody involved in the course. I get a lot of students from a lot of different backgrounds and with a lot of different experiences. The idea is not to get everybody into the same conforming mindset. In fact, it is the opposite: to get people to be more open about different ideas and different notions, and to be inquisitive. Sometimes, students are encouraged to develop a mindset where you listen to the teacher and are asked to memorize what they say. That\u0026rsquo;s definitely not the goal here. I think it is very important that we build up confidence for people to be independent critical thinkers. Another core component of my philosophy is practical skills, because it is really important that we know what to do with the information we have, to adapt to circumstances, and let that inform judgments and decision-making. So I see practical skills as the most relevant components of my teaching, as it is a way of bringing everything together.\nResearch-wise, you are also quite involved in Open Science. Do you incorporate insights from Open Science research into your teaching? What Open Science principles did you integrate in this course? Thomas: When we look at evidence, you can\u0026rsquo;t separate it from the way in which it was created. So, what I do within this course is trying to weave in Open Science dimensions throughout. We look at everything, starting with the standard things that you might expect ‒ like pre-registration, replications, open data sharing, and so on. But we also look at structural factors. That includes, for example, how research and researchers are evaluated. As soon as you look at the incentive structure of research, it becomes clear that it is not always going to lead to the best quality evidence. We also think about the values and the way of thinking associated with open scholarship. Often, it’s about prioritizing transparency. It\u0026rsquo;s about changing the focus from novelty to encouraging openness and reflection. For example, being open about both the limitations and the strengths of evidence. It\u0026rsquo;s not necessarily teaching Open Science, but it\u0026rsquo;s about getting that mindset: what are the factors I should be thinking about and what\u0026rsquo;s the style of thinking that I could adopt to get the answers I need?\nWhich were the most important skills that you wanted your students to develop in this course? Could you give us an example of how you taught students these skills? Thomas: Let me give you one of my favorite examples of this. During the first face to face session, I gave my students evidence on how feedback (star ratings) on scented candles on Amazon changed over time, to encourage the application of critical thinking. For me, that was a really fun way of getting people to think about different things. For example, why would the reviews of scented candles suddenly plummet? And then, obviously, you build into the facts ‒ okay, here are the years that it happened in, here are the time frames, and here is a little bit of the data. This didn\u0026rsquo;t happen with unscented candles but it did happen with other fragrances. So what might that tell us? It turns out we were looking at the evidence behind COVID: reviews for scented candles were worse when people had COVID because they couldn’t smell the candles! I encouraged students to notice little details when evaluating evidence. In this course I wanted my students to develop core academic skills, such as critical reading and critical writing. These are fundamental skills to any research degree, but courses teaching these skills are usually so dry that students can’t see how they relate to practice. I always try to achieve this in a practice-oriented way.\nIn your course, you teach students how to gather and evaluate evidence-based knowledge. Do you integrate different research epistemologies? If so, how? Thomas: Within this course, I do encourage people to think in different ways. I don’t ask my students to look at things from one perspective, but rather encourage them to use multiple points of view. A good example of what I\u0026rsquo;ve done is a book about the evidence behind specific HR practices (Evans, 2022). And the whole purpose of this book was to become the core reading of the module, because it encourages us to think in different ways and to adopt different epistemological viewpoints. This book deconstructs the evidence behind the so-called “best training.” I’ll give you an example. One of the students on the first version of this course was a trained massage therapist. They did their assignment looking at the evidence for the role of in-seat massages for workplace stress. The role of their professional experience in integrating academic evidence became a huge part of what they spoke about in their assignments. How does their experience match with what certain types of academic sources say? And again, there were lots of interesting differences between what quantitative research and qualitative research were saying. And then they had this interesting third point, which was their professional experience of this. This course is designed to facilitate those sorts of links in critical thinking.\nFor their assessment, students had to produce an evidence-based synthesis, evaluation and practical dissemination. Can you tell us a bit more about it? Thomas: I wanted students to apply what they learned by looking into a workplace practice they were interested in and evaluate the evidence behind it. The goal of the assessment is for students to find sources of knowledge, such as academic sources and experts, identify the most important evidence we have about their chosen topic, and to bring all of that information together to form a message. The assessment has three main components, starting with a\ncurated literature review. The purpose is to demonstrate that you can prioritize the most important sources of knowledge, bringing the evidence together to say something. I encourage students to consider not just academic sources, but also practice, expert opinions, and other evidence-based resources.\nSecond, students evaluate the quality of evidence they have, using one of the quality evaluation or risk of bias tools they were introduced to during the course. For this task, students evaluate a piece of evidence that they find to be particularly important and then create a narrative to communicate their evaluation. For example, in the literature review, they might have highlighted a body of evidence or source of evidence that was extremely popular or well cited. But then in the quality evaluation they find that actually there\u0026rsquo;s a quite substantial risk of bias in a certain direction, and we reflect on how these concerns may impact our conclusions.\nThe third component is practical dissemination: a way for students to share with the public what they’ve found in the first two assignment sections. I encourage students to get creative with how they share this information, whether it be a poster, a blog, or a series of videos. Several students have allowed me to share their assignments as examples, which can be found here.\nHow did students respond to the course? Did you notice any changes in their attitudes towards science? Could you share your thoughts on how well the students grappled with the material and assignment in this course? Thomas: I feel like I threw a range of topics at students, and they were always ready to engage with the material and participate in activities. As the course went on, the biggest change I noticed was the questions students asked. They became more willing to question different practices, and their questions were often in line with Open Science practices. For instance, they began to question why someone did not share their data or why a study wasn’t pre-registered. One take away from this course is that the quality of a justification is important for evaluating a choice, particularly when there is not a single right answer. For their assignment, there was no one right way to accomplish the goal because there was a lot of flexibility. This encouraged students to use critical thinking skills, which ultimately made for a good project. I like to share students’ assignments as examples because you can really see the range of ways students accomplished the task. In terms of criticisms, students felt that there was a lot of reading, which is true. However, I try to highlight the skills students get out of situations like these, such as prioritizing tasks, and to ask them to do something with the readings beyond summarizing them. Overall, I think emphasizing the skills students will gain, and making sure the activities focus on skills that will support them in their future employment, can help students get more out of the class.\nSometimes we think that incorporating Open Science into teaching means teaching a course on Open Science, but it’s not always possible to do that. What is your advice for teachers who have pre-defined courses/content but want to introduce more Open Science principles into their teaching? Thomas: My advice is to not necessarily tackle everything at once, but to start small, and try embedding that critical way of thinking, encouraging students to start asking the right questions. For instance, sometimes this might mean breaking down the source of evidence to ask: “what can we take from this evidence if we acknowledge the role of that specific practice/bias/decision?”, “why did they use this sample or sampling strategy?”, “what would be the potential implications if they did this differently?” You are not teaching them what pre-registration is or why it\u0026rsquo;s useful to share data, but helping them gain that same way of thinking, that importance of critically reflecting on evidence. Open Science can also be incorporated in the way we grade students, again prioritizing the skills they developed in synthesizing evidence and thinking critically about it. In my course, the literature review has a criterion for criticality: have they demonstrated critical thoughts about the evidence gathered? Have they picked a diverse range of sources of evidence? If you’re starting a course from scratch, or even picking up a pre-existing module, it\u0026rsquo;s easy to embed little ideas and notions that get students thinking in that critical way. And this can be a really powerful mechanism to teach Open Science principles, especially because some people are openly averse to Open Science ways of thinking and specific practices. So, this just gives you that little first step of a positive kind of development in that direction.\nDo you have a favorite part of the course? Do you think your students have a favorite too? Thomas: I definitely have a favorite piece. It\u0026rsquo;s week one, lecture one, and my first encouragement to students to have a little bit of that flavor of critical thinking. I start off with a broad question: “Wouldn\u0026rsquo;t it be good if we could improve our mental health based upon what we ate for breakfast?” And I talk about a news article that said having Marmite on toast improves your mental health. It\u0026rsquo;s such a beautifully simplistic thing that could have such a positive impact. So everybody wants it to be right. My fun with this first session was to clinically take the fun out of it and deconstruct the evidence. We start by pondering what sort of evidence we would need to trust that claim: epidemiological, experimental, observational, etc… Turns out the study was an experimental trial with different types of multivitamin tablets, and it had nothing to do with Marmite whatsoever, but Marmite has a high amount of the certain type of vitamin they investigated. Then I really stretched this as far as I could by working out with them how much we would have to eat of a certain product to get the vitamin amount that they evaluated. Turns out that if you actually ate that much Marmite, you would more quickly die of sodium poisoning than you would of anything else because of the salt. So really, it wasn\u0026rsquo;t a very clever claim to make. Then we explore further and find out they also tested different vitamins, the study was underpowered, it didn’t declare the vitamin tablets given to the researchers for free as a conflict of interest, and that the distortion from tablets to marmite came somewhere between the university press release and the journalists. For me, this was a very fun exercise, because it is something we all hope would be real, but when we look at the evidence, it actually tells us a very different picture.\nResources Check Thomas’ 12-week module class: https://osf.io/typ4z/ Evans, T. R. (2022). The evidence behind HR: An open evaluation of contemporary HR practices. Routledge. Data enhancement and analysis of the REF 2021 Impact Case Studies (jisc.ac.uk) YouTube video about evidence-based practice: Evidence Based Practice: A Toasty Introduction - YouTube OSF link to “The ongoing and upcoming challenges for the role of news in science communication”: PsyArXiv Preprints | The Ongoing and Upcoming Challenges for the Role of News in Science Communication (osf.io) Suggested citation: Evans, T.R., Wolska, J., Andreolli, G., Woodward, A.M., Merighi, A., \u0026amp; Micheli, L. (2024). FORRT Pedagogies. From mental-health breakfast interventions to evidence-based practice: Teaching students how to critically engage with science. ( https://doi.org/10.17605/OSF.IO/SB5VE) Team Pedagogies Contributors: Julia Wolska, Manchester Metropolitan University, UK Giorgia Andreolli, Verona University, Italy Leticia Micheli, Leiden University, the Netherlands Amanda Mae Woodward, University of Minnesota, United States Alessio Merighi, Video Editor, alessiomerighi.com ","date":1725391859,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1728979736,"objectID":"5a0075ccead0581299b09c46817d69f0","permalink":"https://forrt.org/pedagogies/004-thomas-rhys-evans/","publishdate":"2024-09-03T15:30:59-04:00","relpermalink":"/pedagogies/004-thomas-rhys-evans/","section":"pedagogies","summary":"In this Pedagogies, Dr. Thomas Rhys Evan discusses how he developed and taught a 12-week module about evidence-based practice, focusing on incorporating Open Science principles with an eye on critical thinking skills and professional experience.","tags":["Dr. Thomas Rhys Evans","FORRT Pedagogies","Evidence-based practice "," Teaching Open Science"],"title":"From mental-health breakfast interventions to evidence-based practice","type":"pedagogies"},{"authors":["Dermot Lynott, FORRT"],"categories":[],"content":"\nMain Text The replication crisis has raised many questions for researchers, but it also poses unique challenges for those teaching about science and the scientific process. We now know that many scientific studies cannot be accurately reproduced or their findings replicated, and this has raised concerns about the validity and reliability of scientific research, leading to calls for increased transparency and reproducibility in scientific inquiry.\nFor those involved in teaching science, it is therefore not possible to simply continue as we have been, for example, accepting and presenting published scientific works as the “truth” or final word on any given phenomenon. Rather, we need to take a step back and teach students about the entirety of the research process, both in its idealised form, and addressing the complex reality of a scientific process that is impacted by cognitive biases, human behaviour, and incentive structures.\nIntroducing Students to Open Science These lectures and tutorials introduce students to the scientific process and open research practices, in order to ensure that psychological research is as rigorous, transparent and, ultimately, as reproducible as possible. We also consider problematic practices that have frequently occurred in psychological research, and discuss the impact they have had on psychological knowledge, as well as learning how to avoid such problematic practices in the future. The module content was designed to be delivered at a first-year undergraduate level, and can also be delivered in a pretty modular fashion, allowing the tutor to pick and choose which sessions best suit their programme or student needs.\nSome might wonder if the first year of a degree programme is too early to be learning about open science. Shouldn’t we be teaching them the traditional approach first and then, once they have this grounding, get into open science practices later in their degree? I would strongly disagree with such an approach. There are several reasons why it’s important to teach open science to undergraduate students, and at the earliest possible opportunity. First, open science promotes transparency and reproducibility in research, which are essential principles of scientific inquiry. By teaching students about open science, we can help ensure that they are conducting research in a responsible and ethical manner. Why would we first teach them an approach to research that doesn’t integrate these ideals? It makes no sense to me.\nIn exposing students to open research and to questions of research integrity, they get to see not only the unvarnished nature of scientific inquiry, warts and all, but also to see the true potential of applying open research practices. They can see how open science can make research more collaborative and inclusive, for example by highlighting how to share data openly, so they can more easily share their findings with others, and also being able to build upon the work of their peers. Teaching open science can also help prepare students for the rapidly changing landscape of scientific research, where open and collaborative approaches are becoming increasingly important. Open research skills are frequently highlighted in academic job requirements, as well as cited as best practice by education institutions, charitable foundations, and research funders.\nThe Course Content The content presented here was jointly developed with colleagues at Lancaster University (Dr. Marina Bazhydai, Dr. Sally Linkenauger, Dr. Neil McLatchie), as part of two undergraduate psychology modules, formally listed as Research Integrity and Open Science I (PSYC123) and Research Integrity and Open Science II (PSYC124).\nBecause the courses assume no knowledge of the research process, the course first focusses on the scientific method, thinking about experimental design, and questions of confounds, reliability, and validity. Once we have that under our belts we move on to contrasting “traditional” approaches to research vs an “open science” approach, and cover topics of study preregistration, replications, and sample size considerations.\nWe then turn to some of the thornier questions science and scientists have been faced with: How do we determine what is true in science? How do we know which theories are well supported by evidence and which ones are not? How can we tell if researchers are trying to pull the wool over our eyes? To start to think about these questions we look at learning how to spot and avoid questionable research practices, in favour of practices that are open, transparent and reproducible. In this way, we think about some of the problems faced by researchers, such as cognitive biases, researcher flexibility in reporting outcomes, and even fraud. We ask how we can assess research findings in the face of such problems?\nPSYC123 PSYC124 Introduction and Scientific Method Introduction to False positive psychology Measurement (Distribution/Reliability/Validity) Questionable Research Practices Experimental Designs Biases and incentive structures Confounds and Extraneous Introduction to meta-analysis Traditional vs Open Science Scientific Fraud Study Preregistration Errors and error detection Replication Principles of Open Data Sharing Sample Size Spotlight on Developmental Science These modules will show students how scientific practices in the field have changed rapidly in recent years, with renewed interest in openness and transparency. While we focus on a range of problems that impact our interpretation of research findings, students will also develop an understanding of tools that can help overcome and prevent these issues from arising (e.g., study preregistration, sharing of data, meta-analyses etc.).\nFor tutors and instructors, lecture slides were designed to be covered in about 50 minutes, and the labs can also be completed in around 50 minutes, although some will take a little longer. It’s also worth noting that these modules don\u0026rsquo;t involve any statistical programming, focussing more on open research concepts, but can (and are) taught alongside modules more focussed on learning statistics through R. There is also some flexibility in the order that topics can be delivered. For example, Meta-Analysis is a standalone topic that could be moved later in the course, or handled separately as part of a more advanced research methods module.\nThe course content has an open licence (CC BY), so we encourage instructors to use and modify the content to meet their needs. For instructors, answer sheets are available for the majority of lab exercises, so please just email if you\u0026rsquo;d like a copy. If you do use any of the materials, we would love to hear about it! Indeed, we would be grateful for any feedback you have to offer.\nCourse content is available on the Open Science Framework, so to download everything go here for PSYC123, and here for PSYC124.\nContact information: Dr. Dermot Lynott, Department of Psychology, Maynooth University (Email: dermot.lynott@mu.ie; Homepage.)\n","date":1717995051,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1718361753,"objectID":"b31fb9443c86915f127f32004178713b","permalink":"https://forrt.org/educators-corner/017-teaching-os-to-undergraduates/","publishdate":"2024-06-10T00:50:51-04:00","relpermalink":"/educators-corner/017-teaching-os-to-undergraduates/","section":"educators-corner","summary":"We present a set of 16 lectures and paired tutorials that introduce students to the concepts of open science. Starting from the basics of experimental psychology, we cover concepts of open research from study preregistration and open data to the darker side of research, such as questionable research practices and fraud. Students will learn how to apply open research practices in their own work, as well as learning to identify when research sometimes falls short of the standards we expect.","tags":[],"title":"Course on Open Science 101","type":"educators-corner"},{"authors":["FORRT"],"categories":[],"content":"\nBackground of Reviewer Zero Any writer who puts their work into the world knows it can be a humbling and harsh experience. Negative experiences with peer review, as depicted in the picture below, are all too common. Comments like those illustrated – and others that are unduly harsh, belittling, racist, sexist, or otherwise inappropriate – have been received by scholars at all career stages during peer review. These experiences provide a starting point to question normative practices in scientific peer review, and identify the hidden and blatant consequences for early career scholars, particularly those from underserved groups. Most importantly, we can ask what each of us might do to shift this culture.\nReviewer Zero is a collective founded in 2020 to take concrete, data-driven, anti-racist action to improve the culture of scientific peer review. We were formed with guidance from SPARK Society — an organization that improves visibility and provides mentorship opportunities for cognitive scientists of color. Our broad aim is to change cultural norms in the peer review process, in order to improve the recruitment, retention, and advancement of minoritized scholars in psychology. Collectively, we are a group who has experienced hundreds of rejections and written hundreds of reviews. We’ve received decision letters accepting and rejecting our work, and we have written decision letters accepting or rejecting others’ work.\nLike readers of this blog, Reviewer Zero members know that science can be better than it is, and that shifting the status quo is not easy. The culture of science matters for who engages with it, and normative experiences within a culture can produce different consequences if individuals see themselves as safe or threatened within that culture. The experience of peer review as hostile or negative sets the stage for disparate impacts for minoritized individuals. For example, a survey of peer review experience (Silbiger \u0026amp; Stubler, 2019) found that experiences of unprofessional peer review were quite common - reported by 50% of scientists in the survey. However, individuals from minoritized groups reported a stronger negative impact of these unprofessional reviews, compounding the impact of other biases against such individuals (see Aly et al., 2023, for further discussion preprint available).\nTo understand peer review from the perspective of early career researchers who are underserved by the status quo, Reviewer Zero conducted a survey in 2020 of early career researchers in psychology / neuroscience. Our results are not surprising: We found that cis-men of color reported less helpful feedback compared to White cis-men, and women/nonbinary respondents were more likely than cis-men to report that their peer review experience reduced their belonging in science. Qualitative results from a free-response question about the most memorable experiences with peer review were eye-opening, and heartbreaking. Respondents reported receiving harsh feedback, racist comments, and feeling so demotivated that they left those research topics behind. But there is also opportunity for change: This survey also showed that authors can feel motivated when the process is perceived as fair or when reviews note strengths of the work.\nHarsh reviewer comments can have disparate impacts, demotivating early career researchers and driving them away from the field.\nPhoto by KOBU Agency on Unsplash\nHow Reviewer Zero’s Mission Intersects with the Open Science Community As efforts to improve scientific practice and policy toward more openness and transparency take shape, it is essential to consider who is designing these systems and what perspectives they reflect. To build systems that are truly open and accessible for a global psychological science, we must take proactive steps to include different perspectives, identities, and voices in creating and changing systems. Here, we walk through examples of two kinds of openness that can have implications for minoritized or underserved groups. The essential questions are who is being asked to be “open,” and at what cost?\nOne innovation in peer review is open peer review, which includes disclosure of author and reviewer identities and reviews. Open peer review can include different levels of disclosure: For example, greater transparency without identification can occur if reviews are public but reviewers are not identified. Although identified peer review can have some advantages in that reviewers are clearly accountable to what they write in a review, this system may not be equitably open to all. Individuals who occupy less powerful positions, whether due to their career stage or underrepresented group identities, may experience more backlash or more negative consequences in offering critique – or their perspectives may simply be dismissed in favor of reviews from people who hold more power or come from majority groups. Transparent peer review, where reviews are public but reviewer identities are not, might offer greater security to reviewers. From an author perspective, though, individuals who are contending with identity threat may be unwilling (for good reason) to have critiques of their work openly aired (for further discussion, Aly et al., 2023). The very ability to engage in different aspects of open peer review can vary across individuals with different levels of power in a system.\nWould greater openness in peer review make hostile and unprofessional reviews less likely, or would greater openness in peer review exacerbate negative consequences for members of historically excluded groups? We don’t know the answer to these questions, but we need to ask them.\nGiven that hostile or unprofessional reviews can be common, individuals from historically excluded groups might steer away from participating in open review processes. Navigating peer review is challenging enough without doing so in public. One example comes from Dr. Colleen Murphy, who tweeted in response to eLife’s move to drop accept/reject decisions:\nThe inequities present in peer review need to be addressed before there is any possibility of open review serving a wide range of scholars. Without a deliberate effort to change the existing culture in a way that advances diversity, equity, and inclusion, other efforts might perpetuate or exacerbate these inequities.\nA different form of openness, transparency, and accountability in peer review comes from institutions or organizations. It is not yet routine for journals, societies, or editors to collect or share aggregate-level data about the demographic or identity characteristics of editors, editorial board members, reviewers, or authors. Yet knowing this information could provide valuable insights about who is served by the current practices and how that might change with reforms. What are the demographic or identity characteristics of the editorial board and reviewers? Who is submitting papers, who is being invited to resubmit papers, who appeals editorial decisions, and what are the outcomes? Data about these aspects of scientific publishing are rarely collected, rarely examined by journal staff, and are even more rarely shared with the community. Without transparency and accountability in terms of representation, editor and reviewer choices tend to rely on individuals’ own social networks and perpetuate gender and race homophily. This reliance on social networks may be exacerbated by the growing difficulties with finding scholars to agree to provide peer reviews (Petrescu \u0026amp; Krishen, 2022). Editors may rely on close colleagues who are willing to do them a favor by providing a review, which can in turn further narrow the pool of reviewers and the types of feedback authors get. To counteract this tendency, journals can publicize calls for individuals to register as new reviewers, so that editors have accessible and current listings of scholars with expertise and availability. Generally speaking, clearer and more transparent data about who engages with peer review and how they do so can inform innovations to ease burdens on editors and provide more diverse slates of reviewers.\nHow Reviewer Zero and FORRT work together Like FORRT, Reviewer Zero seeks to illuminate the black box of scientific and academic culture. Peer review is central to scientific careers, but peer review is often left to the “hidden curriculum” of processes that people learn as they go without explicit discussion or training. To bring some light to these unknown processes, Reviewer Zero hosts workshops (both online and at in-person conferences) for both early-career scholars and for editors and reviewers. In the sessions aimed at trainees and early career scholars, we introduce the basics of peer review, to consider the emotional and motivational aspects of navigating peer review, and to provide concrete tools and strategies for responding to editorial decisions. In workshops with editorial teams, we lead discussions about the current culture of peer review and its implications for perpetuating disparities in who contributes to and advances in science, and we discuss concrete steps toward a more constructive and inclusive peer review culture.\nReviewer Zero is developing on-demand resources based on these workshops, and these can complement FORRT resources offered in the adopting principled education section of the website. We particularly advocate for collaboration and conversation moving forward, for identification and promotion of strategies that can advance diversity, equity, and inclusion both in peer review systems and the goals of open science. At times, as noted above, these goals might appear to be in tension with each other - but that should be the start rather than the end of the conversation.\nCall to action Each of us plays a part in maintaining or changing the culture of scientific peer review. We invite you to join us to stay updated on activities at www.reviewerzero.net and to explore our developing on-demand resources.\nThere are steps you can take today to move toward a more constructive and equitable peer review system. As with any culture change, shifting a culture requires multiple actors to begin to think and act differently, and cultural change is more possible when actors at different levels of the system take part (Hamedani \u0026amp; Markus, 2019). If you are someone with power in the system, with experience, and/or with a majority group identity, support minoritized and early career scholars to learn more about the unspoken aspects of peer review (Aly et al., 2024). If you have experience in scientific publication, speak about your rejections as well as your successes. When you deliver critique, remember there is a human on the other end of that communication, even if that person is not identified. If there is something promising or exciting about the work, note it explicitly. Recognize what power you have, and amplify the voices and perspectives of people whose voices are not yet central to the conversation.\nIf you are someone who has been underserved by the current system, know that there are communities and allies who deeply value your contributions to science. Find friends and mentors who can interpret and contextualize reviews with you. If you encounter hostile or unprofessional reviews, know that you have options (including alerting the editor of the journal), and that unfortunately you are not alone.\nWe at Reviewer Zero see equitable practices and reducing group disparities as essential to cultivating excellent science - science that is fair, meaningful, robust, and rigorous. Any move to improve scientific practice can ultimately succeed only if it centers building systems and spaces that represent a diverse range of identities, uphold equitable practices and outcomes, and prioritize inclusivity. Peer review is central to the practice and progress of science, and bettering it to serve a wider range of scientists will improve the rigor, quality, and contribution of science.\nReferences Aly, M., Colunga, E., Crockett, M. J., Goldrick, M., Gomez, P., Kung, F. Y. H., McKee, P. C., Pérez, M., Stilwell, S. M., \u0026amp; Diekman, A. B. (2023). Changing the culture of peer review for a more inclusive and equitable psychological science. Journal of Experimental Psychology: General, 152(12), 3546–3565. https://doi.org/10.1037/xge0001461\nHamedani, M. Y. G., \u0026amp; Markus, H. R. (2019). Understanding Culture Clashes and Catalyzing Change: A Culture Cycle Approach. Frontiers in Psychology, 10. https://www.frontiersin.org/article/10.3389/fpsyg.2019.00700\nPetrescu, M., \u0026amp; Krishen, A. S. (2022). The evolving crisis of the peer-review process. Journal of Marketing Analytics, 10(3), 185-186.\nSilbiger, N. J., \u0026amp; Stubler, A. D. (2019). Unprofessional peer reviews disproportionately harm underrepresented groups in STEM. PeerJ, 7, e8247. https://doi.org/10.7717/peerj.8247\nAly, M., Ansari, S., Colunga, E., Crockett, M. J., Diekman, A. B., Goldrick, M., Gomez, P., Kung, F. Y. H., McKee, P. C., Pérez, M., \u0026amp; Stilwell, S. M. (2024). Mentorship practices that improve the culture of peer review. Nature Reviews Psychology, 3(1), Article 1. https://doi.org/10.1038/s44159-023-00261-1\nAuthor Biography Reviewer Zero Organizing Committee. Contact information: admin@reviewerzero.net.\nFunding This material is based upon work supported by the National Science Foundation under Grant No. 2224777. Any opinions, findings, and conclusions or recommendations expressed in this material are those of the author(s) and do not necessarily reflect the views of the National Science Foundation.\n","date":1715316531,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1726002979,"objectID":"48c4cb758ddc801774172990fa1f098d","permalink":"https://forrt.org/educators-corner/016-reviewer-zero/","publishdate":"2024-05-10T00:48:51-04:00","relpermalink":"/educators-corner/016-reviewer-zero/","section":"educators-corner","summary":"Our work engages with editors, reviewers, and authors to create cultural shifts in scientific peer review to foster diversity, equity, and inclusion in psychology and neuroscience. As peer review practices are revised for greater openness and transparency, it is essential to ask who is being asked to change their behavior, and at what potential cost. The very ability to engage in different aspects of open peer review can vary across individuals with different levels of power in a system.","tags":[],"title":"Shifting the culture of peer review with Reviewer Zero","type":"educators-corner"},{"authors":[""],"categories":[],"content":"\nWe are back with a very special Pedagogies, this time including also the perspectives of students on learning about Open Science (OS)! We have three amazing guests: Prof. Dr. Jordan Wagge and her two outstanding undergraduate students, Jasmine Beltran and Amy Hernandez. Over the past years, Jordan has led the way in developing CREP so that it can i) accomplish much needed replications, and ii) provide tools/structure for instructors who desire to train their students in OS. In this FORRT’s Pedagogies, Jordan and her students share their experiences with CREP and their thoughts about the trials and tribulations of involving students in the process of OS. We hope that this can inspire and help many scholars wishing to incorporate more OS into their teaching, mentoring and research.\nJordan Wagge is a Professor of Psychology and Cognitive Science at the University of Avila (UA) and the Executive Director of CREP. Her research focuses on three areas including replication work, pedagogy, and critical work studies. She has a strong record supporting the OS movement at all levels - from her teaching philosophy, to the development of open teaching materials and resources, and the involvement of students in OS training and practice. But be sure to learn more about Jordan from her website (all links posted below). We are also joined by Jasmine Beltran and Amy Hernandez, Jordan’s students, who have experienced the implementation of CREP in their classrooms first hand. They discuss their perspective as a student, their challenges, and the benefits they gained through this process.\nFORRT’s Team Pedagogies took the opportunity to ask Jordan, Jasmine, and Amy some questions on their journey with CREP (shoutout to the FORRT and academic Twitter community for sending so many great questions). You can watch or listen to the interview in the video or read a summary of their main points below!\nCheck out the full interview here\nCould you tell us a little bit about yourself and how you got involved with Open Science and, more specifically, with CREP? Jordan: I got involved with OS pretty naturally. In graduate school, I taught Statistics and Research Methods. When I taught, I would highlight best practices and how science works. There were all these topics that I felt hypocritical teaching because I would also say “Psychologists don’t do this in practice”. It led me to ask, why don’t we do these things in practice? I naturally fit into the OS and transparency movement because it made sense. I got involved with CREP after it was founded. At a conference, I was seeking co-presenters on how to teach undergraduate students research methods most effectively. I wanted to know how others maximized the limited amount of time given all the learning outcomes associated with a methods course. John Grahe responded. The angle he took is to make the course time to capitalize on a meaningful contribution to the field especially if students are already investing so much energy. At the same time, Mark Brandt and Hans Ijzerman from Tilburg University were interested in replication efforts and began CREP. I reached out and joined from there.\nJasmine: We’ve had Dr. Wagge as an instructor for a year now. Our Statistics course is a 2-semester course. During the first semester, we focused on doing a literature review and collecting material. This semester we are currently collecting data. We were required to engage with CREP but it’s been a fun, educational experience.\nAmy: I am in the class as well as in Dr. Wagge’s lab, so I’ve double dipped with the CREP experience.\nCould you give a brief description of what CREP does and an overview of the process from selecting studies to publication? Jasmine: I can talk about the CREP steps from a student perspective. It’s a Psychology project designed to tackle the replication crisis as well as provide a way for undergraduate students to gain that quality experience in the research process which I think is super important. I am completing a senior research thesis [final year undergraduate dissertation project for undergraduates who want to complete a PhD]. It’s important to gain this experience from CREP before just diving into research without knowing really anything. I would have had no idea how the research process looks like. With CREP, the main steps we took were the following:\nStudies are selected by the CREP Team. Across 9 disciplines, they select the most highly cited papers published in the previous 3 years. The studies undergo scrutiny by executive reviewers and are judged on how likely a student would have the ability to complete the replication in just 1 semester. The reviewers then do a final screening and reach out to the original authors of the selected studies to ensure the highest level of consistency with the original study. Now, students enter the process. Students are first introduced to CREP by their instructor. They then create an Open Science Framework (OSF) page and assemble components. Once the OSF page is drafted, it is submitted to a CREP executive reviewer to assess quality and provide feedback. Students pursue Institutional Review Board (IRB) approval, revise and resubmit the OSF page if needed, pre-register the study and post that on OSF as well. Data is collected, and once complete, students write up the results, discussion, submit materials, and sign a completion pledge. Students are e-mailed completion certificates. Once enough data is collected by several labs, students are invited by the CREP Team to contribute to a meta-analysis manuscript, ideally in a 1st author position, which, as we know, is a pretty big deal for an undergraduate student. Jordan: There’s a publication ( Wagge et al., 2019) which has a flow chart of how the process works from both the instructor’s end as well as the students (see image below). We also have a step by step manual (linked below) that walks students through their exact responsibilities. Of the 19 studies originally selected, we collected data on approximately 15 of them. Three of those studies led to a publication and one of them has a student first author. I am also working on writing another paper right now and it has 30 authors that started in 2013. There are several factors that impact the publication timeline. Some studies have few people choosing to collect data and other studies use software students may not have a background in, like Matlab. These lead to areas CREP is improving on. CREP publications need content experts to ensure high quality work. We started CREP with a sample size rule of thumb (2.5x the original); however, we want to move to conducting power analyses prior to data collection. We warn instructors that studies will eventually end after 1-2 years of being posted. That means that the publication timeline for a study will be at least 1.5 years until we receive a manuscript. Once a content expert has been obtained, they help mentor students writing it up. Independent sites who participate in CREP are able to publish their own collected data. We recommend they publish any extension hypotheses data; however, if they want to publish the data regarding the original hypothesis, they can. We simply note in the CREP manuscript that some of the data has been published already. We are constantly thinking of ways to make this process as efficient as possible.\nFlowchart of the CREP Process (Wagge et al., 2019)\nThe focus on students is what sets CREP apart from other large-scale replication projects. Are students also involved in the publication process? How is authorship discussed and decided? Jordan: We currently give independent site publications autonomy on student authorship. There have been no sites that have published their data, but we have thought of providing guidance through the OSF page’s bibliography. Currently OSF lists the graduate students first alphabetically, and I am last. Students can raise their position by contributing to more CREdiT categories (e.g., collecting data, contributing to the manuscript). For example, if several students write up the results section, the best writer’s draft is used and they are considered to give additional contributions meriting higher authorship. Ideally, we contact every person who has contributed to data. One challenge we face is if instructors still work at their original institution and if students’ emails still work if they graduated. When students are involved, they are also hesitant about their ability to provide meaningful feedback. In a recent project, we developed a survey and we sent the manuscript in that survey, along with guiding questions for students to provide feedback. We had several students contribute that way and that earns them authorship. We may not use all the feedback students provide, but it is considered and makes our manuscript stronger. We will see how this process goes and get students’ feedback on whether they felt they were contributing to authorship in that way.\nThere are many aspects of Open Science that can be taught to students. Could you tell us why CREP chose to focus on replications and what are the benefits of this approach? Jordan: The reason we focus on replications with students is because - without wanting to sound too negative - student projects done in research methods courses are often not heavily based on theories or quality methodology. There is definite value in having this traditional method. Students learn how to test questions and think about how to break down a question through an empirical lens. I joke with my students that topics often chosen are usually pre-workout, sleep, stress, attachment styles, and the Big 5. These topics are interesting, but the questions are often not grounded in theory nor are they the cutting edge questions of what the field is doing. That’s to be expected from someone’s first research project. My own 1st project I conducted was extremely sloppy. I knew that I didn’t have any idea what I was doing. In other scientific fields, it is common for students to conduct replications of classic findings many, many times. Imagine an introductory chemistry student is asked to create a compound in the lab: they are given a manual, told the exact steps, and get the desired outcome. Chemistry instructors never ask their students to form their own hypothesis from the get-go, never ask to imagine what would happen if you mix Chemical X with Chemical Z. Instead, students are first trained by doing. In psychology, a student who wants to study depression is now exposed to several depression inventories and asked to uncover the exact methodology of a study. What CREP offers are curated studies with materials and the opportunity of providing a meaningful contribution to our field by replicating highly cited recent work. Instructors and students don’t have to worry about a lot of the decision making regarding the study. More time can be used in-class to discuss why we pursue the IRB, why this actual research study chose this method, and facing the challenges of being a replicator (e.g., ambiguous language for certain sections). CREP comes from an apprenticeship model and provides scaffolding.\nJasmine: When our class went over 5 or 6 studies we could do, it gave us freedom even if we didn’t get to do our own idea. We still had the choice of what to replicate and those days were my favorite. I appreciate the experience rather than being thrown in the deep-end and told ‘good luck!’.\nWhat would you say are the major learning goals for students in these replication projects? For the students, what did you personally take away from this experience? Amy: I was definitely scared of APA [American Psychological Association] style the first time, but I have improved my writing in APA since then. I am also less intimidated talking with faculty members and generally communicating with them.\nJasmine: I agree with the point about APA style as well as improving my ability to read scientific papers and their different sections. All students are contributing to every section of the research process, including the literature review. I believe the constant exposure to papers makes me feel more confident in understanding scientific papers, knowing I can write similarly\u0026hellip; I think students need to build that confidence because nobody wants to sound stupid. By practicing in class, reading gets easier. Nobody is perfect at it, but practice helps get us there.\nJordan: I originally chose one study to replicate, but now I choose a pool of studies I am willing to lead and have students vote on which one they would like to do. All students in my class write their own papers and get exposure to all parts of the process. I see CREP as a model to implement your own learning outcomes, those of your department, and APA’s listed 2.0 outcomes. We are currently working on a resource putting together writing assignments that would align with different learning outcomes. Instructors can then pick and choose assignments that suit the learning outcomes of their courses or departments.\nWhat are students’ initial reactions when they start with the replication projects? What are the major challenges they may face? For students, what was your experience when you started with CREP? Jasmine: When we first started, I was intimidated and thought my semester would be difficult. As we continued to move into the course, we spent a good amount of time on each step of the CREP process. This helped us maximize our learning. Currently, I am about to start my senior thesis and I am not nervous about that now. The writing ability is definitely the biggest thing I took away from this process.\nAmy: I was intimidated, too. And overwhelmed, thinking about all the things I would not know how to do. The challenge others and myself faced is that we were scared to make mistakes. Many of us wanted to be perfect at it the first time so that fear of ‘screwing up’ held us back from going on.\nImagine I would now start a replication project with a group of students at my university. Could you tell us what you think would be the advantage of doing these student-replications together with CREP? What are the benefits for instructors and for students? Jordan: Having the structure, assignments, and pre-selected studies are important strengths of CREP. We do have lots of materials, for example tutorials on creating an OSF page. CREP also does have external reviewers who aim to get reviews back within 1-2 weeks. It is nice to have feedback from someone else on the project. We provide a huge advantage for replications in general. CREP can be thought of as a professional network. You meet collaborators and build relationships, and I’ve written letters of recommendations for Tenure and Promotion committees for those involved with CREP. And finally, you do have something that you can add for authorship. This is especially important for educators from small teaching schools, who might have difficulties maintaining their own lines of independent research besides the teaching duties. With that being said, if you are interested in doing replications with students, do it your way. Look at CREP materials, download the step by step guide and adapt it to your needs.\nWhat are the different ways in which instructors can incorporate the work with CREP into their teaching/mentoring (e.g., lab course, seminar, other)? Which types of incorporation work best in your opinion? Which challenges do instructors usually experience? 8a. For you students, how was the collaboration with CREP set up? What was your role in the replication and what did you like the most? Do all students have the exact same roles, or do some students have unique roles? Jordan: An instructor could implement replications with CREP in an Introductory to Psychology course up to a graduate level course. If the class is for earlier undergraduates, I recommend the instructor handling some of the ‘behind-the-scenes’ paperwork to make the process easier instead of having students be in charge of everything. Instructors could allow students to do their Master thesis on CREP replications; however, I personally ask my students to have extension hypotheses. I agree with work done by Daniel Quintana that student theses should be replications ( Quintana, 2021). Instructors can do replications with CREP on a variety of topics and disciplines, although some fields like biopsychology, developmental, or certain populations (e.g., children) might be more challenging. Instructors often face timeline issues from their institution such as slow IRBs. I would recommend getting IRB approval before the semester starts. It is also important to get familiar with the projects before proposing them to students. For example, knowing which type of resources and materials the replications require and whether you, as the instructor, would have access to those in your institution. I also think it can be easier for first time instructors to implement CREP either in smaller classes or for 1-1 mentoring.\nJasmine: I mentioned that our Statistics and Methods class was shown several pre-selected studies to choose from. My favorite part was to go through each study in detail and discuss how many participants we would need to collect, what potential problems could arise, and so on. We were given ~100 voting points and each of the 20 students could allocate their points to the different studies. I also did an additional contribution to the project which involved recording myself doing the study procedures for the OSF. Amy also contributed with coding for an ongoing study.\nYou recently got the National Science Foundation (NSF) grant to fund, evaluate, and enhance the activities of CREP. Congratulations! Can you share a little with us how you envision the future of CREP? Jordan: It’s really exciting that we got NSF funding to do research on CREP and learning outcomes. Right now, we have a couple of classes at different universities that are doing pre- and post-testing, and we\u0026rsquo;re gonna keep doing that for the next couple of rounds. We are also implementing a ‘peer mentor’ system that pays students who have undergone a project through CREP a 1-year stipend. These students would then have office hours and mentor other students around the world currently undergoing a replication with CREP. This is because students can feel more comfortable asking other students when learning about the research process. Chosen peer mentors will also be taken to a conference. For any students who have participated in CREP prior, we are also doing a Summer CREP conference to give an opportunity for students who participated in CREP research to present their results. Some of the money is going to building infrastructure for a submission system. We also got a second grant that will be used to create instructor workshops on how to implement CREP in their curriculum. This will be really helpful in sort of getting people to incorporate CREP, but also build out materials that other people can use. One of the sessions will be dedicated to mentoring students in replications with extension hypotheses (specifically ones grounded in Diversity, Equity and Inclusion hypotheses). Lastly, we are investing in more teaching resources and material. We hope to make CREP more accessible so that students get authentic, genuine research experience. We want students to have the opportunity to be real scholars. So we are thinking of more ways to engage students with professional development.\nWhat is the best way to keep updated with the results of your conducted replications? Is there for example a sort of metadata containing all replications and the results? Jordan: That’s definitely on my wishlist items for the future. I’ve currently been posting results on Twitter ( @CREP_psych) to communicate these manuscripts. We are hoping to also improve the website in the future. This is also a good call for volunteers. If anybody wants to help set any of this stuff up, there are lots of places where people can help. Besides help with the website and communication, we also always need reviewers, who are paid 10$ for the review.\nSince CREP started, what have you learned so far about teaching Open Science that you could share with us? Any specific tips for instructors/educators who would like to incorporate OS into their teaching and mentoring? Jordan: The most surprising thing regarding replications is just how invested students were. I was really worried it would feel forced. I don’t have data to support this, but in my experience students seem as interested in these topics as in topics they choose on their own.\nRegarding tips, I don’t believe I have to be the ‘perfect’ teacher. I would say, don’t worry about doing everything. Do what you can and don’t be afraid to reach out and ask people what/how they are doing their class. We have a wonderful community. Being involved in things like SIPS [Society for the Improvement of Psychological Science], FORRT and Twitter really helps to see what other people are saying and doing. My biggest tip is to reach out whenever you have a question. If you are facing an issue about how to run your classroom, there is someone out there who has faced something similar and can help.\nCREP’s resources CREP’s OSF page: The landing page for instructors who would like to start their journey with CREP. It includes CREP’s step-by-step directions and example syllabi that incorporate CREP CREP’s Twitter page Jordan Wagge’s Website: more information about Jordan and her Replication \u0026amp; Open Science Lab (ROSE) Read more about Jordan’s thoughts on CREP from her interview given to Avila University: Helping students become better scientists through authentic research See Dr. Fieke Wagemans from the University of Duisburg Essen lead a conference session introducing CREP: recorded on September 2018 at the University of Bordeaux Youtube Playlist of CREP tutorials: information on how to sign up for CREP, how to create a class OSF page, how to prepare and submit to CREP, and how to complete a CREP project Google Slides presentation on available studies for replication at CREP Suggested citation Wagge, J., Beltran, J., \u0026amp; Hernandez, A. (2023). Open Science education through student participation: Integrating replication training into the classroom. FORRT Pedagogies. https://doi.org/10.17605/OSF.IO/4JRFA\nTeam Pedagogies Contributors:\nJacob Miranda, California State University - East Bay, U.S.\nJulia Wolska, Manchester Metropolitan University, UK.\nGiorgia Andreolli, Verona University, Italy.\nLeticia Micheli, Leiden University, the Netherlands.\nAlessio Merighi, Video Editor, https://vimeo.com/user62908209\n","date":1697796649,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1698062178,"objectID":"97aa7c6e66938b95dfcfb27b698a9ddf","permalink":"https://forrt.org/pedagogies/003-crep/","publishdate":"2023-10-20T10:10:49Z","relpermalink":"/pedagogies/003-crep/","section":"pedagogies","summary":"Jordan Wagge and her students share their experiences participating in the Collaborative Replications and Education Project (CREP). They go over the unique advantages of approaching Open Science (OS) education through student engagement. Resources and infrastructure CREP provides are discussed.","tags":[],"title":"Open Science education through student participation","type":"pedagogies"},{"authors":["By Magdalena Grose-Hodge"],"categories":null,"content":" LEAVE FEEDBACK AND LOOK AT THE LESSON PLAN HERE\nCOMMUNITY PSYCHOLOGY: RESPECT FOR DIVERSITY\nDiversity as one of the core values of Open Science\nObjectives:\nunderstand cultural humility as an approach to diversity define and discuss dimensions of diversity understand privilege and intersectionality and reflect on own privilege using the wheel of privilege Click to see reading list and licenses\nLesson incorporates content from chapter 8 “Introduction to Community Psychology: Becoming an Agent of Change” available online under the CCBY4.0 license.\nBefore you start: What do you understand by diversity in science? What controversies are presently discussed that relate to diversity and the field of psychology?\nFocus on diversity in psychology can be traced back to the 1930s and Vygotsky, the pioneer of Sociocultural Approach\nActivity 1 Respect for Diversity - Introduction\nRead the introduction to diversity below and see if you can answer the questions below it.\nRespect for diversity has been established as a core value for Community Psychology, as indicated in Chapter 1 (Jason et al., 2019). Appreciating diversity in communities includes understanding dimensions of diversity and how to work within diverse community contexts, but also includes a consideration of how to work within systems of inequality. Community psychologists must be mindful of diverse perspectives and experiences when conducting research and designing interventions, as well as working to combat oppression and promote justice and equality. By working within a framework of cultural humility, this chapter attempts to provide a basic understanding of the dimensions of diversity that are most common in Community Psychology research and practice. Further, we explore how these dimensions contribute to complex identities and considerations for community practice. click to see the source\na) What do you know about Cultural Competence and Cultural Humility?\nb) What dimensions of diversity can you think of?\nc) There are some more terms and concepts related to the topic of diversity within communities. Can you explain and discuss these with another student or write down your ideas in points?\nprivilege white privilege intersectionality Activity 2 Respect for Diversity: Jigsaw reading or poster\nNow choose the task you prefer to get a better understanding of the concepts presented above. Bothe tasks will ask you to refer to the source introduced in the previous chapter. Read the instructions, decide on the task and click the button underneath the instructions to see the text.\nJIGSAW READING (group activity)\na) In groups, decide who is going to focus on the points below while skimming through the text in order to summarise it to the rest of the group. Once you decide how to divide the sections, spend 10-15 minutes reading through the relevant parts and preparing to summarise the concepts to your students.\nCREATING A MIND MAP OR A POSTER\nb) Skim through the chapter and create a mind map or a poster with dimensions of diversity and quick explanations. Then add information about cultural humility, privilege and intersectionality. Share the results of your work with another student.\nCultural Humility\nCulture, Race and Gender\nAge, social class and sexual orientation\nAbility / Disability and Religion \u0026amp; Spirituality\nPrivilege and intersectionality\nActivity 4 The Academic Wheel of Privilege\nReflect on how you see or identify yourself in relation to these dimensions. Which groups do you identify with? Have you heard the phrase \u0026ldquo;check your privilege\u0026rdquo;? Have you thought of intersectionality and privilege in relation to yourself? FORRT has developed a tool which allows everyone in academia to reflect on their privilege. Look at the video below, in which Bethan Iley, one of the members of FORRT explains the concept.\nWould you like to try the wheel?\nFor #DisabilityPrideMonth here’s our new preprint on #Neurodiversity and Open Scholarship! We look at #ND, intersectionality, #SocialJustice \u0026amp; #UniversalDesign. Sharing lived experiences \u0026amp; solutions for more equitable inclusion of ND people in academia!\n👉 https://t.co/FcQoppTbvD pic.twitter.com/vhF0vxy4xI\n— FORRT @FORRT@mastodon.social (@FORRTproject) July 14, 2022\nHow to use the wheel: Look at the dimensions of privilege on the outside of the circle and choose the option from the inner circle that describes your situation best. For every answer in the smallest circle (the closest to the middle of the wheel), give yourself 1 point, then 2 points for the option in a slightly bigger circle and finally 3 points of the option closest to the outside of the circle. Share your result with a friend or keep it to yourself. Do you think this tool is useful? Is it important to check our privilege? How can this make you more mindful of other’s experiences?\nReading:\n(to be be done before or after the lesson)\nJason, Leonard A.; Glantsman, Olya; O\u0026rsquo;Brien, Jack F.; and Ramian, Kaitlyn N., \u0026ldquo;Introduction to Community Psychology: Becoming an Agent of Change\u0026rdquo; (2019). College of Science and Health Full Text Publications. 1. https://via.library.depaul.edu/cshtextbooks/1\nFlavio Azevedo, Sara Middleton, Jenny Mai Phan, Steven Kapp, Amélie Gourdon-Kanhukamwe, Bethan Iley, Mahmoud Elsherif, \u0026amp; John J. Shaw. Navigating Academia as Neurodivergent Researchers: Promoting Neurodiversity Within Open Scholarship. APS Observer October 2022\nElsherif, M. M., Middleton, S. L., Phan, J. M., Azevedo, F., Iley, B. J., Grose-Hodge, M., … Dokovova, M. (2022, June 20). Bridging Neurodiversity and Open Scholarship: How Shared Values Can Guide Best Practices for Research Integrity, Social Justice, and Principled Education.\n","date":1696291200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1721643888,"objectID":"c320823529b95753b0c2fb5ee61c41b4","permalink":"https://forrt.org/neurodiversity-lessonbank/community_psychology-diversity/","publishdate":"2023-10-03T00:00:00Z","relpermalink":"/neurodiversity-lessonbank/community_psychology-diversity/","section":"neurodiversity-lessonbank","summary":"LEAVE FEEDBACK AND LOOK AT THE LESSON PLAN HERE\nCOMMUNITY PSYCHOLOGY: RESPECT FOR DIVERSITY\nDiversity as one of the core values of Open Science\nObjectives:\nunderstand cultural humility as an approach to diversity define and discuss dimensions of diversity understand privilege and intersectionality and reflect on own privilege using the wheel of privilege Click to see reading list and licenses","tags":["Lesson Bank"],"title":"COMMUNITY PSYCHOLOGY RESPECT FOR DIVERSITY","type":"neurodiversity-lessonbank"},{"authors":["By Magdalena Grose-Hodge"],"categories":null,"content":"DIVERSITY IN RESEARCH AND OPEN SCIENCE\nUnderstand that research needs to acknowledge the implications of power dynamics Reflect on the importance of participatory research and cultural humility in relation to diversity in science Understand how Open Scholarship promotes diversity in research Activity 1\nYou have discussed diversity in your previous class. Can you remember what the terms CULTURAL COMPETENCE and CULTURAL HUMILITY mean? Read the text below and explain the idea of PARTICIPATORY ACTION RESEARCH. How is it connected to cultural competence and humility?\nAdopting cultural humility is necessary for considering diversity in research. In research, it is important to consider how questions are asked or which samples are included in a study. In addition, the importance of topics of research to diverse communities must be considered, which may require developing research topics and questions with the populations that are being impacted. Participatory action research is a valuable tool for developing topics in an inclusive way and is a method frequently used by community psychologists to find solutions in the social environment (Kidd \u0026amp; Kral, 2005).\nResearch must also consider the power dynamics between the researcher and the community as well as the dynamics within the community. The use of culturally-anchored methodologies is important for exploring research questions in the appropriate context.\nMarginalized groups are often compared to a majority group, but these comparisons may not always acknowledge the implications of power dynamics present in such comparisons.\nWhen developing the methodology, it is important for the researcher to acknowledge one’s own cultural assumptions, experiences, and positions of power. Recognition of these aspects of self will lead to a more careful framing of the research question within context. Finally, it is important to consider where to disseminate research findings to reach wide audiences.\n(Source: Jason, Leonard A.; Glantsman, Olya; O\u0026rsquo;Brien, Jack F.; and Ramian, Kaitlyn N., \u0026ldquo;Introduction to Community Psychology: Becoming an Agent of Change\u0026rdquo; (2019). College of Science and Health Full Text Publications. 1. https://via.library.depaul.edu/cshtextbooks/1 )\nActivity 2\nHow much do you know about Open Science and its values? Watch a presentation on Open Science by Sam Parsons (FORRT). How is Open science connected to diversity?\nClick here to watch the presentation\n![an umbrella with core values of OS underneath. These involve: equity, diversity and inclusion. For more explanations, watch the presentation above. ](SAM OS.png)\nActivity 3\nTake this quiz to check what you have learnt on diversity and research.\nExtra reading:\nLearn more about how open science promotes diverse, just and sustainable research.\nhttps://journals.sagepub.com/doi/full/10.1177/1475725719869164\nhttps://press.rebus.community/introductiontocommunitypsychology/chapter/respect-for-diversity/\nFooter © Magdalena Grose-Hodge for FORRT (Framework of Open and Reproducible Research Training) About ND/Community Psychology - Diversity.html at main · MSGrose-Hodge/ND\n","date":1696291200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1712087370,"objectID":"b9b26270554efe8ce7257eff539275b5","permalink":"https://forrt.org/neurodiversity-lessonbank/diversity_and_research/","publishdate":"2023-10-03T00:00:00Z","relpermalink":"/neurodiversity-lessonbank/diversity_and_research/","section":"neurodiversity-lessonbank","summary":"DIVERSITY IN RESEARCH AND OPEN SCIENCE\nUnderstand that research needs to acknowledge the implications of power dynamics Reflect on the importance of participatory research and cultural humility in relation to diversity in science Understand how Open Scholarship promotes diversity in research Activity 1\nYou have discussed diversity in your previous class.","tags":["Lesson Bank"],"title":"DIVERSITY IN RESEARCH AND OPEN SCIENCE","type":"neurodiversity-lessonbank"},{"authors":["By Magdalena Grose-Hodge"],"categories":null,"content":" LEAVE FEEDBACK HERE\nDeficit approaches to left-handedness and neurodivergence in scientific papers\nFocus on critical reading skills\nObjectives:\n· to recognise implicit bias in academic texts\n· to reflect on normative science and its impact\n· to discuss how OS values and participatory research could promote discussions on epistemological bias and influence existing narratives.\nClick to see reading list\nBefore you start: Are you or do you know anyone who is left-handed? What attitudes to left-handedness were there in the past? Are you familiar with the neurodiversity paradigm?\nWhat is a neurotype? Did you know that the word \u0026ldquo;sinister\u0026rdquo; comes from Latin \u0026ldquo;left\u0026rdquo;? This is likely caused by the dominance of right-handedness in human population and bias against the left-handed minority in the past.\nActivity 1 Critical reading - identifying implicit bias\nLook below at fragments of this article published in The North American Review in 1903, which discusses left-handedness. What can you say about the beliefs the author held about the following groups of people:\nmen /women / white people / people of races other than white / children / left-handed individuals / individuals suffering from mental health problems / individuals with criminal record?\nopen the activity in a new window\nTIP: click on the question marks on both sides of the text to see hints or click on the button above to see the activity in full screen.\nClick on the button below to see statements about the text. Are they true or false? Click on each statement to reveal the answer.\ndecide if the statements are true\nParticipatory research\nDo you think anyone left-handed, non-white or female has been involved in the production of Lambroso\u0026rsquo;s paper or the design of the researcher\u0026rsquo;s studies?\nDo you think this paper would be well-received today?\nIf you have a moment, research the history of a condition that was referred to by doctors and scientists as \u0026ldquo;female hysteria\u0026rdquo;. Raulin, a French physician who studied \u0026ldquo;the condition\u0026rdquo; claimed that while it is not impossible for a man to contract hysteria, it is mainly women who suffer from it because of their \u0026ldquo;lazy and irritable nature\u0026rdquo;.\nWhat are the dangers of conducting research into communities, cultures, or groups of people that we do not identify with?\nCan you think of claims that have been made about different groups of people based on conclusions from studies that did not involve members of the researched communities in the design of the studies? Can you think of reasons why science might pathologe minorities? Think about group psychology - ingroup/outgroup effect (Billig \u0026amp; Tajfel, 1973), and social identity theory (Tajfel 1979) or categorising the world into \u0026ldquo;them\u0026rdquo; and \u0026ldquo;us\u0026rdquo;.\nParticipatory research (PR) encompasses research designs, methods, and frameworks that use systematic inquiry in direct collaboration with those affected by an issue being studied for the purpose of action or change (Vaughn and Jacquez, 2020).\nHow can focusing on participatory research contribute to social justice within the Open Scholarship framework?\nActivity 2 Deficit approaches and pathologising minorities\nIn the 70s, Bakan and colleagues have published a number of highly influential articles that pathologised left handedness. Their claims, which were later revoked (e.g., Bishop, 1990), included that left handedness results from birth trauma and a minor brain damage. Below are some of their papers that discussed the ideas. Discuss how implicit bias may influence methods that are applied by scientists and how this could skew results.\nBakan P (1971) Handedness and Birth Order. Nature 229(195). https://doi.org/10.1038/229195a0 Bakan P (1975) Are left-handers brain damaged? New Scientist 67(1): 200–202. Bakan P, Dibb G, Reed P (1973) Handedness and birth stress. Neuropsychologia 11(3): 363–366. What effect, do you think, this had on the public perceptions of left handedness in the 70s and 80s? And what impact did it have on the population? And the progress of research in the area?\nCan you think of other areas where implicit bias perpetuated in language (has) had a detrimental impact on public perceptions? What groups of people might have been affected?\nActivity 3 Neurodiversity paradigm: definition and introduction\nClick on the link below, which will open the following publication: Azevedo, F., Middleton, S., Mai Phan, J., Kapp, S. K., Gourdon-Kanhukamwe, A., Iley, B., Elsherif, M. M., \u0026amp; Shaw, J. J. (2022). Navigating academia as neurodivergent researchers: promoting neurodiversity within open scholarship. Observer. Find a definition of neurodiversity and neurodivergence, then look at the tweet below and the following questions.\nCan you explain what neurodiversity is in your own words?\nWhat is the difference between neurodiversity and neurodivergence?\nWhat does it mean that an individual is neurodivergent?\nLook at the wheel from the group’s Twitter thread and either discuss it in pairs or take time to familiarise yourself with it on your own. Are you included in the wheel?\nAre non-neurotypical brains presented as deficient in the article and the tweet? In other words, does the framing pathologise neurodivergence?\nSkim through the article and find a definition of neurodiversity\n#Neurodiversity includes neurotypical \u0026amp; neurodivergent people. A person is neurodivergent when their neurology diverges from the neurological majority. Examples of neurodivergence: dyslexia, dyspraxia, OCD/C, personality disorders/conditions, autism, ADHD/C, Tourette’s and tics. pic.twitter.com/swgenhJ4i1\n— FORRT @FORRT@mastodon.social (@FORRTproject) July 14, 2022\nActivity 4 Neurodiversity paradigm: spotting biased language\nNow choose one of the two articles below and skim the abstract and intro. Discuss or reflect on the questions below.\nArticle 1:* Zwaigenbaum, L., \u0026amp; Penner, M. (2018). Autism spectrum disorder: advances in diagnosis and evaluation. BMJ: British Medical Journal, 361. https://www.jstor.org/stable/26959693\nArticle 2:* Faras, H., Al Ateeqi, N., \u0026amp; Tidmarsh, L. (2010). Autism spectrum disorders. Annals of Saudi medicine, 30(4), 295–300. https://doi.org/10.4103/0256-4947.65261\n_How is neurodivergence viewed by the authors? How do the authors see individuals with neurotypes other than neurotypical?\nAre neurotypical and neurodivergent populations perceived equally by the authors or is one of the groups given as a model and the other compared to it by looking at their deficits?\nCan you see any examples of pathologising language? Underline fragments that suggest that neurodivergent brains are deficient rather than different.\nActivity 5 Ableist Language in scientific publications\nLook at some examples of language taken from these and other articles published in the last 20 years in influential journals and discuss the language use. Can you identify the implicit bias? What does it suggest about the perception of different neurotypes held by the authors?\nInstructions: drag and drop the fields to match the language found in papers (orange pin) and the corresponding comment (blue pin). Once you find a pair, put the fields side by side. Once you complete the task, check by clicking the icon in the bottom right corner. For full screen view, click on the square in the top right corner.\nWhat is the assumed norm that autistic individuals are often measured against? Is this explicitly stated? Norms are developed based on observations gathered through research. Where is most research conducted? Which populations is it mainly conducted on? Which groups may be underrepresented or purposefully excluded from studies? How does this affect norms? Normative science is pervasive in many fields. Pay attention to assumed standards and implicit bias in articles you read. How can we become more aware of our own biases? Is it possible for early researchers avoid unquestioned acceptance of paradigms, terminology and methods? Post your reflection below.\nPosts are moderated so they may not appear immediately._\n_Reading:\n(to be be done before or after the lesson)\nBottema-Beutel, K., Kapp, S. K., Lester, J. N., Sasson, N. J., \u0026amp; Hand, B. N. (2021). Avoiding Ableist Language: Suggestions for Autism Researchers. Autism in adulthood : challenges and management, 3(1), 18–29. https://doi.org/10.1089/aut.2020.0014\nFlavio Azevedo, Sara Middleton, Jenny Mai Phan, Steven Kapp, Amélie Gourdon-Kanhukamwe, Bethan Iley, Mahmoud Elsherif, \u0026amp; John J. Shaw. Navigating Academia as Neurodivergent Researchers: Promoting Neurodiversity Within Open Scholarship. APS Observer October 2022\nElsherif, M. M., Middleton, S. L., Phan, J. M., Azevedo, F., Iley, B. J., Grose-Hodge, M., … Dokovova, M. (2022, June 20). Bridging Neurodiversity and Open Scholarship: How Shared Values Can Guide Best Practices for Research Integrity, Social Justice, and Principled Education._\n","date":1696291200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1712087370,"objectID":"382ec4d7f07257afec3c902928bd403a","permalink":"https://forrt.org/neurodiversity-lessonbank/implicit_bias/","publishdate":"2023-10-03T00:00:00Z","relpermalink":"/neurodiversity-lessonbank/implicit_bias/","section":"neurodiversity-lessonbank","summary":"LEAVE FEEDBACK HERE\nDeficit approaches to left-handedness and neurodivergence in scientific papers\nFocus on critical reading skills\nObjectives:\n· to recognise implicit bias in academic texts\n· to reflect on normative science and its impact\n· to discuss how OS values and participatory research could promote discussions on epistemological bias and influence existing narratives.","tags":["Lesson Bank"],"title":"IMPLICIT BIAS AND NORMATIVE SCIENCE","type":"neurodiversity-lessonbank"},{"authors":["By Magdalena Grose-Hodge"],"categories":null,"content":"OPPRESSION AND POWER\nFocus on critical reading skills and discussion\nObjectives:\nto reflect on critical gender and neurodiversity theories\nto read academic texts critically and synthesise information\nto discuss issues related to gender and neuroptype discrimination\nIntroduction and background reading We strongly recommend reading chapter 9 on Oppression from the source below before engaging with this lesson.\nJason, Leonard A.; Glantsman, Olya; O\u0026rsquo;Brien, Jack F.; and Ramian, Kaitlyn N., \u0026ldquo;Introduction to Community Psychology: Becoming an Agent of Change\u0026rdquo; (2019). College of Science and Health Full Text Publications. 1. https://via.library.depaul.edu/cshtextbooks/1Lesson Activity 1 Read and discuss the following citations:\n1. Psychology of oppression reveals that oppression is not always perceptible and repellant. It can be disguised and beguiling. People do not always know they are oppressed. Sometimes they have to be educated about their oppression. The reason is that oppression stunts people’s critical, rational, analytical, and probing capabilities. Normative oppression also becomes taken for granted and mundane and therefore imperceptible (Ratner, 2014).\n2. People cannot reject the system of domination without rejecting [part of] themselves, their own repressive instinctual needs and values (Marcuse, 1969, p. 17).\n3. Advocating the mere tolerance of difference between women is the grossest reformism. It is a total denial of the creative function of difference in our lives. Difference must be not merely tolerated, but seen as a fund of necessary polarities between which our creativity can spark like a dialectic. Only then does the necessity for interdependency Lorde 2 become unthreatening. Only within that interdependency of difference strengths, acknowledged and equal, can the power to seek new ways of being in the world generate, as well as the courage and sustenance to act where there are no charters (Lorde, 1984).\nActivity 2 Now read the short essay written by Audre Lorde in 1984. What does she mean when she says that the master\u0026rsquo;s tools will never dismantle the master\u0026rsquo;s house?\nLorde, Audre. “The Master’s Tools Will Never Dismantle the Master’s House.” 1984. Sister Outsider: Essays and Speeches. Ed. Berkeley, CA: Crossing Press. 110- 114. 2007. Print.\nYou can listen to the essay here:\nHow do you understand these words? What do you think about them?\nActivity 3 Now read Nick Walker\u0026rsquo;s essay titled \u0026ldquo;Throw away the master\u0026rsquo;s tools: Liberating ourselves from the pathology paradigm.\u0026rdquo; What is he advocating for? What alternative to the pathology paradigm does he propose? Can you explain the difference and summarise his arguments?\nhttps://neuroqueer.com/throw-away-the-masters-tools/\nWhat do these texts have in common?\nWhat do Critical gender Theory, Critical Race Theory and Critical Neurodiversity Theory have in common?\nRatner, C. (2014). Psychology of Oppression. In: Teo, T. (eds) Encyclopedia of Critical Psychology. Springer, New York, NY. https://doi.org/10.1007/978-1-4614-5583-7_571\n","date":1696291200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1712087370,"objectID":"a40ca565bae3779dcf731a42ac92a497","permalink":"https://forrt.org/neurodiversity-lessonbank/masterstools/","publishdate":"2023-10-03T00:00:00Z","relpermalink":"/neurodiversity-lessonbank/masterstools/","section":"neurodiversity-lessonbank","summary":"OPPRESSION AND POWER\nFocus on critical reading skills and discussion\nObjectives:\nto reflect on critical gender and neurodiversity theories\nto read academic texts critically and synthesise information\nto discuss issues related to gender and neuroptype discrimination\nIntroduction and background reading We strongly recommend reading chapter 9 on Oppression from the source below before engaging with this lesson.","tags":["Lesson Bank"],"title":"OPPRESSION AND POWER","type":"neurodiversity-lessonbank"},{"authors":["By Magdalena Grose-Hodge"],"categories":null,"content":"The Generalizability Crisis Subject: Research Methods\nDuration: 60 minutes\nLearning Objectives:\nUnderstand the concept of generalizability in research Recognize the factors that contribute to the generalizability crisis Discuss the implications of the generalizability crisis in various fields Introduction What is the generalizability crisis?\nWhat factors contribute to it?\nActivity 1\nActivity 2 - Case studies\nDiscuss problems relating to design that you can identify in the following case studies.\n1. A study examining the efficacy of a new heart medication conducted in a sample of primarily young, healthy individuals, with little representation from older adults and those with pre-existing heart conditions.\n____________________\n2. A study investigating the prevalence of depression in college students, conducted at a single university with a predominantly white student population, and with little representation from students of other races and ethnicities.\n____________________\n3. A study examining the effectiveness of a new cancer treatment conducted in a single hospital in a major metropolitan area, with little representation from patients living in rural or suburban areas.\n____________________\n4. A study examining the effects of a new teaching method on student achievement conducted in a single school with a homogeneous student population, and with little consideration given to cultural or linguistic differences.\n____________________\n5. A study investigating the effectiveness of a new therapy for anxiety conducted using only self-reported measures of anxiety, with no consideration given to other potential outcomes or measures.\n____________________\n6. A study examining the prevalence of a particular health condition conducted in a single survey administered to participants over the course of a single day, with no follow-up or longitudinal data collected.\n____________________\n7. A study examining the efficacy of a new mental health intervention that showed significant improvements in a single clinical trial, but was not replicated in subsequent trials or in real-world clinical settings.\nActivity 3 - Implications\nReflecting on the case studies and research that you are familiar with - what could be some implications of these design flaws?\nNow look at some specific examples. Can you add anything else?\nHere are some specific examples of problems caused by the generalizability crisis. Match them to the titles:\nSome groups might be disadvantaged regarding depression treatment Lack of representation of diverse populations in clinical trials Some groups overlooked in genetic research Inaccurate treatment recommendations for women Black skin conditions not as easy to treat Some studies on dermatological conditions have found that certain treatments may not be as effective for black patients due to differences in skin biology. For example, a study published in the Journal of the American Academy of Dermatology found that black patients with acne may have more resistant and inflammatory acne than white patients, which may require different treatment approaches. Many clinical trials have historically excluded certain populations, such as women, racial and ethnic minorities, and older adults. This can limit our understanding of how certain treatments or interventions work in these populations. For example, a study published in the Journal of the National Cancer Institute found that cancer clinical trials often underrepresent racial and ethnic minorities, which can limit our understanding of the effectiveness of cancer treatments in these populations.\nMany studies on cardiovascular disease have historically focused on men, which can lead to inaccurate treatment recommendations for women. For example, a study published in the Journal of the American College of Cardiology found that women are less likely than men to receive evidence-based treatments for heart attacks, which can result in worse outcomes.\nSome genetic research has been criticized for being too focused on populations of European ancestry, which can limit our understanding of genetic diversity and disease risk in other populations. For example, a study published in the American Journal of Human Genetics found that many genetic studies have underrepresented or excluded African populations, which can limit our understanding of genetic diversity and disease risk in these populations.\nSome mental health studies have focused on populations from Western cultures, which can limit our understanding of mental health in other cultures. For example, a study published in the Journal of Cross-Cultural Psychology found that many measures of depression and anxiety may not be appropriate for use in non-Western cultures, which can limit our understanding of mental health in these populations.\nActivity 4 - Strategies to address the problem\nLook back at the case studies and discuss / reflect on how these flaws could be addressed. Brainstorm how the problem can be mitigated in your field.\n","date":1696291200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1712087370,"objectID":"2b5972a086468d278ac069d3cee23395","permalink":"https://forrt.org/neurodiversity-lessonbank/generalizability/","publishdate":"2023-10-03T00:00:00Z","relpermalink":"/neurodiversity-lessonbank/generalizability/","section":"neurodiversity-lessonbank","summary":"The Generalizability Crisis Subject: Research Methods\nDuration: 60 minutes\nLearning Objectives:\nUnderstand the concept of generalizability in research Recognize the factors that contribute to the generalizability crisis Discuss the implications of the generalizability crisis in various fields Introduction What is the generalizability crisis?\nWhat factors contribute to it?\nActivity 1","tags":["Lesson Bank"],"title":"The Generalizability Crisis","type":"neurodiversity-lessonbank"},{"authors":["","FORRT"],"categories":[],"content":"\nGiven the scarcity of full-time, permanent academic jobs, more and more PhDs are looking abroad for employment. When I was on the job market in 2020–21, I applied to positions in 7 countries. I landed at the University of Nottingham in the UK—which, despite being ostensibly close to my home country of the US in terms of culture and primary language, proved to be an entirely different world. This was apparent from the job application stage, yet at the time, there was no publicly available guidance on differences between the academic job markets in the US and UK.\nEarlier this month, I set out to address this. I ran a webinar, “The Academic Job Market: US vs. UK,” aimed at demystifying both markets for applicants. Interest was so great that I added a second installment; in the end, over 200 people registered. You can watch a recording of the webinar below, which will remain freely accessible to all. The slides are also available to download from my website. I hope this can be a helpful resource to anyone looking to make sense of different market timelines, varying employment structures, and why you can’t address anyone below the rank of full professor in the UK as “Professor.” (This still trips me up constantly.)\nStructural Factors Shape Employment What I’d like to expand on here, however, is something only briefly touched on in the webinar: the structural factors shaping employment in higher education in the US and UK. Whereas much of the academic job market comparison in the two countries is apples and oranges, paying attention to the sociopolitical forces changing higher education reveals much common ground.\nLet me start with a timely example. In mid-April, faculty (teaching staff in UK parlance), postdoctoral fellows, graduate students, and counselors at Rutgers University joined together in unprecedented collective action. In the US, different categories of university staff are represented by different unions, and these are at best loosely networked nationally—there is no equivalent of the University and College Union (UCU), which represents all research and teaching staff, professional services staff, and PhD students at every university in the UK. Yet despite organizational hurdles, some 9,000 Rutgers workers went on strike together for living wages and an end to race and gender pay gaps. I can recall no similar university-wide coordinated action in the US in my lifetime.\nThere are echoes in this struggle of the dispute UCU has waged since before the pandemic. Wages for UK university staff have fallen some 25% in real terms since 2009; the race and gender pay gaps remain unacceptable for universities who claim commitment to equality, diversity, and inclusion (EDI, the UK twist on the US’s DEI: diversity, equity and inclusion). Equally troubling are the echoes in both disputes’ resolutions. The deal at Rutgers, agreed to after a week of action, has been criticized for marginalizing graduate workers’ voices, just as the current proposal under consideration by UCU provides no concrete improvements for PhD students on casualized contracts. Solidarity at Rutgers, as in the UK, has been strongest among the most financially secure staff, with some of the least powerful university employees being left behind. Depending on the discipline, US PhDs will graduate into an academic job market where around 70 percent of jobs are temporary or contingent—a similar landscape to that facing UK PhDs.\nFeatures, Not Bugs Where do these analogous structural failings come from? The answer to this starts with deconstructing the idea that these are “failings,” rather than intentional components of a system that views degrees as commodities and people as cogs in machines. A system wherein the task of teaching is secondary to that of shoring up a university’s research reputation will not mind if teachers have to live in their cars or move from city to city every semester in search of work. Likewise, a system where glossy building façades matter more than what happens inside of those buildings will not mind using historically excluded groups, particularly women of color, as PR fodder while exploiting their labor until they burn out.\nOthers have written far more comprehensively about how these features of higher education (not bugs) have become ever more present in the past few decades. My point is simply that they are unavoidable, regardless of country of residence. Some might claim that coming to the UK will bring reduced research pressures compared to US research-intensive universities, which may be true in terms of the number of publications required but overlooks how the UK’s massive (and growing) grant-grubbing apparatus saps staff time and discourages counterhegemonic work. Others might note that US academic salaries are generally higher than in the UK, which may be true even at poorer institutions but masks the massive (and growing) disparities across and within US institutions. Regardless, the “it’s better over there” narrative sidesteps the reality of the academic job market for many: that, if they are able, they will go wherever hires them.\nTrans-National Sharing It\u0026rsquo;s easy to despair. What’s necessary—and what I hope sharing information across the pond helps encourage, however minimally—is turning collective despair into solidarity. Faculty in the US and UK face different day-to-day pressures, different career benchmarks, and different alphabet soups of acronyms. (Ask me sometime why Master’s students are PGTs but doctoral students are PGRs.) But underneath the surface, our struggles rhyme. It behooves those of us on both sides of the Atlantic to learn about how academia works in the US and UK, as well as in other countries. Our sector is transnational, and transparent, accessible information about that sector is an important foundation from which to exchange expertise, build campaigns, and push for change.\nAuthor Biography Anna A. Meier (she/her/hers) is Assistant Professor in the School of Politics and International Relations at the University of Nottingham. She is a committed advocate for graduate students, solidarity-building, and collective care in higher education. Email: anna.meier@nottingham.ac.uk; Twitter: @AnnaMeierPS; website: annameier.net.\n","date":1685458800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1685464384,"objectID":"e5b3ec38b3469b65a9f940f26c652389","permalink":"https://forrt.org/educators-corner/015-academic-jobs-us-vs-uk/","publishdate":"2023-05-30T16:00:00+01:00","relpermalink":"/educators-corner/015-academic-jobs-us-vs-uk/","section":"educators-corner","summary":"I’m a US-trained academic working in the UK. Here’s why every academic in both countries should learn about the differences—and the common ground—across the pond.","tags":[],"title":"Academic Jobs in the US vs. the UK","type":"educators-corner"},{"authors":["FORRT","Magdalena Grose-Hodge","Lorna Hamilton"],"categories":null,"content":"Watch the recording: Look at the presentations slides: ","date":1678406400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1712653111,"objectID":"145a6bef8de464062b08fd5c53036561","permalink":"https://forrt.org/talk/neurodiversify_your_curriculum/","publishdate":"2023-02-23T00:00:00Z","relpermalink":"/talk/neurodiversify_your_curriculum/","section":"talk","summary":"Unconference 2023 open scholarship practices in Education Research","tags":[],"title":"Workshop:Neurodiversify your Curriculum","type":"talk"},{"authors":["","FORRT"],"categories":[],"content":"\nA child of the replication crisis When Open University Press approached me to write a book on the “replication crisis in psychology”, I was filled with excitement as I knew that discussion of replication issues is scarce within teaching and psychology textbooks. If they don’t teach it, then I could! But I also knew it had to be more than the so-called ‘crisis’; a term associated with pessimism. It had to be about how far the discipline of psychology has come, and the opportunities that open research can bring to reform research training and the wider culture.\nI am a ‘child of the replication crisis’. I completed my PhD in 2016 on \u0026lsquo;stereotype threat\u0026rsquo;; an influential phenomenon that suggests negative societal stereotypes deplete performance. But I got stuck time and time again trying to replicate these classic effects. I felt like a failure. I wish I had known what I know now: that the experiences I was facing were more common than what was gleaned from the published literature. Science is not perfect and nor should it be, but this was not what I was taught through my undergraduate education. So I wrote a book – A Student’s Guide to Open Science – for my younger self. In other words, for the talented and passionate students studying Psychology. By knowing our history – the good parts but also the messy parts – our students represent the grassroots that can make this discipline the best it can be!\nWhat is this book about and why is it needed? By now, many researchers have heard about the ‘replication crisis’ that has swept research disciplines, and the fast-paced improvements brought about by open research reform. However, there is still a gap in educational practices – in the teaching of the replication crisis – which FORRT and its members strive to overcome. Furthermore, when trying to learn about the replication crisis and open science, it can feel overwhelming and there are lots of different resources in many different places. With this in mind, I wrote a “A Student’s Guide to Open Science” as a way of pulling together all of these various pieces of information and useful resources that are currently out there.\nThe book walks students (and their educators) through a recent history of the replication crisis in psychology, the causes and drivers of crises (including academic incentive structures, questionable research practices, and cognitive biases), and open science reform, its benefits and associated challenges. Importantly, to ensure that students understand how to put these new practices into practice, it also includes a handy, digestible guide to implementing open science practices such as preprints, preregistration, Registered Reports, and open materials, data, and code. It is my hope that this book becomes an essential guide to navigating the replication crisis.\nPedagogic resources within the book This book is primarily aimed at students studying psychology at both undergraduate and postgraduate level, but throughout I include useful pedagogic resources that educators can use to teach the replication crisis and open science reform. For example, there is an activity where students plan to replicate a study and can understand the various difficulties they may stumble upon in their quest. Here, they find their favourite article from the journal it was published and walk themselves through different questions to assess whether there is enough information to perform an exact replication – what is the aim of the study? Do the researchers specify their main hypotheses? What is the research design? Are the measures they used detailed enough to understand how they were scored? Do they report whether (and how) they screened or cleaned their data? (see Box 1).\nBox 1. The seat of a replicator. An example pedagogic activity from “A Student’s Guide to Open Science”.\nThis same pedagogic activity can then be expanded to teach about open science reform. Specifically, using the notes that students have taken from their “seat of a replicator activity”, they can then have a go at preregistering their replication using a template from a verified repository (e.g., AsPredicted.org/Open Science Framework). These templates can be downloaded from each repository and students can either complete these as a formative (e.g., seminar activity) or summative assessment (e.g., written report; see Box 2).\nBox 2. Pedagogic activity: preregistering a replication study.\nIn addition to these pedagogic resources, the book also contains top tips for implementing open science practices into student’s research training, including for preprints, preregistration, Registered Reports, and open materials, data, and code. Whilst the focus is mainly on quantitative research, it also provides links and resources to qualitative research. In addition, throughout it discusses the many different organisations and initiatives that have shaped my own learning of open science (see Box 3), thus providing a detailed overview of everything we have learnt so far from the advent of the replication crisis through to current open science reform.\nBox 3. Key resources to get started with open science.\nCharlotte\u0026rsquo;s Key Open Science Resources\n“A Student’s Guide to Open Science” (Pennington, 2023, Open University Press) FORRT’s open educational resources including a bank of lesson plans, replications and reversals, and open science glossary. RIOT Science Club ReproducibiliTea Journal Clubs Podcasts, such as Everything Hertz Global Reproducibility Networks, such as the UKRN and their primers on open science practices The Open Research Calendar New meta-research articles via MetaArXiv Become a student of open science! The replication crisis and open science reform is not currently embedded within the teaching of psychology. It is my hope that with all of these wonderful new and ever-evolving resources, today’s students become critical consumers of science and pave the way to creating a truly robust, transparent, replicable, reproducible and representative psychological science.\nAuthor Bio \u0026amp; Contact: Dr Charlotte Pennington is a Lecturer in Psychology at Aston University, Birmingham, a Fellow of the Higher Education Academy, and a member of FORRT. Her book, “A Student’s Guide to Open Science: Using the Replication Crisis to Reform Psychology” is available in all good bookstores and inspection copies are available from McGraw Hill for review. If you have trouble accessing it, please let Charlotte know via the contact details below and she will do her best to help.\nEmail: c.pennington@aston.ac.uk\nTwitter: @drcpennington\nMastodon: @drcpennington@mastodon.social\n","date":1675921480,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1675939942,"objectID":"78fc47425515a8271d18a68d0812358c","permalink":"https://forrt.org/educators-corner/014-students-guide-to-open-science/","publishdate":"2023-02-09T01:44:40-04:00","relpermalink":"/educators-corner/014-students-guide-to-open-science/","section":"educators-corner","summary":"By now, many researchers have heard about the ‘replication crisis’ that has swept research disciplines and the fast-paced improvements brought about by open research reform. However, there is still a gap in educational practices – in the teaching of the replication crisis – which FORRT and its members strive to overcome. This educator’s corner introduces a new book, “*A Student’s Guide to Open Science: Using the Replication Crisis to Reform Psychology*” and the useful pedagogic resources within it, which allows students to develop a fundamental understanding of the origins and drivers of the replication crisis and how to use open research practices throughout their own research training.","tags":[],"title":"A Student’s Guide to Open Science: Using the Replication Crisis to Reform Psychology","type":"educators-corner"},{"authors":[""],"categories":[],"content":"\nFORRT’s Pedagogies are back with no less than Gilad Feldman. Over the past years, Gilad has completely remodelled his research and teaching to incorporate several aspects of Open Science in a way that is certainly exemplary to many of us. In this FORRT’s Pedagogies, Gilad shares his materials and his insights about his own journey in Open Science. We hope that this can inspire and help many scholars wishing to incorporate more Open Science into their teaching and research.\nGilad Feldman is Assistant Professor at the Department of Psychology at the University of Hong Kong (HKU). His research focuses on judgement and decision making and he strongly supports the Open Science movement at all levels - from his teaching philosophy, to the development of open teaching materials and resources, and the involvement of students in Open Science training and practice. One of Gilad’s most known projects is the student-led pre-registered replications and extensions, which you can find here. But be sure to check all his other amazing meta science resources such as his course syllabi and the “Check me, Replicate me” initiative (all links can be found below).\nFORRT’s Team Pedagogies took the opportunity to ask Gilad some questions on his journey through and with Open Science (shoutout to the FORRT and academic Twitter community for sending so many great questions). You can watch or listen to the interview in the video or read a summary of his main points below! We hope you enjoy and learn as much as we did!\nCheck out the full interview here\nYou have an incredible amount of Open Science resources ranging from courses and replications to incentives to other researchers to replicate or find mistakes in your own work. How did you embark on this Open Science journey? Gilad: In 2015, the big Science paper came out, showing replication rates that in my opinion were alarming. I didn’t understand how bad things really were until this paper came out. I remember being a postdoc and thinking that something big was happening and that I couldn’t in good faith continue with the research I was doing without figuring this out. I then started to think about my principles and decided on my main goals: my research has to be trustworthy, reproducible, replicable. Over time, I also started thinking about equity and inclusion issues. I set my own principles, such as to share materials, data and code. Back in 2016, this was scary stuff! I started to supervise students according to Open Science principles, and the CORE team (Collaborative Open-science and meta REsearch) developed from this. You have managed to put Open Science at the core of your teaching and research. Can you briefly mention the materials and activities you have developed in your own work to accelerate Open Science? Gilad: First of all, I have not developed this on my own. I’ve decided that every time I’m going to make a guide or a template, I’m going to make it collaborative. Everyone who contributes to the document puts their name down and if we submit to a journal, they’re co-authors - from students to professors! We have created a bunch of resources, such as research assessments, community resources (e.g., guides on how to do replications and extensions, how to calculate effect sizes, etc.) and collaborative open resources for statistical analysis, such as the Jamovi guide. Every time I see a need, I open a collaborative guide and share it with others (teaching assistants, students, external reviewers, the CORE team) and advertise it on Twitter. Guides have grown exponentially thanks to the community and now anybody can implement them. Did you face any challenges or critical incidents when incorporating Open Science into your teaching or research? What can scholars expect when embarking on this Open Science journey? Gilad: Being completely open can be intimidating, and it takes some time to convince students that this model works. At the beginning the major issue was the lack of ready-made resources, but I feel that many of the initial challenges have been overcome.\nPsychologists study people, but we rarely point the research that we do to ourselves. I study judgment and decision-making and cognitive biases, and it is amazing how many of these biases we experience ourselves in the academic community and throughout the scientific process. There are lots of biases on how Open Science folks versus more status-quo researchers look at each other. I try to shield my team and students as much as possible from negative comments, but also want them to get a realistic view of the academic world, where there are lots of problems with ego, interests, cognitive biases and heuristics. I try to tell my students that in the end we all want the same thing: the science world is all of us working together. There are lots of challenges, we are humans, but we are learning as a community how to discuss these issues in the open. Humility is key. Taking a step back and listening can help overcome these biases and help us learn from each other. Fostering collaboration: from the teaching portfolio to course (re)design You have an amazing teaching portfolio where you detail all your teaching activities and present your teaching philosophy and goals. Could you tell us what you think are the key elements of your teaching philosophy? Gilad: We really need to reorient what we do in education. I got a lot of inspiration from various projects, such as CREP and the Psychology Science Accelerator. I believe that students and early-career researchers are key actors to address the replication crisis in science. We need to move away from the model of the professor standing in front of students with their powerpoint slides. I believe in alternative approaches such as flipping the classroom. I’m also a fan of problem-based learning: you give problems to students and they work together to figure it out. This works brilliantly in the undergraduate courses that I’m teaching in Hong-Kong. We need to teach students how to learn. We need to understand that science is messy. These are the principles I set (see image below):\nI decided to share everything I create: slides, lectures, videos\u0026hellip; complete transparency! Everything students do is shared on OSF, which may be scary initially, but increases the sense of accountability. And students understand the importance of doing this: even if there’s the possibility of error, you want to be transparent about the process, so other people can help you do better or find your mistake. In fact, there are lots of reasons why I started with replications and extensions: they are more practical, measurable, systematic, valuable for students in their learning journeys and overall more instructive. I believe that this helps students develop their scientific thinking, whatever career path they decide to take.\nCollaboration is one of the pillars of your pedagogy, and in your syllabi you mention a “team contract” that is created together with the students. Could you explain what this team contract is? Why and how investing in/learning team communication can be connected with Open Science practices? Gilad: Team contract is something relatively new that really boosted collaborative work in my courses, mostly because it helps to align expectations. Students sometimes do not like to work together, and they’re used to multiple choice exams and a more structured model. But there is something very important about teamwork, also later when starting to work in a paid job. The most common complaint in my teaching evaluations at the beginning was: “we do not like to work in teams, we prefer to work individually, whenever we work in teams there are a lot of free-riders”. The team contract helps the team to solve issues students might face and it can be adjusted over the semester: basically, it’s about getting the team together to get to know each other and discuss their project following a flexible template. For example, as part of the team contract, students do a “pre-mortem”: they are asked to imagine an issue has happened, what they would do and how they could have prevented it from happening. At the end of the semester, we ask students to reflect on their learning journey (what they did compared to what they planned). Then, students are required to grade themselves. Everyone was very humble and understood that it is not about me telling them “you have to learn this way”. It is about them setting their own goals, holding themselves accountable, overcoming challenges and reflecting on their journey. In fact, they all got precisely the grade they assigned themselves.\nOver time, how did you (re)design your courses to include Open Science principles? Gilad: At the University of Hong Kong, I am very privileged. I have a lot of freedom and flexibility. It is not a big deal to include Open Science principles into my own courses. I just try things out. Whenever I see a good idea, I try to embed it in my teaching. My students are very open-minded and let me do this and evolve together with me.\nMass Replications \u0026amp; Extensions: workflow, students’ feedback and co-authorship You have conducted several replications within your courses. Could you walk us through how it works from beginning to end? For example could you share the administrative/organisational side of it? How many students are involved in each course/replication and how do you motivate students to participate in replications? Gilad: This is complex - I usually give 3 hour workshops on replications! In terms of numbers, some of the courses I teach are large, with 70 students per class. Recently, in the more advanced courses I got about 20 students. We do two types of projects. At the beginning, we completed pre-registered replications and extensions. Within one semester, students went from basic statistics skills to a completed manuscript that was ready to be submitted - with everything written up, from preregistration to data analysis and so on. In 2020, we moved to a model of Registered Reports Stage 1. We got a lot of rejection letters from journals telling us about all the issues in our replications, based on outcome bias and hindsight bias. This is something that Registered Reports tackle very well: I got students to write a Stage 1 Report with a simulated dataset (Qualtrics helps with this) and detailed methodology and analysis sections. The workflow is shown in the image below: For pre-registered replications, the schedule is very tight, and everything is done within a month and a half. Then, I usually don’t sleep for two or three weeks and give two or three teams the same dataset, so that they can peer review one another. Finally, they produce a final report and an early-career researcher takes the lead as first author. Everybody takes credit and everything is written down. We still have about 25 unfinished projects - so if anybody in the FORRT community is interested, they can come and work with us!\nA very similar model is applied to Stage 1 Registered Reports, but without the data collection, while the model for thesis students is a bit different. They have one year, and they do a registered report. We then submit everything to Peer Community in Registered Reports. In terms of impact, students not only gain a publication but they have their chances of being hired (both in academia and industry) increased. This can make a difference for them, so they are very enthusiastic and motivated. They are serious about the work and they know the course itself is different from other courses - hands-on, learning by doing\u0026hellip; Some students appreciate this, but not everybody. Students with other preferences can drop out and choose other courses offered at HKU.\nAs one of your teaching goals, you have mentioned that “students experience the research process from beginning to end”. One of the most amazing things of the Mass Replications and Extensions is that students do make actual contributions to science. In your view, what is the role of students’ contributions to academic literature? Could you share with us the feedback that you got from students involved in replications? Gilad: Students can feel a lot of pressure, a lot of tension, during the process. Especially at the beginning. It’s so different from everything they have done before, so this can lead to some frustrations… I get some emails from students who have graduated and transitioned to industry, and reflecting back they are able to see these courses as a meaningful part of their journey (for good or bad). Some of them understand that science is messy, and others had never imagined they could have a contribution as undergraduates. It’s about empowering students to take part in the process. One thing I try to show them is how many mistakes professors make and that everybody is capable of contributing, helping us do better. Overall, many students find it meaningful, if not memorable - and they’ve learned important skills in the end. This is my aim and what I was hoping for.\nYou published several replications that started out as replications in classrooms. Were students interested in contributing to the research papers as co-authors after the end of the course? Can you explain how authorship is attributed and agreed upon? Gilad: This is an aspect that needs to be sorted out very early on, and we follow a very strict model. In my syllabus there’s a passage about “what it means to take part in this course”. Everybody is a co-author by default, but it’s possible to opt out if one makes it clear at the beginning (and reiterates it at each submission). Until now, and it’s been four years, nobody has opted out. In addition, we don’t want people to be forced by omission, and must ensure that the implications of this model are clear to them. The co-authorship model is explained to them early on, like a co-authorship contract: this is definitely very important to align expectations. I use a mandatory quiz on the course to collect their agreement and keep a written record (we also ask permission to submit articles in their name even after graduation). So, generally, every student is a co-author, with clear documentation of their contribution. We also have a credit contributorship where we keep track of what students, teaching assistants and early-career researchers bring in. Sometimes reviewers push back, so this document is useful to support and justify authorship attribution. Students did everything in their submissions. I guided them here and there but it’s really them who deserve the credit for the initial draft. It is open, transparent and collaborative work, with no room for misunderstandings.\nIn my PhD, I just remember a lot of interactions between students and professors where professors put themselves as first authors and some students were not even mentioned. I don\u0026rsquo;t understand how this is a sustainable system. It\u0026rsquo;s not fair, it\u0026rsquo;s not equitable, it’s not ethical. Undergraduate students, sometimes even high school students, can do science. There\u0026rsquo;s nothing about me having a professor title that makes me more eligible for co-authorship than a student. I believe in as much inclusion as possible, and it doesn\u0026rsquo;t matter where you come from, what rank you are, how old you are. Everybody should be acknowledged and included.\nYou also encourage other researchers to check and replicate your own work (Check me, Replicate me). Do you also incorporate this in your courses, letting students check your own work? Can you explain how that works? Gilad: I started in 2019 to do assessments with students, because I understood that before we do replications students need to see how others do replications. I just wanted them to go and have a look at the top-notch replication work in our field. I realized that I really worry that in ten, twenty years somebody is going to find an error that I have made… and funding, millions of dollars, hundreds of hours are going to be wasted! This is partially why I open up everything: I want others to be able to check, for example, my code - which is something that, when you go through peer review, can be overlooked. So I turned this into a learning opportunity. In 2020, we checked our replication reports from 2019. In 2021, we checked our replication reports from 2020, and so on. I was hoping (praying!) that, on the one hand, students wouldn’t find mistakes. On the other hand, though, if there were mistakes I really wanted students to find them. Last year I created a section of my website where I wrote “I will pay you 5 USD for a minor error, and 50 USD for a major error”. To my great surprise, students found mistakes and I paid them quite a bit of money (which they decided to donate to charity). Win-win. I’m promoting this “Check me, Replicate me” as a pledge that other people can also incorporate and I hope that we\u0026rsquo;ll be able to do more of that in the future.\nDo you have any general advice for new(ish) researchers wanting to be more open and transparent about their research and to educators wanting to implement Open Science in their teaching and mentoring? Gilad: The best way is to just get started. I was able to put my research aside for a year. If you can’t, try to incorporate some Open Science principles in your work. If you can’t free yourself, start doing this with a masters student, as part of a collaboration. Join a team that does this kind of stuff. I suggest starting very, very simple, and building up. We started with a very simple replication, a very simple main effect. It doesn’t have to be complicated, just be as transparent as possible. If you’re too scared, invite me, and I’ll show you how to do it. But in general, start small, or join a big team and see what you can learn. All links to Gilad Feldman’s teaching resources\nTeaching Portfolio: a document detailing achievements, teaching philosophy, goals and practices at HKU (2021) Course Materials: these include course summaries, syllabi, lecture slides, tutorials, teaching evaluations, and videos Open Science Talks and Workshops (since 2020): recordings are available on OSF | Open Science Talks and Workshops and Gilad\u0026rsquo;s Youtube channel A list of collaborative and open resources: guides and tutorials, examples of students’ work, templates, open projects for early-career researchers Resources about Open Science and the Science Reform Collaborative Open-science and meta REsearch: information on the project, mass replications and extensions, the team and how to get involved The .pdf version of this page, with Gilad\u0026rsquo;s answers to the community\u0026rsquo;s question Suggested Citation\nFeldman, G. (2022). Developing principled pedagogies - An Open Science journey: Insights from incorporating open science into teaching and mentoring. FORRT Pedagogies. https://doi.org/10.17605/OSF.IO/QNCBF\nTeam-Pedagogy Contributors:\nGiorgia Andreolli, Verona University, Italy\nJulia Wolska, Manchester Metropolitan University, UK\nFlavio Azevedo, Cambridge University, UK.\nLeticia Micheli, Leiden University, the Netherlands.\n","date":1669646680,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1670925736,"objectID":"403b0c3816f3588ba33cf1567ef5aeee","permalink":"https://forrt.org/pedagogies/002-gilad-feldman/","publishdate":"2022-11-28T10:44:40-04:00","relpermalink":"/pedagogies/002-gilad-feldman/","section":"pedagogies","summary":"Gilad Feldman shares his outstanding *know-how* on how to integrate open scholarship in higher education.","tags":[],"title":"Developing principled pedagogies - An Open Science journey","type":"pedagogies"},{"authors":["","FORRT"],"categories":[],"content":"\nNowhere Lab \u0026amp; FORRT. Nowhere Lab has partnered with FORRT as part of FORRT’s efforts to promote social justice and enable equality of access to open scholarship training! Nowhere Lab members benefit from weekly meetings where they can bring any questions they have about adopting any open scholarship practices (or anything else related to science and/or academia). Dr. Priya Silverstein (Nowhere Lab founder) is also available to answer any questions about engaging in other FORRT initiatives either during the weekly meetings, by reaching out via the Nowhere Lab or FORRT Slack workspaces, or by email. Nowhere Lab also operates informal mentorship – rather than matching mentors and mentees, people with mutual interests and skills are able to be connected and there are several channels in our Slack for seeking help or support for a variety of issues.\nBackground\nThroughout my academic career, I loved being in a lab. I looked forward to biweekly lab meetings as a research assistant, hearing about what the Ph.D. students were up to and wondering about what my own Ph.D. would hold (if I ever got that far). When I finally got a Ph.D. position, I had not one but TWO weekly lab meetings with each of my two supervisors. In the first, we always held journal clubs – taking some of the newest and most exciting developmental science research and dissecting the theory and methods to bits. In the second, we had a mix of journal clubs, presentations, and catch-ups. A Ph.D. can be quite lonely – just you and your super-specific topic – so lab meetings were a welcome time to reflect, process, and think with labmates. During a pandemic-induced furlough from my postdoc, and when I took two alt-academic jobs after this, I really missed this chance to connect with labmates. So, I decided to create my own lab. That’s where Nowhere Lab began.\nWho we are\nNowhere Lab ( http://nowherelab.com/) is an online community for people who would like the lab meeting experience but don’t currently have one. We have members from across all populated continents and career stages: undergraduate students, master\u0026rsquo;s students, Ph.D. students, postdocs, faculty members, and people working outside of academia. We hold weekly meetings and have an active Slack ( join here).\nIt has been a humbling and often emotional experience to hear the stories of those who have joined Nowhere Lab. Some members have joined when caught at a difficult and awkward time where they don’t feel embedded within their Ph.D. lab (e.g. they’re currently writing up) but also don’t have a new lab. Some members have joined when they get their first faculty job and don’t have any students for their lab yet. Some members have recently left academia and have mixed emotions about this transition. Some have joined because they’re considering postgraduate research but have never had any experience of research due to educational priorities in their country. One member even joined because health issues meant they weren’t able to take on new students and thus no longer had their own lab. We’re a home for the lost and are brought closer together by the stories of why we need Nowhere Lab.\nWhat we do\nWhat we do weekly is entirely dictated by members and has included job interview prep, advice on ongoing projects, discussions, statistical training, journal clubs, and much more. For example, we often have conversations where people who have done a Ph.D. share advice with members who are considering one. We decided to share this advice openly in a Ph.D. survival guide, in the hope that it will help people who are considering embarking on a Ph.D. or have just started. This guide covers things to know about doing a Ph.D., what’s great and not great about doing one, top tips, how to choose between two programmes, what to do when you think you’ve made the wrong choice, signs of a toxic environment, and how (not) to handle stress. As well as being useful, creating this guide was also a cathartic process for some members who had less than ideal Ph.D. experiences and wanted to share things they wished they’d known at the time.\nOur goals\nNowhere Lab aims to open science up to anyone and everyone. For example, we have members who work in tech and nonprofits who would not normally consider themselves scientists or get to engage in a scientific community. In addition to advancing diversity as an essential part of open science, we often focus explicitly on methods-based open science skills and knowledge advancement too. For example, we often read papers on open science and metascience in our journal clubs.\nAchievements\nAlthough the Ph.D. survival guide is Nowhere Lab’s first “output”, we have many other success stories that have come from the lab. We have now had several Ph.D. vivas and graduations – we even got to virtually attend one member’s viva which was really interesting for members who’d never seen one before. Careers have been changed – one member got a job from a Nowhere Lab referral, and two members were offered jobs after doing interview prep with the lab. And friendships have been forged – one member even visited me and stayed at my home!\nJoin us\nThere are many reasons why someone might want to join Nowhere Lab:\nNew faculty who have no one in their lab yet Ex-academics who now work in industry Keen undergraduates and masters students Freelance sci-commers/consultants People who are in a toxic lab People between jobs …and many many more! If you’re interested in joining Nowhere Lab, email me at priyasilverstein@gmail.com. We can’t wait to meet you!\nContact information:\nPriya Silverstein [priyasilverstein@gmail.com]\n","date":1662011080,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1662127351,"objectID":"2e3099a92a5507eb1d83158a5cc8577f","permalink":"https://forrt.org/educators-corner/013-introducing-nowhere-lab/","publishdate":"2022-09-01T01:44:40-04:00","relpermalink":"/educators-corner/013-introducing-nowhere-lab/","section":"educators-corner","summary":"Nowhere Lab (http://nowherelab.com/) is an online community for people who would like the lab meeting experience but don’t currently have one. We hold weekly meetings and have an active Slack. We have members from across all populated continents and career stages: undergraduate students, master's students, Ph.D. students, postdocs, faculty members, and people working outside academia.","tags":[],"title":"Introducing Nowhere Lab","type":"educators-corner"},{"authors":["","FORRT"],"categories":[],"content":"\nFive lessons from a non-academic job It’s been one year after leaving academia and starting to work for a charity organisation. For a bit of context, I did my PhD in Psychology, after which I spent four years across two postdocs (one in France, one in England) and I got to the point where I could no longer see my career progress in academia. There are many reasons why I came to this crossroad, but above all, I realised that my passion is with research and data, and less so with the other aspects of academic life. To my delight, my non-academic journey so far has been full of development and challenges and here are some of the lessons I shared, with some additional thoughts. Here are five things I learnt:\nThe three data lessons\nI learnt a lot about data. Real life data is messy AF. I learnt how to join complicated datasets with missing data and how to deal with types of variables I rarely encountered like strings and dates. I cannot underestimate how neat data from academic research is compared to what real life data looks like. I learnt how organisations store their data. Unlike any other datasets I worked with before, organisational data is dynamic. There isn\u0026rsquo;t just one database but rather multiple sets that are related to one another. You need tools like SQL to pull data from internal data systems. SQL queries are also used to power live dashboards which are used to display data in the simplest form. This is where most teams take their data from. Tools like PowerBi and Tableau are the major players here when it comes to data visualisation and internal data sharing. This was a completely new lesson to me because like most academics, I have been used to dealing with only static data, pulled from Qualtrics or arranged neatly within one Excel sheet. The good news about SQL is that I was able to pick it up relatively easily with my programming skills in R but what I still find tricky is finding the piece of data I need in columns and columns of data that aren’t always neatly labelled. I really miss having a neat codebook with all variable names listed! I also had to revisit my thoughts on descriptive statistics. In academia, inferential stats are king because we infer something about a population from a sample. With organisational data, you have access to the whole population so descriptive stats become very powerful. I had to resist an urge to ask “yes, but is this increase statistically significant?” because if all 100 programme members answered the same question about the satisfaction in the following year, and we saw an increase of 5% in satisfaction, this in itself is informational and quantified in terms of the effect size - we don’t need inferential statistics to tell us if this difference was significant at the 0.01 level or at the 0.001 level and whether this would apply to the wider population - this isn’t the point. The two (new) methodological approaches lessons.\nI learnt about quasi-experimental approaches using matching. While in academia I often ran experiments, establishing causation to evidence the impact of established programmes and interventions is more tricky as you often can\u0026rsquo;t randomly assign participants to treatment and control. This led me to learning about quasi-experimental statistical approaches and methods like propensity score matching to estimate the likelihood of treatment and select the ideal matched pairs. Lack of random assignment is also the least of all issues. The environment in which programmes and interventions happen in the real world is far from the controlled experiments we run in labs/online. So how do you assess whether participating in a long-term teacher training programme has a real life impact for the pupils they teach? How do you establish causation and the process by which it happens? These are a lot more tricky questions than I ever thought I had to approach. It turns out there is a whole host of methodologies to address this issue - these are a lot more prevalent approaches in disciplines such as policy (think: policymakers often want to know what impact introducing policy X had on the local population but they aren’t always able to randomly assign two groups of population into an intervention with policy application versus not). I learnt how to run a discrete choice experiment. This is a method that is often employed in marketing, I\u0026rsquo;ve rarely heard of it before but I found an excellent application of this method in the work we were doing. By designing a discrete choice experiment, we were able to assess what changes we could make to programmes we offer to increase their propensity. Would this programme be more popular if we introduced element X or increased Y? This technique looks at implicit preferences for various factors (not just two factors but many at the same time). It puts them directly in competition with one another to calculate propensity of each factor. It\u0026rsquo;s also often used in medical sciences and economics.I had a lot of fun designing this research and I am currently working with internal stakeholders to develop actions from this research - how can we turn this knowledge to attract more people to our programmes? It\u0026rsquo;s fascinating how quickly you can turn research into action. I find the process of reflecting on these lessons somewhat therapeutic because this time last year, I was full of excitement but also filled with some worries as I was embarking on my non-academic path. These lessons tell me that I not only was able to build on my research and data skills that I developed during my academic career, but also this move has allowed me to expand my development in ways I didn’t anticipate before.\nI originally went into academia because I loved this process of learning and developing, stumbling across new challenges and new approaches, but after 7 years I started getting worn out by discussing the same issues while using the same approaches and I craved new challenges. This first year of the non-academic pathway has certainly fulfilled that goal. I’m not saying that academia can’t be a fulfilling path with lots of challenges - I think it’s more about the fit. Digging deeper into what it is really about your job that makes you happy is going to lead you down the right pathways, academic or not.\nContact information: Karolina Urbanska ( k.urbanska@sheffield.ac.uk), Twitter @karo_urb, \u0026amp; website https://www.karolinaurbanska.co.uk/, and link to the original thread: https://twitter.com/karo_urb/status/1529784303885885441.\nEditor\u0026rsquo;s note: The present text is an adapted and expanded version of the widely shared Twitter thread by Karolina which resonated with so many of us. We invited the author if she would be willing to adapt \u0026amp; expand her thread to an Open Scholarship audience.\n","date":1657345480,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1660038643,"objectID":"b2209e02776f81031c8606b55e83dfd6","permalink":"https://forrt.org/educators-corner/012-lessons-from-non-academic-jobs/","publishdate":"2022-07-09T01:44:40-04:00","relpermalink":"/educators-corner/012-lessons-from-non-academic-jobs/","section":"educators-corner","summary":"This piece summarises research and data lessons I learnt one year into my first non-academic job. After seven years in academia, I felt that my development had stalled, I was no longer feeling excited about continuing my career in academia, and I was keen to explore new pathways. One year on, I have so many reflections I want to share. Spoiler alert: My inner nerd is very happy.","tags":[],"title":"Research and data lessons from a non-academic job:","type":"educators-corner"},{"authors":["FORRT","","","","","","","Amélie Gourdon-Kanhukamwe","",""],"categories":null,"content":"Watch the recording: Look at the presentations slides: ","date":1651750200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1660650498,"objectID":"62c26146ef0d399f5cc22912baf470da","permalink":"https://forrt.org/talk/openday22/","publishdate":"2022-05-04T17:00:00-18:30","relpermalink":"/talk/openday22/","section":"talk","summary":"FORRT's first-ever public event, where we presented current FORRT project in just eight minutes each!","tags":[],"title":"FORRT Open Day","type":"talk"},{"authors":["","FORRT"],"categories":[],"content":"\nWhat are the four grand challenges identified by APS members? 1. Globalization and diversity Psychological science has been repeatedly described as WEIRD, meaning Western, Educated, Industrial, Rich, and Democratic. In particular, the persistent use of predominately White and North American college students to study human behaviour has raised concerns about the generalizability of our findings, as well as our legitimacy as a science (Arnett, 2008; Henrich, Heine, \u0026amp; Norenzayan, 2010; IJzerman et al., 2021; Thalmayer et al., 2021; Tindle, 2021). Others argue that we should get rid of the dichotomy altogether (Ghai, 2021).\nHere’s what you can do to enhance globalization and diversity within psychological science:\nInvite international speakers to participate in opportunities that historically privilege U.S. citizens (e.g., colloquium series, workshops, awards, fellowships) Explicitly cite the achievements and ideas of Black, Indigenous, and People of Color within undergraduate and postgraduate courses, particularly Black women (e.g., see existing resources such as the BIPOC-authored Psychology Papers spreadsheet for a detailed list of available scholarship; reference Smith, et al., 2021 for a description of the Cite Black Women campaign) Encourage people to engage with marginalized groups, including Disabled people; reference Educator’s Corner on Navigating Open scholarship for neurodivergent researchers Acknowledge your positionality, meaning the social-historical-political aspects of a researcher that influence your orientations (see Bourke, 2014; Secules, et al., 2021) and include reflexive practice for all research pursuits, including quantitative research programmes (see Jamieson, Pownall, \u0026amp; Govaart, 2022) Consider context and identity by reporting sample demographics (Sabik, et al., 2020), as well as incorporating informed frameworks (e.g., interpret results via critical race theory or open scholarship) 2. Research integrity and applicability Many of us have been warned about the replication crisis in psychology (also commonly referred to as the replicability or reproducibility crisis, as well as the credibility revolution). Not unique to psychology, the crisis highlights that the results of many peer-reviewed scientific studies are either difficult or impossible to replicate (Open Science Collaboration, 2015). However, the legitimacy of the concern with regards to replication remains hotly contested (Feest, 2019; Maxwell, et al., 2015).\nHere’s what you can do to promote research integrity and applicability within psychological science:\nChallenge bias against open science as valuable or legitimate; incentivize early career and contingent scientists to participate by reshaping institutional norms or program milestones (e.g., encouraging students to pre-register dissertation studies; Bahlai, et al., 2019) Shift mentality to encourage collaboration rather than competition (e.g., scientists working across departments or multiple working groups rather than in a singular lab at a particular campus) Raise awareness about the unique experiences of intersectionally invisible participants (e.g., people who share two or more marginalized identities such as Black women; Coles \u0026amp; Pasek, 2020; Purdie-Vaughns \u0026amp; Eibach, 2008) Use existing resources, such as the Open Science Framework or aspredicted.org to collaborate, document, archive, share and register research projects, materials, code, and data; join the Psychological Science Accelerator 3. Collaboration across fields and disciplines As psychologists, we rarely cross borders, whether real or imagined (e.g., departments, universities, countries)! Collaboration across disciplines and fields, however, is extremely beneficial to psychological scientists seeking to identify connections between psychological processes, history, and broader context (2020). Take, for instance, work from Henderson and colleagues on Confederate monuments; they find that the number of lynching victims in a county is a positive and significant predictor of Confederate memorialization in that county (2021). An intersectional lens can be applied to their findings to illustrate how race and social forces such as the Civil War contributed to anti-Black racism in the U.S., particularly during Reconstruction in the South. Through this example, we can also see how the present work invites participation from other fields like American politics, history, and Africana studies.\nHere’s what you can do to collaborate across fields and disciplines within psychological science:\nParticipate in scholarly events outside of psychology, such as the Minority Politics Online Seminar Series Invite speakers from other disciplines to department- or University-led symposiums or conferences; provide remote options for attendance and accessibility grants to stimulate broader participation (e.g., the University of Virginia Diversifying Scholarship Conference) Initiate cross-discipline collaborations (e.g., Gaither \u0026amp; Sims, 2022) Leverage existing networks such as Academic Twitter to incorporate the views of people from relevant disciplines and achieve shared goals (e.g., a systematic co-creation of new knowledge on a specific topic) 4. Strengthening theory In science, a theory is a well-supported explanation of natural phenomena, confirmed repeatedly through observation and experimentation. Theory changes, however, if and when evidence accumulates that the theory cannot explain. Adaptive research in psychology must therefore follow the assumption that our knowledge will expand along with our understanding of human diversity (and vice versa). Doing so will allow room for concepts such as intersectionality and open science to be integrated within psychological science. Take, for instance, Bronfenbrenner’s ecological systems theory. Frustrated with the centrality of white boys, Stern and colleagues developed a new version centered on Black girls (2021). You can see the model here:\nHere’s what you can do to strengthen theory within psychological science:\nExpand theory regarding the mind and behavior to represent human diversity; the development of reliable theories is essential to scientific progress (see Eronen \u0026amp; Bringmann, 2021 for insights into developing good psychological theories) Compile evidence to advance new theories if and when they are no longer applicable or outdated due to history or context (e.g., COVID-19); create theories that center marginalized people and the real-world scenarios they experience in day-to-day life, controlling for relevant covariates Join me!\nIt is possible that I am overoptimistic about the future of psychological science or naive to the systematic obstacles that must be overcome. Still, I would rather be naive than conform to a system that is stagnant to change. When we stand outside the Ivory Tower, aware that the system is corrupt but alone, helpless, and discouraged, we know that the master\u0026rsquo;s tools will never dismantle the master\u0026rsquo;s house (Lorde, 2018). We were meant to be kept out of the house. Ergo, the time has come to build a house of our own, a house founded on principles that reflect and affirm our identities as the marginalized, unconventional, and historically excluded. It is within our avant-garde that we will find our true power, and grapple with the many challenges requiring redress to create a stronger, more robust science.\nHere’s what you can do:\nStrengthen our existing community by joining our Slack group Seek collaborations to pursue broader work exploring intersectionality, open science, and related topics and crowdsource manuscripts Identify funding opportunities to advance our goals: research transparency, reproducibility, rigor, and ethics (e.g., Einstein Foundation Award for Promoting Quality in Research) Spearhead initiatives (e.g., write a post for the Educators’ Corner) Spread the word about FORRT (e.g., share this post with your colleagues, tag us on Twitter) Contact information: Annalisa Myer ( amyer@gradcenter.cuny.edu), Twitter @MyerAnnalisa.\n","date":1650519880,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1650624695,"objectID":"a2d01e615745c960bf5cdb9dd136b913","permalink":"https://forrt.org/educators-corner/011-intersectionality-open-science/","publishdate":"2022-04-21T01:44:40-04:00","relpermalink":"/educators-corner/011-intersectionality-open-science/","section":"educators-corner","summary":"What are the four grand challenges identified by APS members? 1. Globalization and diversity Psychological science has been repeatedly described as WEIRD, meaning Western, Educated, Industrial, Rich, and Democratic. In particular, the persistent use of predominately White and North American college students to study human behaviour has raised concerns about the generalizability of our findings, as well as our legitimacy as a science (Arnett, 2008; Henrich, Heine, \u0026amp; Norenzayan, 2010; IJzerman et al.","tags":[],"title":"Addressing the grand challenges facing psychological science","type":"educators-corner"},{"authors":["FORRT",""],"categories":null,"content":"\nSlides Presentation This browser does not support PDFs. Please download the PDF to view it: Download PDF.\n","date":1644973200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1645036109,"objectID":"8bb692ddd4e87b8446debbe3da02d322","permalink":"https://forrt.org/talk/spsp/","publishdate":"2022-02-16T01:00:00Z","relpermalink":"/talk/spsp/","section":"talk","summary":"FORRT presents its Open Educational Resources to the *Teaching Personality and Social Psychology* Pre-conference at the Annual Meeting of the Society for Personality and Social Psychology in San Francisco.","tags":[],"title":"Integrating Principles of Open and Reproducible Science into Higher Education and Raising Awareness of its Pedagogical Implications","type":"talk"},{"authors":["FORRT",""],"categories":null,"content":"\nSlides Presentation This browser does not support PDFs. Please download the PDF to view it: Download PDF.\n","date":1644109200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1644332954,"objectID":"959977d839489402972ef4f669c42a59","permalink":"https://forrt.org/talk/cop/","publishdate":"2022-02-06T01:00:00Z","relpermalink":"/talk/cop/","section":"talk","summary":"FORRT presents its Open Educational Resources to the Community of Practice (CoP) Training Coordinators.","tags":[],"title":"FORRT's Presentation on its Open Educational Resources to the Community of Practice (CoP) Training Coordinators","type":"talk"},{"authors":["","","","","","","Magdalena Grose","","","","FORRT"],"categories":[],"content":"\nNavigating Open scholarship for neurodivergent researchers Who we are and why is this important? In academia, there has been much discussion about how open scholarship can benefit marginalised voices (e.g. Robertson, 2020; Pownall et al., 2020). However, neurodivergent individuals (e.g. dyslexic, autistic, ADHD) have received little attention. According to the Higher Education Statistics Agency, only 2.2% in 2003/04, 3.9% in 2012/13 to 5.5% in 2019/2020 of staff at universities in the UK disclosed having a physical or neurological disability. However, the true figures are likely to be higher. Despite increased coverage and interest regarding disability issues in academia, concerns regarding negative perceptions of disability disclosure remain. More open discussions are taking place about the lived experiences of disabilities and chronic illnesses, as people with disability are becoming less stigmatised, leading to more disclosures being discussed. However, there is still a question being asked: ‘where are the disabled academics?’ (We are here but you don’t see us!).\nDisability? How do you have a disability? You do not have a deficit with your body, you can still function in society, earn a living and participate in community life. Disability is portrayed as a personal tragedy, with some heroic individuals “overcoming” their terrible handicap. So how are you disabled? The answer is that not all disabilities are visible. The relative proportion of disabled academics is widely underestimated (e.g. Farahar, 2021), likely due to the stigma attached to the concept of disability (Mellifont, 2021). In addition, there are too many assumptions about disability being a physical or intellectual impairment with no discussion on difficulties regarding cognition and mental health. This discussion also fails to encapsulate that the range of difficulties people face may be connected to an individual’s neurodevelopmental characteristics (Singer, 2017). This is in spite of the fact that there are conditions that are described by psychologists as disabling but people with these conditions are treated as if they are not disabled. This is the ‘neurodiverse’ group.\nNeurodiversity is the non-pathological variation in the human brain regarding sociability, learning, attention, mood and other mental functions at a group level (Singer, 2017). An individual is neurodivergent if their neurology diverges from that of the neurological majority. Neurodiversity is critically relevant to the social sciences as it discusses the diverse cognitive behaviours forming the foundations of what it means to be unique and human. Importantly, the neurodiversity movement questions the assumption that all humans must conform to the same expectations in order to flourish.\nSo, you may still be wondering how neurodivergent individuals are disabled? The labelling of disability is a personal matter. It depends on what models we use to interpret it. Under the medical model, disability is portrayed as a flaw, weakness or a biological limitation of the individual. Put simply, it is a personal tragedy that was inflicted on you. Disability under the social theory of disability is defined not as a personal tragedy but as the result of barriers – environmental and social practices that disable, as opposed to enabling, the individual. In addition, the social model makes the distinction that an impairment is a property of an individual body, while disability is a social process. However, it is important to remember that both models are not mutually exclusive, but are required to be used together to open a constructive dialogue between able-bodied and disabled individuals, even among disabled individuals. The important message is that disabilities are dynamic and reasonable adjustments should be made for all groups.\nIn today’s academic environment, the economic and political system of universities, among many other industries, focus on productivity and profit. As a result, measures that focus on productivity and profit such as standards, norms, league tables, achievements and publications are becoming more and more important. Health and wellbeing and slow science are becoming less important, despite the fact that slow science and resting are important for creativity, critical thinking and understanding data, three crucial components to help accumulate knowledge. As a result of this development, burnout becomes more common and this is more prevalent among people with disabilities (Burns et al., 2021).\nPeople with disabilities are more likely to be excluded from the academic workforce with its demands for speed, efficiency and productivity, leading to normalised, ingrained and internalised ableism to an extent that they desire to be able-bodied. As a result of this exclusion, able-bodied individuals may believe that disabled individuals do not contribute to the productivity of the community and argue that their unemployment is the ‘fate of the idle’ (Singer, 2017). This means that in academia and industry, people with disabilities become oppressed, mocked, ridiculed and perceived as a nuisance.\nMany gatekeepers determine whether an individual is neurodivergent and these processes are driven by individuals who are neurotypical. As a result, referral time for these services vary widely, from 4 weeks to 201 weeks within the UK (Lloyd, 2019) and if a person does not fit the criteria, the individual can be ignored and may not receive the much-needed help that they require. This can lead to poor self-esteem, unemployment (e.g. around 22% in autistic people are in any type of employment; see Figure 2 in fact sheet). As a result, neurodivergent individuals may blame themselves for the difficulties they encounter, as opposed to the barriers that society has placed on them.\nDespite this, people of different neurodivergent conditions or families of the people with the conditions have begun meeting and talking to each other about their experiences and one common shared experience is a history of misinterpretation and mistreatment by the dominant neurotypical cultures and its institutions such as academia. As a result of centuries of oppression of disabled people worldwide and a hyper-normalised environment, in addition to seeing the disproportionate impact of the coronavirus pandemic on disabled students (see this amazing paper by Dr Joanna Zawadka), many neurodivergent and disabled staff feel discouraged in an environment that should aim to support them. They do not feel like they belong, their differences are seen as an impairment and their voice does not seem to matter. They do not see themselves represented in psychological science, academia, business, teaching or elsewhere.\nWe are a group of early-career neurotypical and neurodivergent researchers that are a part of the Framework of Open Reproducible Research and Training (FORRT) community, aiming to make academia and the open scholarship community more open to neurodiversity. Everyone, no matter what they identify with, is welcome in this group. We aim to discuss how open scholarship can be intersected with the neurodiversity movement, and emphasise how differences should be highlighted and accepted, whilst supporting the idea of accessibility. Our neurodiversity team is a group that currently consists of individuals that have autism, dyspraxia/DCD, speech-language differences, ADHD, dyslexia, or are neurotypical allies. If you have these or other neurominorities and wish to be part of the team, you are more than welcome to join!\nWhat do we want? We want academia and open scholarship not to be covertly homophobic, racist, sexist and ableist. However, by attempting to adapt and work within the existing defined rules of academia and its power structures, what we do is reinforce that system, irrespective of intentions. Lorde (2018) illustrates this well in the metaphor: the master’s tools (i.e. the dynamics, language, and conceptual framework that create and maintain social inequities) never serve to dismantle the master’s house, but somehow end up building another extension of that mansion. Similarly, the fundamental assumption under the medical model of disability serves to both disempower the individual and strengthen societal perception that the neurodivergent person is the problem. Thus, we need lasting, sustainable, and widespread empowerment that can be obtained by making and propagating the shift from the medical to the social model, or use both in order to encourage a constructive dialogue. Put simply, in order to fulfil our potential, we as neurodivergent people cannot say that there is something wrong with us, and we must use the tools of the social model to temper or remove barriers provided by the medical model.\nAccording to Feminist Standpoint theory (Harding, 1992; Pohlhaus, 2002), it is important that we empower under-represented and marginalised voices in knowledge production, and draw upon the lived experience when designing for under-represented voices. However, this is not possible, as we still follow psychology\u0026rsquo;s positivist (i.e., all authentic knowledge is scientific knowledge) tradition; this assumes that there is a fundamental truth of human diversity and that scientists should be objective. However, the research conducted on neurominorities is marred by the fact scientists are like everyone else. We are humans with different political views and lived experiences as well as biases (e.g. confirmation bias), which affect the questions that are asked and influence researchers’ degrees of freedom. Therefore, studies into neurodiversity are undertaken within structures that are characterised by power relationships, where colour, gender and neurological makeup are in no way neutral. This leads to neurodivergent individuals being treated as an _object _of the conversation, rather than the subject. This is analogous to the way the Eastern Societies and their members were treated by Western Researchers in the mid and late 20th Century (see Said\u0026rsquo;s Orientalism for further discussion; Said, 1976). \u0026ldquo;The Orient\u0026rdquo; was othered, misunderstood and seen as inferior, which led to a very skewed depiction which reflected Western biases. In post-colonial times, our awareness of power relations is, at least in some academic publications, greater than before. However, in the case of neurodiversity, there has been little change as most studies in the area are still conducted by those in possession of greater social power, with little input from neurodivergent researchers. This approach is not only patronising, but very dangerous as it will take centuries to counteract the discrimination it produces.\nThe power structure that dominates psychology and social sciences is from white, male and able-bodied people who treat neurodivergent people as an object of the conversation, not the subject. As argued by Jackson (1998, p.8), \u0026ldquo;Each person is at once a subject for himself or herself - a _who _- and an object for others - a what. And though individuals speak, act, and work toward belonging to a world of others, they simultaneously strive to experience themselves as world makers\u0026rdquo;. Once we consider that scientists are human, and neurodivergent people can be included in this research, then we can co-create projects that allow us to discover the truth about diversity from different perspectives. This is more commonly discussed in qualitative research but rarely even considered in quantitative research. In addition, with different perspectives, there is an emphasis on moving away from the typical White, Educated, Industrial, Rich, Democratic (WEIRD) samples that account for 80% of study samples but only 12% of the world population, despite the fact that this movement does not acknowledge the neurodiversity movement or neurominorities (e.g. autistic, ADHD, dyslexic). In addition, there is a lack of patient involvement in how to make the research more likely to improve the quality of life of neurodivergent people. The need to address this issue and to ensure disabled and marginalised individuals are directly included in research and policy-making decisions that affect them can be expressed by the commonly used slogan “Nothing about us without us”. Emancipatory and/or participatory approaches such as participatory action research (e.g. Bertilsdotter-Rosqvist et al., 2019; Fletcher-Watson et al., 2018; Grant \u0026amp; Kara, 2021; Leadbitter et al., 2021; Strang et al., 2019; Strang et al., 2021) have considerable potential for facilitating this type of collective knowledge creation and driving social change that benefits neurodivergent people in areas that may contrast with many mainstream research approaches.\nA recent movement has become important in education: open scholarship. This reflects the idea that knowledge of all kinds should be openly shared, transparent, rigorous, reproducible, replicable, accumulative, and inclusive (allowing for all knowledge systems). Open scholarship includes all activities that are not solely limited to research such as teaching and pedagogy. One key foundation of open scholarship is accessibility, a key facet that also belongs to the neurodiverse movement (e.g. Brown \u0026amp; Leigh, 2018; Brown et al., 2018). Accessibility and inclusion is where your content, activities and all their components are accessible to all people with disabilities, learning differences, mental health conditions or other health conditions that may affect their learning or engagement with the materials and activities, research activities, clinical training, and teaching (Victor et al., 2021a). It highlights the importance of embracing diversity and making everyone feel welcome and valued (see information sheet). Discussions have been, however, scarce regarding not only how open scholarship can advance the neurodiverse movement, but also how it can benefit from it. It is thus a priority to build community to discuss how the neurodiversity movement can be included in open scholarship, as the lived experience of neurodivergent individuals (including encountered barriers) may help to enhance accessibility, allowing open scholarship to be truly open (Whitaker \u0026amp; Guest, 2020). This in turn may help to dismantle the harmful stereotypes about disabled individuals (Devendorf et al., 2021), providing more specific provisions for neurodivergent and/or disabled researchers (e.g. virtual conferences; see Levitis et al., 2021). Furthermore, including this population in academia will help promote work-life balance, by denormalising overwork and practices that lead to burnout.\nThere has been a recent shift towards the use of Universal Design for Learning (UDL) (i.e., an approach to teaching highlighting that academics/tutors should be proactively, not reactively, inclusive by making adjustments to their teaching without students having to disclose their disability to student disability services) in higher education (Burgstahler \u0026amp; Cory, 2010). UDL has several benefits: by offering a more flexible and inclusive practice, there is no need to disclose one’s disability, irrespective of student status (Clouder et al., 2020). In addition, making the assumption about the student’s intention based on your interpretation of their behaviour can be damaging for neurodivergent students’ self-esteem. University staff should recognise the different manners in which students may communicate and contribute, whilst being open to collaborating with students to find suitable approaches. Put simply, it can be described as neurodiversity involvement for pedagogy.\nIn addition, UDL allows students to engage in the material that best suits their learning. Traditionally, university students are assessed through essays/ dissertations, group/individual presentations or examinations with very little discussion but neurodivergent students may find these types of assignments challenging that otherwise may succeed. UDL encourages educators to examine the students’ strengths, as opposed to weaknesses and allows students to have more choice. Learners could do a recorded presentation, as opposed to presenting in a group or present the skills they have acquired on the course in a different form that may suit them better. This would benefit the students in terms of better preparation for employment, by focusing on the student’s ability and professional values, as opposed to the challenges. This approach is also fully aligned with the UK Professional Standards Framework in Higher Education (UKPSF) as it facilitates educators\u0026rsquo; continuous development (A5), focuses on respecting individual learners and diverse learning communities (V1), promotes participation in higher education and equality of opportunity for learners (V2) and acknowledges the wider context in which higher education operates recognising the implications for recognising the implications for professional practice (V4). Another example is that attendance should not be used as a marker of class engagement, as there are several variables that predict class attendance (e.g. keeping pace with other students, attentional demands, and attendance being socially impossible). At the start of class, educators should signpost the expected outline and inform students before an activity is finished or changed. This lack of physical attendance and/or inability to follow attentional cues is not evidence of a lack of engagement but that students may engage in ways that are not expected and we should meet them where they are now, as opposed to where we expect them to be.\nAn important core property of UDL is to provide choice to allow students to develop agency in their own learning. Lecturers may feel that academic standards will not be maintained or students will not learn the learning outcomes but this choice aims to remove structural barriers that are faced when perhaps making a specific activity unnecessarily difficult, as opposed to reducing the academic level. For instance, lecture capture allows the student to learn in an environment that suits them and to learn at their own pace. Over decades, lecturers have questioned the effectiveness of lecture capture (Nordmann et al., 2021), but students such as dyslexic students, who otherwise struggled, may engage with learning and develop at their own speed (Nightingale et al., 2019). Rather than develop concerns on whether students will continue to engage, UDL offers students an opportunity to develop agency in their learning, a goal that lecturers should encourage. And applied more broadly to the academic employees’ relationships, UDL can also improve the academic culture, providing academic workers with opportunities to engage in their trade in a way that fits their neurocognitive style. Last but not least, UDL promotes and facilitates social justice and equality.\nFinally, we want academia to approach neurodiversity in the same way that true cosmopolitans approach cultural diversity. We want academics to reject the idea that the lived experiences of neurominorities such as dyslexia, autism, ADHD, which differs from the neuromajority, should be pathologised. Rather, these experiences should be accepted as fundamental to the human experience, to allow us to have different perspectives to understand what it means to be human. As a result, by considering this perspective, ‘‘our strengths and deficits will shape, not deny, our humanity’ (Grinker, 2010, p.173).\nPut simply, our team wants people in academia and the open scholarship community to understand that: “Being disabled does not make a person any less of a scientist. Actively listen to your students and/or peers if they disclose their disabilities, as they have entrusted you with sharing this important part about themselves. **If you are in a position of power and a disabled person asks for accommodations, give it to them. **Learn to recognise that disabilities are unique and dynamic. Ultimately, we should strive for universal design, of both our workspaces and pedagogy, as this gives everybody the best chance to fully be themselves and blossom in science” ( Middleton, 2021).\nOur plans Our overall aims are to: reduce the stigma neurodivergent individuals face by raising awareness of the contributions neurodivergent researchers have made, encourage neurodivergent researchers that there is no need to mask or hide our differences, and show that neurodiversity is an asset to open scholarship and academia. Open scholarship, like psychological science, benefits from neurodiversity and disabled perspectives, which have been growing in the past decade (e.g. Chown et al., 2017; Gillespie-Lynch et al., 2017; Grant \u0026amp; Kara, 2021; Kapp, 2020).\nCurrently, we are in the initial stages of creating a database of papers written by neurodivergent researchers. This will be a comprehensive crowd-sourced database of research written by neurodivergent researchers with the aim to allow educators to diversify their syllabi to include neurodivergent researchers to help counteract the bias towards able-bodied researchers. This database will help students feel that they can belong in academia, and that neurodivergent or disabled people can have some important strengths in research contexts (Grant \u0026amp; Kara, 2021). This project aims to be a “living“ resource, which will be regularly updated to include new studies authored by neurodivergent researchers.\nIn addition, we will engender a survey which will be similar to Victor et al. (2021b) and Devendorf et al. (2021). Their surveys investigated prevalence of lived experience, discussed the lived experiences and stigma (e.g. attitudes towards self-relevant research, help-seeking, disclosure) of mental health challenges among applied psychology researchers. The neurodiversity survey will detail the proportion of neurodivergent students and researchers (e.g. undergraduate students, graduate students, post-doctoral researchers) from different countries and different disciplines. It will investigate experiences in academia, possible concerns and barriers (e.g. external stigma, self stigma and difficulties in help-seeking) of neurodivergent students and researchers. In addition, we will discuss attitudes towards the neurodiversity movement, open scholarship, and the intersectionality between neurodiversity and open scholarship. We believe our work will improve neurodiverse representation and awareness. More importantly, we hope our work will also promote the inclusion of neurodiversity within the scientific community and the next generation of neurodivergent scientists. By including a neurodiverse population, “we have a huge opportunity to not only advance our science but also to equitably serve all of humanity” (Ghai, 2021, p2). Thus, we can encourage ourselves to broaden our minds and to truly be “open” in open scholarship.\nFinally, we are also currently planning a manuscript that will take the form of a position statement, detailing how neurodiversity and open scholarship can benefit from each other. With these additional resources to combat the biases, prejudices and misconceptions of disability, you may be surprised what we can bring to the table and we hope these resources can enable us to do just that.\nJoin us! We need to discuss the challenges faced by neurodivergent people, so they can feel seen. It is a wonderful experience to be seen and heard. We feel sad that not all neurodivergent people will encounter the same privileges that we have encountered. They feel these things so hard and this impacts their mental health, and in turn, leads to very upsetting ends. Neurodivergent people have been marginalised in so many ways that we have missed so many opportunities to make their lives that much better. We cannot change what has happened in the past; however, we hope that the resources we create will help improve representation early in students’ careers, so then they can hold their head high and smile, knowing they do not need to mask anymore. Also, they can feel proud that they can be themselves, achieve things with the correct reasonable adjustments and be treated as an equal, not an inferior. Put simply, we would like ally neurotypical academics of the neurodiverse movement and disabled academics to join us at FORRT, so we can provide \u0026ldquo;more effective support…[including] specific provisions for disabled researchers, online conferences and the ability to work from home\u0026rdquo; (Niedernhuber et al., 2021 p.34). These provisions need to continue post-COVID-19 pandemic because we cannot return back to normalcy. Not because it is not possible, but because the conversation about disability, race and gender has truly started and we are not letting this conversation end until everyone feels included in their environment. We want everyone not to just survive, but to thrive and flourish in their respective environment.\nHistory of Team-Neurodiversity This project began on the 24th June 2021 meeting of the Society for the Improvement of Psychological Science in a roundtable discussion, “How can open scholarship support evidence-based learning in people with neurodiverse conditions?” led by Dr. Tamara Kalandaze and Dr. Mahmoud Elsherif. During this roundtable discussion, Dr. Amélie Gourdon-Kanhukamwe attended this discussion. As a result of a fruitful discussion, an initial framework was developed that led to a discussion about how open scholarship and neurodiversity intersect, and on the 28th of June 2021, Team Neurodiversity was created. However, Team Neurodiversity did not have a home to discuss open scholarship and neurodiversity topics. The Framework for Open and Reproducible Research Training (FORRT) kindly offered us a base so we could establish ourselves and become a tour de force in this area. Although it started with five members, it has grown to a smashing total of 56 members based in more than 8 countries and is still increasing! The team has finished working on a database on neurodivergent scholars; a blog about open scholarship and its intersection with neurodiversity, which has been expanded to a position statement manuscript; a manuscript accepted in the British Psychological Society Cognitive Psychology Bulletin and a manuscript accepted in Association in Psychological Science. Currently, we are developing a survey to assess masking and neurodivergence in academia and the convergence between open scholarship values and neurodivergent values. We have a rotating leadership team which changes every six months. To promote diverse and inclusive leadership, anyone can put themselves forward for this role, regardless of their experience. The current team leaders are Magdalena Grose-Hodge and Bethan Iley. Thank you to our previous team leaders: Amélie Gourdon-Kanhukamwe, Flávio Azevedo, and Mahmoud Elsherif.\nReferences Bertilsdotter Rosqvist, H., Kourti, M., Jackson-Perry, D., Brownlow, C., Fletcher, K., Bendelman, D., \u0026amp; O\u0026rsquo;Dell, L. (2019). Doing it differently: emancipatory autism studies within a neurodiverse academic space. Disability \u0026amp; Society, 34(7-8), 1082-1101. https://doi.org/10.1080/09687599.2019.1603102\nBotha M. (2021). Academic, Activist, or Advocate? Angry, Entangled, and Emerging: A Critical Reflection on Autism Knowledge Production. Frontiers in psychology, 12, 727542. https://doi.org/10.3389/fpsyg.2021.727542\nBrown, N., \u0026amp; Leigh, J. (2018). Ableism in academia: where are the disabled and ill academics?. Disability \u0026amp; Society, 33(6), 985-989. https:/doi.org/10.1080/09687599.2018.1455627\nBrown, N., Thompson, P., \u0026amp; Leigh, J. S. (2018). Making academia more accessible. Journal of Perspectives in Applied Academic Practice, 6(2), 82-90. https://doi.org/10.14297/jpaap.v6i2.348\nBurns, K. E., Pattani, R., Lorens, E., Straus, S. E., \u0026amp; Hawker, G. A. (2021). The impact of organizational culture on professional fulfillment and burnout in an academic department of medicine. PloS one, 16(6), e0252778. https://doi.org/10.1371/journal.pone.0252778\nBurgstahler, S. E., \u0026amp; Cory, R. C. (2010). Universal design in higher education: From principles to practice. Harvard Education Press.\nChown, N., Robinson, J., Beardon, L., Downing, J., Hughes, L., Leatherland, J., \u0026hellip; \u0026amp; MacGregor, D. (2017). Improving research about us, with us: a draft framework for inclusive autism research. Disability \u0026amp; society, 32(5), 720-734.https://doi.org/10.1080/09687599.2017.1320273\nClouder, L., Karakus, M., Cinotti, A., Ferreyra, M. V., Fierros, G. A., \u0026amp; Rojo, P. (2020). Neurodiversity in higher education: A narrative synthesis. Higher Education, 80(4), 757-778. https://doi.org/10.1007/s10734-020-00513-6\nDevendorf, A., Victor, S. E., Rottenberg, J., Miller, R., Lewis, S., Muehlenkamp, J. J., \u0026amp; Stage, D. L. (2021, November 7). Stigmatizing our own: Self-relevant research is common but frowned upon in clinical, counseling, and school psychology. https://doi.org/10.31234/osf.io/szg5d\nFarahar, C. (2021, May 13) How can we enable neurodivergent academics to thrive? LSE Higher Education Blog.\nFletcher-Watson, S., Adams, J., Brook, K., Charman, T., Crane, L., Cusack, J., \u0026hellip; \u0026amp; Pellicano, E. (2019). Making the future together: Shaping autism research through meaningful participation. Autism, 23(4), 943-953. https://doi.org/10.1177/1362361318786721\nGillespie-Lynch, K., Kapp, S. K., Brooks, P. J., Pickens, J., \u0026amp; Schwartzman, B. (2017). Whose expertise is it? Evidence for autistic adults as critical autism experts. Frontiers in psychology, 8, 438. https://doi.org/10.3389/fpsyg.2017.00438\nGrant, A., \u0026amp; Kara, H. (2021). Considering the Autistic advantage in qualitative research: the strengths of Autistic researchers. Contemporary Social Science, 16(5), 589-603. https://doi.org/10.1080/21582041.2021.1998589\nGrinker R R. (2010). Commentary: On being autistic and social. Ethos, 38 (1), 172-8.\nHarding, S. (1992). Rethinking standpoint epistemology: What is\u0026quot; strong objectivity?\u0026quot;. The Centennial Review, 36(3), 437-470. https://www.jstor.org/stable/23739232\nJackson, M. (1998) Minima Ethnographica: Intersubjectivity and the Anthropological Project. Chicago: University of Chicago Press.\nKapp, S. K. (2020). Autistic community and the neurodiversity movement: Stories from the frontline. Springer Nature.\nLeadbitter, K., Buckle, K. L., Ellis, C., \u0026amp; Dekker, M. (2021). Autistic self-advocacy and the neurodiversity movement: Implications for autism early intervention research and practice. Frontiers in Psychology, 782.https://doi.org/10.3389/fpsyg.2021.635690\nLevitis, E., Van Praag, C. D. G., Gau, R., Heunis, S., DuPre, E., Kiar, G., \u0026hellip; \u0026amp; Maumet, C. (2021). Centering inclusivity in the design of online conferences—An OHBM–Open Science perspective. GigaScience, 10(8), giab051.https://doi.org/10.1093/gigascience/giab051\nLloyd, T. (2019). An audit of ADHD service provision for adults in England. https://www.adhdfoundation.org.uk/wp-content/uploads/2019/07/Takeda_Will-the-doctor-see-me-now_ADHD-Report.pdf\nLorde, A. (2018). The master\u0026rsquo;s tools will never dismantle the master\u0026rsquo;s house. Penguin UK.\nMellifont, D. (2021). Ableist ivory towers: a narrative review informing about the lived experiences of neurodivergent staff in contemporary higher education. Disability \u0026amp; Society, 1-22. https://doi.org/10.1080/09687599.2021.1965547\nNiedernhuber, M., Haroon, H., \u0026amp; Brown, N. (2021). Disabled scientists’ networks call for more support. Nature, 591(7848), 34-34. https://doi.org/10.1038/d41586-021-00544-8\nNightingale, K. P., Anderson, V., Onens, S., Fazil, Q., \u0026amp; Davies, H. (2019). Developing the inclusive curriculum: Is supplementary lecture recording an effective approach in supporting students with Specific Learning Difficulties (SpLDs)?. Computers \u0026amp; Education, 130, 13-25. https://doi.org/10.1016/j.compedu.2018.11.006.\nNordmann, E., Clark, A., Spaeth, E., \u0026amp; MacKay, J. R. (2021). Lights, camera, active! appreciation of active learning predicts positive attitudes towards lecture capture. Higher Education, 1-22. https://doi.org/10.1007/s10734-020-00674-4\nPohlhaus, G. (2002). Knowing communities: An investigation of Harding\u0026rsquo;s standpoint epistemology. Social epistemology, 16(3), 283-293. https://doi.org/10.1080/0269172022000025633\nSaid, E.W. (1978). Orientalism. New York: Pantheon\nSinger, J. (2017). NeuroDiversity: The Birth of an Idea. Amazon Kindle eBook, self-published.\nStrang, J. F., Klomp, S. E., Caplan, R., Griffin, A. D., Anthony, L. G., Harris, M. C., \u0026hellip; \u0026amp; van der Miesen, A. I. (2019). Community-based participatory design for research that impacts the lives of transgender and/or gender-diverse autistic and/or neurodiverse people. Clinical practice in pediatric psychology, 7(4), 396 .https://doi.org/10.1037/cpp0000310\nStrang, J. F., Knauss, M., van der Miesen, A., McGuire, J. K., Kenworthy, L., Caplan, R., \u0026hellip; \u0026amp; Anthony, L. G. (2021). A clinical program for transgender and gender-diverse neurodiverse/autistic adolescents developed through community-based participatory design. Journal of Clinical Child \u0026amp; Adolescent Psychology, 50(6), 730-745. https://doi.org/10.1080/15374416.2020.1731817\nVictor, S. E., Schleider, J. L., Ammerman, B. A., Bradford, D. E., Devendorf, A., Gruber, J., … Stage, D. L. (2021a, July 12). Leveraging the Strengths of Psychologists with Lived Experience of Psychopathology. https://doi.org/10.31234/osf.io/ksnfd\nVictor, S. E., Devendorf, A., Lewis, S., ROTTENBERG, J., Muehlenkamp, J. J., Stage, D. L., \u0026amp; Miller, R. (2021b, July 12). Only human: Mental health difficulties among clinical, counseling, and school psychology faculty and trainees. https://doi.org/10.31234/osf.io/xbfr6\n","date":1642916680,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1669758376,"objectID":"274dcc900a6878873465ad928fc491dc","permalink":"https://forrt.org/educators-corner/010-neurodiversity/","publishdate":"2022-01-23T01:44:40-04:00","relpermalink":"/educators-corner/010-neurodiversity/","section":"educators-corner","summary":"Navigating Open scholarship for neurodivergent researchers Who we are and why is this important? In academia, there has been much discussion about how open scholarship can benefit marginalised voices (e.g. Robertson, 2020; Pownall et al., 2020). However, neurodivergent individuals (e.g. dyslexic, autistic, ADHD) have received little attention. According to the Higher Education Statistics Agency, only 2.","tags":[],"title":"Navigating Open scholarship for neurodivergent researchers","type":"educators-corner"},{"authors":[""],"categories":[],"content":"\nOpen research is an umbrella term incorporating a range of principles and practices that make research more transparent, reproducible and accessible to everyone in the society. Open and transparent research practices, including research conduct and dissemination of findings, are essential to both those who produce and those who engage with research can access knowledge. Such principles and practices are relevant at all stages of the research cycle and are applicable to all disciplines, though it has been predominantly highlighted in the sciences and medicine. Further, though the principles of open research are consistent across disciplines, open research practices can be implemented differently across disciplines, thereby enhancing the transparency and robustness of research within and across disciplines\nThere are benefits of open research for individual researchers, including improved research quality and discoverability, shorter publication embargo periods, and reduced engagement in questionable research practices (QRPs). Open research practices also support the democratisation of scientific knowledge, and enables a greater number of people to engage with research. Training and education in open research principles and practices at early stages of individuals research journey is essential to maximise use of open research practices contributing the aforementioned benefits. Undergraduate and postgraduate students in particular can benefit significantly from training and education in open research at these formative stages. To date, open research educational resources have typically been aimed at PhD students, postdoctoral and more senior researchers, rather than undergraduate and master’s-level students, despite this learning period serving as the foundation to everything that follows.\nThe Principles and Practices in Open Research: Teaching, Research, Impact and Learning (PaPOR TRaIL) course was developed to provide a foundation in best scientific practice that benefits students, universities, and research as a whole. Our focus was to develop an open educational resource that would provide a comprehensive introduction to open research, and to help students incorporate open research in their research projects.\nPaPOR TRaIL is a freely available online course that was developed with students and research supervisors and funded by a National Forum for the Enhancement of Teaching and Learning in Higher Education award. Though primarily designed for undergraduate and Masters level students, the course is also applicable for PhD students and researchers looking for an introduction to open research. The course includes text-based materials, visuals, videos, templates, real-world examples, and activities for students to complete.\nThe course is comprised of an introductory module that introduces students to what open research is, why it is important, and how to do it. This introductory module can be completed by students as a standalone self-directed module that they complete in their own time. The introductory module is also designed so that it can be used as a Lecture that can be integrated into existing research courses by educators. The introductory module sections are shown below\nOnce students complete the introductory module they can complete any or all of the six practice-based modules that provide a more hands-on approach to _doing _open research Students do not need to complete all six modules, some modules may be more or less relevant to the stage of their research project and they can always come back to do other modules at a later date. When students complete the introductory and all practice modules, they receive a certificate of completion. The six practice-based modules are shown below:\nThe course is openly available to anyone who is interested, and is open for free enrolment here: PaPOR TRaIL\nThe course also has an open licence (CC BY), which means the course content can be re-used and remixed to suit different educator and institutional/organisation needs, provided attribution is given to the PaPOR TRaIL team.\nTo download course content, please enrol at the PaPOR TRaIL site.\nAll course materials will be made available on our OSF page in 2022, following any refinements required in the course based on initial student feedback.\nDetails of the course development can be found here: https://hrbopenresearch.org/articles/3-84\nFurther information, including our PaPOR TRaIL video, can be found here: https://osf.io/863ks/\nContact information\nThe PaPOR TRaIL Course\nDr Karen Matvienko-Sikar, , School of Public Health, University College Cork, on behalf of the PaPOR TRaIL team.\n","date":1641793480,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1642014889,"objectID":"c75d056ab5d3eaf9436dfef8161a2776","permalink":"https://forrt.org/educators-corner/009-papor-trail/","publishdate":"2022-01-10T01:44:40-04:00","relpermalink":"/educators-corner/009-papor-trail/","section":"educators-corner","summary":"PaPOR TRaIL is a freely available course on Open Research. The PaPOR TRaIL course is an open education resource, designed with input from students and research supervisors. The course provides a comprehensive introduction to the _what_ and the _how _of open research.","tags":[],"title":"The PaPOR TRaIL Course","type":"educators-corner"},{"authors":["FORRT","","","","","","",""],"categories":null,"content":"Watch the recording: Look at the presentation: ","date":1638341618,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1638374226,"objectID":"b8537d65558803e5c2774507a0f5dfe3","permalink":"https://forrt.org/talk/aimos21/","publishdate":"2021-12-01T01:54:38-05:00","relpermalink":"/talk/aimos21/","section":"talk","summary":"FORRT conducts hackathon at the 3rd Annual AIMOS Conference on its Replications and Reversals project.","tags":[],"title":"FORRT's Presentation on Replications and Reversals","type":"talk"},{"authors":[""],"categories":[],"content":"Open research helps increase the visibility of important discoveries and extend them to new places. It also helps to increase participation in research by underrepresented and underfunded groups. The open research ecosystem extends beyond just open access to data and publications. The principles of openness in research should extend to the entire research lifecycle, including education. I have chosen the term ecosystem because open research is an interconnected system. This system consists of philosophies, concepts, principles, practices, tools, and resources which when taken together are intended to make the research process open, transparent, accessible, and reusable by anyone.\nI have been the Data Curation Librarian at University of Tennessee, Knoxville, Libraries since 2013. During this time, I have helped hundreds of researchers meet the requirements of research funders and publishers to make their research data openly accessible. I have taught countless webinars and workshops on data management best practices, FAIR data principles, and open data. While open data is a major pillar of open research, it is only one component of the broad landscape of open research. I felt it would be a disservice to the research community I served if I did not situate my work within the larger context of open research and embrace the principles and practices in my own work. This began my quest to learn more about the open research landscape. To have the focused time to learn and to develop a resource for others, I applied and was approved for faculty development leave to devote one full semester to developing a series of training modules on various topics within open research. They were partly for my own benefit to learn the landscape and partly for the benefit of others who I was sure would be interested in learning more about the topic. The resulting product is the Open Research Toolkit (ORT).\nFeedback on the ORT has been positive. My primary goal in creating this was to help librarians skill up on open research principles and practices. The feedback from this audience has been especially positive and grateful. Colleagues have indicated that the ORT’s publication is timely as they are teaching a course on open research or are implementing internal training in their library next semester. I have also heard from domain specialist scientists who have noted it would be helpful as a first introduction to open research concepts for non-academic audiences or early career researchers. My hope for the ORT is that it could be used as a continuing education resource or supplemental materials in courses across a wide range of sciences and social sciences disciplines. To encourage adoption and reuse, I decided to assign it the least restrictive Creative Commons License (CC-By) to all original materials.\nThe process of creating the ORT spanned about a year. I initially began reading and collecting resources on myriad open research topics around January of 2021. All resources were added to an EndNote library organized by topic. I then methodically went through these resources and created a list of main topics I wanted to cover. Once my faculty development leave officially began, I actively created the modules on the identified topics. The first step was creating an outline for each module which pulled in content from all the resources gathered. Next, I created PowerPoint slides for each module. Then I created a script for the modules for someone to follow if they wanted to deliver the module. Then I created a narrated YouTube video of the content of each module to which I added subtitles for accessibility. Finally, I created Google Slides of each module and narration files in PDF and Word formats. All these materials are available on the Open Science Framework.\nEach module contains a slide deck in both PowerPoint and Google Slides, presentation notes in docx and pdf, a narrated video of the slides, and a bibliography of resources related to the topic.\nAll videos are publicly accessible on YouTube, and the video files are available for download from the Open Science Framework. Videos on YouTube contain both English and Spanish subtitles. ORT has been assigned a Creative Commons 4.0 Attribution ( CC-By) license to all original work in the ORT, so anyone can use and adapt however they like with attribution to the author.\nThe modules in the Open Research Toolkit currently cover the following topics. Links to each module’s materials are provided.\nModule 1: The Open Research Ecosystem [ YouTube, PowerPoint, gSlides, Notes]\nModule 2: Principles \u0026amp; Practices of Open Research [ YouTube, PowerPoint, gSlides, Notes]\nModule 3: The Philosophical Underpinning of Open Research [ YouTube, PowerPoint, gSlides, Notes]\nModule 4: Open Access to Published Research [ YouTube, PowerPoint, gSlides, Notes]\nModule 5: Open Access to Research Data [ YouTube, PowerPoint, gSlides, Notes]\nModule 6: FAIR Data Principles [ YouTube, PowerPoint, gSlides, Notes]\nModule 7: Reproducibility of Research [ YouTube, PowerPoint, gSlides, Notes]\nModule 8: The Five Rs of Open Research [ YouTube, PowerPoint, gSlides, Notes]\nModule 9: Open Peer Review [ YouTube, PowerPoint, gSlides, Notes]\nModule 10: Open Licensing of Data \u0026amp; Software [ YouTube, PowerPoint, gSlides, Notes]\nModule 11: Open Advocacy [ YouTube, PowerPoint, gSlides, Notes]\nModule 12: Citizen Science [ YouTube, PowerPoint, gSlides, Notes]\nModule 13: Open Policies [ YouTube, PowerPoint, gSlides, Notes]\nModule 14: Open Education Resources [ YouTube, PowerPoint, gSlides, Notes]\nMore resources and additional modules may be added over time. Future resources and modules may include topics such as open research ethics, open source software and collaborative platforms, and creating more detailed curricula for those interested in using the ORT for a course. Please let me know how you use the material so I can track its adoption and use.\nIn the spirit of openness, if you have ideas for additions or would like to collaborate on new modules, please contact me (info below).\nAll resources in the ORT can be found at the following DOI: https://doi.org/10.17605/OSF.IO/A4FTW.\nAll videos are on the ORT channel on YouTube at the following DOI: http://doi.org/10.7290/ORT_Videos.\nContact information:\nOpen Research Toolkit\nChristopher Eaker\nData Curation Librarian, University of Tennessee Libraries\n","date":1636814680,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1639077140,"objectID":"dcb2002f7e7b527a33b138b53741ca08","permalink":"https://forrt.org/educators-corner/008-open-research-toolkit/","publishdate":"2021-11-13T10:44:40-04:00","relpermalink":"/educators-corner/008-open-research-toolkit/","section":"educators-corner","summary":"Do you want to learn about open research and how you can implement open research principles in your work? The Open Research Toolkit is a series of educational modules designed to give you a basic understanding of the open research ecosystem.","tags":[],"title":"The Open Research Toolkit","type":"educators-corner"},{"authors":["FORRT","","",""],"categories":null,"content":"We presented FORRT at the Taiwanese Psychological Association (TPA) for the symposium \u0026ldquo;International efforts to improve Psychological Science\u0026rdquo;, which was organized by Sau-Chin Chen at Tzu-Chi University. We all \u0026ndash;ReproducibiliTea, COS\u0026rsquo;s OSKB, Montilla et al., Alzahawi \u0026amp; Monin, and FORRT\u0026ndash; presented on open scholarship topics. Here\u0026rsquo;s the symposium abstract:\nBeyond the local psychological issues, the challenges for psychological science are changing the norms and values of psychological researches. In comparison with the sophistication and innovation, many Western psychological researchers appraise the transparency and rigor. The Society for the Improvement of Psychological Science (SIPS) have provided the platforms for the psychologists who are improving the research methods and practices. Since 2016, many proposals have turned to the global research projects or distributed networks for shared purposes. In this symposium five project leaders and representatives share the current situation and achievements. At Q\u0026amp;A section we will answer the questions and discuss the plans to manage these projects in Taiwan. This symposium introduce five projects:\nOpen Scholarship Knowledge Base - “a community of volunteer advocates working with researchers, educators, and anyone interested in open scholarship to build a knowledge base so that researchers and users can discover and apply open practices. “ ReproducibiliTea - “a grassroots journal club initiative that helps researchers create local Open Science journal clubs at their universities to discuss diverse issues, papers and ideas about improving science, reproducibility and the Open Science movement.” Framework for Open and Reproducible Research Training – “a networking brings together educators and scholars working to improve teaching and mentoring practices in higher education.” Non-WIERD guidelines – “a project to aid academic members evaluate and increase diversity and inclusion in psychological research studies.” Crowd science – “a project aims to promote the diversity, rigor, and reliability of scientific research.” The recording For an overview of the whole event, see the conference program and the OSF Repository OSF Repository.\nPresentation slides ","date":1633788e3,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1633797723,"objectID":"f7c7254667e2dafa8b2de8da69b3cf81","permalink":"https://forrt.org/talk/tpa/","publishdate":"2021-01-01T00:00:00Z","relpermalink":"/talk/tpa/","section":"talk","summary":"We presented FORRT at the Taiwanese Psychological Association (TPA) for the symposium \u0026ldquo;International efforts to improve Psychological Science\u0026rdquo;, which was organized by Sau-Chin Chen at Tzu-Chi University. We all \u0026ndash;ReproducibiliTea, COS\u0026rsquo;s OSKB, Montilla et al., Alzahawi \u0026amp; Monin, and FORRT\u0026ndash; presented on open scholarship topics. Here\u0026rsquo;s the symposium abstract:\nBeyond the local psychological issues, the challenges for psychological science are changing the norms and values of psychological researches.","tags":[],"title":"A grassroots ECR-led international organization aiming to integrate Open Scholarship into higher education","type":"talk"},{"authors":[""],"categories":[],"content":"In academia there are some common ‘good practice’ mentoring things. Many of us do the ‘typical’ activities that good mentors do: weekly group meetings, individual meetings, open door policy, practice presentations, intellectual engagement, iterative writing feedback on theses, grants, and papers, collaborative interactions, and promoting work-life balance. But we can do more –much more– than the basics, without it taking a ton of time. Here are some innovative ways to up your mentoring game.\nGet more out of group meetings. Like many labs, we meet each week. But we rotate what we do, each in approximately equal measure: standard journal article discussion, discussion of a research article on Equity/Diversity/Inclusion (EDI), “Slide Improv”, and Fact or Fiction.\nEDI articles are chosen on a rotating basis by students. They are usually primary research articles. We conscientiously choose papers from diverse areas of EDI. These discussions have been fantastic, and have led to changes in how we do and discuss things. An added benefit is that this is a visible way to show the group that EDI is important to me as their supervisor.\nSlide improv gives experience thinking on the fly. One of the greatest fears students have in presenting is “what if I screw up” or “what if I forget what to say,” and slide improv gives them actual practice in how to respond to, and be more comfortable with, those moments. To do slide improv, a student makes a five-slide presentation (no animations). Another student gives it. The topic is supposed to be on something that isn’t our main research area, but does need to be something in science. They typically are a presentation of the background and results of a single publication. The student who is presenting doesn’t have to be accurate, but what they say has to be believable. Students are nervous doing this the first few times, but over time I’ve seen a marked improvement in confidence and presenting skills.\nFact or fiction gives experience in critical thinking. A student gives (brief) details and results for three papers. But one of them is completely fabricated. As a group, we try to figure out which is the fake one. This is fun, and seeing the gears turn as people sort things out is great.\nTo get a handle on how my students are doing, and if there are issues in the lab that need addressing, I do an anonymous poll once per year. The poll covers a range of topics on the quality of life, what is working, and what isn’t. This has been VERY HELPFUL in identifying things that need fixing (and some surprises of things that I thought were issues but are fine). I use a variation of this:\nLoading... If you would the direct link to the google forms, please follow this link.\nOver time, I forget which new students I’ve given ‘key advice’ and expectations. So I made a document that I give to all new students. There is a different document for undergraduate Honours Thesis, MSc, and Ph.D. students, since my advice and expectations for those students differ. The document has basic things such as how many weeks ahead to give me drafts and that all students are expected to attend group meetings, and life things like you will work long hours sometimes, but this should be the exception, not the rule.\nReading papers is important but not urgent, so gets delayed. Every 6 months I require a list of papers the students have read, annotating why they might cite it in their thesis. The expected number of papers is scaled over time. The annotations are really helpful to keep on top of reading, as a searchable document when writing, and for me to see what they are reading. Plus, with a large lab studying diverse topics, this really helps me see some papers I otherwise may miss, too! Every single student I have had make these annotations has said this was incredibly useful when writing their thesis and that they were glad they had done it.\nLastly, don\u0026rsquo;t be afraid to try out new ways of mentoring, and toss things that don\u0026rsquo;t work. I\u0026rsquo;ve tried a lot of things that just didn\u0026rsquo;t work for my group, or didn\u0026rsquo;t work for this group. I touch base frequently to see what works and doesn\u0026rsquo;t, and student\u0026rsquo;s like that I ask and it\u0026rsquo;s adaptable.\nEditor\u0026rsquo;s note: The present text is an adapted version of a widely shared Twitter thread that resonated with so many of us. We thought it was of general interest and deserved to be memorialized, and hence we approached Amanda to adapt the thread to post it here.\n","date":1631544280,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1631639382,"objectID":"2a9d32ecf4adb3f15b359bcfd90c4ab6","permalink":"https://forrt.org/educators-corner/007-easy-steps-to-elevate-your-mentoring/","publishdate":"2021-09-13T10:44:40-04:00","relpermalink":"/educators-corner/007-easy-steps-to-elevate-your-mentoring/","section":"educators-corner","summary":"Mentoring is an integral part of every academic's work, yet, we are still learning how we can be better towards our mentees. In this piece, Amanda walks us through what she learned works for both mentors and mentees.","tags":[],"title":"Innovative Mentoring","type":"educators-corner"},{"authors":[""],"categories":[],"content":"Starting with Qualitative Open Science Practices As a person who was trained primarily in research using quantitative methods but needed to do qualitative research to answer the next most pressing question, I was increasingly learning about qualitative research and how to conduct it properly. However, I had not seen much information about how to conduct qualitative research using open science practices. During my PhD, I had been introduced to open science practices as a way to improve educational research, but almost everything I saw did not seem to align with qualitative research. I asked colleagues what existed with regard to open qualitative research resources. Answer: no clue.\nCall to action We thus gathered people interested in qualitative open science research in education at the Virtual Unconference on Open Scholarship Practices in Education Research sponsored by the Center for Open Science. At the conference, we brought other educational researchers and open science fans together to debate what open science qualitative research might look like, put together a list of publicly available publications and tools, and figure out what to do with this information. By the end of a few of these hackathon sessions, we had a list of videos, websites, and publications to help the community understand how to do this work. The full list is located on the Open Educational Resources website but will be expanded upon here in this blog. This blog is intended to be a soft entry into this space of qualitative open science research, not a comprehensive journey; take the thoughts below as suggestions if they align with your philosophy, project, and Institutional Review Board (IRB).\nOutput We started the resource list with articles that focus on the basics of open science qualitative research. These articles (and Twitter threads) dive into some of the reasons why open science work is important, such as opening the ‘Black box’ of research or helping replication in later studies. And others also touch upon some of the murky issues of doing this work, like the fact that _quant _and _qual _researchers often have different ways of viewing the world and what truth is or can be. This is a good starting point so that you know that there will likely not be consensus on how to do this work well. However, if you read those and still want to try to incorporate open science into your qualitative projects, you can try this in any or all of the following areas: transparency/rigor, open materials, open access, and ethics.\nTransparency \u0026amp; Rigor. For transparency and rigor in qual research, this revolves around having enough information in your research so that other researchers A) know about the researcher(s) and their positionality and B) could do a similar study in the future, also known as replicating the study. For example, in the COREQ guidelines, there is an entire domain dedicated to transparency regarding the researchers and their backgrounds (see picture below). Researchers must also describe the context, questions, use of theory, sampling method, interview techniques, and analysis in a way that someone could imagine themselves performing those tasks. One open science practice that qualitative researchers could use to some degree before even conducting the study is preregistration. Essentially it’s like the dissertation proposal where you lay out your plan in advance. Preregistration (and registered reports, the preregistration done with a specific journal with a publication agreement) helps to ensure that a researcher does not change their stated methods after the study is completed. Explaining every detail of the study might be difficult if you are under space constraints (as with publication length requirements), which brings us to #3: open materials.\nOpen Materials. Thanks to the internet, researchers have websites and repositories where they can upload the tools for others to access. In qualitative research, this might mean interview protocols, memos, coding notebooks, tools (such as Nvivo or R packages), or even the data itself. This provides a sort of audit trail so others can verify the results of the research. There is no all or nothing here; open materials, much like the rest of these open science practices, exist along a spectrum. Not only what researchers share is on a spectrum; researchers can also dictate who may access the open materials. Perhaps it’s the entire public, but it could just be people who want to verify findings (i.e., dissertation committees, participants, reviewers). Below you can see how Bowman and Keene (2018) described open science practices as a layered onion with the innermost layer being the most transparent. However, no matter what or to whom materials are shared, researchers must include their plan within their consent procedures and IRB protocols to not violate any ethical boundaries. Ethics. Ethics must be considered with any research, but given the often close relationships with participants, potentially sensitive data revealed, and the fact that large amounts of data could reveal participants’ identity, ethics are huge when it comes to qualitative research. Resources both for abstract and practical purposes can be found in each section of the resources document (i.e., ethics specific to data management) and general ethical considerations are only within this section.\nOpen Access. Finally, researchers can consider how to share their research study. Publishing within traditional academic journals can limit who can read their work, so journals have begun providing open access tiers where researchers can pay the publisher so that the article is available to the public. Alternatively, researchers can “publish” their work online in a preprint server before it is published in a traditional publication outlet. Pre-prints can help researchers get their work out and get community feedback, although certain publications will not accept articles that have been put in a preprint server or require authors to change the pre-print or the settings on it. Open access as an open science practice appears to be generally the same for both quantitative and qualitative work, so fewer resources are here as they can be found on most open science pages.\nContact us Our team who created this (Rachel Renbarger, Sondra Stegenga, Thomas Lösch, Sebastian Karcher, \u0026amp; Crystal Steltenpohl) hopes that you’ll find this helpful for doing this work! We are continuing to explore this area further, so you can reach out to me at rachelrenbarger (at) gmail.com if you are interested in contributing to the conversation.\n","date":1625496280,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1625561114,"objectID":"0754cd0d05ee654e8dfba8c507d294bb","permalink":"https://forrt.org/educators-corner/006-qualitative-os-practices/","publishdate":"2021-07-05T10:44:40-04:00","relpermalink":"/educators-corner/006-qualitative-os-practices/","section":"educators-corner","summary":"Many education researchers have begun to engage in open science principles, techniques used to improve the replicability and transparency of research studies. However, since much of the advice for open science education research revolves around quantitative articles, we created a resource list for people who want to try one or more open science practices with qualitative research. Resources are included for all aspects of open science, such as making research studies accessible (open access) or pre-registering your study before you even begin. This list is not meant to be exhaustive but as a start for researchers to find practices that work for them and their participants.","tags":[],"title":"Qualitative Open Science Practices","type":"educators-corner"},{"authors":[""],"categories":[],"content":"Making lemonade out of remote teaching 2020 was a year of big changes for the whole world. For the academic community, it was not different. The need to rapidly switch from in-person to online teaching represented a big challenge for most educators. However, with new challenges come also new and big opportunities. Assistant Professor of Economics at Stockholm University, Johannes Haushofer, saw in remote teaching the opportunity to open classes to students from low and middle-income countries.\nRemote Student Exchange was then created as a volunteer and non-profit initiative to match professors willing to offer spots in their classes with interested students from low and middle-income countries. To facilitate the matching process, a website was launched. It has been a great success so far. Two weeks after its launch, there were 1701 students and 88 professors registered and more than 30 courses offered in varied disciplines such as Economics, Business, Psychology, Political Science, Neuroscience, and many others. You can browse the available courses here.\nAt FORRT, we believe that one of the most overlooked benefits of integrating open and reproducible scholarship into higher education is that of social justice. Academia is still a place of privileges that many cannot afford. We highly commend this initiative and in the hopes of helping it grow even further, we have invited Johannes Haushoffer to explain a bit more how this idea came to be and to share his initial experiences with the community.\nThe Remote Student Exchange in a nutshell\nIf you are teaching a remote course in the next few months, we highly encourage you to consider taking part in this initiative. Students can sign up free of charge, but they do not receive official grades or credits. This means that this initiative is informal and in most cases, it should not require permission from universities or department chairs.\nTo participate, visit the website https://remotestudentexchange.org/. In the home page (see figure below) you can sign up as a professor.\nAfter confirming your email account, you will be able to set up the details of the course you want to offer. As shown in the figure below, you will be asked to indicate the subject area, the level of the course (e.g., bachelor, master, Ph.D. level), and any prerequisites for attending the course (e.g., specific courses and areas of knowledge). You should also provide the link to where the classes take place (e.g., Zoom).\nImportantly, you are flexible to decide how many students you can host and how they can participate (see the figure below showing the settings for course availability). You can accept students to actively participate with questions, discussions, and assignments, and/or you can accept students to audit silently only. You can also decide to review applications to your course and choose which students to accept or to admit enrolled students on a first-come-first-served basis.\nInterested students also have to sign up on the website. They will be able to browse the courses being currently offered and apply to take part in a class, provided it still has free spots and that they fulfill the prerequisites (if any).\nIf you have more questions, make sure to access the FAQ and watch the video below where Johannes explains a bit more about this initiative. We hope that many more scholars consider joining it. Share this idea widely with other educators and with students that may be interested. You can find a link to the countries contemplated by this initiative here.\nVideo Interview Direct Link to YouTube\n","date":1621867480,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1622034008,"objectID":"c43d3454733c048b46fc90790ccc24dc","permalink":"https://forrt.org/educators-corner/005-remote-teaching-platform/","publishdate":"2021-05-24T10:44:40-04:00","relpermalink":"/educators-corner/005-remote-teaching-platform/","section":"educators-corner","summary":"Remote Student Exchange was then created as a volunteer and non-profit initiative to match professors willing to offer spots in their classes with interested students from low and middle-income countries. To facilitate the matching process, a website was launched.  It has been a great success so far. Two weeks after its launch, there were 1701 students and 88 professors registered and more than 30 courses offered in varied disciplines such as Economics, Business, Psychology, Political Science, Neuroscience, and many others.","tags":[],"title":"Making lemonade out of remote teaching","type":"educators-corner"},{"authors":["Patrick Forscher"],"categories":[],"content":"Psychological science is in the midst of a “credibility crisis” in which its practitioners re-examine their practices and re-define what constitutes study rigor. Replication studies have formed a critical role in motivating this sense of crisis – a sense of crisis that has led directly to the current movement to improve psychological science through a “credibility revolution”.\nDespite this important role, when I was invited to hold a virtual two-day, 10-hour workshop on replication studies for the students and faculty of the Department of Social and Organizational Psychology at ISCTE in Lisbon, I realized that I did not have any ready-made teaching materials on this important topic. This blog shares the guiding principles of the workshop and my finished materials so that you, the reader, can learn from my experiences.\nMy workshop materials, including a syllabus, suggested readings, and exercises, are freely available at https://osf.io/m9bzh/\nWorkshop overview\nMy workshop describes the why and how of replication studies: why a researcher might want to conduct a replication study and how a researcher should go about conducting a replication study. It also emphasizes some issues that are often neglected in discussions of replication studies, at least in my experience, including the importance of choosing good replication targets, the importance of resource constraints in doing good sample size planning, and the use of simulation studies in sample size planning.\nThe workshop proceeded in four modules, as shown in this workshop slide:\nEach module is structured around one or two learning goals, or big-picture takeaway points that I wanted the students to understand at the end of the module. Each module also breaks up lecture sections with independent or guided exercises. The exercises are intended to both reinforce the content of the lecture and give the students hands-on experience with a broad swathe of the skills that go into replication research.\nThe remaining sections of the blog will describe the content of each module, how this content reinforces the module’s learning goals, and the exercises I used for each module.\nModule 1: The credibility crisis\nLearning goals: The credibility crisis revealed weaknesses in research; these weaknesses are caused by hidden processes that don’t show up in published reports\nThis module is framed around a schematic version of how empirical psychological science works (adapted from Munafo et al., 2017).\nIn the schematic, you generate a hypothesis from theory, design a study to test the hypothesis, collect data based on the study design, analyze the data to test the hypothesis, interpret the results, publish the data, and begin the cycle anew. By using this process to compare discrepancies between your hypothesis and the data, you can identify flaws in your theoretical assumptions, which allows you to revise the theories and improve them.\nHowever, a variety of events made psychologists aware that something about this cycle wasn’t working. First was the observation that only about 8% of the results published in psychology journals are negative ( Fanelli, 2010) – yet these negative results are necessary to identify flaws in theoretical assumptions. Second was the publication of a paper by the well-respected social psychologist Daryl Bem, who, in 2011, published a somewhat unusual paper in the world’s top journal for social psychology, the Journal of Personality and Social Psychology ( Bem, 2011). This paper used methods that met or exceeded the standards of rigor that were typical for the time, but advanced a claim that was patently absurd: that college students (and, by extension, everyday people) could be influenced by future events.\nThis paper suggested either that everything we knew about physics was wrong or (perhaps more likely) that the research methods in social psychology that we used to think were rigorous were somehow flawed.\nI then describe how these observations spurred a credibility crisis (not a “replication crisis”, as the crisis is broader than just a lack of replicability) in which researchers investigated whether and how research methods in social psychology are flawed. I use two exercises to illustrate some of the problems.\nIn the first, the students use a free shiny app to simulate how the combination of the selective publication of positive results and low statistical precision create huge distortions in our understanding of the evidence supporting a particular psychological theory. This exercise demonstrates the concepts of statistical power, precision, and publication bias and demonstrates the general method of using simulation studies to understand what happens when some quantity (in the context of a simulation, a parameter) varies.\nIn the second, the students use FiveThirtyEight’s p-hacking app to illustrate what makes p-hacking possible. I randomly assigned students to either the Republican party or the Democratic party and to either make their assigned party look good or bad. This exercise illustrates how p-hacking can emerge from the combination of flexible definitions / measurement and a desire to obtain a certain result.\nI close the module by describing how published research reports only constitute a subset of what goes into creating a certain research funding and how processes like p-hacking and publication bias, while hidden, can undermine a finding’s credibility. Thus, one way we can uncover the credibility of a finding is to make visible more aspects of the research process. The next module describes one way to achieve this greater level of visibility.\nModule 2: Replications as a way of highlighting what is known\nLearning goals: Close replications test the credibility of a finding. They work best with good documentation \u0026amp; structured ways of choosing replication targets\nThis module starts with the question of how to determine whether a particular finding is “credible”. One way is to fix in place the hypothesis and try to do the study again. When you do the next study, you can either vary certain aspects of the method or duplicate the past method as carefully as possible. Although I don’t yet introduce this terminology, these two sets of scenarios illustrate the ideas behind a close replication and a distant replication.\nI then ask the students to complete an exercise to think about the conclusions that are reasonable in close and distant replications. The goal of this exercise is to illustrate how close replications (where you keep the method similar) are particularly informative when they give you results that differ from a set of past results because they call into question the credibility of the original results. However, they are less informative when you get results that are similar to the original.\nIn contrast, distant replications (where you vary some aspect of the method) are particularly informative when you get results similar to the original results because they allow you to generalize across the methodological feature that you varied. However, they are less informative when you get results that differ from the original because, in addition to all the explanations that apply when you do a close replication, the features that you intentionally varied could also have produced the different results.\nI also use this exercise to highlight the importance of minimizing sampling error and fully documenting procedures; both sampling error and undocumented differences in procedure (“hidden moderators”) provide possible explanations for why replication results differ from original results. This highlights a somewhat hidden side benefit of replications: they force you to very carefully document a particular procedure.\nTo illustrate how to document the procedure behind a replication study, I introduce the students to the “replication recipe” ( Brandt et al., 2014). The replication recipe provides a structured set of questions to guide the process of creating a method section for a replication study. As an exercise, I ask students to fill out the first section of the replication recipe with an article that I assign (I pre-selected two articles that are short and have a relatively simple research design). After the exercise, we discuss the process of using the replication recipe and identify issues that came up – including the poor reporting standards of most (but not all) psychology articles.\nIn the last part of this module, we discuss a way to choose replication targets. I teach a somewhat informal version of a framework developed by Isager and colleagues (2020). As an exercise, the students use the framework to rate the value, uncertainty, and cost of doing the replication study that I assigned them.\nModule 3: Resource planning and piloting\nLearning goals: Resources are critical for making your replication precise; you should think through the sample your resources allow \u0026amp; use those to calculate your power\nThis module subsumes most of the content that would normally be taught as “power analysis”. The reason I frame power analysis as “resource planning” is to emphasize the critical, and often unrecognized, role that resources (time, money, skills) play in the number of observations a replication study achieves. I teach two workflows for planning resources: a resource-first workflow based on identifying the practical constraints to one’s resources and determining the power those constraints allow to detect different effect sizes, and smallest-effect-size-of-interest workflow based on identifying a smallest effect size of interest and the number of observations (i.e., resources) required to achieve different levels of power to detect that effect.\nThe module relies heavily on faux, an R package for simulating fake data that fits a specific set of constraints. I link the idea of simulation studies back to the first module and tell the students that I am giving them a powerful set of tools to conduct their own simulation studies. I do not assume that students know how to use R, but rather wrote two different scripts that make simulating two-group and four-group designs easy, even for a complete R novice. My goal is to give the students some tools to do basic tasks in R, as well as resources to learn more about simulations in R if they have the time and interest.\nA hidden goal of this module is to demonstrate how under-resourced most past research really is. For example, when I illustrate the resource-first workflow, I assume that we have enough resources to achieve 80 observations per cell in a two-group design – a number of resources that meets or exceeds the sample sizes used during the 2000s in social psychology ( Fraley \u0026amp; Vazire, 2014). This design yields abysmal power to detect most reasonably-sized effects.\nAs another example, when I illustrate the smallest-effect-size-of-interest workflow, I assume that the target effect is part of an interaction. Interactions require about 16 times the sample size to detect than main effects, a fact that is illustrated vividly in the simulation-based power curve from this part of the module.\nOne last issue that comes out of the simulations is the number of assumptions that one must make in the process of doing a simulation study. This includes both statistical assumptions, such as the size of the standard deviation of the outcome measure, and non-statistical assumptions, such as the length of time it takes for a typical participate in the study (a fact that is necessary to accurately estimate the number of participants who can participate in a lab-based study, for example). I argue that pilot studies are useful for developing good values for these assumptions. Pilot studies are not useful for directly estimating the value of the target effect size itself ( Albers \u0026amp; Lakens, 2018); in any case it is better to power to a smallest effect size of interest than the expected effect size.\nModule 4: Preregistration\nLearning goals: To preregister something, create an OSF project \u0026amp; put the replication recipe in the registry. There’s evidence this helps make research more credible\nThe bulk of this module is focused around completing a pre-registration for the article assigned to the students in the previous modules. Because the workshop participants have already completed the first part of the replication recipe ( Brandt et al., 2014) for this article, they are already familiar with the article’s purpose and materials. For the first part of this module, the students complete the remainder of the replication recipe (or at least, as much as they can) as part of the last exercise of the workshop.\nThe replication recipe in hand, the students can complete a replication recipe-based preregistration on the Open Science Framework ( OSF). I walk the students through this process and introduce them to the basics of uploading materials on an OSF page. But the students already completed hard parts of preregistration as part of the previous exercises.\nI spend the remainder of this module reviewing evidence of what a well-planned and well-executed preregistration can do for the credibility of research.\nI also briefly cover Registered Reports, which circumvent publication bias through a staged review process. At stage 1, a proposal is reviewed prior to any data collection. At stage 2, the proposal is reviewed again, and as long as the researcher executes their accepted study protocol, published regardless of results. I give the students a list of journals that accepts registered reports if they’re interested in conducting a study with this publication format.\nConclusion: Use my materials!\nReplications have played a critical, though I think sometimes misunderstood, role in spurring the credibility revolution: they are one way of investigating the credibility of a particular research finding. The primary way they do this, I think, is by serving as a tool to highlight previously unknown pieces of information, either during the process of documenting the procedure (so it can be successfully executed) or via the results themselves. This role deserves to be underscored in teaching materials. I also think replications can be a useful venue to highlight other parts of the research process, such as decisions about what to research and resource planning. The structure of my finished workshop reflects this general outlook.\nIf you find my materials useful, please use them! The materials are freely available in this repository. Let’s use our teaching to pass the lessons of the credibility revolution on to the next generation of behavioral scientists.\n","date":1620107080,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1620220665,"objectID":"b11ee1fbb8a9800bfc4377f54bf4084b","permalink":"https://forrt.org/educators-corner/004-teaching-why-how-replication/","publishdate":"2021-05-04T01:44:40-04:00","relpermalink":"/educators-corner/004-teaching-why-how-replication/","section":"educators-corner","summary":"Patrick S. Forscher  walks us through his workshop on how and hy to conduct replication studies. He shares all his materials so you can become learn and teach these concepts.","tags":[],"title":"Teaching the why and how of replication studies","type":"educators-corner"},{"authors":["FORRT","",""],"categories":null,"content":"The recording: The presentation: ","date":1610028e3,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1625928815,"objectID":"640b7b681bae2733bbd968caae453e09","permalink":"https://forrt.org/talk/bitss-2021/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/talk/bitss-2021/","section":"talk","summary":"The BITSS Annual Meeting brings together actors from academia, scholarly publishing, and policy to share novel research and discuss efforts to improve the credibility of social science by advancing research transparency, reproducibility, rigor, and ethics.","tags":[],"title":"FORRT @ 2021 BITSS Annual Meeting","type":"talk"},{"authors":[""],"categories":[],"content":"Developing a comprehensive directory of tools and technologies for social science research methods Often the search and exploration of tools and technologies in social science research is not part of the class curriculum in the same way as the systematic review of literature is. This, sadly, leaves the becoming researcher in a place of disadvantage, in my opinion. In their early research career, students will mostly rely on their supervisor or peers to advise on the tools they use, which is still a very limited sample. However, with strides in technological development, researchers could choose from a growing number of multivariate tools for social science methods rising from within the discipline itself, as well as borrowed from other disciplines or coming from the commercial sector.\nStarting from this premise, we decided to build a tools directory for social scientists, a simple solution for a place where any researcher or student can come and find the right tool for what they need. In this piece, I explain how the tools directory was developed and how it can be used by educators, researchers and students.\nDeveloping the tools directory The initial list was based on software tools and tech platforms that we knew were popular among social science researchers because we’ve commissioned books about them, or they have been prominent within the community. We continued to ask academics, look through papers and other lists like the DiRT Directory from the Digital Humanities, the Digital Methods Initiative and SourceForge. Soon enough, the directory was growing out of control. What we thought would be a simple scroll down page, organised in a few basic categories, was not serving its purpose any longer.\nWith around three hundred different software packages and tools that we knew were used by some or many social science researchers in their work, a new challenge was becoming apparent. It was a paradox-of-choice situation. On one hand, it was increasingly clear why academics often rely solely on recommendations from their peers when choosing a tool. And on the other hand, we knew we needed to explore how one would choose the right tool from a list, and ultimately how to teach others to find the tool that fits their own purposes rather than simply recommending a tool they’ve used.\nAs the list grew, we enlisted the help of a few master students, and started collecting more data: who built these tools, were they free or paid, what cluster of similar tools would they belong to, when were they built, based on the information available could we tell whether they were up to date, scaling, or failed, could we find papers that cited these tools, were the creators recommending a citation etc.\nWhen we hit 400 software packages/tools, we knew we had to promote this list and share it in a way that researchers would actually stumble upon it and have the opportunity to reference it in a lecture or paper. So we wrote a whitepaper summarizing the big trends on the development of tools and tech for social science research. We learned that both commercial and non-commercial tools are popular within the social sciences, but the ones that last longer and are more successful focus beyond the discipline and almost always have a person or teams of people dedicated to raising funds or expanding the community of users and contributors.\nAt 400 software packages/tools, we were still not sure the list was big enough. We then focused on specific methods and researched all the tools available to carry out that method or task within the research process. We looked at the evolution of technologies for that method in particular, as well as how it fits within the development of the method itself. We call these ‘deep dives’. We’ve done deep dives on tools for annotation, or tools for transcription, surveying tools, tools for studying social media data, and we kept finding more software applications within each of these areas. We concluded these deep dives to be quite useful, as they enabled sharing slightly more comprehensive sub-lists of tools that could be used in different modules. We have now 543 tools on the list, and the number keeps growing.\nHow to use the tools directory The full directory is currently available on our GitHub repository as a csv file. We decided to host it on GitHub, in order to be able to update the directory when we come across new tools or after deep dives; ensure it’s always available for others to reuse in its most up-to-date form, and enable instructors, students and researchers to add tools that might be missing.\nEducators teaching research methods or preparatory courses for students’ theses could present the full tools directory to students, so they are more flexible in finding the right tools for their needs and future projects.Students can browse through the list and filter for tools to find a tool that is most appropriate for a research project they are initiating. For example, a student transcribing interviews might look at the transcription tools to find alternatives. Similarly, educators that are teaching a more specialized course, such as introduction to text mining, data visualization, or social data mining, or running online experiments could filter out a sub-list of tools focusing on the explicit method. They could then share this sub-list as part of the course reference materials or assignments.\nFig. 1. The spread of 543 tools and technologies across methods and techniques.\nFig. 2. Filtering to find transcription tools. A student or instructor could filter by column F (the Competitive cluster which contains the method/technique/task/area that we used to categorize the tool) to get a sub-list of tools that could be broadly used for a particular process. If the cluster is too broad, the student can look through the technique (column E), that breaks it down further. For example for social media tools, the technique would include analysis, collection, visualisation etc. If looking for more recent tools, one can filter by the year the tool was launched (column M); or if the student is interested in something that is free, they can check the charges (column N).\nWhile the csv file that contains the tools directory might be easy to update and share, we acknowledge that it might not be that easy to use within a classroom. We are experimenting with a variety of ways that would enable a better display and navigation of the directory, without losing from the ease of updating it.\nIn 2019 we did our first deep dive into the tools for social data science to support our SAGE Campus course on collecting social media data. We created a sublist to share for this course to help learners find the software that might be most appropriate for their own project, especially given the variety of social media platforms available. To render the sub-list in a more friendly way, we used the free version of airtable, which is a no-code app for relational databases with a colorful and modern interface. Students would navigate to this page (Fig 3) to see the sub-list on a single table. They can then find the right tool for their social media project by selecting the platform they want to collect their data from (twitter, instagram, facebook etc), whether they are happy to pay or looking for something that’s free, and the type of task they want to perform: whether they need the tool for collecting the data, analysis, or visualization. Once they have a filtered list, they can also look through the academic papers we’ve linked where each tool has been used, to explore further the potential of the tools.\nFig. 3. Screenshot of the sub-list containing social media tools via the free version of airtable. Similar to working with a csv file (as in Fig. 2), this interface lets the student filter the list down to narrow the choices for a tool they could use to either collect or analyse their data. This interface is web-based, and has a more inviting user experience than working with a csv file. A student can easily see the categories of tools, filter by multiple terms or concepts linked within each of the columns.\nWe envision this sub-list of social media tools to be a starting point, as it helps the learner filter down based on a limited number of criteria, such as: the task that can be achieved (collection, analysis), the social media platform that’s integrated, and the fees.\nWe’ve reused the same sub-list of social media tools with a different interface (pory.io, currently in beta) to render this list of tools more akin to a catalogue of records, that the student can search and filter. This rendering was used in a bootcamp on starting off with social media research. Similar to the airtable rendering, a student could filter based on the task they want to achieve and then click into the tool to get more information and explore which one would work better.\n","date":1609771480,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1613069434,"objectID":"28a47f3aab6c8645557b756f4d9e228b","permalink":"https://forrt.org/educators-corner/003-developing-tools/","publishdate":"2021-01-04T10:44:40-04:00","relpermalink":"/educators-corner/003-developing-tools/","section":"educators-corner","summary":"There are more than 500 different types of software and packages that social scientists can use to collect, visualize and work with different types of data. We’ve developed a list that is openly accessible and can be used by lecturers and their students to explore and filter for types of tools, by their launch data, charge, technique and other criteria.","tags":[],"title":"Developing a comprehensive directory of tools and technologies for social science research methods","type":"educators-corner"},{"authors":[""],"categories":[],"content":"Transparency Checklist Guideline for Education What is it for? As an educational tool, the Checklist can be used to teach and improve the standards of transparency and credibility in research reports made by students. The aim is that students are embedded in transparent and open practices from the beginning of their training.\nSee here for an adapted version of the original checklist for educational purposes.\nHow to use it? Supervisors and instructors can require students to create a Transparency Report along with any empirical research assignment.\nSteps to follow Inform the students about the Transparency Checklist. Add the completion of the checklist to the requirements. Make the checklist app available to students Be ready to answer questions if the students are uncertain about a checklist item. Read the Transparency Report along with the manuscript. Optional steps Explain each item why it is important for transparency. In the classroom, ask the students to evaluate some (weakly and strongly transparent) published research papers. This can help them get familiar with the checklist and realize the importance of transparent reporting. How to evaluate the Transparency Report In the requirements, indicate which items should have a “yes” response. Should you want to make it easier for the students, you can require only the Short (12-item) Checklist: http://www.shinyapps.org/apps/ShortTransparencyChecklist/ When possible, check whether the manuscript is in line with the checklist answers. Reference The following paper introduces the Transparency Checklist and describes how an initial set of items was iteratively evaluated by 45 journal editors, as well as 18 open-science advocates until a consensus was reached about the content and form of the checklist.\nAczel, B., Szaszi, B., Sarafoglou, A. et al. A consensus-based transparency checklist. Nat Hum Behav 4, 4–6 (2020). https://www.nature.com/articles/s41562-019-0772-6\n","date":1601822680,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1602363467,"objectID":"123d64f2d36db5840a89bb5cb4b3e50e","permalink":"https://forrt.org/educators-corner/002-transparencychecklist-balazsaczel/","publishdate":"2020-10-04T10:44:40-04:00","relpermalink":"/educators-corner/002-transparencychecklist-balazsaczel/","section":"educators-corner","summary":"Balazs Aczel adapted the celebrated Transparency Checklist Guideline to a classroom setting, allowing students (and educators) to improve the standards of transparency and credibility in student assignments and student research reports.","tags":[],"title":"Using the 'Transparency Checklist Guidelines' as an Educational tool","type":"educators-corner"},{"authors":[""],"categories":[],"content":"I want to tell y’all about one of my most meaningful teaching experiences. Pull up a chair.\nI’ve been teaching experimental psychology since 2006. It’s the flagship research methods course for undergraduate psychology majors at my institution. We now enroll 120 students each semester and, with the amazing help of 5 TAs, we offer lectures twice per week and a 2.5-hour lab once per week.\nAs many of you know, the shit hit the fan when it comes to psychology research during my time teaching this class. I felt pretty dizzy for a while during and after 2011, the year that marks scales falling from my eyes.\nI didn’t really make many substantial changes to my course right away. I needed to feel like I was on firmer ground first. (I expressed that sentiment in this tweet thread below.)\nBut then, I came to realize that if I didn’t make changes to my teaching in addition to changes to my research, I was complicit. So, I got to work. Standing on the shoulders of the smart people who brought issues to light and modeled pathways to improvement, I shifted my content.\nBuilding on traditional topics like internal, external, \u0026amp; construct validity, reliability, measurement, hypothesis testing, etc., I started teaching about transparency, openness, questionable research practices, and effect sizes.\nWe started to do IRB-approved, preregistered replication research some of which has seen or will see the light of day in published form ( like the example in this tweet).\nWe also started discussing efforts like the ManyLabs studies, the Psychological Science Accelerator ( @PsySciAcc), \u0026amp; the Collaborative Replications and Education Project ( @CREP_psych).\nThis has all reinvigorated my love of teaching in general and this class specifically. I honestly feel like I’m fulfilling a real need. We need people in our society to have the tools to consume research responsibly, whether they go on to produce research themselves or not.\nBut then 2020 happened. After centuries of oppression of Black, Indigenous, People of Color (BIPOC) in the United States, and seeing the disproportionate impact that the coronavirus is having on BIPOC folks, it finally clicked.\nMy students need more from me than validity and reliability and transparency and openness. They need to feel they belong. All of them. Every one of them needs to see themselves in psychological science if that’s how they want to spend their time.\nSo, this semester I introduced a single class session focused on the importance of representation in science. I borrowed ideas from Jessica Remedios ( see her terrific Twitter thread, which ends with her slides).\nI had them read this work about the diversity-innovation paradox by Hofstra and colleagues.\nI also invited them to watch the Picture a Scientist film and/or listen to the Everything Hertz podcast ( @hertzpodcast) about Diversity in science with Jess Wade.\nI talked about psychology’s positivist tradition of believing there’s some sort of objective reality that we can discover if we cleave to principles like empiricism, transparency, \u0026amp; falsifiability and how that position is marred by biases (e.g., confirmation bias, availability).\nIt’s also marred by the fact that scientists are humans with standpoints that affect the questions they ask. And that power structures dominated by men and white people guide what we think is “normal” science. Such ideas aren’t commonly discussed in quantitative research circles.\nI showed them clips from Picture a Scientist so they could appreciate what it’s like to be a woman in science, and the progress that’s been made to remediate gender bias (e.g., the MIT report from 1999).\nWe also talked about the idea that some seem to have been left out, women of color, in particular, but also disabled, LGBTQ, first generation, indigenous people, and all their intersections.\nWe talked about the loss to science, especially when innovative ideas put forward by women, people of color, and women of color are devalued (Hofstra et al., 2020).\nI showed them powerful clips of one woman of color’s experience as a scientist, professor of chemistry, Raychelle Burks ( @DrRubidium). Dr. Burks captured so perfectly the importance of representation.\nI talked about how these problems are not limited to male-dominated sciences but that these issues pervade psychological science too.\nI showed them data from APA demonstrating the move from roughly 78% undergrad psychology majors being female to less than 50% of full professors being female. Lots of factors at play, including gender bias.\nI shared with them an email that a female colleague of color received recently that illustrated the gendered racism she faces in our field. It referenced the hardships faced by white men. (Sorry, what now?)\nI talked about my own standpoint as an educated white woman, a full professor with lots of mentorship from family members and powerful colleagues through the years. I had a lot of help getting where I am in part because of intersecting identities tied to systems of power.\nI expressed my hope that they’ll think about their own intersecting identities and if science is something they love and psychology specifically they should go for it and make room for everybody’s voices.\nI’d never talked about these ideas in my class before this week. I have also never received so much positive feedback from students. They felt seen. I don’t think I’d ever made them feel seen before, not quite like that. I feel good about that.\nI also feel sad. Students feel these things so hard, especially those we’ve marginalized in so many ways. I’ve been in undergrad classrooms for 15 years. I’ve worked with literally hundreds of students. So many missed opportunities.\nI can’t change that. But I can keep it up in future semesters. Urry is ON.\nMeanwhile, here are some of my slides minus film clips and the email. Maybe they’ll help you or someone you know address representation as a foundational idea of science at one of the earliest career stages.\nEditor\u0026rsquo;s note: The present text is an adapted version of widely shared Twitter thread which resonated with so many of us. We thought it is of general interest and deserved to be immortalized, and hence we approached Heather to adapt the thread to post it here. Importantly, in addition to writing extremely current and relevant threads, Heather has also inspired the creation of FORRT. Indeed, FORRT was initiated at the 2018 meeting of the Society for the Improvement of Psychological Science (SIPS) in Heather\u0026rsquo;s “Teaching replicable and reproducible science” hackathon with Kristen Lane.\n","date":1601736280,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1602665064,"objectID":"59aac314bc73af3c939c1e88631cd6c0","permalink":"https://forrt.org/educators-corner/001-representation-heatherurry/","publishdate":"2020-10-03T10:44:40-04:00","relpermalink":"/educators-corner/001-representation-heatherurry/","section":"educators-corner","summary":"Heather Urry discusses the pressing issue of representation in her Experimental Psychology Course and how addressing it became one of the most meaningful teaching experiences of her career.","tags":[],"title":"Addressing the issue of representation in an undergraduate psych class","type":"educators-corner"},{"authors":[""],"categories":[],"content":"\nWelcome to FORRT\u0026rsquo;s first Pedagogy. And what better way to kickstart this initiative than hosting a researcher and educator who has been widely recognized by her peers for her outstanding course and materials on theory of Open and Reproducible Science called Psychology’s Credibility Revolution?\nFORRT is delighted to partner up with Julia Strand for its first Pedagogy!\nCheck out her course announcement (and teaser!)\nJulia Strand is Associate Professor of Psychology at Carleton College and she teaches courses such as Introduction to Psychology, The Psychology of Spoken Words and Sensation and Perception. Julia has also recently prepared and taught a course on Psychology’s Credibility Revolution ( link) which excels in all aspects (e.g., content, creativity, visuals, \u0026amp; functionality).\nFORRT has reached out to the academic community on social media to ask which questions other scholars would like Julia to answer about her teaching materials on the Psychology’s Credibility Revolution course (on both technical and non-technical spheres). Below you can find Julia’s answers to these questions. And you can also find in this OSF repository Julia’s complete teaching materials, from which everyone can learn, adapt, or repurpose.\nKudos to Julia for preparing such an amazing course and many thanks to her for being so open to partner up with FORRT to share her pedagogies with the wider community. We hope this can serve as an inspiration and help scholars, instructors and educational institutions interested in integrating open and reproducible scholarship tenets in their teaching and mentoring. What is your teaching philosophy behind the course “Psychology’s Credibility Revolution”? Julia: I’m trying to do two things with this class. The first is to familiarize students with concepts related to the replication crisis and subsequent credibility revolution. The second is to prepare my students to write their senior honors theses. At Carleton, all students complete a capstone project for their major (“comps”). In the Psychology department, the foundation of that comps is the term paper written in a seminar students take in the fall of their senior year. My Credibility Revolution course is one of those seminars. So in this course, I’m balancing teaching content with helping provide students with the tools they need to write their comps. The comps papers from my section are on a research topic of the student’s choosing, but written through the lens of something we’ve discussed. I’ve had students conduct systematic reviews, trace how findings have replicated (or not) on a particular topic, and evaluate the construct validity of measurement scales, among many others.\nI want students to come away from this course with an appreciation for how research is actually conducted, how the incentive structure of academia can be at odds with scientific rigor, and how we can change our field to encourage science that is rigorous, transparent, and reproducible. I’ve found that teaching open science is a great way to help students be more critical consumers of the literature. I know that many of my students won’t go on to do research in Psychology themselves, but I want them all to have an appreciation for how to evaluate research.\nCould you share your thoughts on how well the students grappled with the material and assignment in this course? Do you have an impression of whether this course influenced your students’ approaches to their own research projects? Julia: Students can really get into this content! I’ve found that talking about open science has a relatively low barrier to entry, even for undergrads. Although it may take time to build up enough subject area expertise to evaluate psychological theories or models, students can readily understand why misplaced incentives will affect behavior, why questionable research practices lead to findings that can’t replicate, and so on. For example, I’ll describe two studies that are intended to test the same question (i.e., studies on age-related priming conducted by John Bargh vs Stéphane Doyen) but differ in methodology. Students then brainstorm all the reasons why the two studies may have reached different conclusions. They are incredibly thoughtful and thorough in identifying all the ways the choices that researchers make can influence the outcome of the study.\nI ask my students for their impressions of how the course has changed how they engage with the literature, and they almost uniformly say they read more critically. They also regularly say (to my great delight!) that they pay much more attention to the methods sections.\nDid you face any barriers to teaching about the credibility revolution? And if so, how did you overcome these? Julia: I haven’t, really! My department is very supportive of letting faculty choose their own topics in these seminars and has been quite receptive to my incorporating open science principles in my teaching and research.\nHow many times have you taught this course? Could you give an indication of how much the course has developed over time? Julia: I’ve taught it twice and will teach it again next year. The course has changed somewhat as I’ve looked for the right balance between teaching content and preparing my students for working on their senior theses. It also changes a bit each time because open science moves fast, so I need to update the content! But the basic bones of the course have been fairly consistent.\nIs there anything that you didn\u0026rsquo;t have time to cover in this course but wish you did? Julia: So many things. Given that Carleton is on trimesters and our terms are a short 9.5 weeks, and we also spend a substantial portion of that doing things related to comps, there are many topics I’d love to be able to include: the alpha wars, philosophy of science, computational and statistical reproducibility, meta-analyses, Bayesian vs frequentist approaches, and more. I ended up cutting content this year to take off pressure around the U.S election, as well.\nWhat aspects of the course material do you think are the most important? For example if you only had 6 weeks to deliver this course. Julia: It can be very easy for courses of this nature to be very pessimistic. The last thing I want is for students to come away with the sense “science is broken and everything is doomed.” So I spend a lot of time and energy helping them to understand all the ways our discipline, and science generally, has figured out to improve the practice of how we do science! I named this course Psychology\u0026rsquo;s Credibility Revolution rather than Psychology’s Replication Crisis because I really wanted to emphasize that the reforms of the last decade are changing how we do science, for the better.\nDo you have a favorite part of the course? Do you think your students have a favorite too? Julia: I love thinking about and teaching measurement and construct validation. I also love Halloween, and it works very nicely that my lessons on those topics tend to be around Halloween time. I created a fake scale that is intended to measure a construct called “Halloweenophilia” (an overwhelming love of all things halloween related) and ask students how they’d assess the construct validation of the instrument. It’s both spooky and fun.\nDid you develop your materials from other teaching materials you found? And, what are your thoughts on adapting other’s teaching materials and making these available? Julia: One reason I was keen to make my course publicly available is that others who teach similar courses have been so generous in sharing their materials! When I started designing this course, I combed through all the syllabi I could find online and made lots of notes about strategies I wanted to emulate and readings to adopt. So I’m happy to facilitate others doing the same.\nA thing about teaching I wish I’d known earlier is that teaching doesn’t have to include a high proportion of invention. I adopt ideas from other teachers all the time. In fact, even if we don’t intend to, all of our teaching philosophies and approaches must be shaped by the courses we’ve taken and strategies we’ve seen others use.\nInstructors need to curate the available resources/readings/assignments/approaches to achieve their course goals, fit their student population, etc, but they don’t need to design every single thing from scratch. In fact, adopting some materials from others frees you up to put more thought and energy into the aspects of your class that you create from scratch.\nDo you have any general advice for a new(ish) lecturer wanting to organise a course similar to this? Julia: Don’t be afraid to take inspiration from and borrow from others (see answer to question 8 above)!\nIn addition to the materials themselves (which are awesome), what should other teachers know or bear in mind when teaching this/similar material? Julia: One challenge I faced is that much of this content is very interconnected—it’s hard to talk about low rates of replication without discussing publication bias, which is hard to talk about without mentioning null hypothesis significance testing, which then gets you into talking about alternatives to p-values….Which is all great, but it can be hard to wait weeks to talk about some topic that seems integral to understanding earlier content. So I think teaching this content requires coming to terms with the fact that you can’t do everything, and can’t do everything at once.\nI’d also recommend using concrete examples from content areas you know well. My research is on spoken word recognition and listening effort (the cognitive resources necessary to understand speech), so when I teach content on measurement for example, I talk a lot about different ways of measuring listening effort. Working through examples that you are very familiar with can help students get an appreciation of the complexities of decisions about experiment design.\nA few final, quick-fire technical questions: how did you record the lectures? How did you edit the lectures (e.g. to have your face and slides on screen)? How do you make the cartoon character of yourself? See Julia’s response to all these technical questions in this great video she prepared!\nAll links\nCourse Syllabus OSF Component in Pedagogies with all Julia\u0026rsquo;s course materials and assignments YouTube video on technical details of her course Teaser for Julia’s Psychology course on “Psychology\u0026rsquo;s credibility revolution The .pdf version of this page, with Julia\u0026rsquo;s answers from the community\u0026rsquo;s question Suggested Citation\nStrand, J. (2020) Open and Reproducible Science walks into a classroom: Insights from teaching a course on Psychology’s Credibility Revolution. FORRT Pedagogies. https://doi.org/10.17605/OSF.IO/ZPB8A\nTeam-Pedagogy Contributors:\nLeticia Micheli, Leiden University, the Netherlands.\nFlavio Azevedo, Cambridge University, UK.\n","date":1601736280,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1669758376,"objectID":"2ad75637f297ce4a607a1f69bc863069","permalink":"https://forrt.org/pedagogies/001-julia-strand/","publishdate":"2020-10-03T10:44:40-04:00","relpermalink":"/pedagogies/001-julia-strand/","section":"pedagogies","summary":"Julia Strand shares her *know-how*, didactics, and teaching materials for her course on Psychology’s Credibility Revolution.","tags":[],"title":"Open and Reproducible Science walks into a classroom","type":"pedagogies"},{"authors":["FORRT","",""],"categories":null,"content":"The recording: The presentation: ","date":1599573600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1625928999,"objectID":"a71a0393cb6bf7649151a90a5b6c954c","permalink":"https://forrt.org/talk/ukrn-orwg-2020/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/talk/ukrn-orwg-2020/","section":"talk","summary":"FORRT was present at the Open Research Working Group (ORWG) virtual meeting.","tags":[],"title":"FORRT presentation @ ORWG2020","type":"talk"},{"authors":["FORRT","",""],"categories":null,"content":"The playlist of recordings: The presentation: ","date":1590069600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1625923631,"objectID":"8c92e557b8e927da98819b6d64f00447","permalink":"https://forrt.org/talk/ica-2020/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/talk/ica-2020/","section":"talk","summary":"The International Communication Association aims to advance the scholarly study of human communication by encouraging and facilitating excellence in academic research worldwide.","tags":[],"title":"FORRT's presentation \u0026 Workshop @ ICA 2020.","type":"talk"},{"authors":["FORRT",""],"categories":null,"content":"The Frankfurt Open Science Initiative organized an Open Science Day in Frankfurt at Goethe University whose theme revolved around Open Science and Teaching.\nFORRT was featured as one of the two main talks, and below you find the recording of FORRT’s presentation.\nFor an overview of the whole event, see this Twitter thead.\nExactly two weeks ago, we had our Open Science Day \u0026amp; talked about #OpenScience in Teaching. We are still super impressed/inspired/amazed/... 😍 The whole event was a full success! 🎉 And we are really happy that we recorded the talks \u0026amp; can now share them! Read more below 👇(1/5) pic.twitter.com/dsplwfr5BH\n\u0026mdash; Frankfurt Open Science Initiative (@OpenScienceFFM) January 29, 2020 ","date":1579096800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1711896344,"objectID":"d167a2c71e8c4d8ff931a7004b916de5","permalink":"https://forrt.org/talk/os-day-goethe/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/talk/os-day-goethe/","section":"talk","summary":"The Frankfurt Open Science Initiative organized an Open Science Day in Frankfurt at Goethe University whose theme revolved around Open Science and Teaching. FORRT was featured as one of the two main talks, and it was recorded.","tags":[],"title":"FORRT presentation @ Open Science Day at Goethe University","type":"talk"},{"authors":["FORRT",""],"categories":null,"content":"Watch the recording: Look at the presentation: ","date":1548446018,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1625923631,"objectID":"7624abe32e97741bbaddf417bcf68d8f","permalink":"https://forrt.org/talk/mzes/","publishdate":"2020-02-23T14:53:38-05:00","relpermalink":"/talk/mzes/","section":"talk","summary":"At its very early stage, FORRT was featured at MZES Open Social Science Conference which aimed at practicing new standards in transparency and reproducibility.","tags":[],"title":"FORRT Presentation @ MZES Open Social Science Conference","type":"talk"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1589633744,"objectID":"ace3349e664b637c64adc42cacf30d0e","permalink":"https://forrt.org/about/us/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/about/us/","section":"about","summary":"","tags":null,"title":"","type":"widget_page"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1646742478,"objectID":"661f496706bbab28bcc7636047a22ac2","permalink":"https://forrt.org/about/ambassadorship/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/about/ambassadorship/","section":"about","summary":"","tags":null,"title":"","type":"widget_page"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1643222597,"objectID":"cc21c693f06dffa419b324affec18b7a","permalink":"https://forrt.org/about/community/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/about/community/","section":"about","summary":"","tags":null,"title":"","type":"widget_page"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1585711304,"objectID":"a0f8b81b22ffddb486facec15c254d7a","permalink":"https://forrt.org/about/get-involved/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/about/get-involved/","section":"about","summary":"","tags":null,"title":"","type":"widget_page"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1589594819,"objectID":"0fb4bbae7dcaffb7c83749ea7a753c31","permalink":"https://forrt.org/about/mission/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/about/mission/","section":"about","summary":"","tags":null,"title":"","type":"widget_page"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1643222597,"objectID":"a3a7d640bac23a65916db39f8d8ef662","permalink":"https://forrt.org/about/partnerships/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/about/partnerships/","section":"about","summary":"","tags":null,"title":"","type":"widget_page"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1626972890,"objectID":"0ea4a98fa34f6a90d112841b31bd6d12","permalink":"https://forrt.org/about/principles/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/about/principles/","section":"about","summary":"","tags":null,"title":"","type":"widget_page"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1643222597,"objectID":"99fc5b940df57d148eda05ee34e30d00","permalink":"https://forrt.org/about/teams/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/about/teams/","section":"about","summary":"","tags":null,"title":"","type":"widget_page"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1636465542,"objectID":"10be3f993ccde92b52bbc50737c99b4f","permalink":"https://forrt.org/adopting/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/adopting/","section":"","summary":"","tags":null,"title":"","type":"widget_page"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1630771682,"objectID":"e125230e6a13b086f80d08ff54bb980c","permalink":"https://forrt.org/awards/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/awards/","section":"","summary":"","tags":null,"title":"","type":"widget_page"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1716042570,"objectID":"05d9788ae04118335c09def69ce70ba2","permalink":"https://forrt.org/awop/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/awop/","section":"","summary":"","tags":null,"title":"","type":"widget_page"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1600660144,"objectID":"c9fc0427a1088386dd38feb12102f051","permalink":"https://forrt.org/clusters/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/clusters/","section":"","summary":"","tags":null,"title":"","type":"widget_page"},{"authors":null,"categories":null,"content":"Code of Conduct As a community we welcome everyone, and encourage a friendly and positive environment.\nThis code of conduct outlines our expectations for participants, members, contributors, and leaders within the community, as well as steps to reporting unacceptable behaviour. We are committed to providing a welcoming and inspiring community for all and expect our code of conduct to be honored. Anyone who violates this code of conduct may be banned from the community.\nOur Pledge We pledge to make participation in our community a harassment-free experience for everyone, regardless of age, body size, visible or invisible disability, ethnicity, sex characteristics, gender identity and expression, level of experience, education, social and economic status, nationality, personal appearance, race, religion, neurodiversity, or sexual identity and orientation.\nWe pledge to act and interact in ways that contribute to an open, welcoming, diverse, inclusive, and healthy community.\nWe pledge that all contributions to FORRT projects are formally credited (for more information see FORRT’s General Collaboration \u0026amp; Writing guidelines).\nOur open community strives to:\nBe friendly and patient.\nBe welcoming: We strive to be a community that welcomes and supports people of all backgrounds and identities. This includes, but is not limited to members of any race, ethnicity, culture, national origin, colour, immigration status, social and economic class, educational level, sex, sexual orientation, gender identity and expression, age, size, family status, abledness, religion, and mental and physical ability. We are welcoming of different communications, thinking styles and speeds (e.g. linear and non-linear communication, free thinking and written communication).\nBe considerate: Your work may be used by other people, and you, in turn, may depend on the work of others. Any decision you take will affect users and colleagues, and you should take those consequences into account when making decisions. One should focus on what is best not just for them as individuals, but for the overall community. Remember that we’re an international community, so you might not be communicating in someone else’s primary language.\nUse preferred pronouns (e.g. she/her/hers, they/their/theirs, he/him/his).\nBe respectful of differing opinions, viewpoints, and experiences: Not all of us will agree all the time, but disagreement is no excuse for poor behaviour and poor manners. We might all experience some frustration now and then, but we cannot allow that frustration to turn into a personal attack. It’s important to remember that a community where people feel uncomfortable or threatened is not a productive one. It is important to demonstrate empathy and kindness toward other people. Please keep in mind that viewpoints and opinions must not interfere with, hinder, or restrain our strongly-held community values on social justice, diversity, equity, inclusion and accessibilty.\nTry to understand why we disagree: Disagreements, both social and technical, happen all the time. It is important that we resolve disagreements and differing views constructively by giving and gracefully accepting constructive feedback. Remember that we’re different. Diversity contributes to the strength of our community, which is composed of people from a wide range of backgrounds. Different people have different perspectives on issues. Being unable to understand why someone holds a viewpoint doesn’t mean that they’re wrong. Don’t forget that it is human to err and blaming each other doesn’t get us anywhere. Instead, focus on helping to resolve issues and learning from mistakes.\nOpen communication and mutual respect: Be generous about information and definition requests. Welcome neurodivergent people to make themselves comfortable – feel invited to tic, self-stimulate/stim, fidget, move around etc.\nBe careful in the words that we choose: We are a community of professionals, and we conduct ourselves professionally. Be kind to others. Do not insult or put down other participants. Public or private harassment and other exclusionary behaviour aren’t acceptable. This includes, but is not limited to:\nUnacceptable Behaviors: Intimidating, harassing, lewd, demeaning, bullying, stalking, or threatening speech or actions. Unwelcome sexual attention. Unwelcome physical contact. Any real or implied threat of physical harm. Sustained disruption of speakers or events (verbally or physically) Violent threats or language directed against another person. Trolling, insulting or derogatory comments, and personal attacks. Discriminatory jokes and language. The use of ableist, sexualized or violent language or imagery, and sexual attention or advances of any kind. Posting (or threatening to post) other people’s personally identifying information (“doxing”). Personal insults, especially those using ableist, racist, xenophobic or sexist terms. Repeated harassment of others. In general, if someone asks you to stop, then stop. Retaliation against an individual for reporting harassment or other unacceptable behaviors or for participating in an investigation of such a claim. Pass judgement about personal choices or force disclosure of one\u0026rsquo;s own experiences and identifications. Other conduct which could reasonably be considered inappropriate in a professional setting. Advocating for, or encouraging, any of the above behaviour. Code of Practice, Research Integrity \u0026amp; Research Misconduct\nFirst and foremost, FORRT expects all of its members to observe the highest standards of ethics and integrity in the conduct of their collaborations in FORRT projects and in FORRT premises (e.g., Slack channel, social media, conferences, events, etc.). In pursuance of such high standards FORRT members must:\nbe honest in proposing, conducting and reporting research. They should strive to ensure the accuracy of research data and results and acknowledge the contributions of others. acquaint themselves with guidance as to best research and collaboration practice and standards of integrity For example, The European Code of Conduct for Research Integrity, World Health Organization’s Code of Conduct for responsible research as well as WHO’s Misconduct in Research, Code of Practice for Research published by the UK Research Integrity Office, and Concordat to Support Research Integrity. comply with ethical and legal obligations as required by statutory and regulatory authorities, including seeking ethical review and approval for research as appropriate. seek to ensure the safety, dignity, wellbeing and rights of those associated with the research effectively and transparently manage any conflicts of interest, whether actual or potential, reporting these to the appropriate authority as necessary ensure that they have the necessary skills and training for their field of research recognise their accountability to FORRT and their peers for the conduct of their research having due regard to subject disciplinary norms, acknowledge that authorship of a research output should be attributed only to a researcher who has made a significant intellectual, scholarly, or practical contribution to that output and is willing to take responsibility for the contribution. Secondly, FORRT defines misconduct in research as including acts of omission as well as acts of commission. It excludes genuine errors that are not due to negligence, differences in interpretation or judgment in evaluating research methods or results, or misconduct unrelated to research processes. It does not include poor research. Misconduct in research and collaboration in FORRT means—but is not limited to—the doing, planning or attempting of any of the following while proposing, carrying out or reporting the results of research:\nfalsification or fabrication of data, including the intentionally misleading or deliberately false reporting of research information misrepresentation of data, including the invention of data and the omission from analysis and publication of inconvenient data failure to follow good practice for the proper preservation, management and sharing of primary data, artifacts and material unacknowledged appropriation of the work of others, including plagiarism, the abuse of confidentiality with respect to unpublished materials, or misappropriation of results, physical materials or other resources misrepresentation of involvement in a research project; for example, the failure to include legitimate author(s) on outputs, or granting authorship where none is warranted, or of credentials, including qualifications, experience, and publication history failure to declare conflicts of interest failure to follow accepted procedures, legal, professional or ethical requirements, or to exercise due care in carrying out responsibilities for avoiding unreasonable harm or risk to humans, other vertebrates, cephalopods or the environment failure to follow existing guidance on good practice in research, including proper handling of privileged, private or confidential information collected on individuals during the research improper conduct in peer review of research proposals, results or manuscripts submitted for publication improper dealing with allegations of misconduct: failing to address possible infringements, or to adhere to agreed procedures in the investigation of alleged research misconduct accepted as a condition of funding Failure to comply with this Code of Practice may give rise to an allegation of Code of Conduct Violation which may be a ground for disciplinary action, and if serious, for dismissal or expulsion from our FORRT community and projects.\nDiversity Statement We encourage everyone to participate and are committed to building a community for all. Although we will fail at times, we seek to treat everyone both as fairly and equally as possible. Whenever a participant has made a mistake, we expect them to take responsibility for it. If someone has been harmed or offended, it is important to apologize to those affected by our mistakes. By listening carefully and respectfully, and doing our best to right the wrong, one is expected to learn from the experience.\nAlthough this list cannot be exhaustive, we explicitly honor diversity in age, gender, gender identity or expression, culture, ethnicity, language, neurodiversity, national origin, profession, race, religion, sexual orientation, socioeconomic status, and technical ability. We will not tolerate discrimination based on any of the protected characteristics above, including participants with disabilities.\nWe also aim to foster a welcoming environment to several approaches to acquisition and accumulation of knowledge (i.e., epistemological pluralism) as a way to enrich perspectives raising awareness, addressing challenges, and implications of Open Scholarship to curricular reform, epistemological uncertainty, and methods of education.\nEnforcement Responsibilities Community leaders are responsible for clarifying and enforcing our standards of acceptable behaviour and will take appropriate and fair corrective action in response to any behaviour that they deem inappropriate, threatening, offensive, or harmful\nScope This Code of Conduct applies within all community spaces, physical and virtual, and also applies when an individual is officially representing the community in public spaces. Examples of representing our community include using an official email address, posting via an official social media account, or acting as an appointed representative at an online or offline event.\nConsequences of Unacceptable Behaviors\nParticipants who are asked to stop any unacceptable behavior are expected to comply immediately. Potential consequences for violations of this code of conduct include, but are not limited to: warning the offender, expulsion from the Slack Community and FORRT events, banning from future FORRT events, and denying or revoking FORRT membership. Reporting\nIf you experience or witness unacceptable behaviour, or have any other concerns, please report it as indicated below. All complaints will be reviewed and investigated promptly and fairly. All community leaders are obligated to respect the privacy and security of the reporter of any incident. All reports will be handled with discretion. How to make a report\nYou can report a suspected code of conduct violation to any member of FORRT\u0026rsquo;s Code of Conduct Committee: Kimberly Quinn; Sam Parsons; or Leticia Micheli. If possible please report the following information Your contact information: Names (real, nicknames, or pseudonyms) of any individuals involved. If there are additional witnesses, please include them as well. Your account of what occurred, and if you believe the incident is ongoing. If there is a publicly available record (e.g. a mailing list archive), please include a link. Any additional information that may be helpful. You may also make a report anonymously through our online form here: https://forms.gle/YQu3FcDTaeLwxRbB6 All reports will be passed along to the Code of Conduct Committee, who will decide on appropriate action. After filing a report, a representative (of your choice if indicated \u0026amp; from the Code of Conduct Committee) will contact you personally, review the incident, follow up with any additional questions, and make a decision as to how to respond. If the person who is harassing you is part of the response team, they will recuse themselves from handling your incident. If the complaint originates from a member of the response team, it will be handled by a different member of the response team. We will respect confidentiality requests for the purpose of protecting victims of abuse. Recusal: Code of Conduct Committee members and any others responsible for enacting or overseeing this policy will recuse themselves if they have a significant conflict of interest, such as being a specific target of harassment, an alleged harasser, or having a close personal or professional relationship with a target or alleged harasser. Attribution \u0026amp; Acknowledgements This Code of Conduct was adapted from Contributor Covenant Code of Conducts (primarily), SIPS CoC, the Open Science UMontreal, and Open Life Science. As well as from Aurora, V., \u0026amp; Gardiner, M. (2019). How to Respond to Code of Conduct Reports.\n","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1689063073,"objectID":"865b6f190b424cd21f9c55ca2ff9b9dd","permalink":"https://forrt.org/coc/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/coc/","section":"","summary":"Code of Conduct As a community we welcome everyone, and encourage a friendly and positive environment.\nThis code of conduct outlines our expectations for participants, members, contributors, and leaders within the community, as well as steps to reporting unacceptable behaviour. We are committed to providing a welcoming and inspiring community for all and expect our code of conduct to be honored.","tags":null,"title":"","type":"page"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1708616806,"objectID":"2dfd440454216e51db1490c3e8487f0b","permalink":"https://forrt.org/contributors/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/contributors/","section":"","summary":"","tags":null,"title":"","type":"widget_page"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1646165560,"objectID":"fd36605688ef45e10dc233c860158012","permalink":"https://forrt.org/cv/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/cv/","section":"","summary":"","tags":null,"title":"","type":"widget_page"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1601933290,"objectID":"b057b00d305c7b68e14fb7a0f19cb5ae","permalink":"https://forrt.org/dei/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/dei/","section":"","summary":"","tags":null,"title":"","type":"widget_page"},{"authors":null,"categories":null,"content":" ","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1670603157,"objectID":"8e88567008400159f6c4754078c0b1d3","permalink":"https://forrt.org/equityinos/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/equityinos/","section":"","summary":" ","tags":null,"title":"","type":"widget_page"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1646742478,"objectID":"8f08a88be19375294cd68b13f54bf7e1","permalink":"https://forrt.org/feedback/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/feedback/","section":"","summary":"","tags":null,"title":"","type":"widget_page"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1636472052,"objectID":"177cf6694690f7545eacfce587484f1a","permalink":"https://forrt.org/impact/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/impact/","section":"","summary":"","tags":null,"title":"","type":"widget_page"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1620683364,"objectID":"d30ad979d19821f890831444f892af7c","permalink":"https://forrt.org/wiki/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/wiki/","section":"","summary":"","tags":null,"title":"","type":"widget_page"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1629391487,"objectID":"726fae55f11444b342e3810ec674914e","permalink":"https://forrt.org/lesson-plans/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/lesson-plans/","section":"","summary":"","tags":null,"title":"","type":"widget_page"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1585324989,"objectID":"858964055de8084aaeb00eebaa3abadf","permalink":"https://forrt.org/manuscript/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/manuscript/","section":"","summary":"","tags":null,"title":"","type":"widget_page"},{"authors":null,"categories":null,"content":" ","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1728227192,"objectID":"cf4401b9ba5dd6723625c7d67b7153a6","permalink":"https://forrt.org/mapping_os/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/mapping_os/","section":"","summary":" ","tags":null,"title":"","type":"widget_page"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1675619649,"objectID":"a724fd3e166a26a593d462a645ad551c","permalink":"https://forrt.org/neurodiversity/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/neurodiversity/","section":"","summary":"","tags":null,"title":"","type":"widget_page"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1601933290,"objectID":"52ffbe850e5ab2f8f95bf84a562db6db","permalink":"https://forrt.org/nexus/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/nexus/","section":"","summary":"","tags":null,"title":"","type":"widget_page"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1716061109,"objectID":"89a37ab721af275d364619d98bea324d","permalink":"https://forrt.org/os-developing-world/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/os-developing-world/","section":"","summary":"","tags":null,"title":"","type":"widget_page"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1716069170,"objectID":"1e2b0218a881962abda9d920a930b645","permalink":"https://forrt.org/positive-changes-replication-crisis/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/positive-changes-replication-crisis/","section":"","summary":"","tags":null,"title":"","type":"widget_page"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1636465542,"objectID":"23d1e75f872528fc12f5f2b142375ff7","permalink":"https://forrt.org/publications/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/publications/","section":"","summary":"","tags":null,"title":"","type":"widget_page"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1589520194,"objectID":"c6625328e7b2e36b114847f299065f54","permalink":"https://forrt.org/resources/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/resources/","section":"","summary":"","tags":null,"title":"","type":"widget_page"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1635357806,"objectID":"a47b72c5175da1c4860fce74832391cb","permalink":"https://forrt.org/reversals/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/reversals/","section":"","summary":"","tags":null,"title":"","type":"widget_page"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1600660144,"objectID":"e49c97832f6f791760378bd8629f7e77","permalink":"https://forrt.org/self-assessment/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/self-assessment/","section":"","summary":"","tags":null,"title":"","type":"widget_page"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1682255110,"objectID":"0d09127d49a82b0e72d63da5772d010e","permalink":"https://forrt.org/syllabus/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/syllabus/","section":"","summary":"","tags":null,"title":"","type":"widget_page"},{"authors":null,"categories":null,"content":" ","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1728047592,"objectID":"ca49e6357fba3228fcac75f8e67ab1a0","permalink":"https://forrt.org/teaching_os/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/teaching_os/","section":"","summary":" ","tags":null,"title":"","type":"widget_page"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1689122843,"objectID":"de7f44354a8ef913b71acb742d79c411","permalink":"https://forrt.org/curated_resources/positive-results-increase-down-the-hiera/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/positive-results-increase-down-the-hiera/","section":"curated_resources","summary":"The hypothesis of a Hierarchy of the Sciences with physical sciences at the top, social sciences at the bottom, and biological sciences in-between is nearly 200 years old. This order is intuitive and reflected in many features of academic life, but whether it reflects the “hardness” of scientific research—i.e., the extent to which research questions and results are determined by data and theories as opposed to non-cognitive factors—is controversial. This study analysed 2434 papers published in all disciplines and that declared to have tested a hypothesis. It was determined how many papers reported a “positive” (full or partial) or “negative” support for the tested hypothesis. If the hierarchy hypothesis is correct, then researchers in “softer” sciences should have fewer constraints to their conscious and unconscious biases, and therefore report more positive outcomes. Results confirmed the predictions at all levels considered: discipline, domain and methodology broadly defined. Controlling for observed differences between pure and applied disciplines, and between papers testing one or several hypotheses, the odds of reporting a positive result were around 5 times higher among papers in the disciplines of Psychology and Psychiatry and Economics and Business compared to Space Science, 2.3 times higher in the domain of social sciences compared to the physical sciences, and 3.4 times higher in studies applying behavioural and social methodologies on people compared to physical and chemical studies on non-biological material. In all comparisons, biological studies had intermediate values. These results suggest that the nature of hypotheses tested and the logical and methodological rigour employed to test them vary systematically across disciplines and fields, depending on the complexity of the subject matter and possibly other factors (e.g., a field's level of historical and/or intellectual development). On the other hand, these results support the scientific status of the social sciences against claims that they are completely subjective, by showing that, when they adopt a scientific approach to discovery, they differ from the natural sciences only by a matter of degree.","tags":["Reproducibility Crisis and Credibility Revolution","Open Science"],"title":"‘‘Positive’’ Results Increase Down the Hierarchy of the Sciences","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"cd7f8319b20c32a9e4e90dd9c6fa5a4a","permalink":"https://forrt.org/curated_resources/divorcing-white-supremacy-culture/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/divorcing-white-supremacy-culture/","section":"curated_resources","summary":"This website is a long-time dream finally bearing fruit, a needed remix of the widely circulated article WHITE SUPREMACY CULTURE originally written and published in 1999.","tags":["White Supremacy","Antiracist"],"title":"(divorcing) WHITE SUPREMACY CULTURE","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1689122843,"objectID":"128cee2a0fe7c5fb32fc08e43abb8d92","permalink":"https://forrt.org/curated_resources/1-500-scientists-lift-the-lid-on-reprodu/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/1-500-scientists-lift-the-lid-on-reprodu/","section":"curated_resources","summary":"Survey sheds light on the ‘crisis’ rocking research.","tags":["Reproducibility Crisis and Credibility Revolution","Open Science"],"title":"1,500 scientists lift the lid on reproducibility","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"21c67576037bbacecc2d1aea373d9c6e","permalink":"https://forrt.org/curated_resources/10-things-for-curating-reproducible-and/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/10-things-for-curating-reproducible-and/","section":"curated_resources","summary":"10 Things for Curating Reproducible and FAIR Research (CURE-FAIR) provides standards-based guidelines to follow CURE-FAIR best practices with regards to publishing and archiving computationally reproducible studies, including the associated computational methods and materials. The 10 Things are modular (users can select which Thing is relevant to their needs as well as the level of guidance they prefer: 'Get started', 'Learn more', and 'Go deeper') and customizable. While focusing on social science research that relies on quantitative data, it can be adapted for broader use in life sciences and other long tail small scale research relying on data analysis. See also https://curating4reproducibility.org/10things/ ","tags":["Reproducibility","Curation","Open Data","Open Science"],"title":"10 Things for Curating Reproducible and FAIR Research","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"cdb69dc72f79c90c4244b3475a144b77","permalink":"https://forrt.org/curated_resources/1economics-270d-research-transparency-me/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/1economics-270d-research-transparency-me/","section":"curated_resources","summary":"This course coversa range of approachesthat aim to enhance the transparency and reproducibility of social science research. It is appropriate for Ph.D. students in social science disciplines and related fields.","tags":["Reproducibility Crisis and Credibility Revolution","Open Science"],"title":"1Economics 270D: Research Transparency Methods in the Social Sciences","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"428d9d2c265d13f9082c64ce7f656efb","permalink":"https://forrt.org/curated_resources/7-easy-steps-to-open-science-an-annotate/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/7-easy-steps-to-open-science-an-annotate/","section":"curated_resources","summary":"The Open Science movement is rapidly changing the scientific landscape. Because exact definitions are often lacking and reforms are constantly evolving, accessible guides to open science are needed. This paper provides an introduction to open science and related reforms in the form of an annotated reading list of seven peer-reviewed articles, following the format of Etz et al. (2018). Written for researchers and students - particularly in psychological science - it highlights and introduces seven topics: understanding open science; open access; open data, materials, and code; reproducible analyses; preregistration and registered reports; replication research; and teaching open science. For each topic, we provide a detailed summary of one particularly informative and actionable article and suggest several further resources. Supporting a broader understanding of open science issues, this overview should enable researchers to engage with, improve, and implement current open, transparent, reproducible, replicable, and cumulative scientific practices.","tags":["Data","Materials","Open Scholarship Guidelines","Policy","Publishing","Reproducibility","Researchers"],"title":"7 Easy Steps to Open Science: An Annotated Reading List","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1689122843,"objectID":"defe6b8567c55ffd2b1ec5843504685d","permalink":"https://forrt.org/curated_resources/a-21-word-solution/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/a-21-word-solution/","section":"curated_resources","summary":"One year after publishing \"False-Positive Psychology,\" we propose a simple implementation of disclosure that requires but 21 words to achieve full transparency. This article is written in a casual tone. It includes phone-taken pictures of milk-jars and references to ice-cream and sardines.","tags":["Reproducibility Crisis and Credibility Revolution","Open Science"],"title":"A 21 word solution.","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1689122843,"objectID":"c30c05c0ceec22ba36ca01b57b07943f","permalink":"https://forrt.org/curated_resources/a-bayesian-perspective-on-the-reproducib/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/a-bayesian-perspective-on-the-reproducib/","section":"curated_resources","summary":"We revisit the results of the recent Reproducibility Project: Psychology by the Open Science Collaboration. We compute Bayes factors—a quantity that can be used to express comparative evidence for an hypothesis but also for the null hypothesis—for a large subset (N = 72) of the original papers and their corresponding replication attempts. In our computation, we take into account the likely scenario that publication bias had distorted the originally published results. Overall, 75% of studies gave qualitatively similar results in terms of the amount of evidence provided. However, the evidence was often weak (i.e., Bayes factor \u003c 10). The majority of the studies (64%) did not provide strong evidence for either the null or the alternative hypothesis in either the original or the replication, and no replication attempts provided strong evidence in favor of the null. In all cases where the original paper provided strong evidence but the replication did not (15%), the sample size in the replication was smaller than the original. Where the replication provided strong evidence but the original did not (10%), the replication sample size was larger. We conclude that the apparent failure of the Reproducibility Project to replicate many target effects can be adequately explained by overestimation of effect sizes (or overestimation of evidence against the null hypothesis) due to small sample sizes and publication bias in the psychological literature. We further conclude that traditional sample sizes are insufficient and that a more widespread adoption of Bayesian methods is desirable.","tags":["Reproducibility Crisis and Credibility Revolution","Open Science"],"title":"A Bayesian Perspective on the Reproducibility Project: Psychology","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"7d7f09fced629c8d8d961cac64d7b4f4","permalink":"https://forrt.org/curated_resources/a-case-for-data-dashboards-first-steps-w/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/a-case-for-data-dashboards-first-steps-w/","section":"curated_resources","summary":"Dashboards for data visualisation, such as R Shiny and Tableau, allow an interactive exploration of data by means of drop-down lists and checkboxes, with no coding for the user. The apps can be useful for both the data analyst and the public.","tags":["Data","Librarians","Open Data","Open Source Software","Publishers","Researchers","Visualization"],"title":"A Case For Data Dashboards: First Steps with R Shiny","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"532db4f4b87f61896578be48deffd519","permalink":"https://forrt.org/curated_resources/a-cautionary-note-on-aggregation-in-educ/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/a-cautionary-note-on-aggregation-in-educ/","section":"curated_resources","summary":"This article addresses aggregation as a fundamental practice in educational psychology and ties it into the idiographic/nomothetic distinction, that is, distinguishing between studying what once was and studying what always is. I address the underlying assumptions of seminal educational research (OECD’s large-scales assessment and Hattie’s synthesizing meta-analyses). I argue that educational psychologists assume a priori general educational principles akin to nomothetic laws without sufficiently scrutinizing the limitations of aggregation. I then contextualize this assumption within the history of psychology, and address how these assumptions shape how educational psychologists view, collect, and examine data. Furthermore, I contextualize this assumption with an example showing a peculiarity of educational research: the existence of multiple perspectives on constructs. Finally, I argue that investing time and resources in the debate on aggregation and the epistemic nature of the insights that educational psychologists generate will ultimately advance the field and help bridge the theory–practice gap.","tags":["Aggregation","Best Practice","Educational Psychology","Idiographic","Nomothetic","Quantitative Psychology"],"title":"A cautionary note on aggregation in educational psychology and beyond","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"234c6cf493f7fa03c2408fb3a63682a5","permalink":"https://forrt.org/curated_resources/a-checklist-is-associated-with-increased/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/a-checklist-is-associated-with-increased/","section":"curated_resources","summary":"Irreproducibility of preclinical biomedical research has gained recent attention. It is suggested that requiring authors to complete a checklist at the time of manuscript submission would improve the quality and transparency of scientific reporting, and ultimately enhance reproducibility. Whether a checklist enhances quality and transparency in reporting preclinical animal studies, however, has not been empirically studied. Here we searched two highly cited life science journals, one that requires a checklist at submission (Nature) and one that does not (Cell), to identify in vivo animal studies. After screening 943 articles, a total of 80 articles were identified in 2013 (pre-checklist) and 2015 (post-checklist), and included for the detailed evaluation of reporting methodological and analytical information. We compared the quality of reporting preclinical animal studies between the two journals, accounting for differences between journals and changes over time in reporting. We find that reporting of randomization, blinding, and sample-size estimation significantly improved when comparing Nature to Cell from 2013 to 2015, likely due to implementation of a checklist. Specifically, improvement in reporting of the three methodological information was at least three times greater when a mandatory checklist was implemented than when it was not. Reporting the sex of animals and the number of independent experiments performed also improved from 2013 to 2015, likely from factors not related to a checklist. Our study demonstrates that completing a checklist at manuscript submission is associated with improved reporting of key methodological information in preclinical animal studies.","tags":["Animal Studies","Clinical Trials","Genetically Modified Animals","Inbred Strains","Medical Journals","Randomized Controlled Trials","Reproducibility","Research Reporting Guidelines","Scientific Publishing","Systematic Reviews"],"title":"A checklist is associated with increased quality of reporting preclinical biomedical research: A systematic review","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1689122843,"objectID":"98f0f6a008dec81b33e223bbb988e1bd","permalink":"https://forrt.org/curated_resources/a-collaborative-approach-to-infant-resea/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/a-collaborative-approach-to-infant-resea/","section":"curated_resources","summary":"The ideal of scientific progress is that we accumulate measurements and integrate these into theory, but recent discussion of replicability issues has cast doubt on whether psychological research conforms to this model. Developmental research—especially with infant participants—also has discipline-specific replicability challenges, including small samples and limited measurement methods. Inspired by collaborative replication efforts in cognitive and social psychology, we describe a proposal for assessing and promoting replicability in infancy research: large-scale, multi-laboratory replication efforts aiming for a more precise understanding of key developmental phenomena. The ManyBabies project, our instantiation of this proposal, will not only help us estimate how robust and replicable these phenomena are, but also gain new theoretical insights into how they vary across ages, linguistic communities, and measurement methods. This project has the potential for a variety of positive outcomes, including less-biased estimates of theoretically important effects, estimates of variability that can be used for later study planning, and a series of best-practices blueprints for future infancy research.","tags":["Reproducibility Crisis and Credibility Revolution","Open Science"],"title":"A collaborative approach to infant research: Promoting reproducibility, best practices, and theory-building. ","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"efe4ab8dc584793b17bc034a940038e7","permalink":"https://forrt.org/curated_resources/a-community-endorsed-open-source-lexicon/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/a-community-endorsed-open-source-lexicon/","section":"curated_resources","summary":"This manuscript describes the ISMRM OSIPI (Open Science Initiative for Perfusion Imaging) lexicon for dynamic contrast-enhanced and dynamic susceptibility-contrast MRI. The lexicon was developed by Taskforce 4.2 of OSIPI to provide standardized definitions of commonly used quantities, models, and analysis processes with the aim of reducing reporting variability. The taskforce was established in February 2020 and consists of medical physicists, engineers, clinicians, data and computer scientists, and DICOM (Digital Imaging and Communications in Medicine) standard experts. Members of the taskforce collaborated via a slack channel and quarterly virtual meetings. Members participated by defining lexicon items and reporting formats that were reviewed by at least two other members of the taskforce. Version 1.0.0 of the lexicon was subject to open review from the wider perfusion imaging community between January and March 2022, and endorsed by the Perfusion Study Group of the ISMRM in the summer of 2022. The initial scope of the lexicon was set by the taskforce and defined such that it contained a basic set of quantities, processes, and models to enable users to report an end-to-end analysis pipeline including kinetic model fitting. We also provide guidance on how to easily incorporate lexicon items and definitions into free-text descriptions (e.g., in manuscripts and other documentation) and introduce an XML-based pipeline encoding format to encode analyses using lexicon definitions in standardized and extensible machine-readable code. The lexicon is designed to be open-source and extendable, enabling ongoing expansion of its content. We hope that widespread adoption of lexicon terminology and reporting formats described herein will increase reproducibility within the field.","tags":["CAPLEX","DCE-MRI","DSC-MRI","ISMRM OSIPI","Perfusion","Standardization","Lexicon"],"title":"A community-endorsed open-source lexicon for contrast agent–based perfusion MRI: A consensus guidelines report from the ISMRM Open Science Initiative for Perfusion Imaging (OSIPI)","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"bfcc6985d4f78e3c1bce3e41c0b9f0da","permalink":"https://forrt.org/curated_resources/a-consensus-based-transparency-checklist/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/a-consensus-based-transparency-checklist/","section":"curated_resources","summary":"We present a consensus-based checklist to improve and document the transparency of research reports in social and behavioural research. An accompanying online application allows users to complete the form and generate a report that they can submit with their manuscript or post to a public repository.","tags":["Inside Your Classroom","Publishing"],"title":"A consensus-based transparency checklist","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1689122843,"objectID":"5461683f0049d1c20a4c000bcc5635f1","permalink":"https://forrt.org/curated_resources/a-duty-to-describe-better-the-devil-you/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/a-duty-to-describe-better-the-devil-you/","section":"curated_resources","summary":"Although many researchers have discussed replication as a means to facilitate self-correcting science, in this article, we identify meta-analyses and evaluating the validity of correlational and causal inferences as additional processes crucial to self-correction. We argue that researchers have a duty to describe sampling decisions they make; without such descriptions, self-correction becomes difficult, if not impossible. We developed the Replicability and Meta-Analytic Suitability Inventory (RAMSI) to evaluate the descriptive adequacy of a sample of studies taken from current psychological literature. Authors described only about 30% of the sampling decisions necessary for self-correcting science. We suggest that a modified RAMSI can be used by authors to guide their written reports and by reviewers to inform editorial recommendations. Finally, we claim that when researchers do not describe their sampling decisions, both readers and reviewers may assume that those decisions do not matter to the outcome of the study, do not affect inferences made from the research findings, do not inhibit inclusion in meta-analyses, and do not inhibit replicability of the study. If these assumptions are in error, as they often are, and the neglected decisions are relevant, then the neglect may create a good deal of mischief in the field.","tags":["Reproducibility Crisis and Credibility Revolution","Open Science"],"title":"A Duty to Describe: Better the Devil You Know Than the Devil You Don't","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"d0fde4ea3bb35d5480934fbbf2444096","permalink":"https://forrt.org/curated_resources/a-funder-imposed-data-publication-requir/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/a-funder-imposed-data-publication-requir/","section":"curated_resources","summary":"Growth of the open science movement has drawn significant attention to data sharing and availability across the scientific community. In this study, we tested the ability to recover data collected under a particular funder-imposed requirement of public availability. We assessed overall data recovery success, tested whether characteristics of the data or data creator were indicators of recovery success, and identified hurdles to data recovery. Overall the majority of data were not recovered (26% recovery of 315 data projects), a similar result to journal-driven efforts to recover data. Field of research was the most important indicator of recovery success, but neither home agency sector nor age of data were determinants of recovery. While we did not find a relationship between recovery of data and age of data, age did predict whether we could find contact information for the grantee. The main hurdles to data recovery included those associated with communication with the researcher; loss of contact with the data creator accounted for half (50%) of unrecoverable datasets, and unavailability of contact information accounted for 35% of unrecoverable datasets. Overall, our results suggest that funding agencies and journals face similar challenges to enforcement of data requirements. We advocate that funding agencies could improve the availability of the data they fund by dedicating more resources to enforcing compliance with data requirements, providing data-sharing tools and technical support to awardees, and administering stricter consequences for those who ignore data sharing preconditions.","tags":["Communications","Data","Data Acquisition","Data Management","Language","Open Data","Open Science","Policy","Public Policy","Research Funding","Research Grants","Science Policy"],"title":"A funder-imposed data publication requirement seldom inspired data sharing","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"f6ba441b97396d0f2e44510de1c5579a","permalink":"https://forrt.org/curated_resources/a-future-for-digital-public-goods-for-mo/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/a-future-for-digital-public-goods-for-mo/","section":"curated_resources","summary":"Digital public goods (DPGs), if implemented with effective policies, can facilitate the realization of the United Nations Sustainable Development Goals (SDGs). However, there are ongoing deliberations on how to define DPGs and assure that society can extract the maximum benefit from the growing number of digital resources. The International Research Center of Big Data for Sustainable Development Goals (CBAS) sees DPGs as an important mechanism to facilitate information-driven policy and decision-making processes for the SDGs. This article presents the results of a CBAS survey of 51 respondents from around the world spanning multiple scientific fields, who shared their expert opinions on DPGs and their thoughts about challenges related to their practical implementation in supporting the SDGs. Based on the survey results, the paper presents core principles in a proposed strategy, including establishment of international standards, adherence to open science and open data principles, and scalability in monitoring SDG indicators. A community-driven strategy to develop DPGs is proposed to accelerate DPG production in service of the SDGs while adhering to the core principles identified in the survey.","tags":["Environmental Impact","Policy"],"title":"A future for digital public goods for monitoring SDG indicators","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"dc1624f5be8361a0b2d6513cdbf05137","permalink":"https://forrt.org/curated_resources/a-guide-to-data-visualization-best-pract/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/a-guide-to-data-visualization-best-pract/","section":"curated_resources","summary":"This applied webinar explores best practices for communicating open educational data with a wide audience. Topics include different methods for encoding data, the use of color and considerations for color blindness, visual perception, common pitfalls, and methods for minimizing cognitive load. Dr. Daniel Anderson, from the University of Oregon, guides the audience through these topics, while also briefly discussing mediums for communication, including data dashboards to reach a larger and more diverse audience.","tags":["Data Communication","Data Visualization","Open Education Research"],"title":"A Guide to Data Visualization: Best Practices for Communicating Open Educational Data","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"70e754dc0ff1a7da0274b83d987011fb","permalink":"https://forrt.org/curated_resources/a-guide-to-supporting-early-career-resea/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/a-guide-to-supporting-early-career-resea/","section":"curated_resources","summary":"Supporting early career researchers in their open science journey is critical for the future of the field. The development of open science behaviors, however, often requires guidance from faculty and colleagues. This webinar will provide practical and concrete steps for early career researchers to follow to advance their open scholarship practice. Additionally, it will offer tips about how to mentor early career researchers in the development of open science behaviors that will last throughout their careers. \n\nIf you are an early career research learning to advance your open science practices or a mentor supporting an early career research, view this informative webinar.","tags":["#earlycareerresearchers #openscience #education"],"title":"A Guide to Supporting Early Career Researchers in Open Science","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1689122843,"objectID":"e103be9710861fcf7b6a1c5696e25aee","permalink":"https://forrt.org/curated_resources/a-manifesto-for-reproducible-science/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/a-manifesto-for-reproducible-science/","section":"curated_resources","summary":"Improving the reliability and efficiency of scientific research will increase the credibility of the published scientific literature and accelerate discovery. Here we argue for the adoption of measures to optimize key elements of the scientific process: methods, reporting and dissemination, reproducibility, evaluation and incentives. There is some evidence from both simulations and empirical studies supporting the likely effectiveness of these measures, but their broad adoption by researchers, institutions, funders and journals will require iterative evaluation and improvement. We discuss the goals of these measures, and how they can be implemented, in the hope that this will facilitate action toward improving the transparency, reproducibility and efficiency of scientific research.","tags":["Reproducibility Crisis and Credibility Revolution","Open Science"],"title":"A manifesto for reproducible science","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1689122843,"objectID":"8f1dd293ee92788959e7824605c8d649","permalink":"https://forrt.org/curated_resources/a-multilab-preregistered-replication-of/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/a-multilab-preregistered-replication-of/","section":"curated_resources","summary":"Good self-control has been linked to adaptive outcomes such as better health, cohesive personal relationships, success in the workplace and at school, and less susceptibility to crime and addictions. In contrast, self-control failure is linked to maladaptive outcomes. Understanding the mechanisms by which self-control predicts behavior may assist in promoting better regulation and outcomes. A popular approach to understanding self-control is the strength or resource depletion model. Self-control is conceptualized as a limited resource that becomes depleted after a period of exertion resulting in self-control failure. The model has typically been tested using a sequential-task experimental paradigm, in which people completing an initial self-control task have reduced self-control capacity and poorer performance on a subsequent task, a state known as ego depletion. Although a meta-analysis of ego-depletion experiments found a medium-sized effect, subsequent meta-analyses have questioned the size and existence of the effect and identified instances of possible bias. The analyses served as a catalyst for the current Registered Replication Report of the ego-depletion effect. Multiple laboratories (k = 23, total N = 2,141) conducted replications of a standardized ego-depletion protocol based on a sequential-task paradigm by Sripada et al. Meta-analysis of the studies revealed that the size of the ego-depletion effect was small with 95% confidence intervals (CIs) that encompassed zero (d = 0.04, 95% CI [−0.07, 0.15]. We discuss implications of the findings for the ego-depletion effect and the resource depletion model of self-control.","tags":["Reproducibility Crisis and Credibility Revolution","Open Science"],"title":"A multilab preregistered replication of the ego-depletion effect.","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"03b3fb38c11a944362326a58774c055c","permalink":"https://forrt.org/curated_resources/a-plea-for-preregistration-in-personalit/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/a-plea-for-preregistration-in-personalit/","section":"curated_resources","summary":"In response to a crisis of confidence, several methodological initiatives have been launched to improve the robustness of psychological science. Given its real-world implications, personality disorders research is all too important to not follow suit. The authors offer a plea for preregistration in personality disorders research, using psychopathic personality (psychopathy) as a prominent case example. To suit action to word, the authors report on a preregistered study and use it to help refute common misconceptions about preregistration as well as to illustrate that the key strength of preregistration: transparency outweighs its (perceived) disadvantages. Although preregistration will not conclusively settle the many debates roiling the field of psychopathy and other personality disorders, it can help to verify the robustness of empirical observations that inform such debates.","tags":["Personality Disorders","Preregistration","Clinical Psychology","Psychopathology"],"title":"A Plea for Preregistration in Personality Disorders Research: The Case of Psychopathy","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1689122843,"objectID":"ee157ff804ee2781f6930be840431d05","permalink":"https://forrt.org/curated_resources/a-power-primer/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/a-power-primer/","section":"curated_resources","summary":"One possible reason for the continued neglect of statistical power analysis in research in the behavioral sciences is the inaccessibility of or difficulty with the standard material. A convenient, although not comprehensive, presentation of required sample sizes is provided. Effect-size indexes and conventional values for these are given for operationally defined small, medium, and large effects. The sample sizes necessary for .80 power to detect effects at these levels are tabled for 8 standard statistical tests: (1) the difference between independent means, (2) the significance of a product–moment correlation, (3) the difference between independent rs, (4) the sign test, (5) the difference between independent proportions, (6) chi-square tests for goodness of fit and contingency tables, (7) 1-way analysis of variance (ANOVA), and (8) the significance of a multiple or multiple partial correlation. ","tags":[""],"title":"A Power Primer","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1689122843,"objectID":"c0254e19470230b72e3338151bb1b99c","permalink":"https://forrt.org/curated_resources/a-powerful-nudge-presenting-calculable-c/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/a-powerful-nudge-presenting-calculable-c/","section":"curated_resources","summary":"If psychologists have recognized the pitfalls of underpowered research for decades, why does it persist? Incentives, perhaps: underpowered research benefits researchers individually (increased productivity), but harms science collectively (inflated Type I error rates and effect size estimates but low replication rates). Yet, researchers can selectively reward power at various scientific bottlenecks (e.g., peer review, hiring, funding, and promotion). We designed a stylized thought experiment to evaluate the degree to which researchers consider power and productivity in hiring decisions. Accomplished psychologists chose between a low sample size candidate and a high sample size candidate who were otherwise identical. We manipulated the degree to which participants received information about (1) productivity, (2) sample size, and (3) directly calculable Type I error and replication rates. Participants were intolerant of the negative consequences of low-power research, yet merely indifferent regarding the practices that logically produce those consequences, unless those consequences were made quite explicit.","tags":["Reproducibility Crisis and Credibility Revolution","Open Science"],"title":"A Powerful Nudge? Presenting Calculable Consequences of Underpowered Research Shifts Incentives Toward Adequately Powered Designs","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"d8ab30bca26ce28a22fba276f82199de","permalink":"https://forrt.org/curated_resources/a-primer-for-choosing-designing-and-eval/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/a-primer-for-choosing-designing-and-eval/","section":"curated_resources","summary":"Registered reports are a publication format that involves peer reviewing studies both before and after carrying out research procedures. Although registered reports were originally developed to combat challenges in quantitative and confirmatory study designs, today registered reports are also available for qualitative and exploratory work. This article provides a brief primer that aims to help researchers in choosing, designing, and evaluating registered reports, which are driven by qualitative methods.","tags":["Qualitative","Registered Reports"],"title":"A primer for choosing, designing and evaluating registered reports for qualitative methods","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"471b376ce439768bce64aea52f1c061c","permalink":"https://forrt.org/curated_resources/a-primer-on-systematic-review-and-meta-a/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/a-primer-on-systematic-review-and-meta-a/","section":"curated_resources","summary":"A systematic review is a rigorous process that involves identifying, selecting, and synthesizing available evidence pertaining to an a priori–defined research question. The resulting evidence base may be summarized qualitatively or through a quantitative analytic approach known as meta-analysis. Systematic review and meta-analysis (SRMAs) have risen in popularity across the scientific realm including diabetes research. Although well-conducted SRMAs are an indispensable tool in informing evidence-based medicine, the proliferation of SRMAs has led to many reviews of questionable quality and misleading conclusions. The objective of this article is to provide up-to-date knowledge and a comprehensive understanding of strengths and limitations of SRMAs. We first provide an overview of the SRMA process and offer ways to identify common pitfalls at key steps. We then describe best practices as well as evolving approaches to mitigate biases, improve transparency, and enhance rigor. We discuss several recent developments in SRMAs including individual-level meta-analyses, network meta-analyses, umbrella reviews, and prospective meta-analyses. Additionally, we outline several strategies that can be used to enhance quality of SRMAs and present key questions that authors, editors, and readers should consider in preparing or critically reviewing SRMAs.","tags":["Systematic Review","Meta Analysis","Diabetes"],"title":"A Primer on Systematic Review and Meta-analysis in Diabetes Research","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1689122843,"objectID":"ba63734f9243e585c2374ca86cb9044d","permalink":"https://forrt.org/curated_resources/a-proposal-for-a-new-editorial-policy-in/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/a-proposal-for-a-new-editorial-policy-in/","section":"curated_resources","summary":"“… there's this desert prison, see, with an old prisoner, resigned to his life, and a young one just arrived. The young one talks constantly of escape, and, after a few months, he makes a break. He's gone a week, and then he's brought back by the guards. He's half dead, crazy with hunger and thirst. He describes how awful it was to the old prisoner. The endless stretches of sand, no oasis, no signs of life anywhere. The old prisoner listens for a while, then says, ‘Yep. I know. I tried to escape myself, twenty years ago.’ The young prisoner says, ‘You did? Why didn't you tell me, all these months I was planning my escape? Why didn't you let me know it was impossible?’ And the old prisoner shrugs, and says, ‘So who publishes negative results?’” (Hudson, 1968, p. 168)","tags":[""],"title":"A proposal for a new editorial policy in the social sciences.","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"f75e9f5cb22f4e85d88a5e045d270e52","permalink":"https://forrt.org/curated_resources/a-proposal-for-the-future-of-scientific/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/a-proposal-for-the-future-of-scientific/","section":"curated_resources","summary":"Science advances through rich, scholarly discussion. More than ever before, digital tools allow us to take that dialogue online. To chart a new future for open publishing, we must consider alternatives to the core features of the legacy print publishing system, such as an access paywall and editorial selection before publication. Although journals have their strengths, the traditional approach of selecting articles before publication (“curate first, publish second”) forces a focus on “getting into the right journals,” which can delay dissemination of scientific work, create opportunity costs for pushing science forward, and promote undesirable behaviors among scientists and the institutions that evaluate them. We believe that a “publish first, curate second” approach with the following features would be a strong alternative: authors decide when and what to publish; peer review reports are published, either anonymously or with attribution; and curation occurs after publication, incorporating community feedback and expert judgment to select articles for target audiences and to evaluate whether scientific work has stood the test of time. These proposed changes could optimize publishing practices for the digital age, emphasizing transparency, peer-mediated improvement, and post-publication appraisal of scientific articles.","tags":["Bibliometrics","Careers","Citation Analysis","Internet","Peer Review","Publishing","Quality Control","Research Funding","Scientific Publishing","Scientists"],"title":"A proposal for the future of scientific publishing in the life sciences","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"156310d2fefb64a32fb3d6b027aa1ec4","permalink":"https://forrt.org/curated_resources/a-recipe-for-extremely-reproducible-enri/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/a-recipe-for-extremely-reproducible-enri/","section":"curated_resources","summary":"Unreliable and irreproducible research is a significant problem that wastes resources and risks undermining the public perception of science. Previous work has highlighted that published enrichment analyses frequently suffer from statistical and reporting flaws. We sought to determine whether this translates into irreproducibility by examining whether the findings of 20 open-access articles published in 2019 describing enrichment analysis with the popular DAVID suite were reproducible. We find that only four articles exhibited a high degree of concordance, while seven exhibited major discrepancies, which we mainly ascribe to deficiencies in methodological reporting. As the tool version used is no longer available, all articles using this tool pre-2021 (~20,800 studies) including this sample of 20 articles are no longer able to be reproduced with the original tools. Based on this, we suggest that results from web-based tools without long-term preservation features should not be included in scientific publications due to the threat of link decay and short reproducibility horizon. Relying exclusively on webtools for analysis may also be in breach of institutional and funder data preservation mandates. We advocate for the adoption of extremely reproducible research workflows, and we provide a detailed protocol for how to achieve it for enrichment analysis using a combination of best practices including literate programming, version control, containerisation, documentation and persistent sharing of data and software.","tags":["Bioinformatics","Docker","Enrichment Analysis","Reproducibility"],"title":"A recipe for extremely reproducible enrichment analysis (and why we need it)","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1689122843,"objectID":"35111cb0e58887990d6dec93c6534938","permalink":"https://forrt.org/curated_resources/a-reliability-generalization-study-of-jo/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/a-reliability-generalization-study-of-jo/","section":"curated_resources","summary":"Background: This paper presents the first meta-analysis for the inter-rater reliability (IRR) of journal peer reviews. IRR is defined as the extent to which two or more independent reviews of the same scientific document agree. Methodology/Principal Findings: Altogether, 70 reliability coefficients (Cohen's Kappa, intra-class correlation [ICC], and Pearson product-moment correlation [r]) from 48 studies were taken into account in the meta-analysis. The studies were based on a total of 19,443 manuscripts; on average, each study had a sample size of 311 manuscripts (minimum: 28, maximum: 1983). The results of the meta-analysis confirmed the findings of the narrative literature reviews published to date: The level of IRR (mean ICC/r2 = .34, mean Cohen's Kappa = .17) was low. To explain the study-to-study variation of the IRR coefficients, meta-regression analyses were calculated using seven covariates. Two covariates that emerged in the meta-regression analyses as statistically significant to gain an approximate homogeneity of the intra-class correlations indicated that, firstly, the more manuscripts that a study is based on, the smaller the reported IRR coefficients are. Secondly, if the information of the rating system for reviewers was reported in a study, then this was associated with a smaller IRR coefficient than if the information was not conveyed. Conclusions/Significance: Studies that report a high level of IRR are to be considered less credible than those with a low level of IRR. According to our meta-analysis the IRR of peer assessments is quite limited and needs improvement (e.g., reader system).","tags":["Reproducibility Crisis and Credibility Revolution","Open Science"],"title":"A Reliability-Generalization Study of Journal Peer Reviews: A Multilevel Meta-Analysis of Inter-Rater Reliability and Its Determinants","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"4cbca3e7893d64cf1ca9beae398e715a","permalink":"https://forrt.org/curated_resources/a-reputation-economy-how-individual-rewa/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/a-reputation-economy-how-individual-rewa/","section":"curated_resources","summary":"Open access to research data has been described as a driver of innovation and a potential cure for the reproducibility crisis in many academic fields. Against this backdrop, policy makers are increasingly advocating for making research data and supporting material openly available online. Despite its potential to further scientific progress, widespread data sharing in small science is still an ideal practised in moderation. In this article, we explore the question of what drives open access to research data using a survey among 1564 mainly German researchers across all disciplines. We show that, regardless of their disciplinary background, researchers recognize the benefits of open access to research data for both their own research and scientific progress as a whole. Nonetheless, most researchers share their data only selectively. We show that individual reward considerations conflict with widespread data sharing. Based on our results, we present policy implications that are in line with both individual reward considerations and scientific progress.","tags":["Data Sharing","Open Data","Publishing","Reproducibility"],"title":"A reputation economy: how individual reward considerations trump systemic arguments for open access to data","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1689122843,"objectID":"fc7cd5092c0c172d192e27b061a077fc","permalink":"https://forrt.org/curated_resources/a-short-personal-future-history-of-revol/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/a-short-personal-future-history-of-revol/","section":"curated_resources","summary":"Crisis of replicability is one term that psychological scientists use for the current introspective phase we are in—I argue instead that we are going through a revolution analogous to a political revolution. Revolution 2.0 is an uprising focused on how we should be doing science now (i.e., in a 2.0 world). The precipitating events of the revolution have already been well-documented: failures to replicate, questionable research practices, fraud, etc. And the fact that none of these events is new to our field has also been well-documented. I suggest four interconnected reasons as to why this time is different: changing technology, changing demographics of researchers, limited resources, and misaligned incentives. I then describe two reasons why the revolution is more likely to catch on this time: technology (as part of the solution) and the fact that these concerns cut across social and life sciences—that is, we are not alone. Neither side in the revolution has behaved well, and each has characterized the other in extreme terms (although, of course, each has had a few extreme actors). Some suggested reforms are already taking hold (e.g., journals asking for more transparency in methods and analysis decisions; journals publishing replications) but the feared tyrannical requirements have, of course, not taken root (e.g., few journals require open data; there is no ban on exploratory analyses). Still, we have not yet made needed advances in the ways in which we accumulate, connect, and extract conclusions from our aggregated research. However, we are now ready to move forward by adopting incremental changes and by acknowledging the multiplicity of goals within psychological science.","tags":["Reproducibility Crisis and Credibility Revolution","Open Science"],"title":"A Short (Personal) Future History of Revolution 2.0","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"632d4b844a7f4bc1a5a745ca1723d67f","permalink":"https://forrt.org/curated_resources/a-short-introduction-to-the-reproducibil/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/a-short-introduction-to-the-reproducibil/","section":"curated_resources","summary":"The Journal of European Psychology Students (JEPS) is an open-access, double-blind, peer-reviewed journal for psychology students worldwide. JEPS is run by highly motivated European psychology students and has been publishing since 2009. By ensuring that authors are always provided with extensive feedback, JEPS gives psychology students the chance to gain experience in publishing and to improve their scientific skills. Furthermore, JEPS provides students with the opportunity to share their research and to take a first step toward a scientific career.","tags":["Publishing"],"title":"A Short Introduction to the Reproducibility Debate in Psychology","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1689122843,"objectID":"b134a5b7bd69cb7fb4f675d367ba24ac","permalink":"https://forrt.org/curated_resources/a-social-priming-data-set-with-troubling/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/a-social-priming-data-set-with-troubling/","section":"curated_resources","summary":"A recent paper by Chatterjee, Rose, and Sinha (2013) reported impressively large “money priming” effects: incidental exposure to concepts relating to cash or credit cards made participants much less generous with their time and money (after cash primes) or much more generous (after credit card primes ). Primes also altered participants’ choices in a word-stem completion task. To explore these effects, we carried out re-analyses of the raw data. A number of strange oddities were brought to light, including a dramatic similarity of the filler word-stem completion responses produced by the 20 subjects who contributed most to the priming effects. We suggest that these oddities undermine the credibility of the paper and require further investigation.","tags":["Reproducibility Crisis and Credibility Revolution","Open Science"],"title":"A Social Priming Data Set With Troubling Oddities","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"360f3ef0d41fcdad4ee4247b97497e9f","permalink":"https://forrt.org/curated_resources/a-social-psychological-model-of-scientif/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/a-social-psychological-model-of-scientif/","section":"curated_resources","summary":"A crescendo of incidents have raised concerns about whether scientific practices in psychology may be suboptimal, sometimes leading to the publication, dissemination, and application of unreliable or misinterpreted findings. Psychology has been a leader in identifying possibly suboptimal practices and proposing reforms that might enhance the efficiency of the scientific process and the publication of robust evidence and interpretations. To help shape future efforts, this paper offers a model of the psychological and socio-structural forces and processes that may influence scientists’ practices. The model identifies practices targeted by interventions and reforms, and which practices remain unaddressed. The model also suggests directions for empirical research to assess how best to enhance the effectiveness of psychological inquiry.","tags":["Meta-Xcience","Questionable Research Practices","Replication Crisis","Reproducibility","Science Reform"],"title":"A Social Psychological Model of Scientific Practices: Explaining Research Practices and Outlining the Potential for Successful Reforms","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1689122843,"objectID":"bd10d587b9bfaf7128143fae2cf0c711","permalink":"https://forrt.org/curated_resources/a-software-tool-for-removing-patient-ide/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/a-software-tool-for-removing-patient-ide/","section":"curated_resources","summary":"We created a software tool that accurately removes all patient identifying information from various kinds of clinical data documents, including laboratory and narrative reports. We created the Medical De-identification System (MeDS), a software tool that de-identifies clinical documents, and performed 2 evaluations. Our first evaluation used 2,400 Health Level Seven (HL7) messages from 10 different HL7 message producers. After modifying the software based on the results of this first evaluation, we performed a second evaluation using 7,190 pathology report HL7 messages. We compared the results of MeDS de-identification process to a gold standard of human review to find identifying strings. For both evaluations, we calculated the number of successful scrubs, missed identifiers, and over-scrubs committed by MeDS and evaluated the readability and interpretability of the scrubbed messages. We categorized all missed identifiers into 3 groups: (1) complete HIPAA-specified identifiers, (2) HIPAA-specified identifier fragments, (3) non-HIPAA–specified identifiers (such as provider names and addresses). In the results of the first-pass evaluation, MeDS scrubbed 11,273 (99.06%) of the 11,380 HIPAA-specified identifiers and 38,095 (98.26%) of the 38,768 non-HIPAA–specified identifiers. In our second evaluation (status postmodification to the software), MeDS scrubbed 79,993 (99.47%) of the 80,418 HIPAA-specified identifiers and 12,689 (96.93%) of the 13,091 non-HIPAA–specified identifiers. Approximately 95% of scrubbed messages were both readable and interpretable. We conclude that MeDS successfully de-identified a wide range of medical documents from numerous sources and creates scrubbed reports that retain their interpretability, thereby maintaining their usefulness for research.","tags":[""],"title":"A Software Tool for Removing Patient Identifying Information from Clinical Documents","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"f2f0b0c0cbbe3d1ede33972a3c281f22","permalink":"https://forrt.org/curated_resources/a-step-by-step-guide-on-preregistration/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/a-step-by-step-guide-on-preregistration/","section":"curated_resources","summary":"Data analysis in psychopathology research typically entails multiple stages of data preprocessing (e.g., coding of physiological measures), statistical decisions (e.g., inclusion of covariates), and reporting (e.g., selecting which variables best answer the research questions). The complexity and lack of transparency of these procedures have resulted in two troubling trends: the central hypotheses and analytical approaches are often selected after observing the data, and the research data are often not properly indexed. These practices are particularly problematic for (experimental) psychopathology research because the data are often hard to gather due to the target populations (e.g., individuals with mental disorders), and because the standard methodological approaches are challenging and time consuming (e.g., longitudinal studies). Here, we present a workflow that covers study preregistration, data anonymization, and the easy sharing of data and experimental material with the rest of the research community. This workflow is tailored to both original studies and secondary statistical analyses of archival data sets. In order to facilitate the implementation of the described workflow, we have developed a free and open-source software program. We argue that this workflow will result in more transparent and easily shareable psychopathology research, eventually increasing and replicability reproducibility in our research field.","tags":["Preregistration","Psychopathology","Clinical Psychology","Data Sharing"],"title":"A step-by-step guide on preregistration and effective data sharing for psychopathology research","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"bd025baa80538553a97f62171f6886ff","permalink":"https://forrt.org/curated_resources/a-study-of-the-impact-of-data-sharing-on/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/a-study-of-the-impact-of-data-sharing-on/","section":"curated_resources","summary":"This study estimates the effect of data sharing on the citations of academic articles, using journal policies as a natural experiment. We begin by examining 17 high-impact journals that have adopted the requirement that data from published articles be publicly posted. We match these 17 journals to 13 journals without policy changes and find that empirical articles published just before their change in editorial policy have citation rates with no statistically significant difference from those published shortly after the shift. We then ask whether this null result stems from poor compliance with data sharing policies, and use the data sharing policy changes as instrumental variables to examine more closely two leading journals in economics and political science with relatively strong enforcement of new data policies. We find that articles that make their data available receive 97 additional citations (estimate standard error of 34). We conclude that: a) authors who share data may be rewarded eventually with additional scholarly citations, and b) data-posting policies alone do not increase the impact of articles published in a journal unless those policies are enforced.","tags":["Citation Analysis","Data","Instrumental Variable Analysis","Policy","Political Science","Publishing","Science Policy","Science Policy and Economics","Scientific Publishing","Scientists","Statistical Data"],"title":"A study of the impact of data sharing on article citations using journal policies as a natural experiment","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1689122843,"objectID":"86aeac014d44554d0f0291d5c3d9a1e7","permalink":"https://forrt.org/curated_resources/a-survey-of-the-statistical-power-of-res/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/a-survey-of-the-statistical-power-of-res/","section":"curated_resources","summary":"We estimated the statistical power of the first and last statistical test presented in 697 papers from 10 behavioral journals. First tests had significantly greater statistical power and reported more significant results (smaller p values) than did last tests. This trend was consistent across journals, taxa, and the type of statistical test used. On average, statistical power was 13–16% to detect a small effect and 40–47% to detect a medium effect. This is far lower than the general recommendation of a power of 80%. By this criterion, only 2–3%, 13–21%, and 37–50% of the tests examined had the requisite power to detect a small, medium, or large effect, respectively. Neither p values nor statistical power varied significantly across the 10 journals or 11 taxa. However, mean p values of first and last tests were significantly correlated across journals (⁠r =.67, n = 10, p =.034⁠), with a similar trend for mean power (⁠ r =.63, n = 10, p =.051⁠). There is therefore some evidence that power and p values are repeatable among journals. Mean p values or power of first and last tests were, however, uncorrelated across taxa. Finally, there was a significant correlation between power and reported p value for both first (⁠ r =.13, n = 684, p =.001⁠) and last tests (⁠ r =.16, n = 654, p \u003c.0001⁠). If true effect sizes are unrelated to study sample sizes, the average true effect size must be nonzero for this pattern to emerge. This suggests that failure to observe significant relationships is partly owing to small sample sizes, as power increases with sample size.","tags":[""],"title":"A survey of the statistical power of research in behavioral ecology and animal behavior.","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"75984d320841e4db0bbc6b5486569d66","permalink":"https://forrt.org/curated_resources/a-survey-on-how-preregistration-affects/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/a-survey-on-how-preregistration-affects/","section":"curated_resources","summary":"The preregistration of research protocols and analysis plans is a main reform innovation to counteract confirmation bias in the social and behavioural sciences. While theoretical reasons to preregister are frequently discussed in the literature, the individually experienced advantages and disadvantages of this method remain largely unexplored. The goal of this exploratory study was to identify the perceived benefits and challenges of preregistration from the researcher’s perspective. To this end, we surveyed 355 researchers, 299 of whom had used preregistration in their own work. The researchers indicated the experienced or expected effects of preregistration on their workflow. The results show that experiences and expectations are mostly positive. Researchers in our sample believe that implementing preregistration improves or is likely to improve the quality of their projects. Criticism of preregistration is primarily related to the increase in work-related stress and the overall duration of the project. While the benefits outweighed the challenges for the majority of researchers with preregistration experience, this was not the case for the majority of researchers without preregistration experience. The experienced advantages and disadvantages identified in our survey could inform future efforts to improve preregistration and thus help the methodology gain greater acceptance in the scientific community.","tags":["Open Science","Metascience","Replication Crisis","Preregistration"],"title":"A survey on how preregistration affects the research workflow: better science but more work","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1689122843,"objectID":"ba4cb888cc037652cc3a3bc82c0719b9","permalink":"https://forrt.org/curated_resources/a-systematic-review-of-statistical-power/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/a-systematic-review-of-statistical-power/","section":"curated_resources","summary":"Statistical power is an inherent part of empirical studies that employ significance testing and is essential for the planning of studies, for the interpretation of study results, and for the validity of study conclusions. This paper reports a quantitative assessment of the statistical power of empirical software engineering research based on the 103 papers on controlled experiments (of a total of 5,453 papers) published in nine major software engineering journals and three conference proceedings in the decade 1993–2002. The results show that the statistical power of software engineering experiments falls substantially below accepted norms as well as the levels found in the related discipline of information systems research. Given this study’s findings, additional attention must be directed to the adequacy of sample sizes and research designs to ensure acceptable levels of statistical power. Furthermore, the current reporting of significance tests should be enhanced by also reporting effect sizes and confidence intervals.","tags":[""],"title":"A systematic review of statistical power in software engineering experiments. ","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1689122843,"objectID":"1578aa55e7eaa2483d8cbe2f62a89baf","permalink":"https://forrt.org/curated_resources/a-tale-of-two-papers/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/a-tale-of-two-papers/","section":"curated_resources","summary":"An abstract about transparency and robustness for two papers","tags":["Blog","Reproducibility Crisis and Credibility Revolution"],"title":"A tale of two papers","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"58dc8bb05c0d738ab293b7bca6beb6db","permalink":"https://forrt.org/curated_resources/a-test-of-the-diffusion-model-explanatio/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/a-test-of-the-diffusion-model-explanatio/","section":"curated_resources","summary":"People with higher IQ scores also tend to perform better on elementary cognitive-perceptual tasks, such as deciding quickly whether an arrow points to the left or the right Jensen (2006). The worst performance rule (WPR) finesses this relation by stating that the association between IQ and elementary-task performance is most pronounced when this performance is summarized by people’s slowest responses. Previous research has shown that the WPR can be accounted for in the Ratcliff diffusion model by assuming that the same ability parameter—drift rate—mediates performance in both elementary tasks and higher-level cognitive tasks. Here we aim to test four qualitative predictions concerning the WPR and its diffusion model explanation in terms of drift rate. In the first stage, the diffusion model was fit to data from 916 participants completing a perceptual two-choice task; crucially, the fitting happened after randomly shuffling the key variable, i.e., each participant’s score on a working memory capacity test. In the second stage, after all modeling decisions were made, the key variable was unshuffled and the adequacy of the predictions was evaluated by means of confirmatory Bayesian hypothesis tests. By temporarily withholding the mapping of the key predictor, we retain flexibility for proper modeling of the data (e.g., outlier exclusion) while preventing biases from unduly influencing the results. Our results provide evidence against the WPR and suggest that it may be less robust and less ubiquitous than is commonly believed.","tags":["Reproducibility"],"title":"A test of the diffusion model explanation for the worst performance rule using preregistration and blinding","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"6689e1abb97c9153406656cca0150638","permalink":"https://forrt.org/curated_resources/a-tutorial-on-cognitive-modeling-for-cog/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/a-tutorial-on-cognitive-modeling-for-cog/","section":"curated_resources","summary":"Cognitive aging researchers are interested in understanding how cognitive processes change in old age, but the relationship between hypothetical latent cognitive processes and observed behavior is often complex and not fully accounted for in standard analyses (e.g., Analysis of variance [ANOVA]). Cognitive models formalize the relationship between underlying processes and observed behavior and are more suitable for identifying what processes are associated with aging. This article provides a tutorial on how to fit and interpret cognitive models to measure age differences in cognitive processes. We work with an example of a two choice discrimination task and describe how to fit models in the highly flexible modeling software Stan. We describe how to use hierarchical modeling to estimate both group and individual effects simultaneously, and we detail model fitting in a Bayesian statistical framework, which, among other benefits, enables aging researchers to quantify evidence for null effects. We contend that more widespread use of cognitive modeling among cognitive aging researchers may be useful for addressing potential issues of nonreplicability in the field, as cognitive modeling is more suitable to addressing questions about what cognitive processes are (or are not) affected by aging.","tags":["Cognitive Aging","Cognitive Modeling","Replication"],"title":"A tutorial on cognitive modeling for cognitive aging research","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1689122843,"objectID":"0db3c56492624ebd8094c3fc8d5b5fe6","permalink":"https://forrt.org/curated_resources/a-unified-framework-to-quantify-the-cred/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/a-unified-framework-to-quantify-the-cred/","section":"curated_resources","summary":"Societies invest in scientific studies to better understand the world and attempt to harness such improved understanding to address pressing societal problems. Published research, however, can be useful for theory or application only if it is credible. In science, a credible finding is one that has repeatedly survived risky falsification attempts. However, state-of-the-art meta-analytic approaches cannot determine the credibility of an effect because they do not account for the extent to which each included study has survived such attempted falsification. To overcome this problem, we outline a unified framework for estimating the credibility of published research by examining four fundamental falsifiability-related dimensions: (a) transparency of the methods and data, (b) reproducibility of the results when the same data-processing and analytic decisions are reapplied, (c) robustness of the results to different data-processing and analytic decisions, and (d) replicability of the effect. This framework includes a standardized workflow in which the degree to which a finding has survived scrutiny is quantified along these four facets of credibility. The framework is demonstrated by applying it to published replications in the psychology literature. Finally, we outline a Web implementation of the framework and conclude by encouraging the community of researchers to contribute to the development and crowdsourcing of this platform.","tags":["Transparency","Reproducibility Crisis and Credibility Revolution","Open Science"],"title":"A Unified Framework to Quantify the Credibility of Scientific Findings","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1689122843,"objectID":"08ba4dfc6603cbeae417b30622f01f32","permalink":"https://forrt.org/curated_resources/a-vast-graveyard-of-undead-theories-publ/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/a-vast-graveyard-of-undead-theories-publ/","section":"curated_resources","summary":"Publication bias remains a controversial issue in psychological science. The tendency of psychological science to avoid publishing null results produces a situation that limits the replicability assumption of science, as replication cannot be meaningful without the potential acknowledgment of failed replications. We argue that the field often constructs arguments to block the publication and interpretation of null results and that null results may be further extinguished through questionable researcher practices. Given that science is dependent on the process of falsification, we argue that these problems reduce psychological science’s capability to have a proper mechanism for theory falsification, thus resulting in the promulgation of numerous “undead” theories that are ideologically popular but have little basis in fact.","tags":["Reproducibility Crisis and Credibility Revolution","Open Science"],"title":"A Vast Graveyard of Undead Theories: Publication Bias and Psychological Science’s Aversion to the Null","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"fc80bab11f98ac565aa2b05fae75fa21","permalink":"https://forrt.org/curated_resources/a-solomon-kurz-website/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/a-solomon-kurz-website/","section":"curated_resources","summary":"A blog posts about Bayesian statistics","tags":["Bayesian statistics"],"title":"A. Solomon Kurz website","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1689122843,"objectID":"9802fc44e10063881cc193500274e759","permalink":"https://forrt.org/curated_resources/abandon-statistical-significance/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/abandon-statistical-significance/","section":"curated_resources","summary":"We discuss problems the null hypothesis significance testing (NHST) paradigm poses for replication and more broadly in the biomedical and social sciences as well as how these problems remain unresolved by proposals involving modified p-value thresholds, confidence intervals, and Bayes factors. We then discuss our own proposal, which is to abandon statistical significance. We recommend dropping the NHST paradigm—and the p-value thresholds intrinsic to it—as the default statistical paradigm for research, publication, and discovery in the biomedical and social sciences. Specifically, we propose that the p-value be demoted from its threshold screening role and instead, treated continuously, be considered along with currently subordinate factors (e.g., related prior evidence, plausibility of mechanism, study design and data quality, real world costs and benefits, novelty of finding, and other factors that vary by research domain) as just one among many pieces of evidence. We have no desire to “ban” p-values or other purely statistical measures. Rather, we believe that such measures should not be thresholded and that, thresholded or not, they should not take priority over the currently subordinate factors. We also argue that it seldom makes sense to calibrate evidence as a function of p-values or other purely statistical measures. We offer recommendations for how our proposal can be implemented in the scientific publication process as well as in statistical decision making more broadly.","tags":[""],"title":"Abandon statistical significance. ","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1721390377,"objectID":"e8c12321314ead834bbdbe977fe14bf3","permalink":"https://forrt.org/glossary/english/abstract_bias/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/english/abstract_bias/","section":"glossary","summary":"","tags":null,"title":"Abstract Bias","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1635961284,"objectID":"73b7216b52e114a7ced618914ee74ca4","permalink":"https://forrt.org/glossary/vbeta/abstract-bias/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/vbeta/abstract-bias/","section":"glossary","summary":"","tags":null,"title":"Abstract Bias","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1720865851,"objectID":"acd5f95091c6cc7f5c525f3e7014a2d4","permalink":"https://forrt.org/glossary/german/abstract_bias/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/german/abstract_bias/","section":"glossary","summary":"","tags":null,"title":"Abstract Bias (Abstract-Verzerrung)","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1721390377,"objectID":"3942c5e84f7863e1ce82f62cf7cc6fb8","permalink":"https://forrt.org/glossary/english/academic_impact/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/english/academic_impact/","section":"glossary","summary":"","tags":null,"title":"Academic Impact","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1635961284,"objectID":"b46d67374c7ff0570195f4e3dd93c877","permalink":"https://forrt.org/glossary/vbeta/academic-impact/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/vbeta/academic-impact/","section":"glossary","summary":"","tags":null,"title":"Academic Impact","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1720865851,"objectID":"bf876eb1b10d5ee4e60da9a7a5c71bf4","permalink":"https://forrt.org/glossary/german/academic_impact/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/german/academic_impact/","section":"glossary","summary":"","tags":null,"title":"Academic Impact (Akademischer Einfluss)","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1689122843,"objectID":"97402db6f192cfd9484572c53aef0a68","permalink":"https://forrt.org/curated_resources/academic-research-in-the-21st-century-ma/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/academic-research-in-the-21st-century-ma/","section":"curated_resources","summary":"Over the last 50 years, we argue that incentives for academic scientists have become increasingly perverse in terms of competition for research funding, development of quantitative metrics to measure performance, and a changing business model for higher education itself. Furthermore, decreased discretionary funding at the federal and state level is creating a hypercompetitive environment between government agencies (e.g., EPA, NIH, CDC), for scientists in these agencies, and for academics seeking funding from all sources—the combination of perverse incentives and decreased funding increases pressures that can lead to unethical behavior. If a critical mass of scientists become untrustworthy, a tipping point is possible in which the scientific enterprise itself becomes inherently corrupt and public trust is lost, risking a new dark age with devastating consequences to humanity. Academia and federal agencies should better support science as a public good, and incentivize altruistic and ethical outcomes, while de-emphasizing output.","tags":["Reproducibility Crisis and Credibility Revolution","Open Science"],"title":"Academic Research in the 21st Century: Maintaining Scientific Integrity in a Climate of Perverse Incentives and Hypercompetition","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1721390377,"objectID":"951ecd1c1d3b48f4e554e6aabb60c838","permalink":"https://forrt.org/glossary/english/accessibility/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/english/accessibility/","section":"glossary","summary":"","tags":null,"title":"Accessibility","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1635961284,"objectID":"cd0b55c802812420e0672f1b32748bde","permalink":"https://forrt.org/glossary/vbeta/accessibility/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/vbeta/accessibility/","section":"glossary","summary":"","tags":null,"title":"Accessibility","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1720865851,"objectID":"068d5793990b739bd50b9abf97242454","permalink":"https://forrt.org/glossary/german/accessibility/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/german/accessibility/","section":"glossary","summary":"","tags":null,"title":"Accessibility (Barrierefreiheit)","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"534fc5f29e9a3e673e2e88b2f5cdafba","permalink":"https://forrt.org/curated_resources/accessibility-data-curation-primer/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/accessibility-data-curation-primer/","section":"curated_resources","summary":"Data curators are uniquely positioned to help improve access not just to individual datasets, but to the world of research data at large. As guides to and stewards of data, curators can counsel researchers on how to build accessibility into data planning, collection, analysis, and archiving. This primer is intended as a starting point for data curators who are invested in improving the accessibility of individual files or datasets, rather than as definitive guide. There is far more work to be done than can be addressed in the scope of this primer. Disability is also a complex concept with a diversity of possible presentations, which will present varying (sometimes even conflicting) accessibility needs.","tags":["Accessibility","Data Sharing","Data Management","Open Data","FAIR Data"],"title":"Accessibility Data Curation Primer","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"828107e3490ba567867e51b0363b0db4","permalink":"https://forrt.org/curated_resources/accuracy-of-effect-size-estimates-from-p/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/accuracy-of-effect-size-estimates-from-p/","section":"curated_resources","summary":"Monte-Carlo simulation was used to model the biasing of effect sizes in published studies. The findings from the simulation indicate that, when a predominant bias to publish studies with statistically significant results is coupled with inadequate statistical power, there will be an overestimation of effect sizes. The consequences such an effect size overestimation will then have on meta-analyses and power analyses are highlighted and discussed along with measures which can be taken to reduce the problem. ","tags":[""],"title":"Accuracy of effect size estimates from published psychological research ","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1721390377,"objectID":"1e9625a03af09887659ca4f6f7f6ef8a","permalink":"https://forrt.org/glossary/english/ad_hominem_bias/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/english/ad_hominem_bias/","section":"glossary","summary":"","tags":null,"title":"Ad hominem bias","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1635961284,"objectID":"c10dc14d71e3a3a684f08520a1a39f3b","permalink":"https://forrt.org/glossary/vbeta/ad-hominem-bias/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/vbeta/ad-hominem-bias/","section":"glossary","summary":"","tags":null,"title":"Ad hominem bias","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1720865851,"objectID":"889f260e3fc6b53f96f0132156ee1c19","permalink":"https://forrt.org/glossary/german/ad_hominem_bias/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/german/ad_hominem_bias/","section":"glossary","summary":"","tags":null,"title":"Ad hominem bias (Ad hominem Verzerrung)","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"387b297662096c2f41e802c1ba496e22","permalink":"https://forrt.org/curated_resources/adapting-open-science-and-pre-registrati/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/adapting-open-science-and-pre-registrati/","section":"curated_resources","summary":"Open science practices, such as pre-registration and data sharing, increase transparency and may improve the replicability of developmental science. However, developmental science has lagged behind other fields in implementing open science practices. This lag may arise from unique challenges and considerations of longitudinal research. In this paper, preliminary guidelines are provided for adapting open science practices to longitudinal research to facilitate researchers' use of these practices. The guidelines propose a serial and modular approach to registration that includes an initial pre-registration of the methods and focal hypotheses of the longitudinal study, along with subsequent pre- or co-registered questions, hypotheses, and analysis plans associated with specific papers. Researchers are encouraged to share their research materials and relevant data with associated papers and to report sufficient information for replicability. In addition, there should be careful consideration of requirements regarding the timing of data sharing, to avoid disincentivizing longitudinal research.","tags":["Development","Longitudinal","Open Science","Preregistration","Replication","Reproducibility"],"title":"Adapting open science and pre-registration to longitudinal research","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1689122843,"objectID":"d70a5dff640a48668decc731f94ed3a8","permalink":"https://forrt.org/curated_resources/adjusting-for-publication-bias-in-metaan/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/adjusting-for-publication-bias-in-metaan/","section":"curated_resources","summary":"We review and evaluate selection methods, a prominent class of techniques first proposed by Hedges (1984) that assess and adjust for publication bias in meta-analysis, via an extensive simulation study. Our simulation covers both restrictive settings as well as more realistic settings and proceeds across multiple metrics that assess different aspects of model performance. This evaluation is timely in light of two recently proposed approaches, the so-called p-curve and p-uniform approaches, that can be viewed as alternative implementations of the original Hedges selection method approach. We find that the p-curve and p-uniform approaches perform reasonably well but not as well as the original Hedges approach in the restrictive setting for which all three were designed. We also find they perform poorly in more realistic settings, whereas variants of the Hedges approach perform well. We conclude by urging caution in the application of selection methods: Given the idealistic model assumptions underlying selection methods and the sensitivity of population average effect size estimates to them, we advocate that selection methods should be used less for obtaining a single estimate that purports to adjust for publication bias ex post and more for sensitivity analysis—that is, exploring the range of estimates that result from assuming different forms of and severity of publication bias.","tags":[""],"title":"Adjusting for Publication Bias in MetaAnalysis. Perspectives on Psychological Science, 11(5), 730–749. ","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"26925d09c9820eceaf0c7d90d114473e","permalink":"https://forrt.org/curated_resources/adopting-open-educational-resources-as-a/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/adopting-open-educational-resources-as-a/","section":"curated_resources","summary":"Social determinants of learning (SDOL) impact students’ abilities to successfully complete their courses. An economic barrier for vulnerable students is the skyrocketing cost of textbooks. An innovative strategy to promote equity in educational delivery is the adoption of open educational resources (OER). The Open Resources for Nursing (Open RN) project published five OER textbooks with input from over 65 contributors and 300 reviewers. Open RN textbooks have received widespread national and international usage. Analysis of student outcomes indicates similar successful course completion rates by all students in OER course sections and increased rates of successful course completion by non-White students.","tags":["Equity in Nursing Education","Nursing Textbooks","Open Educational Resources","Social Determinants of Learning"],"title":"Adopting Open Educational Resources as an Equity Strategy","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1721390377,"objectID":"1cd479fa8d905fd8adf9b2e56d4a6f68","permalink":"https://forrt.org/glossary/english/adversarial/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/english/adversarial/","section":"glossary","summary":"","tags":null,"title":"Adversarial (collaborative) commentary","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1635961284,"objectID":"42b0bb8e900924b687ada87ac5ccffef","permalink":"https://forrt.org/glossary/vbeta/adversarial-collaborative-commentar/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/vbeta/adversarial-collaborative-commentar/","section":"glossary","summary":"","tags":null,"title":"Adversarial (collaborative) commentary","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1720865851,"objectID":"348fc992c6776142c6ccdb6d81d1d7b6","permalink":"https://forrt.org/glossary/german/adversarial/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/german/adversarial/","section":"glossary","summary":"","tags":null,"title":"Adversarial (collaborative) commentary (Gegnerischer (kollaborativer) Kommentar)","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1721390377,"objectID":"2d2de44feca17e41b8354ac39d1f84fc","permalink":"https://forrt.org/glossary/english/adversarial_collaboration/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/english/adversarial_collaboration/","section":"glossary","summary":"","tags":null,"title":"Adversarial collaboration","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1635961284,"objectID":"0d682008b78ac3370d9ecfcdb7d978c1","permalink":"https://forrt.org/glossary/vbeta/adversarial-collaboration/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/vbeta/adversarial-collaboration/","section":"glossary","summary":"","tags":null,"title":"Adversarial collaboration","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1720865851,"objectID":"e2f16aae5e41bbc09b51147222a9bd16","permalink":"https://forrt.org/glossary/german/adversarial_collaboration/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/german/adversarial_collaboration/","section":"glossary","summary":"","tags":null,"title":"Adversarial collaboration (Gegnerische Kollaboration)","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"18c7d0aa48608e8564c7c3774fe22e5d","permalink":"https://forrt.org/curated_resources/advocating-for-change-in-how-science-is/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/advocating-for-change-in-how-science-is/","section":"curated_resources","summary":"Open science practices have the potential to greatly accelerate progress in scientific research if widely adopted, but individual action may not be enough to...","tags":["#centerforopenscience","#openpractice","#openscience","Reproducibility","Research","#TOPguidelines"],"title":"Advocating for Change in How Science is Conducted to Level the Playing Field","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1721390377,"objectID":"b8460cf57dce976c295585f008a6d52e","permalink":"https://forrt.org/glossary/english/affiliation_bias/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/english/affiliation_bias/","section":"glossary","summary":"","tags":null,"title":"Affiliation bias","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1635961284,"objectID":"d754ef285f11a6936dcd430eac7041b4","permalink":"https://forrt.org/glossary/vbeta/affiliation-bias/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/vbeta/affiliation-bias/","section":"glossary","summary":"","tags":null,"title":"Affiliation bias","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1720865851,"objectID":"457ee0bcc68f84e249772d593da4e584","permalink":"https://forrt.org/glossary/german/affiliation_bias/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/german/affiliation_bias/","section":"glossary","summary":"","tags":null,"title":"Affiliation bias (Affiliations-Verzerrung)","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1721390377,"objectID":"213d366edfd24876884e9f97f6c5ab8a","permalink":"https://forrt.org/glossary/english/aleatoric_uncertainty/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/english/aleatoric_uncertainty/","section":"glossary","summary":"","tags":null,"title":"Aleatoric uncertainty","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1635961284,"objectID":"46d5942bbec3029310e66fbed5efbe95","permalink":"https://forrt.org/glossary/vbeta/aleatoric-uncertainty/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/vbeta/aleatoric-uncertainty/","section":"glossary","summary":"","tags":null,"title":"Aleatoric uncertainty","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1720865851,"objectID":"9d89f3e289a418ea93068629af5f8a04","permalink":"https://forrt.org/glossary/german/aleatoric_uncertainty/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/german/aleatoric_uncertainty/","section":"glossary","summary":"","tags":null,"title":"Aleatoric uncertainty (Aleatorische Unsicherheit)","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"603fed833d7f2ff961a1b1711e029786","permalink":"https://forrt.org/curated_resources/all-the-weight-of-our-dreams-on-living-r/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/all-the-weight-of-our-dreams-on-living-r/","section":"curated_resources","summary":"For those of us who are autistic and racialized, we often struggle to find representation in mass media, academic work about autism or race, and the activist and advocacy movements that focus on autism, neurodiversity, disability rights, or racial justice. Most autism and autistic organizations, publications about autism, and broader neurodiversity campaigns are predominantly white. Yet disabled Black and Brown students are most likely to be impacted by the school to prison pipeline; the vast majority of U.S. prisoners are disabled and Black or Brown; racialized people are a global majority (which means that autistic people of color far outnumber white autistic people in the world); and the combined impact of race and disability severely increase likelihood for hate crimes, police violence, all other forms of abuse, and repeated retraumatization. Our stories matter and must be told. We hope that this collection will not only speak sharply against our constant erasure and invisibility as (at least) doubly impacted, but will also provide solace and familiarity for our own out there waiting for stories like theirs to be told.","tags":["Diversity","Equity and Inclusion"],"title":"All the Weight of Our Dreams On Living Racialized Autism","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1721390377,"objectID":"e5e68df0112adeff9ac39d2edceffd6d","permalink":"https://forrt.org/glossary/english/altmetrics/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/english/altmetrics/","section":"glossary","summary":"","tags":null,"title":"Altmetrics","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1720865851,"objectID":"13e713216961e81be2dcbb8a2e244c6b","permalink":"https://forrt.org/glossary/german/altmetrics/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/german/altmetrics/","section":"glossary","summary":"","tags":null,"title":"Altmetrics","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1635961284,"objectID":"38f497e2fb909eda0d8a15159d3a632c","permalink":"https://forrt.org/glossary/vbeta/altmetrics/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/vbeta/altmetrics/","section":"glossary","summary":"","tags":null,"title":"Altmetrics","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1689122843,"objectID":"8424ad9465aecc3feff267a195ce7d68","permalink":"https://forrt.org/curated_resources/always-use-welch-t-test/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/always-use-welch-t-test/","section":"curated_resources","summary":"This is a blogpost describing why we should use Welch t-test instead of Student t-test","tags":["Blog","Code","Reproducibility Crisis and Credibility Revolution"],"title":"Always use Welch t-test","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1721390377,"objectID":"6d962aebe2addecaaa600f5049dc6823","permalink":"https://forrt.org/glossary/english/amnesia/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/english/amnesia/","section":"glossary","summary":"","tags":null,"title":"AMNESIA","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1720865851,"objectID":"1d65197ab13294c2c54cc2cdb09f8ffb","permalink":"https://forrt.org/glossary/german/amnesia/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/german/amnesia/","section":"glossary","summary":"","tags":null,"title":"AMNESIA","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1635961284,"objectID":"b87a800cc7c30b8bd48e55a9b56879ec","permalink":"https://forrt.org/glossary/vbeta/amnesia/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/vbeta/amnesia/","section":"glossary","summary":"","tags":null,"title":"AMNESIA","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1689122843,"objectID":"111fffd0cb86ba9e98adda2f03b51b7d","permalink":"https://forrt.org/curated_resources/an-agenda-for-purely-confirmatory-resear/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/an-agenda-for-purely-confirmatory-resear/","section":"curated_resources","summary":"The veracity of substantive research claims hinges on the way experimental data are collected and analyzed. In this article, we discuss an uncomfortable fact that threatens the core of psychology's academic enterprise: almost without exception, psychologists do not commit themselves to a method of data analysis before they see the actual data. It then becomes tempting to fine tune the analysis to the data in order to obtain a desired result-a procedure that invalidates the interpretation of the common statistical tests. The extent of the fine tuning varies widely across experiments and experimenters but is almost impossible for reviewers and readers to gauge. To remedy the situation, we propose that researchers preregister their studies and indicate in advance the analyses they intend to conduct. Only these analyses deserve the label \"confirmatory,\" and only for these analyses are the common statistical tests valid. Other analyses can be carried out but these should be labeled \"exploratory.\" We illustrate our proposal with a confirmatory replication attempt of a study on extrasensory perception.","tags":["Reproducibility Crisis and Credibility Revolution","Open Science"],"title":"An Agenda for Purely Confirmatory Research","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1689122843,"objectID":"de13edcae3b454211c3cc38dd1a5ba49","permalink":"https://forrt.org/curated_resources/an-assessment-of-the-magnitude-of-effect/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/an-assessment-of-the-magnitude-of-effect/","section":"curated_resources","summary":"This study compiles information from more than 250 meta-analyses conducted over the past 30 years to assess the magnitude of reported effect sizes in the organizational behavior (OB)/human resources (HR) literatures. Our analysis revealed an average uncorrected effect of r = .227 and an average corrected effect of ρ = .278 (SDρ = .140). Based on the distribution of effect sizes we report, Cohen’s effect size benchmarks are not appropriate for use in OB/HR research as they overestimate the actual breakpoints between small, medium, and large effects. We also assessed the average statistical power reported in meta-analytic conclusions and found substantial evidence that the majority of primary studies in the management literature are statistically underpowered. Finally, we investigated the impact of the file drawer problem in meta-analyses and our findings indicate that the file drawer problem is not a significant concern for meta-analysts. We conclude by discussing various implications of this study for OB/HR researchers.","tags":[""],"title":"An assessment of the magnitude of effect sizes: Evidence from 30 years of meta-analysis in management. ","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"c7402c5e8aa3dd2b5b52923a566e2c94","permalink":"https://forrt.org/curated_resources/an-excess-of-positive-results-comparing/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/an-excess-of-positive-results-comparing/","section":"curated_resources","summary":"When studies with positive results that support the tested hypotheses have a higher probability of being published than studies with negative results, the literature will give a distorted view of the evidence for scientific claims. Psychological scientists have been concerned about the degree of distortion in their literature due to publication bias and inflated Type-1 error rates. Registered Reports were developed with the goal to minimise such biases: In this new publication format, peer review and the decision to publish take place before the study results are known. We compared the results in the full population of published Registered Reports in Psychology (N = 71 as of November 2018) with a random sample of hypothesis-testing studies from the standard literature (N = 152) by searching 633 journals for the phrase ‘test* the hypothes*’ (replicating a method by Fanelli, 2010). Analysing the first hypothesis reported in each paper, we found 96% positive results in standard reports, but only 44% positive results in Registered Reports. The difference remained nearly as large when direct replications were excluded from the analysis (96% vs 50% positive results). This large gap suggests that psychologists underreport negative results to an extent that threatens cumulative science. Although our study did not directly test the effectiveness of Registered Reports at reducing bias, these results show that the introduction of Registered Reports has led to a much larger proportion of negative results appearing in the published literature compared to standard reports.","tags":["Publication Bias","Publishing"],"title":"An excess of positive results: Comparing the standard Psychology literature with Registered Reports","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1689122843,"objectID":"b12d09389dfa38da6a556ada7a7de8a7","permalink":"https://forrt.org/curated_resources/an-exploratory-test-for-an-excess-of-sig/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/an-exploratory-test-for-an-excess-of-sig/","section":"curated_resources","summary":"Background The published clinical research literature may be distorted by the pursuit of statistically significant results. Purpose: We aimed to develop a test to explore biases stemming from the pursuit of nominal statistical significance. Methods The exploratory test evaluates whether there is a relative excess of formally significant findings in the published literature due to any reason (e.g., publication bias, selective analyses and outcome reporting, or fabricated data). The number of expected studies with statistically significant results is estimated and compared against the number of observed significant studies. The main application uses alpha = 0.05, but a range of alpha thresholds is also examined. Different values or prior distributions of the effect size are assumed. Given the typically low power (few studies per research question), the test may be best applied across domains of many meta-analyses that share common characteristics (interventions, outcomes, study populations, research environment). Results: We evaluated illustratively eight meta-analyses of clinical trials with 50 studies each and 10 meta-analyses of clinical efficacy for neuroleptic agents in schizophrenia; the 10 meta-analyses were also examined as a composite domain. Different results were obtained against commonly used tests of publication bias. We demonstrated a clear or possible excess of significant studies in 6 of 8 large meta analyses and in the wide domain of neuroleptic treatments. Limitations The proposed test is exploratory, may depend on prior assumptions, and should be applied cautiously. Conclusions An excess of significant findings may be documented in some clinical research fields","tags":[""],"title":"An exploratory test for an excess of significant findings","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1689122843,"objectID":"c1cdc0bf62b5c7d7d29a65521d3f58f9","permalink":"https://forrt.org/curated_resources/an-introduction-to-registered-replicatio/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/an-introduction-to-registered-replicatio/","section":"curated_resources","summary":"An article about an Introduction to Registered Replication Reports at Perspectives on Psychological Science","tags":["Reproducibility Crisis and Credibility Revolution","Open Science"],"title":"An Introduction to Registered Replication Reports at Perspectives on Psychological Science","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"bb9cb3c7bc8004edb334f847390bef2a","permalink":"https://forrt.org/curated_resources/an-introduction-to-registered-reports-fo/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/an-introduction-to-registered-reports-fo/","section":"curated_resources","summary":"In this webinar, Doctors David Mellor (Center for Open Science) and Stavroula Kousta (Nature Human Behavior) discuss the Registered Reports publishing workflow and the benefits it may bring to funders of research. Dr. Mellor details the workflow and what it is intended to do, and Dr. Kousta discusses the lessons learned at Nature Human Behavior from their efforts to implement Registered Reports as a journal.","tags":["Analysis","Data","Education","Materials","Open Scholarship Tools and Technologies","OSF","Publishing","Reproducibility","Research Data Management Tools","Researchers"],"title":"An Introduction to Registered Reports for the Research Funder Community","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"edcdec9405bef486716dd2e0efdf69bb","permalink":"https://forrt.org/curated_resources/an-open-investigation-of-the-reproducibi/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/an-open-investigation-of-the-reproducibi/","section":"curated_resources","summary":"It is widely believed that research that builds upon previously published findings has reproduced the original work. However, it is rare for researchers to perform or publish direct replications of existing results. The Reproducibility Project: Cancer Biology is an open investigation of reproducibility in preclinical cancer biology research. We have identified 50 high impact cancer biology articles published in the period 2010-2012, and plan to replicate a subset of experimental results from each article. A Registered Report detailing the proposed experimental designs and protocols for each subset of experiments will be peer reviewed and published prior to data collection. The results of these experiments will then be published in a Replication Study. The resulting open methodology and dataset will provide evidence about the reproducibility of high-impact results, and an opportunity to identify predictors of reproducibility.","tags":["Data","Methodology","Open Science","Replication","Reproducibility","Reproducibility Project: Cancer Biology","Research Methods"],"title":"An open investigation of the reproducibility of cancer biology research","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"6210551b416aa78769883664ef2fc911","permalink":"https://forrt.org/curated_resources/an-open-science-mri-database-of-over-100/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/an-open-science-mri-database-of-over-100/","section":"curated_resources","summary":"We provide a neuroimaging database consisting of 102 synaesthetic brains using state-of-the-art 3 T MRI protocols from the Human Connectome Project (HCP) which is freely available to researchers. This database consists of structural (T1- and T2-weighted) images together with approximately 24 minutes of resting state data per participant. These protocols are designed to be inter-operable and reproducible so that others can add to the dataset or directly compare it against other normative or special samples. In addition, we provide a ‘deep phenotype’ of our sample which includes detailed information about each participant’s synaesthesia together with associated clinical and cognitive measures. This behavioural dataset, which also includes data from (N = 109) non-synaesthetes, is of importance in its own right and is openly available.","tags":["Cognitive Neuroscience","Human Behaviour"],"title":"An Open Science MRI Database of over 100 Synaesthetic Brains and Accompanying Deep Phenotypic Information","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"8ca3c2da831ae70200ab2709e95f5269","permalink":"https://forrt.org/curated_resources/an-open-science-primer-for-social-scient/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/an-open-science-primer-for-social-scient/","section":"curated_resources","summary":"“Open Science” has become a buzzword in academic circles. However, exactly what it means, why you should care about it, and – most importantly – how it can be put into practice is often not very clear to researchers. In this session of the SSDL, we will provide a brief tour d'horizon of Open Science in which we touch on all of these issues and by which we hope to equip you with a basic understanding of Open Science and a practical tool kit to help you make your research more open to other researchers and the larger interested public. Throughout the presentation, we will focus on giving you an overview of tools and services that can help you open up your research workflow and your publications, all the way from enhancing the reproducibility of your research and making it more collaborative to finding outlets which make the results of your work accessible to everyone. Absolutely no prior experience with open science is required to participate in this talk which should lead into an open conversation among us as a community about the best practices we can and should follow for a more open social science.","tags":["Data","Materials","Open Access Policies","Open Scholarship Guidelines","Policy","Publishing","Reproducibility","Researchers"],"title":"An Open Science Primer for Social Scientists","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"75259ffa94d3285cdd533d441ed2a26b","permalink":"https://forrt.org/curated_resources/an-open-source-pharma-roadmap/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/an-open-source-pharma-roadmap/","section":"curated_resources","summary":"In an Essay, Matthew Todd and colleagues discuss an open source approach to drug development. This Essay outlines how open source methods of working could be applied to the discovery and development of new medicines. There are many potential advantages of an open source approach, such as improved efficiency, the quality and relevance of the research, and wider participation by the scientific and patient communities; a blend of traditional and innovative financing mechanisms will have to be adopted. To evaluate properly the effectiveness of an open source methodology and its potential as an alternative model of drug discovery and development, we recommend that new projects be trialed and existing projects scaled up. Where we stand The scientific and medical community has discovered and developed many groundbreaking medicines that have had a major impact on public health. However, drug development is challenged by a widening gap between health needs and the pharmaceutical industry’s motives and business model, alongside a decrease in efficiency per research dollar spent in medicinal product research and development (R\u0026D), a trend known colloquially as Eroom’s Law. Such fundamental challenges result in frequent high-level calls for new initiatives to develop therapeutics and bring them to market. These include market push and pull mechanisms such as priority review vouchers, advance market commitments, and public R\u0026D funding. New organizational models have also emerged, including public–private partnerships (PPPs) and not-for-profit product development partnerships (PDPs) (for example, the Drugs for Neglected Diseases Initiative [DNDi], the Medicines for Malaria Venture [MMV], and the Global Alliance for Tuberculosis Drug Development [TB Alliance]) that often apply a full “de-linkage” model in which the price of medicines and the cost of R\u0026D are uncoupled.","tags":["Drug Discovery","Drug Research and Development","Finance","Global Health","Open Source Drug Discovery","Open Source Software","Public and Occupational Health","Tuberculosis"],"title":"An open source pharma roadmap","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"1b691b143e64bb92a976572c390ce6ab","permalink":"https://forrt.org/curated_resources/analisis-y-visualizacion-de-datos-usando/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/analisis-y-visualizacion-de-datos-usando/","section":"curated_resources","summary":"Python es un lenguaje de programación general que es útil para escribir scripts para trabajar con datos de manera efectiva y reproducible. Esta es una introducción a Python diseñada para participantes sin experiencia en programación. Estas lecciones pueden enseñarse en un día (~ 6 horas). Las lecciones empiezan con información básica sobre la sintaxis de Python, la interface de Jupyter Notebook, y continúan con cómo importar archivos CSV, usando el paquete Pandas para trabajar con DataFrames, cómo calcular la información resumen de un DataFrame, y una breve introducción en cómo crear visualizaciones. La última lección demuestra cómo trabajar con bases de datos directamente desde Python. Nota: los datos no han sido traducidos de la versión original en inglés, por lo que los nombres de variables se mantienen en inglés y los números de cada observación usan la sintaxis de habla inglesa (coma separador de miles y punto separador de decimales).","tags":["Analysis","Data","Education","Jupyter Notebooks","Open Scholarship Tools and Technologies","Python","Reproducibility","Research Data Management Tools","Researchers"],"title":"Análisis y visualización de datos usando Python","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"21e0545314bfd5073af1134a66a676d4","permalink":"https://forrt.org/curated_resources/analysis-of-open-data-and-computational/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/analysis-of-open-data-and-computational/","section":"curated_resources","summary":"Ongoing technological developments have made it easier than ever before for scientists to share their data, materials, and analysis code. Sharing data and analysis code makes it easier for other researchers to re-use or check published research. These benefits will only emerge if researchers can reproduce the analysis reported in published articles, and if data is annotated well enough so that it is clear what all variables mean. Because most researchers have not been trained in computational reproducibility, it is important to evaluate current practices to identify practices that can be improved. We examined data and code sharing, as well as computational reproducibility of the main results, without contacting the original authors, for Registered Reports published in the psychological literature between 2014 and 2018. Of the 62 articles that met our inclusion criteria, data was available for 40 articles, and analysis scripts for 37 articles. For the 35 articles that shared both data and code and performed analyses in SPSS, R, Python, MATLAB, or JASP, we could run the scripts for 31 articles, and reproduce the main results for 20 articles. Although the articles that shared both data and code (35 out of 62, or 56%) and articles that could be computationally reproduced (20 out of 35, or 57%) was relatively high compared to other studies, there is clear room for improvement. We provide practical recommendations based on our observations, and link to examples of good research practices in the papers we reproduced.","tags":["Analysis","Data","Materials","Open Data","Publishing","Registered Reports","Reproducibility"],"title":"Analysis of Open Data and Computational Reproducibility in Registered Reports in Psychology","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1721390377,"objectID":"f63c0c55239684ae22f34d29c73b0b78","permalink":"https://forrt.org/glossary/english/analytic_flexibility/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/english/analytic_flexibility/","section":"glossary","summary":"","tags":null,"title":"Analytic Flexibility","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1635961284,"objectID":"96328f06e2f3172612b792a5104ced27","permalink":"https://forrt.org/glossary/vbeta/analytic-flexibility/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/vbeta/analytic-flexibility/","section":"glossary","summary":"","tags":null,"title":"Analytic Flexibility","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1720865851,"objectID":"9861ed23d42da8b26849b336a8e737b0","permalink":"https://forrt.org/glossary/german/analytic_flexibility/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/german/analytic_flexibility/","section":"glossary","summary":"","tags":null,"title":"Analytic Flexibility (Analytische Flexibilität)","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1689122843,"objectID":"2d54736bf85470cb85f7dceb00caff1b","permalink":"https://forrt.org/curated_resources/analytic-review-as-a-solution-to-the-mis/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/analytic-review-as-a-solution-to-the-mis/","section":"curated_resources","summary":"In this article, we propose analytic review (AR) as a solution to the problem of misreporting statistical results in psychological science. AR requires authors submitting manuscripts for publication to also submit the data file and syntax used during analyses. Regular reviewers or statistical experts then review reported analyses in order to verify that the analyses reported were actually conducted and that the statistical values are accurately reported. We begin by describing the problem of misreporting in psychology and introduce the basic AR process. We then highlight both primary and secondary benefits of adopting AR and describe different permutations of the AR system, each of which has its own strengths and limitations. We conclude by attempting to dispel three anticipated concerns about AR: that it will increase the workload placed on scholars, that it will infringe on the traditional peer-review process, and that it will hurt the image of the discipline of psychology. Although implementing AR will add one more step to the bureaucratic publication process, we believe it can be implemented in an efficient manner that would greatly assist in decreasing the frequency and impact of misreporting while also providing secondary benefits in other domains of scientific integrity.","tags":["Reproducibility Crisis and Credibility Revolution","Open Science"],"title":"Analytic Review as a Solution to the Misreporting of Statistical Results in Psychological Science","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"28d7635d1202dc7acdfc83b8fa0fcae3","permalink":"https://forrt.org/curated_resources/analytical-code-sharing-practices-in-bio/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/analytical-code-sharing-practices-in-bio/","section":"curated_resources","summary":"Data-driven computational analysis is becoming increasingly important in biomedical research, as the amount of data being generated continues to grow. However, the lack of practices of sharing research outputs, such as data, source code and methods, affects transparency and reproducibility of studies, which are critical to the advancement of science. Many published studies are not reproducible due to insufficient documentation, code, and data being shared. We conducted a comprehensive analysis of 453 manuscripts published between 2016-2021 and found that 50.1% of them fail to share the analytical code. Even among those that did disclose their code, a vast majority failed to offer additional research outputs, such as data. Furthermore, only one in ten papers organized their code in a structured and reproducible manner. We discovered a significant association between the presence of code availability statements and increased code availability (p=2.71×10−9). Additionally, a greater proportion of studies conducting secondary analyses were inclined to share their code compared to those conducting primary analyses (p=1.15*10−07). In light of our findings, we propose raising awareness of code sharing practices and taking immediate steps to enhance code availability to improve reproducibility in biomedical research. By increasing transparency and reproducibility, we can promote scientific rigor, encourage collaboration, and accelerate scientific discoveries. We must prioritize open science practices, including sharing code, data, and other research products, to ensure that biomedical research can be replicated and built upon by others in the scientific community.","tags":["Open Code","Open Data","Biomedical Research","Replication","Reproducibility"],"title":"Analytical code sharing practices in biomedical research","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"836a8ab8ccebabb0836d148c9c512bea","permalink":"https://forrt.org/curated_resources/analyzing-education-data-with-open-scien/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/analyzing-education-data-with-open-scien/","section":"curated_resources","summary":"This workshop demonstrates how using R can advance open science practices in education. We focus on R and RStudio because it is an increasingly widely-used programming language and software environment for data analysis with a large supportive community. We present: a) general strategies for using R to analyze educational data and b) accessing and using data on the Open Science Framework (OSF) with R via the osfr package. This session is for those both new to R and those with R experience looking to learn more about strategies and workflows that can help to make it possible to analyze data in a more transparent, reliable, and trustworthy way. Access the workshop slides and supplemental information at https://osf.io/vtcak/​. \n\nResources:\n\n1) Download R: https://www.r-project.org/​\n2) Download RStudio (a tool that makes R easier to use): https://rstudio.com/products/rstudio/...​\n3) R for Data Science (a free, digital book about how to do data science with R): https://r4ds.had.co.nz/​\n4) Tidyverse R packages for data science: https://www.tidyverse.org/​\n5) RMarkdown from RStudio (including info about R Notebooks): https://rmarkdown.rstudio.com/​\n6) Data Science in Education Using R: https://datascienceineducation.com/​","tags":["Data Analysis Tools","Education Research","Open Data","Open Science","Open Science Framework","OSF","Osfr","R for Researchers","Rstats","Statistical Programming"],"title":"Analyzing Education Data with Open Science Best Practices, R, and OSF","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1721390377,"objectID":"3bbeb115b217261d16efb4419d850827","permalink":"https://forrt.org/glossary/english/anonymity/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/english/anonymity/","section":"glossary","summary":"","tags":null,"title":"Anonymity","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1635961284,"objectID":"958c547cc4c9d435e293e0dd7c6d60dd","permalink":"https://forrt.org/glossary/vbeta/anonymity/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/vbeta/anonymity/","section":"glossary","summary":"","tags":null,"title":"Anonymity","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1720865851,"objectID":"8572b9f5fefeea8c2a0096439c1dd867","permalink":"https://forrt.org/glossary/german/anonymity/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/german/anonymity/","section":"glossary","summary":"","tags":null,"title":"Anonymity (Anonymität)","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1689122843,"objectID":"6c0bd8c3ffb8abb3adb665dfd9814c63","permalink":"https://forrt.org/curated_resources/applied-multiple-regression-correlation/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/applied-multiple-regression-correlation/","section":"curated_resources","summary":"This classic text on multiple regression is noted for its nonmathematical, applied, and data-analytic approach. Readers profit from its verbal-conceptual exposition and frequent use of examples.","tags":["Book"],"title":"Applied Multiple Regression/Correlation Analysis for the Behavioral Sciences","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"052835c2660f88cb0028e31d14f96f1b","permalink":"https://forrt.org/curated_resources/applied-open-science-talk-gsa-2020/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/applied-open-science-talk-gsa-2020/","section":"curated_resources","summary":"Applied Open Science Talk GSA 2020","tags":["Aging","Aging Science","Gerontology","Open Science"],"title":"Applied Open Science Talk GSA 2020","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"22a9ce1dc2d853831700bd6c95c19cd6","permalink":"https://forrt.org/curated_resources/are-choices-based-on-conditional-or-conj/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/are-choices-based-on-conditional-or-conj/","section":"curated_resources","summary":"In this study, we examined participants' choice behavior in a sequential risk-taking task. We were especially interested in the extent to which participants focus on the immediate next choice or consider the entire choice sequence. To do so, we inspected whether decisions were either based on conditional probabilities (e.g., being successful on the immediate next trial) or on conjunctive probabilities (of being successful several times in a row). The results of five experiments with a simplified nine-card Columbia Card Task and a CPT-model analysis show that participants' choice behavior can be described best by a mixture of the two probability types. Specifically, for their first choice, the participants relied on conditional probabilities, whereas subsequent choices were based on conjunctive probabilities. This strategy occurred across different start conditions in which more or less cards were already presented face up. Consequently, the proportion of risky choices was substantially higher when participants started from a state with some cards facing up, compared with when they arrived at that state starting from the very beginning. The results, alternative accounts, and implications are discussed.","tags":["Columbia Card Task (CCT)","Conditional Probability","Conjunctive Probability","Data","Dependent Events","Sequential Risk-Taking"],"title":"Are choices based on conditional or conjunctive probabilities in a sequential risk-taking task?","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"24e3a7f9159ca1ff74168973e8219d28","permalink":"https://forrt.org/curated_resources/are-preregistration-and-registered-repor/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/are-preregistration-and-registered-repor/","section":"curated_resources","summary":"Preregistration of study hypotheses has been proclaimed as the “revolution”1 to curb publication bias and data dredging such as p-hacking and HARKing, i.e., hypothesizing after results are known. Many publishers encourage preregistration in their submission policies,2 and a similar trend is being seen in registered reports, which are preregistration of a study plan, prereview, and acceptance for publication, all before data/results are seen.3 Sharp growth of platforms to facilitate preregistration, e.g., the Open Science Framework, has ensued.\n\nA recent analysis on preregistered studies showed an increase in reports of null findings,4 which should be a reassurance against the specter of p-hacking. However, new evidence indicates that preregistration is being circumvented,5 with authors often deviating from registered plans and not fully disclosing it.","tags":["Preregistration","Registered Reports","Hacking","Editorial"],"title":"Are Preregistration and Registered Reports Vulnerable to Hacking?","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"72ea484190a08a9638acfd68f711ad69","permalink":"https://forrt.org/curated_resources/are-psychology-journals-anti-replication/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/are-psychology-journals-anti-replication/","section":"curated_resources","summary":"Recent research in psychology has highlighted a number of replication problems in the discipline, with publication bias – the preference for publishing original and positive results, and a resistance to publishing negative results and replications- identified as one reason for replication failure. However, little empirical research exists to demonstrate that journals explicitly refuse to publish replications. We reviewed the instructions to authors and the published aims of 1151 psychology journals and examined whether they indicated that replications were permitted and accepted. We also examined whether journal practices differed across branches of the discipline, and whether editorial practices differed between low and high impact journals. Thirty three journals (3%) stated in their aims or instructions to authors that they accepted replications. There was no difference between high and low impact journals. The implications of these findings for psychology are discussed.","tags":["JOURNAL EDITORIAL PRACTICES","P-Hacking","Psychology","Publication Bias","Replication"],"title":"Are Psychology Journals Anti-replication? A Snapshot of Editorial Practices","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1689122843,"objectID":"39517b11e58787ef67551ded9c49e720","permalink":"https://forrt.org/curated_resources/are-we-wasting-a-good-crisis-the-availab/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/are-we-wasting-a-good-crisis-the-availab/","section":"curated_resources","summary":"To study the availability of psychological research data, we requested data from 394 papers, published in all issues of four APA journals in 2012. We found that 38% of the researchers sent their data immediately or after reminders. These findings are in line with estimates of the willingness to share data in psychology from the recent or remote past. Although the recent crisis of confidence that shook psychology has highlighted the importance of open research practices, and technical developments have greatly facilitated data sharing, our findings make clear that psychology is nowhere close to being an open science","tags":["Reproducibility Crisis and Credibility Revolution","Open Science"],"title":"Are We Wasting a Good Crisis? The Availability of Psychological Research Data after the Storm","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"f43fab8f62d45181ce2f6508c54cba4d","permalink":"https://forrt.org/curated_resources/ariadne-a-scientific-navigator-to-find-y/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/ariadne-a-scientific-navigator-to-find-y/","section":"curated_resources","summary":"Performing high-quality research is a challenging endeavor, especially for early career researchers (ECRs). Most research is characterized by an experiential learning approach, which can be time-consuming, error-prone, and frustrating. While most institutions provide a selection of resources to help researchers with their research projects, these resources are often expensive, spread out, hard to find, and difficult to compare with one another in terms of reliability, validity, usability, and practicability. A comprehensive overview of resources that are useful for early career researchers and their supervisors is missing. To address this issue, we created ARIADNE – a living and interactive resource navigator that helps to use and search a dynamically updated database of resources. The open-access database covers a constantly growing list of resources that are useful for each step of a research project, ranging from the planning and designing of study, over the collection and analysis of the data, to the writing and disseminating of findings. By introducing ARIADNE to the research community, we provide 1) a step-by-step guide on how to perform a research project, 2) an overview on resources that are useful at the different steps of such a project, and 3) a glossary of most common terms surrounding the research cycle. By focusing on open-access and open-source resources, we level the playing field for researchers from underprivileged countries or institutions, thereby facilitating open, fair, and reproducible research in the field of neuroscience.","tags":["Data visualization","Code","Reproducibility","Education","Learning","Resource","Tool","Research cycle","Research process","Early career researcher"],"title":"ARIADNE – a scientific navigator to find your way through the resource labyrinth","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"7f056c84f0026fd78a041edc772359ca","permalink":"https://forrt.org/curated_resources/arrested-theory-development-the-misguide/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/arrested-theory-development-the-misguide/","section":"curated_resources","summary":"Science progresses by finding and correcting problems in theories. Good theories are those that help facilitate this process by being hard to vary: They explain what they are supposed to explain, they are consistent with other good theories, and they are not easily adaptable to explain anything. Here we argue that, rather than a lack of distinction between exploratory and confirmatory research, an abundance of flexible theories is a better explanation for the current replicability problems of psychology. We also explain why popular methods-oriented solutions fail to address the real problem of flexibility. Instead, we propose that a greater emphasis on theory criticism by argument might improve replicability.","tags":["Philosophy of Science","Theory Development","Confirmatory Research","Exploratory Research","Preregistration","Direct Replication"],"title":"Arrested Theory Development: The Misguided Distinction Between Exploratory and Confirmatory Research","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1721390377,"objectID":"18b1c7d05b3b3ede9626e75672037f44","permalink":"https://forrt.org/glossary/english/arrive_guidelines/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/english/arrive_guidelines/","section":"glossary","summary":"","tags":null,"title":"ARRIVE Guidelines","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1635961284,"objectID":"e8986ba4f58da4413ae8aaa75ed23376","permalink":"https://forrt.org/glossary/vbeta/arrive-guidelines/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/vbeta/arrive-guidelines/","section":"glossary","summary":"","tags":null,"title":"ARRIVE Guidelines","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1720865851,"objectID":"2b7180319e1d26423a6e895e8249007e","permalink":"https://forrt.org/glossary/german/arrive_guidelines_/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/german/arrive_guidelines_/","section":"glossary","summary":"","tags":null,"title":"ARRIVE Guidelines  (ARRIVE Leitlinien)","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"aa84ccdc9719f55ab8b1d19750eba8a1","permalink":"https://forrt.org/curated_resources/arrive-has-not-arrived-support-for-the-a/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/arrive-has-not-arrived-support-for-the-a/","section":"curated_resources","summary":"Poor research reporting is a major contributing factor to low study reproducibility, financial and animal waste. The ARRIVE (Animal Research: Reporting of In Vivo Experiments) guidelines were developed to improve reporting quality and many journals support these guidelines. The influence of this support is unknown. We hypothesized that papers published in journals supporting the ARRIVE guidelines would show improved reporting compared with those in non-supporting journals. In a retrospective, observational cohort study, papers from 5 ARRIVE supporting (SUPP) and 2 non-supporting (nonSUPP) journals, published before (2009) and 5 years after (2015) the ARRIVE guidelines, were selected. Adherence to the ARRIVE checklist of 20 items was independently evaluated by two reviewers and items assessed as fully, partially or not reported. Mean percentages of items reported were compared between journal types and years with an unequal variance t-test. Individual items and sub-items were compared with a chi-square test. From an initial cohort of 956, 236 papers were included: 120 from 2009 (SUPP; n = 52, nonSUPP; n = 68), 116 from 2015 (SUPP; n = 61, nonSUPP; n = 55). The percentage of fully reported items was similar between journal types in 2009 (SUPP: 55.3 ± 11.5% [SD]; nonSUPP: 51.8 ± 9.0%; p = 0.07, 95% CI of mean difference -0.3–7.3%) and 2015 (SUPP: 60.5 ± 11.2%; nonSUPP; 60.2 ± 10.0%; p = 0.89, 95%CI -3.6–4.2%). The small increase in fully reported items between years was similar for both journal types (p = 0.09, 95% CI -0.5–4.3%). No paper fully reported 100% of items on the ARRIVE checklist and measures associated with bias were poorly reported. These results suggest that journal support for the ARRIVE guidelines has not resulted in a meaningful improvement in reporting quality, contributing to ongoing waste in animal research.","tags":["Adverse Events","Analgesics","Anesthetics","Animal Slaughter","Animal Welfare","Finance","Publishing","Reporting Bias","Reporting Guidelines"],"title":"ARRIVE has not ARRIVEd: Support for the ARRIVE (Animal Research: Reporting of in vivo Experiments) guidelines does not improve the reporting quality of papers in animal welfare, analgesia or anesthesia","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1721390377,"objectID":"1777f0a659da2b828088e25878529169","permalink":"https://forrt.org/glossary/english/article_processing_charge/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/english/article_processing_charge/","section":"glossary","summary":"","tags":null,"title":"Article Processing Charge (APC)","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1635961284,"objectID":"50262caa248cd0dee9dc9e01e602b159","permalink":"https://forrt.org/glossary/vbeta/article-processing-charge-apc/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/vbeta/article-processing-charge-apc/","section":"glossary","summary":"","tags":null,"title":"Article Processing Charge (APC)","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1720865851,"objectID":"1c9b76245a010d0681eb6ab6fd71f8e0","permalink":"https://forrt.org/glossary/german/article_processing_charge/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/german/article_processing_charge/","section":"glossary","summary":"","tags":null,"title":"Article Processing Charge (Artikelverarbeitungsgebühr)","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"09c753abbe133adfd0db9691e747daf9","permalink":"https://forrt.org/curated_resources/artificial-intelligence-and-responsibili/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/artificial-intelligence-and-responsibili/","section":"curated_resources","summary":"Recent decades have witnessed tremendous progress in artificial intelligence and in the development of autonomous systems that rely on artificial intelligence. Critics, however, have pointed to the difficulty of allocating responsibility for the actions of an autonomous system, especially when the autonomous system causes harm or damage. The highly autonomous behavior of such systems, for which neither the programmer, the manufacturer, nor the operator seems to be responsible, has been suspected to generate responsibility gaps. This has been the cause of much concern. In this article, I propose a more optimistic view on artificial intelligence, raising two challenges for responsibility gap pessimists. First, proponents of responsibility gaps must say more about when responsibility gaps occur. Once we accept a difficult-to-reject plausibility constraint on the emergence of such gaps, it becomes apparent that the situations in which responsibility gaps occur are unclear. Second, assuming that responsibility gaps occur, more must be said about why we should be concerned about such gaps in the first place. I proceed by defusing what I take to be the two most important concerns about responsibility gaps, one relating to the consequences of responsibility gaps and the other relating to violations of jus in bello.","tags":["Artificial Intelligence","Responsibility","Ethics","Just in Bello"],"title":"Artificial intelligence and responsibility gaps: What is the problem?","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"4961d07c83974396274d5d435e26b44c","permalink":"https://forrt.org/curated_resources/asl-lexicon-and-reporting-recommendation/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/asl-lexicon-and-reporting-recommendation/","section":"curated_resources","summary":"The 2015 consensus statement published by the International Society for Magnetic Resonance in Medicine (ISMRM) Perfusion Study Group and the European Cooperation in Science and Technology ( COST) Action ASL in Dementia aimed to encourage the implementation of robust arterial spin labeling (ASL) perfusion MRI for clinical applications and promote consistency across scanner types, sites, and studies. Subsequently, the recommended 3D pseudo-continuous ASL sequence has been implemented by most major MRI manufacturers. However, ASL remains a rapidly and widely developing field, leading inevitably to further divergence of the technique and its associated terminology, which could cause confusion and hamper research reproducibility.\n\nOn behalf of the ISMRM Perfusion Study Group, and as part of the ISMRM Open Science Initiative for Perfusion Imaging (OSIPI), the ASL Lexicon Task Force has been working on the development of an ASL Lexicon and Reporting Recommendations for perfusion imaging and analysis, aiming to (1) develop standardized, consensus nomenclature and terminology for the broad range of ASL imaging techniques and parameters, as well as for the physiological constants required for quantitative analysis; and (2) provide a community-endorsed recommendation of the imaging parameters that we encourage authors to include when describing ASL methods in scientific reports/papers.\n\nIn this paper, the sequences and parameters in (pseudo-)continuous ASL, pulsed ASL, velocity-selective ASL, and multi-timepoint ASL for brain perfusion imaging are included. However, the content of the lexicon is not intended to be limited to these techniques, and this paper provides the foundation for a growing online inventory that will be extended by the community as further methods and improvements are developed and established.","tags":["Arterial Spin Labeling","Interoperability","Noninvasive","Perfusion Imaging","Reproducibility"],"title":"ASL lexicon and reporting recommendations: A consensus report from the ISMRM Open Science Initiative for Perfusion Imaging (OSIPI)","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"1bb94a242d5d97d862d95685b4458a97","permalink":"https://forrt.org/curated_resources/assessing-data-availability-and-research/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/assessing-data-availability-and-research/","section":"curated_resources","summary":"There is broad interest to improve the reproducibility of published research. We developed a survey tool to assess the availability of digital research artifacts published alongside peer-reviewed journal articles (e.g. data, models, code, directions for use) and reproducibility of article results. We used the tool to assess 360 of the 1,989 articles published by six hydrology and water resources journals in 2017. Like studies from other fields, we reproduced results for only a small fraction of articles (1.6% of tested articles) using their available artifacts. We estimated, with 95% confidence, that results might be reproduced for only 0.6% to 6.8% of all 1,989 articles. Unlike prior studies, the survey tool identified key bottlenecks to making work more reproducible. Bottlenecks include: only some digital artifacts available (44% of articles), no directions (89%), or all artifacts available but results not reproducible (5%). The tool (or extensions) can help authors, journals, funders, and institutions to self-assess manuscripts, provide feedback to improve reproducibility, and recognize and reward reproducible articles as examples for others.","tags":["Data","Reproducibility"],"title":"Assessing data availability and research reproducibility in hydrology and water resources","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"9c78d46552c97afaf8c19afddbcb2265","permalink":"https://forrt.org/curated_resources/association-between-trial-registration-a/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/association-between-trial-registration-a/","section":"curated_resources","summary":"To increase transparency in research, the International Committee of Medical Journal Editors required, in 2005, prospective registration of clinical trials as a condition to publication. However, many trials remain unregistered or retrospectively registered. We aimed to assess the association between trial prospective registration and treatment effect estimates. Methods This is a meta-epidemiological study based on all Cochrane reviews published between March 2011 and September 2014 with meta-analyses of a binary outcome including three or more randomised controlled trials published after 2006. We extracted trial general characteristics and results from the Cochrane reviews. For each trial, we searched for registration in the report’s full text, contacted the corresponding author if not reported and searched ClinicalTrials.gov and the International Clinical Trials Registry Platform in case of no response. We classified each trial as prospectively registered (i.e. registered before the start date); retrospectively registered, distinguishing trials registered before and after the primary completion date; and not registered. Treatment effect estimates of prospectively registered and other trials were compared by the ratio of odds ratio (ROR) (ROR \u003c1 indicates larger effects in trials not prospectively registered). Results We identified 67 meta-analyses (322 trials). Overall, 225/322 trials (70 %) were registered, 74 (33 %) prospectively and 142 (63 %) retrospectively; 88 were registered before the primary completion date and 54 after. Unregistered or retrospectively registered trials tended to show larger treatment effect estimates than prospectively registered trials (combined ROR = 0.81, 95 % CI 0.65–1.02, based on 32 contributing meta-analyses). Trials unregistered or registered after the primary completion date tended to show larger treatment effect estimates than those registered before this date (combined ROR = 0.84, 95 % CI 0.71–1.01, based on 43 contributing meta-analyses). Conclusions Lack of trial prospective registration may be associated with larger treatment effect estimates.","tags":["Bias","ClinicalTrials.gov","Meta-analysis","Meta-epidemiology","Metascience","Meta-Science","Preregistration","Publishing","Randomised Controlled Trial","Registration","Reproducibility"],"title":"Association between trial registration and treatment effect estimates: a meta-epidemiological study","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1689122843,"objectID":"618473fde8e90c455211e82897d55e0a","permalink":"https://forrt.org/curated_resources/at-what-sample-size-do-correlations-stab/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/at-what-sample-size-do-correlations-stab/","section":"curated_resources","summary":"Sample correlations converge to the population value with increasing sample size, but the estimates are often inaccurate in small samples. In this report we use Monte-Carlo simulations to determine the critical sample size from which on the magnitude of a correlation can be expected to be stable. The necessary sample size to achieve stable estimates for correlations depends on the effect size, the width of the corridor of stability (i.e., a corridor around the true value where deviations are tolerated), and the requested confidence that the trajectory does not leave this corridor any more. Results indicate that in typical scenarios the sample size should approach 250 for stable estimates.","tags":[""],"title":"At what sample size do correlations stabilize? ","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"f9884dd2a82d5732082cf98c0429acfe","permalink":"https://forrt.org/curated_resources/attitudes-towards-animal-study-registrie/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/attitudes-towards-animal-study-registrie/","section":"curated_resources","summary":"Objectives Prospective registration of animal studies has been suggested as a new measure to increase value and reduce waste in biomedical research. We sought to further explore and quantify animal researchers’ attitudes and preferences regarding animal study registries (ASRs). Design Cross-sectional online survey. Setting and participants We conducted a survey with three different samples representing animal researchers: i) corresponding authors from journals with high Eigenfactor, ii) a random Pubmed sample and iii) members of the CAMARADES network. Main outcome measures Perceived level of importance of different aspects of publication bias, the effect of ASRs on different aspects of research as well as the importance of different research types for being registered. Results The survey yielded responses from 413 animal researchers (response rate 7%). The respondents indicated, that some aspects of ASRs can increase administrative burden but could be outweighed by other aspects decreasing this burden. Animal researchers found it more important to register studies that involved animal species with higher levels of cognitive capabilities. The time frame for making registry entries publicly available revealed a strong heterogeneity among respondents, with the largest proportion voting for “access only after consent by the principal investigator” and the second largest proportion voting for “access immediately after registration”. Conclusions The fact that the more senior and experienced animal researchers participating in this survey clearly indicated the practical importance of publication bias and the importance of ASRs underscores the problem awareness across animal researchers and the willingness to actively engage in study registration if effective safeguards for the potential weaknesses of ASRs are put into place. To overcome the first-mover dilemma international consensus statements on how to deal with prospective registration of animal studies might be necessary for all relevant stakeholder groups including animal researchers, academic institutions, private companies, funders, regulatory agencies, and journals.","tags":["Animal Studies","Chi Square Tests","Medical Journals","Psychological Attitudes","Publication Ethics","Publishing","Survey Research","Surveys","Theft"],"title":"Attitudes towards animal study registries and their characteristics: An online survey of three cohorts of animal researchers","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"e495f56a3def5a632b26d5a7a40c876f","permalink":"https://forrt.org/curated_resources/authorization-of-animal-experiments-is-b/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/authorization-of-animal-experiments-is-b/","section":"curated_resources","summary":"Accumulating evidence indicates high risk of bias in preclinical animal research, questioning the scientific validity and reproducibility of published research findings. Systematic reviews found low rates of reporting of measures against risks of bias in the published literature (e.g., randomization, blinding, sample size calculation) and a correlation between low reporting rates and inflated treatment effects. That most animal research undergoes peer review or ethical review would offer the possibility to detect risks of bias at an earlier stage, before the research has been conducted. For example, in Switzerland, animal experiments are licensed based on a detailed description of the study protocol and a harm–benefit analysis. We therefore screened applications for animal experiments submitted to Swiss authorities (n = 1,277) for the rates at which the use of seven basic measures against bias (allocation concealment, blinding, randomization, sample size calculation, inclusion/exclusion criteria, primary outcome variable, and statistical analysis plan) were described and compared them with the reporting rates of the same measures in a representative sub-sample of publications (n = 50) resulting from studies described in these applications. Measures against bias were described at very low rates, ranging on average from 2.4% for statistical analysis plan to 19% for primary outcome variable in applications for animal experiments, and from 0.0% for sample size calculation to 34% for statistical analysis plan in publications from these experiments. Calculating an internal validity score (IVS) based on the proportion of the seven measures against bias, we found a weak positive correlation between the IVS of applications and that of publications (Spearman’s rho = 0.34, p = 0.014), indicating that the rates of description of these measures in applications partly predict their rates of reporting in publications. These results indicate that the authorities licensing animal experiments are lacking important information about experimental conduct that determines the scientific validity of the findings, which may be critical for the weight attributed to the benefit of the research in the harm–benefit analysis. Similar to manuscripts getting accepted for publication despite poor reporting of measures against bias, applications for animal experiments may often be approved based on implicit confidence rather than explicit evidence of scientific rigor. Our findings shed serious doubt on the current authorization procedure for animal experiments, as well as the peer-review process for scientific publications, which in the long run may undermine the credibility of research. Developing existing authorization procedures that are already in place in many countries towards a preregistration system for animal research is one promising way to reform the system. This would not only benefit the scientific validity of findings from animal experiments but also help to avoid unnecessary harm to animals for inconclusive research.","tags":["Experimental Design","Language","Mammals","Peer Review","Reproducibility","Research Reporting Guidelines","Research Validity","Rodents"],"title":"Authorization of Animal Experiments Is Based on Confidence Rather than Evidence of Scientific Rigor","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1721390377,"objectID":"a2f8339f509aba0b751f43d094380f0f","permalink":"https://forrt.org/glossary/english/authorship/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/english/authorship/","section":"glossary","summary":"","tags":null,"title":"Authorship","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1635961284,"objectID":"e94f808ad64780886238fc3b4bbc361f","permalink":"https://forrt.org/glossary/vbeta/authorship/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/vbeta/authorship/","section":"glossary","summary":"","tags":null,"title":"Authorship","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1720865851,"objectID":"6581a0cc4555c701c05219f7a73ec5b4","permalink":"https://forrt.org/glossary/german/authorship/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/german/authorship/","section":"glossary","summary":"","tags":null,"title":"Authorship (Autor:innenschaft)","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"2e160ed79c4c623d12f8fb76b08ba37d","permalink":"https://forrt.org/curated_resources/authorship-practices-must-evolve-to-supp/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/authorship-practices-must-evolve-to-supp/","section":"curated_resources","summary":"Journal authorship practices have not sufficiently evolved to reflect the way research is now done. Improvements to support teams, collaboration, and open science are urgently needed.","tags":["Research Assessment","Open Science","Taxonomy","Medical Journals","Metadata","Centrality","Citation Analysis","Computer Software"],"title":"Authorship practices must evolve to support collaboration and open science","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"c8f8a675bc3721a71beea698828139be","permalink":"https://forrt.org/curated_resources/autistic-community-and-the-neurodiversit/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/autistic-community-and-the-neurodiversit/","section":"curated_resources","summary":"A book about the autistic community and the neurodivergent movement","tags":["Diversity","Equity","Inclusion","Neurodiversity","Autism"],"title":"Autistic community and the Neurodiversity movement: Stories from the Frontline","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"a33c1d155e5a1307cdddb8f1edd60fce","permalink":"https://forrt.org/curated_resources/automation-and-make/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/automation-and-make/","section":"curated_resources","summary":"A Software Carpentry lesson to learn how to use Make Make is a tool which can run commands to read files, process these files in some way, and write out the processed files. For example, in software development, Make is used to compile source code into executable programs or libraries, but Make can also be used to: run analysis scripts on raw data files to get data files that summarize the raw data; run visualization scripts on data files to produce plots; and to parse and combine text files and plots to create papers. Make is called a build tool - it builds data files, plots, papers, programs or libraries. It can also update existing files if desired. Make tracks the dependencies between the files it creates and the files used to create these. If one of the original files (e.g. a data file) is changed, then Make knows to recreate, or update, the files that depend upon this file (e.g. a plot). There are now many build tools available, all of which are based on the same concepts as Make.","tags":["Analysis","Data","Education","Make","Open Scholarship Tools and Technologies","Reproducibility","Research Data Management Tools","Researchers"],"title":"Automation and Make","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1721390377,"objectID":"97b2c4d56137ac46e0a1c0baad53ae38","permalink":"https://forrt.org/glossary/english/auxiliary_hypothesis/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/english/auxiliary_hypothesis/","section":"glossary","summary":"","tags":null,"title":"Auxiliary Hypothesis","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1635961284,"objectID":"cec17ad59ebfc18f9cff5c40e5a2f39e","permalink":"https://forrt.org/glossary/vbeta/auxiliary-hypothesis/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/vbeta/auxiliary-hypothesis/","section":"glossary","summary":"","tags":null,"title":"Auxiliary Hypothesis","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1720865851,"objectID":"d10423e75785177bc0be57465a233057","permalink":"https://forrt.org/glossary/german/auxiliary_hypothesis/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/german/auxiliary_hypothesis/","section":"glossary","summary":"","tags":null,"title":"Auxiliary Hypothesis (Hilfshypothese)","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"708395bbc59446c39191f6357b95f861","permalink":"https://forrt.org/curated_resources/awesome-open-science-resources/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/awesome-open-science-resources/","section":"curated_resources","summary":"Scientific data and tools should, as much as possible, be free as in beer and free as in freedom. The vast majority of science today is paid for by taxpayer-funded grants; at the same time, the incredible successes of science are strong evidence for the benefit of collaboration in knowledgable pursuits. Within the scientific academy, sharing of expertise, data, tools, etc. is prolific, but only recently with the rise of the Open Access movement has this sharing come to embrace the public. Even though most research data is never shared, both the public and even scientists in their own fields are often unaware of just much data, tools, and other resources are made freely available for analysis! This list is a small attempt at bringing light to data repositories and computational science tools that are often siloed according to each scientific discipline, in the hopes of spurring along both public and professional contributions to science.","tags":["Data","Materials","Policy","Publishing","Reproducibility","Researchers"],"title":"Awesome Open Science Resources","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"56cec45a55b6914d161276969fb6e36c","permalink":"https://forrt.org/curated_resources/awesome-reproducible-research/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/awesome-reproducible-research/","section":"curated_resources","summary":"A curated list of reproducible research case studies, projects, tutorials, and media","tags":["Case Studies","Tools","Ontologies","Courses","Exemplars","Runnable Papers","Organizations"],"title":"Awesome Reproducible Research","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1689122843,"objectID":"810e52c0e4baafb97a8d15fec158b069","permalink":"https://forrt.org/curated_resources/bad-science-blog-on-the-guardian-webpage/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/bad-science-blog-on-the-guardian-webpage/","section":"curated_resources","summary":"Articles about Bad Science","tags":["Blog","Reproducibility Knowledge","Open Science"],"title":"Bad Science blog on the Guardian webpages","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1689122843,"objectID":"7a2bff0d603f42235b1e632926cdb27f","permalink":"https://forrt.org/curated_resources/bad-science-log-post-guardian/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/bad-science-log-post-guardian/","section":"curated_resources","summary":"Blogs about bad science","tags":["Blog"],"title":"Bad Science log, post-Guardian","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1635961284,"objectID":"4c47b804fbaf08208196a40e53bdf66a","permalink":"https://forrt.org/glossary/vbeta/badges-open-science/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/vbeta/badges-open-science/","section":"glossary","summary":"","tags":null,"title":"Badges (Open Science)","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1721390377,"objectID":"1b3907714843c5cf1aa28aa660803a93","permalink":"https://forrt.org/glossary/english/badges/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/english/badges/","section":"glossary","summary":"","tags":null,"title":"Badges (Open Science)","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1720865851,"objectID":"167f88ecdca44dce6816fb927359627f","permalink":"https://forrt.org/glossary/german/badges/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/german/badges/","section":"glossary","summary":"","tags":null,"title":"Badges (Open Science) (Open Science Abzeichen)","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"2351b4231a4e7fdd7d84c2bd37b43f01","permalink":"https://forrt.org/curated_resources/badges-for-sharing-data-and-code-at-bios/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/badges-for-sharing-data-and-code-at-bios/","section":"curated_resources","summary":"Background: The reproducibility policy at the journal Biostatistics rewards articles with badges for data and code sharing. This study investigates the effect of badges at increasing reproducible research. Methods: The setting of this observational study is the Biostatistics and Statistics in Medicine (control journal) online research archives. The data consisted of 240 randomly sampled articles from 2006 to 2013 (30 articles per year) per journal. Data analyses included: plotting probability of data and code sharing by article submission date, and Bayesian logistic regression modelling. Results: The probability of data sharing was higher at Biostatistics than the control journal but the probability of code sharing was comparable for both journals. The probability of data sharing increased by 3.9 times (95% credible interval: 1.5 to 8.44 times, p-value probability that sharing increased: 0.998) after badges were introduced at Biostatistics. On an absolute scale, this difference was only a 7.6% increase in data sharing (95% CI: 2 to 15%, p-value: 0.998). Badges did not have an impact on code sharing at the journal (mean increase: 1 time, 95% credible interval: 0.03 to 3.58 times, p-value probability that sharing increased: 0.378). 64% of articles at Biostatistics that provide data/code had broken links, and at Statistics in Medicine, 40%; assuming these links worked only slightly changed the effect of badges on data (mean increase: 6.7%, 95% CI: 0.0% to 17.0%, p-value: 0.974) and on code (mean increase: -2%, 95% CI: -10.0 to 7.0%, p-value: 0.286). Conclusions: The effect of badges at Biostatistics was a 7.6% increase in the data sharing rate, 5 times less than the effect of badges at Psychological Science. Though badges at Biostatistics did not impact code sharing, and had a moderate effect on data sharing, badges are an interesting step that journals are taking to incentivise and promote reproducible research.","tags":["Data"],"title":"Badges for sharing data and code at Biostatistics: an observational study","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1689122843,"objectID":"038c6d6c93bd3604bb785eb31d6c327f","permalink":"https://forrt.org/curated_resources/badges-to-acknowledge-open-practices-a-s/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/badges-to-acknowledge-open-practices-a-s/","section":"curated_resources","summary":"Beginning January 2014, Psychological Science gave authors the opportunity to signal open data and materials if they qualified for badges that accompanied published articles. Before badges, less than 3% of Psychological Science articles reported open data. After badges, 23% reported open data, with an accelerating trend; 39% reported open data in the first half of 2015, an increase of more than an order of magnitude from baseline. There was no change over time in the low rates of data sharing among comparison journals. Moreover, reporting openness does not guarantee openness. When badges were earned, reportedly available data were more likely to be actually available, correct, usable, and complete than when badges were not earned. Open materials also increased to a weaker degree, and there was more variability among comparison journals. Badges are simple, effective signals to promote open practices and improve preservation of data and materials by using independent repositories.","tags":["Reproducibility Crisis and Credibility Revolution","Open Science"],"title":"Badges to Acknowledge Open Practices: A Simple, Low-Cost, Effective Method for Increasing Transparency","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1689122843,"objectID":"b7818689da26041a9fc1d85e2925fb22","permalink":"https://forrt.org/curated_resources/bargain-basement-bayes/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/bargain-basement-bayes/","section":"curated_resources","summary":"A blog about bayesian Statistics","tags":["Blog"],"title":"Bargain Basement Bayes","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1689122843,"objectID":"d706ae6c4d5c04affc9522066c6c980c","permalink":"https://forrt.org/curated_resources/bayes-factor/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/bayes-factor/","section":"curated_resources","summary":"Bayes factors are somewhat essential to Bayesian statistics. Tony O’Hagan explains their basics","tags":[""],"title":"Bayes Factor","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1721390377,"objectID":"d9372fe7c66b94c2b5cdacbdf7bda8a7","permalink":"https://forrt.org/glossary/english/bayes_factor/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/english/bayes_factor/","section":"glossary","summary":"","tags":null,"title":"Bayes Factor","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1635961284,"objectID":"c288852cd1b684f409c96e238d07747c","permalink":"https://forrt.org/glossary/vbeta/bayes-factor/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/vbeta/bayes-factor/","section":"glossary","summary":"","tags":null,"title":"Bayes Factor","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1720865851,"objectID":"5d99fad33ba4beaaf06394c8db350f55","permalink":"https://forrt.org/glossary/german/bayes_factor/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/german/bayes_factor/","section":"glossary","summary":"","tags":null,"title":"Bayes Factor (Bayes-Faktor)","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1689122843,"objectID":"3a4aa5591b7e5e7ae4cccb898bcd6a7a","permalink":"https://forrt.org/curated_resources/bayes-for-beginners-probability-and-like/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/bayes-for-beginners-probability-and-like/","section":"curated_resources","summary":"A blog about Bayes for Beginners: Probability and Likelihood","tags":[""],"title":"Bayes for Beginners: Probability and Likelihood","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"0eb38e9f192bb097a1b38dda0d90a4af","permalink":"https://forrt.org/curated_resources/bayes-rules-an-introduction-to-bayesian/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/bayes-rules-an-introduction-to-bayesian/","section":"curated_resources","summary":"Bayesian statistics?! Once an obscure term outside specialized industry and research circles, Bayesian methods are enjoying a renaissance. The title of this book speaks to what all the fuss is about: Bayes rules! Bayesian methods provide a powerful alternative to the frequentist methods that are ingrained in the standard statistics curriculum. Though frequentist and Bayesian methods share a common goal – learning from data – the Bayesian approach to this goal is gaining popularity for many reasons: (1) Bayesian methods allow us to interpret new data in light of prior information, formally weaving both into a set of updated information; (2) relative to the confidence intervals and p-values utilized in frequentist analyses, Bayesian results are easier to interpret; (3) Bayesian methods can shine in settings where frequentist “likelihood” methods break down; and (4) the computational tools required for applying Bayesian techniques are increasingly accessible. Unfortunately, the popularity of Bayesian statistics has outpaced the curricular resources needed to support it. To this end, the primary goal of Bayes Rules! is to make modern Bayesian thinking, modeling, and computing accessible to a broad audience.","tags":["Bayesian statistics"],"title":"Bayes Rules! An Introduction to Bayesian Modelling in R","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"c66164298c23da28428754b74c14b1f0","permalink":"https://forrt.org/curated_resources/bayesian-cognitive-modelling/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/bayesian-cognitive-modelling/","section":"curated_resources","summary":"This site is dedicated to the book “Bayesian Cognitive Modeling: A Practical Course”, published by Cambridge University Press.","tags":["Bayesian statistics"],"title":"Bayesian Cognitive Modelling","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1689122843,"objectID":"d5e7993776a7bf225b710bb5ef564cb3","permalink":"https://forrt.org/curated_resources/bayesian-inference/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/bayesian-inference/","section":"curated_resources","summary":"This blog post is about a new statistical visualization. This time I’ve tried to illustrate the logic of bayesian updating and hypothesis testing, using a Bayesian t-test.","tags":["Blog","Interaction"],"title":"Bayesian inference","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1721390377,"objectID":"d500020fa1735efde1230ec152dd6fb0","permalink":"https://forrt.org/glossary/english/bayesian_inference/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/english/bayesian_inference/","section":"glossary","summary":"","tags":null,"title":"Bayesian Inference","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1635961284,"objectID":"bf9ddbe133eb86400d69a9f6000da2dc","permalink":"https://forrt.org/glossary/vbeta/bayesian-inference/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/vbeta/bayesian-inference/","section":"glossary","summary":"","tags":null,"title":"Bayesian Inference","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1720865851,"objectID":"236a9a34617581c9a48dbdab37e03e25","permalink":"https://forrt.org/glossary/german/bayesian_inference/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/german/bayesian_inference/","section":"glossary","summary":"","tags":null,"title":"Bayesian Inference (Bayessche Inferenz)","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"5f901739d02122ee429c05d29724e5a5","permalink":"https://forrt.org/curated_resources/bayesian-inference-for-psychology-part-i/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/bayesian-inference-for-psychology-part-i/","section":"curated_resources","summary":"Bayesian hypothesis testing presents an attractive alternative to p value hypothesis testing. Part I of this series outlined several advantages of Bayesian hypothesis testing, including the ability to quantify evidence and the ability to monitor and update this evidence as data come in, without the need to know the intention with which the data were collected. Despite these and other practical advantages, Bayesian hypothesis tests are still reported relatively rarely. An important impediment to the widespread adoption of Bayesian tests is arguably the lack of user-friendly software for the run-of-the-mill statistical problems that confront psychologists for the analysis of almost every experiment: the t-test, ANOVA, correlation, regression, and contingency tables. In Part II of this series we introduce JASP (http://www.jasp-stats.org), an open-source, cross-platform, user-friendly graphical software package that allows users to carry out Bayesian hypothesis tests for standard statistical problems. JASP is based in part on the Bayesian analyses implemented in Morey and Rouder’s BayesFactor package for R. Armed with JASP, the practical advantages of Bayesian hypothesis testing are only a mouse click away.","tags":["Analysis","Data","Statistics"],"title":"Bayesian inference for psychology. Part II: Example applications with JASP","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"67507b9ca0cbd7461730c90e45edf56e","permalink":"https://forrt.org/glossary/english/bayesian-parameter_estimation/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/english/bayesian-parameter_estimation/","section":"glossary","summary":"","tags":null,"title":"Bayesian Parameter Estimation","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1635961284,"objectID":"cd1b23e5873560b8e4ec0b2ff23f4f77","permalink":"https://forrt.org/glossary/vbeta/bayesian-parameter-estimation/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/vbeta/bayesian-parameter-estimation/","section":"glossary","summary":"","tags":null,"title":"Bayesian Parameter Estimation ","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"68e65fb4b891f6ab771a740a7e8480ec","permalink":"https://forrt.org/glossary/german/bayesian-parameter_estimation/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/german/bayesian-parameter_estimation/","section":"glossary","summary":"","tags":null,"title":"Bayesian Parameter Estimation (Bayes’sche Parameter-Schätzung)","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"660d1649fed72742f319cb1880c65a5d","permalink":"https://forrt.org/curated_resources/being-a-reviewer-or-editor-for-registere/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/being-a-reviewer-or-editor-for-registere/","section":"curated_resources","summary":"Experienced Registered Reports editors and reviewers come together to discuss the format and best practices for handling submissions. The panelists also share insights into what editors are looking for from reviewers as well as practical guidelines for writing a Registered Report. ABOUT THE PANELISTS: Chris Chambers , Chris is a professor of cognitive neuroscience at Cardiff University, Chair of the Registered Reports Committee supported by the Center for Open Science, and one of the founders of Registered Reports. He has helped establish the Registered Reports format for over a dozen journals. Anastasia Kiyonaga , Anastasia is a cognitive neuroscientist who uses converging behavioral, brain stimulation, and neuroimaging methods to probe memory and attention processes. She is currently a postdoctoral researcher with Mark D'Esposito in the Helen Wills Neuroscience Institute at the University of California, Berkeley. Before coming to Berkeley, she received her Ph.D. with Tobias Egner in the Duke Center for Cognitive Neuroscience. She will be an Assistant Professor in the Department of Cognitive Science at UC San Diego starting January, 2020. Jason Scimeca , Jason is a cognitive neuroscientist at UC Berkeley. His research investigates the neural systems that support high-level cognitive processes such as executive function, working memory, and the flexible control of behavior. He completed his Ph.D. at Brown University with David Badre and is currently a postdoctoral researcher in Mark D'Esposito's Cognitive Neuroscience Lab. Moderated by David Mellor, Director of Policy Initiatives for the Center for Open Science.","tags":["Analysis","Data","Education","Materials","Open Scholarship Tools and Technologies","OSF","Publishing","Reproducibility","Research Data Management Tools","Researchers"],"title":"Being a Reviewer or Editor for Registered Reports","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1689122843,"objectID":"36b78718794aac967170b824a74045ae","permalink":"https://forrt.org/curated_resources/ben-goldacre-battling-bad-science/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/ben-goldacre-battling-bad-science/","section":"curated_resources","summary":"A video about battling bad science","tags":["Video"],"title":"Ben Goldacre: Battling Bad Science","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1689122843,"objectID":"956cbcca1f08ef0b44424a5474eb98aa","permalink":"https://forrt.org/curated_resources/benefits-of-open-and-high-powered-resear/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/benefits-of-open-and-high-powered-resear/","section":"curated_resources","summary":"Several researchers recently outlined unacknowledged costs of open science practices, arguing these costs may outweigh benefits and stifle discovery of novel findings. We scrutinize these researchers’ (a) statistical concern that heightened stringency with respect to false-positives will increase false-negatives and (b) metascientific concern that larger samples and executing direct replications engender opportunity costs that will decrease the rate of making novel discoveries. We argue their statistical concern is unwarranted given open science proponents recommend such practices to reduce the inflated Type I error rate from .35 down to .05 and simultaneously call for high-powered research to reduce the inflated Type II error rate. Regarding their metaconcern, we demonstrate that incurring some costs is required to increase the rate (and frequency) of making true discoveries because distinguishing true from false hypotheses requires a low Type I error rate, high statistical power, and independent direct replications. We also examine pragmatic concerns raised regarding adopting open science practices for relationship science (preregistration, open materials, open data, direct replications, sample size); while acknowledging these concerns, we argue they are overstated given available solutions. We conclude benefits of open science practices outweigh costs for both individual researchers and the collective field in the long run, but that short term costs may exist for researchers because of the currently dysfunctional academic incentive structure. Our analysis implies our field’s incentive structure needs to change whereby better alignment exists between researcher’s career interests and the field’s cumulative progress. We delineate recent proposals aimed at such incentive structure realignment.","tags":["Reproducibility Crisis and Credibility Revolution","Open Science"],"title":"Benefits of Open and High-Powered Research Outweigh Costs","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"e8028471e414cc492d82b018f222a38c","permalink":"https://forrt.org/curated_resources/berlin-oxford-summer-school-2020/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/berlin-oxford-summer-school-2020/","section":"curated_resources","summary":"This repository contains materials and talks for the Berlin|Oxford Summer School 2020 from 28th September to 1st of October","tags":["Open Research"],"title":"Berlin|Oxford Summer School 2020","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1689122843,"objectID":"8e83558154c1cf50eeea59a722478c59","permalink":"https://forrt.org/curated_resources/best-practices-in-data-analysis-and-shar/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/best-practices-in-data-analysis-and-shar/","section":"curated_resources","summary":"Given concerns about the reproducibility of scientific findings, neuroimaging must define best practices for data analysis, results reporting, and algorithm and data sharing to promote transparency, reliability and collaboration. We describe insights from developing a set of recommendations on behalf of the Organization for Human Brain Mapping and identify barriers that impede these practices, including how the discipline must change to fully exploit the potential of the world’s neuroimaging data.","tags":["Reproducibility Crisis and Credibility Revolution","Open Science"],"title":"Best practices in data analysis and sharing in neuroimaging using MRI","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1689122843,"objectID":"7384aebb4e954c00cd87e01a651da2ff","permalink":"https://forrt.org/curated_resources/best-research-practices-in-psychology-il/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/best-research-practices-in-psychology-il/","section":"curated_resources","summary":"In recent years, a robust movement has emerged within psychology to increase the evidentiary value of our science. This movement, which has analogs throughout the empirical sciences, is broad and diverse, but its primary emphasis has been on the reduction of statistical false positives. The present article addresses epistemological and pragmatic issues that we, as a field, must consider as we seek to maximize the scientific value of this movement. Regarding epistemology, this article contrasts the false-positives-reduction (FPR) approach with an alternative, the error balance (EB) approach, which argues that any serious consideration of optimal scientific practice must contend simultaneously with both false-positive and false-negative errors. Regarding pragmatics, the movement has devoted a great deal of attention to issues that frequently arise in laboratory experiments and one-shot survey studies, but it has devoted less attention to issues that frequently arise in intensive and/or longitudinal studies. We illustrate these epistemological and pragmatic considerations with the case of relationship science, one of the many research domains that frequently employ intensive and/or longitudinal methods. Specifically, we examine 6 research prescriptions that can help to reduce false-positive rates: preregistration, prepublication sharing of materials, postpublication sharing of data, close replication, avoiding piecemeal publication, and increasing sample size. For each, we offer concrete guidance not only regarding how researchers can improve their research practices and balance the risk of false-positive and false-negative errors, but also how the movement can capitalize upon insights from research practices within relationship science to make the movement stronger and more inclusive","tags":["Reproducibility Crisis and Credibility Revolution","Open Science"],"title":"Best research practices in psychology: Illustrating epistemological and pragmatic considerations with the case of relationship science. ","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1689122843,"objectID":"50d5bc753b38719e2ff664774daeaeeb","permalink":"https://forrt.org/curated_resources/better-p-curves-making-p-curve-analysis/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/better-p-curves-making-p-curve-analysis/","section":"curated_resources","summary":"When studies examine true effects, they generate right-skewed p-curves, distributions of statistically significant results with more low (.01 s) than high (.04 s) p values. What else can cause a right-skewed p-curve? First, we consider the possibility that researchers report only the smallest significant p value (as conjectured by Ulrich \u0026 Miller, 2015), concluding that it is a very uncommon problem. We then consider more common problems, including (a) p-curvers selecting the wrong p values, (b) fake data, (c) honest errors, and (d) ambitiously p-hacked (beyond p \u003c .05) results. We evaluate the impact of these common problems on the validity of p-curve analysis, and provide practical solutions that substantially increase its robustness.","tags":[""],"title":"Better P-curves: Making P-curve Analysis More Robust to Errors, Fraud, and Ambitious P-hacking, a Reply to Ulrich and Miller (2015)","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"d5d0fbeb6e8af2fee1fb30679a197999","permalink":"https://forrt.org/curated_resources/beyond-kindness-a-proposal-for-the-flour/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/beyond-kindness-a-proposal-for-the-flour/","section":"curated_resources","summary":"We argue that many of the crises currently afflicting science can be associated with a present failure of science to sufficiently embody its own values. Here, we propose a response beyond mere crisis resolution based on the observation that an ethical framework of flourishing derived from the Buddhist tradition aligns surprisingly well with the values of science itself. This alignment, we argue, suggests a recasting of science from a competitively managed activity of knowledge production to a collaboratively organized moral practice that puts kindness and sharing at its core. We end by examining how a flourishing framework could be embodied in academic practice, from individual to organizational levels, and how that could help to arrive at a flourishing of scientists and science alike.","tags":["Academia","Flourishing","Ethics","Culture","Management"],"title":"Beyond kindness: A proposal for the flourishing of science and scientists","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1689122843,"objectID":"6e9147e347c0f4da9f2b3f107257bd15","permalink":"https://forrt.org/curated_resources/beyond-power-calculations-assessing-type/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/beyond-power-calculations-assessing-type/","section":"curated_resources","summary":"Statistical power analysis provides the conventional approach to assess error rates when designing a research study. However, power analysis is flawed in that a narrow emphasis on statistical significance is placed as the primary focus of study design. In noisy, small-sample settings, statistically significant results can often be misleading. To help researchers address this problem in the context of their own studies, we recommend design calculations in which (a) the probability of an estimate being in the wrong direction (Type S [sign] error) and (b) the factor by which the magnitude of an effect might be overestimated (Type M [magnitude] error or exaggeration ratio) are estimated. We illustrate with examples from recent published research and discuss the largest challenge in a design calculation: coming up with reasonable estimates of plausible effect sizes based on external information.","tags":[""],"title":"Beyond Power Calculations: Assessing Type S (Sign) and Type M (Magnitude) Errors","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1721390377,"objectID":"350f288266ef4416d645d5c37ead6662","permalink":"https://forrt.org/glossary/english/bids_data_structure/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/english/bids_data_structure/","section":"glossary","summary":"","tags":null,"title":"BIDS data structure","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1635961284,"objectID":"708f3f8396d15871b2a88634cdeb2c89","permalink":"https://forrt.org/glossary/vbeta/bids-data-structure/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/vbeta/bids-data-structure/","section":"glossary","summary":"","tags":null,"title":"BIDS data structure","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1720865851,"objectID":"78958621f9c02922f0945c84f7839a11","permalink":"https://forrt.org/glossary/german/bids_data_structure/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/german/bids_data_structure/","section":"glossary","summary":"","tags":null,"title":"BIDS data structure (BIDS Datenstruktur)","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1689122843,"objectID":"f41695f352d214d1e18df81a676c7ae0","permalink":"https://forrt.org/curated_resources/big-correlations-in-little-studies-infla/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/big-correlations-in-little-studies-infla/","section":"curated_resources","summary":"Vul, Harris, Winkielman, and Pashler (2009), (this issue) argue that correlations in many cognitive neuroscience studies are grossly inflated due to a widespread tendency to use nonindependent analyses. In this article, I argue that Vul et al.'s primary conclusion is correct, but for different reasons than they suggest. I demonstrate that the primary cause of grossly inflated correlations in whole-brain fMRI analyses is not nonindependence, but the pernicious combination of small sample sizes and stringent alpha-correction levels. Far from defusing Vul et al.'s conclusions, the simulations presented suggest that the level of inflation may be even worse than Vul et al.'s empirical analysis would suggest.","tags":[""],"title":"Big Correlations in Little Studies: Inflated fMRI Correlations Reflect Low Statistical Power-Commentary on Vul Et Al. (2009)","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"e7a6c7acec87d02ba8081512ac6db274","permalink":"https://forrt.org/curated_resources/big-team-science-initiatives-a-catalyst/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/big-team-science-initiatives-a-catalyst/","section":"curated_resources","summary":"Keener et al. (2023) raise concerns about the trustworthiness of Industrial/Organizational (IO) Psychology research and related fields due to the low reproducibility and replicability of research findings. The authors provide various solutions to resolve this crisis, such as improving training, realigning incentives, and adopting open science practices. Our commentary elaborates on one solution to which they briefly allude: Big Team Science Initiatives (BTSIs). BTSIs allow scholars to address the trustworthiness of our science by facilitating large sample theory testing, sharing and allocating resources, and selecting appropriate research strategies, all of which support the reproducibility and replication of research. Further, we propose that BTSIs may facilitate researcher training, encourage data sharing and materials, and realign incentives in our field. We discuss how BTSIs could be implemented in IO psychology and related fields, identifying and drawing upon similar BTSIs in related disciplines. Thus, our commentary is an extension of the focal article, encouraging scholars to collaboratively address the “crisis of confidence” facing our field using a big team science approach.","tags":["Big Team Science Initiatives","Big Team Science","Large Collaborations","Team Science"],"title":"Big team science initiatives: A catalyst for trustworthy advancements in IO psychology","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"870882b86845247c7e2c8509be81f430","permalink":"https://forrt.org/curated_resources/biomedical-publishing-past-historic-pres/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/biomedical-publishing-past-historic-pres/","section":"curated_resources","summary":"Academic journals have been publishing the results of biomedical research for more than 350 years. Reviewing their history reveals that the ways in which journals vet submissions have changed over time, culminating in the relatively recent appearance of the current peer-review process. Journal brand and Impact Factor have meanwhile become quality proxies that are widely used to filter articles and evaluate scientists in a hypercompetitive prestige economy. The Web created the potential for a more decoupled publishing system in which articles are initially disseminated by preprint servers and then undergo evaluation elsewhere. To build this future, we must first understand the roles journals currently play and consider what types of content screening and review are necessary and for which papers. A new, open ecosystem involving preprint servers, journals, independent content-vetting initiatives, and curation services could provide more multidimensional signals for papers and avoid the current conflation of trust, quality, and impact. Academia should strive to avoid the alternative scenario, however, in which stratified publisher silos lock in submissions and simply perpetuate this conflation.","tags":["Impact Factor","Publishing","Open Access","Preprints"],"title":"Biomedical publishing: Past historic, present continuous, future conditional","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1689122843,"objectID":"7d63a81470226b2bee3673f2daa25512","permalink":"https://forrt.org/curated_resources/bite-size-science-and-its-undesired-side/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/bite-size-science-and-its-undesired-side/","section":"curated_resources","summary":"Short and rapid publication of research findings has many advantages. However, there is another side of the coin that needs careful consideration. We argue that the most dangerous aspect of a shift toward “bite-size” publishing is the relationship between study size and publication bias. Findings based on a single study or a study based on a limited sample size are more likely to be false positive, because the false positive rate remains constant, whereas the true positive rate (the power) declines as sample size declines. Pressure on productivity and on novelty value further exacerbates the problem.","tags":["Reproducibility Crisis and Credibility Revolution","Open Science"],"title":"Bite-Size Science and Its Undesired Side Effects","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1721390377,"objectID":"c4b44f4dd8db39a397bc73036c6f3d82","permalink":"https://forrt.org/glossary/english/bizarre/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/english/bizarre/","section":"glossary","summary":"","tags":null,"title":"BIZARRE","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1720865851,"objectID":"62f6a65169f2ae674afc43f23bbcf480","permalink":"https://forrt.org/glossary/german/bizarre/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/german/bizarre/","section":"glossary","summary":"","tags":null,"title":"BIZARRE","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1635961284,"objectID":"56851b2e86165806e4582dc081fbff80","permalink":"https://forrt.org/glossary/vbeta/bizarre/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/vbeta/bizarre/","section":"glossary","summary":"","tags":null,"title":"BIZARRE","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"5619273f383a6e34da18428810ba7c36","permalink":"https://forrt.org/curated_resources/boas-praticas-em-ciencia/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/boas-praticas-em-ciencia/","section":"curated_resources","summary":"Reprodutibilidade e replicabilidade de estudos científicos. \nRandomização, cegamento, planejamento amostral e pré-registro de projeto. Diferença entre estudos exploratórios e confirmatórios, P hacking, HARKing e double dipping. Práticas de openscience. Publicação de artigos científicos, critérios de autoria, diretrizes e checklists de redação, revisão pelos pares. Má conduta científica (plágio, fabricação e falsificação).","tags":["Reproducibility Crisis and Credibility Revolution","Open Science"],"title":"Boas práticas em ciência","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1721390377,"objectID":"45658b1ebdc5bf1175f4bf822d288145","permalink":"https://forrt.org/glossary/english/bottom_up_approach/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/english/bottom_up_approach/","section":"glossary","summary":"","tags":null,"title":"Bottom-up approach (to Open Scholarship)","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1635961284,"objectID":"1462b0c6b2ab9664533558bb9345720b","permalink":"https://forrt.org/glossary/vbeta/bottom-up-approach-to-open-scholars/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/vbeta/bottom-up-approach-to-open-scholars/","section":"glossary","summary":"","tags":null,"title":"Bottom-up approach (to Open Scholarship) ","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1720865851,"objectID":"152c2193b906e457c8584c7e3817a8c6","permalink":"https://forrt.org/glossary/german/bottom_up_approach/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/german/bottom_up_approach/","section":"glossary","summary":"","tags":null,"title":"Bottom-up approach (to Open Scholarship) (Bottom-Up Ansatz)","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1720865851,"objectID":"9a7c632eca835f338773ae41d10ac54b","permalink":"https://forrt.org/glossary/german/boundary_condition/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/german/boundary_condition/","section":"glossary","summary":"","tags":null,"title":"Boundary condition (Randbedingungen)! New term - comments needed !","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1721390377,"objectID":"a90796cb115315c49fbc0ba9d10ae27b","permalink":"https://forrt.org/glossary/english/bracketing_interviews/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/english/bracketing_interviews/","section":"glossary","summary":"","tags":null,"title":"Bracketing Interviews","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1635961284,"objectID":"7f026c1a8bb48c7be22de4001d295934","permalink":"https://forrt.org/glossary/vbeta/bracketing-interviews/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/vbeta/bracketing-interviews/","section":"glossary","summary":"","tags":null,"title":"Bracketing Interviews","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1720865851,"objectID":"0c16c32908d0dc7b057ef286d53b43a8","permalink":"https://forrt.org/glossary/german/bracketing_interviews/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/german/bracketing_interviews/","section":"glossary","summary":"","tags":null,"title":"Bracketing Interviews (Klammer-Interviews)","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1689122843,"objectID":"b4dd5441cc92876bd9db0d273e27af7b","permalink":"https://forrt.org/curated_resources/bradley-efron-frequentist-accuracy-of-ba/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/bradley-efron-frequentist-accuracy-of-ba/","section":"curated_resources","summary":"A video about Frequentist accuracy of Bayesian estimates","tags":["Video","Bayesian"],"title":"Bradley Efron: Frequentist accuracy of Bayesian estimates","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"c36cd33a331c63af2929dbabf9ca4fd5","permalink":"https://forrt.org/curated_resources/breaking-free-how-preregistration-hurts/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/breaking-free-how-preregistration-hurts/","section":"curated_resources","summary":"Pre-registration has become an increasingly popular proposal to address concerns regarding questionable research practices. Yet preregistration does not necessarily solve these problems. It also causes additional problems, including raising costs for more junior and less resourced scholars. In addition, pre-registration restricts creativity and diminishes the broader scientific enterprise. In this way, pre-registration neither solves the problems it is intended to address, nor does it come without costs. Pre-registration is neither necessary nor sufficient for producing novel or ethical work. In short, pre-registration represents a form of virtue signaling that is more performative than actual.","tags":["Preregistration","Critique"],"title":"Breaking free: How preregistration hurts scholars and science","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"79571906cf116d30060173664926e2b9","permalink":"https://forrt.org/curated_resources/bringing-institutional-multidisciplinary/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/bringing-institutional-multidisciplinary/","section":"curated_resources","summary":"The FAIR Principles are a set of good practices to improve the reproducibility and quality of data in an Open Science context. Different sets of indicators have been proposed to evaluate the FAIRness of digital objects, including datasets that are usually stored in repositories or data portals. However, indicators like those proposed by the Research Data Alliance are provided from a high-level perspective that can be interpreted and they are not always realistic to particular environments like multidisciplinary repositories. This paper describes FAIR EVA, a new tool developed within the European Open Science Cloud context that is oriented to particular data management systems like open repositories, which can be customized to a specific case in a scalable and automatic environment. It aims to be adaptive enough to work for different environments, repository software and disciplines, taking into account the flexibility of the FAIR Principles. As an example, we present DIGITAL.CSIC repository as the first target of the tool, gathering the particular needs of a multidisciplinary institution as well as its institutional repository.","tags":["Information Technology","Scientific Data","Software"],"title":"Bringing institutional multidisciplinary repositories into the FAIR picture","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"80ff3c8e8638b8112e0b28e374bf2dc9","permalink":"https://forrt.org/curated_resources/bringing-the-pre-registration-revolution/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/bringing-the-pre-registration-revolution/","section":"curated_resources","summary":"Preregistration, which involves documentation of hypotheses, methods, and plans for data analysis prior to data collection or analysis, has been lauded as 1 potential solution to the replication crisis in psychological science. Yet, many researchers have been slow to adopt preregistration, and the next generation of researchers is offered little formalized instruction in creating comprehensive preregistrations. In this article, we describe a collaborative workshop-based preregistration course designed and taught by Jennifer L. Tackett. We provide a brief overview of preregistration, including resources available, common concerns with preregistration, and responses to these concerns. We then describe the goals, structure, and evolution of our preregistration course and provide examples of enrolled students’ final research products. We conclude with reflections on the strengths and opportunities for growth for the 1st iteration of this course and suggestions for others who are interested in implementing similar open science–focused courses in their training programs.","tags":["Preregistration","Graduate Training"],"title":"Bringing the (pre)registration revolution to graduate training","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1721390377,"objectID":"03ebd890c467b7e8454d5310c895ef43","permalink":"https://forrt.org/glossary/english/bropenscience/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/english/bropenscience/","section":"glossary","summary":"","tags":null,"title":"Bropenscience","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1720865851,"objectID":"90172b092e6a8c323b40dad5ffefc1d7","permalink":"https://forrt.org/glossary/german/bropenscience/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/german/bropenscience/","section":"glossary","summary":"","tags":null,"title":"Bropenscience","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1635961284,"objectID":"43d0f881efb0da3f12ca43acde3c7d71","permalink":"https://forrt.org/glossary/vbeta/bropenscience/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/vbeta/bropenscience/","section":"glossary","summary":"","tags":null,"title":"Bropenscience","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1689122843,"objectID":"c0cc9708b1321b2cd97c9201e8ca6ebc","permalink":"https://forrt.org/curated_resources/calculating-and-reporting-effect-sizes-t/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/calculating-and-reporting-effect-sizes-t/","section":"curated_resources","summary":"Effect sizes are the most important outcome of empirical studies. Most articles on effect sizes highlight their importance to communicate the practical significance of results. For scientists themselves, effect sizes are most useful because they facilitate cumulative science. Effect sizes can be used to determine the sample size for follow-up studies, or examining effects across studies. This article aims to provide a practical primer on how to calculate and report effect sizes for t-tests and ANOVA's such that effect sizes can be used in a-priori power analyses and meta-analyses. Whereas many articles about effect sizes focus on between-subjects designs and address within-subjects designs only briefly, I provide a detailed overview of the similarities and differences between within- and between-subjects designs. I suggest that some research questions in experimental psychology examine inherently intra-individual effects, which makes effect sizes that incorporate the correlation between measures the best summary of the results. Finally, a supplementary spreadsheet is provided to make it as easy as possible for researchers to incorporate effect size calculations into their workflow.","tags":[""],"title":"Calculating and reporting effect sizes to facilitate cumulative science: a practical primer for t-tests and ANOVAs","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1689122843,"objectID":"e84ae64393467155af4c65609c6ebcf3","permalink":"https://forrt.org/curated_resources/calculating-the-overlap-of-two-normal-di/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/calculating-the-overlap-of-two-normal-di/","section":"curated_resources","summary":"I read this post over at the blog Cartesian Faith about Probability and Monte Carlo methods. The post describe how to numerically intregate using Monte Carlo methods. I thought the results looked cool so I used the method to calculate the overlap of two normal distributions that are separated by a Cohen’s d of 0.8. You should head over to the original post if you want a more detailed explanation of the method. And I should add that this is not the most efficient way to calculate the overlap of two gaussian distributions, but it is a fun and pretty intuitive way, plus you can make a cool plot of the result. However, I also show how to get the overlap using the cumulative distribution function and using R’s built-in integration function.","tags":["Blog","Tutorial"],"title":"Calculating the overlap of two normal distributions using monte carlo intergration","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"720bd4ad9382e2d3fc42a9ce0d8d4109","permalink":"https://forrt.org/curated_resources/campbell-open-and-reproducible-science-s/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/campbell-open-and-reproducible-science-s/","section":"curated_resources","summary":"The purpose of this course is to acquaint students with recent developments in open science and reproducibility of the research workflow. By the end of this course students will be familiar with documenting their research workflow (e.g., idea generation, hypotheses, study materials and procedures, re-usable data sets, annotated code, meta-data, output), in both a private and public manner, from beginning to end in a way that allows others to reproduce their methods, analyses, and results. Students will alsobecome familiar with using the Open Science Framework to document their own research workflow.","tags":["Reproducibility Crisis and Credibility Revolution","Open Science","Conceptual and statistical knowledge"],"title":"Campbell Open and Reproducible Science Syllabus","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"867d4bb3bfaed141e65545a75b7a0eff","permalink":"https://forrt.org/curated_resources/capstone-advanced-general/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/capstone-advanced-general/","section":"curated_resources","summary":"How do psychologists determine what is true and what is false about human behavior, affect, and cognition? The question encompasses more than we can know from a single study or even a single research paper, and the issues run deeper than just research methods. Instead, we need to consider what it means to conduct and understand science.\nWe must consider why scientists, as humans themselves, can fall prey to biases and fallacies that interfere with their ability to draw sound conclusions. This course focuses on an emerging field known as meta-science (i.e., the study of the process of science itself) in the context of research in psychology. \nWe begin by considering whether and if the research practices of psychologists need to change. We touch briefly on philosophy of science and epistemology before attempting to determine how psychologists form cumulative knowledge and theories. \nWe examine meta-analysis and its criticisms, drawing on several prominent historical and recent cases. Student pairs will lead discussion on reviews of diverse research. Capstone serves as a bookend for the psychology major. We examine the field of psychology in a broad, integrative way using a seminar format. A seminar means that this course primarily involves close reading, reflective writing, andthoughtful discussion. With almost no lecture in this-course, students should expect to read and write a lot.","tags":["Transparency","Reproducibility Crisis and Credibility Revolution","Open Science"],"title":"Capstone: Advanced General","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1689122843,"objectID":"4129c29fc81dbebd133fd6e7dc468951","permalink":"https://forrt.org/curated_resources/cargo-cult-science/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/cargo-cult-science/","section":"curated_resources","summary":"some remarks on science, pseudoscience, and learning how to not fool yourself. Caltech's 1974 commencement address","tags":["Reproducibility Crisis and Credibility Revolution","Open Science"],"title":"Cargo cult science. ","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1721390377,"objectID":"d88e3537eeecc3c953406f4a5a94f74b","permalink":"https://forrt.org/glossary/english/carking/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/english/carking/","section":"glossary","summary":"","tags":null,"title":"CARKing","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1720865851,"objectID":"2e2b89eb6277a9faa210befc02a38773","permalink":"https://forrt.org/glossary/german/carking/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/german/carking/","section":"glossary","summary":"","tags":null,"title":"CARKing","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1635961284,"objectID":"a7023edf230a243cb63cae2081b4a230","permalink":"https://forrt.org/glossary/vbeta/carking/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/vbeta/carking/","section":"glossary","summary":"","tags":null,"title":"CARKing","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"8169db6b88889ae4f1f9beca35bda2cc","permalink":"https://forrt.org/curated_resources/carpentries-instructor-training/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/carpentries-instructor-training/","section":"curated_resources","summary":"A two-day introduction to modern evidence-based teaching practices, built and maintained by the Carpentry community.","tags":["Analysis","Data","Education","Open Scholarship Tools and Technologies","Reproducibility","Research Data Management Tools","Researchers"],"title":"Carpentries Instructor Training","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1721390377,"objectID":"ff4044f04b06d12f7b63a8e656c41483","permalink":"https://forrt.org/glossary/english/center_for_open_science/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/english/center_for_open_science/","section":"glossary","summary":"","tags":null,"title":"Center for Open Science (COS)","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1720865851,"objectID":"08094b3fa5166451990a253a76c6962d","permalink":"https://forrt.org/glossary/german/center_for_open_science/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/german/center_for_open_science/","section":"glossary","summary":"","tags":null,"title":"Center for Open Science (COS)","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1635961284,"objectID":"459eac01c9ba49bb5f9981e883231638","permalink":"https://forrt.org/glossary/vbeta/center-for-open-science-cos/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/vbeta/center-for-open-science-cos/","section":"glossary","summary":"","tags":null,"title":"Center for Open Science (COS)","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"299b79f7df9820e8ec4c8ea18aa68a1c","permalink":"https://forrt.org/curated_resources/challenges-and-opportunities-in-preregis/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/challenges-and-opportunities-in-preregis/","section":"curated_resources","summary":"The credibility revolution in social science has led to the recommendation and adoption of practices to increase the replicability of scientific findings. Many of the recommended practices, such as replication and preregistration, present unique challenges for aging research given its reliance on long-term longitudinal data. In this tutorial, we propose preregistered coordinated data analysis as a promising approach that involves both replication and preregistration, but that overcomes the aforementioned challenges by using existing data. We discuss the benefits of preregistering coordinated data analysis and provide an add-on template to be used in conjunction with existing preregistration templates for preregistering coordinated data analysis.","tags":["Preregistration","Replicability","Template"],"title":"Challenges and opportunities in preregistration of coordinated data analysis: A tutorial and template","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1689122843,"objectID":"c93d605930d4a5c644b8f784c377ff78","permalink":"https://forrt.org/curated_resources/change-starts-with-journal-editors-in-re/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/change-starts-with-journal-editors-in-re/","section":"curated_resources","summary":"The editors of the Journal of Advanced Academics comment on Makel (2014). The replicability crisis in psychology is summarized in terms of three focal issues: the “file drawer” problem, lack of replication studies, and the null hypothesis significance testing paradigm. The authors argue that journal editors are uniquely positioned to address all three of these problems via the adoption of new policies for review and publication.","tags":["Reproducibility Crisis and Credibility Revolution","Open Science"],"title":"Change starts with journal editors: In response to Makel (2014).","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"cbf3f797950f6736aab683395b4e14db","permalink":"https://forrt.org/curated_resources/changing-the-culture-of-peer-review-for/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/changing-the-culture-of-peer-review-for/","section":"curated_resources","summary":"Peer review is a core component of scientific progression. Although peer review ideally improves research and promotes rigor, it also has consequences for what types of research are published and cited, and who wants to (and is able to) advance in research-focused careers. Despite peer review’s place as a core scientific practice, few reviewers receive training or oversight to ensure their feedback is helpful, professional, and culturally sensitive. Here, we critically examine the peer review system at multiple levels, from ideas to institutions, interactions, and individuals. We highlight initiatives that aim to change the normative negativity of peer review and provide authors with constructive, actionable feedback that is sensitive to diverse identities, methods, topics, and environments. We conclude with a call to action for how individuals, groups, and organizations can improve the culture of peer review. Changes in the peer review system must be made with an eye to diversity (increasing the range of identities and experiences constituting the field), equity (fair processes and outcomes across groups), and inclusion (experiences that promote belonging across groups). These changes can improve scientists’ experience of peer review, promote diverse perspectives and identities, and enhance the quality and impact of science.","tags":["Peer review","open review","diversity","inclusion","equity"],"title":"Changing the Culture of Peer Review for a More Inclusive and Equitable Psychological Science","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1689122843,"objectID":"ed9dcfa0ac7b87b66baa117f750830c3","permalink":"https://forrt.org/curated_resources/chaos-in-the-brickyard/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/chaos-in-the-brickyard/","section":"curated_resources","summary":"A paper about open science and novelty","tags":["Reproducibility Crisis and Credibility Revolution","Open Science"],"title":"Chaos in the brickyard","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"e59fd30c122da9ac505586183ff037fc","permalink":"https://forrt.org/curated_resources/characteristics-of-clinical-trials-regis/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/characteristics-of-clinical-trials-regis/","section":"curated_resources","summary":"Context Recent reports highlight gaps between guidelines-based treatment recommendations and evidence from clinical trials that supports those recommendations. Strengthened reporting requirements for studies registered with ClinicalTrials.gov enable a comprehensive evaluation of the national trials portfolio.\n\nObjective To examine fundamental characteristics of interventional clinical trials registered in the ClinicalTrials.gov database.\n\nMethods A data set comprising 96 346 clinical studies from ClinicalTrials.gov was downloaded on September 27, 2010, and entered into a relational database to analyze aggregate data. Interventional trials were identified and analyses were focused on 3 clinical specialties—cardiovascular, mental health, and oncology—that together encompass the largest number of disability-adjusted life-years lost in the United States.\n\nMain Outcome Measures Characteristics of registered clinical trials as reported data elements in the trial registry; how those characteristics have changed over time; differences in characteristics as a function of clinical specialty; and factors associated with use of randomization, blinding, and data monitoring committees (DMCs).\n\nResults The number of registered interventional clinical trials increased from 28 881 (October 2004–September 2007) to 40 970 (October 2007–September 2010), and the number of missing data elements has generally declined. Most interventional trials registered between 2007 and 2010 were small, with 62% enrolling 100 or fewer participants. Many clinical trials were single-center (66%; 24 788/37 520) and funded by organizations other than industry or the National Institutes of Health (NIH) (47%; 17 592/37 520). Heterogeneity in the reported methods by clinical specialty; sponsor type; and the reported use of DMCs, randomization, and blinding was evident. For example, reported use of DMCs was less common in industry-sponsored vs NIH-sponsored trials (adjusted odds ratio [OR], 0.11; 95% CI, 0.09-0.14), earlier-phase vs phase 3 trials (adjusted OR, 0.83; 95% CI, 0.76-0.91), and mental health trials vs those in the other 2 specialties. In similar comparisons, randomization and blinding were less frequently reported in earlier-phase, oncology, and device trials.\n\nConclusion Clinical trials registered in ClinicalTrials.gov are dominated by small trials and contain significant heterogeneity in methodological approaches, including reported use of randomization, blinding, and DMCs.","tags":["Clinical Trials","Preregistration"],"title":"Characteristics of Clinical Trials Registered in ClinicalTrials.gov, 2007-2010","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"9fd5794b9de15563484ab207bb15930b","permalink":"https://forrt.org/curated_resources/choice-of-analysis-pathway-dramatically/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/choice-of-analysis-pathway-dramatically/","section":"curated_resources","summary":"Breaking Continuous Flash Suppression (bCFS) has been adopted as an appealing means to study human visual awareness, but the literature is beclouded by inconsistent and contradictory results. Although previous reviews have focused chiefly on design pitfalls and instances of false reasoning, we show in this study that the choice of analysis pathway can have severe effects on the statistical output when applied to bCFS data. Using a representative dataset designed to address a specific controversy in the realm of language processing under bCFS, namely whether psycholinguistic variables affect access to awareness, we present a range of analysis methods based on real instances in the published literature, and indicate how each approach affects the perceived outcome. We provide a summary of published bCFS studies indicating the use of data transformation and trimming, and highlight that more compelling analysis methods are sparsely used in this field. We discuss potential interpretations based on both classical and more complex analyses, to highlight how these differ. We conclude that an adherence to openly available data and analysis pathways could provide a great benefit to this field, so that conclusions can be tested against multiple analyses as standard practices are updated.","tags":["Data"],"title":"Choice of analysis pathway dramatically affects statistical outcomes in breaking continuous flash suppression","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1689122843,"objectID":"379f6d8afa8952c449405151529f3e6a","permalink":"https://forrt.org/curated_resources/choosing-prediction-over-explanation-in/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/choosing-prediction-over-explanation-in/","section":"curated_resources","summary":"Psychology has historically been concerned, first and foremost, with explaining the causal mechanisms that give rise to behavior. Randomized, tightly controlled experiments are enshrined as the gold standard of psychological research, and there are endless investigations of the various mediating and moderating variables that govern various behaviors. We argue that psychology’s near-total focus on explaining the causes of behavior has led much of the field to be populated by research programs that provide intricate theories of psychological mechanism but that have little (or unknown) ability to predict future behaviors with any appreciable accuracy. We propose that principles and techniques from the field of machine learning can help psychology become a more predictive science. We review some of the fundamental concepts and tools of machine learning and point out examples where these concepts have been used to conduct interesting and important psychological research that focuses on predictive research questions. We suggest that an increased focus on prediction, rather than explanation, can ultimately lead us to greater understanding of behavior.","tags":["Reproducibility Crisis and Credibility Revolution","Open Science"],"title":"Choosing prediction over explanation in psychology: Lessons from machine learning.","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1689122843,"objectID":"2e85c30c00ffde83ed906e56fedf684b","permalink":"https://forrt.org/curated_resources/chrome-book-data-science/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/chrome-book-data-science/","section":"curated_resources","summary":"Chromebook Data Science (CBDS) is a free, massive open online educational program offered through Leanpub to help anyone who can read, write, and use a computer to move into data science, the number one rated job.","tags":["Data Science Tool"],"title":"Chrome Book Data Science","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"b31c0f0d505f91ee43150ffe07eb7a78","permalink":"https://forrt.org/curated_resources/ciencia-aberta-e-reprodutivel/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/ciencia-aberta-e-reprodutivel/","section":"curated_resources","summary":"Esta disciplina destina-se a alunos em níveis de Mestrado e Doutorado do Programade Pós-Graduação em Ciências da Saúde: Cardiologia e Ciências Cardiovasculares e outros programas de pós-graduação. A disciplina está estruturada para promover discussão e capacitação em recursos de transparência e reprodutibilidade em diferentes momentos da pesquisa científica. Os encontros envolverão leituras prévias, as quais são essenciais para real aproveitamento das discussões. Esta disciplina busca compor elementos sobre filosofia da ciência, métodos em pesquisa, erecursos/tecnologia para projetos, e visa aplicabilidade para uma pesquisa mais robusta e eficiente. De forma transversal aos conteúdos, serão trabalhadas as práticas individuais para processos transparentes em pesquisa e análise crítica sobre a ciência atual.","tags":["Reproducibility Crisis and Credibility Revolution","Open Science"],"title":"Ciência aberta e reprodutível","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1689122843,"objectID":"88bd6ce76e9085344931f6c4624487c3","permalink":"https://forrt.org/curated_resources/circle-of-willis-episode-with-sinime-vaz/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/circle-of-willis-episode-with-sinime-vaz/","section":"curated_resources","summary":"Welcome to Episode 8, where I talk to SIMINE VAZIRE, Associate Professor of Psychology at the University of California at Davis, about the stability of personality, our ability to know ourselves, and some of the nuances within the prescriptive advice of the Open Science Movement. Simine wears a number of different hats. In recent years, she’s been at or near the center of ongoing conversations among scientists about the virtues and challenges of open science. As part of this work, she co-founded the Society for the Improvement of Psychological Science (SIPS) and co-hosts a science podcast (with Sanjay Srivastava and Alexa Tullett) called THE BLACK GOAT. Simine is also editor in chief of the journal Social Psychological and Personality Science and a senior editor at Collabra. Interestingly, Simine has also been a part of the conversation about the process of criticism in science. As most listeners well know, criticism is unquestionably essential if science is going to be self-correcting (which is of course the whole point!). One question the field has been grappling with is the point at which criticism crosses over into harassment and bullying—a question at the heart of a recent op-ed Simine wrote for Slate. I have my own thoughts on this question, which I’ll save for another time, but one of the reasons I was so keen to ask Simine to be on Circle of Willis is that I find her approach to grappling with such questions to be equal parts humble, charitable, and firm. She isn’t likely to allow a legitimate criticism to be brushed aside in order to avoid hurting someone's feelings, but neither is she going to participate in (or for that matter tolerate) bullying. I think that in our age of shoot-from-the-hip outrage, that can be a hard path to find, let alone walk, and I genuinely admire her efforts. There are many other things I love about Simine, but as you’ll hear in this episode, at or near the top of the list of her agreeable traits is that she’ll be the first to tell any of you that sometimes she’s wrong. We try to be right while tolerating (and admitting to) our mistakes. Oh, and — seriously — keep a notepad handy for this episode. Simine is unusually quotable!","tags":["Podcast","Reproducibility Crisis and Credibility Revolution"],"title":"Circle of Willis (episode with Sinime Vazire)","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1721390377,"objectID":"cdb942ee61a299516258d60e719f5224","permalink":"https://forrt.org/glossary/english/citation_bias/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/english/citation_bias/","section":"glossary","summary":"","tags":null,"title":"Citation bias","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1635961284,"objectID":"c9477af4a8b83992c4799b603d1af5bc","permalink":"https://forrt.org/glossary/vbeta/citation-bias/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/vbeta/citation-bias/","section":"glossary","summary":"","tags":null,"title":"Citation bias","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1720865851,"objectID":"88aa74433d4b13c4de2b345b9a42c596","permalink":"https://forrt.org/glossary/german/citation_bias/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/german/citation_bias/","section":"glossary","summary":"","tags":null,"title":"Citation bias (Zitations-Verzerrung)","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1721390377,"objectID":"ec85d8454654d4330613dc005b23ec6a","permalink":"https://forrt.org/glossary/english/citation_diversity_statement/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/english/citation_diversity_statement/","section":"glossary","summary":"","tags":null,"title":"Citation Diversity Statement","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1635961284,"objectID":"ac2a290ce7ba98b8e385fd104684d659","permalink":"https://forrt.org/glossary/vbeta/citation-diversity-statement/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/vbeta/citation-diversity-statement/","section":"glossary","summary":"","tags":null,"title":"Citation Diversity Statement","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1720865851,"objectID":"6ff1b322ba74151565a5407f81240f87","permalink":"https://forrt.org/glossary/german/citation_diversity_statement/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/german/citation_diversity_statement/","section":"glossary","summary":"","tags":null,"title":"Citation Diversity Statement (Zitations-Diversitäts-Erklärung)","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"dd7b6cb6030c149d44f3ebfd1e0919c0","permalink":"https://forrt.org/curated_resources/citing-and-being-cited-data-code-edition/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/citing-and-being-cited-data-code-edition/","section":"curated_resources","summary":"Introduction to citations as a presentation. Citing data and code as well as getting citations for data and code.","tags":["Citation","Data","Materials","Open Scholarship Guidelines","Repositories","Research Data Management Tools","Researchers"],"title":"Citing and Being Cited: Data \u0026 Code Edition","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1721390377,"objectID":"0c68e064b39c1997a18fa6336c531c28","permalink":"https://forrt.org/glossary/english/citizen_science/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/english/citizen_science/","section":"glossary","summary":"","tags":null,"title":"Citizen Science","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1635961284,"objectID":"79d294aefe82fc2d04abedd025f8260e","permalink":"https://forrt.org/glossary/vbeta/citizen-science/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/vbeta/citizen-science/","section":"glossary","summary":"","tags":null,"title":"Citizen Science","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1720865851,"objectID":"b3c438870fb0312acb5eee82c096e417","permalink":"https://forrt.org/glossary/german/citizen_science/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/german/citizen_science/","section":"glossary","summary":"","tags":null,"title":"Citizen Science (Bürger:innenwissenschaft)","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1721390377,"objectID":"0ec00a5b053e8e3894e120ee932cdf17","permalink":"https://forrt.org/glossary/english/ckan/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/english/ckan/","section":"glossary","summary":"","tags":null,"title":"CKAN","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1720865851,"objectID":"324097baf2e3c07a2c11470df5e6510d","permalink":"https://forrt.org/glossary/german/ckan/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/german/ckan/","section":"glossary","summary":"","tags":null,"title":"CKAN","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1635961284,"objectID":"e8f860f94b27993572cad0602e3473df","permalink":"https://forrt.org/glossary/vbeta/ckan/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/vbeta/ckan/","section":"glossary","summary":"","tags":null,"title":"CKAN","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"ce7f65b37239272ecde44c476a2bd3d5","permalink":"https://forrt.org/curated_resources/clinical-trial-registration-and-reportin/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/clinical-trial-registration-and-reportin/","section":"curated_resources","summary":"Many clinical trials conducted by academic organizations are not published, or are not published completely. Following the US Food and Drug Administration Amendments Act of 2007, “The Final Rule” (compliance date April 18, 2017) and a National Institutes of Health policy clarified and expanded trial registration and results reporting requirements. We sought to identify policies, procedures, and resources to support trial registration and reporting at academic organizations. Methods We conducted an online survey from November 21, 2016 to March 1, 2017, before organizations were expected to comply with The Final Rule. We included active Protocol Registration and Results System (PRS) accounts classified by ClinicalTrials.gov as a “University/Organization” in the USA. PRS administrators manage information on ClinicalTrials.gov. We invited one PRS administrator to complete the survey for each organization account, which was the unit of analysis. Results Eligible organization accounts (N = 783) included 47,701 records (e.g., studies) in August 2016. Participating organizations (366/783; 47%) included 40,351/47,701 (85%) records. Compared with other organizations, Clinical and Translational Science Award (CTSA) holders, cancer centers, and large organizations were more likely to participate. A minority of accounts have a registration (156/366; 43%) or results reporting policy (129/366; 35%). Of those with policies, 15/156 (11%) and 49/156 (35%) reported that trials must be registered before institutional review board approval is granted or before beginning enrollment, respectively. Few organizations use computer software to monitor compliance (68/366; 19%). One organization had penalized an investigator for non-compliance. Among the 287/366 (78%) accounts reporting that they allocate staff to fulfill ClinicalTrials.gov registration and reporting requirements, the median number of full-time equivalent staff is 0.08 (interquartile range = 0.02–0.25). Because of non-response and social desirability, this could be a “best case” scenario. Conclusions Before the compliance date for The Final Rule, some academic organizations had policies and resources that facilitate clinical trial registration and reporting. Most organizations appear to be unprepared to meet the new requirements. Organizations could enact the following: adopt policies that require trial registration and reporting, allocate resources (e.g., staff, software) to support registration and reporting, and ensure there are consequences for investigators who do not follow standards for clinical research.","tags":["Publishing","Reporting"],"title":"Clinical trial registration and reporting: a survey of academic organizations in the United States","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"8acaa7395e5af71ab9fdca2418074352","permalink":"https://forrt.org/curated_resources/clinical-trial-registration-a-statement/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/clinical-trial-registration-a-statement/","section":"curated_resources","summary":"Altruism and trust lie at the heart of research on human subjects. Altruistic individuals volunteer for research because they trust that their participation will contribute to improved health for others and that researchers will minimise risks to participants. In return for the altruism and trust that make clinical research possible, the research enterprise has an obligation to conduct research ethically and to report it honestly. Honest reporting begins with revealing the existence of all clinical studies, even those that reflect unfavourably on a research sponsor’s product. Unfortunately, selective reporting of trials does occur, and it distorts the body of evidence available for clinical decision-making. Researchers (and journal editors) are generally most enthusiastic about the publication of trials that show either a large effect of a new treatment (positive trials) or equivalence of two approaches to treatment (non-inferiority trials). Researchers (and journals) typically are less excited about trials that show that a new treatment is inferior to standard treatment (negative trials) and even less interested in trials that are neither clearly positive nor clearly negative, since inconclusive trials will not in themselves change practice. Irrespective of their scientific interest, trial results that place financial interests at risk are particularly likely to remain unpublished and hidden from public view. The interests of the sponsor or authors notwithstanding, anyone should be able to learn of any trial’s existence and its important characteristics.","tags":["Preregistration","Medicine","Clinical Trial Registration"],"title":"Clinical trial registration: A statement from the International Committee of Medical Journal Editors","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"fc0079a7292f89d7641b2fcd27c98662","permalink":"https://forrt.org/curated_resources/clinical-trial-registration-looking-back/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/clinical-trial-registration-looking-back/","section":"curated_resources","summary":"In 2005, the International Committee of Medical Journal Editors (ICMJE) initiated a policy requiring investigators to deposit information about trial design into an accepted clinical trials registry before the onset of patient enrolment. 1 This policy aimed to ensure that information about the existence and design of clinically directive trials was publicly available, an ideal that leaders in evidence-based medicine have advocated for decades. 2 The policy precipitated much angst among research investigators and sponsors, who feared that registration would be burdensome and would stifle competition. Yet, the response to this policy has been overwhelming. The ICMJE promised to re-evaluate the policy 2 years after implementation. Here, we summarise that re-evaluation, specifically commenting on registries that meet the policy requirements, the types of studies that require registration, and the registration of trial results. As is always the case, the ICMJE establishes policy only for the 12 member journals, but many other journals have adopted our initial trial registration recommendations, and we hope that they will also adopt the modifications discussed in this update.","tags":["Trial Registration","Medicine","Preregistration"],"title":"Clinical trial registration: Looking back and moving ahead","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"4d016d41b8f114eecfb6b3761929d7f3","permalink":"https://forrt.org/curated_resources/clinical-trial-registration-transparency/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/clinical-trial-registration-transparency/","section":"curated_resources","summary":"All parties associated in clinical trials—patients, doctors, scientists, industry—share a common desire for a vigorous clinical research enterprise that brings innovations to patients as quickly as possible. However, recent scandals in the UK  and elsewhere have diminished public trust in the clinical research industry. This distrust is worrisome because industry funds an increasing proportion of clinical trials, and distrust might result in lower enrolment in trials and slower innovation.","tags":["Trial Registration","Medicine","Preregistration"],"title":"Clinical trial registration: transparency is the watchword","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1721390377,"objectID":"a04fdf885b60080aa3667d27e86435c0","permalink":"https://forrt.org/glossary/english/co_production/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/english/co_production/","section":"glossary","summary":"","tags":null,"title":"Co-production","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1635961284,"objectID":"5cb1eb3d07b4519e653f66ebc43a5cdf","permalink":"https://forrt.org/glossary/vbeta/co-production/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/vbeta/co-production/","section":"glossary","summary":"","tags":null,"title":"Co-production","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1720865851,"objectID":"95eb11d61be87590cc2788777744e288","permalink":"https://forrt.org/glossary/german/co_production/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/german/co_production/","section":"glossary","summary":"","tags":null,"title":"Co-production (Ko-Produktion)","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1721390377,"objectID":"feb00e1ed97fc27ad5fe72c74d7c5601","permalink":"https://forrt.org/glossary/english/coar_community_framework_for_good_practices_in_repositories/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/english/coar_community_framework_for_good_practices_in_repositories/","section":"glossary","summary":"","tags":null,"title":"COAR Community Framework for Good Practices in Repositories","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1720865851,"objectID":"a79eb21cab61dda7fa3813f48b7b0e40","permalink":"https://forrt.org/glossary/german/coar_community_framework_for_good_practices_in_repositories/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/german/coar_community_framework_for_good_practices_in_repositories/","section":"glossary","summary":"","tags":null,"title":"COAR Community Framework for Good Practices in Repositories","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1635961284,"objectID":"84c2047aa04432a58996ebebc72c28ea","permalink":"https://forrt.org/glossary/vbeta/coar-community-framework-for-good-p/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/vbeta/coar-community-framework-for-good-p/","section":"glossary","summary":"","tags":null,"title":"COAR Community Framework for Good Practices in Repositories","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1721390377,"objectID":"ff721a5a246f2f3e7cf4ce8e10ff490a","permalink":"https://forrt.org/glossary/english/code_review/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/english/code_review/","section":"glossary","summary":"","tags":null,"title":"Code review","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1635961284,"objectID":"b00630a953d18ab530c456d8d44b9eb5","permalink":"https://forrt.org/glossary/vbeta/code-review/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/vbeta/code-review/","section":"glossary","summary":"","tags":null,"title":"Code review","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1720865851,"objectID":"34da4449d9fcbbd7c41263cda106aa7a","permalink":"https://forrt.org/glossary/german/code_review/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/german/code_review/","section":"glossary","summary":"","tags":null,"title":"Code review (Code-Überprüfung)","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1721390377,"objectID":"79f4faf276e32c8714da1e3580eb1d56","permalink":"https://forrt.org/glossary/english/codebook/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/english/codebook/","section":"glossary","summary":"","tags":null,"title":"Codebook","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1635961284,"objectID":"136fb7fb4acba9fd115d5d56da67f4cb","permalink":"https://forrt.org/glossary/vbeta/codebook/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/vbeta/codebook/","section":"glossary","summary":"","tags":null,"title":"Codebook","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1720865851,"objectID":"b3cbb6b7a3c8d52ff6f9b371b01060d7","permalink":"https://forrt.org/glossary/german/codebook/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/german/codebook/","section":"glossary","summary":"","tags":null,"title":"Codebook (Codebuch)","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1689122843,"objectID":"72621c57f38cf7844e47aff529091a58","permalink":"https://forrt.org/curated_resources/cognitive-science-stackexchange-site-for/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/cognitive-science-stackexchange-site-for/","section":"curated_resources","summary":"A website about questions on psychology","tags":["Website"],"title":"Cognitive Science StackExchange site (for psychology Q\u0026A)","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1721390377,"objectID":"83685ece6a29e30c5a2c2fe8b1a6c313","permalink":"https://forrt.org/glossary/english/collaborative_replication_and_education_project/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/english/collaborative_replication_and_education_project/","section":"glossary","summary":"","tags":null,"title":"Collaborative Replication and Education Project (CREP)","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1720865851,"objectID":"1878ff1625f8a8c4d097bb40dff72162","permalink":"https://forrt.org/glossary/german/collaborative_replication_and_education_project/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/german/collaborative_replication_and_education_project/","section":"glossary","summary":"","tags":null,"title":"Collaborative Replication and Education Project (CREP)","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1635961284,"objectID":"d1fcd445fc6cdd02f5906a0416a0d11b","permalink":"https://forrt.org/glossary/vbeta/collaborative-replication-and-educa/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/vbeta/collaborative-replication-and-educa/","section":"glossary","summary":"","tags":null,"title":"Collaborative Replication and Education Project (CREP)","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1721390377,"objectID":"027a7ba362f1e5374d9a33037c354c77","permalink":"https://forrt.org/glossary/english/committee_on_best_practices_in_data_analysis_and_sharing/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/english/committee_on_best_practices_in_data_analysis_and_sharing/","section":"glossary","summary":"","tags":null,"title":"Committee on Best Practices in Data Analysis and Sharing (COBIDAS)","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1720865851,"objectID":"d34f60438a708e67eeae687333ced838","permalink":"https://forrt.org/glossary/german/committee_on_best_practices_in_data_analysis_and_sharing/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/german/committee_on_best_practices_in_data_analysis_and_sharing/","section":"glossary","summary":"","tags":null,"title":"Committee on Best Practices in Data Analysis and Sharing (COBIDAS)","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1635961284,"objectID":"1503d4a78aa635ea6a43f26a2b8f8718","permalink":"https://forrt.org/glossary/vbeta/committee-on-best-practices-in-data/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/vbeta/committee-on-best-practices-in-data/","section":"glossary","summary":"","tags":null,"title":"Committee on Best Practices in Data Analysis and Sharing (COBIDAS)","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"53325a02d3cfc20c1e2a039653712b6e","permalink":"https://forrt.org/curated_resources/common-statistical-tests-are-linear-mode/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/common-statistical-tests-are-linear-mode/","section":"curated_resources","summary":"This document is summarised in the table below. It shows the linear models underlying common parametric and “non-parametric” tests. Formulating all the tests in the same language highlights the many similarities between them. ","tags":["Statistics"],"title":"Common statistical tests are linear models (or: how to teach stats)","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1721390377,"objectID":"8137ddbf63de1ce93c57ccd2f68a955d","permalink":"https://forrt.org/glossary/english/communality/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/english/communality/","section":"glossary","summary":"","tags":null,"title":"Communality","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1635961284,"objectID":"45abeebcf3952b9e511392298b39c4bb","permalink":"https://forrt.org/glossary/vbeta/communality/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/vbeta/communality/","section":"glossary","summary":"","tags":null,"title":"Communality","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1720865851,"objectID":"92f50a4cbba83c7aa340baaed6490a6f","permalink":"https://forrt.org/glossary/german/communality/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/german/communality/","section":"glossary","summary":"","tags":null,"title":"Communality (Kommunalität)","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1721390377,"objectID":"745bb99ba830d4cbdd65e36c832da3d0","permalink":"https://forrt.org/glossary/english/community_projects/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/english/community_projects/","section":"glossary","summary":"","tags":null,"title":"Community Projects","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1635961284,"objectID":"21a8e1627db1adc0ce81df45fab848ed","permalink":"https://forrt.org/glossary/vbeta/community-projects/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/vbeta/community-projects/","section":"glossary","summary":"","tags":null,"title":"Community Projects","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1720865851,"objectID":"5db6e487d1bb23f24c6b68f670fb0801","permalink":"https://forrt.org/glossary/german/community_projects/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/german/community_projects/","section":"glossary","summary":"","tags":null,"title":"Community Projects (Gemeinschaftsprojekte)","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"2836b14aa1d41fee1bbcc9122e282c56","permalink":"https://forrt.org/curated_resources/compare-a-prospective-cohort-study-corre/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/compare-a-prospective-cohort-study-corre/","section":"curated_resources","summary":"Discrepancies between pre-specified and reported outcomes are an important source of bias in trials. Despite legislation, guidelines and public commitments on correct reporting from journals, outcome misreporting continues to be prevalent. We aimed to document the extent of misreporting, establish whether it was possible to publish correction letters on all misreported trials as they were published, and monitor responses from editors and trialists to understand why outcome misreporting persists despite public commitments to address it. Methods We identified five high-impact journals endorsing Consolidated Standards of Reporting Trials (CONSORT) (New England Journal of Medicine, The Lancet, Journal of the American Medical Association, British Medical Journal, and Annals of Internal Medicine) and assessed all trials over a six-week period to identify every correctly and incorrectly reported outcome, comparing published reports against published protocols or registry entries, using CONSORT as the gold standard. A correction letter describing all discrepancies was submitted to the journal for all misreported trials, and detailed coding sheets were shared publicly. The proportion of letters published and delay to publication were assessed over 12 months of follow-up. Correspondence received from journals and authors was documented and themes were extracted. Results Sixty-seven trials were assessed in total. Outcome reporting was poor overall and there was wide variation between journals on pre-specified primary outcomes (mean 76% correctly reported, journal range 25–96%), secondary outcomes (mean 55%, range 31–72%), and number of undeclared additional outcomes per trial (mean 5.4, range 2.9–8.3). Fifty-eight trials had discrepancies requiring a correction letter (87%, journal range 67–100%). Twenty-three letters were published (40%) with extensive variation between journals (range 0–100%). Where letters were published, there were delays (median 99 days, range 0–257 days). Twenty-nine studies had a pre-trial protocol publicly available (43%, range 0–86%). Qualitative analysis demonstrated extensive misunderstandings among journal editors about correct outcome reporting and CONSORT. Some journals did not engage positively when provided correspondence that identified misreporting; we identified possible breaches of ethics and publishing guidelines. Conclusions All five journals were listed as endorsing CONSORT, but all exhibited extensive breaches of this guidance, and most rejected correction letters documenting shortcomings. Readers are likely to be misled by this discrepancy. We discuss the advantages of prospective methodology research sharing all data openly and pro-actively in real time as feedback on critiqued studies. This is the first empirical study of major academic journals’ willingness to publish a cohort of comparable and objective correction letters on misreported high-impact studies. Suggested improvements include changes to correspondence processes at journals, alternatives for indexed post-publication peer review, changes to CONSORT’s mechanisms for enforcement, and novel strategies for research on methods and reporting.","tags":["Bias","Data","Preregistration","Registration","Reporting","Reporting Guidelines","Reproducibility"],"title":"COMPare: a prospective cohort study correcting and monitoring 58 misreported trials in real time","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"cfaee56ba505557ae094c1b709863259","permalink":"https://forrt.org/curated_resources/comparing-analysis-blinding-with-preregi/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/comparing-analysis-blinding-with-preregi/","section":"curated_resources","summary":"In psychology, preregistration is the most widely used method to ensure the confirmatory status of analyses. However, the method has disadvantages: Not only is it perceived as effortful and time-consuming, but reasonable deviations from the analysis plan demote the status of the study to exploratory. An alternative to preregistration is analysis blinding, in which researchers develop their analysis on an altered version of the data. In this experimental study, we compare the reported efficiency and convenience of the two methods in the context of the Many-Analysts Religion Project. In this project, 120 teams answered the same research questions on the same data set, either preregistering their analysis (n = 61) or using analysis blinding (n = 59). Our results provide strong evidence (Bayes factor [BF] = 71.40) for the hypothesis that analysis blinding leads to fewer deviations from the analysis plan, and if teams deviated, they did so on fewer aspects. Contrary to our hypothesis, we found strong evidence (BF = 13.19) that both methods required approximately the same amount of time. Finally, we found no and moderate evidence on whether analysis blinding was perceived as less effortful and frustrating, respectively. We conclude that analysis blinding does not mean less work, but researchers can still benefit from the method because they can plan more appropriate analyses from which they deviate less frequently.","tags":["Open Science","Metascience","Replication Crisis","Many Analysts","Open Data","Open Materials","Preregistration"],"title":"Comparing Analysis Blinding With Preregistration in the Many-Analysts Religion Project","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1689122843,"objectID":"bf321fece5406ddd7be2f55f51bead53","permalink":"https://forrt.org/curated_resources/comparing-published-scientific-journal-a/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/comparing-published-scientific-journal-a/","section":"curated_resources","summary":"Academic publishers claim that they add value to scholarly communications by coordinating reviews and contributing and enhancing text during publication. These contributions come at a considerable cost: U.S. academic libraries paid $1.7 billion for serial subscriptions in 2008 alone. Library budgets, in contrast, are flat and not able to keep pace with serial price inflation. We have investigated the publishers' value proposition by conducting a comparative study of pre-print papers and their final published counterparts. This comparison had two working assumptions: 1) if the publishers' argument is valid, the text of a pre-print paper should vary measurably from its corresponding final published version, and 2) by applying standard similarity measures, we should be able to detect and quantify such differences. Our analysis revealed that the text contents of the scientific papers generally changed very little from their pre-print to final published versions. These findings contribute empirical indicators to discussions of the added value of commercial publishers and therefore should influence libraries' economic decisions regarding access to scholarly publications.","tags":["Reproducibility Knowledge"],"title":"Comparing Published Scientific Journal Articlesto Their Pre-print VersionsMart","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"7e9b2ee9ed43b5b11e0ab8285165034b","permalink":"https://forrt.org/curated_resources/comparison-of-preregistration-platforms/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/comparison-of-preregistration-platforms/","section":"curated_resources","summary":"Preregistration forces researchers to front-load a lot of decision-making to an early stage of a project. Choosing which preregistration platform to use is one of those early decisions, and because a preregistration cannot be moved, that choice is permanent. This article aims to help researchers who are already interested in preregistration choose a platform by clarifying differences between them. Preregistration criteria and features are explained and analyzed for popular sites that cater to a broad range of research fields, including: GitHub, AsPredicted, Zenodo, the Open Science Framework (OSF), and an “open-ended” variant of OSF. Platforms such as ClinicalTrials.gov for clinical research and PROSPERO for systematic reviews are not compared because they are limited to specific types of studies rather than general purpose research. While a private prespecification document can help mitigate self-deception, this guide considers publicly shared preregistrations that aim to improve credibility. It therefore defines three of the criteria (a timestamp, a registry, and persistence) as a bare minimum to meet the definition of a preregistration. Additional helpful features are also listed. GitHub and AsPredicted do not meet all three basic criteria. Zenodo and OSF meet the basic criteria and vary in which additional features they offer.","tags":["Preregistration"],"title":"Comparison of Preregistration Platforms","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"8f81f1e65a1500ce92848fb7f967f2ae","permalink":"https://forrt.org/curated_resources/comparison-of-registered-and-published-o/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/comparison-of-registered-and-published-o/","section":"curated_resources","summary":"Clinical trial registries can improve the validity of trial results by facilitating comparisons between prospectively planned and reported outcomes. Previous reports on the frequency of planned and reported outcome inconsistencies have reported widely discrepant results. It is unknown whether these discrepancies are due to differences between the included trials, or to methodological differences between studies. We aimed to systematically review the prevalence and nature of discrepancies between registered and published outcomes among clinical trials. Methods We searched MEDLINE via PubMed, EMBASE, and CINAHL, and checked references of included publications to identify studies that compared trial outcomes as documented in a publicly accessible clinical trials registry with published trial outcomes. Two authors independently selected eligible studies and performed data extraction. We present summary data rather than pooled analyses owing to methodological heterogeneity among the included studies. Results Twenty-seven studies were eligible for inclusion. The overall risk of bias among included studies was moderate to high. These studies assessed outcome agreement for a median of 65 individual trials (interquartile range [IQR] 25–110). The median proportion of trials with an identified discrepancy between the registered and published primary outcome was 31 %; substantial variability in the prevalence of these primary outcome discrepancies was observed among the included studies (range 0 % (0/66) to 100 % (1/1), IQR 17–45 %). We found less variability within the subset of studies that assessed the agreement between prospectively registered outcomes and published outcomes, among which the median observed discrepancy rate was 41 % (range 30 % (13/43) to 100 % (1/1), IQR 33–48 %). The nature of observed primary outcome discrepancies also varied substantially between included studies. Among the studies providing detailed descriptions of these outcome discrepancies, a median of 13 % of trials introduced a new, unregistered outcome in the published manuscript (IQR 5–16 %). Conclusions Discrepancies between registered and published outcomes of clinical trials are common regardless of funding mechanism or the journals in which they are published. Consistent reporting of prospectively defined outcomes and consistent utilization of registry data during the peer review process may improve the validity of clinical trial publications.","tags":["Data","Publishing"],"title":"Comparison of registered and published outcomes in randomized controlled trials: a systematic review","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1689122843,"objectID":"21fc45c52120a108177fa2be2b400976","permalink":"https://forrt.org/curated_resources/comparison-of-registered-and-published-p/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/comparison-of-registered-and-published-p/","section":"curated_resources","summary":"Context: As of 2005, the International Committee of Medical Journal Editors required investigators to register their trials prior to participant enrollment as a precondition for publishing the trial's findings in member journals. Objective: To assess the proportion of registered trials with results recently published in journals with high impact factors; to compare the primary outcomes specified in trial registries with those reported in the published articles; and to determine whether primary outcome reporting bias favored significant outcomes. Data sources and study selection: MEDLINE via PubMed was searched for reports of randomized controlled trials (RCTs) in 3 medical areas (cardiology, rheumatology, and gastroenterology) indexed in 2008 in the 10 general medical journals and specialty journals with the highest impact factors. Data extraction: For each included article, we obtained the trial registration information using a standardized data extraction form. Results: Of the 323 included trials, 147 (45.5%) were adequately registered (ie, registered before the end of the trial, with the primary outcome clearly specified). Trial registration was lacking for 89 published reports (27.6%), 45 trials (13.9%) were registered after the completion of the study, 39 (12%) were registered with no or an unclear description of the primary outcome, and 3 (0.9%) were registered after the completion of the study and had an unclear description of the primary outcome. Among articles with trials adequately registered, 31% (46 of 147) showed some evidence of discrepancies between the outcomes registered and the outcomes published. The influence of these discrepancies could be assessed in only half of them and in these statistically significant results were favored in 82.6% (19 of 23). Conclusion: Comparison of the primary outcomes of RCTs registered with their subsequent publication indicated that selective outcome reporting is prevalent.","tags":[""],"title":"Comparison of Registered and Published Primary Outcomes in Randomized Controlled Trials","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1721390377,"objectID":"fdfff49f19196b2e5ae929c2fd5ff004","permalink":"https://forrt.org/glossary/english/compendium/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/english/compendium/","section":"glossary","summary":"","tags":null,"title":"Compendium","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1635961284,"objectID":"afc577d74380607f707663b63b9a6923","permalink":"https://forrt.org/glossary/vbeta/compendium/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/vbeta/compendium/","section":"glossary","summary":"","tags":null,"title":"Compendium","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1720865851,"objectID":"2807990774d24302cf4035d75bc5e852","permalink":"https://forrt.org/glossary/german/compendium/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/german/compendium/","section":"glossary","summary":"","tags":null,"title":"Compendium (Handbuch)","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1721390377,"objectID":"ab8aa537918bca89e82241834ec52e57","permalink":"https://forrt.org/glossary/english/computational_reproducibility/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/english/computational_reproducibility/","section":"glossary","summary":"","tags":null,"title":"Computational reproducibility","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1635961284,"objectID":"4fa731ca7e9d33eb3a6db8e0ff772606","permalink":"https://forrt.org/glossary/vbeta/computational-reproducibility/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/vbeta/computational-reproducibility/","section":"glossary","summary":"","tags":null,"title":"Computational reproducibility","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1720865851,"objectID":"6165fd8f07b2bd28960ab01e4119ebc1","permalink":"https://forrt.org/glossary/german/computational_reproducibility/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/german/computational_reproducibility/","section":"glossary","summary":"","tags":null,"title":"Computational reproducibility (rechnerische Reproduzierbarkeit)","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"e305f6c2c5b427a76364025d74850d0f","permalink":"https://forrt.org/curated_resources/computational-social-science-is-growing/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/computational-social-science-is-growing/","section":"curated_resources","summary":"Puberty is a phase in which individuals often test the boundaries of themselves and surrounding others and further define their identity – and thus their uniqueness compared to other individuals. Similarly, as Computational Social Science (CSS) grows up, it must strike a balance between its own practices and those of neighboring disciplines to achieve scientific rigor and refine its identity. However, there are certain areas within CSS that are reluctant to adopt rigorous scientific practices from other fields, which can be observed through an overreliance on passively collected data (e.g., through digital traces, wearables) without questioning the validity of such data. This paper argues that CSS should embrace the potential of combining both passive and active measurement practices to capitalize on the strengths of each approach, including objectivity and psychological quality. Additionally, the paper suggests that CSS would benefit from integrating practices and knowledge from other established disciplines, such as measurement validation, theoretical embedding, and open science practices. Based on this argument, the paper provides ten recommendations for CSS to mature as an interdisciplinary field of research.","tags":["Computational Social Science","Passive Measurement","Digital Trace Data","Validity","Open science Practices","Metascience"],"title":"Computational social science is growing up: why puberty consists of embracing measurement validation, theory development, and open science practices","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1721390377,"objectID":"fa6a0575e47378b4756369656effe472","permalink":"https://forrt.org/glossary/english/conceptual_replication/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/english/conceptual_replication/","section":"glossary","summary":"","tags":null,"title":"Conceptual replication","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1635961284,"objectID":"34ecb5d97f0d33162735e23ea0938e04","permalink":"https://forrt.org/glossary/vbeta/conceptual-replication/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/vbeta/conceptual-replication/","section":"glossary","summary":"","tags":null,"title":"Conceptual replication","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1720865851,"objectID":"3149f454d9ccf473f1ff5f590d0cd6e7","permalink":"https://forrt.org/glossary/german/conceptual_replication/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/german/conceptual_replication/","section":"glossary","summary":"","tags":null,"title":"Conceptual replication (Konzeptionelle Replikation)","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"3688c2fec745d916cd17ebf464bf318a","permalink":"https://forrt.org/curated_resources/conducting-research-with-people-in-lower/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/conducting-research-with-people-in-lower/","section":"curated_resources","summary":"In recent years, the field of psychology has increasingly recognized the importance of conducting research with lower-socioeconomic-status (SES) participants. Given that SES can powerfully shape people’s thoughts and actions, socioeconomically diverse samples are necessary for rigorous, generalizable research. However, even when researchers aim to collect data with these samples, they often encounter methodological and practical challenges to recruiting and retaining lower-SES participants in their studies. We propose that there are two key factors to consider when trying to recruit and retain lower-SES participants—trust and accessibility. Researchers can build trust by creating personal connections with participants and communities, paying participants fairly, and considering how participants will view their research. Researchers can enhance accessibility by recruiting in participants’ own communities, tailoring study administration to participants’ circumstances, and being flexible in payment methods. Our goal is to provide recommendations that can help to build a more inclusive science.","tags":["Diversity","Participants","Socioeconomic Status","SES"],"title":"Conducting Research With People in Lower-Socioeconomic-Status Contexts","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1721390377,"objectID":"863ef41c5a9d992a4edcab53d8e59c7d","permalink":"https://forrt.org/glossary/english/confirmation_bias/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/english/confirmation_bias/","section":"glossary","summary":"","tags":null,"title":"Confirmation bias","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1635961284,"objectID":"d5254ebdacc051c92a43c7f4e69252ae","permalink":"https://forrt.org/glossary/vbeta/confirmation-bias/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/vbeta/confirmation-bias/","section":"glossary","summary":"","tags":null,"title":"Confirmation bias","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1720865851,"objectID":"e1f0b00f89cf99edec1637ed2ef0ba4f","permalink":"https://forrt.org/glossary/german/confirmation_bias/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/german/confirmation_bias/","section":"glossary","summary":"","tags":null,"title":"Confirmation bias (Bestätigungsverzerrung)","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1721390377,"objectID":"50c61c330bcec4f8311c302e7b9daedd","permalink":"https://forrt.org/glossary/english/confirmatory_analyses/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/english/confirmatory_analyses/","section":"glossary","summary":"","tags":null,"title":"Confirmatory analyses","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1635961284,"objectID":"5f6f2e33352857e5c3db34392d8d7701","permalink":"https://forrt.org/glossary/vbeta/confirmatory-analyses/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/vbeta/confirmatory-analyses/","section":"glossary","summary":"","tags":null,"title":"Confirmatory analyses","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1720865851,"objectID":"8d0d9b33ea5079568e21a82bbe2f9ee4","permalink":"https://forrt.org/glossary/german/confirmatory_analyses/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/german/confirmatory_analyses/","section":"glossary","summary":"","tags":null,"title":"Confirmatory analyses (konfirmatorische Analysen)","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1721390377,"objectID":"cda15abb9aa3204993f3d6d0927ab91c","permalink":"https://forrt.org/glossary/english/conflict_of_interest/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/english/conflict_of_interest/","section":"glossary","summary":"","tags":null,"title":"Conflict of interest","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1635961284,"objectID":"bc9cf1cbd16cd286b60f92e8ca45766a","permalink":"https://forrt.org/glossary/vbeta/conflict-of-interest/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/vbeta/conflict-of-interest/","section":"glossary","summary":"","tags":null,"title":"Conflict of interest ","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1720865851,"objectID":"3e834357ef87377dbfa7ca8c3f194a48","permalink":"https://forrt.org/glossary/german/conflict_of_interest/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/german/conflict_of_interest/","section":"glossary","summary":"","tags":null,"title":"Conflict of interest (Interessenkonflikt) ]","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"f84b281f2fe33d681f28e7b1734b65cc","permalink":"https://forrt.org/curated_resources/connecting-research-tools-to-the-open-sc/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/connecting-research-tools-to-the-open-sc/","section":"curated_resources","summary":"This webinar (recorded Sept. 27, 2017) introduces how to connect other services as add-ons to projects on the Open Science Framework (OSF; https://osf.io). Connecting services to your OSF projects via add-ons enables you to pull together the different parts of your research efforts without having to switch away from tools and workflows you wish to continue using. The OSF is a free, open source web application built to help researchers manage their workflows. The OSF is part collaboration tool, part version control software, and part data archive. The OSF connects to popular tools researchers already use, like Dropbox, Box, Github and Mendeley, to streamline workflows and increase efficiency.","tags":["Analysis","Data","Education","Materials","Open Scholarship Tools and Technologies","OSF","Publishing","Reproducibility","Research Data Management Tools","Researchers"],"title":"Connecting Research Tools to the Open Science Framework (OSF)","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"bb86bd74f49ac66930449b8987ccfacf","permalink":"https://forrt.org/curated_resources/consequences-of-low-statistical-power/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/consequences-of-low-statistical-power/","section":"curated_resources","summary":"This video will go over three issues that can arise when scientific studies have low statistical power. All materials shown in the video, as well as the content from our other videos, can be found here: https://osf.io/7gqsi/","tags":["Analysis","Data","Education","Materials","Open Scholarship Tools and Technologies","OSF","Publishing","Reproducibility","Research Data Management Tools","Researchers"],"title":"Consequences of Low Statistical Power","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1689122843,"objectID":"9d9029a7268363db7061e5f7cab7ff07","permalink":"https://forrt.org/curated_resources/consequences-of-prejudice-against-the-nu/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/consequences-of-prejudice-against-the-nu/","section":"curated_resources","summary":"Examined the consequences of prejudice against accepting the null hypothesis through (a) a mathematical model intended to stimulate the research-publication process and (b) case studies of apparent erroneous rejections of the null hypothesis in published psychological research. The input parameters for the model characterize investigators' probabilities of selecting a problem for which the null hypothesis is true, of reporting, following up on, or abandoning research when data do or do not reject the null hypothesis, and they characterize editors' probabilities of publishing manuscripts concluding in favor of or against the null hypothesis. With estimates of the input parameters based on a questionnaire survey of 75 social psychologists, the model output indicates a dysfunctional research-publication system. Particularly, the model indicates that there may be relatively few publications on problems for which the null hypothesis is (at least to a reasonable approximation) true, and of these, a high proportion will erroneously reject the null hypothesis. The case studies provide additional support for this conclusion. It is concluded that research traditions and customs of discrimination against accepting the null hypothesis may be very detrimental to research progress","tags":[""],"title":"Consequences of prejudice against the null hypothesis.","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1689122843,"objectID":"b69a27600e1aff0d3ef3b33e98234268","permalink":"https://forrt.org/curated_resources/conservative-tests-under-satisficing-mod/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/conservative-tests-under-satisficing-mod/","section":"curated_resources","summary":"Publication bias leads consumers of research to observe a selected sample of statistical estimates calculated by producers of research. We calculate critical values for statistical significance that could help to adjust after the fact for the distortions created by this selection effect, assuming that the only source of publication bias is file drawer bias. These adjusted critical values are easy to calculate and differ from unadjusted critical values by approximately 50%—rather than rejecting a null hypothesis when the t-ratio exceeds 2, the analysis suggests rejecting a null hypothesis when the t-ratio exceeds 3. Samples of published social science research indicate that on average, across research fields, approximately 30% of published t-statistics fall between the standard and adjusted cutoffs.","tags":["Reproducibility Crisis and Credibility Revolution","Open Science"],"title":"Conservative tests under satisficing models of publication bias.","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1721390377,"objectID":"2142b98d353c05529945d4e58179a7f6","permalink":"https://forrt.org/glossary/english/consortium_authorship/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/english/consortium_authorship/","section":"glossary","summary":"","tags":null,"title":"Consortium authorship","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1635961284,"objectID":"bc5b67472d3e126614b75c3503b8994a","permalink":"https://forrt.org/glossary/vbeta/consortium-authorship/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/vbeta/consortium-authorship/","section":"glossary","summary":"","tags":null,"title":"Consortium authorship","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1720865851,"objectID":"e334fcc5c34bc78b54d86bdbff85d08d","permalink":"https://forrt.org/glossary/german/consortium_authorship/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/german/consortium_authorship/","section":"glossary","summary":"","tags":null,"title":"Consortium authorship (Konsortium Autor:innenschaft)","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1720865851,"objectID":"60485d9317ab651b49f19835eea02406","permalink":"https://forrt.org/glossary/german/constraints_on_generality/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/german/constraints_on_generality/","section":"glossary","summary":"","tags":null,"title":"Constraints on Generality (COG, Beschränkungen der Generalisierbarkeit)","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1721390377,"objectID":"46cacf5bb6105ee1491ddd50c18b53b1","permalink":"https://forrt.org/glossary/english/constraints_on_generality/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/english/constraints_on_generality/","section":"glossary","summary":"","tags":null,"title":"Constraints on Generality (COG)","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1635961284,"objectID":"eefd74c185b72d10208c10d28ac7fbdd","permalink":"https://forrt.org/glossary/vbeta/constraints-on-generality-cog/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/vbeta/constraints-on-generality-cog/","section":"glossary","summary":"","tags":null,"title":"Constraints on Generality (COG)","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1689122843,"objectID":"bd967d0054216f150739a1f5b6d61f29","permalink":"https://forrt.org/curated_resources/constraints-on-generality-statements-are/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/constraints-on-generality-statements-are/","section":"curated_resources","summary":"Whether or not a replication attempt counts as “direct” often cannot be determined definitively after the fact as a result of flexibility in how procedural differences are interpreted. Specifying constraints on generality in original articles can eliminate ambiguity in advance, thereby leading to a more cumulative science.","tags":["Reproducibility Crisis and Credibility Revolution","Open Science"],"title":"Constraints on generality statements are needed to define direct replication","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1689122843,"objectID":"0ef0c59b01c6664a7617a2166e8c1a1f","permalink":"https://forrt.org/curated_resources/construct-validation-in-social-and-perso/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/construct-validation-in-social-and-perso/","section":"curated_resources","summary":"The verity of results about a psychological construct hinges on the validity of its measurement, making construct validation a fundamental methodology to the scientific process. We reviewed a representative sample of articles published in the Journal of Personality and Social Psychology for construct validity evidence. We report that latent variable measurement, in which responses to items are used to represent a construct, is pervasive in social and personality research. However, the field does not appear to be engaged in best practices for ongoing construct validation. We found that validity evidence of existing and author-developed scales was lacking, with coefficient a often being the only psychometric evidence reported. We provide a discussion of why the construct validation framework is important for social and personality researchers and recommendations for improving practice.","tags":[""],"title":"Construct validation in social and personality research: Current practice and recommendations. ","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1721390377,"objectID":"9ab6184998e12d842a8eb372d290e3f2","permalink":"https://forrt.org/glossary/english/construct_validity/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/english/construct_validity/","section":"glossary","summary":"","tags":null,"title":"Construct validity","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1635961284,"objectID":"497067c5a850b0181216dbbe8b025d9b","permalink":"https://forrt.org/glossary/vbeta/construct-validity/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/vbeta/construct-validity/","section":"glossary","summary":"","tags":null,"title":"Construct validity","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1720865851,"objectID":"b06c9c706751ac4dc1b9eb83d7645e63","permalink":"https://forrt.org/glossary/german/construct_validity/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/german/construct_validity/","section":"glossary","summary":"","tags":null,"title":"Construct validity (Konstruktvalidität)","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1721390377,"objectID":"17a4af1ad5d529f17c11d15d6051a63e","permalink":"https://forrt.org/glossary/english/content_validity/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/english/content_validity/","section":"glossary","summary":"","tags":null,"title":"Content validity","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1635961284,"objectID":"330ae9be259304ac81f9da790794cedb","permalink":"https://forrt.org/glossary/vbeta/content-validity/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/vbeta/content-validity/","section":"glossary","summary":"","tags":null,"title":"Content validity","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1720865851,"objectID":"bab02228b9e44573c7fc10db3a987538","permalink":"https://forrt.org/glossary/german/content_validity/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/german/content_validity/","section":"glossary","summary":"","tags":null,"title":"Content validity (Inhaltsvalidität)","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1689122843,"objectID":"6e7a19f16b8e3ca308261056be889561","permalink":"https://forrt.org/curated_resources/continuously-cumulating-meta-analysis-an/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/continuously-cumulating-meta-analysis-an/","section":"curated_resources","summary":"The current crisis in scientific psychology about whether our findings are irreproducible was presaged years ago by Tversky and Kahneman (1971), who noted that even sophisticated researchers believe in the fallacious Law of Small Numbers—erroneous intuitions about how imprecisely sample data reflect population phenomena. Combined with the low power of most current work, this often leads to the use of misleading criteria about whether an effect has replicated. Rosenthal (1990) suggested more appropriate criteria, here labeled the continuously cumulating meta-analytic (CCMA) approach. For example, a CCMA analysis on a replication attempt that does not reach significance might nonetheless provide more, not less, evidence that the effect is real. Alternatively, measures of heterogeneity might show that two studies that differ in whether they are significant might have only trivially different effect sizes. We present a nontechnical introduction to the CCMA framework (referencing relevant software), and then explain how it can be used to address aspects of replicability or more generally to assess quantitative evidence from numerous studies. We then present some examples and simulation results using the CCMA approach that show how the combination of evidence can yield improved results over the consideration of single studies.","tags":["Reproducibility Crisis and Credibility Revolution","Open Science"],"title":"Continuously Cumulating Meta-Analysis and Replicability","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1721390377,"objectID":"e3392b7fb3d49dc8f490ac882c4a4c2d","permalink":"https://forrt.org/glossary/english/contribution/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/english/contribution/","section":"glossary","summary":"","tags":null,"title":"Contribution","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1635961284,"objectID":"5bf18efa4f169241f91645d4abebbb45","permalink":"https://forrt.org/glossary/vbeta/contribution/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/vbeta/contribution/","section":"glossary","summary":"","tags":null,"title":"Contribution ","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1720865851,"objectID":"6812bdc76f427b77c2dcf064b2532e70","permalink":"https://forrt.org/glossary/german/contribution/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/german/contribution/","section":"glossary","summary":"","tags":null,"title":"Contribution (Beitrag)","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1689122843,"objectID":"c27ac25fbb6c9fb351646650a9994c65","permalink":"https://forrt.org/curated_resources/correcting-for-bias-in-psychology-a-comp/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/correcting-for-bias-in-psychology-a-comp/","section":"curated_resources","summary":"Publication bias and questionable research practices in primary research can lead to badly overestimated effects in meta-analysis. Methodologists have proposed a variety of statistical approaches to correct for such overestimation. However, it is not clear which methods work best for data typically seen in psychology. Here, we present a comprehensive simulation study in which we examined how some of the most promising meta-analytic methods perform on data that might realistically be produced by research in psychology. We simulated several levels of questionable research practices, publication bias, and heterogeneity, and used study sample sizes empirically derived from the literature. Our results clearly indicated that no single meta-analytic method consistently outperformed all the others. Therefore, we recommend that meta-analysts in psychology focus on sensitivity analyses—that is, report on a variety of methods, consider the conditions under which these methods fail (as indicated by simulation studies such as ours), and then report how conclusions might change depending on which conditions are most plausible. Moreover, given the dependence of meta-analytic methods on untestable assumptions, we strongly recommend that researchers in psychology continue their efforts to improve the primary literature and conduct large-scale, preregistered replications. We provide detailed results and simulation code at https://osf.io/rf3ys and interactive figures at http://www.shinyapps.org/apps/metaExplorer/.","tags":["Reproducibility Crisis and Credibility Revolution","Open Science"],"title":"Correcting for Bias in Psychology: A Comparison of Meta-Analytic Methods","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1689122843,"objectID":"5f8b0c4530743848758dc3a8029b71a9","permalink":"https://forrt.org/curated_resources/correlational-effect-size-benchmarks/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/correlational-effect-size-benchmarks/","section":"curated_resources","summary":"Effect size information is essential for the scientific enterprise and plays an increasingly central role in the scientific process. We extracted 147,328 correlations and developed a hierarchical taxonomy of variables reported in Journal of Applied Psychology and Personnel Psychology from 1980 to 2010 to produce empirical effect size benchmarks at the omnibus level, for 20 common research domains, and for an even finer grained level of generality. Results indicate that the usual interpretation and classification of effect sizes as small, medium, and large bear almost no resemblance to findings in the field, because distributions of effect sizes exhibit tertile partitions at values approximately one-half to one-third those intuited by Cohen (1988). Our results offer information that can be used for research planning and design purposes, such as producing better informed non-nil hypotheses and estimating statistical power and planning sample size accordingly. We also offer information useful for understanding the relative importance of the effect sizes found in a particular study in relationship to others and which research domains have advanced more or less, given that larger effect sizes indicate a better understanding of a phenomenon. Also, our study offers information about research domains for which the investigation of moderating effects may be more fruitful and provide information that is likely to facilitate the implementation of Bayesian analysis. Finally, our study offers information that practitioners can use to evaluate the relative effectiveness of various types of interventions. ","tags":[""],"title":"Correlational Effect Size Benchmarks","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1721390377,"objectID":"cf98b6669551109074f25513ced0fdb2","permalink":"https://forrt.org/glossary/english/corrigendum/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/english/corrigendum/","section":"glossary","summary":"","tags":null,"title":"Corrigendum","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1635961284,"objectID":"be913c13c3af75981a5ac9c9cac7cf27","permalink":"https://forrt.org/glossary/vbeta/corrigendum/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/vbeta/corrigendum/","section":"glossary","summary":"","tags":null,"title":"Corrigendum","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1720865851,"objectID":"2b8a5b7aefc78c2104758b6b0d129623","permalink":"https://forrt.org/glossary/german/corrigendum/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/german/corrigendum/","section":"glossary","summary":"","tags":null,"title":"Corrigendum (Korrigendum)","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"dd27191729f0e2c6a565f7d360466f66","permalink":"https://forrt.org/curated_resources/cos-registered-reports-portal/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/cos-registered-reports-portal/","section":"curated_resources","summary":"Registered Reports: Peer review before results are known to align scientific values and practices.\n\nRegistered Reports is a publishing format used by over 250 journals that emphasizes the importance of the research question and the quality of methodology by conducting peer review prior to data collection. High quality protocols are then provisionally accepted for publication if the authors follow through with the registered methodology.\n\nThis format is designed to reward best practices in adhering to the hypothetico-deductive model of the scientific method. It eliminates a variety of questionable research practices, including low statistical power, selective reporting of results, and publication bias, while allowing complete flexibility to report serendipitous findings.\n\nThis page includes information on Registered Reports including readings on Registered Reports, Participating Journals, Details \u0026 Workflow, Resources for Editors, Resources For Funders, FAQs, and Allied Initiatives.","tags":["Preregistration","Publishing Models","Registered Reports","Reproducibility"],"title":"COS Registered Reports Portal","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"aca73bbc18035388ebb687c24890a1fa","permalink":"https://forrt.org/curated_resources/course-materials-project-tier/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/course-materials-project-tier/","section":"curated_resources","summary":"Learn about courses, in a wide range of fields at a variety of institutions, where principles and resources from Project TIER have been used to teach transparent research methods.","tags":["Reproducibility Crisis and Credibility Revolution","Open Science","Conceptual and statistical knowledge"],"title":"Course materials | Project TIER","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1689122843,"objectID":"e92453aba7066065bcd7129edf3d8a50","permalink":"https://forrt.org/curated_resources/course-syllabi-for-open-and-reproducible/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/course-syllabi-for-open-and-reproducible/","section":"curated_resources","summary":"A collection of course syllabi from any discipline featuring content to examine or improve open and reproducible research practices. Email to join project, access articles, or add other syllabi.","tags":["Collection","Reproducibility Knowledge"],"title":"course syllabi for open and reproducible science","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1721390377,"objectID":"bf494056714330710b045e179f0ac4d4","permalink":"https://forrt.org/glossary/english/creative_commons/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/english/creative_commons/","section":"glossary","summary":"","tags":null,"title":"Creative Commons (CC) license","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1720865851,"objectID":"54ed02c8d89b609241a4e54d590fada1","permalink":"https://forrt.org/glossary/german/creative_commons/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/german/creative_commons/","section":"glossary","summary":"","tags":null,"title":"Creative Commons (CC) license","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1635961284,"objectID":"37573b8822b5f4ef84705ce4e6d1b725","permalink":"https://forrt.org/glossary/vbeta/creative-commons-cc-license/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/vbeta/creative-commons-cc-license/","section":"glossary","summary":"","tags":null,"title":"Creative Commons (CC) license","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1721390377,"objectID":"883e1347a234c78cc8b3dec824ee6683","permalink":"https://forrt.org/glossary/english/creative_destruction_approach_to_replication/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/english/creative_destruction_approach_to_replication/","section":"glossary","summary":"","tags":null,"title":"Creative destruction approach to replication","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1635961284,"objectID":"c1ef003831051b2438962119db45a05c","permalink":"https://forrt.org/glossary/vbeta/creative-destruction-approach-to-re/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/vbeta/creative-destruction-approach-to-re/","section":"glossary","summary":"","tags":null,"title":"Creative destruction approach to replication","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1720865851,"objectID":"a91c7c53285f35c3bb2dcbdeebea586b","permalink":"https://forrt.org/glossary/german/creative_destruction_approach_to_replication/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/german/creative_destruction_approach_to_replication/","section":"glossary","summary":"","tags":null,"title":"Creative destruction approach to replication (Schöpferische Zerstörung als Herangehensweise an Replikationen)","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"4fe30f235f0a0d8d3742134194558366","permalink":"https://forrt.org/curated_resources/credibility-of-preprints-an-interdiscipl/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/credibility-of-preprints-an-interdiscipl/","section":"curated_resources","summary":"Preprints increase accessibility and can speed scholarly communication if researchers view them as credible enough to read and use. Preprint services do not provide the heuristic cues of a journal's reputation, selection, and peer-review processes that, regardless of their flaws, are often used as a guide for deciding what to read. We conducted a survey of 3759 researchers across a wide range of disciplines to determine the importance of different cues for assessing the credibility of individual preprints and preprint services. We found that cues related to information about open science content and independent verification of author claims were rated as highly important for judging preprint credibility, and peer views and author information were rated as less important. As of early 2020, very few preprint services display any of the most important cues. By adding such cues, services may be able to help researchers better assess the credibility of preprints, enabling scholars to more confidently use preprints, thereby accelerating scientific communication and discovery.","tags":["Preprints"],"title":"Credibility of preprints: an interdisciplinary survey of researchers","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1721390377,"objectID":"7dbdffb519c638c08ed1a294d7784d8a","permalink":"https://forrt.org/glossary/english/credibility_revolution/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/english/credibility_revolution/","section":"glossary","summary":"","tags":null,"title":"Credibility revolution","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1635961284,"objectID":"6ff819b8514dc2fa043f7d76532fdacd","permalink":"https://forrt.org/glossary/vbeta/credibility-revolution/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/vbeta/credibility-revolution/","section":"glossary","summary":"","tags":null,"title":"Credibility revolution","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1720865851,"objectID":"faf71f0a7dbad0657625245e4a175e4f","permalink":"https://forrt.org/glossary/german/credibility_revolution/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/german/credibility_revolution/","section":"glossary","summary":"","tags":null,"title":"Credibility revolution (Glaubhaftigkeitsrevolution)","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1721390377,"objectID":"3b1eaa7f02ea823e7fc580cbf56d92f1","permalink":"https://forrt.org/glossary/english/credit/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/english/credit/","section":"glossary","summary":"","tags":null,"title":"CRediT","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1720865851,"objectID":"573e57127d7907f3c29fb3de0623fa05","permalink":"https://forrt.org/glossary/german/credit/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/german/credit/","section":"glossary","summary":"","tags":null,"title":"CRediT","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1635961284,"objectID":"5fa00df68b6e6fcacc1445f1220798e2","permalink":"https://forrt.org/glossary/vbeta/credit/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/vbeta/credit/","section":"glossary","summary":"","tags":null,"title":"CRediT","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1689122843,"objectID":"6c5fdce2ffa07df24afb13061cf3ba94","permalink":"https://forrt.org/curated_resources/crep-project/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/crep-project/","section":"curated_resources","summary":"CREP’s mission is to provide training, support, and professional growth opportunities for students and instructors completing replication projects, while also addressing the need for direct and direct+ replications of highly-cited studies in the field.","tags":["Reproducibility Knowledge","OSF Project"],"title":"CREP project","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"4208323a8e9b8eccbe99e78303c34363","permalink":"https://forrt.org/curated_resources/crep-tutorials/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/crep-tutorials/","section":"curated_resources","summary":"This playlist was created to help students and their instructors complete CREP projects.","tags":["Reproducibility and replicability"],"title":"CREP tutorials","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"4d51045fa06b4c109794200cd59e858d","permalink":"https://forrt.org/curated_resources/crisis-research-fast-and-slow/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/crisis-research-fast-and-slow/","section":"curated_resources","summary":"A blog about the crisis of research","tags":["Open science"],"title":"Crisis research, fast and slow","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1721390377,"objectID":"6a2f53d0fdaeef00bea48e0bcd13bb51","permalink":"https://forrt.org/glossary/english/criterion_validity/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/english/criterion_validity/","section":"glossary","summary":"","tags":null,"title":"Criterion validity","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1635961284,"objectID":"17ba4c7fc744436ea72f48a2780258dd","permalink":"https://forrt.org/glossary/vbeta/criterion-validity/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/vbeta/criterion-validity/","section":"glossary","summary":"","tags":null,"title":"Criterion validity","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1720865851,"objectID":"f57947e2000b302ce56a8888c5766f3e","permalink":"https://forrt.org/glossary/german/criterion_validity/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/german/criterion_validity/","section":"glossary","summary":"","tags":null,"title":"Criterion validity (Kriteriumsvalidität)","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1689122843,"objectID":"e6f85d704ba0b251bdb7fc7ec6b35445","permalink":"https://forrt.org/curated_resources/crossvalidated-stackexchange-site-for-st/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/crossvalidated-stackexchange-site-for-st/","section":"curated_resources","summary":"A website about questions on statistics","tags":["Website"],"title":"CrossValidated StackExchange site (for statistics Q\u0026A)","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1721390377,"objectID":"841cf54bc5461bd343812597cdea4907","permalink":"https://forrt.org/glossary/english/crowdsourced_research/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/english/crowdsourced_research/","section":"glossary","summary":"","tags":null,"title":"Crowdsourced Research","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1635961284,"objectID":"baf4bef0ffb8d9a5a4a750af6433883c","permalink":"https://forrt.org/glossary/vbeta/crowdsourced-research/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/vbeta/crowdsourced-research/","section":"glossary","summary":"","tags":null,"title":"Crowdsourced Research","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1720865851,"objectID":"be2fd9cc38fb92fa6ace5b03d7c093a7","permalink":"https://forrt.org/glossary/german/crowdsourced_research/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/german/crowdsourced_research/","section":"glossary","summary":"","tags":null,"title":"Crowdsourced Research (Crowdsourcing-Forschung)","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"8b3e6dbc6b91b220b340daed37b562fe","permalink":"https://forrt.org/curated_resources/crowdsourced-research-many-hands-make-ti/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/crowdsourced-research-many-hands-make-ti/","section":"curated_resources","summary":"Crowdsourcing research can balance discussions, validate findings and better inform policy, say Raphael Silberzahn and Eric L. Uhlmann.","tags":["Reproducibility Crisis and Credibility Revolution","Open Science"],"title":"Crowdsourced research: Many hands make tight work","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1721390377,"objectID":"253358424db12287a3a0d1486604506d","permalink":"https://forrt.org/glossary/english/cultural_taxation/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/english/cultural_taxation/","section":"glossary","summary":"","tags":null,"title":"Cultural taxation","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1635961284,"objectID":"9ef222309767ca1e4ea0776e0ed8f3f5","permalink":"https://forrt.org/glossary/vbeta/cultural-taxation/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/vbeta/cultural-taxation/","section":"glossary","summary":"","tags":null,"title":"Cultural taxation","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1720865851,"objectID":"94ed49583952fdc3e040814010115456","permalink":"https://forrt.org/glossary/german/cultural_taxation/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/german/cultural_taxation/","section":"glossary","summary":"","tags":null,"title":"Cultural taxation (kulturelle Taxierung)","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1721390377,"objectID":"690b89b9a1018a14afd7b945265376df","permalink":"https://forrt.org/glossary/english/cumulative_science/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/english/cumulative_science/","section":"glossary","summary":"","tags":null,"title":"Cumulative science","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1635961284,"objectID":"4fd4b2794e3348202945b586dd675e42","permalink":"https://forrt.org/glossary/vbeta/cumulative-science/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/vbeta/cumulative-science/","section":"glossary","summary":"","tags":null,"title":"Cumulative science","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1720865851,"objectID":"3babceb37960f675dccfdafdbbfe9121","permalink":"https://forrt.org/glossary/german/cumulative_science/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/german/cumulative_science/","section":"glossary","summary":"","tags":null,"title":"Cumulative science (kumulative Wissenschaft)","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"cdfbaa86721aaeb3246ea2611d1a867f","permalink":"https://forrt.org/curated_resources/curate-science/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/curate-science/","section":"curated_resources","summary":"Curate Science is a unified curation system and platform to verify that research is transparent and credible. It will allow researchers, journals, universities, funders, teachers, journalists, and the general public to ensure:- Transparency: Ensure research meets minimum transparency standards appropriate to the article type and employed methodologies.- Credibility: Ensure follow-up scrutiny is linked to its parent paper, including critical commentaries, reproducibility/robustness re-analyses, and new sample replications.","tags":["Funders","Open Scholarship Guidelines","Publishers","Reproducibility","Research Administration","Researchers","Research Integrity"],"title":"Curate Science","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1689122843,"objectID":"74244be4f9af87b71ab810c83e82f218","permalink":"https://forrt.org/curated_resources/curating-research-assets-a-tutorial-on-t/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/curating-research-assets-a-tutorial-on-t/","section":"curated_resources","summary":"Recent calls for improving reproducibility have increased attention to the ways in which researchers curate, share, and collaborate on their research assets. In this Tutorial, we explain how version control systems, such as the popular Git program, support these functions and then show how to use Git with a graphical interface in the RStudio program. This Tutorial is written for researchers with no previous experience using version control systems and covers both single-user and collaborative workflows. The online Supplemental Material provides information on advanced Git command-line functions. Git presents an elegant solution to specific challenges to curating, sharing, and collaborating on research assets and can be implemented in common workflows with little extra effort.","tags":[""],"title":"Curating Research Assets: A Tutorial on the Git Version Control System","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"ddc58c1b0c7966814e0eaa0f9cdbb4b6","permalink":"https://forrt.org/curated_resources/current-incentives-for-scientists-lead-t/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/current-incentives-for-scientists-lead-t/","section":"curated_resources","summary":"We can regard the wider incentive structures that operate across science, such as the priority given to novel findings, as an ecosystem within which scientists strive to maximise their fitness (i.e., publication record and career success). Here, we develop an optimality model that predicts the most rational research strategy, in terms of the proportion of research effort spent on seeking novel results rather than on confirmatory studies, and the amount of research effort per exploratory study. We show that, for parameter values derived from the scientific literature, researchers acting to maximise their fitness should spend most of their effort seeking novel results and conduct small studies that have only 10%–40% statistical power. As a result, half of the studies they publish will report erroneous conclusions. Current incentive structures are in conflict with maximising the scientific value of research; we suggest ways that the scientific ecosystem could be improved.","tags":["Careers","Careers in Research","Drug Discovery","Ecosystems","Peer Review","Research Assessment","Research Errors","Scientists"],"title":"Current Incentives for Scientists Lead to Underpowered Studies with Erroneous Conclusions","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"77198da162a9a27a21f099ba676f33e3","permalink":"https://forrt.org/curated_resources/curtailing-the-use-of-preregistration-a/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/curtailing-the-use-of-preregistration-a/","section":"curated_resources","summary":"Improving the usability of psychological research has been encouraged through practices such as prospectively registering research plans. Registering research aligns with the open-science movement, as the registration of research protocols in publicly accessible domains can result in reduced research waste and increased study transparency. In medicine and psychology, two different terms, registration and preregistration, have been used to refer to study registration, but applying inconsistent terminology to represent one concept can complicate both educational outreach and epidemiological investigation. Consistently using one term across disciplines to refer to the concept of study registration may improve the understanding and uptake of this practice, thereby supporting the movement toward improving the reliability and reproducibility of research through study registration. We recommend encouraging use of the original term, registration, given its widespread and long-standing use, including in national registries.","tags":["Registration","Open Science","Preregistration","Registered Reports"],"title":"Curtailing the Use of Preregistration: A Misused Term","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1689122843,"objectID":"048bbf97a35cb83fc9617d6be363975b","permalink":"https://forrt.org/curated_resources/dagitty/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/dagitty/","section":"curated_resources","summary":"DAGitty is a browser-based environment for creating, editing, and analyzing causal diagrams (also known as directed acyclic graphs or causal Bayesian networks). The focus is on the use of causal diagrams for minimizing bias in empirical studies in epidemiology and other disciplines.","tags":["Website"],"title":"DAGitty","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1635961284,"objectID":"dd740ac19faf39361239f660da0e6a7c","permalink":"https://forrt.org/glossary/vbeta/data-access-and-research-transparen/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/vbeta/data-access-and-research-transparen/","section":"glossary","summary":"","tags":null,"title":"Data Access and Research Transparency (DA-RT)","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1721390377,"objectID":"5e32738a5fbf3fa9f41eeaf9bcacd689","permalink":"https://forrt.org/glossary/english/data_access_and_research_transparency/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/english/data_access_and_research_transparency/","section":"glossary","summary":"","tags":null,"title":"Data Access and Research Transparency (DA-RT)","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1720865851,"objectID":"a27165b3bd0ebf3d87f1293d74d83363","permalink":"https://forrt.org/glossary/german/data_access_and_research_transparency/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/german/data_access_and_research_transparency/","section":"glossary","summary":"","tags":null,"title":"Data Access and Research Transparency (DA-RT)","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"fa33dde7b702175cb034034e9fd7d177","permalink":"https://forrt.org/curated_resources/data-analysis-and-visualization-in-pytho/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/data-analysis-and-visualization-in-pytho/","section":"curated_resources","summary":"Python is a general purpose programming language that is useful for writing scripts to work effectively and reproducibly with data. This is an introduction to Python designed for participants with no programming experience. These lessons can be taught in one and a half days (~ 10 hours). They start with some basic information about Python syntax, the Jupyter notebook interface, and move through how to import CSV files, using the pandas package to work with data frames, how to calculate summary information from a data frame, and a brief introduction to plotting. The last lesson demonstrates how to work with databases directly from Python.","tags":["Analysis","Data","Education","Open Scholarship Tools and Technologies","Python","Reproducibility","Research Data Management Tools","Researchers"],"title":"Data Analysis and Visualization in Python for Ecologists","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"82a2df91f153089afde7e701b98b89a6","permalink":"https://forrt.org/curated_resources/data-analysis-and-visualization-in-r-for/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/data-analysis-and-visualization-in-r-for/","section":"curated_resources","summary":"Data Carpentry lesson from Ecology curriculum to learn how to analyse and visualise ecological data in R. Data Carpentry’s aim is to teach researchers basic concepts, skills, and tools for working with data so that they can get more done in less time, and with less pain. The lessons below were designed for those interested in working with ecology data in R. This is an introduction to R designed for participants with no programming experience. These lessons can be taught in a day (~ 6 hours). They start with some basic information about R syntax, the RStudio interface, and move through how to import CSV files, the structure of data frames, how to deal with factors, how to add/remove rows and columns, how to calculate summary statistics from a data frame, and a brief introduction to plotting. The last lesson demonstrates how to work with databases directly from R.","tags":["Analysis","Data","Education","Open Scholarship Tools and Technologies","R","Reproducibility","Research Data Management Tools","Researchers"],"title":"Data Analysis and Visualization in R for Ecologists","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"ecf603163399c122d670bbc970251df1","permalink":"https://forrt.org/curated_resources/data-analysis-and-visualization-with-pyt/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/data-analysis-and-visualization-with-pyt/","section":"curated_resources","summary":"Python is a general purpose programming language that is useful for writing scripts to work effectively and reproducibly with data. This is an introduction to Python designed for participants with no programming experience. These lessons can be taught in a day (~ 6 hours). They start with some basic information about Python syntax, the Jupyter notebook interface, and move through how to import CSV files, using the pandas package to work with data frames, how to calculate summary information from a data frame, and a brief introduction to plotting. The last lesson demonstrates how to work with databases directly from Python.","tags":["Analysis","Data","Education","Open Scholarship Tools and Technologies","Python","Reproducibility","Research Data Management Tools","Researchers"],"title":"Data Analysis and Visualization with Python for Social Scientists","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"ea14d811b71d80596f981ae82970a6b4","permalink":"https://forrt.org/curated_resources/data-availability-reusability-and-analyt/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/data-availability-reusability-and-analyt/","section":"curated_resources","summary":"Access to data is a critical feature of an efficient, progressive and ultimately self-correcting scientific ecosystem. But the extent to which in-principle benefits of data sharing are realized in practice is unclear. Crucially, it is largely unknown whether published findings can be reproduced by repeating reported analyses upon shared data (‘analytic reproducibility’). To investigate this, we conducted an observational evaluation of a mandatory open data policy introduced at the journal Cognition. Interrupted time-series analyses indicated a substantial post-policy increase in data available statements (104/417, 25% pre-policy to 136/174, 78% post-policy), although not all data appeared reusable (23/104, 22% pre-policy to 85/136, 62%, post-policy). For 35 of the articles determined to have reusable data, we attempted to reproduce 1324 target values. Ultimately, 64 values could not be reproduced within a 10% margin of error. For 22 articles all target values were reproduced, but 11 of these required author assistance. For 13 articles at least one value could not be reproduced despite author assistance. Importantly, there were no clear indications that original conclusions were seriously impacted. Mandatory open data policies can increase the frequency and quality of data sharing. However, suboptimal data curation, unclear analysis specification and reporting errors can impede analytic reproducibility, undermining the utility of data sharing and the credibility of scientific findings.","tags":["Data","Policy","Reproducibility"],"title":"Data availability, reusability, and analytic reproducibility: evaluating the impact of a mandatory open data policy at the journal Cognition","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"7546f7df4c40f703aedb5b1fe0bd2d24","permalink":"https://forrt.org/curated_resources/data-carpentry/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/data-carpentry/","section":"curated_resources","summary":"Data Carpentry trains researchers in the core data skills for efficient, shareable, and reproducible research practices. We run accessible, inclusive training workshops; teach openly available, high-quality, domain-tailored lessons; and foster an active, inclusive, diverse instructor community that promotes and models reproducible research as a community norm.","tags":["Analyses","Data","Reproducibility","Research Data Management","Researchers"],"title":"Data Carpentry","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"7b2aec354380bcbde67c397cedfec6f2","permalink":"https://forrt.org/curated_resources/data-carpentry-for-biologists/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/data-carpentry-for-biologists/","section":"curated_resources","summary":"The Biology Semester-long Course was developed and piloted at the University of Florida in Fall 2015. Course materials include readings, lectures, exercises, and assignments that expand on the material presented at workshops focusing on SQL and R.","tags":["Analysis","Data","Education","Open Scholarship Tools and Technologies","R","Reproducibility","Research Data Management Tools","Researchers","SQL"],"title":"Data Carpentry for Biologists","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"de7ff9f01c134077d4031aa822fe3fc5","permalink":"https://forrt.org/curated_resources/data-cleaning-and-management-using-openr/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/data-cleaning-and-management-using-openr/","section":"curated_resources","summary":"Course materials on using OpenRefine, a powerful tool for cleaning and transforming tabular data.","tags":["Analysis","Data","Open Scholarship Tools and Technologies","Research Data Management Tools","Researchers"],"title":"Data Cleaning and Management Using OpenRefine","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"a13071fbe0891990599e6ab0f19b9d91","permalink":"https://forrt.org/curated_resources/data-cleaning-with-openrefine-for-ecolog/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/data-cleaning-with-openrefine-for-ecolog/","section":"curated_resources","summary":"A part of the data workflow is preparing the data for analysis. Some of this involves data cleaning, where errors in the data are identified and corrected or formatting made consistent. This step must be taken with the same care and attention to reproducibility as the analysis. OpenRefine (formerly Google Refine) is a powerful free and open source tool for working with messy data: cleaning it and transforming it from one format into another. This lesson will teach you to use OpenRefine to effectively clean and format data and automatically track any changes that you make. Many people comment that this tool saves them literally months of work trying to make these edits by hand.","tags":["Analysis","Data","Education","Open Scholarship Tools and Technologies","Reproducibility","Research Data Management Tools","Researchers"],"title":"Data Cleaning with OpenRefine for Ecologists","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"d030362dc15ced8d4a4daf61c0bbb113","permalink":"https://forrt.org/curated_resources/data-intro-for-archivists/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/data-intro-for-archivists/","section":"curated_resources","summary":"This Library Carpentry lesson introduces archivists to working with data. At the conclusion of the lesson you will: be able to explain terms, phrases, and concepts in code or software development; identify and use best practice in data structures; use regular expressions in searches.","tags":["Analysis","Data","Education","Librarians","Open Scholarship Tools and Technologies","Reproducibility","Research Data Management Tools"],"title":"Data Intro for Archivists","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"04548fbaa854222cfd5584e496ff18f6","permalink":"https://forrt.org/curated_resources/data-is-present-open-workshops-and-hacka/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/data-is-present-open-workshops-and-hacka/","section":"curated_resources","summary":"Original data has become more accessible thanks to cultural and technological advances. On the internet, we can find innumerable data sets from sources such as scientific journals and repositories, local and national governments, and non-governmental organisations. Often, these data may be presented in novel ways, by creating new tables or plots, or by integrating additional data. Free, open-source software has become a great companion for open data. This open scholarship project offers free workshops and coding meet-ups (hackathons) to learn and practise data presentation, across the UK. It is made possible by a fellowship of the Software Sustainability Institute.","tags":["Data","Librarians","Open Data","Open Source Software","Publishers","Researchers","Visualization"],"title":"Data Is Present: Open Workshops and Hackathons","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"3e3bb621eb24ccd155f3f65473f7afd7","permalink":"https://forrt.org/curated_resources/data-management/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/data-management/","section":"curated_resources","summary":"A series of video of data management","tags":["Data management"],"title":"Data management","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"8031fa666ee92ae606ebdc3473aa24fd","permalink":"https://forrt.org/curated_resources/data-management-reproducibility/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/data-management-reproducibility/","section":"curated_resources","summary":"Introduction to data management and reproducibility for researchers as a presentation.","tags":["Analysis","Data","Open Scholarship Tools and Technologies","Reproducibility","Research Data Management","Researchers"],"title":"Data Management \u0026 Reproducibility","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"87c2438aae1fadca582028ca47e957e5","permalink":"https://forrt.org/curated_resources/data-management-and-data-sharing-in-psyc/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/data-management-and-data-sharing-in-psyc/","section":"curated_resources","summary":"Providing access to research data collected as part of scientific publications and publicly funded research projects is now regarded as a central aspect of an open and transparent scientific practice and is increasingly being called for by funding institutions and scientific journals. To this end, researchers should strive to comply with the so-called FAIR principles (of scientific data management), that is, research data should be findable, accessible, interoperable, and reusable. Systematic data management supports these goals and, at the same time, makes it possible to achieve them efficiently. With these revised recommendations on data management and data sharing, which also draw on feedback from a 2018 survey of its members, the German Psychological Society (Deutsche Gesellschaft für Psychologie; DGPs) specifies important basic principles of data management in psychology. Initially, based on discipline-specific definitions of raw data, primary data, secondary data, and metadata, we provide recommendations on the degree of data processing necessary when publishing data. We then discuss data protection as well as aspects of copyright and data usage before defining the qualitative requirements for trustworthy research data repositories. This is followed by a detailed discussion of pragmatic aspects of data sharing, such as the differences between Type 1 and Type 2 data publications, restrictions on use (embargo period), the definition of \"scientific use\" by secondary users of shared data, and recommendations on how to resolve potential disputes. Particularly noteworthy is the new recommendation of distinct \"access categories\" for data, each with different requirements in terms of data protection or research ethics. These range from completely open data without usage restrictions (\"access category 0\") to data shared under a set of standardized conditions (e.g., reuse restricted to scientific purposes; \"access category 1\"), individualized usage agreements (\"access category 2\"), and secure data access under strictly controlled conditions (e.g., in a research data center; “access category 3\"). The practical implementation of this important innovation, however, will require data repositories to provide the necessary technical functionalities. In summary, the revised recommendations aim to present pragmatic guidelines for researchers to handle psychological research data in an open and transparent manner, while addressing structural challenges to data sharing solutions that are beneficial for all involved parties.","tags":["Open data"],"title":"Data Management and Data Sharing in Psychological Science: Revision of the DGPs Recommendations","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"ce3273359adbaa1a31cf21fca9786694","permalink":"https://forrt.org/curated_resources/data-management-for-psychological-scienc/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/data-management-for-psychological-scienc/","section":"curated_resources","summary":"Data management - including data preparation, cleaning, storage, and sharing - is critical to psychological research. Despite its importance, data management is rarely formally taught to students. This syllabus provides detailed descriptions of data management topics, resources, and activities that can be used to create a course or workshop on data management. The syllabus is formatted as a series of modules that motivate the importance of high-quality data management and provide information on best practices at various stages - (1) What is data management and why should we care about it? , (2) Data setup and collection, (3) Data storage, (4) Data cleaning and analysis, (5) Data sharing, (6) Locating existing data, and (7) Writing a data management plan. Each module raises key questions and common errors, as well as resources and suggested assignments to help identify and circumvent mistakes and vulnerabilities. The syllabus is extremely comprehensive and should be tailored for individual use, including putting more emphasis on data management practices specific to one’s subfield. There are many different types of resources, including journal articles, blog posts, podcasts, slide decks, etc. and the syllabus can be adapted for graduate seminars, advanced undergraduate courses, or individual study. Because this syllabus links to work from many researchers, please be sure to give appropriate credit to content creators.","tags":["Data management","Reproducible Analyses"],"title":"Data Management for Psychological Science: A Crowdsourced Syllabus","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1720865851,"objectID":"fd4a20c98b6c5d077fc94aa519fe71e7","permalink":"https://forrt.org/glossary/german/data_management_plan/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/german/data_management_plan/","section":"glossary","summary":"","tags":null,"title":"Data management plan (DMP; Datenmanagementplan)","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1721390377,"objectID":"e0e341992a2a672730157d1a87bc06d0","permalink":"https://forrt.org/glossary/english/data_management_plan/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/english/data_management_plan/","section":"glossary","summary":"","tags":null,"title":"Data management plan (DMP)","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1635961284,"objectID":"2a5754851dced06293aa368a4ec391be","permalink":"https://forrt.org/glossary/vbeta/data-management-plan-dmp/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/vbeta/data-management-plan-dmp/","section":"glossary","summary":"","tags":null,"title":"Data management plan (DMP)","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"898716a9510ac229b280ede50b95f7c0","permalink":"https://forrt.org/curated_resources/data-management-plans-as-linked-open-dat/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/data-management-plans-as-linked-open-dat/","section":"curated_resources","summary":"Background\nOpen Science Graphs (OSGs) are scientific knowledge graphs representing different entities of the research lifecycle (e.g. projects, people, research outcomes, institutions) and the relationships among them. They present a contextualized view of current research that supports discovery, re-use, reproducibility, monitoring, transparency and omni-comprehensive assessment. A Data Management Plan (DMP) contains information concerning both the research processes and the data collected, generated and/or re-used during a project’s lifetime. Automated solutions and workflows that connect DMPs with the actual data and other contextual information (e.g., publications, fundings) are missing from the landscape. DMPs being submitted as deliverables also limit their findability. In an open and FAIR-enabling research ecosystem information linking between research processes and research outputs is essential. ARGOS tool for FAIR data management contributes to the OpenAIRE Research Graph (RG) and utilises its underlying services and trusted sources to progressively automate validation and automations of Research Data Management (RDM) practices.\n\nResults\nA comparative analysis was conducted between the data models of ARGOS and OpenAIRE Research Graph against the DMP Common Standard. Following this, we extended ARGOS with export format converters and semantic tagging, and the OpenAIRE RG with a DMP entity and semantics between existing entities and relationships. This enabled the integration of ARGOS machine actionable DMPs (ma-DMPs) to the OpenAIRE OSG, enriching and exposing DMPs as FAIR outputs.\n\nConclusions\nThis paper, to our knowledge, is the first to introduce exposing ma-DMPs in OSGs and making the link between OSGs and DMPs, introducing the latter as entities in the research lifecycle. Further, it provides insight to ARGOS DMP service interoperability practices and integrations to populate the OpenAIRE Research Graph with DMP entities and relationships and strengthen both FAIRness of outputs as well as information exchange in a standard way.","tags":["Research Data Management","Data Management Plans","FAIR DMPs","Machine Actionable","Research Graphs","Knowledge Graphs","Open Science"],"title":"Data management plans as linked open data: exploiting ARGOS FAIR and machine actionable outputs in the OpenAIRE research graph","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"e50227b079621b77081d7ed08d2a63c6","permalink":"https://forrt.org/curated_resources/data-management-with-sql-for-ecologists/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/data-management-with-sql-for-ecologists/","section":"curated_resources","summary":"Databases are useful for both storing and using data effectively. Using a relational database serves several purposes. It keeps your data separate from your analysis. This means there’s no risk of accidentally changing data when you analyze it. If we get new data we can rerun a query to find all the data that meets certain criteria. It’s fast, even for large amounts of data. It improves quality control of data entry (type constraints and use of forms in Access, Filemaker, etc.) The concepts of relational database querying are core to understanding how to do similar things using programming languages such as R or Python. This lesson will teach you what relational databases are, how you can load data into them and how you can query databases to extract just the information that you need.","tags":["Analysis","Data","Databases","Education","Open Scholarship Tools and Technologies","Reproducibility","Research Data Management Tools","Researchers","SQL"],"title":"Data Management with SQL for Ecologists","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"0a327bc17377ea31ff4afad114e3451e","permalink":"https://forrt.org/curated_resources/data-management-with-sql-for-social-scie/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/data-management-with-sql-for-social-scie/","section":"curated_resources","summary":"This is an alpha lesson to teach Data Management with SQL for Social Scientists, We welcome and criticism, or error; and will take your feedback into account to improve both the presentation and the content. Databases are useful for both storing and using data effectively. Using a relational database serves several purposes. It keeps your data separate from your analysis. This means there’s no risk of accidentally changing data when you analyze it. If we get new data we can rerun a query to find all the data that meets certain criteria. It’s fast, even for large amounts of data. It improves quality control of data entry (type constraints and use of forms in Access, Filemaker, etc.) The concepts of relational database querying are core to understanding how to do similar things using programming languages such as R or Python. This lesson will teach you what relational databases are, how you can load data into them and how you can query databases to extract just the information that you need.","tags":["Analysis","Data","Education","Open Scholarship Tools and Technologies","Reproducibility","Research Data Management Tools","Researchers","SQL"],"title":"Data Management with SQL for Social Scientists","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"7709785206694f9ad070fb06188b1a23","permalink":"https://forrt.org/curated_resources/data-organization-in-spreadsheets-for-ec/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/data-organization-in-spreadsheets-for-ec/","section":"curated_resources","summary":"Good data organization is the foundation of any research project. Most researchers have data in spreadsheets, so it’s the place that many research projects start. We organize data in spreadsheets in the ways that we as humans want to work with the data, but computers require that data be organized in particular ways. In order to use tools that make computation more efficient, such as programming languages like R or Python, we need to structure our data the way that computers need the data. Since this is where most research projects start, this is where we want to start too! In this lesson, you will learn: Good data entry practices - formatting data tables in spreadsheets How to avoid common formatting mistakes Approaches for handling dates in spreadsheets Basic quality control and data manipulation in spreadsheets Exporting data from spreadsheets In this lesson, however, you will not learn about data analysis with spreadsheets. Much of your time as a researcher will be spent in the initial ‘data wrangling’ stage, where you need to organize the data to perform a proper analysis later. It’s not the most fun, but it is necessary. In this lesson you will learn how to think about data organization and some practices for more effective data wrangling. With this approach you can better format current data and plan new data collection so less data wrangling is needed.","tags":["Analysis","Data","Education","Open Scholarship Tools and Technologies","Reproducibility","Research Data Management Tools","Researchers","Spreadsheets"],"title":"Data Organization in Spreadsheets for Ecologists","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"2fdd8864df59c9f642423c88007ae438","permalink":"https://forrt.org/curated_resources/data-organization-in-spreadsheets-for-so/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/data-organization-in-spreadsheets-for-so/","section":"curated_resources","summary":"Lesson on spreadsheets for social scientists. Good data organization is the foundation of any research project. Most researchers have data in spreadsheets, so it’s the place that many research projects start. Typically we organize data in spreadsheets in ways that we as humans want to work with the data. However computers require data to be organized in particular ways. In order to use tools that make computation more efficient, such as programming languages like R or Python, we need to structure our data the way that computers need the data. Since this is where most research projects start, this is where we want to start too! In this lesson, you will learn: Good data entry practices - formatting data tables in spreadsheets How to avoid common formatting mistakes Approaches for handling dates in spreadsheets Basic quality control and data manipulation in spreadsheets Exporting data from spreadsheets In this lesson, however, you will not learn about data analysis with spreadsheets. Much of your time as a researcher will be spent in the initial ‘data wrangling’ stage, where you need to organize the data to perform a proper analysis later. It’s not the most fun, but it is necessary. In this lesson you will learn how to think about data organization and some practices for more effective data wrangling. With this approach you can better format current data and plan new data collection so less data wrangling is needed.","tags":["Analysis","Data","Education","Open Scholarship Tools and Technologies","Reproducibility","Research Data Management Tools","Researchers","Spreadsheets"],"title":"Data Organization in Spreadsheets for Social Scientists","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"d14593c4ba61ce841e00aa5cebea6db0","permalink":"https://forrt.org/curated_resources/data-overuse-in-aging-research-emerging/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/data-overuse-in-aging-research-emerging/","section":"curated_resources","summary":"Aging and lifespan development researchers have been fortunate to have public access to many longitudinal datasets. These data are valuable and see high utilization, yet this has a considerable downside. Many of these are heavily overused. Overuse of publicly available datasets creates dependency among published research papers giving the false impression of independent contributions to knowledge by reporting the same associations over multiple papers. This is a potentially serious problem in the aging literature given the high use of a relatively small number of well-known studies. Any irregularities or sampling biases in this relatively small number of samples have outsize influence on perceived answers to key aging questions. We detail this problem, focusing on issues of dependency among studies, sampling bias and overfitting, and contradictory estimates of the same effect from the same data in independent publications. We provide solutions, including greater use of data sharing, pre-registrations, holdout samples, split-sample cross-validation, and coordinated analysis. We argue these valuable datasets are public resources that are being diminished by overuse, with parallels in environmental science. Taking a conservation perspective, we hold that these practices (pre-registration, holdout samples) can preserve data resources for future generations of researchers. ","tags":["Lifespan Development","Aging","Data Overuse","Bias"],"title":"Data overuse in aging research: Emerging issues and potential solutions","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1689122843,"objectID":"74d0a2d2eb9aa8463c56a39f47bde39e","permalink":"https://forrt.org/curated_resources/data-peeking-without-p-hacking/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/data-peeking-without-p-hacking/","section":"curated_resources","summary":"Blog post going over data peeking without p-hacking","tags":["Blog","Code","Open Science","Reproducibility Crisis and Credibility Revolution"],"title":"Data peeking without p-hacking","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"13ddf882d1ce18807398642fbbc76af0","permalink":"https://forrt.org/curated_resources/data-policies-of-highly-ranked-social-sc/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/data-policies-of-highly-ranked-social-sc/","section":"curated_resources","summary":"By encouraging and requiring that authors share their data in order to publish articles, scholarly journals have become an important actor in the movement to improve the openness of data and the reproducibility of research. But how many social science journals encourage or mandate that authors share the data supporting their research findings? How does the share of journal data policies vary by discipline? What influences these journals' decisions to adopt such policies and instructions? And what do those policies and instructions look like? We discuss the results of our analysis of the instructions and policies of 291 highly-ranked journals publishing social science research, where we studied the contents of journal data policies and instructions across 14 variables, such as when and how authors are asked to share their data, and what role journal ranking and age play in the existence and quality of data policies and instructions. We also compare our results to the results of other studies that have analyzed the policies of social science journals, although differences in the journals chosen and how each study defines what constitutes a data policy limit this comparison.We conclude that a little more than half of the journals in our study have data policies. A greater share of the economics journals have data policies and mandate sharing, followed by political science/international relations and psychology journals. Finally, we use our findings to make several recommendations: Policies should include the terms â€œdata,â€� â€œdatasetâ€� or more specific terms that make it clear what to make available; policies should include the benefits of data sharing; journals, publishers, and associations need to collaborate more to clarify data policies; and policies should explicitly ask for qualitative data.","tags":["Data","Data-Sharing Policies","Funder Policies","Open Data","Open Science","Policy"],"title":"Data policies of highly-ranked social science journals","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1689122843,"objectID":"e6ead13353ead97bdd05cae31919dc5a","permalink":"https://forrt.org/curated_resources/data-reuse-and-the-open-data-citation-ad/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/data-reuse-and-the-open-data-citation-ad/","section":"curated_resources","summary":"Background. Attribution to the original contributor upon reuse of published data is important both as a reward for data creators and to document the provenance of research findings. Previous studies have found that papers with publicly available datasets receive a higher number of citations than similar studies without available data. However, few previous analyses have had the statistical power to control for the many variables known to predict citation rate, which has led to uncertain estimates of the “citation benefit”. Furthermore, little is known about patterns in data reuse over time and across datasets. Method and Results. Here, we look at citation rates while controlling for many known citation predictors and investigate the variability of data reuse. In a multivariate regression on 10,555 studies that created gene expression microarray data, we found that studies that made data available in a public repository received 9% (95% confidence interval: 5% to 13%) more citations than similar studies for which the data was not made available. Date of publication, journal impact factor, open access status, number of authors, first and last author publication history, corresponding author country, institution citation history, and study topic were included as covariates. The citation benefit varied with date of dataset deposition: a citation benefit was most clear for papers published in 2004 and 2005, at about 30%. Authors published most papers using their own datasets within two years of their first publication on the dataset, whereas data reuse papers published by third-party investigators continued to accumulate for at least six years. To study patterns of data reuse directly, we compiled 9,724 instances of third party data reuse via mention of GEO or ArrayExpress accession numbers in the full text of papers. The level of third-party data use was high: for 100 datasets deposited in year 0, we estimated that 40 papers in PubMed reused a dataset by year 2, 100 by year 4, and more than 150 data reuse papers had been published by year 5. Data reuse was distributed across a broad base of datasets: a very conservative estimate found that 20% of the datasets deposited between 2003 and 2007 had been reused at least once by third parties. Conclusion. After accounting for other factors affecting citation rate, we find a robust citation benefit from open data, although a smaller one than previously reported. We conclude there is a direct effect of third-party data reuse that persists for years beyond the time when researchers have published most of the papers reusing their own data. Other factors that may also contribute to the citation benefit are considered. We further conclude that, at least for gene expression microarray data, a substantial fraction of archived datasets are reused, and that the intensity of dataset reuse has been steadily increasing since 2003.","tags":[""],"title":"Data reuse and the open data citation advantage","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1721390377,"objectID":"65c5c7d986949c0e794f008cc9dfcd1b","permalink":"https://forrt.org/glossary/english/data_sharing/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/english/data_sharing/","section":"glossary","summary":"","tags":null,"title":"Data sharing","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1720865851,"objectID":"d076138a4c98ee35504760b0f3810151","permalink":"https://forrt.org/glossary/german/data_sharing/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/german/data_sharing/","section":"glossary","summary":"","tags":null,"title":"Data sharing","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1635961284,"objectID":"7b4a1fe0ab1b35c8566e39d8138e6bdf","permalink":"https://forrt.org/glossary/vbeta/data-sharing/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/vbeta/data-sharing/","section":"glossary","summary":"","tags":null,"title":"Data sharing","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"14e07f4bb43741d9cc8b5540d1215880","permalink":"https://forrt.org/curated_resources/data-sharing-by-scientists-practices-and/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/data-sharing-by-scientists-practices-and/","section":"curated_resources","summary":"Background Scientific research in the 21st century is more data intensive and collaborative than in the past. It is important to study the data practices of researchers – data accessibility, discovery, re-use, preservation and, particularly, data sharing. Data sharing is a valuable part of the scientific method allowing for verification of results and extending research from prior results. Methodology/Principal Findings A total of 1329 scientists participated in this survey exploring current data sharing practices and perceptions of the barriers and enablers of data sharing. Scientists do not make their data electronically available to others for various reasons, including insufficient time and lack of funding. Most respondents are satisfied with their current processes for the initial and short-term parts of the data or research lifecycle (collecting their research data; searching for, describing or cataloging, analyzing, and short-term storage of their data) but are not satisfied with long-term data preservation. Many organizations do not provide support to their researchers for data management both in the short- and long-term. If certain conditions are met (such as formal citation and sharing reprints) respondents agree they are willing to share their data. There are also significant differences and approaches in data management practices based on primary funding agency, subject discipline, age, work focus, and world region. Conclusions/Significance Barriers to effective data sharing and preservation are deeply rooted in the practices and culture of the research process as well as the researchers themselves. New mandates for data management plans from NSF and other federal agencies and world-wide attention to the need to share and preserve data could lead to changes. Large scale programs, such as the NSF-sponsored DataNET (including projects like DataONE) will both bring attention and resources to the issue and make it easier for scientists to apply sound data management principles.","tags":["Data","Data Acquisition","Data Management","Data Processing","Ecology and Environmental Sciences","Europe","Scientists","Social Sciences","Surveys"],"title":"Data Sharing by Scientists: Practices and Perceptions","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"4f6413203f239d1af18bbc749f60cf5d","permalink":"https://forrt.org/curated_resources/data-sharing-in-plos-one-an-analysis-of/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/data-sharing-in-plos-one-an-analysis-of/","section":"curated_resources","summary":"A number of publishers and funders, including PLOS, have recently adopted policies requiring researchers to share the data underlying their results and publications. Such policies help increase the reproducibility of the published literature, as well as make a larger body of data available for reuse and re-analysis. In this study, we evaluate the extent to which authors have complied with this policy by analyzing Data Availability Statements from 47,593 papers published in PLOS ONE between March 2014 (when the policy went into effect) and May 2016. Our analysis shows that compliance with the policy has increased, with a significant decline over time in papers that did not include a Data Availability Statement. However, only about 20% of statements indicate that data are deposited in a repository, which the PLOS policy states is the preferred method. More commonly, authors state that their data are in the paper itself or in the supplemental information, though it is unclear whether these data meet the level of sharing required in the PLOS policy. These findings suggest that additional review of Data Availability Statements or more stringent policies may be needed to increase data sharing.","tags":["Data","Institutional Repositories","Open Data","Policy","Public Policy","Publishing","Randomized Controlled Trials","Reproducibility","Research Validity","Science Policy","Scientific Publishing"],"title":"Data sharing in PLOS ONE: An analysis of Data Availability Statements","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1689122843,"objectID":"5807fba848500b3a44a61085591a4369","permalink":"https://forrt.org/curated_resources/data-sharing-in-psychology-a-survey-on-b/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/data-sharing-in-psychology-a-survey-on-b/","section":"curated_resources","summary":"Despite its potential to accelerate academic progress in psychological science, public data sharing remains relatively uncommon. In order to discover the perceived barriers to public data sharing and possible means for lowering them, we conducted a survey, which elicited responses from 600 authors of articles in psychology. The results confirmed that data are shared only infrequently. Perceived barriers included respondents’ belief that sharing is not a common practice in their fields, their preference to share data only upon request, their perception that sharing requires extra work, and their lack of training in sharing data. Our survey suggests that strong encouragement from institutions, journals, and funders will be particularly effective in overcoming these barriers, in combination with educational materials that demonstrate where and how data can be shared effectively.","tags":[""],"title":"Data Sharing in Psychology: A Survey on Barriers and Preconditions","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"1ce607c431b13801e7ae49aee9ec40dd","permalink":"https://forrt.org/curated_resources/data-sharing-practices-in-collaborative/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/data-sharing-practices-in-collaborative/","section":"curated_resources","summary":"Introduction\nThe practice of creating large databases has become increasingly common by combining research participants’ data into larger repositories. Funders now require that data sharing be considered in newly funded research project, unless there are justifiable reasons not to do so. Access to genomic data brings along a host of ethical concerns as well as fairness and equity in the conduct of collaborative research between researchers from high- income and low-and middle-income countries.\n\nMaterials and methods\nThis systematic review protocol will be developed in line with PRISMA -guidelines which refers to Open Science Framework, registered in PROSPERO (https://www.crd.york.ac.uk/prospero/) record CRD42022297984 and published in a peer reviewed journal. Data sources will include PubMed, google scholar, EMBASE, Web of science and MEDLINE. Both published and grey literature will be searched. Subject matter experts including bioethicists, principal investigators of genomic research projects and research administrators will be contacted. After de-duplication, titles and abstracts will be screened for eligibility. Data extraction will be undertaken using a piloted form designed in EPPI-Reviewer software before conducting risk of bias assessments by a pair of reviewers, acting independently. Any discrepancies will be resolved by consensus. Analysis will be done using a structured narrative synthesis and where feasible metanalysis. This review will attempt to highlight the context of data sharing practices in the global North-South and South-South collaborative human genomic research in low- and middle-income countries. This review will enhance the body of evidence on ethical, legal and social implications of data sharing in international collaborative genomic research setting criteria for data sharing. The full report will be shared with relevant stakeholders including universities, civil society, funders, and departments of genomic research to ensure an adequate reach in low-and middle-income countries (LMICs).","tags":["Human Genomics","Low and Middle Income Countries","Systematic Reviews","Database Searching","Science Policy","Research Ethics","Research Design","Survey Research"],"title":"Data sharing practices in collaborative human genomic research in low- and middle-income countries: A systematic review protocol","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"a39292e01bacde50049cd57b4531b711","permalink":"https://forrt.org/curated_resources/data-tutorial-power-analysis-using-g-pow/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/data-tutorial-power-analysis-using-g-pow/","section":"curated_resources","summary":"A tutorial on a priori and sensitivity power analyses using G*power","tags":["Power analysis"],"title":"Data tutorial: Power analysis using G*Power","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1721390377,"objectID":"cd168ffb534697aac879e5bf63707fd7","permalink":"https://forrt.org/glossary/english/data_visualisation/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/english/data_visualisation/","section":"glossary","summary":"","tags":null,"title":"Data visualisation","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1635961284,"objectID":"a7152167b1aa7072dbf6ae73288ddd7c","permalink":"https://forrt.org/glossary/vbeta/data-visualisation/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/vbeta/data-visualisation/","section":"glossary","summary":"","tags":null,"title":"Data visualisation","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1720865851,"objectID":"4a6e73748c9522e7968d87a508eace48","permalink":"https://forrt.org/glossary/german/data_visualisation/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/german/data_visualisation/","section":"glossary","summary":"","tags":null,"title":"Data visualisation (Datenvisualisierung)","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"778030666123cfa8518d128b061a6ddc","permalink":"https://forrt.org/curated_resources/data-wrangling-and-processing-for-genomi/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/data-wrangling-and-processing-for-genomi/","section":"curated_resources","summary":"Data Carpentry lesson to learn how to use command-line tools to perform quality control, align reads to a reference genome, and identify and visualize between-sample variation. A lot of genomics analysis is done using command-line tools for three reasons: 1) you will often be working with a large number of files, and working through the command-line rather than through a graphical user interface (GUI) allows you to automate repetitive tasks, 2) you will often need more compute power than is available on your personal computer, and connecting to and interacting with remote computers requires a command-line interface, and 3) you will often need to customize your analyses, and command-line tools often enable more customization than the corresponding GUI tools (if in fact a GUI tool even exists). In a previous lesson, you learned how to use the bash shell to interact with your computer through a command line interface. In this lesson, you will be applying this new knowledge to carry out a common genomics workflow - identifying variants among sequencing samples taken from multiple individuals within a population. We will be starting with a set of sequenced reads (.fastq files), performing some quality control steps, aligning those reads to a reference genome, and ending by identifying and visualizing variations among these samples. As you progress through this lesson, keep in mind that, even if you aren’t going to be doing this same workflow in your research, you will be learning some very important lessons about using command-line bioinformatic tools. What you learn here will enable you to use a variety of bioinformatic tools with confidence and greatly enhance your research efficiency and productivity.","tags":["Analysis","Data","Education","Genomics","Inside Your Classroom","Open Scholarship Tools and Technologies","Reproducibility","Research Data Management Tools","Researchers","Shell"],"title":"Data Wrangling and Processing for Genomics","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1689122843,"objectID":"3c0b027c1ee2c18ee31b96159e7c2e20","permalink":"https://forrt.org/curated_resources/data-sharing-is-caring/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/data-sharing-is-caring/","section":"curated_resources","summary":"Data sharing promotes scientific progress by permitting replication of prior scientific analyses and by increasing the return on the human and financial investments made in data collection. The costs of data sharing can be reduced through the implementation of best practices in data management across the research life cycle; this article provides specific guidance on these practices. The benefits of data sharing will be reaped when researchers who share their data are rewarded with citations and recognition of the intellectual value inherent in producing new scientific data.","tags":[""],"title":"Data: Sharing Is Caring","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"5b774ece5e7d9fa3b1e1e3366885a959","permalink":"https://forrt.org/curated_resources/databases-and-sql/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/databases-and-sql/","section":"curated_resources","summary":"Software Carpentry lesson that teaches how to use databases and SQL In the late 1920s and early 1930s, William Dyer, Frank Pabodie, and Valentina Roerich led expeditions to the Pole of Inaccessibility in the South Pacific, and then onward to Antarctica. Two years ago, their expeditions were found in a storage locker at Miskatonic University. We have scanned and OCR the data they contain, and we now want to store that information in a way that will make search and analysis easy. Three common options for storage are text files, spreadsheets, and databases. Text files are easiest to create, and work well with version control, but then we would have to build search and analysis tools ourselves. Spreadsheets are good for doing simple analyses, but they don’t handle large or complex data sets well. Databases, however, include powerful tools for search and analysis, and can handle large, complex data sets. These lessons will show how to use a database to explore the expeditions’ data.","tags":["Analysis","Data","Education","Open Scholarship Tools and Technologies","Reproducibility","Research Data Management Tools","Researchers","SQL"],"title":"Databases and SQL","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"ae233bd2cb61db3375d4c9fc98a77a08","permalink":"https://forrt.org/curated_resources/debate-statistical-analysis-plans-for-ob/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/debate-statistical-analysis-plans-for-ob/","section":"curated_resources","summary":"Background All clinical research benefits from transparency and validity. Transparency and validity of studies may increase by prospective registration of protocols and by publication of statistical analysis plans (SAPs) before data have been accessed to discern data-driven analyses from pre-planned analyses. Main message Like clinical trials, recommendations for SAPs for observational studies increase the transparency and validity of findings. We appraised the applicability of recently developed guidelines for the content of SAPs for clinical trials to SAPs for observational studies. Of the 32 items recommended for a SAP for a clinical trial, 30 items (94%) were identically applicable to a SAP for our observational study. Power estimations and adjustments for multiplicity are equally important in observational studies and clinical trials as both types of studies usually address multiple hypotheses. Only two clinical trial items (6%) regarding issues of randomisation and definition of adherence to the intervention did not seem applicable to observational studies. We suggest to include one new item specifically applicable to observational studies to be addressed in a SAP, describing how adjustment for possible confounders will be handled in the analyses. Conclusion With only few amendments, the guidelines for SAP of a clinical trial can be applied to a SAP for an observational study. We suggest SAPs should be equally required for observational studies and clinical trials to increase their transparency and validity.","tags":["Authorship","Data","Data Management","Publishing","Randomized Trial","Research Ethics"],"title":"DEBATE-statistical analysis plans for observational studies","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"a269209214fce26e74fe49a1d76cd923","permalink":"https://forrt.org/curated_resources/declaration-of-common-standards-for-the/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/declaration-of-common-standards-for-the/","section":"curated_resources","summary":"Preregistration of studies is a recognized tool in clinical research to improve the quality and reporting of all gained results. In preclinical research, preregistration could boost the translation of published results into clinical breakthroughs. When studies rely on animal testing or form the basis of clinical trials, maximizing the validity and reliability of research outcomes becomes in addition an ethical obligation. Nevertheless, the implementation of preregistration in animal research is still slow. However, research institutions, funders, and publishers start valuing preregistration, and thereby level the way for its broader acceptance in the future. A total of 3 public registries, the OSF registry, preclinicaltrials.eu, and animalstudyregistry.org already encourage the preregistration of research involving animals. Here, they jointly declare common standards to make preregistration a valuable tool for better science. Registries should meet the following criteria: public accessibility, transparency in their financial sources, tracking of changes, and warranty and sustainability of data. Furthermore, registration templates should cover a minimum set of mandatory information and studies have to be uniquely identifiable. Finally, preregistered studies should be linked to any published outcome. To ensure that preregistration becomes a powerful instrument, publishers, funders, and institutions should refer to registries that fulfill these minimum standards.","tags":["Open Science","Preregistration","Animal Research","Preclinical Research","3R","Research Methods"],"title":"Declaration of common standards for the preregistration of animal research—speeding up the scientific progress","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1721390377,"objectID":"017b52cda2b9aa9bf9f4c004fc25c440","permalink":"https://forrt.org/glossary/english/decolonisation/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/english/decolonisation/","section":"glossary","summary":"","tags":null,"title":"Decolonisation","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1635961284,"objectID":"e4e6d29a2bc092d27f50d97c6f0b7475","permalink":"https://forrt.org/glossary/vbeta/decolonisation/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/vbeta/decolonisation/","section":"glossary","summary":"","tags":null,"title":"Decolonisation","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1720865851,"objectID":"69366ec1050158d9ae5efe51c46b554d","permalink":"https://forrt.org/glossary/german/decolonisation/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/german/decolonisation/","section":"glossary","summary":"","tags":null,"title":"Decolonisation (Dekolonialisierung)","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"f5dc353e40fb79905395bc1165389e26","permalink":"https://forrt.org/curated_resources/deep-dive-into-open-scholarship-collabor/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/deep-dive-into-open-scholarship-collabor/","section":"curated_resources","summary":"This deep dive session on replications and large-scale collaborations introduces a glossary of relevant terms, the problems these initiatives address, and some tools to get started. Panelists start with content knowledge transfer but switch to more interactive conversation for Q\u0026A and conversation.","tags":["Open Science","Replication","Research Best Practices","Research Collaboration","Research Replication"],"title":"Deep Dive into Open Scholarship: Collaboration and Replication","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"ceeaa65ce3a91108bdccfc4994096527","permalink":"https://forrt.org/curated_resources/deep-dive-into-open-scholarship-data-mat/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/deep-dive-into-open-scholarship-data-mat/","section":"curated_resources","summary":"In this deep dive session, Dr. Willa van Dijk discusses how transparency with data, materials, and code is beneficial for educational research and education researchers. She illustrates these points by sharing experiences with transparency that were crucial to her success. She then shifts gears to provide tips and tricks for planning a new research project with transparency in mind, including attention to potential pitfalls, and also discusses adapting materials from previous projects to share.","tags":["Code Transparency","Educational Research","Education Research","Inside Your Classroom","Open Code","Open Data","Open Materials","Open Scholarship","Open Science","Research Best Practices"],"title":"Deep Dive into Open Scholarship: Data, Materials, and Code Transparency","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"8c7cd03c04311b06fb39ec342cd2e743","permalink":"https://forrt.org/curated_resources/deep-dive-into-open-scholarship-preprint/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/deep-dive-into-open-scholarship-preprint/","section":"curated_resources","summary":"In this deep dive session, we discuss the current model of scholarly publishing, and highlight the challenges and limitations of this model of research dissemination. We then focus on the value of open access and elaborate on different open access levels (Gold, Bronze, and Green), before discussing how preprints/postprints may be leveraged to promote open access.","tags":["Open Access","Open Scholarship","Open Science","Preprints","Scholarly Publishing"],"title":"Deep Dive into Open Scholarship: Preprints and OA","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"75bb59e6e1be5551ada8f6dc659fb317","permalink":"https://forrt.org/curated_resources/deep-dive-into-open-scholarship-preregis/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/deep-dive-into-open-scholarship-preregis/","section":"curated_resources","summary":"In this deep dive session, Amanda Montoya (UCLA) and Karen Rambo-Hernandez (Texas A\u0026M University) introduce the basics of preregistration and Registered Reports: two methods for creating a permanent record of a research plan prior to conducting data collection. They discuss the conceptual similarities and practical differences between pre-registration and registered reports. They provide practical advice from their own experiences using these practices in research labs and resources available for researchers interested in using these approaches. The session concludes with questions and discussion about adopting these practices and unique considerations for implementing these practices in education research.","tags":["Open Scholarship","Open Science","Preregistration","Registered Reports","Research Best Practices"],"title":"Deep Dive into Open Scholarship: Preregistration and Registered Reports","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"bd8cddfa9a3ac19d84346a1eb3df7abc","permalink":"https://forrt.org/curated_resources/deep-dive-on-open-practices-understandin/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/deep-dive-on-open-practices-understandin/","section":"curated_resources","summary":"As sharing data openly becomes more and more the norm, and not just because of mandates for federal funding, more researchers may become more interested in sharing data. Benefits of data sharing for educational research include increased collaboration, acceleration of knowledge through novel and creative research questions, and an increase in equitable opportunities for early career researchers and faculty at under-resourced institutions. In this session, Sara Hart covers the benefits of data sharing as well as the “how to” of how to prepare data for sharing. Participants are provided information about data sharing and resources to support their own data sharing.","tags":["Center for Open Science","Data Sharing","Education","Sara Hart","Unconference 2022"],"title":"Deep Dive on Open Practices: Understanding Data Sharing with Sara Hart","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"1ce617781b4fb635ab6a04bcb055ed03","permalink":"https://forrt.org/curated_resources/deep-dive-on-open-practices-understandin_2/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/deep-dive-on-open-practices-understandin_2/","section":"curated_resources","summary":"In this deep dive session, we introduce the basics of pre-registration: a method for creating a permanent record of a research plan prior to conducting data collection and/or data analysis. We discuss the conceptual similarities and practical differences between pre-registration and registered reports and traditional approaches to educational research. We provide some practical advice from our own experiences using this practice in our own research and resources available for researchers interested in pre-registering their work. Finally, we end with questions and discussion about adopting pre-registration practices and unique considerations for implementing pre-registration in education research.","tags":["Center for Open Science","Education","Karen Rambo-Hernandez","Open Science","Preregistration","Pre-registration in Education Research","Pre-registration in Science","Scott Peters","Unconference 2022"],"title":"Deep Dive on Open Practices: Understanding Preregistration with Scott Peters \u0026 Karen Rambo-Hernandez","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"1a2b5ec11efbe7059901c4ffcffd9c57","permalink":"https://forrt.org/curated_resources/deep-dive-on-open-practices-understandin_3/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/deep-dive-on-open-practices-understandin_3/","section":"curated_resources","summary":"Deep Dive on Open Practices: Understanding Registered Reports in Education Research with Amanda Montoya and Betsy McCoach - Registered reports are a new publication mechanism where peer review and the decision to publish the results of a study occur prior to data collection and/or analysis. Registered reports share many characteristics with preregistration but are distinct by involving the journal prior to completing the study. Journals in the field of education are increasingly offering opportunities to publish registered reports. Registered reports offer a variety of benefits to both the researcher and to the research field. In this workshop, we will discuss the basics of registered reports, benefits and limitations of registered reports, and which journals in education accept registered reports. We provide some practical advice on deciding which projects are appropriate for registered reports, implementing registered reports, and time management throughout the process. We discuss how special cases can be implemented as registered reports, such as secondary data analysis, replications, meta-analyses, and longitudinal studies.","tags":["Amanda Montoya","Betsy McCoach","Center for Open Science","Education","Open Science","Registered Reports","Registered Reports in Education Research","Unconference 2022"],"title":"Deep Dive on Open Practices: Understanding Registered Reports in Education Research","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"4f74fa182c5f3f71a304bf16b91db866","permalink":"https://forrt.org/curated_resources/deep-dive-on-open-practices-understandin_4/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/deep-dive-on-open-practices-understandin_4/","section":"curated_resources","summary":"Deep Dive on Open Practices: Understanding Replication in Education Research with Matt Makel - In this deep dive session, we introduce the purpose of replication, different conceptions of replication, and some models for implementation in education. Relevant terms, methods, publication possibilities, and existing funding mechanisms are reviewed. Frequently asked questions and potential answers are shared.","tags":["Center for Open Science","Education","Matt Makel","Open Science","Replication in Education Research","Unconference 2022"],"title":"Deep Dive on Open Practices: Understanding Replication in Education Research with Matt Makel","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1689122843,"objectID":"864d79dc8472f89311ea16febdd54c89","permalink":"https://forrt.org/curated_resources/defining-and-distinguishing-validity-int/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/defining-and-distinguishing-validity-int/","section":"curated_resources","summary":"The concept of validity has suffered because the term has been used to refer to 2 incompatible concerns: the degree of support for specified interpretations of test scores (i.e., intended score meaning) and the degree of support for specified applications (i.e., intended test uses). This article has 3 purposes: (a) to provide a brief summary of current validity theory, (b) to illustrate the incompatibility of incorporating score meaning and score use into a single concept, and (c) to propose and describe a framework that both accommodates and differentiates validation of test score inferences and justification of test use.","tags":[""],"title":"Defining and distinguishing validity: Interpretations of score meaning and justifications of test use.","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"c78e7b74aa3c0e4a04d5c2b37e8074cb","permalink":"https://forrt.org/curated_resources/defining-and-growing-the-field-of-metasc/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/defining-and-growing-the-field-of-metasc/","section":"curated_resources","summary":"In this talk, Professor Fidler argues how the field of metascience contrasts with many scientific disciplines because it works in service to science with a goal to improve the process by which science is conducted. The importance of creating a defined community is that is allows for norms to develop and for proper credit to be given for this work, without which it will be marginalized or demeaned. \n---\nAre you a funder interested in supporting research on the scientific process? Learn more about the communities mobilizing around the emerging field of metascience by visiting metascience.com. Funders are encouraged to review and adopt the practices overviewed at cos.io/top-funders as part of the solution to issues discussed during the Funders Forum.","tags":["Research"],"title":"Defining and Growing the Field of Metascience","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1689122843,"objectID":"4abf4c676e57986f377ec9bd6b3c7362","permalink":"https://forrt.org/curated_resources/degrees-of-freedom-in-planning-running-a/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/degrees-of-freedom-in-planning-running-a/","section":"curated_resources","summary":"The designing, collecting, analyzing, and reporting of psychological studies entail many choices that are often arbitrary. The opportunistic use of these so-called researcher degrees of freedom aimed at obtaining statistically significant results is problematic because it enhances the chances of false positive results and may inflate effect size estimates. In this review article, we present an extensive list of 34 degrees of freedom that researchers have in formulating hypotheses, and in designing, running, analyzing, and reporting of psychological research. The list can be used in research methods education, and as a checklist to assess the quality of preregistrations and to determine the potential for bias due to (arbitrary) choices in unregistered studies.","tags":[""],"title":"Degrees of Freedom in Planning, Running, Analyzing, and Reporting Psychological Studies: A Checklist to Avoid p-Hacking","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1689122843,"objectID":"d13bdc02405bfb2900972c763f24631d","permalink":"https://forrt.org/curated_resources/degrees-of-freedom-tutorial/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/degrees-of-freedom-tutorial/","section":"curated_resources","summary":"A lot of researchers seem to be struggling with their understanding of the statistical concept of degrees of freedom. Most do not really care about why degrees of freedom are important to statistical tests, but just want to know how to calculate and report them. This page will help. For those interested in learning more about degrees of freedom, take a look at the following resources: This chapter in the little handbook of statistical practice Walker, H. W. (1940). Degrees of Freedom. Journal of Educational Psychology, 31(4), 253-269. I couldn’t find any resource on the web that explains calculating degrees of freedom in a simple and clear manner and believe this page will fill that void. It reflects my current understanding of degrees of freedom, based on what I read in textbooks and scattered sources on the web. Feel free to add or comment.","tags":["Blog","Tutorial"],"title":"Degrees of Freedom Tutorial","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1721390377,"objectID":"c1fe273f9a187436b1487b2b08e21f86","permalink":"https://forrt.org/glossary/english/demarcation_criterion/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/english/demarcation_criterion/","section":"glossary","summary":"","tags":null,"title":"Demarcation criterion","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1635961284,"objectID":"81817729f86a4a02df84f447bc89bb23","permalink":"https://forrt.org/glossary/vbeta/demarcation-criterion/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/vbeta/demarcation-criterion/","section":"glossary","summary":"","tags":null,"title":"Demarcation criterion ","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1720865851,"objectID":"e48c295cd670fee3309906b30ee79e85","permalink":"https://forrt.org/glossary/german/demarcation_criterion/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/german/demarcation_criterion/","section":"glossary","summary":"","tags":null,"title":"Demarcation criterion (Demarkationskriterium)","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"97ec0edb84bfe3035c32f06e1d8aa0da","permalink":"https://forrt.org/curated_resources/dementia-big-data-and-open-science/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/dementia-big-data-and-open-science/","section":"curated_resources","summary":"Although there is clear potential to improve science and innovation systems through big data and open science, barriers still remain with respect to data sharing efforts. How can the available massive and diverse data collections be used and shared more efficiently to boost global research and innovation and improve care? What actions are needed to facilitate open access to research data generated with public funding?\n\nThe OECD is bringing together policy makers, funding agencies and researchers to tackle the issue of open access to data, focused around developing good practice and principles on data governance. Four case studies highlight best practice and identify barriers to progress.\n\nFollowing an OECD-hosted consultation with the Ontario Brain Institute (OBI), the United Kingdom Medical Research Council (MRC), and the US Alzheimer’s Association, two concrete examples of global data sharing have been created. The first, focused on providing a wealth of open-source biomedical data for the community (deep data), builds upon GAAIN, the Global Alzheimer’s Association Interactive Network, and links eleven international partners through a federated network of data resources. The capability of this network is being extended significantly through connections with the French National Alzheimer’s Database (BNA), the European Medicines Informatics Framework (EMIF), and the Canadian based Longitudinal Online Research and Imaging System (LORIS). The second focused on linking big data approaches at the population level (broad data), is a complementary collaboration between the Canadian Consortium on Neurodegeneration in Ageing and the Dementias Platform UK to share and analyse large-scale complex population-wide datasets from up to 2 million individuals, including imaging, genomics and health data.\n\nAs a result, these collaborations will enable the aggregation of an unprecedented volume of individual and population-level data, offering an open science solution to help research to more efficiently tackle Alzheimer’s disease and related disorders.","tags":["Aging Science","Big Data","Consumer Policy","Dementia","E-commerce","E-Government","Health","Innovation","OECD","Openness","Open Science","Science","Security"],"title":"Dementia, Big Data and Open Science","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1689122843,"objectID":"9db93efdf4e989d704366cf5a1bab653","permalink":"https://forrt.org/curated_resources/der-umgang-mit-forschungsdaten-im-fach-p/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/der-umgang-mit-forschungsdaten-im-fach-p/","section":"curated_resources","summary":"Calls for public access to research data have been ongoing for some time. For instance, in their “Recommendations for Secure Storage and Availability of Digital Primary Research Data” (2009) the German Research Foundation (“Deutsche Forschungsgemeinschaft”, DFG) demanded that publicly funded data are freely available after the completion of a project. In line with this, in 2010 the Alliance of Science Organizations in Germany called for long-term storage of and generally free access to research data. In September 2015, the DFG published data management guidelines that affirmed these goals and asked research associations to consider their data management regulations and to develop appropriate standards for discipline-specific use and sharing of research data. The German Psychological Society (DGPs) joins the DFG and the Alliance of Science Organizations in Germany in their mission to specify the DFG guidelines for the field of psychology. This document (a) emphasizes the importance of sustainable research data management, (b) defines what “primary data” are and how they should be stored, (c) defines standards and potential data sharing restrictions, and (d) defines the rights and duties of researchers that share data and researchers that use secondary data. The German Psychological Associations adopted these recommendations in September 2016.","tags":["Reproducibility Crisis and Credibility Revolution","Open Science"],"title":"Der Umgang mit Forschungsdaten im Fach Psychologie: Konkretisierung der DFG-Leitlinien. [Data Management in Psychological Science: Specification of the DFG Guidelines].","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"5aff5f5699317745319ab8cb5b5ba5e0","permalink":"https://forrt.org/curated_resources/designing-for-preregistration-a-user-cen/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/designing-for-preregistration-a-user-cen/","section":"curated_resources","summary":"The replication crisis---a failure to replicate foundational studies---has sparked a conversation in psychology, HCI, and beyond about scientific reliability. To address the crisis, researchers increasingly adopt preregistration: the practice of documenting research plans before conducting a study. Done properly, preregistration should reduce bias from taking exploratory findings as confirmatory. It is crucial to treat preregistration, often an online form/template, as a user-centered design problem to ensure preregistration achieves its intended goal. To understand preregistration in practice, we conducted 14 semi-structured interviews with preregistration users (researchers) who ranged in seniority and experience. We identified two main purposes researchers have for using preregistration, in addition to different user roles and adoption barriers. With the ultimate goal of improving the reliability of scientific findings, we suggest opportunities to explicitly support the different aspects of preregistration use based on our findings.","tags":["Human-Centered Computing","Human Computer Interaction (HCI)","Empirical Studies in HCI","Interaction design"],"title":"Designing for Preregistration: A User-Centered Perspective","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1689122843,"objectID":"1366db881687a6f37e87b9849947b25b","permalink":"https://forrt.org/curated_resources/detecting-and-avoiding-likely-false-posi/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/detecting-and-avoiding-likely-false-posi/","section":"curated_resources","summary":"Recently there has been a growing concern that many published research ﬁndings do not hold up in attempts to replicate them. We argue that this problem may originate from a culture of ‘you can publish if you found a signiﬁcant effect’. This culture creates a systematic bias against the null hypothesis which renders meta-analyses questionable and may even lead to a situation where hypotheses become difﬁcult to falsify. In order to pinpoint the sources of errorand possible solutions, we review current scientiﬁc practices with regard to their effect on the probability of drawing a false-positive conclusion. We explain why the proportion of published false-positive ﬁndings is expected to increase with(i) decreasing sample size, (ii) increasing pursuit of novelty, (iii) various forms of multiple testing and researcher ﬂexibility,and (iv) incorrect P-values, especially due to unaccounted pseudoreplication, i.e. the non-independence of data points(clustered data). We provide examples showing how statistical pitfalls and psychological traps lead to conclusions that are biased and unreliable, and we show how these mistakes can be avoided. Ultimately, we hope to contribute to a culture of ‘you can publish if your study is rigorous’. To this end, we highlight promising strategies towards making science more objective. Speciﬁcally, we enthusiastically encourage scientists to preregister their studies (including a priori hypotheses and complete analysis plans), to blind observers to treatment groups during data collection and analysis,and unconditionally to report all results. Also, we advocate reallocating some efforts away from seeking novelty and discovery and towards replicating important research ﬁndings of one’s own and of others for the beneﬁt of the scientiﬁc community as a whole. We believe these efforts will be aided by a shift in evaluation criteria away from the current system which values metrics of ‘impact’ almost exclusively and towards a system which explicitly values indices of scientiﬁc rigour","tags":["Reproducibility Crisis and Credibility Revolution","Open Science"],"title":"Detecting and avoiding likely false-positive findings: A practical guide","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1689122843,"objectID":"df7fc2338a85b72620c50253f87c4765","permalink":"https://forrt.org/curated_resources/detecting-fraud-in-social-science/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/detecting-fraud-in-social-science/","section":"curated_resources","summary":"He's been called a \"Data vigilante.\" In this episode, Prof. Uri Simonsohn describes how he detects fraudulent work in psychology and economics -- what clues tip him off? How big of a problem is fraud relative to other issues like P-hacking? And what solutions are there?","tags":["Podcast","Reproducibility Knowledge","Reproducibility Crisis and Credibility Revolution"],"title":"Detecting fraud in social science","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1721390377,"objectID":"aa13aba8c0bf0db55a477b533d1a75f1","permalink":"https://forrt.org/glossary/english/direct_replication/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/english/direct_replication/","section":"glossary","summary":"","tags":null,"title":"Direct replication","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1635961284,"objectID":"1aa7d5ade15e53db6c0b006b37c711ed","permalink":"https://forrt.org/glossary/vbeta/direct-replication/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/vbeta/direct-replication/","section":"glossary","summary":"","tags":null,"title":"Direct replication","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1720865851,"objectID":"8cfaf014401a2d7b0e6229df34317019","permalink":"https://forrt.org/glossary/german/direct_replication/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/german/direct_replication/","section":"glossary","summary":"","tags":null,"title":"Direct replication (direkte Replikation)","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"53603217427bf76f31aaffb02c75acf5","permalink":"https://forrt.org/curated_resources/disclosing-and-managing-non-financial-co/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/disclosing-and-managing-non-financial-co/","section":"curated_resources","summary":"In the last decade, there has been increased recognition of the importance of disclosing and managing non-financial conflicts of interests to safeguard the objectivity, integrity, and trustworthiness of scientific research. While funding agencies and academic institutions have had policies for addressing non-financial interests in grant peer review and research oversight since the 1990s, scientific journals have been only recently begun to develop such policies. An impediment to the formulation of effective journal policies is that non-financial interests can be difficult to recognize and define. Journals can overcome this problem by providing guidance concerning the types of non-financial interests that should be disclosed, including direct research interests, direct professional interests, expert testimony, involvement in litigation, holding a leadership position in a non-governmental organization, providing technical or scientific advice to a non-governmental organization, and personal or professional relationships. The guidance should apply to authors, editors, and reviewers.","tags":["Conflicts of Interest","Financial","Non-Financial","Bias","Ethics","Policy","Journals"],"title":"Disclosing and Managing Non-Financial Conflicts of Interest in Scientific Publications","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"03c4da68ffd9dea1fc3129e99346575f","permalink":"https://forrt.org/curated_resources/discrepancies-in-the-registries-of-diet/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/discrepancies-in-the-registries-of-diet/","section":"curated_resources","summary":"This cross-sectional study examines discrepancies between registered protocols and subsequent publications for drug and diet trials whose findings were published in prominent clinical journals in the last decade. ClinicalTrials.gov was established in 2000 in response to the Food and Drug Administration Modernization Act of 1997, which called for registration of trials of investigational new drugs for serious diseases. Subsequently, the scope of ClinicalTrials.gov expanded to all interventional studies, including diet trials. Presently, prospective trial registration is required by the National Institutes of Health for grant funding and many clinical journals for publication.1 Registration may reduce risk of bias from selective reporting and post hoc changes in design and analysis.1,2 Although a study3 of trials with ethics approval in Finland in 2007 identified numerous discrepancies between registered protocols and subsequent publications, the consistency of diet trial registration and reporting has not been well explored.","tags":["Bias","Preregistration","Registration","Reporting","Reporting Guidelines","Reproducibility"],"title":"Discrepancies in the Registries of Diet vs Drug Trials","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"7e82c2a9efed3fb6a5db2ad1fc24f2b5","permalink":"https://forrt.org/curated_resources/discrepancy-review-a-feasibility-study-o/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/discrepancy-review-a-feasibility-study-o/","section":"curated_resources","summary":"Undisclosed discrepancies often exist between study registrations and their associated publications. Discrepancies can increase risk of bias, and when undisclosed, they disguise this increased risk of bias from readers. To remedy this issue, we developed an intervention called discrepancy review. We provided journals with peer reviewers specifically assigned to check for undisclosed discrepancies between registrations and manuscripts submitted to journals. We performed discrepancy review on 18 manuscripts submitted to Nicotine and Tobacco Research and three manuscripts submitted to the European Journal of Personality. We iteratively refined the discrepancy review process based on feedback from discrepancy reviewers, editors and authors. Authors addressed the majority of discrepancy reviewer comments, and there was no opposition to running a trial from authors, editors or discrepancy reviewers. Outcome measures for a trial of discrepancy review could include the presence of primary or secondary outcome discrepancies, whether publications that are not the primary report from a clinical trial registration are clearly described as such, whether registrations are permanent, and an overarching subjective assessment of the impact of discrepancies in published articles. We found that discrepancy review could feasibly be introduced as a regular practice at some journals interested in this process. A full trial of discrepancy review would be needed to evaluate its impact on reducing undisclosed discrepancies.","tags":["Peer Review","Outcome Switching","Selective Reporting","Pre-egistration","Trial Registration","Metaresearch"],"title":"Discrepancy review: A feasibility study of a novel peer review intervention to reduce undisclosed discrepancies between registrations and publications","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"31bcf949c2fbcbc20af86f5b349ce44f","permalink":"https://forrt.org/curated_resources/dissemination-and-publication-of-researc/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/dissemination-and-publication-of-researc/","section":"curated_resources","summary":"Objectives To identify and appraise empirical studies on publication and related biases published since 1998; to assess methods to deal with publication and related biases; and to examine, in a random sample of published systematic reviews, measures taken to prevent, reduce and detect dissemination bias. Data sources The main literature search, in August 2008, covered the Cochrane Methodology Register Database, MEDLINE, EMBASE, AMED and CINAHL. In May 2009, PubMed, PsycINFO and OpenSIGLE were also searched. Reference lists of retrieved studies were also examined. Review methods In Part I, studies were classified as evidence or method studies and data were extracted according to types of dissemination bias or methods for dealing with it. Evidence from empirical studies was summarised narratively. In Part II, 300 systematic reviews were randomly selected from MEDLINE and the methods used to deal with publication and related biases were assessed. Results Studies with significant or positive results were more likely to be published than those with non-significant or negative results, thereby confirming findings from a previous HTA report. There was convincing evidence that outcome reporting bias exists and has an impact on the pooled summary in systematic reviews. Studies with significant results tended to be published earlier than studies with non-significant results, and empirical evidence suggests that published studies tended to report a greater treatment effect than those from the grey literature. Exclusion of non-English-language studies appeared to result in a high risk of bias in some areas of research such as complementary and alternative medicine. In a few cases, publication and related biases had a potentially detrimental impact on patients or resource use. Publication bias can be prevented before a literature review (e.g. by prospective registration of trials), or detected during a literature review (e.g. by locating unpublished studies, funnel plot and related tests, sensitivity analysis modelling), or its impact can be minimised after a literature review (e.g. by confirmatory large-scale trials, updating the systematic review). The interpretation of funnel plot and related statistical tests, often used to assess publication bias, was often too simplistic and likely misleading. More sophisticated modelling methods have not been widely used. Compared with systematic reviews published in 1996, recent reviews of health-care interventions were more likely to locate and include non-English-language studies and grey literature or unpublished studies, and to test for publication bias. Conclusions Dissemination of research findings is likely to be a biased process, although the actual impact of such bias depends on specific circumstances. The prospective registration of clinical trials and the endorsement of reporting guidelines may reduce research dissemination bias in clinical research. In systematic reviews, measures can be taken to minimise the impact of dissemination bias by systematically searching for and including relevant studies that are difficult to access. Statistical methods can be useful for sensitivity analyses. Further research is needed to develop methods for qualitatively assessing the risk of publication bias in systematic reviews, and to evaluate the effect of prospective registration of studies, open access policy and improved publication guidelines.","tags":["Data","Open Data","Preregistration","Publication Bias","Publishing","Reporting","Systematic Reviews"],"title":"Dissemination and publication of research findings: an updated review of related biases","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1689122843,"objectID":"77502959c8f2b64e7e99caf02214392e","permalink":"https://forrt.org/curated_resources/distribution-of-p-values-when-comparing/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/distribution-of-p-values-when-comparing/","section":"curated_resources","summary":"An interactive visualisation of the distribution of p-values when comparing two groups","tags":["Blog","Interaction","Simulation","Tutorial"],"title":"Distribution of p-values when comparing two groups","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"88d46d0654a503f6b609b2c9b5022a6d","permalink":"https://forrt.org/curated_resources/diversifying-research-methods-syllabi/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/diversifying-research-methods-syllabi/","section":"curated_resources","summary":"This is a collection of research methodology articles, which are first- or senior-authored by women, to promote diverse perspectives in teaching students about research methods and contributing to improving research practices.","tags":["Reproducibility Crisis and Credibility Revolution","Open Science","Conceptual and statistical knowledge"],"title":"Diversifying Research Methods Syllabi","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1721390377,"objectID":"61f9b403a92d915c60aa0599ed31a8a3","permalink":"https://forrt.org/glossary/english/diversity/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/english/diversity/","section":"glossary","summary":"","tags":null,"title":"Diversity","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1720865851,"objectID":"7bf68fc9bf5d6c9c145edea6646278ad","permalink":"https://forrt.org/glossary/german/diversity/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/german/diversity/","section":"glossary","summary":"","tags":null,"title":"Diversity","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1635961284,"objectID":"51abd53a71c89829a58ee7fc739a6dab","permalink":"https://forrt.org/glossary/vbeta/diversity/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/vbeta/diversity/","section":"glossary","summary":"","tags":null,"title":"Diversity","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"aa89b3a216a80799d8fe818b0bbe037e","permalink":"https://forrt.org/curated_resources/do-p-values-lose-their-meaning-in-explor/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/do-p-values-lose-their-meaning-in-explor/","section":"curated_resources","summary":"Several researchers have recently argued that p values lose their meaning in exploratory analyses due to an unknown inflation of the alpha level (e.g., Nosek \u0026 Lakens, 2014; Wagenmakers, 2016). For this argument to be tenable, the familywise error rate must be defined in relation to the number of hypotheses that are tested in the same study or article. Under this conceptualization, the familywise error rate is usually unknowable in exploratory analyses because it is usually unclear how many hypotheses have been tested on a spontaneous basis and then omitted from the final research report. In the present article, I argue that it is inappropriate to conceptualize the familywise error rate in relation to the number of hypotheses that are tested. Instead, it is more appropriate to conceptualize familywise error in relation to the number of different tests that are conducted on the same null hypothesis in the same study. Under this conceptualization, alpha-level adjustments in exploratory analyses are (a) less necessary and (b) objectively verifiable. As a result, p values do not lose their meaning in exploratory analyses.","tags":["Confirmatory Research","Exploratory Research","Familywise Error Rate","P Values","Type I Errors"],"title":"Do p values lose their meaning in exploratory analyses? It depends how you define the familywise error rate","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"da202a9df2b5fbdd662dcbff1fe47bc4","permalink":"https://forrt.org/curated_resources/do-pre-analysis-plans-hamper-publication/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/do-pre-analysis-plans-hamper-publication/","section":"curated_resources","summary":"Scholars assert that pre-analysis plans (PAPs) generate boring, lab-report style papers and thus hamper publication. We test this claim by comparing the publication rates of experimental NBER working papers with and without PAPs. We find that articles with PAPs are slightly less likely to be published. However, conditional on being published, PAP-generated papers are significantly more likely to land in top-five journals. Also, PAP-based journal articles generate more citations. Our findings suggest that the alleged trade-off between career concerns and the scientific credibility that comes from registering and adhering to a PAP is less stark than is sometimes alleged.","tags":["Preregistration","Pre-analysis Plans"],"title":"Do Pre-analysis Plans Hamper Publication?","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"e6734fea84ef2645c8793b5142adf7ac","permalink":"https://forrt.org/curated_resources/do-pre-registration-and-pre-analysis-pla/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/do-pre-registration-and-pre-analysis-pla/","section":"curated_resources","summary":"Randomized controlled trials (RCTs) are increasingly prominent in economics, with pre-registration and pre-analysis plans (PAPs) promoted as important in ensuring the credibility of findings. We investigate whether these tools reduce the extent of p-hacking and publication bias by collecting and studying the universe of test statistics, 15,992 in total, from RCTs published in 15 leading economics journals from 2018 through 2021. In our primary analysis, we find no meaningful difference in the distribution of test statistics from pre-registered studies, compared to their non-pre-registered counterparts. However, pre-registered studies that have a complete PAP are significantly less p-hacked. These results point to the importance of PAPs, rather than pre-registration in itself, in ensuring credibility.","tags":["Pre-analysis Plan","Pre-registration","p-Hacking","Publication Bias","Research Credibility"],"title":"Do Pre-Registration and Pre-Analysis Plans Reduce p-Hacking and Publication Bias?","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1689122843,"objectID":"82acea7f4bee6ed0559435ffb75d7c78","permalink":"https://forrt.org/curated_resources/do-statistical-reporting-standards-affec/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/do-statistical-reporting-standards-affec/","section":"curated_resources","summary":"We examine the APSR and the AJPS for the presence of publication bias due to reliance on the 0.05 significance level. Our analysis employs a broad interpretation of publication bias, which we define as the outcome that occurs when, for whatever reason, publication practices lead to bias in the published parameter estimates. We examine the effect of the 0.05 significance level on the pattern of published findings using a \"caliper\" test, a novel method for comparing studies with heterogeneous effects, and find that we can reject the hypothesis of no publication bias at the 1 in 32 billion level. Our findings therefore raise the possibility that the results reported in the leading political science journals may be misleading due to publication bias. We also discuss some of the reasons for publication bias and propose reforms to reduce its impact on research.","tags":[""],"title":"Do Statistical Reporting Standards Affect What Is Published? Publication Bias in Two Leading Political Science Journals","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1689122843,"objectID":"46d64ad6d6857f9bddbdf61381da5245","permalink":"https://forrt.org/curated_resources/do-studies-of-statistical-power-have-an/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/do-studies-of-statistical-power-have-an/","section":"curated_resources","summary":"The long-term impact of studies of statistical power is investigated using J. Cohen's (1962) pioneering work as an example. We argue that the impact is nil; the power of studies in the same journal that Cohen reviewed (now the Journal of Abnormal Psychology) has not increased over the past 24 years. In 1960 the median power (i.e., the probability that a significant result will be obtained if there is a true effect) was .46 for a medium size effect, whereas in 1984 it was only .37. The decline of power is a result of alpha-adjusted procedures. Low power seems to go unnoticed: only 2 out of 64 experiments mentioned power, and it was never estimated. Nonsignificance was generally interpreted as confirmation of the null hypothesis (if this was the research hypothesis), although the median power was as low as .25 in these cases. We discuss reasons for the ongoing neglect of power.","tags":[""],"title":"Do studies of statistical power have an effect on the power of studies?","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"fc831c01ad4bd7235ff29efeb3f9026b","permalink":"https://forrt.org/curated_resources/does-preregistration-improve-the-credibi/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/does-preregistration-improve-the-credibi/","section":"curated_resources","summary":"Preregistration entails researchers registering their planned research hypotheses, methods, and analyses in a time-stamped document before they undertake their data collection and analyses. This document is then made available with the published research report to allow readers to identify discrepancies between what the researchers originally planned to do and what they actually ended up doing. This historical transparency is supposed to facilitate judgments about the credibility of the research findings. The present article provides a critical review of 17 of the reasons behind this argument. The article covers issues such as HARKing, multiple testing, p-hacking, forking paths, optional stopping, researchers' biases, selective reporting, test severity, publication bias, and replication rates. It is concluded that preregistration's historical transparency does not facilitate judgments about the credibility of research findings when researchers provide contemporary transparency in the form of (a) clear rationales for current hypotheses and analytical approaches, (b) public access to research data, materials, and code, and (c) demonstrations of the robustness of research conclusions to alternative interpretations and analytical approaches.","tags":["Forking Paths","HARKing","Multiple Testing","Optional Stopping","P-hacking","Preregistration","Publication Bias"],"title":"Does preregistration improve the credibility of research findings?","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"e101adba40b827d7c652401fbda033a1","permalink":"https://forrt.org/curated_resources/does-use-of-the-consort-statement-impact/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/does-use-of-the-consort-statement-impact/","section":"curated_resources","summary":"Background\nThe Consolidated Standards of Reporting Trials (CONSORT) Statement is intended to facilitate better reporting of randomised clinical trials (RCTs). A systematic review recently published in the Cochrane Library assesses whether journal endorsement of CONSORT impacts the completeness of reporting of RCTs; those findings are summarised here.\n\nMethods\nEvaluations assessing the completeness of reporting of RCTs based on any of 27 outcomes formulated based on the 1996 or 2001 CONSORT checklists were included; two primary comparisons were evaluated. The 27 outcomes were: the 22 items of the 2001 CONSORT checklist, four sub-items describing blinding and a â€˜total summary score' of aggregate items, as reported. Relative risks (RR) and 99% confidence intervals were calculated to determine effect estimates for each outcome across evaluations.\n\nResults\nFifty-three reports describing 50 evaluations of 16,604 RCTs were assessed for adherence to at least one of 27 outcomes. Sixty-nine of 81 meta-analyses show relative benefit from CONSORT endorsement on completeness of reporting. Between endorsing and non-endorsing journals, 25 outcomes are improved with CONSORT endorsement, five of these significantly (Î± = 0.01). The number of evaluations per meta-analysis was often low with substantial heterogeneity; validity was assessed as low or unclear for many evaluations.\n\nConclusions\nThe results of this review suggest that journal endorsement of CONSORT may benefit the completeness of reporting of RCTs they publish. No evidence suggests that endorsement hinders the completeness of RCT reporting. However, despite relative improvements when CONSORT is endorsed by journals, the completeness of reporting of trials remains sub-optimal. Journals are not sending a clear message about endorsement to authors submitting manuscripts for publication. As such, fidelity of endorsement as an â€˜intervention' has been weak to date. Journals need to take further action regarding their endorsement and implementation of CONSORT to facilitate accurate, transparent and complete reporting of trials.","tags":["Bias","Inside Your Classroom","Publishing","Reporting","Reporting Bias","Reporting Guidelines"],"title":"Does use of the CONSORT Statement impact the completeness of reporting of randomised controlled trials published in medical journals? A Cochrane reviewa","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1721390377,"objectID":"ef9bb26ac834d6e9ad38e292972e49bb","permalink":"https://forrt.org/glossary/english/doi/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/english/doi/","section":"glossary","summary":"","tags":null,"title":"DOI (digital object identifier)","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1720865851,"objectID":"178e625a1013da06aab700aac9c9a900","permalink":"https://forrt.org/glossary/german/doi/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/german/doi/","section":"glossary","summary":"","tags":null,"title":"DOI (digital object identifier)","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1635961284,"objectID":"52ec96eba984ccfd19e3b1b4f026c7e9","permalink":"https://forrt.org/glossary/vbeta/doi-digital-object-identifier/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/vbeta/doi-digital-object-identifier/","section":"glossary","summary":"","tags":null,"title":"DOI (digital object identifier)","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1721390377,"objectID":"369c2190e25d5eb4778513a79519d28e","permalink":"https://forrt.org/glossary/english/dora/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/english/dora/","section":"glossary","summary":"","tags":null,"title":"DORA","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1720865851,"objectID":"8fc63e623ece13b75851b69edc3d20c3","permalink":"https://forrt.org/glossary/german/dora/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/german/dora/","section":"glossary","summary":"","tags":null,"title":"DORA","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1635961284,"objectID":"1ca2e4b7315e3128c2f7be49c2e0ec41","permalink":"https://forrt.org/glossary/vbeta/dora/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/vbeta/dora/","section":"glossary","summary":"","tags":null,"title":"DORA","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1721390377,"objectID":"7d52003dae8ebdecc8e504421dfb1808","permalink":"https://forrt.org/glossary/english/double_consciousness/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/english/double_consciousness/","section":"glossary","summary":"","tags":null,"title":"Double consciousness","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1635961284,"objectID":"300da71a0da2bdbe52fce80eda2d79e0","permalink":"https://forrt.org/glossary/vbeta/double-consciousness/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/vbeta/double-consciousness/","section":"glossary","summary":"","tags":null,"title":"Double consciousness","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1720865851,"objectID":"ca24b47c84639dff23ca3f230ee4c999","permalink":"https://forrt.org/glossary/german/double_consciousness/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/german/double_consciousness/","section":"glossary","summary":"","tags":null,"title":"Double consciousness (Doppeltes Bewusstsein)","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1721390377,"objectID":"d2ebfde42f3ff82d152dc325c69fadae","permalink":"https://forrt.org/glossary/english/double_blind_peer_review/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/english/double_blind_peer_review/","section":"glossary","summary":"","tags":null,"title":"Double-blind peer review","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1635961284,"objectID":"6b6176c3ae0efe16ee3a6d18ae1ee47c","permalink":"https://forrt.org/glossary/vbeta/double-blind-peer-review/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/vbeta/double-blind-peer-review/","section":"glossary","summary":"","tags":null,"title":"Double-blind peer review","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1720865851,"objectID":"8f446987b28d455e63b5fdd83f4501d6","permalink":"https://forrt.org/glossary/german/double_blind_peer_review/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/german/double_blind_peer_review/","section":"glossary","summary":"","tags":null,"title":"Double-blind peer review (Doppelblinde Begutachtung)","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"f4490896b95328f1c4e9d31a17fd5e7e","permalink":"https://forrt.org/curated_resources/draft-dated-2016-01-11-please-see-culear/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/draft-dated-2016-01-11-please-see-culear/","section":"curated_resources","summary":"This course will provide a survey of current personality research and theorywith an emphasis on recent discussions around methods and scientific reporting. We will consider recent accomplishments, questions, and controversies in personality psychology. This includes significant breadth, but given the potential scope,cannot be comprehensive. We will touch on prominent approaches in personality (i.e., traits, goals, motives, emotions, the self, the unconscious, genetics, physiology, culture, evolution, development and change, etc.). I believe the course content will berelevant to most psychology students, and I encourage students to bring their own lines of research to the course for discussion. There are individual differences in almost everything, and personality is also concerned with how psychological processes ‘come together’ within people.\nBeyond personality content, it is critical to understand the methods used to generate knowledge about personality. In recent years, meta-research method issues (e.g., reproducibility, open science, etc.) have yielded provocative findings, vigorous discussion, and innovations. This movement will comprise another major theme for the course in 2016. Readings and activities will provide tools that will help you improve your own research practices, and to better interpret the evidencein published reports. The standards, requirements, and values of publishing high quality research in psychology are changing quickly, and we will engage with the very latest developments in ways that will likely increase the quality and impact of your ownwork. We will also discuss the advantages and disadvantages of potential reforms to the way psychologists (and other scientists) conduct and report on research.","tags":["Reproducibility Crisis and Credibility Revolution","Open Science"],"title":"Draft dated 2016-01-11; please see cuLearn for possible updatesp.1PSYC 5601:Contemporary Research in Personality","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1689122843,"objectID":"be12a23d2d3d1325efd79016b24f8424","permalink":"https://forrt.org/curated_resources/dyadic-data-analysis-methodology-in-the/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/dyadic-data-analysis-methodology-in-the/","section":"curated_resources","summary":"Interpersonal phenomena such as attachment, conflict, person perception, helping, and influence have traditionally been studied by examining individuals in isolation, which falls short of capturing their truly interpersonal nature. This book offers state-of-the-art solutions to this age-old problem by presenting methodological and data-analytic approaches useful in investigating processes that take place among dyads: couples, coworkers, or parent-child, teacher-student, or doctor-patient pairs, to name just a few. Rich examples from psychology and across the behavioral and social sciences help build the researcher's ability to conceptualize relationship processes; model and test for actor effects, partner effects, and relationship effects; and model the statistical interdependence that can exist between partners. The companion website provides clarifications, elaborations, corrections, and data and files for each chapter.","tags":["Book"],"title":"Dyadic Data Analysis (Methodology in the Social Sciences)","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1720865851,"objectID":"ea5b852814ec1ef40adb9555a110977c","permalink":"https://forrt.org/glossary/german/early_career_researchers/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/german/early_career_researchers/","section":"glossary","summary":"","tags":null,"title":"Early career researchers (ECRs, Nachwuchswissenschaftler:innen)","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1721390377,"objectID":"140d8328b2f21e533d0d205428defcb5","permalink":"https://forrt.org/glossary/english/early_career_researchers/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/english/early_career_researchers/","section":"glossary","summary":"","tags":null,"title":"Early career researchers (ECRs)","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1635961284,"objectID":"ec9e736183b1492feb3f262ae36f3db7","permalink":"https://forrt.org/glossary/vbeta/early-career-researchers-ecrs/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/vbeta/early-career-researchers-ecrs/","section":"glossary","summary":"","tags":null,"title":"Early career researchers (ECRs)","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1689122843,"objectID":"0b632e7634930db15c56059eedcb958f","permalink":"https://forrt.org/curated_resources/easy-preregistration-will-beneft-any-res/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/easy-preregistration-will-beneft-any-res/","section":"curated_resources","summary":"An article about easy preregistration will benefit any research.","tags":[""],"title":"Easy preregistration will beneft any research","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1721390377,"objectID":"bcb54aa8358fc1ac1b1878bd56412d51","permalink":"https://forrt.org/glossary/english/economic_and_societal_impact/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/english/economic_and_societal_impact/","section":"glossary","summary":"","tags":null,"title":"Economic and societal impact","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1635961284,"objectID":"3762ee3772def3d23f8cef1c399a086b","permalink":"https://forrt.org/glossary/vbeta/economic-and-societal-impact/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/vbeta/economic-and-societal-impact/","section":"glossary","summary":"","tags":null,"title":"Economic and societal impact","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1720865851,"objectID":"abba8207722423bcd134f8883a478bba","permalink":"https://forrt.org/glossary/german/economic_and_societal_impact/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/german/economic_and_societal_impact/","section":"glossary","summary":"","tags":null,"title":"Economic and societal impact (wirtschaftliche und gesellschaftliche Auswirkungen","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"e0610dbddf00b3bfd5748a3a2506bda1","permalink":"https://forrt.org/curated_resources/economic-statistics-with-calculus/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/economic-statistics-with-calculus/","section":"curated_resources","summary":"A syllabus about economics and statistics","tags":["Economics","statistics"],"title":"Economic Statistics with Calculus","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"0d3a1cfe5c887be28693de9736ae1a15","permalink":"https://forrt.org/curated_resources/economics-lesson-with-stata/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/economics-lesson-with-stata/","section":"curated_resources","summary":"A Data Carpentry curriculum for Economics is being developed by Dr. Miklos Koren at Central European University. These materials are being piloted locally. Development for these lessons has been supported by a grant from the Sloan Foundation.","tags":["Analysis","Data","Education","Open Scholarship Tools and Technologies","Reproducibility","Research Data Management Tools","Researchers"],"title":"Economics Lesson with Stata","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1689122843,"objectID":"40423880e2290ad32571698ded8359fa","permalink":"https://forrt.org/curated_resources/editorial-bias-against-replication-resea/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/editorial-bias-against-replication-resea/","section":"curated_resources","summary":"why aren't more replications published? While there are many possible reasons, one simple one could be that journals prefer not to publish them. As authors learn that replications are not likely to be accepted by journals, they would carry out and submit less replications, thus further reducing the possibility of replications being published. To test basic facts in this area, this study measured journal editors' attitudes toward publishing replication studies. We found a strong bias against publishing replications.","tags":["Reproducibility Crisis and Credibility Revolution","Open Science"],"title":"Editorial Bias Against Replication Research","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"3207075bf6f7957919d00cf86c751bdb","permalink":"https://forrt.org/curated_resources/educational-psychologist-educational-psy/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/educational-psychologist-educational-psy/","section":"curated_resources","summary":"Special Issue of\u0026nbsp;Educational Psychologist - Educational Psychology in the Open Science Era\nRecently, scholars have noted how several \u0026ldquo;old school\u0026rdquo; practices\u0026mdash;a host of well-regarded, long-standing scientific norms\u0026mdash;in combination, sometimes compromise the credibility of research. In response, other scholarly fields have developed several \u0026ldquo;open science\u0026rdquo; norms and practices to address these credibility issues. Against this backdrop, this special issue explores the extent to which and how these norms should be adopted and adapted for educational psychology and education more broadly.\n","tags":["Research"],"title":"Educational Psychologist - Educational Psychology in the Open Science Era","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1689122843,"objectID":"83c5c62b801dba788401b4f703f67db6","permalink":"https://forrt.org/curated_resources/effect-of-open-peer-review-on-quality-of/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/effect-of-open-peer-review-on-quality-of/","section":"curated_resources","summary":"Objectives: To examine the effect on peer review of asking reviewers to have their identity revealed to the authors of the paper. Design: Randomised trial. Consecutive eligible papers were sent to two reviewers who were randomised to have their identity revealed to the authors or to remain anonymous. Editors and authors were blind to the intervention. Main outcome measures: The quality of the reviews was independently rated by two editors and the corresponding author using a validated instrument. Additional outcomes were the time taken to complete the review and the recommendation regarding publication. A questionnaire survey was undertaken of the authors of a cohort of manuscripts submitted for publication to find out their views on open peer review. Results: Two editors' assessments were obtained for 113out of 125 manuscripts, and the corresponding author's assessment was obtained for 105.Reviewers randomised to be asked to be identified were 12% (95% confidence interval 0.2% to 24%) more likely to decline to review than reviewers randomised to remain anonymous (35% v 23%). There was no significant difference in quality (scored on a scale of 1to 5) between anonymous reviewers (3.06(SD 0.72)) and identified reviewers (3.09(0.68)) (P=0.68, 95% confidence interval for difference −align=baseline\u003e0.19 to 0.12), and no significant difference in the recommendation regarding publication or time taken to review the paper. The editors' quality score for reviews (3.05(SD 0.70)) was significantly higher than that of authors (2.90(0.87))(P\u003c0.005, 95%confidence interval for difference − align=baseline\u003e0.26 to − align=baseline\u003e0.03). Most authors were in favour of open peer review.Conclusions: Asking reviewers to consent to being identified to the author had no important effect on the quality of the review, the recommendation regarding publication, or the time taken to review, but it significantly increased the likelihood of reviewers declining to review.","tags":["Transparency","Open Science"],"title":"Effect of open peer review on quality of reviews and on reviewers'recommendations: a randomised trial","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"dd51acc66ba44a49c46c585abac92d27","permalink":"https://forrt.org/curated_resources/effect-of-population-heterogenization-on/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/effect-of-population-heterogenization-on/","section":"curated_resources","summary":"In animal experiments, animals, husbandry and test procedures are traditionally standardized to maximize test sensitivity and minimize animal use, assuming that this will also guarantee reproducibility. However, by reducing within-experiment variation, standardization may limit inference to the specific experimental conditions. Indeed, we have recently shown in mice that standardization may generate spurious results in behavioral tests, accounting for poor reproducibility, and that this can be avoided by population heterogenization through systematic variation of experimental conditions. Here, we examined whether a simple form of heterogenization effectively improves reproducibility of test results in a multi-laboratory situation. Each of six laboratories independently ordered 64 female mice of two inbred strains (C57BL/6NCrl, DBA/2NCrl) and examined them for strain differences in five commonly used behavioral tests under two different experimental designs. In the standardized design, experimental conditions were standardized as much as possible in each laboratory, while they were systematically varied with respect to the animals' test age and cage enrichment in the heterogenized design. Although heterogenization tended to improve reproducibility by increasing within-experiment variation relative to between-experiment variation, the effect was too weak to account for the large variation between laboratories. However, our findings confirm the potential of systematic heterogenization for improving reproducibility of animal experiments and highlight the need for effective and practicable heterogenization strategies.","tags":["Animal Behavior","Experimental Design","Factorial Design","Field Tests","Inbred Strains","Mice","Reproducibility","Research Methods","Study Design","White Light"],"title":"Effect of Population Heterogenization on the Reproducibility of Mouse Behavior: A Multi-Laboratory Study","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1689122843,"objectID":"8d4b5034c39e73ac095c5e66f86b3609","permalink":"https://forrt.org/curated_resources/effect-size-and-power-in-assessing-moder/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/effect-size-and-power-in-assessing-moder/","section":"curated_resources","summary":"The authors conducted a 30-year review (1969-1998) of the size of moderating effects of categorical variables as assessed using multiple regression. The median observed effect size (f(2)) is only .002, but 72% of the moderator tests reviewed had power of .80 or greater to detect a targeted effect conventionally defined as small. Results suggest the need to minimize the influence of artifacts that produce a downward bias in the observed effect size and put into question the use of conventional definitions of moderating effect sizes. As long as an effect has a meaningful impact, the authors advise researchers to conduct a power analysis and plan future research designs on the basis of smaller and more realistic targeted effect sizes.","tags":[""],"title":"Effect Size and Power in Assessing Moderating Effects of Categorical Variables Using Multiple Regression: A 30-Year Review","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1689122843,"objectID":"65111c6f5ae69f821e2fb554ff28bf6c","permalink":"https://forrt.org/curated_resources/effect-size-estimates-current-use-calcul/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/effect-size-estimates-current-use-calcul/","section":"curated_resources","summary":"The Publication Manual of the American Psychological Association (American Psychological Association, 2001, 2010) calls for the reporting of effect sizes and their confidence intervals. Estimates of effect size are useful for determining the practical or theoretical importance of an effect, the relative contributions of factors, and the power of an analysis. We surveyed articles published in 2009 and 2010 in the Journal of Experimental Psychology: General, noting the statistical analyses reported and the associated reporting of effect size estimates. Effect sizes were reported for fewer than half of the analyses; no article reported a confidence interval for an effect size. The most often reported analysis was analysis of variance, and almost half of these reports were not accompanied by effect sizes. Partial eta squared was the most commonly reported effect size estimate for analysis of variance. For t tests, 2/3 of the articles did not report an associated effect size estimate; Cohen’s d was the most often reported. We provide a straightforward guide to understanding, selecting, calculating, and interpreting effect sizes for many types of data and to methods for calculating effect size confidence intervals and power analysis.","tags":[""],"title":"Effect Size Estimates: Current Use, Calculations, and Interpretation","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1689122843,"objectID":"c255737404c023df48f8448c5dcd0e66","permalink":"https://forrt.org/curated_resources/effect-size-estimation-in-neuroimaging/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/effect-size-estimation-in-neuroimaging/","section":"curated_resources","summary":"A central goal of translational neuroimaging is to establish robust links between brain measures and clinical outcomes. Success hinges on the development of brain biomarkers with large effect sizes. With large enough effects, a measure may be diagnostic of outcomes at the individual patient level. Surprisingly, however, standard brain-mapping analyses are not designed to estimate or optimize the effect sizes of brain-outcome relationships, and estimates are often biased. Here, we review these issues and how to estimate effect sizes in neuroimaging research.","tags":[""],"title":"Effect Size Estimation in Neuroimaging. ","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1689122843,"objectID":"230d1f911dc601f75aa3aac681dac3e7","permalink":"https://forrt.org/curated_resources/effect-size-guidelines-for-individual-di/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/effect-size-guidelines-for-individual-di/","section":"curated_resources","summary":"Individual differences researchers very commonly report Pearson correlations between their variables of interest. Cohen (1988) provided guidelines for the purposes of interpreting the magnitude of a correlation, as well as estimating power. Specifically, r = 0.10, r = 0.30, and r = 0.50 were recommended to be considered small, medium, and large in magnitude, respectively. However, Cohen's effect size guidelines were based principally upon an essentially qualitative impression, rather than a systematic, quantitative analysis of data. Consequently, the purpose of this investigation was to develop a large sample of previously published meta-analytically derived correlations which would allow for an evaluation of Cohen's guidelines from an empirical perspective. Based on 708 meta-analytically derived correlations, the 25th, 50th, and 75th percentiles corresponded to correlations of 0.11, 0.19, and 0.29, respectively. Based on the results, it is suggested that Cohen's correlation guidelines are too exigent, as \u003c 3% of correlations in the literature were found to be as large as r = 0.50. Consequently, in the absence of any other information, individual differences researchers are recommended to consider correlations of 0.10, 0.20, and 0.30 as relatively small, typical, and relatively large, in the context of a power analysis, as well as the interpretation of statistical results from a normative perspective.","tags":[""],"title":"Effect size guidelines for individual differences researchers","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1689122843,"objectID":"807cb4a1908ba32869ee8c8a653215cc","permalink":"https://forrt.org/curated_resources/effect-sizes-and-p-values-what-should-be/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/effect-sizes-and-p-values-what-should-be/","section":"curated_resources","summary":"Despite publication of many well-argued critiques of null hypothesis testing (NHT), behavioral science researchers continue to rely heavily on this set of practices. Although we agree with most critics' catalogs of NHT's flaws, this article also takes the unusual stance of identifying virtues that may explain why NHT continues to be so extensively used. These virtues include providing results in the form of a dichotomous (yes/no) hypothesis evaluation and providing an index (p value) that has a justifiable mapping onto confidence in repeatability of a null hypothesis rejection. The most-criticized flaws of NHT can be avoided when the importance of a hypothesis, rather than the p value of its test, is used to determine that a finding is worthy of report, and when p approximately equal to .05 is treated as insufficient basis for confidence in the replicability of an isolated non-null finding. Together with many recent critics of NHT, we also urge reporting of important hypothesis tests in enough descriptive detail to permit secondary uses such as meta-analysis.","tags":[""],"title":"Effect sizes and p values: What should be reported and what should be replicated? ","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1689122843,"objectID":"cd6fb3d273b3b52820e4d3a87083356a","permalink":"https://forrt.org/curated_resources/effect-sizes-why-when-and-how-to-use-the/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/effect-sizes-why-when-and-how-to-use-the/","section":"curated_resources","summary":"The effect size (ES) is the magnitude of a study outcome or research finding, such as the strength of the relationship obtained between an independent variable and a dependent variable. Two types of ES indicators are sampled here: the difference-type and the correlational (or r-type). Both are well suited to situations in which there are two groups or two conditions, whereas the r-type, used in association with focused statistical procedures (contrasts), is also ideal in situations where there are more than two groups or conditions and there are predicted overall patterns to be evaluated. Also discussed are procedures for computing confidence intervals and null-counternull intervals as well as a systematic approach to comparing and combining competing predictions expressed in the form of contrast weights and ES indicators.","tags":[""],"title":"Effect Sizes: Why, When, and How to Use Them","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1689122843,"objectID":"33747d2127342c2800a265531b39fdb4","permalink":"https://forrt.org/curated_resources/eight-common-but-false-objections-to-the/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/eight-common-but-false-objections-to-the/","section":"curated_resources","summary":"Logically and conceptually, the use of statistical significance testing in the analysis of research data has been thoroughly discredited. However, reliance on significance testing is strongly embedded in the minds and habits of researchers, and therefore proposals to replace significance testing with point estimate estimates and confidence intervals often encounter strong resistance. This chapter examines eight of the most commonly voiced objects to reform of data analysis practices and shows each of them to be erroneous. The objections are: (a) Without significance tests we would not know whether a finding is real or just due to chance; (b) hypothesis testing would not be possible without significance tests; (c) the problem is not significance tests but failure to develop a tradition of replicationg studies; (d) when studies have a large number of relationships, we need significance tests to identify those that are real and not just due to chance; (e) confidence intervals are themselves significance tests; (f) significance testing ensures objectivity in the interpretation of research data; (g) it is the misuse, not the use, of significance testing that is the problem; and (h) it is futile to try to reform data analysis methods, so why try? Each of these objections is intuitively appealing and plausible but is easily shown to be logically and intellectually bankrupt. The same is true of the almost 80 other objects we have collected. Statistical significance testing retards the growth of scientific knowledge; it never makes a positive contribution. After decades of unsuccessful efforts, it now appears possible that reform of data analysis procedures will finally succeed. If so, a major impediment to the advance of scientific knowledge will have been removed.","tags":["Reproducibility Crisis and Credibility Revolution","Open Science"],"title":"Eight common but false objections to the discontinuation of significance testing in the analysis of research data.","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"78a9e35525bc16a2d0c065c657da2af1","permalink":"https://forrt.org/curated_resources/el-control-de-versiones-con-git/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/el-control-de-versiones-con-git/","section":"curated_resources","summary":"Software Carpentry lección para control de versiones con Git Para ilustrar el poder de Git y GitHub, usaremos la siguiente historia como un ejemplo motivador a través de esta lección. El Hombre Lobo y Drácula han sido contratados por Universal Missions para investigar si es posible enviar su próximo explorador planetario a Marte. Ellos quieren poder trabajar al mismo tiempo en los planes, pero ya han experimentado ciertos problemas anteriormente al hacer algo similar. Si se rotan por turnos entonces cada uno gastará mucho tiempo esperando a que el otro termine, pero si trabajan en sus propias copias e intercambian los cambios por email, las cosas se perderán, se sobreescribirán o se duplicarán. Un colega sugiere utilizar control de versiones para lidiar con el trabajo. El control de versiones es mejor que el intercambio de ficheros por email: Nada se pierde una vez que se incluye bajo control de versiones, a no ser que se haga un esfuerzo sustancial. Como se van guardando todas las versiones precedentes de los ficheros, siempre es posible volver atrás en el tiempo y ver exactamente quién escribió qué en un día en particular, o qué versión de un programa fue utilizada para generar un conjunto de resultados en particular. Como se tienen estos registros de quién hizo qué y en qué momento, es posible saber a quién preguntar si se tiene una pregunta en un momento posterior y, si es necesario, revertir el contenido a una versión anterior, de forma similar a como funciona el comando “deshacer” de los editores de texto. Cuando varias personas colaboran en el mismo proyecto, es posible pasar por alto o sobreescribir de manera accidental los cambios hechos por otra persona. El sistema de control de versiones notifica automáticamente a los usuarios cada vez que hay un conflicto entre el trabajo de una persona y la otra. Los equipos no son los únicos que se benefician del control de versiones: los investigadores independientes se pueden beneficiar en gran medida. Mantener un registro de qué ha cambiado, cuándo y por qué es extremadamente útil para todos los investigadores si alguna vez necesitan retomar el proyecto en un momento posterior (e.g. un año después, cuando se ha desvanecido el recuerdo de los detalles).","tags":["Analysis","Data","Education","Git","Open Scholarship Tools and Technologies","Reproducibility","Research Data Management Tools","Researchers"],"title":"El Control de Versiones con Git","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"2e4f8a3704b983b47352caf5de14e36c","permalink":"https://forrt.org/curated_resources/eleven-strategies-for-making-reproducibl/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/eleven-strategies-for-making-reproducibl/","section":"curated_resources","summary":"Across disciplines, researchers increasingly recognize that open science and reproducible research practices may accelerate scientific progress by allowing others to reuse research outputs and by promoting rigorous research that is more likely to yield trustworthy results. While initiatives, training programs, and funder policies encourage researchers to adopt reproducible research and open science practices, these practices are uncommon in many fields. Researchers need training to integrate these practices into their daily work. We organized a virtual brainstorming event, in collaboration with the German Reproducibility Network, to discuss strategies for making reproducible research and open science training the norm at research institutions. Here, we outline eleven strategies, concentrated in three areas: (1) offering training, (2) adapting research assessment criteria and program requirements, and (3) building communities. We provide a brief overview of each strategy, offer tips for implementation, and provide links to resources. Our goal is to encourage members of the research community to think creatively about the many ways they can contribute and collaborate to build communities, and make reproducible research and open science training the norm. Researchers may act in their roles as scientists, supervisors, mentors, instructors, and members of curriculum, hiring or evaluation committees. Institutional leadership and research administration and support staff can accelerate progress by implementing change across their institutions.","tags":["Curriculum Design","Higher Education","Hiring","Open Science","Reproducible Research","Research Institutions","Scientific Rigor","Teaching","Transparency"],"title":"Eleven Strategies for Making Reproducible Research and Open Science Training the Norm at Research Institutions","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"c33743c0ce58f98872cfd94b3203b41f","permalink":"https://forrt.org/curated_resources/elsevier-title-level-pricing-dissecting/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/elsevier-title-level-pricing-dissecting/","section":"curated_resources","summary":"INTRODUCTION This study will explore the issue of pricing opacity associated with prices paid by academic libraries that have recently unbundled from the Elsevier Big Deal journal package. Additionally, this study will provide metrics for assessing the fair market value (FMV) of unbundled journal packages. The pricing metrics will assist academic libraries in negotiations of subscription and open access agreements. METHODS Pricing information was gathered from five academic libraries. The data was analyzed to arrive at two key metrics (adjustment from list price and the average cost per journal) for establishing comparables, i.e., prices paid by similarly sized institutions, to assess the collective FMVs for unbundled Elsevier journal packages. RESULTS \u0026 DISCUSSION The study results show that significant variations existed in the way institutions were charged for content. Additionally, the comparables show wide variations among institutions when measured by the overall adjustment from list price and the average cost per journal. CONCLUSION The pricing metrics developed in this study, adjustment from list price (ALP) and average cost per journal (ACJ), will help libraries assess their final net prices for individual journal subscriptions. The results will be useful to administrators, collection development personnel, and negotiating teams in understanding the prices paid by other institutions for unbundled journal packages to determine FMVs.","tags":["Journal","Subscription"],"title":"Elsevier Title Level Pricing: Dissecting the Bowl of Spaghetti","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1721390377,"objectID":"f902f96a810230d76aa76a4efa4ddc9d","permalink":"https://forrt.org/glossary/english/embargo_period/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/english/embargo_period/","section":"glossary","summary":"","tags":null,"title":"Embargo Period","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1635961284,"objectID":"b492c88fca884caf35f05e7254d63a39","permalink":"https://forrt.org/glossary/vbeta/embargo-period/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/vbeta/embargo-period/","section":"glossary","summary":"","tags":null,"title":"Embargo Period","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1720865851,"objectID":"de69906adb62c95a0ad74805f5d86e12","permalink":"https://forrt.org/glossary/german/embargo_period/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/german/embargo_period/","section":"glossary","summary":"","tags":null,"title":"Embargo Period (Sperrfrist)","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"e72a58a3f34af6def97edfd1859a2668","permalink":"https://forrt.org/curated_resources/embedding-open-and-reproducible-science/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/embedding-open-and-reproducible-science/","section":"curated_resources","summary":"Recently, there has been a growing emphasis on embedding open and reproducible approaches into research. One essential step in accomplishing this larger goal is to embed such practices into undergraduate and postgraduate research training. However, this often requires substantial time and resources to implement. Also, while many pedagogical resources are regularly developed for this purpose, they are not often openly and actively shared with the wider community. The creation and public sharing of open educational resources is useful for educators who wish to embed open scholarship and reproducibility into their teaching and learning. In this article, we describe and openly share a bank of teaching resources and lesson plans on the broad topics of open scholarship, open science, replication, and reproducibility that can be integrated into taught courses, to support educators and instructors. These resources were created as part of the Society for the Improvement of Psychological Science (SIPS) hackathon at the 2021 Annual Conference, and we detail this collaborative process in the article. By sharing these open pedagogical resources, we aim to reduce the labour required to develop and implement open scholarship content to further the open scholarship and open educational materials movement.","tags":["Learning","Lesson Plans","Pedagogical Resources","Teaching"],"title":"Embedding open and reproducible science into teaching: A bank of lesson plans and resources","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"a91821e74ce2a238aa93c43ac11db5c8","permalink":"https://forrt.org/curated_resources/emerging-scientific-research-practices/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/emerging-scientific-research-practices/","section":"curated_resources","summary":"This course aims to introduce students to current controversies and new developments in recommended scientific practices. The course is meant to help students think critically about how to conduct better empirical research and how to draw better-informed statistical inferences. The course will be conducted in a “seminar meets workshop” style, with a focus on discussion, understanding, and accumulating hands-on experience with different research practices supporting inference. The course covers a range of approaches that aim to enhance the transparency and reproducibility of scientific research. Although many examples will stem from social and personality psychology, this course is appropriate for Ph.D. students across social science disciplines and related fields. Topics will include: Understanding the problem: introduction to the “replication crisis,” developments and debate How issues related to the interpretation of p-values vs. effect sizes and confidence intervals have contributed to the reproducibility issues and the accumulation of knowledge Understand how questionable research practices (QRPs) and publication bias distort the scientific record; learn how to interpret evidence from the scientific literature given these biases (e.g., with the use of p-curve analysis) Recommendations for improving conduct and reporting of research: e.g., pre-registration, open science tools (OSF) and methods, power analysis The future of science and publishing in the new era.","tags":["Reproducibility Crisis and Credibility Revolution","Open Science"],"title":"Emerging Scientific Research Practices","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"e49897debfd1f37aa6f735c3b339c21c","permalink":"https://forrt.org/curated_resources/empirical-assessment-of-published-effect/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/empirical-assessment-of-published-effect/","section":"curated_resources","summary":"We have empirically assessed the distribution of published effect sizes and estimated power by analyzing 26,841 statistical records from 3,801 cognitive neuroscience and psychology papers published recently. The reported median effect size was D = 0.93 (interquartile range: 0.64–1.46) for nominally statistically significant results and D = 0.24 (0.11–0.42) for nonsignificant results. Median power to detect small, medium, and large effects was 0.12, 0.44, and 0.73, reflecting no improvement through the past half-century. This is so because sample sizes have remained small. Assuming similar true effect sizes in both disciplines, power was lower in cognitive neuroscience than in psychology. Journal impact factors negatively correlated with power. Assuming a realistic range of prior probabilities for null hypotheses, false report probability is likely to exceed 50% for the whole literature. In light of our findings, the recently reported low replication success in psychology is realistic, and worse performance may be expected for cognitive neuroscience.","tags":["Analysis","Behavioral Neuroscience","Bibliometrics","Cognitive Neuroscience","Data","Neuroscience","Psychology","Publishing","Scientific Publishing","Statistical Data","Statistical Distributions","Statistics"],"title":"Empirical assessment of published effect sizes and power in the recent cognitive neuroscience and psychology literature","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1689122843,"objectID":"d6500f48c5f0379191070bfaddc8f0ee","permalink":"https://forrt.org/curated_resources/empirical-evaluation-of-very-large-treat/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/empirical-evaluation-of-very-large-treat/","section":"curated_resources","summary":"Context Most medical interventions have modest effects, but occasionally some clinical trials may find very large effects for benefits or harms. Objective To evaluate the frequency and features of very large effects in medicine. Data Sources Cochrane Database of Systematic Reviews (CDSR, 2010, issue 7). Study Selection We separated all binary-outcome CDSR forest plots with comparisons of interventions according to whether the first published trial, a subsequent trial (not the first), or no trial had a nominally statistically significant (P \u003c .05) very large effect (odds ratio [OR], ≥5). We also sampled randomly 250 topics from each group for further in-depth evaluation. Data Extraction We assessed the types of treatments and outcomes in trials with very large effects, examined how often large-effect trials were followed up by other trials on the same topic, and how these effects compared against the effects of the respective meta-analyses. Results Among 85 002 forest plots (from 3082 reviews), 8239 (9.7%) had a significant very large effect in the first published trial, 5158 (6.1%) only after the first published trial, and 71 605 (84.2%) had no trials with significant very large effects. Nominally significant very large effects typically appeared in small trials with median number of events: 18 in first trials and 15 in subsequent trials. Topics with very large effects were less likely than other topics to address mortality (3.6% in first trials, 3.2% in subsequent trials, and 11.6% in no trials with significant very large effects) and were more likely to address laboratory-defined efficacy (10% in first trials,10.8% in subsequent, and 3.2% in no trials with significant very large effects). First trials with very large effects were as likely as trials with no very large effects to have subsequent published trials. Ninety percent and 98% of the very large effects observed in first and subsequently published trials, respectively, became smaller in meta-analyses that included other trials; the median odds ratio decreased from 11.88 to 4.20 for first trials, and from 10.02 to 2.60 for subsequent trials. For 46 of the 500 selected topics (9.2%; first and subsequent trials) with a very large-effect trial, the meta-analysis maintained very large effects with P \u003c .001 when additional trials were included, but none pertained to mortality-related outcomes. Across the whole CDSR, there was only 1 intervention with large beneficial effects on mortality, P \u003c .001, and no major concerns about the quality of the evidence (for a trial on extracorporeal oxygenation for severe respiratory failure in newborns). Conclusions Most large treatment effects emerge from small studies, and when additional trials are performed, the effect sizes become typically much smaller. Well-validated large effects are uncommon and pertain to nonfatal outcomes.","tags":["Reproducibility Crisis and Credibility Revolution","Open Science"],"title":"Empirical Evaluation of Very Large Treatment Effects of Medical Interventions","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"80ae0215de2cbccd4249bf7110a35109","permalink":"https://forrt.org/curated_resources/empirical-study-of-data-sharing-by-autho/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/empirical-study-of-data-sharing-by-autho/","section":"curated_resources","summary":"Background Many journals now require authors share their data with other investigators, either by depositing the data in a public repository or making it freely available upon request. These policies are explicit, but remain largely untested. We sought to determine how well authors comply with such policies by requesting data from authors who had published in one of two journals with clear data sharing policies. Methods and Findings We requested data from ten investigators who had published in either PLoS Medicine or PLoS Clinical Trials. All responses were carefully documented. In the event that we were refused data, we reminded authors of the journal's data sharing guidelines. If we did not receive a response to our initial request, a second request was made. Following the ten requests for raw data, three investigators did not respond, four authors responded and refused to share their data, two email addresses were no longer valid, and one author requested further details. A reminder of PLoS's explicit requirement that authors share data did not change the reply from the four authors who initially refused. Only one author sent an original data set. Conclusions We received only one of ten raw data sets requested. This suggests that journal policies requiring data sharing do not lead to authors making their data sets available to independent investigators.","tags":["Clinical Trials","Data","Data Acquisition","Genomic Libraries","Genomic Medicine","Genomics","Health Care Policy","Medicine and Health Sciences","Open Access","Open Access Publishing","Publishing","Scientific Publishing"],"title":"Empirical Study of Data Sharing by Authors Publishing in PLoS Journals","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1689122843,"objectID":"3ca658b34ded0685d64f6a7c572188fd","permalink":"https://forrt.org/curated_resources/enabling-open-science-initiatives-in-cli/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/enabling-open-science-initiatives-in-cli/","section":"curated_resources","summary":"The psychological and psychiatric communities are generating data on an ever-increasing scale. To ensure that society reaps the greatest utility in research and clinical care from such rich resources, there is significant interest in wide-scale, open data sharing to foster scientific endeavors. However, it is imperative that such open-science initiatives ensure that data-privacy concerns are adequately addressed. In this article, we focus on these issues in clinical research. We review the privacy risks and then discuss how they can be mitigated through appropriate governance mechanisms that are both social (e.g., the application of data-use agreements) and technological (e.g., de-identification of structured data and unstructured narratives). We also discuss the benefits and drawbacks of these mechanisms, particularly as regards data fidelity. Our focus is on de-identification methods that meet regulatory requirements, such as the Privacy Rule of the Health Insurance Portability and Accountability Act of 1996. To illustrate their potential, we show how the principles we discuss have been applied in a large-scale clinical database and distributed research networks. We close this article with a discussion of challenges in supporting data privacy as open-science initiatives grow in their scale and complexity.","tags":["Reproducibility Crisis and Credibility Revolution","Open Science"],"title":"Enabling Open-Science Initiatives in Clinical Psychology and Psychiatry Without Sacrificing Patients’ Privacy: Current Practices and Future Challenges","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"f2a4b7afe5fd642cda6031267e118869","permalink":"https://forrt.org/curated_resources/enhancing-reproducibility-through-rigor/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/enhancing-reproducibility-through-rigor/","section":"curated_resources","summary":"The information provided on this website is designed to assist the extramural community in addressing rigor and transparency in NIH grant applications and progress reports. Scientific rigor and transparency in conducting biomedical research is key to the successful application of knowledge toward improving health outcomes. \n\nDefinition Scientific rigor is the strict application of the scientific method to ensure unbiased and well-controlled experimental design, methodology, analysis, interpretation and reporting of results. \n\nGoals The NIH strives to exemplify and promote the highest level of scientific integrity, public accountability, and social responsibility in the conduct of science. Grant applications instructions and the criteria by which reviewers are asked to evaluate the scientific merit of the application are intended to: \n• ensure that NIH is funding the best and most rigorous science, \n• highlight the need for applicants to describe details that may have been previously overlooked, \n• highlight the need for reviewers to consider such details in their reviews through updated review language, and \n• minimize additional burden.","tags":["Reproducibility"],"title":"Enhancing Reproducibility through Rigor and Transparency","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1689122843,"objectID":"aa72a7f60a0d41a743e6804363c8bfd7","permalink":"https://forrt.org/curated_resources/enhancing-transparency-of-the-research-p/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/enhancing-transparency-of-the-research-p/","section":"curated_resources","summary":"The purpose of this paper is to extend to the field of relationship science, recent discussions and suggested changes in open research practises. We demonstrate different ways that greater transparency of the research process in our field will accelerate scientific progress by increasing accuracy of reported research findings. Importantly, we make concrete recommendations for how relationship researchers can transition to greater disclosure of research practices in a manner that is sensitive to the unique design features of methodologies employed by relationship scientists. We discuss how to implement these recommendations for four different research designs regularly used in relationship research and practical limitations regarding implementing our recommendations and provide potential solutions to these problems.","tags":["Reproducibility Crisis and Credibility Revolution","Open Science"],"title":"Enhancing transparency of the research process to increase accuracy of findings: A guide for relationship researchers","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1721390377,"objectID":"bdb666d20ca14870bcad815ddfaa6bcd","permalink":"https://forrt.org/glossary/english/epistemic_uncertainty/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/english/epistemic_uncertainty/","section":"glossary","summary":"","tags":null,"title":"Epistemic uncertainty","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1635961284,"objectID":"45044966e22653258fb38bd0ebf4480c","permalink":"https://forrt.org/glossary/vbeta/epistemic-uncertainty/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/vbeta/epistemic-uncertainty/","section":"glossary","summary":"","tags":null,"title":"Epistemic uncertainty","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1720865851,"objectID":"0d997da97fda3a5ad5a6ee745ce5dad2","permalink":"https://forrt.org/glossary/german/epistemic_uncertainty/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/german/epistemic_uncertainty/","section":"glossary","summary":"","tags":null,"title":"Epistemic uncertainty (epistemische Unsicherheit)","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1721390377,"objectID":"7ed8f3a560b78e6a47cc8abefaa4bbe7","permalink":"https://forrt.org/glossary/english/epistemology/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/english/epistemology/","section":"glossary","summary":"","tags":null,"title":"Epistemology","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1635961284,"objectID":"5396133a85ef8bce0692b2860c26c09a","permalink":"https://forrt.org/glossary/vbeta/epistemology/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/vbeta/epistemology/","section":"glossary","summary":"","tags":null,"title":"Epistemology","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1720865851,"objectID":"77f835cd04b9db9e3e0d88e1e6d622ff","permalink":"https://forrt.org/glossary/german/epistemology/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/german/epistemology/","section":"glossary","summary":"","tags":null,"title":"Epistemology (Epistemiologie / Erkenntnistheorie)","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"6c1f3e3ff06994d06d36aeab11ec09ce","permalink":"https://forrt.org/curated_resources/eplatypus-an-ecosystem-for-computational/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/eplatypus-an-ecosystem-for-computational/","section":"curated_resources","summary":"Motivation\nThe maturation of systems immunology methodologies requires novel and transparent computational frameworks capable of integrating diverse data modalities in a reproducible manner.\n\nResults\nHere, we present the ePlatypus computational immunology ecosystem for immunogenomics data analysis, with a focus on adaptive immune repertoires and single-cell sequencing. ePlatypus is an open-source web-based platform and provides programming tutorials and an integrative database that helps elucidate signatures of B and T cell clonal selection. Furthermore, the ecosystem links novel and established bioinformatics pipelines relevant for single-cell immune repertoires and other aspects of computational immunology such as predicting ligand-receptor interactions, structural modeling, simulations, machine learning, graph theory, pseudotime, spatial transcriptomics and phylogenetics. The ePlatypus ecosystem helps extract deeper insight in computational immunology and immunogenomics and promote open science.","tags":["Systems Immunology","Bioinformatics","Computational Immunology","Ligand-receptor Interactions","Structural Modeling","Simulations","Machine Learning","Graph Theory","Pseudotime","Spatial Transcriptomics","Phylogenetics"],"title":"ePlatypus: an ecosystem for computational analysis of immunogenomics data","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1721390377,"objectID":"84a8c895afa5bacb06fbda19c7ebdb4d","permalink":"https://forrt.org/glossary/english/equity/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/english/equity/","section":"glossary","summary":"","tags":null,"title":"Equity","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1635961284,"objectID":"5f0809eb1b7ac7447057bee42aed4afe","permalink":"https://forrt.org/glossary/vbeta/equity/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/vbeta/equity/","section":"glossary","summary":"","tags":null,"title":"Equity","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1720865851,"objectID":"ec3f2eeaef88c3172b02c3ee1c46f5d6","permalink":"https://forrt.org/glossary/german/equity_/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/german/equity_/","section":"glossary","summary":"","tags":null,"title":"Equity  (Gleichstellung)","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"55c1082af3c40709095a1e3973c8fc1f","permalink":"https://forrt.org/curated_resources/equity-transparency-and-accountability-o/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/equity-transparency-and-accountability-o/","section":"curated_resources","summary":"Knowledge is essential to saving lives and improving wellbeing. The term open science has been applied to improving the transparency of knowledge generation, but open science also has the potential to address many of the problems of inequity, inaccuracy, and misconduct that plague research, as well as to build public trust.","tags":["Equity","Transparency","Accountability","Open Science","Misconduct","Accuracy"],"title":"Equity, transparency, and accountability: Open science for the 21st century","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1721390377,"objectID":"e572e42d302e4d532d8e8df424fe1c33","permalink":"https://forrt.org/glossary/english/equivalence_testing/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/english/equivalence_testing/","section":"glossary","summary":"","tags":null,"title":"Equivalence Testing","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1635961284,"objectID":"5080797b79e6bd6dd66157c253003616","permalink":"https://forrt.org/glossary/vbeta/equivalence-testing/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/vbeta/equivalence-testing/","section":"glossary","summary":"","tags":null,"title":"Equivalence Testing","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1720865851,"objectID":"8ed95d02ddb80b03d7014cc486d24701","permalink":"https://forrt.org/glossary/german/equivalence_testing/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/german/equivalence_testing/","section":"glossary","summary":"","tags":null,"title":"Equivalence Testing (Äquivalenztesten)","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"e8564beebab95860ba712291cb11a572","permalink":"https://forrt.org/curated_resources/equivalence-testing-for-psychological-re/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/equivalence-testing-for-psychological-re/","section":"curated_resources","summary":"Psychologists must be able to test both for the presence of an effect and for the absence of an effect. In addition to testing against zero, researchers can use the two one-sided tests (TOST) procedure to test for equivalence and reject the presence of a smallest effect size of interest (SESOI). The TOST procedure can be used to determine if an observed effect is surprisingly small, given that a true effect at least as extreme as the SESOI exists. We explain a range of approaches to determine the SESOI in psychological science and provide detailed examples of how equivalence tests should be performed and reported. Equivalence tests are an important extension of the statistical tools psychologists currently use and enable researchers to falsify predictions about the presence, and declare the absence, of meaningful effects.","tags":["Aging","Equivalence Testing","Falsification","Frequentist","Null-hypothesis","Null-hypothesis Significance Test","Open Materials","Open Science","Power"],"title":"Equivalence Testing for Psychological Research: A Tutorial","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"1f6d86f6ba05220d26c201353506dd43","permalink":"https://forrt.org/curated_resources/equivalence-tests-a-practical-primer-for/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/equivalence-tests-a-practical-primer-for/","section":"curated_resources","summary":"Scientists should be able to provide support for the absence of a meaningful effect. Currently, researchers often incorrectly conclude an effect is absent based a nonsignificant result. A widely recommended approach within a frequentist framework is to test for equivalence. In equivalence tests, such as the two one-sided tests (TOST) procedure discussed in this article, an upper and lower equivalence bound is specified based on the smallest effect size of interest. The TOST procedure can be used to statistically reject the presence of effects large enough to be considered worthwhile. This practical primer with accompanying spreadsheet and R package enables psychologists to easily perform equivalence tests (and power analyses) by setting equivalence bounds based on standardized effect sizes and provides recommendations to prespecify equivalence bounds. Extending your statistical tool kit with equivalence tests is an easy way to improve your statistical and theoretical inferences.","tags":["Analysis","Reproducibility","Statistics"],"title":"Equivalence Tests: A Practical Primer for t Tests, Correlations, and Meta-Analyses","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1689122843,"objectID":"2d7422c3d5faeaafe3999efea51995a6","permalink":"https://forrt.org/curated_resources/erroneous-analyses-of-interactions-in-ne/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/erroneous-analyses-of-interactions-in-ne/","section":"curated_resources","summary":"In theory, a comparison of two experimental effects requires a statistical test on their difference. In practice, this comparison is often based on an incorrect procedure involving two separate tests in which researchers conclude that effects differ when one effect is significant (P \u003c 0.05) but the other is not (P \u003e 0.05). We reviewed 513 behavioral, systems and cognitive neuroscience articles in five top-ranking journals (Science, Nature, Nature Neuroscience, Neuron and The Journal of Neuroscience) and found that 78 used the correct procedure and 79 used the incorrect procedure. An additional analysis suggests that incorrect analyses of interactions are even more common in cellular and molecular neuroscience. We discuss scenarios in which the erroneous procedure is particularly beguiling.","tags":[""],"title":"Erroneous analyses of interactions in neuroscience: a problem of significance. ","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1721390377,"objectID":"9dee0afe3b64940b70e15d7516727318","permalink":"https://forrt.org/glossary/english/error_detection/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/english/error_detection/","section":"glossary","summary":"","tags":null,"title":"Error detection","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1635961284,"objectID":"e509af4583519e145ffbc89d4e9d06a1","permalink":"https://forrt.org/glossary/vbeta/error-detection/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/vbeta/error-detection/","section":"glossary","summary":"","tags":null,"title":"Error detection","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1720865851,"objectID":"c40b4cafb812e6cc7d66d74054f2207f","permalink":"https://forrt.org/glossary/german/error_detection/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/german/error_detection/","section":"glossary","summary":"","tags":null,"title":"Error detection (Fehlererfassung)","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"489787c28915cdec0c64592fdfc662b4","permalink":"https://forrt.org/curated_resources/establishing-trust-in-automated-reasonin/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/establishing-trust-in-automated-reasonin/","section":"curated_resources","summary":"Since its beginnings in the 1940s, automated reasoning by computers has become a tool of ever growing importance in scientific research.So far, the rules underlying automated reasoning have mainly beenformulated by humans, in the form of program source code. Rulesderived from large amounts of data, via machine learning techniques,are a complementary approach currently under intense development.The question of why we should trust these systems, and the resultsobtained with their help, has been discussed by philosophers of sciencebut has so far received little attention by practitioners. The presentwork focuses on independent reviewing, an important source of trustin science, and identifies the characteristics of automated reasoningsystems that affect their reviewability. It also discusses possible stepstowards increasing reviewability and trustworthiness via a combinationof technical and social measure","tags":["computational science","machine learning","reliability","reviewability","software"],"title":"Establishing trust in automated reasoning","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"2311c02dac3039190301295239592ad8","permalink":"https://forrt.org/curated_resources/estimating-the-prevalence-of-transparenc/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/estimating-the-prevalence-of-transparenc/","section":"curated_resources","summary":"Psychological science is navigating an unprecedented period of introspection about the credibility and utility of its research. A number of reform initiatives aimed at increasing adoption of transparency and reproducibility-related research practices appear to have been effective in specific contexts; however, their broader, collective impact amidst a wider discussion about research credibility and reproducibility is largely unknown. In the present study, we estimated the prevalence of several transparency and reproducibility-related indicators in the psychology literature published between 2014-2017 by manually assessing these indicators in a random sample of 250 articles. Over half of the articles we examined were publicly available (154/237, 65% [95% confidence interval, 59% to 71%]). However, sharing of important research resources such as materials (26/183, 14% [10% to 19%]), study protocols (0/188, 0% [0% to 1%]), raw data (4/188, 2% [1% to 4%]), and analysis scripts (1/188, 1% [0% to 1%]) was rare. Pre-registration was also uncommon (5/188, 3% [1% to 5%]). Although many articles included a funding disclosure statement (142/228, 62% [56% to 69%]), conflict of interest disclosure statements were less common (88/228, 39% [32% to 45%]). Replication studies were rare (10/188, 5% [3% to 8%]) and few studies were included in systematic reviews (21/183, 11% [8% to 16%]) or meta-analyses (12/183, 7% [4% to 10%]). Overall, the findings suggest that transparent and reproducibility-related research practices are far from routine in psychological science. Future studies can use the present findings as a baseline to assess progress towards increasing the credibility and utility of psychology research.","tags":["Data"],"title":"Estimating the prevalence of transparency and reproducibility-related research practices in psychology (2014-2017)","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1689122843,"objectID":"b30aaf78557c3acb3a51e3834bfafeed","permalink":"https://forrt.org/curated_resources/estimating-the-reproducibility-of-psycho/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/estimating-the-reproducibility-of-psycho/","section":"curated_resources","summary":"Reproducibility is a defining feature of science, but the extent to which it characterizes current research is unknown. We conducted replications of 100 experimental and correlational studies published in three psychology journals using high-powered designs and original materials when available. Replication effects were half the magnitude of original effects, representing a substantial decline. Ninety-seven percent of original studies had statistically significant results. Thirty-six percent of replications had statistically significant results; 47% of original effect sizes were in the 95% confidence interval of the replication effect size; 39% of effects were subjectively rated to have replicated the original result; and if no bias in original results is assumed, combining original and replication results left 68% with statistically significant effects. Correlational tests suggest that replication success was better predicted by the strength of original evidence than by characteristics of the original and replication teams.","tags":["Reproducibility Crisis and Credibility Revolution","Open Science"],"title":"Estimating the reproducibility of psychological science","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"96cd3b1a86fd2392a3227f3f2272da2b","permalink":"https://forrt.org/curated_resources/european-sociological-association-journa/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/european-sociological-association-journa/","section":"curated_resources","summary":"The MIT Press is thrilled to announce a groundbreaking partnership with the European Sociological Association (ESA), marking a significant step forward in the world of academic open access publishing. We are proud to welcome European Societies and European Journal of Cultural and Political Sociology to MIT Press as premier diamond open access publications, with new issues commencing in 2025.","tags":["Journal","Open Access"],"title":"European Sociological Association journals European Societies and European Journal of Cultural and Political Sociology move to  diamond open access at the MIT Press","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"efc5d3bc0d816921df56ad6090a953e3","permalink":"https://forrt.org/curated_resources/evaluating-content-related-validity-evid/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/evaluating-content-related-validity-evid/","section":"curated_resources","summary":"Validity evidence based on test content is critical to meaningful interpretation of test scores. Within high-stakes testing and accountability frameworks, content-related validity evidence is typically gathered via alignment studies, with panels of experts providing qualitative judgments on the degree to which test items align with the representative content standards. Various summary statistics are then calculated (e.g., categorical concurrence, balance of representation) to aid in decision-making. In this paper, we propose an alternative approach for gathering content-related validity evidence that capitalizes on the overlap in vocabulary used in test items and the corresponding content standards, which we define as textual congruence. We use a text-based, machine learning model, specifically topic modeling, to identify clusters of related content within the standards. This model then serves as the basis from which items are evaluated. We illustrate our method by building a model from the Next Generation Science Standards, with textual congruence evaluated against items within the Oregon statewide alternate assessment. We discuss the utility of this approach as a source of triangulating and diagnostic information and show how visualizations can be used to evaluate the overall coverage of the content standards across the test items.","tags":["Machine Learning","Text-mining","Textual Congruence","Validity"],"title":"Evaluating Content-Related Validity Evidence Using a Text-Based Machine Learning Procedure","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"14c81d72acecddfca1573821679fd8e1","permalink":"https://forrt.org/curated_resources/evaluating-registered-reports-a-naturali/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/evaluating-registered-reports-a-naturali/","section":"curated_resources","summary":"Registered Reports (RRs) is a publishing model in which initial peer review is conducted prior to knowing the outcomes of the research. In-principle acceptance of papers at this review stage combats publication bias, and provides a clear distinction between confirmatory and exploratory research. Some editors raise a practical concern about adopting RRs. By reducing publication bias, RRs may produce more negative or mixed results and, if such results are not valued by the research community, receive less citations as a consequence. If so, by adopting RRs, a journal’s impact factor may decline. Despite known flaws with impact factor, it is still used as a heuristic for judging journal prestige and quality. Whatever the merits of considering impact factor as a decision-rule for adopting RRs, it is worthwhile to know whether RRs are cited less than other articles. We will conduct a naturalistic comparison of citation and altmetric impact between published RRs and comparable empirical articles from the same journals.","tags":["Preregistration","Publishing","Registered Reports"],"title":"Evaluating Registered Reports: A Naturalistic Comparative Study of Article Impact","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1689122843,"objectID":"9b290b0b64b7179445a3c9d6f63f5ca9","permalink":"https://forrt.org/curated_resources/evaluating-the-r-index-and-the-p-curve/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/evaluating-the-r-index-and-the-p-curve/","section":"curated_resources","summary":"This blog evaluates the R-Index and the P-Curve","tags":["Blog"],"title":"Evaluating the R-Index and the P-Curve","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"c96c6ab53c426b182b202b26aab9e5a2","permalink":"https://forrt.org/curated_resources/evaluation-of-transparency-and-openness/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/evaluation-of-transparency-and-openness/","section":"curated_resources","summary":"Objective\nThe goals of this study were to evaluate the extent that physical therapy journals support open science research practices by adhering to the Transparency and Openness Promotion guidelines and to assess the relationship between journal scores and their respective journal impact factor.\n\nMethods\nScimago, mapping studies, the National Library of Medicine, and journal author guidelines were searched to identify physical therapy journals for inclusion. Journals were graded on 10 standards (29 available total points) related to transparency with data, code, research materials, study design and analysis, preregistration of studies and statistical analyses, replication, and open science badges. The relationship between journal transparency and openness scores and their journal impact factor was determined.\n\nResults\nThirty-five journals’ author guidelines were assigned transparency and openness factor scores. The median score (interquartile range) across journals was 3.00 out of 29 (3.00) points (for all journals the scores ranged from 0–8). The 2 standards with the highest degree of implementation were design and analysis transparency (reporting guidelines) and study preregistration. No journals reported on code transparency, materials transparency, replication, and open science badges. Transparency and openness promotion factor scores were a significant predictor of journal impact factor scores.\n\nConclusion\nThere is low implementation of the transparency and openness promotion standards by physical therapy journals. Transparency and openness promotion factor scores demonstrated predictive abilities for journal impact factor scores. Policies from journals must improve to make open science practices the standard in research. Journals are in an influential position to guide practices that can improve the rigor of publication which, ultimately, enhances the evidence-based information used by physical therapists.\n\nImpact\nTransparent, open, and reproducible research will move the profession forward by improving the quality of research and increasing the confidence in results for implementation in clinical care.","tags":["Openness","Reproducibility of Results","Research","Science","Transparency"],"title":"Evaluation of Transparency and Openness Guidelines in Physical Therapy Journals","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1689122843,"objectID":"97c212355d11879d8f8def91193b6ce5","permalink":"https://forrt.org/curated_resources/everything-hertz/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/everything-hertz/","section":"curated_resources","summary":"A podcast about open science and psychology","tags":["Podcast","Reproducibility Crisis and Credibility Revolution"],"title":"Everything Hertz","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"76a573a69944b284d79b55980fd368f0","permalink":"https://forrt.org/curated_resources/evidence-of-insufficient-quality-of-repo/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/evidence-of-insufficient-quality-of-repo/","section":"curated_resources","summary":"Despite the importance of patent landscape analyses in the commercialization process for life science and healthcare technologies, the quality of reporting for patent landscapes published in academic journals is inadequate. Patents in the life sciences are a critical metric of innovation and a cornerstone for the commercialization of new life-science- and healthcare-related technologies. Patent landscaping has emerged as a methodology for analyzing multiple patent documents to uncover technological trends, geographic distributions of patents, patenting trends and scope, highly cited patents and a number of other uses. Many such analyses are published in high-impact journals, potentially allowing them to gain high visibility among academic, industry and government stakeholders. Such analyses may be used to inform decision-making processes, such as prioritization of funding areas, identification of commercial competition (and therefore strategy development), or implementation of policy to encourage innovation or to ensure responsible licensing of technologies. Patent landscaping may also provide a means for answering fundamental questions regarding the benefits and drawbacks of patenting in the life sciences, a subject on which there remains considerable debate but limited empirical evidence.","tags":["Licensing","Reproducibility"],"title":"Evidence of insufficient quality of reporting in patent landscapes in the life sciences","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1721390377,"objectID":"691b19acd157f037a3bbff0ea034762c","permalink":"https://forrt.org/glossary/english/evidence_synthesis/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/english/evidence_synthesis/","section":"glossary","summary":"","tags":null,"title":"Evidence Synthesis","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1635961284,"objectID":"1202560e575f766099baf665bc4bd21e","permalink":"https://forrt.org/glossary/vbeta/evidence-synthesis/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/vbeta/evidence-synthesis/","section":"glossary","summary":"","tags":null,"title":"Evidence Synthesis","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1720865851,"objectID":"7cbc9ef9e7e194cfa087dd3c816ea344","permalink":"https://forrt.org/glossary/german/evidence_synthesis/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/german/evidence_synthesis/","section":"glossary","summary":"","tags":null,"title":"Evidence Synthesis (Evidenzsynthese)","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1689122843,"objectID":"2550a2998be4315c742531b5c296d608","permalink":"https://forrt.org/curated_resources/evolution-of-reporting-p-values-in-the-b/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/evolution-of-reporting-p-values-in-the-b/","section":"curated_resources","summary":"Importance: The use and misuse of P values has generated extensive debates. Objective: To evaluate in large scale the P values reported in the abstracts and full text of biomedical research articles over the past 25 years and determine how frequently statistical information is presented in ways other than P values. Design: Automated text-mining analysis was performed to extract data on P values reported in 12,821,790 MEDLINE abstracts and in 843,884 abstracts and full-text articles in PubMed Central (PMC) from 1990 to 2015. Reporting of P values in 151 English-language core clinical journals and specific article types as classified by PubMed also was evaluated. A random sample of 1000 MEDLINE abstracts was manually assessed for reporting of P values and other types of statistical information; of those abstracts reporting empirical data, 100 articles were also assessed in full text. Main outcomes and measures: P values reported. Results: Text mining identified 4,572,043 P values in 1,608,736 MEDLINE abstracts and 3,438,299 P values in 385,393 PMC full-text articles. Reporting of P values in abstracts increased from 7.3% in 1990 to 15.6% in 2014. In 2014, P values were reported in 33.0% of abstracts from the 151 core clinical journals (n = 29,725 abstracts), 35.7% of meta-analyses (n = 5620), 38.9% of clinical trials (n = 4624), 54.8% of randomized controlled trials (n = 13,544), and 2.4% of reviews (n = 71,529). The distribution of reported P values in abstracts and in full text showed strong clustering at P values of .05 and of .001 or smaller. Over time, the \"best\" (most statistically significant) reported P values were modestly smaller and the \"worst\" (least statistically significant) reported P values became modestly less significant. Among the MEDLINE abstracts and PMC full-text articles with P values, 96% reported at least 1 P value of .05 or lower, with the proportion remaining steady over time in PMC full-text articles. In 1000 abstracts that were manually reviewed, 796 were from articles reporting empirical data; P values were reported in 15.7% (125/796 [95% CI, 13.2%-18.4%]) of abstracts, confidence intervals in 2.3% (18/796 [95% CI, 1.3%-3.6%]), Bayes factors in 0% (0/796 [95% CI, 0%-0.5%]), effect sizes in 13.9% (111/796 [95% CI, 11.6%-16.5%]), other information that could lead to estimation of P values in 12.4% (99/796 [95% CI, 10.2%-14.9%]), and qualitative statements about significance in 18.1% (181/1000 [95% CI, 15.8%-20.6%]); only 1.8% (14/796 [95% CI, 1.0%-2.9%]) of abstracts reported at least 1 effect size and at least 1 confidence interval. Among 99 manually extracted full-text articles with data, 55 reported P values, 4 presented confidence intervals for all reported effect sizes, none used Bayesian methods, 1 used false-discovery rates, 3 used sample size/power calculations, and 5 specified the primary outcome. Conclusions and relevance: In this analysis of P values reported in MEDLINE abstracts and in PMC articles from 1990-2015, more MEDLINE abstracts and articles reported P values over time, almost all abstracts and articles with P values reported statistically significant results, and, in a subgroup analysis, few articles included confidence intervals, Bayes factors, or effect sizes. Rather than reporting isolated P values, articles should include effect sizes and uncertainty metrics.","tags":[""],"title":"Evolution of Reporting P Values in the Biomedical Literature, 1990-2015","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1689122843,"objectID":"b62b9b93bfe69c3b36d51f5ba4ab3983","permalink":"https://forrt.org/curated_resources/excess-success-for-psychology-articles-i/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/excess-success-for-psychology-articles-i/","section":"curated_resources","summary":"This article describes a systematic analysis of the relationship between empirical data and theoretical conclusions for a set of experimental psychology articles published in the journal Science between 2005–2012. When the success rate of a set of empirical studies is much higher than would be expected relative to the experiments’ reported effects and sample sizes, it suggests that null findings have been suppressed, that the experiments or analyses were inappropriate, or that the theory does not properly follow from the data. The analyses herein indicate such excess success for 83% (15 out of 18) of the articles in Science that report four or more studies and contain sufficient information for the analysis. This result suggests a systematic pattern of excess success among psychology articles in the journal Science.","tags":["Reproducibility Crisis and Credibility Revolution","Open Science"],"title":"Excess Success for Psychology Articles in the Journal Science","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1689122843,"objectID":"5fb425cd378f9795373dc8a068bf5bf1","permalink":"https://forrt.org/curated_resources/expectations-for-replications-are-yours/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/expectations-for-replications-are-yours/","section":"curated_resources","summary":"Failures to replicate published psychological research findings have contributed to a \"crisis of confidence.\" Several reasons for these failures have been proposed, the most notable being questionable research practices and data fraud. We examine replication from a different perspective and illustrate that current intuitive expectations for replication are unreasonable. We used computer simulations to create thousands of ideal replications, with the same participants, wherein the only difference across replications was random measurement error. In the first set of simulations, study results differed substantially across replications as a result of measurement error alone. This raises questions about how researchers should interpret failed replication attempts, given the large impact that even modest amounts of measurement error can have on observed associations. In the second set of simulations, we illustrated the difficulties that researchers face when trying to interpret and replicate a published finding. We also assessed the relative importance of both sampling error and measurement error in producing variability in replications. Conventionally, replication attempts are viewed through the lens of verifying or falsifying published findings. We suggest that this is a flawed perspective and that researchers should adjust their expectations concerning replications and shift to a meta-analytic mind-set.","tags":["Reproducibility Crisis and Credibility Revolution","Open Science"],"title":"Expectations for Replications: Are Yours Realistic?","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"e9b9f1a2e6c7e32d3fdfd40697bfe258","permalink":"https://forrt.org/curated_resources/experimentation-and-manipulation-with-pr/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/experimentation-and-manipulation-with-pr/","section":"curated_resources","summary":"Preregistration requires scientists to describe the planned research activities before their project begins. Preregistration improves transparency in empirical research and is an institutional response to scientific misconduct. This paper studies the impact of a preregistration requirement in a model in which a sender can generate information for a receiver by running private experiments. The sender can also engage in uninformative manipulation. This paper argues that a preregistration requirement can discourage p-hacking, but also result in even more detrimental faked studies.","tags":["Preregistration","Experimentation","Manipulation"],"title":"Experimentation and manipulation with preregistration","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"8f1ede784a9d2ea1c5dbeb7e9d864a09","permalink":"https://forrt.org/curated_resources/experimenter-as-automaton-experimenter-a/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/experimenter-as-automaton-experimenter-a/","section":"curated_resources","summary":"The crisis of confidence in the social sciences has many corollaries which impact our research practices. One of these is a push towards maximal and mechanical objectivity in quantitative research. This stance is reinforced by major journals and academic institutions that subtly yet certainly link objectivity with integrity and rigor. The converse implication of this may be an association between subjectivity and low quality. Subjectivity is one of qualitative methodology’s best assets, however. In qualitative methodology, that subjectivity is often given voice through reflexivity. It is used to better understand our own role within the research process, and is a means through which the researcher may oversee how they influence their research. Given that the actions of researchers have led to the poor reproducibility characterising the crisis of confidence, it is worthwhile to consider whether reflexivity can help improve the validity of research findings in quantitative psychology. In this report, we describe a combination approach of research: the data of a series of interviews helps us elucidate the link between reflexive practice and quality of research, through the eyes of practicing academics. Through our exploration of the position of the researcher in their research, we shed light on how the reflections of the researcher can impact the quality of their research findings, in the context of the current crisis of confidence. The validity of these findings is tempered, however, by limitations to the sample, and we advise caution on the part of our audience in their reading of our conclusions.","tags":["Reflexivity","Subjectivity","Reproducibility"],"title":"Experimenter as automaton; experimenter as human: Exploring the position of the researcher in scientific research","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1689122843,"objectID":"680dfb9ed78886341518d4dac58036de","permalink":"https://forrt.org/curated_resources/experiments-with-more-than-one-random-fa/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/experiments-with-more-than-one-random-fa/","section":"curated_resources","summary":"Traditional methods of analyzing data from psychological experiments are based on the assumption that there is a single random factor (normally participants) to which generalization is sought. However, many studies involve at least two random factors (e.g., participants and the targets to which they respond, such as words, pictures, or individuals). The application of traditional analytic methods to the data from such studies can result in serious bias in testing experimental effects. In this review, we develop a comprehensive typology of designs involving two random factors, which may be either crossed or nested, and one fixed factor, condition. We present appropriate linear mixed models for all designs and develop effect size measures. We provide the tools for power estimation for all designs. We then discuss issues of design choice, highlighting power and feasibility considerations. Our goal is to encourage appropriate analytic methods that produce replicable results for studies involving new samples of both participants and targets.","tags":[""],"title":"Experiments with More Than One Random Factor: Designs, Analytic Models, and Statistical Power.","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1721390377,"objectID":"b4a2a1471f3fb859ba5c76f41e6bd5ba","permalink":"https://forrt.org/glossary/english/exploratory_data_analysis/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/english/exploratory_data_analysis/","section":"glossary","summary":"","tags":null,"title":"Exploratory data analysis","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1635961284,"objectID":"10dab4bbbc35cd06a247ac638845687c","permalink":"https://forrt.org/glossary/vbeta/exploratory-data-analysis/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/vbeta/exploratory-data-analysis/","section":"glossary","summary":"","tags":null,"title":"Exploratory data analysis","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1720865851,"objectID":"528df36ad57cd1b3333fdeae09f5196e","permalink":"https://forrt.org/glossary/german/exploratory_data_analysis/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/german/exploratory_data_analysis/","section":"glossary","summary":"","tags":null,"title":"Exploratory data analysis (Explorative Datenanalyse)","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1689122843,"objectID":"0ec3b8435dcaffaf3c168bcdb45b0f8a","permalink":"https://forrt.org/curated_resources/exploratory-factor-analysis/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/exploratory-factor-analysis/","section":"curated_resources","summary":"This book provides a non-mathematical introduction to the underlying theory of Efa and reviews the key decisions that must be made in its implementation. Among the issues discussed are the use of confirmatory versus exploratory factor analysis, the use of principal components analysis versus common factor analysis, procedures for determining the appropriate number of factors, and methods for rotating factor solutions. Explanations and illustrations of the application of different factor analytic procedures are provided for analyses using common statistical packages (Spss and Sas), as well as a free package available on the web (Comprehensive Exploratory Factor Analysis). In addition, practical instructions are provided for conducting a number of useful factor analytic procedures not included in the statistical packages.","tags":["Book"],"title":"Exploratory Factor Analysis","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"56d40a7110601e29af5f76f8d561badb","permalink":"https://forrt.org/curated_resources/exploratory-hypothesis-tests-can-be-more/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/exploratory-hypothesis-tests-can-be-more/","section":"curated_resources","summary":"Preregistration has been proposed as a useful method for making a publicly verifiable distinction between confirmatory hypothesis tests, which involve planned tests of ante hoc hypotheses, and exploratory hypothesis tests, which involve unplanned tests of post hoc hypotheses. This distinction is thought to be important because it has been proposed that confirmatory hypothesis tests provide more compelling results (less uncertain, less tentative, less open to bias) than exploratory hypothesis tests. In this article, we challenge this proposition and argue that there are several advantages of exploratory hypothesis tests that can make their results more compelling than those of confirmatory hypothesis tests. We also consider some potential disadvantages of exploratory hypothesis tests and conclude that their advantages can outweigh the disadvantages. We conclude that exploratory hypothesis tests avoid researcher commitment and researcher prophecy biases, reduce the probability of data fraud, are more appropriate in the context of unplanned deviations, facilitate inference to the best explanation, and allow peer reviewers to make additional contributions at the data analysis stage. In contrast, confirmatory hypothesis tests may lead to an inappropriate level of confidence in research conclusions, less appropriate analyses in the context of unplanned deviations, and greater bias and errors in theoretical inferences.","tags":["Accommodation","Exploratory Analyses","Confirmatory Analyses","Prediction","Preregistration","Hypothesis Testing"],"title":"Exploratory hypothesis tests can be more compelling than confirmatory hypothesis tests","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"60942a4e0e61d3d70826b6d23ab11e12","permalink":"https://forrt.org/curated_resources/exploring-pre-registration-and-pre-analy/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/exploring-pre-registration-and-pre-analy/","section":"curated_resources","summary":"In recent years, the discipline of political science has experienced demands and moves toward greater research transparency. While in quantitative research increased transparency through replication has become a fairly accepted convention, in qualitative political science, the debate over whether and how to practice transparency is on-going. The practical tools and guidelines put forward to increase reliability of qualitative work so far emphasize a later stage of research, i.e. the phase of data analysis and sharing transcripts. To complement these suggestions, in this paper we explore how pre-registration and pre-analysis plans – as more recently introduced in experimental social science – can foster production transparency and analytic transparency in qualitative research. We argue that a “qualitative version” of pre-registration and PAPs can potentially incorporate a variety of approaches to inference.We discuss the general beneﬁts and costs of such tools, and how they could inform qualitative research.Then, we provide a pre-registration template for such purposes.","tags":["Transparency","Replication","Qualitative Inference","Pre-Analysis Plans","Preregistration"],"title":"Exploring Pre-registration and Pre-analysis Plans for Qualitative Inference","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1721390377,"objectID":"464d023b23ab2283498458c8e86531a6","permalink":"https://forrt.org/glossary/english/external_validity/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/english/external_validity/","section":"glossary","summary":"","tags":null,"title":"External Validity","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1635961284,"objectID":"e036ec5ee16a38cea62bd765e302aa1a","permalink":"https://forrt.org/glossary/vbeta/external-validity/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/vbeta/external-validity/","section":"glossary","summary":"","tags":null,"title":"External Validity","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1720865851,"objectID":"695bb81eced6e427767adee82b35d819","permalink":"https://forrt.org/glossary/german/external_validity/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/german/external_validity/","section":"glossary","summary":"","tags":null,"title":"External Validity (externe Validität)","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1721390377,"objectID":"9fac0c5bb1bdb7b98fba7bcf74083cc7","permalink":"https://forrt.org/glossary/english/face_validity/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/english/face_validity/","section":"glossary","summary":"","tags":null,"title":"Face validity","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1635961284,"objectID":"3d46b15543dd75e63ea95d7223eb1bc7","permalink":"https://forrt.org/glossary/vbeta/face-validity/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/vbeta/face-validity/","section":"glossary","summary":"","tags":null,"title":"Face validity","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1720865851,"objectID":"7128256c3ddd283c9b26cf524af8ebe0","permalink":"https://forrt.org/glossary/german/face_validity/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/german/face_validity/","section":"glossary","summary":"","tags":null,"title":"Face validity (Augenscheinvalidität)","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"85b770285fd21c42ff71ccde29f99a6c","permalink":"https://forrt.org/curated_resources/facilitating-open-science-practices-for/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/facilitating-open-science-practices-for/","section":"curated_resources","summary":"Researchers increasingly engage in adopting open science practices in the field of research syntheses, such as preregistration. Preregistration is a central open science practice in empirical research to enhance transparency in the research process and it gains steady adoption in the context of conducting research synthesis. From an interdisciplinary perspective, frameworks and particularly templates are lacking which support researchers preparing a preregistration. To this end, we introduce preregRS, a template to guide researchers across disciplines through the process of preregistering research syntheses. We utilized an R Markdown template file to provide a framework that structures the process of preparing a preregistration. Researchers can write up the preregistration using the template file similar to filling out a form, with the template providing additional hints and further information for the decisions along the framework. We integrated the R Markdown template in an R package for easy installation and use, but also provide a browser-based option for users granting low-barrier access. PreregRS constitutes a first step to facilitate and support preregistration with research syntheses for all disciplines. It further adds to establishing open science practices in conducting research syntheses.","tags":["Open Science","Preregistration","Rmarkdown","R Package"],"title":"Facilitating open science practices for research syntheses: PreregRS guides preregistration","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1689122843,"objectID":"a9d5f0cc249af53f27625b933beeeadf","permalink":"https://forrt.org/curated_resources/facts-are-more-important-than-novelty-re/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/facts-are-more-important-than-novelty-re/","section":"curated_resources","summary":"Despite increased attention to methodological rigor in education research, the field has focused heavily on experimental design and not on the merit of replicating important results. The present study analyzed the complete publication history of the current top 100 education journals ranked by 5-year impact factor and found that only 0.13% of education articles were replications. Contrary to previous findings in medicine, but similar to psychology, the majority of education replications successfully replicated the original studies. However, replications were significantly less likely to be successful when there was no overlap in authorship between the original and replicating articles. The results emphasize the importance of third-party, direct replications in helping education research improve its ability to shape education policy and practice.","tags":["Reproducibility Crisis and Credibility Revolution","Open Science"],"title":"Facts Are More Important Than Novelty: Replication in the Education Sciences","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1721390377,"objectID":"a53b07e26e490fe887c63e0351294f5a","permalink":"https://forrt.org/glossary/english/fair_principles/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/english/fair_principles/","section":"glossary","summary":"","tags":null,"title":"FAIR principles","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1635961284,"objectID":"4fa453891c0084ce2e97236552094b69","permalink":"https://forrt.org/glossary/vbeta/fair-principles/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/vbeta/fair-principles/","section":"glossary","summary":"","tags":null,"title":"FAIR principles","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1720865851,"objectID":"03d5e09d241779fab948e3fd006b5454","permalink":"https://forrt.org/glossary/german/fair_principles/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/german/fair_principles/","section":"glossary","summary":"","tags":null,"title":"FAIR principles (FAIR Prinzipien)","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1689122843,"objectID":"a73154779d0ac8ad172f98bd2de7bc28","permalink":"https://forrt.org/curated_resources/faking-science-a-true-story-of-academic/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/faking-science-a-true-story-of-academic/","section":"curated_resources","summary":"A book about Academic Fraud","tags":["Textbook","Reproducibility Crisis and Credibility Revolution"],"title":"Faking Science: A True Story of Academic Fraud","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1689122843,"objectID":"da83f430c34c9b64b4f2c292607f5590","permalink":"https://forrt.org/curated_resources/false-positive-psychology/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/false-positive-psychology/","section":"curated_resources","summary":"A lecture on when analysis goes wrong A look at false-positive psychology","tags":["Lecture","Reproducibility Knowledge"],"title":"False Positive Psychology","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1689122843,"objectID":"93972061a3ae8711f7cef964b2e7511f","permalink":"https://forrt.org/curated_resources/false-positive-citations/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/false-positive-citations/","section":"curated_resources","summary":"We describe why we wrote “False-Positive Psychology,” analyze how it has been cited, and explain why the integrity of experimental psychology hinges on the full disclosure of methods, the sharing of materials and data, and, especially, the preregistration of analyses.","tags":["Reproducibility Crisis and Credibility Revolution","Open Science"],"title":"False-Positive Citations","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1689122843,"objectID":"bb4aa906f4014d61196911fd0aea4035","permalink":"https://forrt.org/curated_resources/false-positive-psychology-undisclosed-fl/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/false-positive-psychology-undisclosed-fl/","section":"curated_resources","summary":"In this article, we accomplish two things. First, we show that despite empirical psychologists’ nominal endorsement of a low rate of false-positive findings (≤ .05), flexibility in data collection, analysis, and reporting dramatically increases actual false-positive rates. In many cases, a researcher is more likely to falsely find evidence that an effect exists than to correctly find evidence that it does not. We present computer simulations and a pair of actual experiments that demonstrate how unacceptably easy it is to accumulate (and report) statistically significant evidence for a false hypothesis. Second, we suggest a simple, low-cost, and straightforwardly effective disclosure-based solution to this problem. The solution involves six concrete requirements for authors and four guidelines for reviewers, all of which impose a minimal burden on the publication process.","tags":["Reproducibility Crisis and Credibility Revolution","Open Science"],"title":"False-Positive Psychology: Undisclosed Flexibility in Data Collection and Analysis Allows Presenting Anything as Significant","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1689122843,"objectID":"b92fa52c232349286a5320091a5695c7","permalink":"https://forrt.org/curated_resources/falsely-reassuring-analyses-of-all-p-val/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/falsely-reassuring-analyses-of-all-p-val/","section":"curated_resources","summary":"A blog post that describes analyses of all p-values","tags":["Blog"],"title":"Falsely Reassuring: Analyses of ALL p-values","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1689122843,"objectID":"86885c04d6aa72b3188dc73e667ebf8c","permalink":"https://forrt.org/curated_resources/falsifiability-is-not-optional/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/falsifiability-is-not-optional/","section":"curated_resources","summary":"Finkel, Eastwick, and Reis (2016; FER2016) argued the post-2011 methodological reform movement has focused narrowly on replicability, neglecting other essential goals of research. We agree multiple scientific goals are essential, but argue, however, a more fine-grained language, conceptualization, and approach to replication is needed to accomplish these goals. Replication is the general empirical mechanism for testing and falsifying theory. Sufficiently methodologically similar replications, also known as direct replications, test the basic existence of phenomena and ensure cumulative progress is possible a priori. In contrast, increasingly methodologically dissimilar replications, also known as conceptual replications, test the relevance of auxiliary hypotheses (e.g., manipulation and measurement issues, contextual factors) required to productively investigate validity and generalizability. Without prioritizing replicability, a field is not empirically falsifiable. We also disagree with FER2016’s position that “bigger samples are generally better, but . . . that very large samples could have the downside of commandeering resources that would have been better invested in other studies” (abstract). We identify problematic assumptions involved in FER2016’s modifications of our original research-economic model, and present an improved model that quantifies when (and whether) it is reasonable to worry that increasing statistical power will engender potential trade-offs. Sufficiently powering studies (i.e., \u003e80%) maximizes both research efficiency and confidence in the literature (research quality). Given that we are in agreement with FER2016 on all key open science points, we are eager to start seeing the accelerated rate of cumulative knowledge development of social psychological phenomena such a sufficiently transparent, powered, and falsifiable approach will generate.","tags":["Reproducibility Crisis and Credibility Revolution","Open Science"],"title":"Falsifiability Is Not Optional","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1689122843,"objectID":"3139576f5309462bb7d40ac2485256fd","permalink":"https://forrt.org/curated_resources/fearing-the-future-of-empirical-psycholo/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/fearing-the-future-of-empirical-psycholo/","section":"curated_resources","summary":"In this methodological commentary, we use Bem’s (2011) recent article reporting experimental evidence for psi as a case study for discussing important deficiencies in modal research practice in empirical psychology. We focus on (a) overemphasis on conceptual rather than close replication, (b) insufficient attention to verifying the soundness of measurement and experimental procedures, and (c) flawed implementation of null hypothesis significance testing. We argue that these deficiencies contribute to weak method-relevant beliefs that, in conjunction with overly strong theory-relevant beliefs, lead to a systemic and pernicious bias in the interpretation of data that favors a researcher’s theory. Ultimately, this interpretation bias increases the risk of drawing incorrect conclusions about human psychology. Our analysis points to concrete recommendations for improving research practice in empirical psychology. We recommend (a) a stronger emphasis on close replication, (b) routinely verifying the integrity of measurement instruments and experimental procedures, and (c) using stronger, more diagnostic forms of null hypothesis testing.","tags":["Reproducibility Crisis and Credibility Revolution","Open Science"],"title":"Fearing the future of empirical psychology: Bem’s (2011) evidence of psi as a case study in deficiencies in modal research practice. ","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"280a2d72cf3e45e3ae5745c5703e45bd","permalink":"https://forrt.org/curated_resources/feasibility-of-emulating-clinical-trials/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/feasibility-of-emulating-clinical-trials/","section":"curated_resources","summary":"The US Food and Drug Administration (FDA) has developed a framework to use data gathered outside of clinical trials (eg, electronic health record [EHR] and insurance claims data) for evaluations of medical product safety and effectiveness. These methods, typically characterized as target trial emulations, are expected to be most useful for evaluating new clinical indications for authorized drugs and postmarketing trial requirements. Previous studies found that few trials can be feasibly emulated using claims and/or structured EHR data. Accordingly, we examined the feasibility of using contemporary data gathered outside clinical trials to emulate the pivotal trials supporting supplemental new drug applications (sNDAs) and supplemental biologics license applications (sBLAs) approved by the FDA from 2017 to 2019.","tags":["Clinical Trials","Data","Drugs","Biologics"],"title":"Feasibility of Emulating Clinical Trials Supporting US FDA Supplemental Indication Approvals of Drugs and Biologics","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1689122843,"objectID":"1c81b361fcd4254708be42f15098863b","permalink":"https://forrt.org/curated_resources/feeling-the-future-experimental-evidence/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/feeling-the-future-experimental-evidence/","section":"curated_resources","summary":"The term psi denotes anomalous processes of information or energy transfer that are currently unexplained in terms of known physical or biological mechanisms. Two variants of psi are precognition (conscious cognitive awareness) and premonition (affective apprehension) of a future event that could not otherwise be anticipated through any known inferential process. Precognition and premonition are themselves special cases of a more general phenomenon: the anomalous retroactive influence of some future event on an individual's current responses, whether those responses are conscious or nonconscious, cognitive or affective. This article reports 9 experiments, involving more than 1,000 participants, that test for retroactive influence by \"time-reversing\" well-established psychological effects so that the individual's responses are obtained before the putatively causal stimulus events occur. Data are presented for 4 time-reversed effects: precognitive approach to erotic stimuli and precognitive avoidance of negative stimuli; retroactive priming; retroactive habituation; and retroactive facilitation of recall. The mean effect size (d) in psi performance across all 9 experiments was 0.22, and all but one of the experiments yielded statistically significant results. The individual-difference variable of stimulus seeking, a component of extraversion, was significantly correlated with psi performance in 5 of the experiments, with participants who scored above the midpoint on a scale of stimulus seeking achieving a mean effect size of 0.43. Skepticism about psi, issues of replication, and theories of psi are also discussed.","tags":["Reproducibility Crisis and Credibility Revolution","Open Science"],"title":"Feeling the Future: Experimental Evidence for Anomalous Retroactive Influences on Cognition and Affect","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1721390377,"objectID":"7c7700c3e4bb2400ab15d691c750c587","permalink":"https://forrt.org/glossary/english/feminist_psychology/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/english/feminist_psychology/","section":"glossary","summary":"","tags":null,"title":"Feminist psychology","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1635961284,"objectID":"0f30e4d665b6b700fbd3eda18fa383d3","permalink":"https://forrt.org/glossary/vbeta/feminist-psychology/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/vbeta/feminist-psychology/","section":"glossary","summary":"","tags":null,"title":"Feminist psychology","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1720865851,"objectID":"6c9b6a098b5c83ae2b0db8309bf65e9f","permalink":"https://forrt.org/glossary/german/feminist_psychology/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/german/feminist_psychology/","section":"glossary","summary":"","tags":null,"title":"Feminist psychology (Feministische Psychologie)","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"bdcaacba3a884ef60a700412391ef397","permalink":"https://forrt.org/curated_resources/fifty-years-of-research-on-questionable/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/fifty-years-of-research-on-questionable/","section":"curated_resources","summary":"Questionable research practises (QRPs) have been the focus of the scientific community amid greater scrutiny and evidence highlighting issues with replicability across many fields of science. To capture the most impactful publications and the main thematic domains in the literature on QRPs, this study uses a document co-citation analysis. The analysis was conducted on a sample of 341 documents that covered the past 50 years of research in QRPs. Nine major thematic clusters emerged. Statistical reporting and statistical power emerged as key areas of research, where systemic-level factors in how research is conducted are consistently raised as the precipitating factors for QRPs. There is also an encouraging shift in the focus of research into open science practises designed to address engagement in QRPs. Such a shift is indicative of the growing momentum of the open science movement, and more research can be conducted on how these practises are employed on the ground and how their uptake by researchers can be further promoted. However, the results suggest that, while pre-registration and registered reports receive the most research interest, less attention has been paid to other open science practises (e.g. data sharing).","tags":["Scientific Integrity","Ethics of Research","Questionable Research Practices"],"title":"Fifty years of research on questionable research practises in science: Quantitative analysis of co-citation patterns","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"3a5d3d5f17bd13afc1c99eee1203d8d6","permalink":"https://forrt.org/curated_resources/finding-evaluating-open-data/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/finding-evaluating-open-data/","section":"curated_resources","summary":"Introduction to finding and evaluating Open Data by NYU DataServices.","tags":["Data","Licenses","Open Data","Researchers"],"title":"Finding \u0026 Evaluating Open Data","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1720865851,"objectID":"af18cc8f4f1185b32be59a8fbeddfef4","permalink":"https://forrt.org/glossary/german/first_last_author_emphasis_norm/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/german/first_last_author_emphasis_norm/","section":"glossary","summary":"","tags":null,"title":"First-last-author-emphasis norm (FLAE, Norm der Betonung der Erst- und Letztautor:innenschaft)","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1721390377,"objectID":"2814bb344ab60f9de030b40f08d14ad8","permalink":"https://forrt.org/glossary/english/first_last_author_emphasis_norm/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/english/first_last_author_emphasis_norm/","section":"glossary","summary":"","tags":null,"title":"First-last-author-emphasis norm (FLAE)","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1635961284,"objectID":"24d7b54d3cf800b5162f60cd60830559","permalink":"https://forrt.org/glossary/vbeta/first-last-author-emphasis-norm-fla/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/vbeta/first-last-author-emphasis-norm-fla/","section":"glossary","summary":"","tags":null,"title":"First-last-author-emphasis norm (FLAE)","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1689122843,"objectID":"a9de81a7df3e42ca5f475072a919dd0d","permalink":"https://forrt.org/curated_resources/fishing-commitment-and-communication-a-p/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/fishing-commitment-and-communication-a-p/","section":"curated_resources","summary":"Social scientists generally enjoy substantial latitude in selecting measures and models for hypothesis testing. Coupled with publication and related biases, this latitude raises the concern that researchers may intentionally or unintentionally select models that yield positive findings, leading to an unreliable body of published research. To combat this “fishing” problem in medical studies, leading journals now require preregistration of designs that emphasize the prior identification of dependent and independent variables. However, we demonstrate here that even with this level of advanced specification, the scope for fishing is considerable when there is latitude over selection of covariates, subgroups, and other elements of an analysis plan. These concerns could be addressed through the use of a form of comprehensive registration. We experiment with such an approach in the context of an ongoing field experiment for which we drafted a complete “mock report” of findings using fake data on treatment assignment. We describe the advantages and disadvantages of this form of registration and propose that a comprehensive but nonbinding approach be adopted as a first step to combat fishing by social scientists. Likely effects of comprehensive but nonbinding registration are discussed, the principal advantage being communication rather than commitment, in particular that it generates a clear distinction between exploratory analyses and genuine tests.","tags":["Reproducibility Crisis and Credibility Revolution","Open Science"],"title":"Fishing, Commitment, and Communication: A Proposal for Comprehensive Nonbinding Research Registration","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"6a6dafdb235b730e7874645eb7de3681","permalink":"https://forrt.org/curated_resources/five-selfish-reasons-to-work-reproducibl/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/five-selfish-reasons-to-work-reproducibl/","section":"curated_resources","summary":"And so, my fellow scientists: ask not what you can do for reproducibility; ask what reproducibility can do for you! Here, I present five reasons why working reproducibly pays off in the long run and is in the self-interest of every ambitious, career-oriented scientist.A complex equation on the left half of a black board, an even more complex equation on the right half. A short sentence links the two equations: “Here a miracle occurs”. Two mathematicians in deep thought. “I think you should be more explicit in this step”, says one to the other.This is exactly how it seems when you try to figure out how authors got from a large and complex data set to a dense paper with lots of busy figures. Without access to the data and the analysis code, a miracle occurred. And there should be no miracles in science.Working transparently and reproducibly has a lot to do with empathy: put yourself into the shoes of one of your collaboration partners and ask yourself, would that person be able to access my data and make sense of my analyses. Learning the tools of the trade (Box 1) will require commitment and a massive investment of your time and energy. A priori it is not clear why the benefits of working reproducibly outweigh its costs.Here are some reasons: because reproducibility is the right thing to do! Because it is the foundation of science! Because the world would be a better place if everyone worked transparently and reproducibly! You know how that reasoning sounds to me? Just like yaddah, yaddah, yaddah …It’s not that I think these reasons are wrong. It’s just that I am not much of an idealist; I don’t care how science should be. I am a realist; I try to do my best given how science actually is. And, whether you like it or not, science is all about more publications, more impact factor, more money and more career. More, more, more… so how does working reproducibly help me achieve more as a scientist.","tags":["Organizational Change","Reproducibility","Researchers"],"title":"Five selfish reasons to work reproducibly","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1721390377,"objectID":"c96515e043ecc5ceacd9d6f24ea74ac9","permalink":"https://forrt.org/glossary/english/forrt/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/english/forrt/","section":"glossary","summary":"","tags":null,"title":"FORRT","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1720865851,"objectID":"0ddc04235f2f6fc64fe5df40987fe8ce","permalink":"https://forrt.org/glossary/german/forrt/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/german/forrt/","section":"glossary","summary":"","tags":null,"title":"FORRT","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1635961284,"objectID":"dd6b7fa4f633752e9dca1023edd96a67","permalink":"https://forrt.org/glossary/vbeta/forrt/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/vbeta/forrt/","section":"glossary","summary":"","tags":null,"title":"FORRT","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"44c9596f467d6a1b93b33abe31c65a0f","permalink":"https://forrt.org/curated_resources/foster-open-science/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/foster-open-science/","section":"curated_resources","summary":"The FOSTER portal is an e-learning platform that brings together the best training resources addressed to those who need to know more about Open Science, or need to develop strategies and skills for implementing Open Science practices in their daily workflows. Here you will find a growing collection of training materials. Many different users - from early-career researchers, to data managers, librarians, research administrators, and graduate schools - can benefit from the portal. In order to meet their needs, the existing materials will be extended from basic to more advanced-level resources. In addition, discipline-specific resources will be created.","tags":["Analysis","Data","Education","Materials","Policy","Publishing"],"title":"Foster Open Science","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"0d85eaee525de53744611f57dc0695f9","permalink":"https://forrt.org/curated_resources/four-simple-recommendations-to-encourage/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/four-simple-recommendations-to-encourage/","section":"curated_resources","summary":"Scientific research relies on computer software, yet software is not always developed following practices that ensure its quality and sustainability. This manuscript does not aim to propose new software development best practices, but rather to provide simple recommendations that encourage the adoption of existing best practices. Software development best practices promote better quality software, and better quality software improves the reproducibility and reusability of research. These recommendations are designed around Open Source values, and provide practical suggestions that contribute to making research software and its source code more discoverable, reusable and transparent. This manuscript is aimed at developers, but also at organisations, projects, journals and funders that can increase the quality and sustainability of research software by encouraging the adoption of these recommendations.","tags":["Reproducibility"],"title":"Four simple recommendations to encourage best practices in research software","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"b94b157bffedaab5f0affcd20956b959","permalink":"https://forrt.org/curated_resources/free-and-low-cost-resources-for-graduate/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/free-and-low-cost-resources-for-graduate/","section":"curated_resources","summary":"A list of free or cheap resources (e.g. free and open statistical software) that support the processes of science ","tags":["Open science"],"title":"Free and low cost resources for graduate students, postdocs, and early career researchers (or really anyone else)","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1721390377,"objectID":"ae5310501a443f5a7aeedf90fdb3a81c","permalink":"https://forrt.org/glossary/english/free_our_knowledge_platform/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/english/free_our_knowledge_platform/","section":"glossary","summary":"","tags":null,"title":"Free Our Knowledge Platform","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1720865851,"objectID":"42a1223e41e19632898710f917277e36","permalink":"https://forrt.org/glossary/german/free_our_knowledge_platform/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/german/free_our_knowledge_platform/","section":"glossary","summary":"","tags":null,"title":"Free Our Knowledge Platform","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1635961284,"objectID":"4562d17a50ec46174b80d9751a7913c0","permalink":"https://forrt.org/glossary/vbeta/free-our-knowledge-platform/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/vbeta/free-our-knowledge-platform/","section":"glossary","summary":"","tags":null,"title":"Free Our Knowledge Platform","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"d56ec4f0dbf4dd972a92358fcb3b91ff","permalink":"https://forrt.org/curated_resources/from-policy-to-practice-lessons-learned/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/from-policy-to-practice-lessons-learned/","section":"curated_resources","summary":"In the past few years, there has been a notable shift in the open science landscape as more countries and international agencies release recommendations and implementation guidelines for open scholarship. In August 2022, the US White House Office of Science and Technology (OSTP) released a memo with guidance that all federally funded research articles be (1) open access and (2) include sharing of underlying datasets in public repositories. The global open scholarship conversation has shifted from making a case for open science to developing operational workflows to assess, monitor, and enforce open policies that can normalize, simplify, and streamline these processes for use in daily research practice. As various workflows are proposed, there is a need for collective action across funders, institutions, and governments to align on open science policies and practices to reduce the cost and friction of adoption.\n\nHere, we examine the practices of the Aligning Science Across Parkinson’s (ASAP) initiative, whose mission is to accelerate the pace of discovery and inform the path to a cure for Parkinson’s disease through collaboration, research-enabling resources, and data sharing. ASAP was conceived through an open-by-design framework from the start. To learn more, please see the ASAP Blueprint for Collaborative Open Science, which provides a detailed overview of the ASAP open science policies, templates, and reports. Grantees within the ASAP Collaborative Research Network (CRN), an international, multidisciplinary, and multi-institutional network of collaborating investigators, are already required to be compliant with the recommendations of the OSTP memo by adhering to ASAP’s open science policies. For example, ASAP requires the posting of a preprint at the time of (or before) article submission, immediate open access for all publications, and a mandatory CC-BY license. Additionally, at the time of publication, all underlying research outputs (protocols, code, datasets) must be posted to a FAIR repository and all research outputs from ASAP-funded research must have DOIs or other appropriate identifiers, such as RRIDs for material resources, appropriately linked to the manuscript (see Table 1 for list of identifier types). Here, we evaluate the feasibility, ease, impact, and improvement to our open science policies as they were implemented within the ASAP CRN program and discuss our lessons learned to assist other funders and institutions considering open science implementation.","tags":["Computer Software","Open Science","Science Policy","Research Assessment","Software Tools","Parkinson Disease","Programming Languages","Reproducibility"],"title":"From policy to practice: Lessons learned from an open science funding initiative","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"01aed36c33ca95ec2fceb1c7c75d990d","permalink":"https://forrt.org/curated_resources/from-pre-registration-to-publication-a-n/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/from-pre-registration-to-publication-a-n/","section":"curated_resources","summary":"Meta-analysis synthesizes a body of research investigating a common research question. Outcomes from meta-analyses provide a more objective and transparent summary of a research area than traditional narrative reviews. Moreover, they are often used to support research grant applications, guide clinical practice, and direct health policy. The aim of this article is to provide a practical and non-technical guide for psychological scientists that outlines the steps involved in planning and performing a meta-analysis of correlational datasets. I provide a supplementary R script to demonstrate each analytical step described in the paper, which is readily adaptable for researchers to use for their analyses. While the worked example is the analysis of a correlational dataset, the general meta-analytic process described in this paper is applicable for all types of effect sizes. I also emphasize the importance of meta-analysis protocols and pre-registration to improve transparency and help avoid unintended duplication. An improved understanding this tool will not only help scientists to conduct their own meta-analyses but also improve their evaluation of published meta-analyses.","tags":["Meta-Analysis","Primer","Methods","Preregistration","Statistics","Publication Bias"],"title":"From pre-registration to publication: A non-technical primer for conducting a meta-analysis to synthesize correlational data","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"07d9bc4a2fea41192fa03e8ad01a0ecf","permalink":"https://forrt.org/curated_resources/from-private-incentives-to-public-health/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/from-private-incentives-to-public-health/","section":"curated_resources","summary":"Pandemic preparedness and response have relied primarily on market dynamics to drive development and availability of new health products. Building on calls for transformation, we propose a new value proposition that instead prioritises equity from the research and development (R\u0026D) stage and that strengthens capacity to control outbreaks when and where they occur. Key elements include regional R\u0026D hubs free to adapt well established technology platforms, and independent clinical trials networks working with researchers, regulators, and health authorities to better study questions of comparative benefit and real-world efficacy. Realising these changes requires a shift in emphasis: from pandemic response to outbreak control, from one-size-fits-all economies of scale to R\u0026D and manufacture for local need, from de novo product development to last-mile innovation through adaptation of existing technologies, and from proprietary, competitive R\u0026D to open science and financing for the common good that supports collective management and sharing of technology and know-how.","tags":["Research and Development","Pandemic Preparedness"],"title":"From private incentives to public health need: rethinking research and development for pandemic preparedness","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"bcbba12eab5333a02b3125994816db31","permalink":"https://forrt.org/curated_resources/funder-data-sharing-policies-overview-an/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/funder-data-sharing-policies-overview-an/","section":"curated_resources","summary":"This report covers funder data-sharing policies/practices, and provides recommendations to funders and others as they consider their own policies. It was commissioned by Robert Wood Johnson Foundation in 2017. If any comments or questions, please contact Stephanie Wykstra (stephanie.wykstra@gmail.com).","tags":["Data","Data-Sharing Policies","Funder Policies","Open Data","Open Science","Policy"],"title":"Funder Data-Sharing Policies: Overview and Recommendations","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1689122843,"objectID":"a04f1e84b1897e53712ce911447b0d41","permalink":"https://forrt.org/curated_resources/g-power/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/g-power/","section":"curated_resources","summary":"G*Power is a tool to compute statistical power analyses for many different t tests, F tests, χ2 tests, z tests and some exact tests. G*Power can also be used to compute effect sizes and to display graphically the results of power analyses","tags":["Reproducibility Knowledge","Power Analysis Tool"],"title":"G*Power","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1721390377,"objectID":"0583204bc71648a3c13f3babf5e08c27","permalink":"https://forrt.org/glossary/english/g_power/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/english/g_power/","section":"glossary","summary":"","tags":null,"title":"G*Power","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1720865851,"objectID":"daa3581c39b5e8fc4ea0c4d459ea68ae","permalink":"https://forrt.org/glossary/german/g_power/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/german/g_power/","section":"glossary","summary":"","tags":null,"title":"G*Power","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1635961284,"objectID":"e1e3f6a84855918be333242346114e82","permalink":"https://forrt.org/glossary/vbeta/g-power/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/vbeta/g-power/","section":"glossary","summary":"","tags":null,"title":"G*Power","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1721390377,"objectID":"434374a496921835609275aa6708dfc8","permalink":"https://forrt.org/glossary/english/gaming/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/english/gaming/","section":"glossary","summary":"","tags":null,"title":"Gaming (the system)","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1635961284,"objectID":"c46307b87bf44c13a1fe56d52cf110ad","permalink":"https://forrt.org/glossary/vbeta/gaming-the-system/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/vbeta/gaming-the-system/","section":"glossary","summary":"","tags":null,"title":"Gaming (the system)","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1720865851,"objectID":"071c8f003a0183478a3d34afa873ebf1","permalink":"https://forrt.org/glossary/german/gaming/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/german/gaming/","section":"glossary","summary":"","tags":null,"title":"Gaming (the system) (das System überlisten)","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1721390377,"objectID":"78d3478612332fdd9d9896aa42b17025","permalink":"https://forrt.org/glossary/english/garden_of_forking_paths/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/english/garden_of_forking_paths/","section":"glossary","summary":"","tags":null,"title":"Garden of forking paths","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1635961284,"objectID":"f4e440711a2714cb984d135e444cca95","permalink":"https://forrt.org/glossary/vbeta/garden-of-forking-paths/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/vbeta/garden-of-forking-paths/","section":"glossary","summary":"","tags":null,"title":"Garden of forking paths","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1720865851,"objectID":"b535a2c3491cbc46ee12a62956bd161f","permalink":"https://forrt.org/glossary/german/garden_of_forking_paths/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/german/garden_of_forking_paths/","section":"glossary","summary":"","tags":null,"title":"Garden of forking paths (Garten der Weggabelungen)","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1720865851,"objectID":"ef371b44a27b9d3fae7bce9f79150c8c","permalink":"https://forrt.org/glossary/german/general_data_protection_regulation/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/german/general_data_protection_regulation/","section":"glossary","summary":"","tags":null,"title":"General Data Protection Regulation (GDPR, dt. Datenschutzgrundverordnung DSGVO)","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1721390377,"objectID":"a6230dbdce9c052ccbe9f81db83f60d3","permalink":"https://forrt.org/glossary/english/general_data_protection_regulation/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/english/general_data_protection_regulation/","section":"glossary","summary":"","tags":null,"title":"General Data Protection Regulation (GDPR)","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1635961284,"objectID":"d6131e2f4c196062bca7f882b36fd248","permalink":"https://forrt.org/glossary/vbeta/general-data-protection-regulation-/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/vbeta/general-data-protection-regulation-/","section":"glossary","summary":"","tags":null,"title":"General Data Protection Regulation (GDPR)","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"0f536c918445ea12dfa6a75b6b8b8fff","permalink":"https://forrt.org/curated_resources/general-principles-of-preclinical-study/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/general-principles-of-preclinical-study/","section":"curated_resources","summary":"Preclinical studies using animals to study the potential of a therapeutic drug or strategy are important steps before translation to clinical trials. However, evidence has shown that poor quality in the design and conduct of these studies has not only impeded clinical translation but also led to significant waste of valuable research resources. It is clear that experimental biases are related to the poor quality seen with preclinical studies. In this chapter, we will focus on hypothesis testing type of preclinical studies and explain general concepts and principles in relation to the design of in vivo experiments, provide definitions of experimental biases and how to avoid them, and discuss major sources contributing to experimental biases and how to mitigate these sources. We will also explore the differences between confirmatory and exploratory studies, and discuss available guidelines on preclinical studies and how to use them. This chapter, together with relevant information in other chapters in the handbook, provides a powerful tool to enhance scientific rigour for preclinical studies without restricting creativity.","tags":["Experimental Bias","Hypothesis Generating","Hypothesis Testing","In Vivo Studies","Preclinical Research","Reproducibility"],"title":"General Principles of Preclinical Study Design","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1721390377,"objectID":"d7392d671b1b61eb1c415a1d830eabfb","permalink":"https://forrt.org/glossary/english/generalizability/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/english/generalizability/","section":"glossary","summary":"","tags":null,"title":"Generalizability","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1635961284,"objectID":"ebc883d4de1ecbf3bfadf4448ccff724","permalink":"https://forrt.org/glossary/vbeta/generalizability/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/vbeta/generalizability/","section":"glossary","summary":"","tags":null,"title":"Generalizability","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1720865851,"objectID":"69ffe659cc059c6424a5972efaeae893","permalink":"https://forrt.org/glossary/german/generalizability/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/german/generalizability/","section":"glossary","summary":"","tags":null,"title":"Generalizability (Generalisierbarkeit)","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"7b3685f0ece3febf2de8b93e972cc4af","permalink":"https://forrt.org/curated_resources/genomics-workshop-overview/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/genomics-workshop-overview/","section":"curated_resources","summary":"Workshop overview for the Data Carpentry genomics curriculum. Data Carpentry’s aim is to teach researchers basic concepts, skills, and tools for working with data so that they can get more done in less time, and with less pain. This workshop teaches data management and analysis for genomics research including: best practices for organization of bioinformatics projects and data, use of command-line utilities, use of command-line tools to analyze sequence quality and perform variant calling, and connecting to and using cloud computing. This workshop is designed to be taught over two full days of instruction. Please note that workshop materials for working with Genomics data in R are in “alpha” development. These lessons are available for review and for informal teaching experiences, but are not yet part of The Carpentries’ official lesson offerings. Interested in teaching these materials? We have an onboarding video and accompanying slides available to prepare Instructors to teach these lessons. After watching this video, please contact team@carpentries.org so that we can record your status as an onboarded Instructor. Instructors who have completed onboarding will be given priority status for teaching at centrally-organized Data Carpentry Genomics workshops.","tags":["Analysis","Data","Education","Genomics","Open Scholarship Tools and Technologies","Reproducibility","Research Data Management Tools","Researchers"],"title":"Genomics Workshop Overview","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"cbfa3861e78537479a84079a72d92e88","permalink":"https://forrt.org/curated_resources/geospatial-workshop-overview/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/geospatial-workshop-overview/","section":"curated_resources","summary":"Data Carpentry’s aim is to teach researchers basic concepts, skills, and tools for working with data so that they can get more done in less time, and with less pain. Interested in teaching these materials? We have an onboarding video available to prepare Instructors to teach these lessons. After watching this video, please contact team@carpentries.org so that we can record your status as an onboarded Instructor. Instructors who have completed onboarding will be given priority status for teaching at centrally-organized Data Carpentry Geospatial workshops.","tags":["Analysis","Data","Education","Geospatial","Open Scholarship Tools and Technologies","Reproducibility","Research Data Management Tools","Researchers"],"title":"Geospatial Workshop Overview","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"8f284e01813bde98ce0c9ac8a08236c2","permalink":"https://forrt.org/curated_resources/getting-involved-with-top-factor/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/getting-involved-with-top-factor/","section":"curated_resources","summary":"This webinar provides an overview of TOP Factor: its rationale, how it is being used, and how each of the TOP standards relate to individual scores. We also cover how to get involved with TOP Factor by inviting interested community members to suggest journals be added to the database and/or evaluate journal policies for submission.","tags":["Center for Open Science","Data Citation","Journal Impact Factor","Open Science","Preregistration","Replication","Reproducibility","Research","Research Analysis","Research Best Practices","Research Design","Scientific Funding","Scientific Publishing","TOP Factor","TOP Guidelines"],"title":"Getting Involved with TOP Factor","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"54f7788357c886a48479f4440ce3797c","permalink":"https://forrt.org/curated_resources/ggplot-colors-best-tricks-you-will-love/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/ggplot-colors-best-tricks-you-will-love/","section":"curated_resources","summary":"This article presents multiple great solutions you should know for changing ggplot colors.","tags":["Data visualisation"],"title":"GGPlot Colors Best Tricks You Will Love","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1721390377,"objectID":"7877c4fbd2f67a47144d7031b1804d8b","permalink":"https://forrt.org/glossary/english/gift/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/english/gift/","section":"glossary","summary":"","tags":null,"title":"Gift (or Guest) Authorship","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1635961284,"objectID":"54bfc2b64f9e3fda53adef5ddb91f7e2","permalink":"https://forrt.org/glossary/vbeta/gift-or-guest-authorship/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/vbeta/gift-or-guest-authorship/","section":"glossary","summary":"","tags":null,"title":"Gift (or Guest) Authorship","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1720865851,"objectID":"eade27a26573e53ffb68f2a4d6c6ad8b","permalink":"https://forrt.org/glossary/german/gift/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/german/gift/","section":"glossary","summary":"","tags":null,"title":"Gift (or Guest) Authorship (geschenkte Autor:innenschaft / Gastautor:innenschaft)","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1721390377,"objectID":"2657e468bbcd42fd8dfc94a2b9e14d54","permalink":"https://forrt.org/glossary/english/git/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/english/git/","section":"glossary","summary":"","tags":null,"title":"Git","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1720865851,"objectID":"c6f727e83d7b98d0c8925df199834664","permalink":"https://forrt.org/glossary/german/git/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/german/git/","section":"glossary","summary":"","tags":null,"title":"Git","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1635961284,"objectID":"850501765c1f05266e90bd662e664a85","permalink":"https://forrt.org/glossary/vbeta/git/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/vbeta/git/","section":"glossary","summary":"","tags":null,"title":"Git","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1689122843,"objectID":"ec88177100b19e77c7153f86d041047c","permalink":"https://forrt.org/curated_resources/giving-community-psychology-away-a-case/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/giving-community-psychology-away-a-case/","section":"curated_resources","summary":"Amidst increased pressure for transparency in science, researchers and community members are calling for open access to study stimuli and measures, data, and results. These arguments coincidentally align with calls within community psychology to find innovative ways to support communities and increase the prominence of our field. This paper aims to (1) define the current context for community psychologists in open access publishing, (2) illustrate the alignment between open access publishing and community psychology principles, and (3) demonstrate how to engage in open access publishing using community psychology values. Currently, there are several facilitators (e.g. an increasing number of open access journals, the proliferation of blogs, and social media) and barriers (e.g. Article Processing Charges (APCs), predatory journals) to publishing in open access venues. Openly sharing our research findings aligns with our values of (1) citizen participation, (2) social justice, and (3) collaboration and community strengths. Community psychologists desiring to engage in open access publishing can ask journals to waive APCs, publish pre-prints, use blogs and social media to share results, and push for systemic change in a publishing system that disenfranchises researchers, students, and community members.","tags":[""],"title":"Giving Community Psychology Away: A case for open access publishing","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1689122843,"objectID":"a1eb32f3ab7a723c72501dddba477839","permalink":"https://forrt.org/curated_resources/glimmpse/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/glimmpse/","section":"curated_resources","summary":"Welcome to GLIMMPSE. The GLIMMPSE software calculates power and sample size for study designs with normally distributed outcomes. Select one of the options below to begin a power or sample size calculation.","tags":["Reproducibility Knowledge","Power Analysis Tool"],"title":"GLIMMPSE","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"a436fffdffaeeefa578fe08008022d6c","permalink":"https://forrt.org/curated_resources/good-clinical-practice-improves-rigor-an/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/good-clinical-practice-improves-rigor-an/","section":"curated_resources","summary":"Clinical trials are governed by principles of good clinical practice (GCP), which can strengthen the achievement of rigor, reproducibility, and transparency in scientific research. Rigor, reproducibility, and transparency are key for producing findings with greater certainty. Clinical trials are closely supervised, often by a clinical trial coordinating center, data safety and monitoring board, and a funding agency, with policies that are a manifestation of GCP and support rigor, reproducibility, and transparency. The multisite Advanced Cognitive Training for Independent and Vital Elderly (ACTIVE) study is an example clinical trial of relevance to a psychology and aging audience that utilized many protocols that can be applied to single-laboratory designs, including a manualized protocol with accompanying scientific rationale, predefined analysis plans, standardization of procedures across field sites, assurance of competence of study staff in study procedures, transparent coding/entry/transmittal of data, regular quality assurance, and open publication of data. Despite substantial resource discrepancies between the two, single-laboratory studies can model the GCP principles utilized in large clinical trials to provide an excellent foundation for rigor, reproducibility, and transparency.","tags":["Rigor","Transparency","Clinical Trials"],"title":"Good clinical practice improves rigor and transparency: Lessons from the ACTIVE trial","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"fc709920da9ee2e46a91e46eb1932b38","permalink":"https://forrt.org/curated_resources/good-enough-practices-in-scientific-comp/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/good-enough-practices-in-scientific-comp/","section":"curated_resources","summary":"Computers are now essential in all branches of science, but most researchers are never taught the equivalent of basic lab skills for research computing. As a result, data can get lost, analyses can take much longer than necessary, and researchers are limited in how effectively they can work with software and data. Computing workflows need to follow the same practices as lab projects and notebooks, with organized data, documented steps, and the project structured for reproducibility, but researchers new to computing often don't know where to start. This paper presents a set of good computing practices that every researcher can adopt, regardless of their current level of computational skill. These practices, which encompass data management, programming, collaborating with colleagues, organizing projects, tracking work, and writing manuscripts, are drawn from a wide variety of published sources from our daily lives and from our work with volunteer organizations that have delivered workshops to over 11,000 people since 2010.","tags":["Computer Software","Control Systems","Data Management","Data Processing","Programming Languages","Reproducibility","Software Tools","Source Code"],"title":"Good enough practices in scientific computing","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1689122843,"objectID":"932429e25a09d4b61d9ba753b8cb3064","permalink":"https://forrt.org/curated_resources/good-science-bad-science/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/good-science-bad-science/","section":"curated_resources","summary":"A syllabi about open science: good science and bad science.","tags":["Syllabus"],"title":"Good Science, Bad Science","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1721390377,"objectID":"53585ba545ed2ef23dce9d24db6bb08d","permalink":"https://forrt.org/glossary/english/goodhart_s_law/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/english/goodhart_s_law/","section":"glossary","summary":"","tags":null,"title":"Goodhart’s Law","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1635961284,"objectID":"2bc589d6f3952b0e80ab45d7a6c4ceae","permalink":"https://forrt.org/glossary/vbeta/goodhart-s-law/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/vbeta/goodhart-s-law/","section":"glossary","summary":"","tags":null,"title":"Goodhart’s Law","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1720865851,"objectID":"e02f16eecbbe880c8e53794c2b0cffba","permalink":"https://forrt.org/glossary/german/goodhart_s_law/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/german/goodhart_s_law/","section":"glossary","summary":"","tags":null,"title":"Goodhart’s Law (Goodharts Gesetz)","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"b7fdd714647ceb215d231269c4994068","permalink":"https://forrt.org/curated_resources/graduate-research-methods/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/graduate-research-methods/","section":"curated_resources","summary":"Completion of this course will provide a foundation for the practice of science. We will wrestle with the fundamental issues for designing and executing a program of research, and in the interpretation and reporting of research results. The class is organized around the development and execution of a single, actual research project from conception through completion. Class hours are devoted to conceptual issues in research design, execution and interpretation. Lab hours are devoted to presentation and critique of research plans.","tags":["Transparency","Reproducibility Crisis and Credibility Revolution","Open Science"],"title":"Graduate Research Methods","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1689122843,"objectID":"7d83ea3f130bda7ab27ca177ea5011ef","permalink":"https://forrt.org/curated_resources/graphical-causal-models/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/graphical-causal-models/","section":"curated_resources","summary":"This chapter discusses the use of directed acyclic graphs (DAGs) for causal inference in the observational social sciences. It focuses on DAGs’ main uses, discusses central principles, and gives applied examples. DAGs are visual representations of qualitative causal assumptions: They encode researchers’ beliefs about how the world works. Straightforward rules map these causal assumptions onto the associations and independencies in observable data. The two primary uses of DAGs are (1) determining the identifiability of causal effects from observed data and (2) deriving the testable implications of a causal model. Concepts covered in this chapter include identification, d-separation, confounding, endogenous selection, and overcontrol. Illustrative applications then demonstrate that conditioning on variables at any stage in a causal process can induce as well as remove bias, that confounding is a fundamentally causal rather than an associational concept, that conventional approaches to causal mediation analysis are often biased, and that causal inference in social networks inherently faces endogenous selection bias. The chapter discusses several graphical criteria for the identification of causal effects of single, time-point treatments (including the famous backdoor criterion), as well identification criteria for multiple, time-varying treatments.","tags":[""],"title":"Graphical Causal Models","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1689122843,"objectID":"de7244e22add6a32c7229f34d0b3ea3b","permalink":"https://forrt.org/curated_resources/guide-your-students-to-become-better-res/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/guide-your-students-to-become-better-res/","section":"curated_resources","summary":"Blog post going over making undergraduate students better consumers of research","tags":["Blog","Open Science"],"title":"Guide Your Students to Become Better Research Consumers","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"e69a93d769570158760b49cd6f40e043","permalink":"https://forrt.org/curated_resources/guidelines-for-evaluating-the-comparabil/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/guidelines-for-evaluating-the-comparabil/","section":"curated_resources","summary":"Proprietary genetic datasets are valuable for boosting the statistical power of genome-wide association studies (GWASs), but their use can restrict investigators from publicly sharing the resulting summary statistics. Although researchers can resort to sharing down-sampled versions that exclude restricted data, down-sampling reduces power and might change the genetic etiology of the phenotype being studied. These problems are further complicated when using multivariate GWAS methods, such as genomic structural equation modeling (Genomic SEM), that model genetic correlations across multiple traits. Here, we propose a systematic approach to assess the comparability of GWAS summary statistics that include versus exclude restricted data. Illustrating this approach with a multivariate GWAS of an externalizing factor, we assessed the impact of down-sampling on (1) the strength of the genetic signal in univariate GWASs, (2) the factor loadings and model fit in multivariate Genomic SEM, (3) the strength of the genetic signal at the factor level, (4) insights from gene-property analyses, (5) the pattern of genetic correlations with other traits, and (6) polygenic score analyses in independent samples. For the externalizing GWAS, although down-sampling resulted in a loss of genetic signal and fewer genome-wide significant loci; the factor loadings and model fit, gene-property analyses, genetic correlations, and polygenic score analyses were found robust. Given the importance of data sharing for the advancement of open science, we recommend that investigators who generate and share down-sampled summary statistics report these analyses as accompanying documentation to support other researchers’ use of the summary statistics.","tags":["Genomic SEM","Summary Statistics","Data Removal","Down-sample","Leave-one-out","Meta-analysis","Genomics","Genome-wide Association Study"],"title":"Guidelines for Evaluating the Comparability of Down-Sampled GWAS Summary Statistics","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1721390377,"objectID":"1f376ee7bc1398622b6c6f4c0881a77c","permalink":"https://forrt.org/glossary/english/h_index/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/english/h_index/","section":"glossary","summary":"","tags":null,"title":"H-index","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1635961284,"objectID":"9ee3863a365aec8ca823925f2a8f4dba","permalink":"https://forrt.org/glossary/vbeta/h-index/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/vbeta/h-index/","section":"glossary","summary":"","tags":null,"title":"H-index","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1720865851,"objectID":"cf71341be53efb154ad6a3d0a6d64ad4","permalink":"https://forrt.org/glossary/german/h_index/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/german/h_index/","section":"glossary","summary":"","tags":null,"title":"H-index (H-Index, Hirsch Index)","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"dd139753ef5c2e1ba1e22287dbf1edbb","permalink":"https://forrt.org/curated_resources/habits-and-perceptions-regarding-open-sc/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/habits-and-perceptions-regarding-open-sc/","section":"curated_resources","summary":"The article describes the results of the online survey on open science (OS) carried out on researchers affiliated with universities and Spanish research centres and focused on open access to scientific publications, the publication process, the management of research data and the review of open articles. The main objective was to identify the perception and habits of researchers with regard to practices closely linked to open science and the scientific value added is that offers an in-depth picture of researchers as one of the main actors to whom this transformation and implementation of open science will fall. It focuses on the different aspects of OS: open access, open data, publication process and open review in order to identify habits and perceptions. This is to make possible an implementation of the OS movement. The survey was carried out among researchers who had published in the years 2020–2021, according to data obtained from WoS. It was emailed to a total of 8,188 researchers and obtained a total of 666 responses, of which 554 were complete, the rest being forms with some questions unanswered. The main results showed that open access still requires the diffusion of practices and services provided by the institution, as well as training (library or equivalent service) and institutional support from the competent authorities (vice rectors or equivalent) in specific aspects such as data management. In the case of data, around 50% of respondents stated they had stored data in a repository, and of all the options, the most frequently given was that of an institutional repository, followed by a discipline repository. Among the main reasons for doing this, we found transparency, visibility of data and the ability to validate results. For those who stated they had never stored data, the most frequent reasons for not having done so were privacy and confidentiality, the lack of a mandated data policy or a lack of knowledge of how to do it. In terms of open peer review, participants mentioned a certain reticence to the opening of evaluations due to potential conflicts of interest that may arise or because lower-quality content might be accepted in order to avoid conflicts. In addition, the hierarchical structure of senior researcher versus junior researcher might affect reviews. The main conclusions indicate a need for persuasion of OA to take place; APCs are an economic barrier rather than the main criterion for journal selection; OPR practices may seem innovative and emerging; scientific and evaluation policies seem to have a clear effect on the behaviour of researchers; researchers state that they share research data more for reasons of persuasion than out of obligation. Researchers do question the pathways or difficulties that may arise on a day-to-day basis and seem aware that we are undergoing change, where academic evaluation or policies related to open science, its implementation and habits among researchers may change. In this sense, more and better support is needed on the part of institutions and faculty support services.","tags":["Open Science","Science Policy","Open Access Publishing","Research Funding","Data Management","Open Peer Review","Open Data"],"title":"Habits and perceptions regarding open science by researchers from Spanish institutions","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1689122843,"objectID":"e7989f6130a23c924251837f8bd9f3c1","permalink":"https://forrt.org/curated_resources/hack-your-way-to-scientific-glory/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/hack-your-way-to-scientific-glory/","section":"curated_resources","summary":"You’re a social scientist with a hunch: The U.S. economy is affected by whether Republicans or Democrats are in office. Try to show that a connection exists, using real data going back to 1948. For your results to be publishable in an academic journal, you’ll need to prove that they are “statistically significant” by achieving a low enough p-value.","tags":["Demo","Reproducibility Knowledge"],"title":"Hack Your Way To Scientific Glory","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1721390377,"objectID":"67dfbc1f02e208cbec8c2858cb4295e1","permalink":"https://forrt.org/glossary/english/hackathon/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/english/hackathon/","section":"glossary","summary":"","tags":null,"title":"Hackathon","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1720865851,"objectID":"5bdb8f72238e6d6643b64a34f353d1f8","permalink":"https://forrt.org/glossary/german/hackathon/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/german/hackathon/","section":"glossary","summary":"","tags":null,"title":"Hackathon","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1635961284,"objectID":"cbc6016f30c661d8159b619f705576bc","permalink":"https://forrt.org/glossary/vbeta/hackathon/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/vbeta/hackathon/","section":"glossary","summary":"","tags":null,"title":"Hackathon","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"f37aa537adf121a0cb5fa3ca5cb07b0f","permalink":"https://forrt.org/curated_resources/hackathon-encouraging-open-science-pract/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/hackathon-encouraging-open-science-pract/","section":"curated_resources","summary":"This list of resources consists of resources for researchers, editors, and reviewers interested in practicing open science principles, particularly in education research. This list is not exhaustive but meant as a starting point for individuals wanting to learn more about doing open science work specifically for qualitative research. For more general information about open science research, please visit https://www.cos.io/.","tags":["Open Science","Qualitative Research","Research"],"title":"Hackathon: Encouraging Open Science Practices in Qualitative Education Research","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1689122843,"objectID":"ed7eb7a97b6f492ffb470b84c90d71a8","permalink":"https://forrt.org/curated_resources/hail-the-impossible-p-values-evidence-an/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/hail-the-impossible-p-values-evidence-an/","section":"curated_resources","summary":"Significance testing based on p-values is standard in psychological research and teaching. Typically, research articles and textbooks present and use p as a measure of statistical evidence against the null hypothesis (the Fisherian interpretation), although using concepts and tools based on a completely different usage of p as a tool for controlling long-term decision errors (the Neyman–Pearson interpretation). There are four major problems with using p as a measure of evidence and these problems are often overlooked in the domain of psychology. First, p is uniformly distributed under the null hypothesis and can therefore never indicate evidence for the null. Second, p is conditioned solely on the null hypothesis and is therefore unsuited to quantify evidence, because evidence is always relative in the sense of being evidence for or against a hypothesis relative to another hypothesis. Third, p designates probability of obtaining evidence (given the null), rather than strength of evidence. Fourth, p depends on unobserved data and subjective intentions and therefore implies, given the evidential interpretation, that the evidential strength of observed data depends on things that did not happen and subjective intentions. In sum, using p in the Fisherian sense as a measure of statistical evidence is deeply problematic, both statistically and conceptually, while the Neyman–Pearson interpretation is not about evidence at all. In contrast, the likelihood ratio escapes the above problems and is recommended as a tool for psychologists to represent the statistical evidence conveyed by obtained data relative to two hypotheses","tags":[""],"title":"Hail the impossible: p-values, evidence, and likelihood. ","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"0ea2dc09f3356be175d4113f46ab111e","permalink":"https://forrt.org/curated_resources/hark-no-more-on-the-preregistration-of-c/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/hark-no-more-on-the-preregistration-of-c/","section":"curated_resources","summary":"Experimental preregistration is required for publication in many scientific disciplines and venues. When experimental intentions are preregistered, reviewers and readers can be confident that experimental evidence in support of reported hypotheses is not the result of HARKing, which stands for Hypothesising After the Results are Known. We review the motivation and outcomes of experimental preregistration across a variety of disciplines, as well as previous work commenting on the role of evaluation in HCI research. We then discuss how experimental preregistration could be adapted to the distinctive characteristics of Human-Computer Interaction empirical research, to the betterment of the discipline.","tags":["Human-Centered Computing","Human Computer Interaction","Preregistration"],"title":"HARK No More: On the Preregistration of CHI Experiments","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1721390377,"objectID":"1d8d6e5b7eb78dd5ab234b3769eaf282","permalink":"https://forrt.org/glossary/english/harking/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/english/harking/","section":"glossary","summary":"","tags":null,"title":"HARKing","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1720865851,"objectID":"59745278679db9fa99a4a8c6d75c914a","permalink":"https://forrt.org/glossary/german/harking/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/german/harking/","section":"glossary","summary":"","tags":null,"title":"HARKing","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1635961284,"objectID":"375e64e9e3e2f53c5a61be8abdc83e69","permalink":"https://forrt.org/glossary/vbeta/harking/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/vbeta/harking/","section":"glossary","summary":"","tags":null,"title":"HARKing","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1689122843,"objectID":"fc51f4f2d12c0ecc8c8b0468af6c202d","permalink":"https://forrt.org/curated_resources/harking-how-badly-can-cherry-picking-and/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/harking-how-badly-can-cherry-picking-and/","section":"curated_resources","summary":"The practice of hypothesizing after results are known (HARKing) has been identified as a potential threat to the credibility of research results. We conducted simulations using input values based on comprehensive meta-analyses and reviews in applied psychology and management (e.g., strategic management studies) to determine the extent to which two forms of HARKing behaviors might plausibly bias study outcomes and to examine the determinants of the size of this effect. When HARKing involves cherry-picking, which consists of searching through data involving alternative measures or samples to find the results that offer the strongest possible support for a particular hypothesis or research question, HARKing has only a small effect on estimates of the population effect size. When HARKing involves question trolling, which consists of searching through data involving several different constructs, measures of those constructs, interventions, or relationships to find seemingly notable results worth writing about, HARKing produces substantial upward bias particularly when it is prevalent and there are many effects from which to choose. Results identify the precise circumstances under which different forms of HARKing behaviors are more or less likely to have a substantial impact on a study’s substantive conclusions and the field’s cumulative knowledge. We offer suggestions for authors, consumers of research, and reviewers and editors on how to understand, minimize, detect, and deter detrimental forms of HARKing in future research.","tags":["Reproducibility Crisis and Credibility Revolution","Open Science"],"title":"HARKing: How Badly Can Cherry-Picking and Question Trolling Produce Bias in Published Results?","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1689122843,"objectID":"3496d420f06b230dddc6c3c5fbfc0959","permalink":"https://forrt.org/curated_resources/harking-hypothesizing-after-the-results/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/harking-hypothesizing-after-the-results/","section":"curated_resources","summary":"This article considers a practice in scientific communication termed HARKing (Hypothesizing After the Results are Known). HARKing is defined as presenting a post hoc hypothesis (i.e., one based on or informed by one's results) in one's research report as i f it were, in fact, an a priori hypotheses. Several forms of HARKing are identified and survey data are presented that suggests that at least some forms of HARKing are widely practiced and widely seen as inappropriate. I identify several reasons why scientists might HARK. Then I discuss several reasons why scientists ought not to HARK. It is conceded that the question of whether HARKing ' s costs exceed its benefits is a complex one that ought to be addressed through research, open discussion, and debate. To help stimulate such discussion (and for those such as myself who suspect that HARKing's costs do exceed its benefits), I conclude the article with some suggestions for deterring HARKing.","tags":[""],"title":"HARKing: Hypothesizing after the results are known","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"16255f1c22402859eea5ca16aa70a639","permalink":"https://forrt.org/curated_resources/harry-potter-and-the-methods-of-reproduc/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/harry-potter-and-the-methods-of-reproduc/","section":"curated_resources","summary":"\"Harry Potter and the Methods of Reproducibility -- A brief Introduction to Open Science\" gives a brief overview of Open Science, particularly reproducibility, for newcomers to the topic. It introduces the concept of questionable research practices (QRPs) and Open Science solutions to these QRPs, such as preregistrations, registered reports, Open Data, Open Code, and Open Materials.","tags":["Reproducibility","Researchers","Students"],"title":"Harry Potter and the Methods of Reproducibility","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1721390377,"objectID":"237957101109ac47dfd71977a3e6288a","permalink":"https://forrt.org/glossary/english/hidden_moderators/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/english/hidden_moderators/","section":"glossary","summary":"","tags":null,"title":"Hidden Moderators","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1635961284,"objectID":"3e537743cc47c2c9d80eea60ce4387bd","permalink":"https://forrt.org/glossary/vbeta/hidden-moderators/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/vbeta/hidden-moderators/","section":"glossary","summary":"","tags":null,"title":"Hidden Moderators ","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1720865851,"objectID":"1a0c9dd7f5450f69f93daca739a9d0a1","permalink":"https://forrt.org/glossary/german/hidden_moderators/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/german/hidden_moderators/","section":"glossary","summary":"","tags":null,"title":"Hidden Moderators (Versteckte Moderatoren)","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"9fb4ed23fb17728643affdeb33dbda6a","permalink":"https://forrt.org/curated_resources/history-and-methods-of-psychology-syllab/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/history-and-methods-of-psychology-syllab/","section":"curated_resources","summary":"This course is designed to expose students to the history of psychology through a study of the methods used in research over time, with an emphasis on methods used in developmental psychology. Unlike most traditional history of psychology courses, we will focus less on the emergence and differentiation of different schools of psychological thought, and more about the emergence and differentiation of different methods of empirical inquiry. Unlike traditional methods courses, we will focus less on specific research designs and analytic techniques, and more on broader issues of inference that permeate all psychological research (i.e., meta-psychology). Importantly, the historical focus of the course will be grounded in contemporary methodological issues, both to illustrate how many of the current issues have persisted for decades, but also to highlight the tremendous advances in potential solutions we have seen in recent years. Through this course, students will develop a basic familiarity with a core set of issues in the history of psychology and will be competent in rudimentary meta-psychology. These skills are intended to greatly enhance the research acumen of the students, both as rigorous producers of new research and informed consumers of existing work.","tags":["Reproducibility Crisis and Credibility Revolution","Open Science"],"title":"History and Methods of Psychology Syllabus","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"983acba86a19facd043b765cacaf214f","permalink":"https://forrt.org/curated_resources/history-and-philosophy-of-psychology/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/history-and-philosophy-of-psychology/","section":"curated_resources","summary":"In this course, we will examine the on-going methodological controversies around psychology, cognitive science, and cognitive neuroscience. We will look at the question of replication, statistical reform, measurement of psychological attributes, incentivesfor a successful science, etc. We will read articles and book chapters by scientists and statisticians in addition to some relevant articles by philosophers of science. There is no prerequisites for this course.","tags":["Philosophy","History of Psychology"],"title":"History and Philosophy of Psychology","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"310c57318aec154159e2c5857d7f4156","permalink":"https://forrt.org/curated_resources/history-repeating-guidelines-to-address/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/history-repeating-guidelines-to-address/","section":"curated_resources","summary":"Research in the last decade has expressed considerable optimism about the clinical potential of psychedelics for the treatment of mental disorders. This optimism is reflected in an increase in research papers, investments by pharmaceutical companies, patents, media coverage, as well as political and legislative changes. However, psychedelic science is facing serious challenges that threaten the validity of core findings and raise doubt regarding clinical efficacy and safety. In this paper, we introduce the 10 most pressing challenges, grouped into easy, moderate, and hard problems. We show how these problems threaten internal validity (treatment effects are due to factors unrelated to the treatment), external validity (lack of generalizability), construct validity (unclear working mechanism), or statistical conclusion validity (conclusions do not follow from the data and methods). These problems tend to co-occur in psychedelic studies, limiting conclusions that can be drawn about the safety and efficacy of psychedelic therapy. We provide a roadmap for tackling these challenges and share a checklist that researchers, journalists, funders, policymakers, and other stakeholders can use to assess the quality of psychedelic science. Addressing today’s problems is necessary to find out whether the optimism regarding the therapeutic potential of psychedelics has been warranted and to avoid history repeating itself.","tags":["Open Science","Psychedelics","Psychotherapy","Questionable Research Practices","Validity"],"title":"History repeating: Guidelines to address common problems in psychedelic science","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1689122843,"objectID":"422d343a19e820c7dfabb6f2a29ac2e6","permalink":"https://forrt.org/curated_resources/how-and-whether-to-teach-undergraduates/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/how-and-whether-to-teach-undergraduates/","section":"curated_resources","summary":"Over the past 10 years, crises surrounding replication, fraud, and best practices in research methods have dominated discussions in the field of psychology. However, no research exists examining how to communicate these issues to undergraduates and what effect this has on their attitudes toward the field. We developed and validated a 1-hr lecture communicating issues surrounding the replication crisis and current recommendations to increase reproducibility. Pre- and post-lecture surveys suggest that the lecture serves as an excellent pedagogical tool. Following the lecture, students trusted psychological studies slightly less but saw greater similarities between psychology and natural science fields. We discuss challenges for instructors taking the initiative to communicate these issues to undergraduates in an evenhanded way.","tags":["Reproducibility Crisis and Credibility Revolution"],"title":"How (and Whether) to Teach Undergraduates About the Replication Crisis in Psychological Science","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"c37e18a404a7b6e29167a50ecb1e11b9","permalink":"https://forrt.org/curated_resources/how-can-preregistration-contribute-to-re/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/how-can-preregistration-contribute-to-re/","section":"curated_resources","summary":"Comprehensive Results in Social Psychology (CRSP) is a novel journal for preregistered research (so-called registered reports, RR) in the field of social psychology. It offers RR-only publications, with the possibility of adding exploratory analysis and data as well. After submission of introduction, hypotheses, methods, procedure, and analysis plan, submitted manuscripts are reviewed prior to data collection. If the peer review process results in a positive evaluation of the manuscript, an initial publication agreement (IPA) is issued upon which publication of the manuscript (given adherence to the registered protocol) independent of the obtained results is possible. CRSP seeks to complement the publication options in our field by making transparent confirmatory and exploratory research possible.","tags":["Preregistration","Registered Reports","Social Psychology"],"title":"How can preregistration contribute to research in our field?","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"63dfe5f7facef7fe9f415d72a36c8d5d","permalink":"https://forrt.org/curated_resources/how-racist-policing-took-over-american-c/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/how-racist-policing-took-over-american-c/","section":"curated_resources","summary":"“The problem is the way policing was built,” historian Khalil Muhammad says.","tags":["Diversity","Equity","Inclusion"],"title":"How racist policing took over American cities, explained by a historian","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1689122843,"objectID":"f3b6f36ef0a326e2c765cf205d704b50","permalink":"https://forrt.org/curated_resources/how-scientists-can-stop-fooling-themselv/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/how-scientists-can-stop-fooling-themselv/","section":"curated_resources","summary":"Sampling simulated data can reveal common ways in which our cognitive biases mislead us.","tags":["Reproducibility Crisis and Credibility Revolution","Open Science"],"title":"How scientists can stop fooling themselves","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"40322b6de6f8b634b19aa3751e3e83f7","permalink":"https://forrt.org/curated_resources/how-significant-are-the-public-dimension/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/how-significant-are-the-public-dimension/","section":"curated_resources","summary":"Much of the work done by faculty at both public and private universities has significant public dimensions: it is often paid for by public funds; it is often aimed at serving the public good; and it is often subject to public evaluation. To understand how the public dimensions of faculty work are valued, we analyzed review, promotion, and tenure documents from a representative sample of 129 universities in the US and Canada. Terms and concepts related to public and community are mentioned in a large portion of documents, but mostly in ways that relate to service, which is an undervalued aspect of academic careers. Moreover, the documents make significant mention of traditional research outputs and citation-based metrics: however, such outputs and metrics reward faculty work targeted to academics, and often disregard the public dimensions. Institutions that seek to embody their public mission could therefore work towards changing how faculty work is assessed and incentivized.","tags":["Academic Careers","Higher Education","Institutional Policy","Metrics","Open Access","Publishing","Scholarly Communications"],"title":"How significant are the public dimensions of faculty work in review, promotion and tenure documents?","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"f82f50e46ead9907202af408ffa67dba","permalink":"https://forrt.org/curated_resources/how-to-classify-detect-and-manage-univar/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/how-to-classify-detect-and-manage-univar/","section":"curated_resources","summary":"Researchers often lack knowledge about how to deal with outliers when analyzing their data. Even more frequently, researchers do not pre-specify how they plan to manage outliers. In this paper we aim to improve research practices by outlining what you need to know about outliers. We start by providing a functional definition of outliers. We then lay down an appropriate nomenclature/classification of outliers. This nomenclature is used to understand what kinds of outliers can be encountered and serves as a guideline to make appropriate decisions regarding the conservation, deletion, or recoding of outliers. These decisions might impact the validity of statistical inferences as well as the reproducibility of our experiments. To be able to make informed decisions about outliers you first need proper detection tools. We remind readers why the most common outlier detection methods are problematic and recommend the use of the median absolute deviation to detect univariate outliers, and of the Mahalanobis-MCD distance to detect multivariate outliers. An R package was created that can be used to easily perform these detection tests. Finally, we promote the use of pre-registration to avoid flexibility in data analysis when handling outliers.","tags":["Psychology","Transparency","Outliers","Preregistration","Robust Detection","Malahanobis Distance","Median Absolute Deviation","Minimum Covariance Determinant"],"title":"How to classify, detect, and manage univariate and multivariate outliers, with emphasis on pre-registration","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"9119cdf63fcf0c6bdd54a6efc1a1b04c","permalink":"https://forrt.org/curated_resources/how-to-crack-pre-registration-toward-tra/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/how-to-crack-pre-registration-toward-tra/","section":"curated_resources","summary":"The reproducibility problem that exists in various academic fields has been discussed in recent years, and it has been revealed that scientists discreetly engage in several questionable research practices (QRPs). For example, the practice of hypothesizing after the results are known (HARKing) involves the reconstruction of hypotheses and stories after results have been obtained (Kerr, 1998) and thereby promotes the retrospective fabrication of favorable hypotheses (cf. Bem, 2004). P-hacking encompasses various untruthful manipulations for obtaining p-values less than 0.05 (Simmons et al., 2011). Such unethical practices dramatically increase the number of false positive findings and thereby encourage the intentional fabrication of evidence as the basis of scientific knowledge and theory, which leads to individual profits for researchers.","tags":["Preregistration"],"title":"How to Crack Pre-registration: Toward Transparent and Open Science","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"a3aa4a84611c3b77af5dd05181a55fbb","permalink":"https://forrt.org/curated_resources/how-to-make-data-open-stop-overlooking-l/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/how-to-make-data-open-stop-overlooking-l/","section":"curated_resources","summary":"Digital archivists are already experts at tackling the complex challenges of making research data open and accessible. We can help to smooth the transition.","tags":["Research Data","Research Management","Funding","Policy","Librarians","Digital Archiving","Open Data"],"title":"How to make data open? Stop overlooking librarians","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1689122843,"objectID":"54b5fefd3275d1106620fdb65e1e8e94","permalink":"https://forrt.org/curated_resources/how-to-make-more-published-research-true/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/how-to-make-more-published-research-true/","section":"curated_resources","summary":"An essay about How to Make More Published Research True","tags":["Reproducibility Crisis and Credibility Revolution","Open Science"],"title":"How to Make More Published Research True","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"134d146d7b9e9118abb057fd22be2482","permalink":"https://forrt.org/curated_resources/how-to-use-osf-as-an-electronic-lab-note/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/how-to-use-osf-as-an-electronic-lab-note/","section":"curated_resources","summary":"This webinar outlines how to use the free Open Science Framework (OSF) as an Electronic Lab Notebook for personal work or private collaborations. Fundamental features we cover include how to record daily activity, how to store images or arbitrary data files, how to invite collaborators, how to view old versions of files, and how to connect all this usage to more complex structures that support the full work of a lab across multiple projects and experiments.","tags":["Analysis","Data","Education","Electronic Lab Notebooks","Materials","Open Scholarship Tools and Technologies","OSF","Publishing","Reproducibility","Research Data Management Tools","Researchers"],"title":"How to Use OSF as an Electronic Lab Notebook","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1721390377,"objectID":"394b58c71787cbe31128a8a8550d23a0","permalink":"https://forrt.org/glossary/english/hypothesis/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/english/hypothesis/","section":"glossary","summary":"","tags":null,"title":"Hypothesis","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1635961284,"objectID":"cadefb5b38c452447ded5b3590ceebb7","permalink":"https://forrt.org/glossary/vbeta/hypothesis/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/vbeta/hypothesis/","section":"glossary","summary":"","tags":null,"title":"Hypothesis","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1720865851,"objectID":"81e608ceeb69db5fa3b8e820dc7681d9","permalink":"https://forrt.org/glossary/german/hypothesis/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/german/hypothesis/","section":"glossary","summary":"","tags":null,"title":"Hypothesis (Hypothese)","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1689122843,"objectID":"b83f4d2370edc04cb7c0c8ab2dfc53e6","permalink":"https://forrt.org/curated_resources/i-fooled-millions-into-thinking-chocolat/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/i-fooled-millions-into-thinking-chocolat/","section":"curated_resources","summary":"A blog about reproducibility crisis","tags":["Reproducibility Crisis and Credibility Revolution","Open Science"],"title":"I Fooled Millions Into Thinking Chocolate Helps Weight Loss. Here's How.","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1721390377,"objectID":"6be96039768923af395d7362b637f2d2","permalink":"https://forrt.org/glossary/english/i10_index/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/english/i10_index/","section":"glossary","summary":"","tags":null,"title":"i10-index","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1635961284,"objectID":"9e69011e3e4b5a2cfb19fa8971668375","permalink":"https://forrt.org/glossary/vbeta/i10-index/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/vbeta/i10-index/","section":"glossary","summary":"","tags":null,"title":"i10-index","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1720865851,"objectID":"782e6c177690d907882051eb221ae247","permalink":"https://forrt.org/glossary/german/i10_index/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/german/i10_index/","section":"glossary","summary":"","tags":null,"title":"I10-index (i10-Index)","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1689122843,"objectID":"cdea8ffed04d24d193364acd28a44f36","permalink":"https://forrt.org/curated_resources/identifying-participants-in-the-personal/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/identifying-participants-in-the-personal/","section":"curated_resources","summary":"We linked names and contact information to publicly available profiles in the Personal Genome Project. These profiles contain medical and genomic information, including details about medications, procedures and diseases, and demographic information, such as date of birth, gender, and postal code. By linking demographics to public records such as voter lists, and mining for names hidden in attached documents, we correctly identified 84 to 97 percent of the profiles for which we provided names. Our ability to learn their names is based on their demographics, not their DNA, thereby revisiting an old vulnerability that could be easily thwarted with minimal loss of research value. So, we propose technical remedies for people to learn about their demographics to make better decisions.","tags":["Reproducibility Crisis and Credibility Revolution","Open Science"],"title":"Identifying Participants in the Personal Genome Project by Name","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1721390377,"objectID":"104da18050c63719ac4316d5ce1d85d7","permalink":"https://forrt.org/glossary/english/ideological_bias/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/english/ideological_bias/","section":"glossary","summary":"","tags":null,"title":"Ideological bias","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1635961284,"objectID":"bbee064306d1934a54ff3140cb722340","permalink":"https://forrt.org/glossary/vbeta/ideological-bias/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/vbeta/ideological-bias/","section":"glossary","summary":"","tags":null,"title":"Ideological bias","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1720865851,"objectID":"4a1bb67b32d9e93d8233dfe930457daf","permalink":"https://forrt.org/glossary/german/ideological_bias/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/german/ideological_bias/","section":"glossary","summary":"","tags":null,"title":"Ideological bias (Ideologische Verzerrung)","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"f27c3eb8d98556ccd610c3d8c9e6a646","permalink":"https://forrt.org/curated_resources/iit-vs-gnwt-and-the-meaning-of-evidence/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/iit-vs-gnwt-and-the-meaning-of-evidence/","section":"curated_resources","summary":"This post follows one I wrote earlier in the summer, after ASSC. Since then I have been trying (in a first-person therapy sort of way) to figure out what made me so worried after the IIT vs. GNWT showdown, and the media coverage that followed it. In this post (which is going to be more technical, and more focused on theories of consciousness) I aim to articulate why many of us were concerned about how the results of the first Accelerating Research on Consciousness initiative were portrayed, and the lessons that this holds for future ARCs, particularly those attempting to test theories that are metaphysically unusual such as IIT.\n\nI am personally invested in trying to understand what happened here, as together with Axel Cleeremans I am co-leading a similar project, also funded by TWCF, in which we are comparing different higher-order theories (HOTs). Our project hasn’t started yet, so now feels like a good time to think about how best to organise ourselves. I also want to start with a disclaimer: I thoroughly admire the efforts to change the field that the Cogitate project has engaged in. Running adversarial collaborations is hard – and running the first one is no doubt an order of magnitude harder. Simply put, we would not be having these discussions (and I would not be writing this blog post) If it wasn’t for their project. We would in all likelihood be pursuing “regular” projects which, as Yaron and colleagues have strikingly highlighted, have repeatedly suffered from confirmation bias and “looking under the lamppost”.\n\nIn what follows I focus on three distinct issues. The first is the origin of predictions, and the idiosyncratic nature of these. The second is the role of background assumptions, and how consciousness science interfaces with mainstream neuroscience. The third is the bigger-picture (and perhaps harder to resolve) issue of how to test between two theories that have radically different metaphysical starting points and implications.","tags":["Consciousness","IIT","GNWT","Accelerating Research on Consciousness","Higher Order Theories"],"title":"IIT vs. GNWT and the meaning of evidence in consciousness science","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"72dd0e4b5e5f0a3e610e59cb28707e5c","permalink":"https://forrt.org/curated_resources/image-processing-with-python/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/image-processing-with-python/","section":"curated_resources","summary":"This lesson shows how to use Python and skimage to do basic image processing. With support from an NSF iUSE grant, Dr. Tessa Durham Brooks and Dr. Mark Meysenburg at Doane College, Nebraska, USA have developed a curriculum for teaching image processing in Python. This lesson is currently being piloted at different institutions. This pilot phase will be followed by a clean-up phase to incorporate suggestions and feedback from the pilots into the lessons and to make the lessons teachable by the broader community. Development for these lessons has been supported by a grant from the Sloan Foundation.","tags":["Analysis","Data","Education","Image Processing","Open Scholarship Tools and Technologies","Python","Reproducibility","Research Data Management Tools","Researchers"],"title":"Image Processing with Python","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"86cccb4cc68b00a0c76438b040c2be28","permalink":"https://forrt.org/curated_resources/impact-of-genetic-background-and-experim/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/impact-of-genetic-background-and-experim/","section":"curated_resources","summary":"Limiting the debilitating consequences of ageing is a major medical challenge of our time. Robust pharmacological interventions that promote healthy ageing across diverse genetic backgrounds may engage conserved longevity pathways. Here we report results from the Caenorhabditis Intervention Testing Program in assessing longevity variation across 22 Caenorhabditis strains spanning 3 species, using multiple replicates collected across three independent laboratories. Reproducibility between test sites is high, whereas individual trial reproducibility is relatively low. Of ten pro-longevity chemicals tested, six significantly extend lifespan in at least one strain. Three reported dietary restriction mimetics are mainly effective across C. elegans strains, indicating species and strain-specific responses. In contrast, the amyloid dye ThioflavinT is both potent and robust across the strains. Our results highlight promising pharmacological leads and demonstrate the importance of assessing lifespans of discrete cohorts across repeat studies to capture biological variation in the search for reproducible ageing interventions.","tags":["Analysis","Genetics","Reproducibility","Research Methods"],"title":"Impact of genetic background and experimental reproducibility on identifying chemical compounds with robust longevity effects","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"bebf618f508dfdee710da60fc2a60f9f","permalink":"https://forrt.org/curated_resources/improving-our-science/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/improving-our-science/","section":"curated_resources","summary":"The goal of science is to accumulate knowledge about nature. There are scientific values guiding how scientists should work, and scientific practices guiding how scientists do work. This course will examine the discrepancy between scientific values and scientific practices. What are the ordinary daily practices of scientists ,laboratories, and disciplines that deviate from scientific values? Why are they different What can, and should, be done about it We will develop and implement strategies to improve alignment between our own scientific values and practices.","tags":["Transparency","Reproducibility Crisis and Credibility Revolution","Open Science"],"title":"Improving (Our) Science","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"33f0b925ba87b96bdc1b29a6494b5fee","permalink":"https://forrt.org/curated_resources/improving-our-science-reproducibility-re/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/improving-our-science-reproducibility-re/","section":"curated_resources","summary":"The goal of the course is to become a better scientist. You will learn about newest standards for scientific openness, and how they influence the reporting and interpretation of empirical evidence. One component of the course is an intervention to assist you as a practicing scientist. The hope is that the course will help you to lay out the ideal norms and practices and then give you a bit of practice in implementing them. A second component of the course will examine the discrepancy between scientific values and normative scientific practices. What are the ordinary daily practices of scientists, laboratories, and disciplines and when do they fail to follow best practices? We will consider (and propose) solutions for bringing scientific practice in concert with scientific goals.The third component will be the evaluation of those solutions. Which interventions hold the most promise for improving scientific practice and how will we be able to judge?This is not a typical graduate course. However, it is very relevant to doing effective research. It is not quite a research methods class, not quite a professional issues class, and not quite a self-improvement class, but it is a bit of all of three. The substantive content of interest is your own graduate field and research projects. The focus of this class is on (a) the practices you use to become expert and contribute to that field, and (b) the normative practices that disrupt the development of knowledge in that field. ","tags":["Reproducibility Crisis and Credibility Revolution","Open Science","Conceptual and statistical knowledge"],"title":"Improving (Our) Science: Reproducibility, Reporting, and Openness","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"f82f8edd8cc23299a1dcd7c42eddcc14","permalink":"https://forrt.org/curated_resources/improving-evidence-based-practice-throug/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/improving-evidence-based-practice-throug/","section":"curated_resources","summary":"Preregistration is the practice of publicly publishing plans on central components of the research process before access to, or collection, of data. Within the context of the replication crisis, open science practices like preregistration have been pivotal in facilitating greater transparency in research. However, such practices have been applied nearly exclusively to basic academic research, with rare consideration of the relevance to applied and consultancy-based research. This is particularly problematic as such research is typically reported with very low levels of transparency and accountability despite being disseminated as influential gray literature to inform practice. Evidence-based practice is best served by an appreciation of multiple sources of quality evidence, thus the current review considers the potential of preregistration to improve both the accessibility and credibility of applied research toward more rigorous evidence-based practice. The current three-part review outlines, first, the opportunities of preregistration for applied research, and second, three barriers – practical challenges, stakeholder roles, and the suitability of preregistration. Last, this review makes four recommendations to overcome these barriers and maximize the opportunities of preregistration for academics, industry, and the structures they are held within – changes to preregistration templates, new types of templates, education and training, and recognition and structural changes.","tags":["Preregistration","Academia","Industry","Consulting","Applied Research","Evidence-based Practices"],"title":"Improving evidence-based practice through preregistration of applied research: Barriers and recommendations","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1689122843,"objectID":"fd105b1b2a203717131ee5895a6b8a47","permalink":"https://forrt.org/curated_resources/improving-the-dependability-of-research/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/improving-the-dependability-of-research/","section":"curated_resources","summary":"In this article, the Society for Personality and Social Psychology (SPSP) Task Force on Publication and Research Practices offers a brief statistical primer and recommendations for improving the dependability of research. Recommendations for research practice include (a) describing and addressing the choice of N (sample size) and consequent issues of statistical power, (b) reporting effect sizes and 95% confidence intervals (CIs), (c) avoiding “questionable research practices” that can inflate the probability of Type I error, (d) making available research materials necessary to replicate reported results, (e) adhering to SPSP’s data sharing policy, (f) encouraging publication of high-quality replication studies, and (g) maintaining flexibility and openness to alternative standards and methods. Recommendations for educational practice include (a) encouraging a culture of “getting it right,” (b) teaching and encouraging transparency of data reporting, (c) improving methodological instruction, and (d) modeling sound science and supporting junior researchers who seek to “get it right.”","tags":[""],"title":"Improving the Dependability of Research in Personality and Social Psychology: Recommendations for Research and Educational Practice","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"107fd5049c0b656ebe2eb8ae43c00161","permalink":"https://forrt.org/curated_resources/improving-transparency-in-observational/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/improving-transparency-in-observational/","section":"curated_resources","summary":"Social science research has undergone a credibility revolution, but these gains are at risk due to problematic research practices. Existing research on transparency has centered around randomized controlled trials, which constitute only a small fraction of research in economics. In this paper, I highlight three scenarios in which study preregistration can be credibly applied in non-experimental settings: cases where researchers collect their own data; prospective studies; and research using restricted-access data.","tags":["Preregistration","Social Science"],"title":"Improving transparency in observational social science research: A pre-analysis plan approach","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1689122843,"objectID":"22f480f1829236bbd93692030621a297","permalink":"https://forrt.org/curated_resources/improving-your-statistical-inferences/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/improving-your-statistical-inferences/","section":"curated_resources","summary":"This course aims to help you to draw better statistical inferences from empirical research. First, we will discuss how to correctly interpret p-values, effect sizes, confidence intervals, Bayes Factors, and likelihood ratios, and how these statistics answer different questions you might be interested in. Then, you will learn how to design experiments where the false positive rate is controlled, and how to decide upon the sample size for your study, for example in order to achieve high statistical power. Subsequently, you will learn how to interpret evidence in the scientific literature given widespread publication bias, for example by learning about p-curve analysis. Finally, we will talk about how to do philosophy of science, theory construction, and cumulative science, including how to perform replication studies, why and how to pre-register your experiment, and how to share your results following Open Science principles. ","tags":["Course"],"title":"Improving your statistical inferences","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"b76e19ad3f5b70c2386a7f4d1ec713fc","permalink":"https://forrt.org/curated_resources/in-praise-of-moderation-suggestions-for/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/in-praise-of-moderation-suggestions-for/","section":"curated_resources","summary":"Pre-Analysis Plans (PAPs) for randomized evaluations are becoming increasingly common in Economics, but their definition remains unclear and their practical applications therefore vary widely. Based on our collective experiences as researchers and editors, we articulate a set of principles for the ex-ante scope and ex-post use of PAPs. We argue that the key benefits of a PAP can usually be realized by completing the registration fields in the AEA RCT Registry. Specific cases where more detail may be warranted include when subgroup analysis is expected to be particularly important, or a party to the study has a vested interest. However, a strong norm for more detailed pre-specification can be detrimental to knowledge creation when implementing field experiments in the real world. An ex-post requirement of strict adherence to pre-specified plans, or the discounting of non-pre-specified work, may mean that some experiments do not take place, or that interesting observations and new theories are not explored and reported. Rather, we recommend that the final research paper be written and judged as a distinct object from the “results of the PAP”; to emphasize this distinction, researchers could consider producing a short, publicly available report (the “populated PAP”) that populates the PAP to the extent possible and briefly discusses any barriers to doing so.","tags":["Preregistration","Pre-Analysis Plans","Economics"],"title":"In Praise of Moderation: Suggestions for the Scope and Use of Pre-Analysis Plans for RCTs in Economics","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1689122843,"objectID":"0b292e7d0d2cd72b0795f8f9e6e5481d","permalink":"https://forrt.org/curated_resources/in-praise-of-the-null-hypothesis-statist/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/in-praise-of-the-null-hypothesis-statist/","section":"curated_resources","summary":"Jacob Cohen (see record 1995-12080-001) raised a number of questions about the logic and information value of the null hypothesis statistical test (NHST). Specifically, he suggested that: (1) The NHST does not tell us what we want to know; (2) the null hypothesis is always false; and (3) the NHST lacks logical integrity. It is the author's view that although there may be good reasons to give up the NHST, these particular points made by Cohen are not among those reasons. When addressing these points, the author also attempts to demonstrate the elegance and usefulness of the NHST.","tags":[""],"title":"In Praise of the Null Hypothesis Statistical Test","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1689122843,"objectID":"8fc19b9b6a1dbad75fe677195974ef80","permalink":"https://forrt.org/curated_resources/inappropriate-fiddling-with-statistical/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/inappropriate-fiddling-with-statistical/","section":"curated_resources","summary":"Much has been written regarding p-values below certain thresholds (most notably 0.05) denoting statistical significance and the tendency of such p-values to be more readily publishable in peer-reviewed journals. Intuition suggests that there may be a tendency to manipulate statistical analyses to push a ‘‘near significant p-value’’ to a level that is considered significant. This article presents a method for detecting the presence of such manipulation (herein called ‘‘fiddling’’) in a distribution of p-values from independent studies. Simulations are used to illustrate the properties of the method. The results suggest that the method has low type I error and that power approaches acceptable levels as the number of p-values being studied approaches 1000.","tags":[""],"title":"Inappropriate fiddling with statistical analyses to obtain a desirable p-value: tests to detect its presence in published literature. ","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1721390377,"objectID":"e540633bbd33fb53e300b62627d935ef","permalink":"https://forrt.org/glossary/english/incentive_structure/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/english/incentive_structure/","section":"glossary","summary":"","tags":null,"title":"Incentive structure","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1635961284,"objectID":"d010da7ad13adb2c58fd4c88519189f5","permalink":"https://forrt.org/glossary/vbeta/incentive-structure/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/vbeta/incentive-structure/","section":"glossary","summary":"","tags":null,"title":"Incentive structure","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1720865851,"objectID":"2e9dd682ec6ee4c454036504af2f6f75","permalink":"https://forrt.org/glossary/german/incentive_structure/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/german/incentive_structure/","section":"glossary","summary":"","tags":null,"title":"Incentive structure (Anreizsystem)","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1721390377,"objectID":"81cb7b87345c99bcc1f1b28b9f2ee069","permalink":"https://forrt.org/glossary/english/inclusion/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/english/inclusion/","section":"glossary","summary":"","tags":null,"title":"Inclusion","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1635961284,"objectID":"d8f861884d51eadb8d3423b731ef0240","permalink":"https://forrt.org/glossary/vbeta/inclusion/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/vbeta/inclusion/","section":"glossary","summary":"","tags":null,"title":"Inclusion","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1720865851,"objectID":"cc193084aec2aef148b3ba16324719c4","permalink":"https://forrt.org/glossary/german/inclusion/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/german/inclusion/","section":"glossary","summary":"","tags":null,"title":"Inclusion (Inklusion, Inklusivität)","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"fedae7d1cf625b207664fac1e2ea4211","permalink":"https://forrt.org/curated_resources/incorporating-ecological-momentary-asses/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/incorporating-ecological-momentary-asses/","section":"curated_resources","summary":"Ecological momentary assessment (EMA) represents a promising approach to study cognitive aging. In contrast to laboratory-based studies, EMA involves the repeated sampling of experiences in daily life contexts, enabling investigators to gain access to dynamic processes (e.g., situational contexts, intraindividual variability) that are likely to strongly contribute to aging and age-related change across the adult life-span. As such, EMA approaches complement the prevailing research methods in the field of cognitive aging (e.g., laboratory-based paradigms, neuroimaging), while also providing the opportunity to replicate and extend findings from the laboratory in more naturalistic contexts. Following an overview of the methodological and conceptual strengths of EMA approaches in cognitive aging research, we discuss best practices for researchers interested in implementing EMA studies. A key goal is to highlight the tremendous potential for combining EMA methods with other laboratory-based approaches, in order to increase the robustness, replicability, and real-world implications of research findings in the field of cognitive aging.","tags":["Ecological Momentary Assessment","Methods","Cognitive Aging","Robustness","Replicability","Replication","Generalizability","Transferability"],"title":"Incorporating ecological momentary assessment into multimethod investigations of cognitive aging: Promise and practical considerations","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"39f2fade88711c00e624c5112ccfbb5a","permalink":"https://forrt.org/curated_resources/increasing-efficiency-of-preclinical-res/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/increasing-efficiency-of-preclinical-res/","section":"curated_resources","summary":"Despite the potential benefits of sequential designs, studies evaluating treatments or experimental manipulations in preclinical experimental biomedicine almost exclusively use classical block designs. Our aim with this article is to bring the existing methodology of group sequential designs to the attention of researchers in the preclinical field and to clearly illustrate its potential utility. Group sequential designs can offer higher efficiency than traditional methods and are increasingly used in clinical trials. Using simulation of data, we demonstrate that group sequential designs have the potential to improve the efficiency of experimental studies, even when sample sizes are very small, as is currently prevalent in preclinical experimental biomedicine. When simulating data with a large effect size of d = 1 and a sample size of n = 18 per group, sequential frequentist analysis consumes in the long run only around 80% of the planned number of experimental units. In larger trials (n = 36 per group), additional stopping rules for futility lead to the saving of resources of up to 30% compared to block designs. We argue that these savings should be invested to increase sample sizes and hence power, since the currently underpowered experiments in preclinical biomedicine are a major threat to the value and predictiveness in this research domain.","tags":["Bayesian Method","Bayesian Statistics","Clinical Trials","Experimental Design","Medicine and Health Sciences","Probability Distribution","Research Design","Research Errors"],"title":"Increasing efficiency of preclinical research by group sequential designs","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"8abe31c245eacd752cb1bf5cacf8c85c","permalink":"https://forrt.org/curated_resources/increasing-the-transparency-of-systemati/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/increasing-the-transparency-of-systemati/","section":"curated_resources","summary":"This paper presents a generalized registration form for systematic reviews that can be used when currently available forms are not adequate. The form is designed to be applicable across disciplines (i.e., psychology, economics, law, physics, or any other field) and across review types (i.e., scoping review, review of qualitative studies, meta-analysis, or any other type of review). That means that the reviewed records may include research reports as well as archive documents, case law, books, poems, etc. Items were selected and formulated to optimize broad applicability instead of specificity, forgoing some benefits afforded by a tighter focus. This PRISMA 2020 compliant form is a fallback for more specialized forms and can be used if no specialized form or registration platform is available. When accessing this form on the Open Science Framework website, users will therefore first be guided to specialized forms when they exist. In addition to this use case, the form can also serve as a starting point for creating registration forms that cater to specific fields or review types.","tags":["Registration","Systematic Reviews","Psychology","Economics","Law","Physics","Meta-Analysis"],"title":"Increasing the transparency of systematic reviews: Presenting a generalized registration form","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1689122843,"objectID":"34d4c671cd51a18bb016650164765dbd","permalink":"https://forrt.org/curated_resources/increasing-transparency-through-a-multiv/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/increasing-transparency-through-a-multiv/","section":"curated_resources","summary":"Empirical research inevitably includes constructing a data set by processing raw data into a form ready for statistical analysis. Data processing often involves choices among several reasonable options for excluding, transforming, and coding data. We suggest that instead of performing only one analysis, researchers could perform a multiverse analysis, which involves performing all analyses across the whole set of alternatively processed data sets corresponding to a large set of reasonable scenarios. Using an example focusing on the effect of fertility on religiosity and political attitudes, we show that analyzing a single data set can be misleading and propose a multiverse analysis as an alternative practice. A multiverse analysis offers an idea of how much the conclusions change because of arbitrary choices in data construction and gives pointers as to which choices are most consequential in the fragility of the result.","tags":["Reproducibility Crisis and Credibility Revolution","Open Science"],"title":"Increasing transparency through a multiverse analysis.","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"5a25e42cf7181752c91436b6d49006fa","permalink":"https://forrt.org/curated_resources/increasing-value-and-reducing-waste-addr/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/increasing-value-and-reducing-waste-addr/","section":"curated_resources","summary":"The methods and results of health research are documented in study protocols, full study reports (detailing all analyses), journal reports, and participant-level datasets. However, protocols, full study reports, and participant-level datasets are rarely available, and journal reports are available for only half of all studies and are plagued by selective reporting of methods and results. Furthermore, information provided in study protocols and reports varies in quality and is often incomplete. When full information about studies is inaccessible, billions of dollars in investment are wasted, bias is introduced, and research and care of patients are detrimentally affected. To help to improve this situation at a systemic level, three main actions are warranted. First, academic institutions and funders should reward investigators who fully disseminate their research protocols, reports, and participant-level datasets. Second, standards for the content of protocols and full study reports and for data sharing practices should be rigorously developed and adopted for all types of health research. Finally, journals, funders, sponsors, research ethics committees, regulators, and legislators should endorse and enforce policies supporting study registration and wide availability of journal reports, full study reports, and participant-level datasets.","tags":["Protocols","Preregistration","Open Access","Open Data","Funding"],"title":"Increasing value and reducing waste: addressing inaccessible research","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1721390377,"objectID":"c8a9fa2324b87dd5a7aafbea99024c9f","permalink":"https://forrt.org/glossary/english/induction/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/english/induction/","section":"glossary","summary":"","tags":null,"title":"Induction","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1635961284,"objectID":"ed5b82bc440633bc1390ad61a943cfb4","permalink":"https://forrt.org/glossary/vbeta/induction/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/vbeta/induction/","section":"glossary","summary":"","tags":null,"title":"Induction ","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1720865851,"objectID":"6738bae9f733c376d6ddf5e7aa462000","permalink":"https://forrt.org/glossary/german/induction/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/german/induction/","section":"glossary","summary":"","tags":null,"title":"Induction (Induktion)","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1689122843,"objectID":"8d67c544c98e96570f28748965fc9971","permalink":"https://forrt.org/curated_resources/instead-of-playing-the-game-it-is-time-t/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/instead-of-playing-the-game-it-is-time-t/","section":"curated_resources","summary":"The last ten years have witnessed increasing awareness of questionable research practices (QRPs) in the life sciences [1,2], including p-hacking [3], HARKing [4], lack of replication [5], publication bias [6], low statistical power [7] and lack of data sharing ([8]; see Figure 1). Concerns about such behaviours have been raised repeatedly for over half a century [9–11] but the incentive structure of academia has not changed to address them. Despite the complex motivations that drive academia, many QRPs stem from the simple fact that the incentives which offer success to individual scientists conflict with what is best for science [12]. On the one hand are a set of gold standards that centuries of the scientific method have proven to be crucial for discovery: rigour, reproducibility, and transparency. On the other hand are a set of opposing principles born out of the academic career model: the drive to produce novel and striking results, the importance of confirming prior expectations, and the need to protect research interests from competitors. Within a culture that pressures scientists to produce rather than discover, the outcome is a biased and impoverished science in which most published results are either unconfirmed genuine discoveries or unchallenged fallacies [13]. This observation implies no moral judgement of scientists, who are as much victims of this system as they are perpetrators.","tags":["Reproducibility Crisis and Credibility Revolution","Open Science"],"title":"Instead of “playing the game” it is time to change the rules: Registered Reports at AIMS Neuroscience and beyond. ","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1689122843,"objectID":"0cf37e92e495a8eac13d6b2134c447d3","permalink":"https://forrt.org/curated_resources/intellectual-humility-is-central-to-scie/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/intellectual-humility-is-central-to-scie/","section":"curated_resources","summary":"Transparency is indispensable for accuracy and correction in science, and is discussed frequently in the credibility revolution. A less often mentioned aspect of credibility is the need for intellectual humility: When scientific communication is overconfident or contains too many exaggerations, the field stands to lose its credibility, even if the methods and statistics underlying the research are sound. We argue that intellectual humility is given a great deal of lip service, but is too rarely valued - we may say that we as scientists ought to be intellectually humble, but our actions as a field suggest that this is not a priority. Although we acknowledge that intellectual humility is presented as a widely accepted scientific norm, we argue that current research practice does not actually incentivize intellectual humility. A promising solution could be to use our roles as reviewers to incentivize authors putting the flaws and uncertainty in their work front and center, thus giving their critics ammunition to find their errors. We describe several ways reviewers (and authors) can contribute to increasing humility in practice, instead of passively waiting for the system to change.","tags":["Reproducibility Crisis and Credibility Revolution","Open Science"],"title":"Intellectual humility is central to science","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1721390377,"objectID":"c7e696549521b37d013a6a900f159f5e","permalink":"https://forrt.org/glossary/english/interaction_fallacy/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/english/interaction_fallacy/","section":"glossary","summary":"","tags":null,"title":"Interaction Fallacy","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1635961284,"objectID":"86746f551a591d68e42799ad4484de4a","permalink":"https://forrt.org/glossary/vbeta/interaction-fallacy/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/vbeta/interaction-fallacy/","section":"glossary","summary":"","tags":null,"title":"Interaction Fallacy","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1720865851,"objectID":"c9ea62c3b4d8fc97ee3b8caa568e1810","permalink":"https://forrt.org/glossary/german/interaction_fallacy/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/german/interaction_fallacy/","section":"glossary","summary":"","tags":null,"title":"Interaction Fallacy (Interaktionsfehlschluss)","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1721390377,"objectID":"293f27e42759bb87cd125477839988cb","permalink":"https://forrt.org/glossary/english/interlocking/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/english/interlocking/","section":"glossary","summary":"","tags":null,"title":"Interlocking","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1635961284,"objectID":"dbd192a962704ec59eb4d1d3f5a83824","permalink":"https://forrt.org/glossary/vbeta/interlocking/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/vbeta/interlocking/","section":"glossary","summary":"","tags":null,"title":"Interlocking","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1720865851,"objectID":"f0c59e8b0cf10f3e8d3a3e3a51fb3f80","permalink":"https://forrt.org/glossary/german/interlocking/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/german/interlocking/","section":"glossary","summary":"","tags":null,"title":"Interlocking (Verzahnung)","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1689122843,"objectID":"8a8f2aee3ace255078ed9fe2f59efc26","permalink":"https://forrt.org/curated_resources/internal-conceptual-replications-do-not/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/internal-conceptual-replications-do-not/","section":"curated_resources","summary":"Recently, many psychological effects have been surprisingly difficult to reproduce. This article asks why, and investigates whether conceptually replicating an effect in the original publication is related to the success of independent, direct replications. Two prominent accounts of low reproducibility make different predictions in this respect. One account suggests that psychological phenomena are dependent on unknown contexts that are not reproduced in independent replication attempts. By this account, internal replications indicate that a finding is more robust and, thus, that it is easier to independently replicate it. An alternative account suggests that researchers employ questionable research practices (QRPs), which increase false positive rates. By this account, the success of internal replications may just be the result of QRPs and, thus, internal replications are not predictive of independent replication success. The data of a large reproducibility project support the QRP account: replicating an effect in the original publication is not related to independent replication success. Additional analyses reveal that internally replicated and internally unreplicated effects are not very different in terms of variables associated with replication success. Moreover, social psychological effects in particular appear to lack any benefit from internal replications. Overall, these results indicate that, in this dataset at least, the influence of QRPs is at the heart of failures to replicate psychological findings, especially in social psychology. Variable, unknown contexts appear to play only a relatively minor role. I recommend practical solutions for how QRPs can be avoided.","tags":["Reproducibility Crisis and Credibility Revolution","Open Science"],"title":"Internal conceptual replications do not increase independent replication success","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1721390377,"objectID":"020ee5d684d91c6efcbdc70d52b0f080","permalink":"https://forrt.org/glossary/english/internal_validity/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/english/internal_validity/","section":"glossary","summary":"","tags":null,"title":"Internal Validity","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1635961284,"objectID":"faa443b5e75717b60c544ec790a0f4ec","permalink":"https://forrt.org/glossary/vbeta/internal-validity/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/vbeta/internal-validity/","section":"glossary","summary":"","tags":null,"title":"Internal Validity","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1720865851,"objectID":"87f17ce305cac54b376cd3ed99ce9471","permalink":"https://forrt.org/glossary/german/internal_validity/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/german/internal_validity/","section":"glossary","summary":"","tags":null,"title":"Internal Validity (interne Validität)","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1689122843,"objectID":"bcd7ef5e268d8ce470897eda119bc061","permalink":"https://forrt.org/curated_resources/interpreting-confidence-intervals/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/interpreting-confidence-intervals/","section":"curated_resources","summary":"A blog about the visualisation of Confidence intervals","tags":["Blog","Interaction","Simulation","Tutorial"],"title":"Interpreting confidence intervals","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1689122843,"objectID":"b64d256572a15ecd49931dd36da6acc5","permalink":"https://forrt.org/curated_resources/interpreting-correlations-an-interactive/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/interpreting-correlations-an-interactive/","section":"curated_resources","summary":"Correlation is one of the most widely used tools in statistics. The correlation coefficient summarizes the association between two variables. In this visualization I show a scatter plot of two variables with a given correlation. The variables are samples from the standard normal distribution, which are then transformed to have a given correlation by using Cholesky decomposition. By moving the slider you will see how the shape of the data changes as the association becomes stronger or weaker. You can also look at the Venn diagram to see the amount of shared variance between the variables. It is also possible drag the data points to see how the correlation is influenced by outliers.","tags":["Blog","Interaction","Simulation","Tutorial"],"title":"Interpreting Correlations: an interactive visualization","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1689122843,"objectID":"ebc510d42ada36f8339d05dcb2be120d","permalink":"https://forrt.org/curated_resources/interpreting-effect-sizes-toward-a-quant/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/interpreting-effect-sizes-toward-a-quant/","section":"curated_resources","summary":"Improved research practice is based on estimation of effect sizes rather than statistical significance. We discuss the challenging task of interpreting effect sizes in the research context, with particular attention to social psychological research. We emphasize the need to acknowledge the uncertainty in an effect size estimate, as signaled by the confidence interval. Interpretation must consider the independent variables, participants, measures, and other aspects of the research. Comparison with other results in the research field, and consideration of theoretical and practical implications are useful strategies. Researchers should consider the possible value of agreeing on benchmarks to help guide effect size interpretation, at least within focused research fields. More broadly, researchers should wherever possible think of experimental manipulations as well as results in quantitative terms. Doing so is fundamental for designing ingenious, informative experiments, understanding research results and their implications, developing theory, and building a quantitative cumulative social psychology","tags":[""],"title":"Interpreting effect sizes: Toward a quantitative cumulative social psychology","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1721390377,"objectID":"88083b9d86f18d47ed64cc9e42d45057","permalink":"https://forrt.org/glossary/english/intersectionality/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/english/intersectionality/","section":"glossary","summary":"","tags":null,"title":"Intersectionality","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1635961284,"objectID":"5979990e4a6ee440e1d4a8e37f9422d6","permalink":"https://forrt.org/glossary/vbeta/intersectionality/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/vbeta/intersectionality/","section":"glossary","summary":"","tags":null,"title":"Intersectionality","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1720865851,"objectID":"be51368484600fead82cbf30ea6ecad8","permalink":"https://forrt.org/glossary/german/intersectionality/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/german/intersectionality/","section":"glossary","summary":"","tags":null,"title":"Intersectionality (Intersektionalität)","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"d84bdc6e5c981699ec502d4a71aad832","permalink":"https://forrt.org/curated_resources/intro-to-calculating-confidence-interval/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/intro-to-calculating-confidence-interval/","section":"curated_resources","summary":"This video will introduce how to calculate confidence intervals around effect sizes using the MBESS package in R. All materials shown in the video, as well as content from our other videos, can be found here: https://osf.io/7gqsi/","tags":["Analysis","Data","Education","Materials","Open Scholarship Tools and Technologies","OSF","Publishing","Reproducibility","Research Data Management Tools","Researchers","Statistics"],"title":"Intro to Calculating Confidence Intervals","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"70c12562b12ce886c298f98ba08ef328","permalink":"https://forrt.org/curated_resources/intro-to-r-and-rstudio-for-genomics/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/intro-to-r-and-rstudio-for-genomics/","section":"curated_resources","summary":"Welcome to R! Working with a programming language (especially if it’s your first time) often feels intimidating, but the rewards outweigh any frustrations. An important secret of coding is that even experienced programmers find it difficult and frustrating at times – so if even the best feel that way, why let intimidation stop you? Given time and practice* you will soon find it easier and easier to accomplish what you want. Why learn to code? Bioinformatics – like biology – is messy. Different organisms, different systems, different conditions, all behave differently. Experiments at the bench require a variety of approaches – from tested protocols to trial-and-error. Bioinformatics is also an experimental science, otherwise we could use the same software and same parameters for every genome assembly. Learning to code opens up the full possibilities of computing, especially given that most bioinformatics tools exist only at the command line. Think of it this way: if you could only do molecular biology using a kit, you could probably accomplish a fair amount. However, if you don’t understand the biochemistry of the kit, how would you troubleshoot? How would you do experiments for which there are no kits? R is one of the most widely-used and powerful programming languages in bioinformatics. R especially shines where a variety of statistical tools are required (e.g. RNA-Seq, population genomics, etc.) and in the generation of publication-quality graphs and figures. Rather than get into an R vs. Python debate (both are useful), keep in mind that many of the concepts you will learn apply to Python and other programming languages. Finally, we won’t lie; R is not the easiest-to-learn programming language ever created. So, don’t get discouraged! The truth is that even with the modest amount of R we will cover today, you can start using some sophisticated R software packages, and have a general sense of how to interpret an R script. Get through these lessons, and you are on your way to being an accomplished R user! * We very intentionally used the word practice. One of the other “secrets” of programming is that you can only learn so much by reading about it. Do the exercises in class, re-do them on your own, and then work on your own problems.","tags":["Analysis","Data","Education","Inside Your Classroom","Open Scholarship Tools and Technologies","R","Reproducibility","Research Data Management Tools","Researchers"],"title":"Intro to R and RStudio for Genomics","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1689122843,"objectID":"90aec3bb8fa19bc112fb29d103c9614a","permalink":"https://forrt.org/curated_resources/intro-to-the-special-issue/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/intro-to-the-special-issue/","section":"curated_resources","summary":"A paper about a special issue on cognitive modelling","tags":[""],"title":"Intro to the special issue","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1689122843,"objectID":"929d594297bd6b4da10c48cb81025326","permalink":"https://forrt.org/curated_resources/introducing-jasp/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/introducing-jasp/","section":"curated_resources","summary":"A blog about JASP to replace SPSS","tags":["Blog","Reproducibility Knowledge"],"title":"Introducing JASP","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"05669c9bf17a276c0bfa13d407aae22c","permalink":"https://forrt.org/curated_resources/introducing-preregistration-of-research/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/introducing-preregistration-of-research/","section":"curated_resources","summary":"Archaeology has an issue with \"just-in-time\" research, where insufficient attention is paid to articulating a research design before fieldwork begins. Data collection, management, and analysis approaches are under-planned and, often, evolve during fieldwork. While reducing the amount of preparation time for busy researchers, these tendencies reduce the reliability of research by exacerbating the effects of cognitive biases and perverse professional incentives. They cost time later through the accrual of technical debt. Worse, these practices hinder research transparency and scalability by undermining the quality, consistency, and compatibility of data. Archaeologists would benefit from embracing the \"preregistration revolution\" sweeping other disciplines. By publicly committing to research design and methodology ahead of time, researchers can produce more robust research, generate useful and reusable datasets, and reduce the time spent correcting problems with data. Preregistration can accommodate the diversity of archaeological research, including quantitative and qualitative approaches, hypothesis-testing and hypothesis-generating research paradigms, and place-specific and generalizing aims. It is appropriate regardless of the technical approach to data collection and analysis. More broadly, it encourages a more considered, thoughtful approach to research design. Preregistration templates for the social sciences can be adopted for use by archaeologists.","tags":["Arts and Humanities","Digital Humanities","History","Philosophy","Philosophy of Science","Archaeology","Preregistration"],"title":"Introducing Preregistration of Research Design to Archaeology","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"0ac83131ee987ea88e60da4ad20deadd","permalink":"https://forrt.org/curated_resources/introduction-materials-for-reproducible/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/introduction-materials-for-reproducible/","section":"curated_resources","summary":"Workshop goals\n- Why are we teaching this\n- Why is this important\n- For future and current you\n- For research as a whole\n- Lack of reproducibility in research is a real problem\n\nMaterials and how we'll use them\n- Workshop landing page, with\n\n- links to the Materials\n- schedule\n\nStructure oriented along the Four Facets of Reproducibility:\n\n- Documentation\n- Organization\n- Automation\n- Dissemination\n\nWill be available after the Workshop\n\nHow this workshop is run\n- This is a Carpentries Workshop\n- that means friendly learning environment\n- Code of Conduct\n- active learning\n- work with the people next to you\n- ask for help","tags":["Organizing","Reproducibility"],"title":"Introduction materials for Reproducible Research Curriculum","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"4226dd6139e20c3b357a938008325fea","permalink":"https://forrt.org/curated_resources/introduction-to-cloud-computing-for-geno/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/introduction-to-cloud-computing-for-geno/","section":"curated_resources","summary":"Data Carpentry lesson to learn how to work with Amazon AWS cloud computing and how to transfer data between your local computer and cloud resources. The cloud is a fancy name for the huge network of computers that host your favorite websites, stream movies, and shop online, but you can also harness all of that computing power for running analyses that would take days, weeks or even years on your local computer. In this lesson, you’ll learn about renting cloud services that fit your analytic needs, and how to interact with one of those services (AWS) via the command line.","tags":["Analysis","Cloud Computing","Data","Education","Open Scholarship Tools and Technologies","Reproducibility","Research Data Management Tools","Researchers"],"title":"Introduction to Cloud Computing for Genomics","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"b91e4d5137a0c7f3638a2c46ec73a61c","permalink":"https://forrt.org/curated_resources/introduction-to-geospatial-concepts/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/introduction-to-geospatial-concepts/","section":"curated_resources","summary":"Data Carpentry lesson to understand data structures and common storage and transfer formats for spatial data. The goal of this lesson is to provide an introduction to core geospatial data concepts. It is intended for learners who have no prior experience working with geospatial data, and as a pre-requisite for the R for Raster and Vector Data lesson . This lesson can be taught in approximately 75 minutes and covers the following topics: Introduction to raster and vector data format and attributes Examples of data types commonly stored in raster vs vector format Introduction to categorical vs continuous raster data and multi-layer rasters Introduction to the file types and R packages used in the remainder of this workshop Introduction to coordinate reference systems and the PROJ4 format Overview of commonly used programs and applications for working with geospatial data The Introduction to R for Geospatial Data lesson provides an introduction to the R programming language while the R for Raster and Vector Data lesson provides a more in-depth introduction to visualization (focusing on geospatial data), and working with data structures unique to geospatial data. The R for Raster and Vector Data lesson assumes that learners are already familiar with both geospatial data concepts and the core concepts of the R language.","tags":["Analysis","Data","Education","Geospatial","Open Scholarship Tools and Technologies","R","Reproducibility","Research Data Management Tools","Researchers","Spatial"],"title":"Introduction to Geospatial Concepts","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"dace038da7dca701df075f5719cdc2fc","permalink":"https://forrt.org/curated_resources/introduction-to-geospatial-raster-and-ve/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/introduction-to-geospatial-raster-and-ve/","section":"curated_resources","summary":"Data Carpentry lesson to open, work with, and plot vector and raster-format spatial data in R. The episodes in this lesson cover how to open, work with, and plot vector and raster-format spatial data in R. Additional topics include working with spatial metadata (extent and coordinate reference systems), reprojecting spatial data, and working with raster time series data.","tags":["Analysis","Data","Education","Geospatial","Open Scholarship Tools and Technologies","R","Reproducibility","Research Data Management Tools","Researchers","Spatial"],"title":"Introduction to Geospatial Raster and Vector Data with R","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"d8286da6b61e0d34378cedab64c4ed8f","permalink":"https://forrt.org/curated_resources/introduction-to-git-github/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/introduction-to-git-github/","section":"curated_resources","summary":"This workshop introduces the basic concepts of Git version control. Whether you're new to version control or just need an explanation of Git and GitHub, this two hour tutorial will help you understand the concepts of distributed version control. Get to know basic Git concepts and GitHub workflows through step-by-step lessons. We'll even rewrite a bit of history, and touch on how to undo (almost) anything with Git. This is a class for users who are comfortable with a command-line interface.","tags":["Analysis","Open Scholarship Tools and Technologies","Research Data Management Tools","Researchers"],"title":"Introduction to Git \u0026 GitHub","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"1d034d51cc23107523bf5772a2294e54","permalink":"https://forrt.org/curated_resources/introduction-to-jupyter-notebooks/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/introduction-to-jupyter-notebooks/","section":"curated_resources","summary":"This class is designed for first-time and longer-term users of Jupyter Notebooks, a workspace for writing code. The class focuses on using Notebooks to facilitate sharing and publishing of script workflows. It aims to provide users with knowledge about shortcuts, plugins, and best practices for maximizing re-usability and shareability of Notebook contents.","tags":["Analysis","Open Scholarship Tools and Technologies","Research Data Management Tools","Researchers"],"title":"Introduction to Jupyter Notebooks","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1689122843,"objectID":"e0a370bf2c88447d9e0d82bf6fceec3b","permalink":"https://forrt.org/curated_resources/introduction-to-mediation-moderation-and/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/introduction-to-mediation-moderation-and/","section":"curated_resources","summary":"Explaining the fundamentals of mediation and moderation analysis, this engaging book also shows how to integrate the two using an innovative strategy known as conditional process analysis. Procedures are described for testing hypotheses about the mechanisms by which causal effects operate, the conditions under which they occur, and the moderation of mechanisms. Relying on the principles of ordinary least squares regression, Andrew Hayes carefully explains the estimation and interpretation of direct and indirect effects, probing and visualization of interactions, and testing of questions about moderated mediation. Examples using data from published studies illustrate how to conduct and report the analyses described in the book. Of special value, the book introduces and documents PROCESS, a macro for SPSS and SAS that does all the computations described in the book. The companion website (www.afhayes.com) offers free downloads of PROCESS plus data files for the book's examples.","tags":["Book"],"title":"Introduction to Mediation, Moderation, and Conditional Process Analysis, First Edition: A Regression-Based Approach (Methodology in the Social Sciences)","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1689122843,"objectID":"2b3a17257636d0a8caffc932d42db30c","permalink":"https://forrt.org/curated_resources/introduction-to-meta-analysis/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/introduction-to-meta-analysis/","section":"curated_resources","summary":"A book about meta analyses","tags":[""],"title":"Introduction to Meta-Analysis","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"37fa59d20a43836178d3812120ff9fec","permalink":"https://forrt.org/curated_resources/introduction-to-open-science-principles/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/introduction-to-open-science-principles/","section":"curated_resources","summary":"This series of lecture slides were used during the \"Advanced Simulation for Health Economic Analysis\" course at the University of Twente, which took place in February - May 2023.\n\nThe following Open Science topics were introduced to the course participants. All these topics are included in the current lecture slides series.\n\n1. Introduction to the concept of Open Science, and its relation to Health Economics  and Outcomes Research and Open Source Modelling\n2. Introduction to R and good coding practices\n3. Introduction to version control system\n4. Introduction to reproducible research and creating reproducible reports using R markdown\n5. Introduction to the FAIR principles and data management\n6. Introduction to academic publishing process and types of publications\n7. Introduction to public outreach and letting students design and plan a public outreach activity","tags":["Health Economics","Open Science","Introduction"],"title":"Introduction to Open Science principles and practices (in Health Economics)","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"2b9783ad8536e7c87f07dfa9acba0e2d","permalink":"https://forrt.org/curated_resources/introduction-to-power/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/introduction-to-power/","section":"curated_resources","summary":"This video is an introduction to power analyses to improve the reproducibility of your research.","tags":["Analysis","Data","Education","Materials","Open Scholarship Tools and Technologies","OSF","Publishing","Reproducibility","Research Data Management Tools","Researchers","Statistics"],"title":"Introduction to power","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"766fe5e02ad785379de76b3a94bffa47","permalink":"https://forrt.org/curated_resources/introduction-to-power-analyses-in-r/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/introduction-to-power-analyses-in-r/","section":"curated_resources","summary":"This video will introduce how to calculate statistical power in R using the pwr package. \n\nAll materials shown in the video, as well as content from our other videos, can be found here: https://osf.io/7gqsi/.","tags":["Analysis","Reproducibility"],"title":"Introduction to Power Analyses in R","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"102acf973a7ba73d2b33a420c8a238a4","permalink":"https://forrt.org/curated_resources/introduction-to-preprints/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/introduction-to-preprints/","section":"curated_resources","summary":"This is a recording of a 45 minute introductory webinar on preprints. With our guest speaker Philip Cohen, we’ll cover what preprints/postprints are, the benefits of preprints, and address some common concerns researcher may have. We’ll show how to determine whether you can post preprints/postprints, and also demonstrate how to use OSF preprints (https://osf.io/preprints/) to share preprints. The OSF is the flagship product of the Center for Open Science, a non-profit technology start-up dedicated to improving the alignment between scientific values and scientific practices. Learn more at cos.io and osf.io, or email contact@cos.io.","tags":["Analysis","Data","Education","Materials","Open Scholarship Tools and Technologies","OSF","Publishing","Reproducibility","Research Data Management Tools","Researchers"],"title":"Introduction to Preprints","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"a4cef5b05e539642f81006556580d711","permalink":"https://forrt.org/curated_resources/introduction-to-r-for-geospatial-data/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/introduction-to-r-for-geospatial-data/","section":"curated_resources","summary":"The goal of this lesson is to provide an introduction to R for learners working with geospatial data. It is intended as a pre-requisite for the R for Raster and Vector Data lesson for learners who have no prior experience using R. This lesson can be taught in approximately 4 hours and covers the following topics: Working with R in the RStudio GUI Project management and file organization Importing data into R Introduction to R’s core data types and data structures Manipulation of data frames (tabular data) in R Introduction to visualization Writing data to a file The the R for Raster and Vector Data lesson provides a more in-depth introduction to visualization (focusing on geospatial data), and working with data structures unique to geospatial data.","tags":["Analysis","Data","Education","Geospatial","Open Scholarship Tools and Technologies","R","Reproducibility","Research Data Management Tools","Researchers"],"title":"Introduction to R for Geospatial Data","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"c4fae06f4ca4933a06f28b7a486acba4","permalink":"https://forrt.org/curated_resources/introduction-to-research-data-management/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/introduction-to-research-data-management/","section":"curated_resources","summary":"An introduction to the concepts and best practices of research data management.","tags":["Open Scholarship Guidelines","Research Data Management","Researchers","ResearchersOpen Scholarship Guidelines"],"title":"Introduction to Research Data Management","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"01da164659ae3f0a372c0eb698d599b5","permalink":"https://forrt.org/curated_resources/introduction-to-simulations/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/introduction-to-simulations/","section":"curated_resources","summary":"Introduction to simulation - first of a series of short lectures given at University of Oxford in February 2021. Very basic for those with little or no background, starting with simulation in Excel. In part 1, we simulate random data to show how easy it is to get a 'significant' effect if you adopt methods of p-hacking. In part 2, we simulate data with a real group difference and show how you can fail to find the effect in a sample if you have insufficient statistical power. Syllabus: https://docs.google.com/document/d/1Ho6Sm1hZVZfKzXnxMhx-AC-c_6uFQmLx/edit ","tags":["Simulation"],"title":"Introduction to simulations","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"4f3adb567dfe90b02c94a1ae47c94dd2","permalink":"https://forrt.org/curated_resources/introduction-to-the-command-line-for-eco/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/introduction-to-the-command-line-for-eco/","section":"curated_resources","summary":"Command line interface (OS shell) and graphic user interface (GUI) are different ways of interacting with a computer’s operating system. The shell is a program that presents a command line interface which allows you to control your computer using commands entered with a keyboard instead of controlling graphical user interfaces (GUIs) with a mouse/keyboard combination. There are quite a few reasons to start learning about the shell: The shell gives you power. The command line gives you the power to do your work more efficiently and more quickly. When you need to do things tens to hundreds of times, knowing how to use the shell is transformative. To use remote computers or cloud computing, you need to use the shell.","tags":["Analysis","Data","Education","Open Scholarship Tools and Technologies","Reproducibility","Research Data Management Tools","Researchers","Shell"],"title":"Introduction to the Command Line for Economics","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"0549bd8d896fa184473f69a62f611629","permalink":"https://forrt.org/curated_resources/introduction-to-the-command-line-for-gen/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/introduction-to-the-command-line-for-gen/","section":"curated_resources","summary":"Data Carpentry lesson to learn to navigate your file system, create, copy, move, and remove files and directories, and automate repetitive tasks using scripts and wildcards with genomics data. Command line interface (OS shell) and graphic user interface (GUI) are different ways of interacting with a computer’s operating system. The shell is a program that presents a command line interface which allows you to control your computer using commands entered with a keyboard instead of controlling graphical user interfaces (GUIs) with a mouse/keyboard combination. There are quite a few reasons to start learning about the shell: For most bioinformatics tools, you have to use the shell. There is no graphical interface. If you want to work in metagenomics or genomics you’re going to need to use the shell. The shell gives you power. The command line gives you the power to do your work more efficiently and more quickly. When you need to do things tens to hundreds of times, knowing how to use the shell is transformative. To use remote computers or cloud computing, you need to use the shell.","tags":["Analysis","Data","Education","Open Scholarship Tools and Technologies","Reproducibility","Research Data Management Tools","Researchers","Shell"],"title":"Introduction to the Command Line for Genomics","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"56bcc3c361762ee25c562e092d2a1f60","permalink":"https://forrt.org/curated_resources/introduction-to-web-scraping/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/introduction-to-web-scraping/","section":"curated_resources","summary":"Web scraping is the process of extracting data from websites. Some data that is available on the web is presented in a format that makes it easier to collect and use it, for example in the form of downloadable comma-separated values (CSV) datasets that can then be imported in a spreadsheet or loaded into a data analysis script. Often however, even though it is publicly available, data is not readily available for reuse. For example it can be contained in a PDF, or a table on a website, or spread across multiple web pages. There are a variety of ways to scrape a website to extract information for reuse. In its simplest form, this can be achieved by copying and pasting snippets from a web page, but this can be unpractical if there is a large amount of data to be extracted, or if it spread over a large number of pages. Instead, specialized tools and techniques can be used to automate this process, by defining what sites to visit, what information to look for, and whether data extraction should stop once the end of a page has been reached, or whether to follow hyperlinks and repeat the process recursively. Automating web scraping also allows to define whether the process should be run at regular intervals and capture changes in the data.","tags":["Analysis","Data","Education","Open Scholarship Tools and Technologies","Reproducibility","Research Data Management Tools","Researchers","Web Scraping"],"title":"Introduction to web scraping","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1689122843,"objectID":"3a19d69fa32cc898b6fce4aeb67e86c9","permalink":"https://forrt.org/curated_resources/investigating-variation-in-replicability/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/investigating-variation-in-replicability/","section":"curated_resources","summary":"Although replication is a central tenet of science, direct replications are rare in psychology. This research tested variation in the replicability of 13 classic and contemporary effects across 36 independent samples totaling 6,344 participants. In the aggregate, 10 effects replicated consistently. One effect – imagined contact reducing prejudice – showed weak support for replicability. And two effects – flag priming influencing conservatism and currency priming influencing system justification – did not replicate. We compared whether the conditions such as lab versus online or US versus international sample predicted effect magnitudes. By and large they did not. The results of this small sample of effects suggest that replicability is more dependent on the effect itself than on the sample and setting used to investigate the effect.","tags":["Reproducibility Crisis and Credibility Revolution","Open Science"],"title":"Investigating Variation in Replicability: A “Many Labs” Replication Project","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1689122843,"objectID":"a4a82d491a938671890a7d0d344a7619","permalink":"https://forrt.org/curated_resources/investigation-and-its-discontents-some-c/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/investigation-and-its-discontents-some-c/","section":"curated_resources","summary":"Examines several prominent trends in the conduct of psychological research and considers how they may limit progress in the field. Failure to appreciate important differences in temperament among researchers, and differences in the particular talents researchers bring to their work have prevented the development in psychology of a vigorous tradition of fruitful theoretical inquiry. Misplaced emphasis on quantitative \"productivity,\" a problem for all disciplines, is shown to have particularly unfortunate results in psychology. Problems associated with the distorting effects of seeking grant support are shown to interact with the first two difficulties. Finally, the distorting effects of certain kinds of experimental studies are discussed, together with their implications for progress in this field.","tags":["Reproducibility Crisis and Credibility Revolution","Open Science"],"title":"Investigation and its discontents: Some constraints on progress in psychological research. ","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1689122843,"objectID":"5134e255248f9da5d175818d0a8609c4","permalink":"https://forrt.org/curated_resources/ioannidis-j-p-a-2012-why-science-is-not/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/ioannidis-j-p-a-2012-why-science-is-not/","section":"curated_resources","summary":"The ability to self-correct is considered a hallmark of science. However, self-correction does not always happen to scientific evidence by default. The trajectory of scientific credibility can fluctuate over time, both for defined scientific fields and for science at-large. History suggests that major catastrophes in scientific credibility are unfortunately possible and the argument that “it is obvious that progress is made” is weak. Careful evaluation of the current status of credibility of various scientific fields is important in order to understand any credibility deficits and how one could obtain and establish more trustworthy results. Efficient and unbiased replication mechanisms are essential for maintaining high levels of scientific credibility. Depending on the types of results obtained in the discovery and replication phases, there are different paradigms of research: optimal, self-correcting, false nonreplication, and perpetuated fallacy. In the absence of replication efforts, one is left with unconfirmed (genuine) discoveries and unchallenged fallacies. In several fields of investigation, including many areas of psychological science, perpetuated and unchallenged fallacies may comprise the majority of the circulating evidence. I catalogue a number of impediments to self-correction that have been empirically studied in psychological science. Finally, I discuss some proposed solutions to promote sound replication practices enhancing the credibility of scientific results as well as some potential disadvantages of each of them. Any deviation from the principle that seeking the truth has priority over any other goals may be seriously damaging to the self-correcting functions of science.","tags":["Reproducibility Crisis and Credibility Revolution","Open Science"],"title":"Ioannidis, J. P. A. (2012). Why science is not necessarily self-correcting. Perspectives on Psychological Science, 7, 645-654.","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"ec922f01c7f8ecd8026af3657d83e8f6","permalink":"https://forrt.org/curated_resources/is-preregistration-worthwhile/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/is-preregistration-worthwhile/","section":"curated_resources","summary":"Proponents of preregistration argue that, among other benefits, it improves the diagnosticity of statistical tests. In the strong version of this argument, preregistration does this by solving statistical problems, such as family-wise error rates. In the weak version, it nudges people to think more deeply about their theories, methods, and analyses. We argue against both: the diagnosticity of statistical tests depend entirely on how well statistical models map onto underlying theories, and so improving statistical techniques does little to improve theories when the mapping is weak. There is also little reason to expect that preregistration will spontaneously help researchers to develop better theories (and, hence, better methods and analyses).","tags":["Preregistration","Theory Development","Inference"],"title":"Is Preregistration Worthwhile?","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1689122843,"objectID":"c7c94d2be6376d8d41acff6a0d942408","permalink":"https://forrt.org/curated_resources/is-psychology-suffering-from-a-replicati/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/is-psychology-suffering-from-a-replicati/","section":"curated_resources","summary":"Psychology has recently been viewed as facing a replication crisis because efforts to replicate past study findings frequently do not show the same result. Often, the first study showed a statistically significant result but the replication does not. Questions then arise about whether the first study results were false positives, and whether the replication study correctly indicates that there is truly no effect after all. This article suggests these so-called failures to replicate may not be failures at all, but rather are the result of low statistical power in single replication studies, and the result of failure to appreciate the need for multiple replications in order to have enough power to identify true effects. We provide examples of these power problems and suggest some solutions using Bayesian statistics and meta-analysis. Although the need for multiple replication studies may frustrate those who would prefer quick answers to psychology’s alleged crisis, the large sample sizes typically needed to provide firm evidence will almost always require concerted efforts from multiple investigators. As a result, it remains to be seen how many of the recently claimed failures to replicate will be supported or instead may turn out to be artifacts of inadequate sample sizes and single study replications.","tags":["Reproducibility Crisis and Credibility Revolution","Open Science"],"title":"Is Psychology Suffering From a Replication Crisis? What Does “Failure to Replicate” Really Mean?","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1689122843,"objectID":"a8f8e2a8cdf9172f7965e4758e338b6c","permalink":"https://forrt.org/curated_resources/is-the-call-to-abandon-p-values-the-red/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/is-the-call-to-abandon-p-values-the-red/","section":"curated_resources","summary":"A paper about Is the call to abandon p-values the red herring of the replicability crisis?","tags":[""],"title":"Is the call to abandon p-values the red herring of the replicability crisis? ","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1689122843,"objectID":"b4b9450a1474a66551ebf46658c4560a","permalink":"https://forrt.org/curated_resources/is-the-replicability-crisis-overblown-th/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/is-the-replicability-crisis-overblown-th/","section":"curated_resources","summary":"We discuss three arguments voiced by scientists who view the current outpouring of concern about replicability as overblown. The first idea is that the adoption of a low alpha level (e.g., 5%) puts reasonable bounds on the rate at which errors can enter the published literature, making false-positive effects rare enough to be considered a minor issue. This, we point out, rests on statistical misunderstanding: The alpha level imposes no limit on the rate at which errors may arise in the literature (Ioannidis, 2005b). Second, some argue that whereas direct replication attempts are uncommon, conceptual replication attempts are common—providing an even better test of the validity of a phenomenon. We contend that performing conceptual rather than direct replication attempts interacts insidiously with publication bias, opening the door to literatures that appear to confirm the reality of phenomena that in fact do not exist. Finally, we discuss the argument that errors will eventually be pruned out of the literature if the field would just show a bit of patience. We contend that there are no plausible concrete scenarios to back up such forecasts and that what is needed is not patience, but rather systematic reforms in scientific practice.","tags":["Reproducibility Crisis and Credibility Revolution","Open Science"],"title":"Is the replicability crisis overblown? Three arguments examined","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1689122843,"objectID":"1d128ad3b2f0f3576ec0fb180b3c0303","permalink":"https://forrt.org/curated_resources/is-there-a-credibility-crisis-in-strateg/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/is-there-a-credibility-crisis-in-strateg/","section":"curated_resources","summary":"Recent studies report an inability to replicate previously published research, leading some to suggest that scientific knowledge is facing a credibility crisis. In this essay, we provide evidence on whether strategic management research may itself be vulnerable to these concerns. We conducted a study whereby we attempted to reproduce the empirical findings of 88 articles appearing in the Strategic Management Journal using data reported in the articles themselves. About 70% of the studies did not disclose enough data to permit independent tests of reproducibility of their findings. Of those that could be retested, almost one-third reported hypotheses as statistically significant which were no longer so and far more significant results were found to be non-significant in the reproductions than in the opposite direction. Collectively, incomplete reporting practices, disclosure errors, and possible opportunism limit the reproducibility of most studies. Until disclosure standards and requirements change to include more complete reporting and facilitate tests of reproducibility, the strategic management field appears vulnerable to a credibility crisis.","tags":["Reproducibility Crisis and Credibility Revolution","Open Science"],"title":"Is there a credibility crisis in strategic management research? Evidence on the reproducibility of study findings","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1689122843,"objectID":"bcaea72378010021c014ac40c5a5df6c","permalink":"https://forrt.org/curated_resources/is-there-a-free-lunch-in-inference/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/is-there-a-free-lunch-in-inference/","section":"curated_resources","summary":"The field of psychology, including cognitive science, is vexed by a crisis of confidence. Although the causes and solutions are varied, we focus here on a common logical problem in inference. The default mode of inference is significance testing, which has a free lunch property where researchers need not make detailed assumptions about the alternative to test the null hypothesis. We present the argument that there is no free lunch; that is, valid testing requires that researchers test the null against a well-specified alternative. We show how this requirement follows from the basic tenets of conventional and Bayesian probability. Moreover, we show in both the conventional and Bayesian framework that not specifying the alternative may lead to rejections of the null hypothesis with scant evidence. We review both frequentist and Bayesian approaches to specifying alternatives, and we show how such specifications improve inference. The field of cognitive science will benefit because consideration of reasonable alternatives will undoubtedly sharpen the intellectual underpinnings of research.","tags":["Reproducibility Crisis and Credibility Revolution","Open Science","Transparency"],"title":"Is There a Free Lunch in Inference?","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"25c8b6db2ff984cc68b0163389f49537","permalink":"https://forrt.org/curated_resources/issues-in-the-registration-of-clinical-t/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/issues-in-the-registration-of-clinical-t/","section":"curated_resources","summary":"Public concerns about the perils associated with incomplete or delayed reporting of results from clinical trials has heightened interest in trial registries and results databases. Here we review the current status of trial registration efforts and the challenges in developing a comprehensive system of trial registration and reporting of results. ClinicalTrials.gov, the largest trial registry with 36 249 trials from approximately 140 countries, has procedures in place to help ensure that records are valid and informative. Key challenges include the need to minimize inadvertent duplicate registrations, to ensure that interventions have unambiguous names, and to have a search engine that identifies all trials that meet a user's specifications. Recent policy initiatives have called for the development of a database of trial results. Several issues confound the implementation of such a database, including the lack of an accepted format or process for providing summaries of trial results to the public and concerns about disseminating data in the absence of independent scientific review.","tags":["Preregistration","Clinical Trials"],"title":"Issues in the Registration of Clinical Trials","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1689122843,"objectID":"d6a04826cf638df60d694601927aab9f","permalink":"https://forrt.org/curated_resources/it-s-time-to-broaden-the-replicability-c/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/it-s-time-to-broaden-the-replicability-c/","section":"curated_resources","summary":"Psychology is in the early stages of examining a crisis of replicability stemming from several high-profile failures to replicate studies in experimental psychology. This important conversation has largely been focused on social psychology, with some active participation from cognitive psychology. Nevertheless, several other major domains of psychological science—including clinical science—have remained insulated from this discussion. The goals of this article are to (a) examine why clinical psychology and allied fields, such as counseling and school psychology, have not been central participants in the replicability conversation; (b) review concerns and recommendations that are less (or more) applicable to or appropriate for research in clinical psychology and allied fields; and (c) generate take-home messages for scholars and consumers of the literature in clinical psychology and allied fields, as well as reviewers, editors, and colleagues from other areas of psychological science.","tags":["Reproducibility Crisis and Credibility Revolution","Open Science"],"title":"It’s Time to Broaden the Replicability Conversation: Thoughts for and From Clinical Psychological Science","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1721390377,"objectID":"f6d6bddcd81c84cf32b60697a0923e85","permalink":"https://forrt.org/glossary/english/jabref/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/english/jabref/","section":"glossary","summary":"","tags":null,"title":"JabRef","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1720865851,"objectID":"a79302b8780c0cdb7a7baf96fa6ed2c8","permalink":"https://forrt.org/glossary/german/jabref/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/german/jabref/","section":"glossary","summary":"","tags":null,"title":"JabRef","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1635961284,"objectID":"ab2715b0433f34cc665db59f1200df12","permalink":"https://forrt.org/glossary/vbeta/jabref/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/vbeta/jabref/","section":"glossary","summary":"","tags":null,"title":"JabRef","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1721390377,"objectID":"0611b732194239b3adecb1a163a82d85","permalink":"https://forrt.org/glossary/english/jamovi/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/english/jamovi/","section":"glossary","summary":"","tags":null,"title":"Jamovi","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1720865851,"objectID":"f942c87c512bd6b3b7a96ce847c44f2f","permalink":"https://forrt.org/glossary/german/jamovi/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/german/jamovi/","section":"glossary","summary":"","tags":null,"title":"Jamovi","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1635961284,"objectID":"09f98b872d923e580bf02485377310aa","permalink":"https://forrt.org/glossary/vbeta/jamovi/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/vbeta/jamovi/","section":"glossary","summary":"","tags":null,"title":"Jamovi","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1721390377,"objectID":"c6871d842ef469a1114ea82507fce3b2","permalink":"https://forrt.org/glossary/english/jasp/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/english/jasp/","section":"glossary","summary":"","tags":null,"title":"JASP","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1720865851,"objectID":"9329d68756b0848e2a13033b881d7401","permalink":"https://forrt.org/glossary/german/jasp/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/german/jasp/","section":"glossary","summary":"","tags":null,"title":"JASP","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1635961284,"objectID":"4296a168a838c28ce1712f76eff44a49","permalink":"https://forrt.org/glossary/vbeta/jasp/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/vbeta/jasp/","section":"glossary","summary":"","tags":null,"title":"JASP","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1721390377,"objectID":"8364ab2748ccf41721dc418928ceca2a","permalink":"https://forrt.org/glossary/english/journal_impact_factor_/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/english/journal_impact_factor_/","section":"glossary","summary":"","tags":null,"title":"Journal Impact Factor™","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1635961284,"objectID":"b5452d828d47049f66328b91b18acd7d","permalink":"https://forrt.org/glossary/vbeta/journal-impact-factor/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/vbeta/journal-impact-factor/","section":"glossary","summary":"","tags":null,"title":"Journal Impact Factor™","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1720865851,"objectID":"dfb7e538d199747631e6f60d9ff58314","permalink":"https://forrt.org/glossary/german/journal_impact_factor_/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/german/journal_impact_factor_/","section":"glossary","summary":"","tags":null,"title":"Journal Impact Factor™ (Zeitschriften-Impact-Faktor)","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"cda78e56689d5d59bc9d28fd118657b7","permalink":"https://forrt.org/curated_resources/joy-and-rigor-in-behavioral-science/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/joy-and-rigor-in-behavioral-science/","section":"curated_resources","summary":"In the past decade, behavioral science has seen the introduction of beneficial reforms to reduce false positive results. Serving as the motivational backdrop for the present research, we wondered whether these reforms might have unintended negative consequences for researchers’ behavior and emotional experiences. In an experiment simulating the research process, Study 1 (N = 449 researchers) suggested that engaging in a pre-registration task impeded the discovery of an interesting but non-hypothesized result. Study 2 (N = 400 researchers) indicated that relative to confirmatory research, researchers found exploratory research more enjoyable, motivating, and interesting; and less anxiety-inducing, frustrating, boring, and scientific. These studies raise the possibility that emphasizing confirmation can shift researchers away from exploration, and that such a shift could degrade the subjective experience of conducting research. Study 3 (N = 314 researchers) introduced a scale to measure “prediction preoccupation”—the feeling of heightened concern over, and fixation with, confirming predictions.","tags":["Kindness"],"title":"Joy and rigor in behavioral science","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1721390377,"objectID":"4d8b48c6c2e3657c955159f4170b8e84","permalink":"https://forrt.org/glossary/english/json_file/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/english/json_file/","section":"glossary","summary":"","tags":null,"title":"JSON file","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1720865851,"objectID":"0d534fbcc421ab778a61cc85f9ba68fb","permalink":"https://forrt.org/glossary/german/json_file/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/german/json_file/","section":"glossary","summary":"","tags":null,"title":"JSON file","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1635961284,"objectID":"625737c579435ab9edc1f49b70b9eb1f","permalink":"https://forrt.org/glossary/vbeta/json-file/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/vbeta/json-file/","section":"glossary","summary":"","tags":null,"title":"JSON file","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"cf716eeb6184bd98b3ff811d0a0b0627","permalink":"https://forrt.org/curated_resources/jupyter-notebooks-with-r-git/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/jupyter-notebooks-with-r-git/","section":"curated_resources","summary":"Today we are going to learn the basics of literate programming using Jupyter Notebooks, a popular tool in data science, with the R kernel, so we can run R code in our notebooks. We’ll then take a look at how we use Git and GitHub to keep track of all the versions of our work, collaborate with others, and be open!","tags":["Git","GitHub","Jupyter Notebooks","Open Scholarship Tools and Technologies","R","Reproducibility","Research Data Management Tools","Researchers"],"title":"Jupyter Notebooks with R \u0026 Git","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1689122843,"objectID":"105e4a1abab3e7bf2edbbc8c0134aff2","permalink":"https://forrt.org/curated_resources/just-post-it-the-lesson-from-two-cases-o/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/just-post-it-the-lesson-from-two-cases-o/","section":"curated_resources","summary":"I argue that requiring authors to post the raw data supporting their published results has the benefit, among many others, of making fraud much less likely to go undetected. I illustrate this point by describing two cases of suspected fraud I identified exclusively through statistical analysis of reported means and standard deviations. Analyses of the raw data behind these published results provided invaluable confirmation of the initial suspicions, ruling out benign explanations (e.g., reporting errors, unusual distributions), identifying additional signs of fabrication, and also ruling out one of the suspected fraud’s explanations for his anomalous results. If journals, granting agencies, universities, or other entities overseeing research promoted or required data posting, it seems inevitable that fraud would be reduced","tags":[""],"title":"Just Post It: The Lesson From Two Cases of Fabricated Data Detected by Statistics Alone","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1689122843,"objectID":"0eb664364811b2d74e4124f9d13078ce","permalink":"https://forrt.org/curated_resources/justify-your-alpha/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/justify-your-alpha/","section":"curated_resources","summary":"In response to recommendations to redefine statistical significance to P ≤ 0.005, we propose that researchers should transparently report and justify all choices they make when designing a study, including the alpha level.","tags":[""],"title":"Justify your alpha","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"b47be71f18984bc7379023020e40d9ca","permalink":"https://forrt.org/curated_resources/key-practices-for-the-language-scientist/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/key-practices-for-the-language-scientist/","section":"curated_resources","summary":"Materials for the Key Practices for the Language Scientist course taught from January to March 2020 at the Max Planck Institute for Psycholinguistics in Nijmegen.","tags":["Teaching"],"title":"Key Practices for the Language Scientist","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1721390377,"objectID":"1047ae880a6c12cadf356f5ab3d1ef81","permalink":"https://forrt.org/glossary/english/knowledge_acquisition/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/english/knowledge_acquisition/","section":"glossary","summary":"","tags":null,"title":"Knowledge acquisition","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1720865851,"objectID":"9627d212ced803bcb215118347475c60","permalink":"https://forrt.org/glossary/german/knowledge_acquisition/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/german/knowledge_acquisition/","section":"glossary","summary":"","tags":null,"title":"Knowledge acquisition","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1635961284,"objectID":"dd4e8c2d945dfe9b2e01ad823f1146c9","permalink":"https://forrt.org/glossary/vbeta/knowledge-acquisition/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/vbeta/knowledge-acquisition/","section":"glossary","summary":"","tags":null,"title":"Knowledge acquisition","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"f686722c18b3cfac67c4cb2a93f38af0","permalink":"https://forrt.org/curated_resources/l-autorat-et-nous-de-la-collaboration-au/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/l-autorat-et-nous-de-la-collaboration-au/","section":"curated_resources","summary":"La notion d’autorat est devenue incontournable dans le monde actuel de la recherche et le contexte d’évaluation principalement quantitative dans laquelle elle s’inscrit. Après un bref exposé des dérives que la notion exacerbée d’autorat peut engendrer dans un contexte de course à la publication (autorat fantôme, obsession des métriques et phénomènes de coercition de citations, achat d’autorat et production à grande échelle d’études factices par le biais de paper mills, etc.), cet exposé dressera un aperçu de certains mécanismes et positionnements collectifs qui visent à court-circuiter la notion d’autorat individuel. Une attention particulière sera apportée à la création et l’évolution du personnage fictif Camille Noûs, un collectif scientifique qui revendique explicitement rejeter la “gestion managériale et bibliométrique de la recherche”. Enfin, l’exposé conclura en présentant certaines démarches et outils concrets pour prévenir les conflits d’autorat et les nombreux rapports de force dont ils peuvent faire l'objet.\n\nThe concept of authorship has become essential in the current research world and the primarily quantitative evaluation context in which it operates. After a brief discussion of the excesses that an exaggerated notion of authorship can lead to in the race for publication (ghost authorship, obsession with metrics, coercive citation practices, purchase of authorship, and large-scale production of fake studies through paper mills, etc.), this presentation will provide an overview of certain mechanisms and collective positions aimed at bypassing the notion of individual authorship. Special attention will be given to the creation and evolution of the fictional character Camille Noûs, a scientific collective that explicitly claims to reject the “managerial and bibliometric management of research.” Finally, the presentation will conclude by introducing certain approaches and concrete tools to prevent authorship conflicts and the numerous power dynamics they can involve.\n","tags":["authorship; research ethics; publication ethics; research integrity"],"title":"L'autorat et \"Noûs\" : De la collaboration au rapport de force dans la production scientifique","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"4eab73aebec3ab4f2389c3a9b7343727","permalink":"https://forrt.org/curated_resources/la-terminal-de-unix/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/la-terminal-de-unix/","section":"curated_resources","summary":"Software Carpentry lección para la terminal de Unix La terminal de Unix ha existido por más tiempo que la mayoría de sus usuarios. Ha sobrevivido tanto tiempo porque es una herramienta poderosa que permite a las personas hacer cosas complejas con sólo unas pocas teclas. Lo más importante es que ayuda a combinar programas existentes de nuevas maneras y automatizar tareas repetitivas, en vez de estar escribiendo las mismas cosas una y otra vez. El uso del terminal o shell es fundamental para usar muchas otras herramientas poderosas y recursos informáticos (incluidos los supercomputadores o “computación de alto rendimiento”). Esta lección te guiará en el camino hacia el uso eficaz de estos recursos.","tags":["Analysis","Data","Education","Open Scholarship Tools and Technologies","Reproducibility","Research Data Management Tools","Researchers","Shell"],"title":"La Terminal de Unix","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"a4b400f54b07c340f919cb482c0999bd","permalink":"https://forrt.org/curated_resources/lab-js-online-research-made-easy/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/lab-js-online-research-made-easy/","section":"curated_resources","summary":"lab.js is a free, open, online study builder for the behavioral and cognitive sciences. (it works great in the lab, too)","tags":["Educators","Open Source","Open Source Software","Researchers"],"title":"lab.js Online research made easy","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"5fdba18b98d05ec24c47315cd9f40321","permalink":"https://forrt.org/curated_resources/landmark-college-institute-for-research/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/landmark-college-institute-for-research/","section":"curated_resources","summary":"LCIRT was established in 2001 to pioneer LD research, discover innovative strategies and practices, and improve teaching and learning outcomes for students with learning disabilities (like dyslexia), ADHD, and autism spectrum disorder (ASD), and educators in high school and college settings. Currently, the Institute staff shares this information with education professionals through webinars, online certificate courses, on-site and online workshops, and the signature Summer Institute for educators, among other activities. Fully integrated within the College, LCIRT is instrumental in promoting and leveraging the knowledge and expertise of Landmark College's faculty and staff. The Neurocognitive Lab, which includes vitual reality (VR) equipment, makes LCIRT a popular environment for student research.","tags":["Diversity","Equity","Inclusion","Neurodiversity"],"title":"Landmark College Institute for Research \u0026 Training (LCIRT) ","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"1fcb140ccdcaf1578ece31503ecb42b1","permalink":"https://forrt.org/curated_resources/last-contact-a-science-adventure/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/last-contact-a-science-adventure/","section":"curated_resources","summary":"Last Contact is a game is a game that is intended to inform people about the open science movement. In the game players encounter common problems of current scientific practice in form of a parody science fiction scenario. The problems that players encounter are just some of the problems that the Open Science Movement tries to solve.\nOpen science is the movement to make scientific research (including publications, data, physical samples, and software) and its dissemination accessible to all levels of society, amateur or professional. Open science is transparent and accessible knowledge that is shared and developed through collaborative networks. It encompasses practices such as publishing open research, campaigning for open access, encouraging scientists to practice open-notebook science (such as openly sharing data and code, broader dissemination and engagement in science and generally making it easier to publish, access and communicate scientific knowledge.","tags":["Puzzle","Escape Room"],"title":"Last Contact: A Science Adventure","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1689122843,"objectID":"dfdf97bc71ce7f1eeea881bfd762fa90","permalink":"https://forrt.org/curated_resources/last-week-tonight-with-john-oliver-scien/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/last-week-tonight-with-john-oliver-scien/","section":"curated_resources","summary":"A video about scientific studies","tags":["Video"],"title":"Last Week Tonight with John Oliver: Scientific Studies (HBO)","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1689122843,"objectID":"a78505ebc53d79a2d7338bd033f5671c","permalink":"https://forrt.org/curated_resources/latent-variable-modeling-with-r/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/latent-variable-modeling-with-r/","section":"curated_resources","summary":"This book demonstrates how to conduct latent variable modeling (LVM) in R by highlighting the features of each model, their specialized uses, examples, sample code and output, and an interpretation of the results. Each chapter features a detailed example including the analysis of the data using R, the relevant theory, the assumptions underlying the model, and other statistical details to help readers better understand the models and interpret the results. Every R command necessary for conducting the analyses is described along with the resulting output which provides readers with a template to follow when they apply the methods to their own data. The basic information pertinent to each model, the newest developments in these areas, and the relevant R code to use them are reviewed. Each chapter also features an introduction, summary, and suggested readings. A glossary of the text’s boldfaced key terms and key R commands serve as helpful resources. The book is accompanied by a website with exercises, an answer key, and the in-text example data sets.","tags":["Book"],"title":"Latent Variable Modeling with R","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1689122843,"objectID":"97f8908a10074999a12014fcb85fe4ed","permalink":"https://forrt.org/curated_resources/learn-stats-with-jamovi/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/learn-stats-with-jamovi/","section":"curated_resources","summary":"A statistical tutorial about using Jamovi ","tags":["Statistical Book","Tutorial"],"title":"Learn Stats with Jamovi","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"00ef2cfa612bd49a65ce0b7e195e67f3","permalink":"https://forrt.org/curated_resources/learning-statistics-with-r/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/learning-statistics-with-r/","section":"curated_resources","summary":"The book is associated with the lsr package on CRAN and GitHub. The package is probably okay for many introductory teaching purposes, but some care is required. The package does have some limitations (e.g., the etaSquared function does strange things for unbalanced ANOVA designs), and it has not been updated in a while.","tags":["Statistics"],"title":"Learning Statistics with R","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"4274944ae971175b1c94db65130ed1b4","permalink":"https://forrt.org/curated_resources/leonardo-da-vinci-preregistration-and-th/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/leonardo-da-vinci-preregistration-and-th/","section":"curated_resources","summary":"There has been much talk of psychological science undergoing a renaissance with recent years being marked by dramatic changes in research practices and to the publishing landscape. This article briefly summarises a number of the ways in which psychological science can improve its rigor, lessen use of questionable research practices and reduce publication bias. The importance of preregistration as a useful tool to increase transparency of science and improve the robustness of our evidence base, especially in COVID-19 times, is presented. Moreover, the benefits of using Registered Reports, the article format that allows peer review of research studies before the results are known, are outlined. Finally, the article argues that the scientific architecture and the academic reward structure need to change with a move towards “slow science” and away from the “publish or perish” culture.","tags":["Preregistration","Health Psychology"],"title":"Leonardo da Vinci, preregistration and the Architecture of Science: Towards a More Open and Transparent Research Culture","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1689122843,"objectID":"8c1a8fdee9ac2c5a6215f60bc8b08589","permalink":"https://forrt.org/curated_resources/let-s-publish-fewer-papers/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/let-s-publish-fewer-papers/","section":"curated_resources","summary":"A paper about publishing few papers","tags":["Reproducibility Crisis and Credibility Revolution","Open Science"],"title":"Let's Publish Fewer Papers","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1689122843,"objectID":"4bc731e5497842428150e8e6f050893c","permalink":"https://forrt.org/curated_resources/let-s-put-our-money-where-our-mouth-is-i/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/let-s-put-our-money-where-our-mouth-is-i/","section":"curated_resources","summary":"A number of scholars recently have argued for fundamental changes in the way psychological scientists conduct and report research. The behavior of researchers is influenced partially by incentive structures built into the manuscript evaluation system, and change in researcher practices will necessitate a change in the way journal reviewers evaluate manuscripts. This article outlines specific recommendations for reviewers that are designed to facilitate open data reporting and to encourage researchers to disseminate the most generative and replicable studies. These recommendations include changing the way reviewers respond to imperfections in empirical data, focusing less on individual tests of statistical significance and more on meta-analyses, being more open to null findings and failures to replicate previous research, and attending carefully to the theoretical contribution of a manuscript in addition to its methodological rigor. The article also calls for greater training and guidance for reviewers so that they can evaluate research in a manner that encourages open reporting and ultimately strengthens our science.","tags":["Reproducibility Crisis and Credibility Revolution","Open Science"],"title":"Let’s Put Our Money Where Our Mouth Is: If Authors Are to Change Their Ways, Reviewers (and Editors) Must Change With Them","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"348b932cf1b53bf95b1742e045203180","permalink":"https://forrt.org/curated_resources/level-up-the-reproducibility-of-your-dat/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/level-up-the-reproducibility-of-your-dat/","section":"curated_resources","summary":"Purpose: To introduce methods and tools in organization, documentation, automation, and dissemination of research that nudge it further along the reproducibility spectrum.OutcomeParticipants feel more confident applying reproducibility methods and tools to their own research projects.ProcessParticipants practice new methods and tools with code and data during the workshop to explore what they do and how they might work in a research workflow. Participants can compare benefits of new practices and ask questions to help clarify which would provide them the most value to adopt.","tags":["Librarians","Reproducibility","Research Data Management Tools","Researchers"],"title":"Level up the reproducibility of your data and code! A 2-hour, hands-on workshop","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"c7b0af5a422a19816784efb082e05d25","permalink":"https://forrt.org/curated_resources/leveraging-open-ecosystems-to-enhance-re/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/leveraging-open-ecosystems-to-enhance-re/","section":"curated_resources","summary":"Open source infrastructure has paved the way for mission-aligned research stakeholders to create a united vision of interoperable tools and services that accelerate scholarly communication, fill technology gaps, converge solutions, and enable access and discoverability.\n\nHear from a panel of research groups that have taken advantage of interoperable infrastructure to leverage more robust workflows to support rigorous, reproducible research. We also discuss the steps stakeholders and institutions can take to integrate OSF’s open API with existing services to establish streamlined researcher workflows. \n\nView the slides from this presentation by visiting osf.io/ux7ed.","tags":["Center for Open Science","Jupyter","Open Code","Open Infrastructure","Open Research","Open Science","Open Science Framework","Open Source","OSF","Osfr","Protocols.io","Research","Research Integration","Research Tools"],"title":"Leveraging Open Ecosystems to Enhance Reproducible Workflows","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"ce55ec2b25d850be8d7112210f3fac57","permalink":"https://forrt.org/curated_resources/library-carpentry-introduction-to-git/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/library-carpentry-introduction-to-git/","section":"curated_resources","summary":"Library Carpentry lesson: An introduction to Git. What We Will Try to Do Begin to understand and use Git/GitHub. You will not be an expert by the end of the class. You will probably not even feel very comfortable using Git. This is okay. We want to make a start but, as with any skill, using Git takes practice. Be Excellent to Each Other If you spot someone in the class who is struggling with something and you think you know how to help, please give them a hand. Try not to do the task for them: instead explain the steps they need to take and what these steps will achieve. Be Patient With The Instructor and Yourself This is a big group, with different levels of knowledge, different computer systems. This isn’t your instructor’s full-time job (though if someone wants to pay them to play with computers all day they’d probably accept). They will do their best to make this session useful. This is your session. If you feel we are going too fast, then please put up a pink sticky. We can decide as a group what to cover.","tags":["Analysis","Data","Education","Git","Librarians","Open Scholarship Tools and Technologies","Reproducibility","Research Data Management Tools"],"title":"Library Carpentry: Introduction to Git","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"831ab7b682cf4fc4f4e5eba8b5db72b0","permalink":"https://forrt.org/curated_resources/library-carpentry-introduction-to-workin/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/library-carpentry-introduction-to-workin/","section":"curated_resources","summary":"This Library Carpentry lesson introduces librarians and others to working with data. This Library Carpentry lesson introduces people with library- and information-related roles to working with data using regular expressions. The lesson provides background on the regular expression language and how it can be used to match and extract text and to clean data.","tags":["Analysis","Data","Education","Librarians","Open Scholarship Tools and Technologies","Reproducibility","Research Data Management Tools"],"title":"Library Carpentry: Introduction to Working with Data (Regular Expressions)","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"23bedfc26ffd099b9429624bc4bb0624","permalink":"https://forrt.org/curated_resources/library-carpentry-openrefine/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/library-carpentry-openrefine/","section":"curated_resources","summary":"Library Carpentry lesson: an introduction to OpenRefine for Librarians This Library Carpentry lesson introduces people working in library- and information-related roles to working with data in OpenRefine. At the conclusion of the lesson you will understand what the OpenRefine software does and how to use the OpenRefine software to work with data files.","tags":["Analysis","Data","Education","Librarians","OpenRefine","Open Scholarship Tools and Technologies","Reproducibility","Research Data Management Tools"],"title":"Library Carpentry: OpenRefine","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"ba278c1ef410fb25e5203d2aee1fd3d5","permalink":"https://forrt.org/curated_resources/library-carpentry-sql/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/library-carpentry-sql/","section":"curated_resources","summary":"Library Carpentry, an introduction to SQL for Librarians This Library Carpentry lesson introduces librarians to relational database management system using SQLite. At the conclusion of the lesson you will: understand what SQLite does; use SQLite to summarise and link data.","tags":["Analysis","Data","Education","Librarians","Open Scholarship Tools and Technologies","Reproducibility","Research Data Management Tools","SQL"],"title":"Library Carpentry: SQL","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"96d66e3ced309ea0ebb547b36734bfa5","permalink":"https://forrt.org/curated_resources/library-carpentry-the-unix-shell/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/library-carpentry-the-unix-shell/","section":"curated_resources","summary":"Library Carpentry lesson to learn how to use the Shell. This Library Carpentry lesson introduces librarians to the Unix Shell. At the conclusion of the lesson you will: understand the basics of the Unix shell; understand why and how to use the command line; use shell commands to work with directories and files; use shell commands to find and manipulate data.","tags":["Analysis","Data","Education","Librarians","Open Scholarship Tools and Technologies","Reproducibility","Research Data Management Tools","Shell"],"title":"Library Carpentry: The UNIX Shell","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"ee1f67f877a4c1de560bb146c41f41f3","permalink":"https://forrt.org/curated_resources/licensing-your-research/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/licensing-your-research/","section":"curated_resources","summary":"Join us for a 30 minute guest webinar by Brandon Butler, Director of Information Policy at the University of Virginia. This webinar will introduce questions to think about when picking a license for your research. You can signal which license you pick using the License Picker on the Open Science Framework (OSF; https://osf.io). The OSF is a free, open source web application built to help researchers manage their workflows. The OSF is part collaboration tool, part version control software, and part data archive. The OSF connects to popular tools researchers already use, like Dropbox, Box, Github, Mendeley, and now is integrated with JASP, to streamline workflows and increase efficiency.","tags":["Analysis","Data","Education","Licensing","Materials","Open Scholarship Tools and Technologies","OSF","Publishing","Reproducibility","Research Data Management Tools","Researchers"],"title":"Licensing your research","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1635961284,"objectID":"d451389c9d2326e9ad94460659771d06","permalink":"https://forrt.org/glossary/vbeta/likelihood-function/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/vbeta/likelihood-function/","section":"glossary","summary":"","tags":null,"title":"Likelihood function","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"ec3d67499fa3cf9c017bbecba7713757","permalink":"https://forrt.org/glossary/english/likelihood-function/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/english/likelihood-function/","section":"glossary","summary":"","tags":null,"title":"Likelihood function","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"073d63ca10aff63a2d27294287667eb3","permalink":"https://forrt.org/glossary/german/likelihood-function/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/german/likelihood-function/","section":"glossary","summary":"","tags":null,"title":"Likelihood function (Likelihood-Funktion)","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1689122843,"objectID":"ba620f460a286ecdc75c79da47b82d1f","permalink":"https://forrt.org/curated_resources/likelihood-of-null-effects-of-large-nhlb/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/likelihood-of-null-effects-of-large-nhlb/","section":"curated_resources","summary":"Background: We explore whether the number of null results in large National Heart Lung, and Blood Institute (NHLBI) funded trials has increased over time. Methods: We identified all large NHLBI supported RCTs between 1970 and 2012 evaluating drugs or dietary supplements for the treatment or prevention of cardiovascular disease. Trials were included if direct costs \u003e$500,000/year, participants were adult humans, and the primary outcome was cardiovascular risk, disease or death. The 55 trials meeting these criteria were coded for whether they were published prior to or after the year 2000, whether they registered in clinicaltrials.gov prior to publication, used active or placebo comparator, and whether or not the trial had industry co-sponsorship. We tabulated whether the study reported a positive, negative, or null result on the primary outcome variable and for total mortality. Results: 17 of 30 studies (57%) published prior to 2000 showed a significant benefit of intervention on the primary outcome in comparison to only 2 among the 25 (8%) trials published after 2000 (χ2=12.2,df= 1, p=0.0005). There has been no change in the proportion of trials that compared treatment to placebo versus active comparator. Industry co-sponsorship was unrelated to the probability of reporting a significant benefit. Pre-registration in clinical trials.gov was strongly associated with the trend toward null findings. Conclusions: The number NHLBI trials reporting positive results declined after the year 2000. Prospective declaration of outcomes in RCTs, and the adoption of transparent reporting standards, as required by clinicaltrials.gov, may have contributed to the trend toward null findings.","tags":["Reproducibility Crisis and Credibility Revolution","Open Science","Transparency"],"title":"Likelihood of Null Effects of Large NHLBI Clinical Trials Has Increased over Time","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1721390377,"objectID":"4b186476f15b2bf64f7d78f6a8be1222","permalink":"https://forrt.org/glossary/english/likelihood_principle/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/english/likelihood_principle/","section":"glossary","summary":"","tags":null,"title":"Likelihood Principle","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1635961284,"objectID":"6a2189095cc7ab4ee312eadaed15b163","permalink":"https://forrt.org/glossary/vbeta/likelihood-principle/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/vbeta/likelihood-principle/","section":"glossary","summary":"","tags":null,"title":"Likelihood Principle ","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1720865851,"objectID":"a3c79fb26fb859f82d7f233091326e59","permalink":"https://forrt.org/glossary/german/likelihood_principle/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/german/likelihood_principle/","section":"glossary","summary":"","tags":null,"title":"Likelihood Principle (Likelihood Prinzip)","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"93698e0e7b0d179782f14f164c45f053","permalink":"https://forrt.org/curated_resources/linking-to-data-effect-on-citation-rates/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/linking-to-data-effect-on-citation-rates/","section":"curated_resources","summary":"Is there a difference in citation rates between articles that were published with links to data and articles that were not? Besides being interesting from a purely academic point of view, this question is also highly relevant for the process of furthering science. Data sharing not only helps the process of verification of claims, but also the discovery of new findings in archival data. However, linking to data still is a far cry away from being a \"practice\", especially where it comes to authors providing these links during the writing and submission process. You need to have both a willingness and a publication mechanism in order to create such a practice. Showing that articles with links to data get higher citation rates might increase the willingness of scientists to take the extra steps of linking data sources to their publications. In this presentation we will show this is indeed the case: articles with links to data result in higher citation rates than articles without such links. The ADS is funded by NASA Grant NNX09AB39G.","tags":["Astrophysics","Computer Science","Data","Data Sharing","Digital Libraries","Instrumentation","Methods","Metrics"],"title":"Linking to Data - Effect on Citation Rates in Astronomy","type":"curated_resources"},{"authors":null,"categories":null,"content":"You can find the list of all references that were used to create the Glossary.\nWe are currently working on a better way to display and cross-link the references with the terms they are used for. A free and open platform for sharing MRI, MEG, EEG, iEEG, ECoG, ASL, and PET data—OpenNeuro. (n.d.). OpenNeuro. Retrieved 9 July 2021, from https://openneuro.org/ Abele-Brehm, A. E., Gollwitzer, M., Steinberg, U., \u0026amp; Schönbrodt, F. D. (2019). Attitudes Toward Open Science and Public Data Sharing: A Survey Among Members of the German Psychological Society. Social Psychology, 50(4), 252–260. https://doi.org/10.1027/1864-9335/a000384 Aczel, B., Szaszi, B., Nilsonne, G., Van den Akker, O., Albers, C. J., van Assen, M. A. L. M., Bastiaansen, J. A., Benjamin, D. J., Boehm, U., Botvinik-Nezer, R., Bringmann, L. F., Busch, N., Caruyer, E., Cataldo, A. M., Cowan, N., Delios, A., van Dongen, N. N. N., Donkin, C., van Doorn, J., … Wagenmakers, E.-J. (2021). Guidance for conducting and reporting multi-analyst studies [Preprint]. MetaArXiv. https://doi.org/10.31222/osf.io/5ecnh Aczel, B., Szaszi, B., Sarafoglou, A., Kekecs, Z., Kucharský, Š., Benjamin, D., Chambers, C. D., Fisher, A., Gelman, A., Gernsbacher, M. A., Ioannidis, J. P., Johnson, E., Jonas, K., Kousta, S., Lilienfeld, S. O., Lindsay, D. S., Morey, C. C., Munafò, M., Newell, B. R., … Wagenmakers, E.-J. (2020). A consensus-based transparency checklist. Nature Human Behaviour, 4(1), 4–6. https://doi.org/10.1038/s41562-019-0772-6 Albayrak-Aydemir, N. (2018a, April 16). Diversity helps but decolonisation is the key to equality in higher education. Contemporary Issues in Teaching and Learning. https://lsepgcertcitl.wordpress.com/2018/04/16/diversity-helps-but-decolonisation-is-the-key-to-equality-in-higher-education/ Albayrak-Aydemir, N. (2018b, November 29). Academics’ role on the future of higher education: Important but unrecognised. Contemporary Issues in Teaching and Learning. https://lsepgcertcitl.wordpress.com/2018/11/29/academics-role-on-the-future-of-higher-education-important-but-unrecognised/ Albayrak-Aydemir, N. (2020, February 20). ‘The hidden costs of being a scholar from the Global South’ is locked The hidden costs of being a scholar from the Global South. LSE Higher Education. https://blogs.lse.ac.uk/highereducation/2020/02/20/the-hidden-costs-of-being-a-scholar-from-the-global-south/ Albayrak-Aydemir, N., \u0026amp; Okoroji, C. (n.d.). Facing the challenges of postgraduate study as a minority student (A Guide for Psychology Postgraduates: Surviving Postgraduate Study, pp. 63–66). The British Psychological Society. Ali, M. J. (2021). Understanding the Altmetrics. Seminars in Ophthalmology, 1–3. https://doi.org/10.1080/08820538.2021.1930806 ALLEA - All European Academies. (2017). The European Code of Conduct for Research Integrity (Revised Edition). ALLEA. https://allea.org/code-of-conduct/ American Psychological Association,Task Force on Socioeconomic Status. (2007). Report of the APA task force on Socioeconomic status. American Psychological Association. Anderson, A. A., Scheufele, D. A., Brossard, D., \u0026amp; Corley, E. A. (2012). The Role of Media and Deference to Scientific Authority in Cultivating Trust in Sources of Information about Emerging Technologies. International Journal of Public Opinion Research, 24(2), 225–237. https://doi.org/10.1093/ijpor/edr032 Angrist, J. D., \u0026amp; Pischke, J.-S. (2010). The Credibility Revolution in Empirical Economics: How Better Research Design is Taking the Con out of Econometrics. Journal of Economic Perspectives, 24(2), 3–30. https://doi.org/10.1257/jep.24.2.3 Arslan, R. C. (2019). How to Automatically Document Data With the codebook Package to Facilitate Data Reuse. Advances in Methods and Practices in Psychological Science, 2(2), 169–187. https://doi.org/10.1177/2515245919838783 Australian Reproducibility Network. (n.d.). Australian Reproducibility Network. Retrieved 10 July 2021, from http://www.aus-rn.org/ Authorship \u0026amp; contributorship | The BMJ. (n.d.). The British Medical Journal. https://www.bmj.com/about-bmj/resources-authors/article-submission/authorship-contributorship Azevedo, F. (n.d.). Ideology May Help Explain Anti-Scientific Attitudes | Psychology Today. Retrieved 11 July 2021, from https://www.psychologytoday.com/intl/blog/social-justice-pacifists/202107/ideology-may-help-explain-anti-scientific-attitudes Azevedo, F., \u0026amp; Jost, J. T. (2021). The ideological basis of antiscientific attitudes: Effects of authoritarianism, conservatism, religiosity, social dominance, and system justification. Group Processes \u0026amp; Intergroup Relations, 24(4), 518–549. https://doi.org/10.1177/1368430221990104 Bak, H.-J. (2001). Education and Public Attitudes toward Science: Implications for the ‘Deficit Model’ of Education and Support for Science and Technology. Social Science Quarterly, 82(4), 779–795. https://www.jstor.org/stable/42955760 Banks, G. C., Rogelberg, S. G., Woznyj, H. M., Landis, R. S., \u0026amp; Rupp, D. E. (2016). Editorial: Evidence on Questionable Research Practices: The Good, the Bad, and the Ugly. Journal of Business and Psychology, 31(3), 323–338. https://doi.org/10.1007/s10869-016-9456-7 Barba, L. A. (2018). Terminologies for Reproducible Research. ArXiv:1802.03311 [Cs]. http://arxiv.org/abs/1802.03311 Bardsley, N. (2018). What lessons does the “replication crisis”\u0026nbsp; in psychology hold for experimental economics? In A. Lewis (Ed.), The Cambridge Handbook of Psychology and Economic Behavior (2nd ed.). CAMBRIDGE UNIVERSITY PRESS. Barnes, R. M., Johnston, H. M., MacKenzie, N., Tobin, S. J., \u0026amp; Taglang, C. M. (2018). The effect of ad hominem attacks on the evaluation of claims promoted by scientists. PLOS ONE, 13(1), e0192025. https://doi.org/10.1371/journal.pone.0192025 Bartoš, F., \u0026amp; Schimmack, U. (2020). Z-Curve.2.0: Estimating Replication Rates and Discovery Rates [Preprint]. PsyArXiv. https://doi.org/10.31234/osf.io/urgtn Bateman, I., Kahneman, D., Munro, A., Starmer, C., \u0026amp; Sugden, R. (2005). Testing competing models of loss aversion: An adversarial collaboration. Journal of Public Economics, 89(8), 1561–1580. https://doi.org/10.1016/j.jpubeco.2004.06.013 Baturay, M. H. (2015). An Overview of the World of MOOCs. Procedia - Social and Behavioral Sciences, 174, 427–433. https://doi.org/10.1016/j.sbspro.2015.01.685 Bazeley, P. (2003). Defining ‘Early Career’ in Research. Higher Education, 45(3), 257–279. https://doi.org/10.1023/A:1022698529612 Beffara Bret, B., Beffara Bret, A., \u0026amp; Nalborczyk, L. (2021). A fully automated, transparent, reproducible, and blind protocol for sequential analyses. Meta-Psychology, 5. https://doi.org/10.15626/MP.2018.869 Behrens, J. T. (1997). Principles and procedures of exploratory data analysis. Psychological Methods, 2(2), 131–160. https://doi.org/10.1037/1082-989X.2.2.131 Beller, S., \u0026amp; Bender, A. (2017). Theory, the Final Frontier? A Corpus-Based Analysis of the Role of Theory in Psychological Articles. Frontiers in Psychology, 8, 951. https://doi.org/10.3389/fpsyg.2017.00951 Benoit, K., Conway, D., Lauderdale, B. E., Laver, M., \u0026amp; Mikhaylov, S. (2016). Crowd-sourced Text Analysis: Reproducible and Agile Production of Political Data. American Political Science Review, 110(2), 278–295. https://doi.org/10.1017/S0003055416000058 Bhopal, R., Rankin, J., McColl, E., Thomas, L., Kaner, E., Stacy, R., Pearson, P., Vernon, B., \u0026amp; Rodgers, H. (1997). The vexed question of authorship: Views of researchers in a British medical faculty. BMJ, 314(7086), 1009–1009. https://doi.org/10.1136/bmj.314.7086.1009 BIAS | Definition of BIAS by Oxford Dictionary on Lexico.com also meaning of BIAS. (n.d.). Lexico Dictionaries | English. Retrieved 9 July 2021, from https://www.lexico.com/definition/bias BIDS. (2020a). About BIDS. Brain Imaging Data Structure. https://bids.neuroimaging.io/ BIDS. (2020b). Modality agnostic files—Brain Imaging Data Structure v1.6.0. Brain Imaging Data Structure. https://bids-specification.readthedocs.io/en/stable/03-modality-agnostic-files.html Bik, E. M., Casadevall, A., \u0026amp; Fang, F. C. (2016). The Prevalence of Inappropriate Image Duplication in Biomedical Research Publications. MBio, 7(3). https://doi.org/10.1128/mBio.00809-16 Bilder, G. (2013, September 20). DOIs unambiguously and persistently identify published, trustworthy, citable online scholarly literature. Right? [Website]. Crossref. https://www.crossref.org/blog/dois-unambiguously-and-persistently-identify-published-trustworthy-citable-online-scholarly-literature-right/ Bishop, D. V. (2020). The psychology of experimental psychologists: Overcoming cognitive constraints to improve research: The 47th Sir Frederic Bartlett Lecture. Quarterly Journal of Experimental Psychology, 73(1), 1–19. https://doi.org/10.1177/1747021819886519 Björneborn, L., \u0026amp; Ingwersen, P. (2004). Toward a basic framework for webometrics. Journal of the American Society for Information Science and Technology, 55(14), 1216–1227. https://doi.org/10.1002/asi.20077 Blohowiak, B. B., Cohoon, J., de-Wit, L., Eich, E., Farach, F. J., Hasselman, F., Holcombe, A. O., Humphreys, M., Lewis, M., \u0026amp; Nosek, B. A. (2013). Badges to Acknowledge Open Practices. https://osf.io/tvyxz/ BMJ. (2015, September 22). Introducing ‘How to write and publish a Study Protocol’ using BMJ’s new eLearning programme: Research to Publication. BMJ Open. https://blogs.bmj.com/bmjopen/2015/09/22/introducing-how-to-write-and-publish-a-study-protocol-using-bmjs-new-elearning-programme-research-to-publication/ Boivin, A., Richards, T., Forsythe, L., Grégoire, A., L’Espérance, A., Abelson, J., \u0026amp; Carman, K. L. (2018). Evaluating patient and public involvement in research. BMJ, k5147. https://doi.org/10.1136/bmj.k5147 Bol, T., de Vaan, M., \u0026amp; van de Rijt, A. (2018). The Matthew effect in science funding. Proceedings of the National Academy of Sciences, 115(19), 4887–4890. https://doi.org/10.1073/pnas.1719557115 Bollen, K. A. (1989). Structural equations with latent variables. Wiley. Borenstein, M. (Ed.). (2009). Introduction to meta-analysis. John Wiley \u0026amp; Sons. Bornmann, L., Ganser, C., Tekles, A., \u0026amp; Leydesdorff, L. (2019). Does the $h_\\alpha$ index reinforce the Matthew effect in science? Agent-based simulations using Stata and R. ArXiv:1905.11052 [Physics]. http://arxiv.org/abs/1905.11052 Borsboom, D., Mellenbergh, G. J., \u0026amp; van Heerden, J. (2004). The Concept of Validity. Psychological Review, 111(4), 1061–1071. https://doi.org/10.1037/0033-295X.111.4.1061 Borsboom, D., van der Maas, H., Dalege, J., Kievit, R., \u0026amp; Haig, B. (2020). Theory Construction Methodology: A practical framework for theory formation in psychology [Preprint]. PsyArXiv. https://doi.org/10.31234/osf.io/w5tp8 Bortoli, S. (2021, April 1). NIHR Guidance on co-producing a research project. Learning For Involvement. https://www.learningforinvolvement.org.uk/?opportunity=nihr-guidance-on-co-producing-a-research-project Bourne, P. E., Polka, J. K., Vale, R. D., \u0026amp; Kiley, R. (2017). Ten simple rules to consider regarding preprint submission. PLOS Computational Biology, 13(5), e1005473. https://doi.org/10.1371/journal.pcbi.1005473 Bouvy, J. C., \u0026amp; Mujoomdar, M. (2019). All-Male Panels and Gender Diversity of Issue Panels and Plenary Sessions at ISPOR Europe. PharmacoEconomics - Open, 3(3), 419–422. https://doi.org/10.1007/s41669-019-0153-0 Box, G. E. P. (1976). Science and Statistics. Journal of the American Statistical Association, 71(356), 791–799. https://doi.org/10.1080/01621459.1976.10480949 Bramoulle, Y., \u0026amp; Saint-Paul, G. (2007). Research Cycles. SSRN Electronic Journal. https://doi.org/10.2139/ssrn.965816 Brand, A., Allen, L., Altman, M., Hlava, M., \u0026amp; Scott, J. (2015). Beyond authorship: Attribution, contribution, collaboration, and credit. Learned Publishing, 28(2), 151–155. https://doi.org/10.1087/20150211 Brandt, M. J., IJzerman, H., Dijksterhuis, A., Farach, F. J., Geller, J., Giner-Sorolla, R., Grange, J. A., Perugini, M., Spies, J. R., \u0026amp; van ’t Veer, A. (2014). The Replication Recipe: What makes for a convincing replication? Journal of Experimental Social Psychology, 50, 217–224. https://doi.org/10.1016/j.jesp.2013.10.005 Braun, V., \u0026amp; Clarke, V. (2013). Successful qualitative research: A practical guide for beginners. Sage. https://books.google.co.uk/books?hl=en\u0026amp;lr=\u0026amp;id=nYMQAgAAQBAJ\u0026amp;oi=fnd\u0026amp;pg=PP2\u0026amp;ots=SqJAD7C-5w\u0026amp;sig=6hBnRUj4z31CbylBTRzfIudISME#v=onepage\u0026amp;q\u0026amp;f=false Brembs, B., Button, K., \u0026amp; Munafò, M. (2013). Deep impact: Unintended consequences of journal rank. Frontiers in Human Neuroscience, 7. https://doi.org/10.3389/fnhum.2013.00291 Brewer, P. R., \u0026amp; Ley, B. L. (2013). Whose Science Do You Believe? Explaining Trust in Sources of Scientific Information About the Environment. Science Communication, 35(1), 115–137. https://doi.org/10.1177/1075547012441691 Breznau, N., Rinke, E. M., Wuttke, A., Adem, M., Adriaans, J., Alvarez-Benjumea, A., Andersen, H. K., Auer, D., Azevedo, F., Bahnsen, O., Balzer, D., Bauer, G., Bauer, P., Baumann, M., Baute, S., Benoit, V., Bernauer, J., Berning, C., Berthold, A., … Nguyen, H. H. V. (2021). Observing Many Researchers Using the Same Data and Hypothesis Reveals a Hidden Universe of Uncertainty [Preprint]. MetaArXiv. https://doi.org/10.31222/osf.io/cd5j9 Breznau, N., Rinke, E. M., Wuttke, A., Nguyen, H. H. V., Adem, M., Adriaans, J., Akdeniz, E., Alvarez-Benjumea, A., Andersen, H. K., Auer, D., Azevedo, F., Bahnsen, O., Bai, L., Balzer, D., Bauer, G., Bauer, P., Baumann, M., Baute, S., Benoit, V., … Żółtak, T. (2021). How Many Replicators Does It Take to Achieve Reliability? Investigating Researcher Variability in a Crowdsourced Replication [Preprint]. SocArXiv. https://doi.org/10.31235/osf.io/j7qta Brod, M., Tesler, L. E., \u0026amp; Christensen, T. L. (2009). Qualitative research and content validity: Developing best practices based on science and experience. Quality of Life Research, 18(9), 1263–1278. https://doi.org/10.1007/s11136-009-9540-9 Brooks, T. A. (1985). Private acts and public objects: An investigation of citer motivations. Journal of the American Society for Information Science, 36(4), 223–229. https://doi.org/10.1002/asi.4630360402 Brown, J. (2010). An introduction to overlay journals (Repositories Support Project, pp. 1–6). University College London. Brown, N. J. L., \u0026amp; Heathers, J. A. J. (2017). The GRIM Test: A Simple Technique Detects Numerous Anomalies in the Reporting of Results in Psychology. Social Psychological and Personality Science, 8(4), 363–369. https://doi.org/10.1177/1948550616673876 Brown, N., Thompson, P., \u0026amp; Leigh, J. S. (2018). Making Academia More Accessible. Journal of Perspectives in Applied Academic Practice, 6(2), 82–90. https://doi.org/10.14297/jpaap.v6i2.348 Brulé, J. F., \u0026amp; Blount, A. (1989). Knowledge acquisition. McGraw-Hill. Brunner, J., \u0026amp; Schimmack, U. (2020). Estimating Population Mean Power Under Conditions of Heterogeneity and Selection for Significance. Meta-Psychology, 4. https://doi.org/10.15626/MP.2018.874 Bruns, S. B., \u0026amp; Ioannidis, J. P. A. (2016). P-Curve and p-Hacking in Observational Research. PLOS ONE, 11(2), e0149144. https://doi.org/10.1371/journal.pone.0149144 Budapest Open Access Initiative | Read the Budapest Open Access Initiative. (2002, February 14). https://www.budapestopenaccessinitiative.org/read Busse, C., Kach, A. P., \u0026amp; Wagner, S. M. (2017). Boundary Conditions: What They Are, How to Explore Them, Why We Need Them, and When to Consider Them. Organizational Research Methods, 20(4), 574–609. https://doi.org/10.1177/1094428116641191 Button, K. S., Chambers, C. D., Lawrence, N., \u0026amp; Munafò, M. R. (2020). Grassroots Training for Reproducible Science: A Consortium-Based Approach to the Empirical Dissertation. Psychology Learning \u0026amp; Teaching, 19(1), 77–90. https://doi.org/10.1177/1475725719857659 Button, K. S., Lawrence, N., Chambers, C. D., \u0026amp; Munafò, M. R. (2016). Instilling scientific rigour at the grassroots. The Psychologist, 29(16), 158–167. Byrne, J. A., \u0026amp; Christopher, J. (2020). Digital magic, or the dark arts of the 21 st century—How can journals and peer reviewers detect manuscripts and publications from paper mills? FEBS Letters, 594(4), 583–589. https://doi.org/10.1002/1873-3468.13747 Campbell, D. T. (1957). Factors relevant to the validity of experiments in social settings. Psychological Bulletin, 54(4), 297–312. https://doi.org/10.1037/h0040950 Campbell, D. T., \u0026amp; Stanley, J. C. (2011). Experimental and quasi-experimental designs for research. Wadsworth. Carp, J. (2012). On the Plurality of (Methodological) Worlds: Estimating the Analytic Flexibility of fMRI Experiments. Frontiers in Neuroscience, 6. https://doi.org/10.3389/fnins.2012.00149 Carsey, T. M. (2014). Making DA-RT a Reality. PS: Political Science \u0026amp; Politics, 47(01), 72–77. https://doi.org/10.1017/S1049096513001753 Carter, A., Tilling, K., \u0026amp; Munafo, M. R. (2021). Considerations of sample size and power calculations given a range of analytical scenarios [Preprint]. PsyArXiv. https://doi.org/10.31234/osf.io/tcqrn Case, C. M. (1928). Scholarship in sociology. Sociology and Social Research, 12, 323–340. Cassidy, S. A., Dimova, R., Giguère, B., Spence, J. R., \u0026amp; Stanley, D. J. (2019). Failing Grade: 89% of Introduction-to-Psychology Textbooks That Define or Explain Statistical Significance Do So Incorrectly. Advances in Methods and Practices in Psychological Science, 2(3), 233–239. https://doi.org/10.1177/2515245919858072 Center for Open Science. (n.d.). Registered Reports. Retrieved 10 July 2021, from https://www.cos.io/initiatives/registered-reports Centre for Open Science. (n.d.). Show Your Work. Share Your Work. Centre for Open Science. https://www.cos.io/ Chambers, C. D. (2013). Registered Reports: A new publishing initiative at Cortex. Cortex, 49(3), 609–610. https://doi.org/10.1016/j.cortex.2012.12.016 Chambers, C. D., Dienes, Z., McIntosh, R. D., Rotshtein, P., \u0026amp; Willmes, K. (2015). Registered Reports: Realigning incentives in scientific publishing. Cortex, 66, A1–A2. https://doi.org/10.1016/j.cortex.2015.03.022 Chambers, C. D., \u0026amp; Tzavella, L. (2020). The past, present, and future of Registered Reports [Preprint]. MetaArXiv. https://doi.org/10.31222/osf.io/43298 Chartier, C. R., Riegelman, A., \u0026amp; McCarthy, R. J. (2018). StudySwap: A Platform for Interlab Replication, Collaboration, and Resource Exchange. Advances in Methods and Practices in Psychological Science, 1(4), 574–579. https://doi.org/10.1177/2515245918808767 Chuard, P. J. C., Vrtílek, M., Head, M. L., \u0026amp; Jennions, M. D. (2019). Evidence that nonsignificant results are sometimes preferred: Reverse P-hacking or selective reporting? PLOS Biology, 17(1), e3000127. https://doi.org/10.1371/journal.pbio.3000127 CKAN - The open source data management system. (n.d.). Ckan. Retrieved 9 July 2021, from https://ckan.org/ Claerbout, J. F., \u0026amp; Karrenbach, M. (1992). Electronic documents give reproducible research a new meaning. SEG Technical Program Expanded Abstracts 1992, 601–604. https://doi.org/10.1190/1.1822162 Clark, H., Elsherif, M. M., \u0026amp; Leavens, D. A. (2019). Ontogeny vs. phylogeny in primate/canid comparisons: A meta-analysis of the object choice task. Neuroscience \u0026amp; Biobehavioral Reviews, 105, 178–189. https://doi.org/10.1016/j.neubiorev.2019.06.001 Closed access. (n.d.). CASRAI. Retrieved 9 July 2021, from https://casrai.org/term/closed-access/ Cohen, J. (1962). The statistical power of abnormal-social psychological research: A review. The Journal of Abnormal and Social Psychology, 65(3), 145–153. https://doi.org/10.1037/h0045186 Cohen, J. (1988). Statistical power analysis for the behavioral sciences (2nd ed). L. Erlbaum Associates. Cohn, J. P. (2008). Citizen Science: Can Volunteers Do Real Research? BioScience, 58(3), 192–197. https://doi.org/10.1641/B580303 Collaborative Assessment forTrustworthy Science|The repliCATS project. (n.d.). University of Melbourne. Retrieved 10 July 2021, from https://replicats.research.unimelb.edu.au/ Committee on Reproducibility and Replicability in Science, Board on Behavioral, Cognitive, and Sensory Sciences, Committee on National Statistics, Division of Behavioral and Social Sciences and Education, Nuclear and Radiation Studies Board, Division on Earth and Life Studies, Board on Mathematical Sciences and Analytics, Committee on Applied and Theoretical Statistics, Division on Engineering and Physical Sciences, Board on Research Data and Information, Committee on Science, Engineering, Medicine, and Public Policy, Policy and Global Affairs, \u0026amp; National Academies of Sciences, Engineering, and Medicine. (2019). Reproducibility and Replicability in Science (p. 25303). National Academies Press. https://doi.org/10.17226/25303 Confederation Of Open Access Repositories. (2020). COAR Community Framework for Best Practices in Repositories. (Version 1). Zenodo. https://doi.org/10.5281/ZENODO.4110829 Cook, T. D., \u0026amp; Campbell, D. T. (1979). Quasi-experimentation: Design \u0026amp; analysis issues for field settings. Rand McNally College Pub. Co. Corley, K. G., \u0026amp; Gioia, D. A. (2011). Building Theory about Theory Building: What Constitutes a Theoretical Contribution? Academy of Management Review, 36(1), 12–32. https://doi.org/10.5465/amr.2009.0486 Cornwall, A., \u0026amp; Jewkes, R. (1995). What is participatory research? Social Science \u0026amp; Medicine, 41(12), 1667–1676. https://doi.org/10.1016/0277-9536(95)00127-S Correction or retraction? (2006). Nature, 444(7116), 123–124. https://doi.org/10.1038/444123b Corti, L. (2019). Managing and sharing research data: A guide to good practice (2nd edition). SAGE Publications. Cowan, N., Belletier, C., Doherty, J. M., Jaroslawska, A. J., Rhodes, S., Forsberg, A., Naveh-Benjamin, M., Barrouillet, P., Camos, V., \u0026amp; Logie, R. H. (2020). How Do Scientific Views Change? Notes From an Extended Adversarial Collaboration. Perspectives on Psychological Science, 15(4), 1011–1025. https://doi.org/10.1177/1745691620906415 CRediT - Contributor Roles Taxonomy. (n.d.). Casrai. Retrieved 9 July 2021, from https://casrai.org/credit/ Crenshaw, K. (1989). Demarginalizing the Intersection of Race and Sex: A Black Feminist Critique of Antidiscrimination Doctrine, Feminist Theory and Antiracist Politics. University of Chicago Legal Forum, 1989(1), 8. https://chicagounbound.uchicago.edu/uclf/vol1989/iss1/8 Cronbach, L. J., \u0026amp; Meehl, P. E. (1955). Construct validity in psychological tests. Psychological Bulletin, 52(4), 281–302. https://doi.org/10.1037/h0040957 Cronin, B. (2001). Hyperauthorship: A postmodern perversion or evidence of a structural shift in scholarly communication practices? Journal of the American Society for Information Science and Technology, 52(7), 558–569. https://doi.org/10.1002/asi.1097 Crosetto, P. (2021, April 12). Is MDPI a predatory publisher? Paolo Crosetto. https://paolocrosetto.wordpress.com/2021/04/12/is-mdpi-a-predatory-publisher/ Crutzen, R., Ygram Peters, G.-J., \u0026amp; Mondschein, C. (2019). Why and how we should care about the General Data Protection Regulation. Psychology \u0026amp; Health, 34(11), 1347–1357. https://doi.org/10.1080/08870446.2019.1606222 Crüwell, S., van Doorn, J., Etz, A., Makel, M. C., Moshontz, H., Niebaum, J. C., Orben, A., Parsons, S., \u0026amp; Schulte-Mecklenbeck, M. (2019). Seven Easy Steps to Open Science: An Annotated Reading List. Zeitschrift Für Psychologie, 227(4), 237–248. https://doi.org/10.1027/2151-2604/a000387 Curran, P. J. (2009). The seemingly quixotic pursuit of a cumulative psychological science: Introduction to the special issue. Psychological Methods, 14(2), 77–80. https://doi.org/10.1037/a0015972 Curry, S. (2012, August 13). Sick of Impact Factors | Reciprocal Space. Reciprocal Space. http://occamstypewriter.org/scurry/2012/08/13/sick-of-impact-factors/ d’Espagnat, B. (2008). Is Science Cumulative? A Physicist Viewpoint. In L. Soler, H. Sankey, \u0026amp; P. Hoyningen-Huene (Eds.), Rethinking Scientific Change and Theory Comparison (pp. 145–151). Springer Netherlands. https://doi.org/10.1007/978-1-4020-6279-7_10 Data Management Expert Guide—CESSDA TRAINING. (n.d.). CESSDA. Retrieved 10 July 2021, from https://www.cessda.eu/Training/Training-Resources/Library/Data-Management-Expert-Guide Data management plans | Stanford Libraries. (n.d.). Stanford Libraries. Retrieved 9 July 2021, from https://library.stanford.edu/research/data-management-services/data-management-plans Data protection. (n.d.). [Text]. European Commission - European Commission. Retrieved 9 July 2021, from https://ec.europa.eu/info/law/law-topic/data-protection_en Datacite Metadata Schema. (n.d.). DataCite Schema. Retrieved 9 July 2021, from https://schema.datacite.org/ Davies, G. M., \u0026amp; Gray, A. (2015). Don’t let spurious accusations of pseudoreplication limit our ability to learn from natural experiments (and other messy kinds of ecological monitoring). Ecology and Evolution, 5(22), 5295–5304. https://doi.org/10.1002/ece3.1782 Day, S., Rennie, S., Luo, D., \u0026amp; Tucker, J. D. (2020). Open to the public: Paywalls and the public rationale for open access medical research publishing. Research Involvement and Engagement, 6(1), 8. https://doi.org/10.1186/s40900-020-0182-y Declaration on Research Assessment. (n.d.). Health Research Board. Retrieved 9 July 2021, from https://www.hrb.ie/funding/funding-schemes/before-you-apply/how-we-assess-applications/declaration-on-research-assessment/ Del Giudice, M., \u0026amp; Gangestad, S. W. (2021). A Traveler’s Guide to the Multiverse: Promises, Pitfalls, and a Framework for the Evaluation of Analytic Decisions. Advances in Methods and Practices in Psychological Science, 4(1), 251524592095492. https://doi.org/10.1177/2515245920954925 Deutsche Forschungsgemeinschaft. (2019). Guidelines for Safeguarding Good Research Practice. Code of Conduct. https://doi.org/10.5281/ZENODO.3923602 DeVellis, R. F. (2017). Scale development: Theory and applications (Fourth edition). SAGE. Devezer, B., Navarro, D. J., Vandekerckhove, J., \u0026amp; Ozge Buzbas, E. (2021). The case for formal methodology in scientific reform. Royal Society Open Science, 8(3), rsos.200805, 200805. https://doi.org/10.1098/rsos.200805 Dickersin, K., \u0026amp; Min, Y.-I. (1993). Publication Bias: The Problem That Won’t Go Away. Annals of the New York Academy of Sciences, 703(1 Doing More Go), 135–148. https://doi.org/10.1111/j.1749-6632.1993.tb26343.x Dienes, Z. (2008). Understanding Psychology as a Science: An Introduction to Scientific and Statistical Inference. Palgrave Macmillan. https://books.google.ca/books?id=qCQdBQAAQBAJ Dienes, Z. (2011). Bayesian Versus Orthodox Statistics: Which Side Are You On? Perspectives on Psychological Science, 6(3), 274–290. https://doi.org/10.1177/1745691611406920 Dienes, Z. (2014). Using Bayes to get the most out of non-significant results. Frontiers in Psychology, 5. https://doi.org/10.3389/fpsyg.2014.00781 Dienes, Z. (2016). How Bayes factors change scientific practice. Journal of Mathematical Psychology, 72, 78–89. https://doi.org/10.1016/j.jmp.2015.10.003 Digital Object Identifier System Handbook. (n.d.). DOI. Retrieved 9 July 2021, from https://www.doi.org/hb.html Directory of Open Access Journals. (n.d.). Retrieved 11 July 2021, from https://doaj.org/apply/transparency/ Doll, R., \u0026amp; Hill, A. B. (1954). The Mortality of Doctors in Relation to Their Smoking Habits. BMJ, 1(4877), 1451–1455. https://doi.org/10.1136/bmj.1.4877.1451 Domov | SKRN (Slovak Reproducibility network). (n.d.). SKRN. Retrieved 10 July 2021, from https://slovakrn.wixsite.com/skrn Download JASP. (n.d.). JASP - Free and User-Friendly Statistical Software. Retrieved 9 July 2021, from https://jasp-stats.org/download/ Drost, E. A. (2011). Validity and reliability in social science research. Education Research and Perspectives, 38(1), 105–123. Du Bois, W. E. B. (2018). The souls of Black folk: Essays and sketches. Duval, S., \u0026amp; Tweedie, R. (2000a). A Nonparametric ‘Trim and Fill’ Method of Accounting for Publication Bias in Meta-Analysis. Journal of the American Statistical Association, 95(449), 89. https://doi.org/10.2307/2669529 Duval, S., \u0026amp; Tweedie, R. (2000b). Trim and Fill: A Simple Funnel-Plot-Based Method of Testing and Adjusting for Publication Bias in Meta-Analysis. Biometrics, 56(2), 455–463. https://doi.org/10.1111/j.0006-341X.2000.00455.x Duyx, B., Swaen, G. M. H., Urlings, M. J. E., Bouter, L. M., \u0026amp; Zeegers, M. P. (2019). The strong focus on positive results in abstracts may cause bias in systematic reviews: A case study on abstract reporting bias. Systematic Reviews, 8(1), 174. https://doi.org/10.1186/s13643-019-1082-9 Eagly, A. H., \u0026amp; Riger, S. (2014). Feminism and psychology: Critiques of methods and epistemology. American Psychologist, 69(7), 685–702. https://doi.org/10.1037/a0037372 Easterbrook, S. M. (2014). Open code for open science? Nature Geoscience, 7(11), 779–781. https://doi.org/10.1038/ngeo2283 Ebersole, C. R., Atherton, O. E., Belanger, A. L., Skulborstad, H. M., Allen, J. M., Banks, J. B., Baranski, E., Bernstein, M. J., Bonfiglio, D. B. V., Boucher, L., Brown, E. R., Budiman, N. I., Cairo, A. H., Capaldi, C. A., Chartier, C. R., Chung, J. M., Cicero, D. C., Coleman, J. A., Conway, J. G., … Nosek, B. A. (2016). Many Labs 3: Evaluating participant pool quality across the academic semester via replication. Journal of Experimental Social Psychology, 67, 68–82. https://doi.org/10.1016/j.jesp.2015.10.012 Editorial Director. (2021, May). What is a group author (collaborative author) and does it need an ORCID? JMIR Publications. https://support.jmir.org/hc/en-us/articles/115001449591-What-is-a-group-author-collaborative-author-and-does-it-need-an-ORCID- Eldermire, E. (n.d.). LibGuides: Measuring your research impact: i10-Index. Retrieved 9 July 2021, from https://guides.library.cornell.edu/impact/author-impact-10 Eley, A. R. (Ed.). (2012). Becoming a successful early career researcher. Routledge. Ellemers, N. (2021). Science as collaborative knowledge generation. British Journal of Social Psychology, 60(1), 1–28. https://doi.org/10.1111/bjso.12430 Elliott, K. C., \u0026amp; Resnik, D. B. (2019). Making Open Science Work for Science and Society. Environmental Health Perspectives, 127(7), 075002. https://doi.org/10.1289/EHP4808 Elm, E. von, Altman, D. G., Egger, M., Pocock, S. J., Gøtzsche, P. C., \u0026amp; Vandenbroucke, J. P. (2007). Strengthening the reporting of observational studies in epidemiology (STROBE) statement: Guidelines for reporting observational studies. BMJ, 335(7624), 806–808. https://doi.org/10.1136/bmj.39335.541782.AD Elman, C., Gerring, J., \u0026amp; Mahoney, J. (Eds.). (2020). The production of knowledge: Enhancing progress in social science. Cambridge University Press. Elmore, S. A. (2018). Preprints: What Role Do These Have in Communicating Scientific Results? Toxicologic Pathology, 46(4), 364–365. https://doi.org/10.1177/0192623318767322 Embargo (academic publishing). (2021). In Wikipedia. https://en.wikipedia.org/w/index.php?title=Embargo_(academic_publishing)\u0026amp;oldid=1016895567 Epskamp, S., \u0026amp; Nuijten, M. B. (2018). statcheck: Extract Statistics from Articles and Recompute p Values (1.3.0) [Computer software]. https://CRAN.R-project.org/package=statcheck Esterling, K., Brady, D., \u0026amp; Schwitzgebel, E. (2021). The Necessity of Construct and External Validity for Generalized Causal Claims [Preprint]. Open Science Framework. https://doi.org/10.31219/osf.io/2s8w5 Etz, A., Gronau, Q. F., Dablander, F., Edelsbrunner, P. A., \u0026amp; Baribault, B. (2018). How to become a Bayesian in eight easy steps: An annotated reading list. Psychonomic Bulletin \u0026amp; Review, 25(1), 219–234. https://doi.org/10.3758/s13423-017-1317-5 European Commission. (2021). European Commission. Responsible Research \u0026amp; Innovation | Horizon 2020. https://ec.europa.eu/programmes/horizon2020/en/h2020-section/responsible-research-innovation Evans, G., \u0026amp; Durant, J. (1995). The relationship between knowledge and attitudes in the public understanding of science in Britain. Public Understanding of Science, 4(1), 57–74. https://doi.org/10.1088/0963-6625/4/1/004 Evans, O., \u0026amp; Rubin, M. (2021). In a Class on Their Own: Investigating the Role of Social Integration in the Association Between Social Class and Mental Well-Being. Personality and Social Psychology Bulletin, 014616722110211. https://doi.org/10.1177/01461672211021190 Evidence Synthesis. (n.d.). LSHTM. Retrieved 9 July 2021, from https://www.lshtm.ac.uk/research/centres/centre-evaluation/evidence-synthesis Fanelli, D. (2010). Do Pressures to Publish Increase Scientists’ Bias? An Empirical Support from US States Data. PLoS ONE, 5(4), e10271. https://doi.org/10.1371/journal.pone.0010271 Fanelli, D. (2018). Opinion: Is science really facing a reproducibility crisis, and do we need it to? Proceedings of the National Academy of Sciences, 115(11), 2628–2631. https://doi.org/10.1073/pnas.1708272114 Farrow, R. (2017). Open education and critical pedagogy. Learning, Media and Technology, 42(2), 130–146. https://doi.org/10.1080/17439884.2016.1113991 Faul, F., Erdfelder, E., Buchner, A., \u0026amp; Lang, A.-G. (2009). Statistical power analyses using G*Power 3.1: Tests for correlation and regression analyses. Behavior Research Methods, 41(4), 1149–1160. https://doi.org/10.3758/BRM.41.4.1149 Faul, F., Erdfelder, E., Lang, A.-G., \u0026amp; Buchner, A. (2007). G*Power 3: A flexible statistical power analysis program for the social, behavioral, and biomedical sciences. Behavior Research Methods, 39(2), 175–191. https://doi.org/10.3758/BF03193146 Ferson, S., Joslyn, C. A., Helton, J. C., Oberkampf, W. L., \u0026amp; Sentz, K. (2004). Summary from the epistemic uncertainty workshop: Consensus amid diversity. Reliability Engineering \u0026amp; System Safety, 85(1–3), 355–369. https://doi.org/10.1016/j.ress.2004.03.023 Fiedler, K., Kutzner, F., \u0026amp; Krueger, J. I. (2012). The Long Way From α-Error Control to Validity Proper: Problems With a Short-Sighted False-Positive Debate. Perspectives on Psychological Science, 7(6), 661–669. https://doi.org/10.1177/1745691612462587 Fiedler, K., \u0026amp; Schwarz, N. (2016). Questionable Research Practices Revisited. Social Psychological and Personality Science, 7(1), 45–52. https://doi.org/10.1177/1948550615612150 Filipe, A., Renedo, A., \u0026amp; Marston, C. (2017). The co-production of what? Knowledge, values, and social relations in health care. PLOS Biology, 15(5), e2001403. https://doi.org/10.1371/journal.pbio.2001403 Findley, M. G., Jensen, N. M., Malesky, E. J., \u0026amp; Pepinsky, T. B. (2016). Can Results-Free Review Reduce Publication Bias? The Results and Implications of a Pilot Study. Comparative Political Studies, 49(13), 1667–1703. https://doi.org/10.1177/0010414016655539 Finlay, L., \u0026amp; Gough, B. (Eds.). (2003). Reflexivity: A practical guide for researchers in health and social sciences. Blackwell Science. Flake, J. K., \u0026amp; Fried, E. I. (2020). Measurement Schmeasurement: Questionable Measurement Practices and How to Avoid Them. Advances in Methods and Practices in Psychological Science, 3(4), 456–465. https://doi.org/10.1177/2515245920952393 Fletcher-Watson, S., Adams, J., Brook, K., Charman, T., Crane, L., Cusack, J., Leekam, S., Milton, D., Parr, J. R., \u0026amp; Pellicano, E. (2019). Making the future together: Shaping autism research through meaningful participation. Autism, 23(4), 943–953. https://doi.org/10.1177/1362361318786721 Foreman-Mackey, D., Hogg, D. W., Lang, D., \u0026amp; Goodman, J. (2013). emcee: The MCMC Hammer. Publications of the Astronomical Society of the Pacific, 125(925), 306–312. https://doi.org/10.1086/670067 Forrt. (2019). Introducing a Framework for Open and Reproducible Research Training (FORRT) [Preprint]. Open Science Framework. https://doi.org/10.31219/osf.io/bnh7p FORRT - Framework for Open and Reproducible Research Training. (n.d.). FORRT. Retrieved 9 July 2021, from https://forrt.org/ Foster, MSLS, E. D., \u0026amp; Deardorff, MLIS, A. (2017). Open Science Framework (OSF). Journal of the Medical Library Association, 105(2). https://doi.org/10.5195/JMLA.2017.88 Franco, A., Malhotra, N., \u0026amp; Simonovits, G. (2014). Publication bias in the social sciences: Unlocking the file drawer. Science, 345(6203), 1502–1505. https://doi.org/10.1126/science.1255484 Frank, M. C., Bergelson, E., Bergmann, C., Cristia, A., Floccia, C., Gervain, J., Hamlin, J. K., Hannon, E. E., Kline, M., Levelt, C., Lew-Williams, C., Nazzi, T., Panneton, R., Rabagliati, H., Soderstrom, M., Sullivan, J., Waxman, S., \u0026amp; Yurovsky, D. (2017). A Collaborative Approach to Infant Research: Promoting Reproducibility, Best Practices, and Theory-Building. Infancy, 22(4), 421–435. https://doi.org/10.1111/infa.12182 Franzoni, C., \u0026amp; Sauermann, H. (2014). Crowd science: The organization of scientific research in open collaborative projects. Research Policy, 43(1), 1–20. https://doi.org/10.1016/j.respol.2013.07.005 Fraser, H., Bush, M., Wintle, B., Mody, F., Smith, E. T., Hanea, A., Gould, E., Hemming, V., Hamilton, D. G., Rumpff, L., Wilkinson, D. P., Pearson, R., Singleton Thorn, F., Ashton, raquel, Willcox, A., Gray, C. T., Head, A., Ross, M., Groenewegen, R., … Fidler, F. (2021). Predicting reliability through structured expert elicitation with repliCATS (Collaborative Assessments for Trustworthy Science) [Preprint]. MetaArXiv. https://doi.org/10.31222/osf.io/2pczv Free Our Knowledge. (n.d.). About. Free Our Knowledge. Retrieved 9 July 2021, from https://freeourknowledge.org/about/ Frigg, R., \u0026amp; Hartmann, S. (2020). Models in Science. In E. N. Zalta (Ed.), The Stanford Encyclopedia of Philosophy (Spring 2020). Metaphysics Research Lab, Stanford University. https://plato.stanford.edu/archives/spr2020/entries/models-science/ Frith, U. (2020). Fast Lane to Slow Science. Trends in Cognitive Sciences, 24(1), 1–2. https://doi.org/10.1016/j.tics.2019.10.007 Galligan, F., \u0026amp; Dyas-Correia, S. (2013). Altmetrics: Rethinking the Way We Measure. Serials Review, 39(1), 56–61. https://doi.org/10.1080/00987913.2013.10765486 Garson, G. D. (2012). Testing Statistical Assumptions (2012 edition). North Carolina State University. Gelman, A., \u0026amp; Carlin, J. (2014). Beyond Power Calculations: Assessing Type S (Sign) and Type M (Magnitude) Errors. Perspectives on Psychological Science, 9(6), 641–651. https://doi.org/10.1177/1745691614551642 Gelman, A., \u0026amp; Loken, E. (2013). The garden of forking paths: Why multiple comparisons can be a problem, even when there is no “fishing expedition” or “p-hacking” and the research hypothesis was posited ahead of time [Doctoral dissertation, Columbia University]. http://www.stat.columbia.edu/~gelman/research/unpublished/p_hacking.pdf Gelman, A., \u0026amp; Stern, H. (2006). The Difference Between “Significant” and “Not Significant” is not Itself Statistically Significant. The American Statistician, 60(4), 328–331. https://doi.org/10.1198/000313006X152649 Generalizability. (2018). In B. B. Frey, The SAGE Encyclopedia of Educational Research, Measurement, and\u0026nbsp; \u0026nbsp; \u0026nbsp; Evaluation. SAGE Publications, Inc. https://doi.org/10.4135/9781506326139.n284 Gentleman, R. (2005). Reproducible Research: A Bioinformatics Case Study. Statistical Applications in Genetics and Molecular Biology, 4(1). https://doi.org/10.2202/1544-6115.1034 Get Involved—Creative Commons. (n.d.). Creative Commons. Retrieved 9 July 2021, from https://creativecommons.org/about/get-involved/ Geyer, C., J. (2003). Maximum Likelihood in R (pp. 1–9) [Preprint]. Open Science Framework. Geyer, C., J. (2007). Stat 5102 Notes: Maximum Likelihood (pp. 1–8) [Preprint]. Open Science Framework. Gilroy, P. (2002). The black Atlantic: Modernity and double consciousness (3. impr., reprint). Verso. Giner-Sorolla, R., Carpenter, T., Montoya, A., \u0026amp; Neil Lewis, J. (2019). SPSP Power Analysis Working Group 2019. https://osf.io/9bt5s/ Ginsparg, P. (1997). Winners and Losers in the Global Research Village. The Serials Librarian, 30(3–4), 83–95. https://doi.org/10.1300/J123v30n03_13 Ginsparg, P. (2001, February 20). Creating a global knowledge network. Cornell University. http://www.cs.cornell.edu/~ginsparg/physics/blurb/pg01unesco.html Gioia, D. A., \u0026amp; Pitre, E. (1990). Multiparadigm Perspectives on Theory Building. Academy of Management Review, 15(4), 584–602. https://doi.org/10.5465/amr.1990.4310758 Git—About Version Control. (n.d.). Git. Retrieved 9 July 2021, from https://git-scm.com/book/en/v2/Getting-Started-About-Version-Control Glass, D. J., \u0026amp; Hall, N. (2008). A Brief History of the Hypothesis. Cell, 134(3), 378–381. https://doi.org/10.1016/j.cell.2008.07.033 Gollwitzer, M., Abele-Brehm, A., Fiebach, C., Ramthun, R., Scheel, A. M., Schönbrodt, F. D., \u0026amp; Steinberg, U. (2020). Data Management and Data Sharing in Psychological Science: Revision of the DGPs Recommendations [Preprint]. PsyArXiv. https://doi.org/10.31234/osf.io/24ncs Goodman, S. N., Fanelli, D., \u0026amp; Ioannidis, J. P. A. (2016). What does research reproducibility mean? Science Translational Medicine, 8(341), 341ps12-341ps12. https://doi.org/10.1126/scitranslmed.aaf5027 Goodman, S. W., \u0026amp; Pepinsky, T. B. (2019). Gender Representation and Strategies for Panel Diversity: Lessons from the APSA Annual Meeting. PS: Political Science \u0026amp; Politics, 52(4), 669–676. https://doi.org/10.1017/S1049096519000908 Gorgolewski, K. J., Auer, T., Calhoun, V. D., Craddock, R. C., Das, S., Duff, E. P., Flandin, G., Ghosh, S. S., Glatard, T., Halchenko, Y. O., Handwerker, D. A., Hanke, M., Keator, D., Li, X., Michael, Z., Maumet, C., Nichols, B. N., Nichols, T. E., Pellman, J., … Poldrack, R. A. (2016). The brain imaging data structure, a format for organizing and describing outputs of neuroimaging experiments. Scientific Data, 3(1), 160044. https://doi.org/10.1038/sdata.2016.44 Graham, I. D., McCutcheon, C., \u0026amp; Kothari, A. (2019). Exploring the frontiers of research co-production: The Integrated Knowledge Translation Research Network concept papers. Health Research Policy and Systems, 17(1), 88, s12961-019-0501–0507. https://doi.org/10.1186/s12961-019-0501-7 GRN · German Reproducibility Network. (n.d.). German Reproducibility Network. Retrieved 10 July 2021, from https://reproducibilitynetwork.de/ Grossmann, A., \u0026amp; Brembs, B. (2021). Current market rates for scholarly publishing services. F1000Research, 10, 20. https://doi.org/10.12688/f1000research.27468.1 Grzanka, P. R., Flores, M. J., VanDaalen, R. A., \u0026amp; Velez, G. (2020). Intersectionality in psychology: Translational science for social justice. Translational Issues in Psychological Science, 6(4), 304–313. https://doi.org/10.1037/tps0000276 Guenther, E. A., \u0026amp; Rodriguez, J. K. (2020, October 14). What’s wrong with ‘manels’ and what can we do about them. The Conversation. http://theconversation.com/whats-wrong-with-manels-and-what-can-we-do-about-them-148068 Guest, O. (2017, June 5). @BrianNosek @ctitusbrown @StuartBuck1 @DaniRabaiotti @Julie_B92 @jeroenbosman @blahah404 @OSFramework Thanks! Hopefully this thread \u0026amp; many other similar discussions \u0026amp; blogs will help make it less Bropen Science and more Open Science. *hides* [Tweet]. @o_guest. https://twitter.com/o_guest/status/871675631062458368 Guest, O., \u0026amp; Martin, A. E. (2021). How Computational Modeling Can Force Theory Building in Psychological Science. Perspectives on Psychological Science, 174569162097058. https://doi.org/10.1177/1745691620970585 Guide to the UK General Data Protection Regulation (UK GDPR). (2021, July 1). ICO. https://ico.org.uk/for-organisations/guide-to-data-protection/guide-to-the-general-data-protection-regulation-gdpr/ Haak, L. L., Fenner, M., Paglione, L., Pentz, E., \u0026amp; Ratner, H. (2012). ORCID: A system to uniquely identify researchers. Learned Publishing, 25(4), 259–264. https://doi.org/10.1087/20120404 Hackett, R., \u0026amp; Kelly, S. (2020). Publishing ethics in the era of paper mills. Biology Open, 9(10), bio056556. https://doi.org/10.1242/bio.056556 Hahn, G. J., \u0026amp; Meeker, W. Q. (1993). Assumptions for Statistical Inference. The American Statistician, 47(1), 1–11. https://doi.org/10.1080/00031305.1993.10475924 Hardwicke, T. E., Bohn, M., MacDonald, K., Hembacher, E., Nuijten, M. B., Peloquin, B. N., deMayo, B. E., Long, B., Yoon, E. J., \u0026amp; Frank, M. C. (2021). Analytic reproducibility in articles receiving open data badges at the journal Psychological Science: An observational study. Royal Society Open Science, 8(1), 201494. https://doi.org/10.1098/rsos.201494 Hardwicke, T. E., Jameel, L., Jones, M., Walczak, E. J., \u0026amp; Magis-Weinberg, L. (2014). Only Human: Scientists, Systems, and Suspect Statistics. Opticon1826, 16. https://doi.org/10.5334/opt.ch Harris, C. R., Millman, K. J., van der Walt, S. J., Gommers, R., Virtanen, P., Cournapeau, D., Wieser, E., Taylor, J., Berg, S., Smith, N. J., Kern, R., Picus, M., Hoyer, S., van Kerkwijk, M. H., Brett, M., Haldane, A., del Río, J. F., Wiebe, M., Peterson, P., … Oliphant, T. E. (2020). Array programming with NumPy. Nature, 585(7825), 357–362. https://doi.org/10.1038/s41586-020-2649-2 Hart, D., \u0026amp; Silka, L. (n.d.). Rebuilding the Ivory Tower: A Bottom-Up Experiment in Aligning Research With Societal Needs. Issues in Science and Technology, 36(3), 79–85. https://issues.org/aligning-research-with-societal-needs/ Hartgerink, C. H. J., Wicherts, J. M., \u0026amp; van Assen, M. A. L. M. (2017). Too Good to be False: Nonsignificant Results Revisited. Collabra: Psychology, 3(1), 9. https://doi.org/10.1525/collabra.71 Hayes, B. C., \u0026amp; Tariq, V. N. (2000). Gender differences in scientific knowledge and attitudes toward science: A comparative study of four Anglo-American nations. Public Understanding of Science, 9(4), 433–447. https://doi.org/10.1088/0963-6625/9/4/306 Haynes, S. N., Richard, D. C. S., \u0026amp; Kubany, E. S. (1995). Content validity in psychological assessment: A functional approach to concepts and methods. Psychological Assessment, 7(3), 238–247. https://doi.org/10.1037/1040-3590.7.3.238 Healy, K. (2018). Data visualization: A practical introduction. Princeton University Press. Heathers, J. A., Anaya, J., van der Zee, T., \u0026amp; Brown, N. J. (2018). Recovering data from summary statistics: Sample Parameter Reconstruction via Iterative TEchniques (SPRITE) [Preprint]. PeerJ Preprints. https://doi.org/10.7287/peerj.preprints.26968v1 Hendriks, F., Kienhues, D., \u0026amp; Bromme, R. (2016). Trust in Science and the Science of Trust. In B. Blöbaum (Ed.), Trust and Communication in a Digitized World (pp. 143–159). Springer International Publishing. https://doi.org/10.1007/978-3-319-28059-2_8 Henrich, J., Heine, S. J., \u0026amp; Norenzayan, A. (2010). The weirdest people in the world? Behavioral and Brain Sciences, 33(2–3), 61–83. https://doi.org/10.1017/S0140525X0999152X Henrich, J. P. (2020). The WEIRDest people in the world: How the West Became Psychologically Peculiar and Particularly Prosperous. Farrar, Straus and Giroux. Herrmannova, D., \u0026amp; Knoth, P. (2016). Semantometrics: Towards Fulltext-based Research Evaluation. Proceedings of the 16th ACM/IEEE-CS on Joint Conference on Digital Libraries, 235–236. https://doi.org/10.1145/2910896.2925448 Heyman, T., Moors, P., \u0026amp; Rabagliati, H. (2020). The benefits of adversarial collaboration for commentaries. Nature Human Behaviour, 4(12), 1217–1217. https://doi.org/10.1038/s41562-020-00978-6 Higgins, J. P. T., \u0026amp; Cochrane Collaboration (Eds.). (2020). Cochrane handbook for systematic reviews of interventions (Second edition). Wiley-Blackwell. Himmelstein, D. S., Rubinetti, V., Slochower, D. R., Hu, D., Malladi, V. S., Greene, C. S., \u0026amp; Gitter, A. (2019). Open collaborative writing with Manubot. PLOS Computational Biology, 15(6), e1007128. https://doi.org/10.1371/journal.pcbi.1007128 Hirsch, J. E. (2005). An index to quantify an individual’s scientific research output. Proceedings of the National Academy of Sciences, 102(46), 16569–16572. https://doi.org/10.1073/pnas.0507655102 Hitchcock, C., Meyer, A., Rose, D., \u0026amp; Jackson, R. (2002). Providing New Access to the General Curriculum: Universal Design for Learning. TEACHING Exceptional Children, 35(2), 8–17. https://doi.org/10.1177/004005990203500201 Hoekstra, R., Kiers, H., \u0026amp; Johnson, A. (2012). Are Assumptions of Well-Known Statistical Techniques Checked, and Why (Not)? Frontiers in Psychology, 3. https://doi.org/10.3389/fpsyg.2012.00137 Hogg, D. W., Bovy, J., \u0026amp; Lang, D. (2010). Data analysis recipes: Fitting a model to data. ArXiv:1008.4686 [Astro-Ph, Physics:Physics]. http://arxiv.org/abs/1008.4686 Hoijtink, H., Mulder, J., van Lissa, C., \u0026amp; Gu, X. (2019). A tutorial on testing hypotheses using the Bayes factor. Psychological Methods, 24(5), 539–556. https://doi.org/10.1037/met0000201 Holcombe, A. O. (2019). Contributorship, Not Authorship: Use CRediT to Indicate Who Did What. Publications, 7(3), 48. https://doi.org/10.3390/publications7030048 Holden, R. R. (2010). Face Validity. In I. B. Weiner \u0026amp; W. E. Craighead (Eds.), The Corsini Encyclopedia of Psychology (p. corpsy0341). John Wiley \u0026amp; Sons, Inc. https://doi.org/10.1002/9780470479216.corpsy0341 Home | re3data.org. (n.d.). DataCite Schema. Retrieved 10 July 2021, from https://www.re3data.org/ Homepage. (n.d.). Open Science MOOC. Retrieved 9 July 2021, from https://opensciencemooc.eu/ Houtkoop, B. L., Chambers, C., Macleod, M., Bishop, D. V. M., Nichols, T. E., \u0026amp; Wagenmakers, E.-J. (2018). Data Sharing in Psychology: A Survey on Barriers and Preconditions. Advances in Methods and Practices in Psychological Science, 1(1), 70–85. https://doi.org/10.1177/2515245917751886 How to Make Inclusivity More Than Just an Office Buzzword. (n.d.). Kellogg Insight. Retrieved 9 July 2021, from https://insight.kellogg.northwestern.edu/article/how-to-make-inclusivity-more-than-just-an-office-buzzword Https://improvingpsych.org/. (n.d.). Retrieved 10 July 2021, from https://improvingpsych.org/ Huber, B., Barnidge, M., Gil de Zúñiga, H., \u0026amp; Liu, J. (2019). Fostering public trust in science: The role of social media. Public Understanding of Science, 28(7), 759–777. https://doi.org/10.1177/0963662519869097 Huber, C. (2016a, November 1). The Stata Blog » Introduction to Bayesian statistics, part 1: The basic concepts. The Stata Blog. https://blog.stata.com/2016/11/01/introduction-to-bayesian-statistics-part-1-the-basic-concepts/ Huber, C. (2016b, November 15). Introduction to Bayesian statistics, part 2: MCMC and the Metropolis–Hastings algorithm. The Stata Blog. https://blog.stata.com/2016/11/15/introduction-to-bayesian-statistics-part-2-mcmc-and-the-metropolis-hastings-algorithm/ Huelin, R., Iheanacho, I., Payne, K., \u0026amp; Sandman, K. (2015). What’s in a Name? Systematic and Non-Systematic Literature Reviews, and Why the Distinction Matters—Evidera (The Evidence Forum, pp. 34–37). https://www.evidera.com/resource/whats-in-a-name-systematic-and-non-systematic-literature-reviews-and-why-the-distinction-matters/ Hüffmeier, J., Mazei, J., \u0026amp; Schultze, T. (2016). Reconceptualizing replication as a sequence of different studies: A replication typology. Journal of Experimental Social Psychology, 66, 81–92. https://doi.org/10.1016/j.jesp.2015.09.009 Hunter, J. E., \u0026amp; Schmidt, F. L. (2015). Methods of meta-analysis: Correcting error and bias in research findings (Third edition). SAGE. Hurlbert, S. H. (1984). Pseudoreplication and the Design of Ecological Field Experiments. Ecological Monographs, 54(2), 187–211. https://doi.org/10.2307/1942661 ICMJE | Home. (n.d.). International Committee of Medical Journal Editors. Retrieved 11 July 2021, from http://www.icmje.org/ Ikeda A., Xu H., Fuji N., Zhu S., \u0026amp; Yamada Y. (2019). Questionable research practices following pre-registration. 心理学評論刊行会. https://doi.org/10.24602/sjpr.62.3_281 Initial revision of ‘git’, the information manager from hell · git/git@e83c516. (n.d.). GitHub. Retrieved 9 July 2021, from https://github.com/git/git/commit/e83c5163316f89bfbde7d9ab23ca2e25604af290 International Committee of Medical Journal Editors. (n.d.). ICMJE | Recommendations | Author Responsibilities—Disclosure of Financial and Non-Financial Relationships and Activities, and Conflicts of Interest. ICJME. http://www.icmje.org/recommendations/browse/roles-and-responsibilities/author-responsibilities--conflicts-of-interest.html INVOLVE – INVOLVE Supporting public involvement in NHS, public health and social care research. (n.d.). Retrieved 9 July 2021, from https://www.invo.org.uk/ Ioannidis, J. P. A. (2005). Why Most Published Research Findings Are False. PLoS Medicine, 2(8), e124. https://doi.org/10.1371/journal.pmed.0020124 Ioannidis, J. P. A., Fanelli, D., Dunne, D. D., \u0026amp; Goodman, S. N. (2015). Meta-research: Evaluation and Improvement of Research Methods and Practices. PLOS Biology, 13(10), e1002264. https://doi.org/10.1371/journal.pbio.1002264 JabRef—Free Reference Manager—Stay on top of your Literature. (n.d.). JabRef. Retrieved 9 July 2021, from https://www.jabref.org/ Jacobson, D., \u0026amp; Mustafa, N. (2019). Social Identity Map: A Reflexivity Tool for Practicing Explicit Positionality in Critical Qualitative Research. International Journal of Qualitative Methods, 18, 160940691987007. https://doi.org/10.1177/1609406919870075 Jafar, A. J. N. (2018). What is positionality and should it be expressed in quantitative studies? Emergency Medicine Journal, emermed-2017-207158. https://doi.org/10.1136/emermed-2017-207158 James, K. L., Randall, N. P., \u0026amp; Haddaway, N. R. (2016). A methodology for systematic mapping in environmental sciences. Environmental Evidence, 5(1), 7. https://doi.org/10.1186/s13750-016-0059-6 Jamovi—Stats. Open. Now. (n.d.). Jamovi. Retrieved 9 July 2021, from https://www.jamovi.org/ Jannot, A.-S., Agoritsas, T., Gayet-Ageron, A., \u0026amp; Perneger, T. V. (2013). Citation bias favoring statistically significant studies was present in medical research. Journal of Clinical Epidemiology, 66(3), 296–301. https://doi.org/10.1016/j.jclinepi.2012.09.015 John, L. K., Loewenstein, G., \u0026amp; Prelec, D. (2012). Measuring the Prevalence of Questionable Research Practices With Incentives for Truth Telling. Psychological Science, 23(5), 524–532. https://doi.org/10.1177/0956797611430953 Jones, A., Worrall, S., Rudin, L., Duckworth, J. J., \u0026amp; Christiansen, P. (2021). May I have your attention, please? Methodological and analytical flexibility in the addiction stroop. Addiction Research \u0026amp; Theory, 1–14. https://doi.org/10.1080/16066359.2021.1876847 Joseph, T. D., \u0026amp; Hirshfield, L. E. (2011). ‘Why don’t you get somebody new to do it?’ Race and cultural taxation in the academy. Ethnic and Racial Studies, 34(1), 121–141. https://doi.org/10.1080/01419870.2010.496489 Kalliamvakou, E., Gousios, G., Blincoe, K., Singer, L., German, D. M., \u0026amp; Damian, D. (2014). The promises and perils of mining GitHub. Proceedings of the 11th Working Conference on Mining Software Repositories - MSR 2014, 92–101. https://doi.org/10.1145/2597073.2597074 kamraro. (2014, April 1). Responsible research \u0026amp; innovation [Text]. Horizon 2020 - European Commission. https://ec.europa.eu/programmes/horizon2020/en/h2020-section/responsible-research-innovation Kathawalla, U.-K., Silverstein, P., \u0026amp; Syed, M. (2021). Easing Into Open Science: A Guide for Graduate Students and Their Advisors. Collabra: Psychology, 7(1), 18684. https://doi.org/10.1525/collabra.18684 Kelley, T. (1927). Interpretation of educational measurements. World Book Co. Kerr, J. R., \u0026amp; Wilson, M. S. (2021). Right-wing authoritarianism and social dominance orientation predict rejection of science and scientists. Group Processes \u0026amp; Intergroup Relations, 24(4), 550–567. https://doi.org/10.1177/1368430221992126 Kerr, N. L. (1998). HARKing: Hypothesizing After the Results are Known. Personality and Social Psychology Review, 2(3), 196–217. https://doi.org/10.1207/s15327957pspr0203_4 Kerr, N. L., Ao, X., Hogg, M. A., \u0026amp; Zhang, J. (2018). Addressing replicability concerns via adversarial collaboration: Discovering hidden moderators of the minimal intergroup discrimination effect. Journal of Experimental Social Psychology, 78, 66–76. https://doi.org/10.1016/j.jesp.2018.05.001 Kidwell, M. C., Lazarević, L. B., Baranski, E., Hardwicke, T. E., Piechowski, S., Falkenberg, L.-S., Kennett, C., Slowik, A., Sonnleitner, C., Hess-Holden, C., Errington, T. M., Fiedler, S., \u0026amp; Nosek, B. A. (2016). Badges to Acknowledge Open Practices: A Simple, Low-Cost, Effective Method for Increasing Transparency. PLOS Biology, 14(5), e1002456. https://doi.org/10.1371/journal.pbio.1002456 Kienzler, H., \u0026amp; Fontanesi, C. (2017). Learning through inquiry: A Global Health Hackathon. Teaching in Higher Education, 22(2), 129–142. https://doi.org/10.1080/13562517.2016.1221805 Kiernan, C. (1999). Participation in Research by People with Learning Disability: Origins and Issues. British Journal of Learning Disabilities, 27(2), 43–47. https://doi.org/10.1111/j.1468-3156.1999.tb00084.x King, G. (1995). Replication, Replication. PS: Political Science and Politics, 28(3), 444. https://doi.org/10.2307/420301 Kitzes, J., Turek, D., \u0026amp; Deniz, F. (Eds.). (2018). The practice of reproducible research: Case studies and lessons from the data-intensive sciences. University of California Press. Kiureghian, A. D., \u0026amp; Ditlevsen, O. (2009). Aleatory or epistemic? Does it matter? Structural Safety, 31(2), 105–112. https://doi.org/10.1016/j.strusafe.2008.06.020 Klein, O., Hardwicke, T. E., Aust, F., Breuer, J., Danielsson, H., Mohr, A. H., IJzerman, H., Nilsonne, G., Vanpaemel, W., \u0026amp; Frank, M. C. (2018). A Practical Guide for Transparency in Psychological Science. Collabra: Psychology, 4(1), 20. https://doi.org/10.1525/collabra.158 Klein, R. A., Ratliff, K. A., Vianello, M., Adams, R. B., Bahník, Š., Bernstein, M. J., Bocian, K., Brandt, M. J., Brooks, B., Brumbaugh, C. C., Cemalcilar, Z., Chandler, J., Cheong, W., Davis, W. E., Devos, T., Eisner, M., Frankowska, N., Furrow, D., Galliani, E. M., … Nosek, B. A. (2014). Investigating Variation in Replicability: A “Many Labs” Replication Project. Social Psychology, 45(3), 142–152. https://doi.org/10.1027/1864-9335/a000178 Klein, R. A., Vianello, M., Hasselman, F., Adams, B. G., Adams, R. B., Alper, S., Aveyard, M., Axt, J. R., Babalola, M. T., Bahník, Š., Batra, R., Berkics, M., Bernstein, M. J., Berry, D. R., Bialobrzeska, O., Binan, E. D., Bocian, K., Brandt, M. J., Busching, R., … Nosek, B. A. (2018). Many Labs 2: Investigating Variation in Replicability Across Samples and Settings. Advances in Methods and Practices in Psychological Science, 1(4), 443–490. https://doi.org/10.1177/2515245918810225 Kleinberg, B., Mozes, M., van der Toolen, Y., \u0026amp; Verschuere, B. (2017). NETANOS - Named entity-based Text Anonymization for Open Science [Preprint]. Open Science Framework. https://doi.org/10.31219/osf.io/w9nhb Knoth, P., \u0026amp; Herrmannova, D. (n.d.). Towards Semantometrics: A New Semantic Similarity Based Measure for Assessing a Research Publication’s Contribution. D-Lib Magazine, 20(11/12), 8. https://doi.org/10.1045/november14-knoth Koole, S. L., \u0026amp; Lakens, D. (2012). Rewarding Replications: A Sure and Simple Way to Improve Psychological Science. Perspectives on Psychological Science, 7(6), 608–614. https://doi.org/10.1177/1745691612462586 Kreuter, F. (Ed.). (2013). Improving Surveys with Paradata: Analytic Uses of Process Information. John Wiley \u0026amp; Sons, Inc. https://doi.org/10.1002/9781118596869 Kruschke, J. K. (2015). Doing Bayesian data analysis: A tutorial with R, JAGS, and Stan (2nd ed.). Academic Press. Kuhn, T. S. (1996). The structure of scientific revolutions (3rd ed). University of Chicago Press. Kukull, W. A., \u0026amp; Ganguli, M. (2012). Generalizability: The trees, the forest, and the low-hanging fruit. Neurology, 78(23), 1886–1891. https://doi.org/10.1212/WNL.0b013e318258f812 L. Haven, T., \u0026amp; Van Grootel, Dr. L. (2019). Preregistering qualitative research. Accountability in Research, 26(3), 229–244. https://doi.org/10.1080/08989621.2019.1580147 Laakso, M., \u0026amp; Björk, B.-C. (2013). Delayed open access: An overlooked high-impact category of openly available scientific literature. Journal of the American Society for Information Science and Technology, 64(7), 1323–1329. https://doi.org/10.1002/asi.22856 Laine, H. (2017). Afraid of Scooping – Case Study on Researcher Strategies against Fear of Scooping in the Context of Open Science. Data Science Journal, 16, 29. https://doi.org/10.5334/dsj-2017-029 Lakatos, I. (1978). The Methodology of Scientiﬁc Research Programs: Vol. I. Cambridge University Press. Lakens, D. (2014). Performing high-powered studies efficiently with sequential analyses: Sequential analyses. European Journal of Social Psychology, 44(7), 701–710. https://doi.org/10.1002/ejsp.2023 Lakens, D. (2020a, May 11). The 20% Statistician: Red Team Challenge. The 20% Statistician. http://daniellakens.blogspot.com/2020/05/red-team-challenge.html Lakens, D. (2020b). Pandemic researchers—Recruit your own best critics. Nature, 581(7807), 121–121. https://doi.org/10.1038/d41586-020-01392-8 Lakens, D. (2021a). Sample Size Justification [Preprint]. PsyArXiv. https://doi.org/10.31234/osf.io/9d3yf Lakens, D. (2021b). The Practical Alternative to the p Value Is the Correctly Used p Value. Perspectives on Psychological Science, 16(3), 639–648. https://doi.org/10.1177/1745691620958012 Lakens, D., McLatchie, N., Isager, P. M., Scheel, A. M., \u0026amp; Dienes, Z. (2020). Improving Inferences About Null Effects With Bayes Factors and Equivalence Tests. The Journals of Gerontology: Series B, 75(1), 45–57. https://doi.org/10.1093/geronb/gby065 Lakens, D., Scheel, A. M., \u0026amp; Isager, P. M. (2018). Equivalence Testing for Psychological Research: A Tutorial. Advances in Methods and Practices in Psychological Science, 1(2), 259–269. https://doi.org/10.1177/2515245918770963 Largent, E. A., \u0026amp; Snodgrass, R. T. (2016). Blind Peer Review by Academic Journals. In Blinding as a Solution to Bias (pp. 75–95). Elsevier. https://doi.org/10.1016/B978-0-12-802460-7.00005-X Larivière, V., Desrochers, N., Macaluso, B., Mongeon, P., Paul-Hus, A., \u0026amp; Sugimoto, C. R. (2016). Contributorship and division of labor in knowledge production. Social Studies of Science, 46(3), 417–435. https://doi.org/10.1177/0306312716650046 Lazic, S. E. (2019, September 16). Genuine replication and pseudoreplication: What’s the difference? | BMJ Open Science. BMJ Open Science. https://blogs.bmj.com/openscience/2019/09/16/genuine-replication-and-pseudoreplication-whats-the-difference/ Leavens, D. A., Bard, K. A., \u0026amp; Hopkins, W. D. (2010). BIZARRE chimpanzees do not represent “the chimpanzee”. Behavioral and Brain Sciences, 33(2–3), 100–101. https://doi.org/10.1017/S0140525X10000166 Leavy, P. (2017). Research design: Quantitative, qualitative, mixed methods, arts-based, and community-based participatory research approaches. Guilford Press. LeBel, E. P., McCarthy, R. J., Earp, B. D., Elson, M., \u0026amp; Vanpaemel, W. (2018). A Unified Framework to Quantify the Credibility of Scientific Findings. Advances in Methods and Practices in Psychological Science, 1(3), 389–402. https://doi.org/10.1177/2515245918787489 LeBel, E. P., Vanpaemel, W., Cheung, I., \u0026amp; Campbell, L. (2019). A Brief Guide to Evaluate Replications. Meta-Psychology, 3. https://doi.org/10.15626/MP.2018.843 Ledgerwood, A., Hudson, S. T. J., Lewis, N. A., Maddox, K. B., Pickett, C., Remedios, J. D., Cheryan, S., Diekman, A., Dutra, N. B., Goh, J. X., Goodwin, S., Munakata, Y., Navarro, D., Onyeador, I. N., Srivastava, S., \u0026amp; Wilkins, C. L. (2021). The Pandemic as a Portal: Reimagining Psychological Science as Truly Open and Inclusive [Preprint]. PsyArXiv. https://doi.org/10.31234/osf.io/gdzue Lee, R. M. (1993). Doing research on sensitive topics. Sage Publications. Lewandowsky, S., \u0026amp; Bishop, D. (2016). Research integrity: Don’t let transparency damage science. Nature, 529(7587), 459–461. https://doi.org/10.1038/529459a Lewandowsky, S., \u0026amp; Oberauer, K. (2021). Worldview-motivated rejection of science and the norms of science. Cognition, 215, 104820. https://doi.org/10.1016/j.cognition.2021.104820 Licenses \u0026amp; Standards | Open Source Initiative. (n.d.). Open Source Initative. Retrieved 9 July 2021, from https://opensource.org/licenses Lin, D., Crabtree, J., Dillo, I., Downs, R. R., Edmunds, R., Giaretta, D., De Giusti, M., L’Hours, H., Hugo, W., Jenkyns, R., Khodiyar, V., Martone, M. E., Mokrane, M., Navale, V., Petters, J., Sierman, B., Sokolova, D. V., Stockhause, M., \u0026amp; Westbrook, J. (2020). The TRUST Principles for digital repositories. Scientific Data, 7(1), 144. https://doi.org/10.1038/s41597-020-0486-7 Lind, F., Gruber, M., \u0026amp; Boomgaarden, H. G. (2017). Content Analysis by the Crowd: Assessing the Usability of Crowdsourcing for Coding Latent Constructs. Communication Methods and Measures, 11(3), 191–209. https://doi.org/10.1080/19312458.2017.1317338 Lindsay, D. S. (2015). Replication in Psychological Science. Psychological Science, 26(12), 1827–1832. https://doi.org/10.1177/0956797615616374 Lindsay, D. S. (2020). Seven steps toward transparency and replicability in psychological science. Canadian Psychology/Psychologie Canadienne, 61(4), 310–317. https://doi.org/10.1037/cap0000222 Lintott, C. J., Schawinski, K., Slosar, A., Land, K., Bamford, S., Thomas, D., Raddick, M. J., Nichol, R. C., Szalay, A., Andreescu, D., Murray, P., \u0026amp; Vandenberg, J. (2008). Galaxy Zoo: Morphologies derived from visual inspection of galaxies from the Sloan Digital Sky Survey ★. Monthly Notices of the Royal Astronomical Society, 389(3), 1179–1189. https://doi.org/10.1111/j.1365-2966.2008.13689.x Liu, H., \u0026amp; Priest, S. (2009). Understanding public support for stem cell research: Media communication, interpersonal communication and trust in key actors. Public Understanding of Science, 18(6), 704–718. https://doi.org/10.1177/0963662508097625 Liu, Y., Gordon, M., Wang, J., Bishop, M., Chen, Y., Pfeiffer, T., Twardy, C., \u0026amp; Viganola, D. (2020). Replication Markets: Results, Lessons, Challenges and Opportunities in AI Replication. ArXiv:2005.04543 [Cs]. http://arxiv.org/abs/2005.04543 Longino, H. E. (1990). Science as social knowledge: Values and objectivity in scientific inquiry. Princeton University Press. Longino, H. E. (1992). Taking Gender Seriously in Philosophy of Science. PSA: Proceedings of the Biennial Meeting of the Philosophy of Science Association, 1992(2), 333–340. https://doi.org/10.1086/psaprocbienmeetp.1992.2.192847 Lu, J., Qiu, Y., \u0026amp; Deng, A. (2019). A note on Type S/M errors in hypothesis testing. British Journal of Mathematical and Statistical Psychology, 72(1), 1–17. https://doi.org/10.1111/bmsp.12132 Lüdtke, O., Ulitzsch, E., \u0026amp; Robitzsch, A. (2020). A Comparison of Penalized Maximum Likelihood Estimation and Markov Chain Monte Carlo Techniques for Estimating Confirmatory Factor Analysis Models with Small Sample Sizes [Preprint]. PsyArXiv. https://doi.org/10.31234/osf.io/u3qag Lutz, M. (2019). Programming Python (Fourth edition). O’Reilly. Lynch, Jr., J. G. (1982). On the External Validity of Experiments in Consumer Research. Journal of Consumer Research, 9(3), 225. https://doi.org/10.1086/208919 Macfarlane, B., \u0026amp; Cheng, M. (2008). Communism, Universalism and Disinterestedness: Re-examining Contemporary Support among Academics for Merton’s Scientific Norms. Journal of Academic Ethics, 6(1), 67–78. https://doi.org/10.1007/s10805-008-9055-y Makowski, D., Ben-Shachar, M. S., Chen, S. H. A., \u0026amp; Lüdecke, D. (2019). Indices of Effect Existence and Significance in the Bayesian Framework. Frontiers in Psychology, 10, 2767. https://doi.org/10.3389/fpsyg.2019.02767 Martinez-Acosta, V. G., \u0026amp; Favero, C. B. (2018). A Discussion of Diversity and Inclusivity at the Institutional Level: The Need for a Strategic Plan. Journal of Undergraduate Neuroscience Education: JUNE: A Publication of FUN, Faculty for Undergraduate Neuroscience, 16(3), A252–A260. Marwick, B., Boettiger, C., \u0026amp; Mullen, L. (2018). Packaging Data Analytical Work Reproducibly Using R (and Friends). The American Statistician, 72(1), 80–88. https://doi.org/10.1080/00031305.2017.1375986 Masur, P. K. (2020). Understanding the Effects of Analytical Choices on Finding the Privacy Paradox: A Specification Curve Analysis of Large-Scale Survey Data [Preprint]. Open Science Framework. https://osf.io/m72gb/ McElreath, R. (2020). Statistical rethinking: A Bayesian course with examples in R and Stan (2nd ed.). Taylor and Francis, CRC Press. McNutt, M. K., Bradford, M., Drazen, J. M., Hanson, B., Howard, B., Jamieson, K. H., Kiermer, V., Marcus, E., Pope, B. K., Schekman, R., Swaminathan, S., Stang, P. J., \u0026amp; Verma, I. M. (2018). Transparency in authors’ contributions and responsibilities to promote integrity in scientific publication. Proceedings of the National Academy of Sciences, 115(11), 2557–2560. https://doi.org/10.1073/pnas.1715374115 Medical Research Centre. (2019). Identifiability, anonymisation and pseudonymisation. Medical Research Centre. https://mrc.ukri.org/documents/pdf/gdpr-guidance-note-5-identifiability-anonymisation-and-pseudonymisation/ Medin, D. L. (2012, February 1). Rigor Without Rigor Mortis: The APS Board Discusses Research Integrity [Blog]. Association for Psychological Science. https://www.psychologicalscience.org/observer/scientific-rigor Melissa S. Anderson, Emily A. Ronning, Raymond De Vries, \u0026amp; Brian C. Martinson. (2010). Extending the Mertonian Norms: Scientists’ Subscription to Norms of Research. The Journal of Higher Education, 81(3), 366–393. https://doi.org/10.1353/jhe.0.0095 Mellers, B., Hertwig, R., \u0026amp; Kahneman, D. (2001). Do Frequency Representations Eliminate Conjunction Effects? An Exercise in Adversarial Collaboration. Psychological Science, 12(4), 269–275. https://doi.org/10.1111/1467-9280.00350 Menke, C. (2015). A Note on Science and Democracy? Robert K. Mertons Ethos of Science. In R. Klausnitzer, C. Spoerhase, \u0026amp; D. Werle (Eds.), Ethos und Pathos der Geisteswissenschaften. DE GRUYTER. https://doi.org/10.1515/9783110375008-013 Mertens, G., \u0026amp; Krypotos, A.-M. (2019). Preregistration of Analyses of Preexisting Data. Psychologica Belgica, 59(1), 338–352. https://doi.org/10.5334/pb.493 Merton, R. K. (1938). Science and the Social Order. Philosophy of Science, 5(3), 321–337. https://doi.org/10.1086/286513 Merton, R. K. (1968). The Matthew Effect in Science: The reward and communication systems of science are considered. Science, 159(3810), 56–63. https://doi.org/10.1126/science.159.3810.56 Meslin, E. M. (2008). Achieving global justice in health through global research ethics: Supplementing Macklin’s ‘top-down’ approach with one from the ‘ground up’. In R. M. Green, A. Donovan, \u0026amp; S. A. Jauss (Eds.), Global bioethics: Issues of conscience for the twenty-first century (pp. 163–177). Clarendon Press ; Oxford University Press. Michener, W. K. (2015). Ten Simple Rules for Creating a Good Data Management Plan. PLOS Computational Biology, 11(10), e1004525. https://doi.org/10.1371/journal.pcbi.1004525 Mischel, W. (2009, January 1). Becoming a Cumulative Science. Association for Psychological Science. https://www.psychologicalscience.org/observer/becoming-a-cumulative-science Moher, D., Bouter, L., Kleinert, S., Glasziou, P., Sham, M. H., Barbour, V., Coriat, A.-M., Foeger, N., \u0026amp; Dirnagl, U. (2020). The Hong Kong Principles for assessing researchers: Fostering research integrity. PLOS Biology, 18(7), e3000737. https://doi.org/10.1371/journal.pbio.3000737 Moher, D., Liberati, A., Tetzlaff, J., Altman, D. G., \u0026amp; The PRISMA Group. (2009). Preferred Reporting Items for Systematic Reviews and Meta-Analyses: The PRISMA Statement. PLoS Medicine, 6(7), e1000097. https://doi.org/10.1371/journal.pmed.1000097 Moher, D., Naudet, F., Cristea, I. A., Miedema, F., Ioannidis, J. P. A., \u0026amp; Goodman, S. N. (2018). Assessing scientists for hiring, promotion, and tenure. PLOS Biology, 16(3), e2004089. https://doi.org/10.1371/journal.pbio.2004089 Monroe, K. R. (2018). The Rush to Transparency: DA-RT and the Potential Dangers for Qualitative Research. Perspectives on Politics, 16(1), 141–148. https://doi.org/10.1017/S153759271700336X Morabia, A., Have, T. T., \u0026amp; Landis, J. R. (1997). Interaction Fallacy. Journal of Clinical Epidemiology, 50(7), 809–812. https://doi.org/10.1016/S0895-4356(97)00053-X Moran, H., Karlin, L., Lauchlan, E., Rappaport, S. J., Bleasdale, B., Wild, L., \u0026amp; Dorr, J. (2020). Understanding Research Culture: What researchers think about the culture they work in. Wellcome Open Research, 5, 201. https://doi.org/10.12688/wellcomeopenres.15832.1 Moretti, M. (2020, August 12). Beyond Open-washing: Are Narratives the Future of Open Data Portals? | by matteo moretti | Nightingale | Medium. Nightingale. https://medium.com/nightingale/beyond-open-washing-are-stories-and-narratives-the-future-of-open-data-portals-93228d8882f3 Morey, R. D., Chambers, C. D., Etchells, P. J., Harris, C. R., Hoekstra, R., Lakens, D., Lewandowsky, S., Morey, C. C., Newman, D. P., Schönbrodt, F. D., Vanpaemel, W., Wagenmakers, E.-J., \u0026amp; Zwaan, R. A. (2016). The Peer Reviewers’ Openness Initiative: Incentivizing open research practices through peer review. Royal Society Open Science, 3(1), 150547. https://doi.org/10.1098/rsos.150547 Morgan, C. (1998). The DOI (Digital Object Identifier). Serials: The Journal for the Serials Community, 11(1), 47–51. https://doi.org/10.1629/1147 Moshontz, H., Campbell, L., Ebersole, C. R., IJzerman, H., Urry, H. L., Forscher, P. S., Grahe, J. E., McCarthy, R. J., Musser, E. D., Antfolk, J., Castille, C. M., Evans, T. R., Fiedler, S., Flake, J. K., Forero, D. A., Janssen, S. M. J., Keene, J. R., Protzko, J., Aczel, B., … Chartier, C. R. (2018). The Psychological Science Accelerator: Advancing Psychology Through a Distributed Collaborative Network. Advances in Methods and Practices in Psychological Science, 1(4), 501–515. https://doi.org/10.1177/2515245918797607 Moshontz, H., Ebersole, C. R., Weston, S. J., \u0026amp; Klein, R. A. (2021). A guide for many authors: Writing manuscripts in large collaborations. Social and Personality Psychology Compass, 15(4). https://doi.org/10.1111/spc3.12590 Mourby, M., Mackey, E., Elliot, M., Gowans, H., Wallace, S. E., Bell, J., Smith, H., Aidinlis, S., \u0026amp; Kaye, J. (2018). Are ‘pseudonymised’ data always personal data? Implications of the GDPR for administrative data research in the UK. Computer Law \u0026amp; Security Review, 34(2), 222–233. https://doi.org/10.1016/j.clsr.2018.01.002 Muller, J. Z. (2018). The tyranny of metrics. Princeton University Press. Munn, Z., Peters, M. D. J., Stern, C., Tufanaru, C., McArthur, A., \u0026amp; Aromataris, E. (2018). Systematic review or scoping review? Guidance for authors when choosing between a systematic or scoping review approach. BMC Medical Research Methodology, 18(1), 143. https://doi.org/10.1186/s12874-018-0611-x Muthukrishna, M., Bell, A. V., Henrich, J., Curtin, C. M., Gedranovich, A., McInerney, J., \u0026amp; Thue, B. (2020). Beyond Western, Educated, Industrial, Rich, and Democratic (WEIRD) Psychology: Measuring and Mapping Scales of Cultural and Psychological Distance. Psychological Science, 31(6), 678–701. https://doi.org/10.1177/0956797620916782 Naudet, F., Ioannidis, J. P. A., Miedema, F., Cristea, I. A., Goodman, Steven N., J., \u0026amp; Moher, D. (2018, June 4). Six principles for assessing scientists for hiring, promotion, and tenure. Impact of Social Sciences Blog. http://eprints.lse.ac.uk/90753/ Navarro, D. (2020). Paths in strange spaces: A comment on preregistration [Preprint]. PsyArXiv. https://doi.org/10.31234/osf.io/wxn58 Nelson, L. D., Simmons, J. P., \u0026amp; Simonsohn, U. (2012). Let’s Publish Fewer Papers. Psychological Inquiry, 23(3), 291–293. https://doi.org/10.1080/1047840X.2012.705245 Neuroskeptic. (2012). The Nine Circles of Scientific Hell. Perspectives on Psychological Science, 7(6), 643–644. https://doi.org/10.1177/1745691612459519 Nichols, T. E., Das, S., Eickhoff, S. B., Evans, A. C., Glatard, T., Hanke, M., Kriegeskorte, N., Milham, M. P., Poldrack, R. A., Poline, J.-B., Proal, E., Thirion, B., Van Essen, D. C., White, T., \u0026amp; Yeo, B. T. T. (2017). Best practices in data analysis and sharing in neuroimaging using MRI. Nature Neuroscience, 20(3), 299–303. https://doi.org/10.1038/nn.4500 Nickerson, R. S. (1998). Confirmation Bias: A Ubiquitous Phenomenon in Many Guises. Review of General Psychology, 2(2), 175–220. https://doi.org/10.1037/1089-2680.2.2.175 Nieuwenhuis, S., Forstmann, B. U., \u0026amp; Wagenmakers, E.-J. (2011). Erroneous analyses of interactions in neuroscience: A problem of significance. Nature Neuroscience, 14(9), 1105–1107. https://doi.org/10.1038/nn.2886 Nimon, K. F. (2012). Statistical Assumptions of Substantive Analyses Across the General Linear Model: A Mini-Review. Frontiers in Psychology, 3. https://doi.org/10.3389/fpsyg.2012.00322 Nisbet, M. C., Scheufele, D. A., Shanahan, J., Moy, P., Brossard, D., \u0026amp; Lewenstein, B. V. (2002). Knowledge, Reservations, or Promise?: A Media Effects Model for Public Perceptions of Science and Technology. Communication Research, 29(5), 584–608. https://doi.org/10.1177/009365002236196 Nittrouer, C. L., Hebl, M. R., Ashburn-Nardo, L., Trump-Steele, R. C. E., Lane, D. M., \u0026amp; Valian, V. (2018). Gender disparities in colloquium speakers at top universities. Proceedings of the National Academy of Sciences, 115(1), 104–108. https://doi.org/10.1073/pnas.1708414115 Nosek, B. A. (2019, June 11). Strategy for Culture Change. Center for Open Science. https://www.cos.io/blog/strategy-for-culture-change Nosek, B. A., \u0026amp; Bar-Anan, Y. (2012). Scientific Utopia: I. Opening Scientific Communication. Psychological Inquiry, 23(3), 217–243. https://doi.org/10.1080/1047840X.2012.692215 Nosek, B. A., Ebersole, C. R., DeHaven, A. C., \u0026amp; Mellor, D. T. (2018). The preregistration revolution. Proceedings of the National Academy of Sciences, 115(11), 2600–2606. https://doi.org/10.1073/pnas.1708274114 Nosek, B. A., \u0026amp; Errington, T. M. (2020). What is replication? PLOS Biology, 18(3), e3000691. https://doi.org/10.1371/journal.pbio.3000691 Nosek, B. A., \u0026amp; Lakens, D. (2014). Registered Reports: A Method to Increase the Credibility of Published Results. Social Psychology, 45(3), 137–141. https://doi.org/10.1027/1864-9335/a000192 Nosek, B. A., Spies, J. R., \u0026amp; Motyl, M. (2012). Scientific Utopia: II. Restructuring Incentives and Practices to Promote Truth Over Publishability. Perspectives on Psychological Science, 7(6), 615–631. https://doi.org/10.1177/1745691612459058 Noy, N. F., \u0026amp; Guinness, D. L. (2001). Ontology Development 101: A Guide to Creating Your First Ontology. Stanford Knowledge Systems Laboratory Technical Report\u0026nbsp; KSL-01-05 and Stanford Medical Informatics Technical Report SMI-2001-0880. https://protege.stanford.edu/publications/ontology_development/ontology101.pdf Nuijten, M. B., Hartgerink, C. H. J., van Assen, M. A. L. M., Epskamp, S., \u0026amp; Wicherts, J. M. (2016). The prevalence of statistical reporting errors in psychology (1985–2013). Behavior Research Methods, 48(4), 1205–1226. https://doi.org/10.3758/s13428-015-0664-2 Nüst, D., Boettiger, C., \u0026amp; Marwick, B. (2018). How to Read a Research Compendium. ArXiv:1806.09525 [Cs]. http://arxiv.org/abs/1806.09525 Obels, P., Lakens, D., Coles, N. A., Gottfried, J., \u0026amp; Green, S. A. (2020). Analysis of Open Data and Computational Reproducibility in Registered Reports in Psychology. Advances in Methods and Practices in Psychological Science, 3(2), 229–237. https://doi.org/10.1177/2515245920918872 Oberauer, K., \u0026amp; Lewandowsky, S. (2019). Addressing the theory crisis in psychology. Psychonomic Bulletin \u0026amp; Review, 26(5), 1596–1618. https://doi.org/10.3758/s13423-019-01645-2 OER Commons. (n.d.). OER Commons. Retrieved 9 July 2021, from https://www.oercommons.org/ Open Aire. (n.d.). Amnesia Anonymization Tool—Data anonymization made easy. High Accuracy Data Anonymisation. Retrieved 9 July 2021, from https://amnesia.openaire.eu/ Open Educational Resources (OER). (2017, July 20). UNESCO. https://en.unesco.org/themes/building-knowledge-societies/oer Open Scholarship Knowledge Base | OER Commons. (n.d.). OER Commons. Retrieved 9 July 2021, from https://www.oercommons.org/hubs/OSKB Open Science Collaboration. (2015). Estimating the reproducibility of psychological science. Science, 349(6251), aac4716–aac4716. https://doi.org/10.1126/science.aac4716 Open Source in Open Science | FOSTER. (n.d.). Foster. Retrieved 9 July 2021, from https://www.fosteropenscience.eu/foster-taxonomy/open-source-open-science Orben, A. (2019). A journal club to fix science. Nature, 573(7775), 465–465. https://doi.org/10.1038/d41586-019-02842-8 ORCID. (n.d.). ORCID. Retrieved 9 July 2021, from https://orcid.org/ OSF. (n.d.). Open Science Framework. Retrieved 9 July 2021, from https://osf.io/ OSF | StudySwap: A platform for interlab replication, collaboration, and research resource exchange. (n.d.). Retrieved 10 July 2021, from https://osf.io/meetings/StudySwap/ Ottmann, G., Laragy, C., Allen, J., \u0026amp; Feldman, P. (2011). Coproduction in Practice: Participatory Action Research to Develop a Model of Community Aged Care. Systemic Practice and Action Research, 24(5), 413–427. https://doi.org/10.1007/s11213-011-9192-x Our Approach | Co-Production Collective. (n.d.). Co-Production Collective. Retrieved 9 July 2021, from https://www.coproductioncollective.co.uk/what-is-co-production/our-approach Padilla, A. M. (1994). Research news and Comment: Ethnic Minority Scholars; Research, and Mentoring: Current and Future Issues. Educational Researcher, 23(4), 24–27. https://doi.org/10.3102/0013189X023004024 Page, M. J., Moher, D., Bossuyt, P. M., Boutron, I., Hoffmann, T. C., Mulrow, C. D., Shamseer, L., Tetzlaff, J. M., Akl, E. A., Brennan, S. E., Chou, R., Glanville, J., Grimshaw, J. M., Hróbjartsson, A., Lalu, M. M., Li, T., Loder, E. W., Mayo-Wilson, E., McDonald, S., … McKenzie, J. E. (2021). PRISMA 2020 explanation and elaboration: Updated guidance and exemplars for reporting systematic reviews. BMJ, n160. https://doi.org/10.1136/bmj.n160 Patience, G. S., Galli, F., Patience, P. A., \u0026amp; Boffito, D. C. (2019). Intellectual contributions meriting authorship: Survey results from the top cited authors across all science categories. PLOS ONE, 14(1), e0198117. https://doi.org/10.1371/journal.pone.0198117 Pautasso, M. (2013). Ten Simple Rules for Writing a Literature Review. PLoS Computational Biology, 9(7), e1003149. https://doi.org/10.1371/journal.pcbi.1003149 Pavlov, Y. G., Adamian, N., Appelhoff, S., Arvaneh, M., Benwell, C., Beste, C., Bland, A., Bradford, D. E., Bublatzky, F., Busch, N., Clayson, P. E., Cruse, D., Czeszumski, A., Dreber, A., Dumas, G., Ehinger, B. V., Ganis, G., He, X., Hinojosa, J. A., … Mushtaq, F. (2020). #EEGManyLabs: Investigating the Replicability of Influential EEG Experiments [Preprint]. PsyArXiv. https://doi.org/10.31234/osf.io/528nr PCI Registered Reports. (n.d.). PCI. Retrieved 9 July 2021, from https://rr.peercommunityin.org/about/about Peer Community In – A free recommendation process of scientific preprints based on peer-reviews. (n.d.). Retrieved 9 July 2021, from https://peercommunityin.org/ Peer, E., Brandimarte, L., Samat, S., \u0026amp; Acquisti, A. (2017). Beyond the Turk: Alternative platforms for crowdsourcing behavioral research. Journal of Experimental Social Psychology, 70, 153–163. https://doi.org/10.1016/j.jesp.2017.01.006 Peng, R. D. (2011). Reproducible Research in Computational Science. Science, 334(6060), 1226–1227. https://doi.org/10.1126/science.1213847 Percie du Sert, N., Hurst, V., Ahluwalia, A., Alam, S., Avey, M. T., Baker, M., Browne, W. J., Clark, A., Cuthill, I. C., Dirnagl, U., Emerson, M., Garner, P., Holgate, S. T., Howells, D. W., Karp, N. A., Lazic, S. E., Lidster, K., MacCallum, C. J., Macleod, M., … Würbel, H. (2020). The ARRIVE guidelines 2.0: Updated guidelines for reporting animal research. PLOS Biology, 18(7), e3000410. https://doi.org/10.1371/journal.pbio.3000410 Pernet, C. (2016). Null hypothesis significance testing: A short tutorial. F1000Research, 4, 621. https://doi.org/10.12688/f1000research.6963.3 Pernet, C., Garrido, M. I., Gramfort, A., Maurits, N., Michel, C. M., Pang, E., Salmelin, R., Schoffelen, J. M., Valdes-Sosa, P. A., \u0026amp; Puce, A. (2020). Issues and recommendations from the OHBM COBIDAS MEEG committee for reproducible EEG and MEG research. Nature Neuroscience, 23(12), 1473–1483. https://doi.org/10.1038/s41593-020-00709-0 Pernet, C. R., Appelhoff, S., Gorgolewski, K. J., Flandin, G., Phillips, C., Delorme, A., \u0026amp; Oostenveld, R. (2019). EEG-BIDS, an extension to the brain imaging data structure for electroencephalography. Scientific Data, 6(1), 103. https://doi.org/10.1038/s41597-019-0104-8 Peterson, D., \u0026amp; Panofsky, A. (2020). Metascience as a scientific social movement [Preprint]. SocArXiv. https://doi.org/10.31235/osf.io/4dsqa Petre, M., \u0026amp; Wilson, G. (2014). Code Review For and By Scientists. ArXiv:1407.5648 [Cs]. http://arxiv.org/abs/1407.5648 ‘Plan S’ and ‘cOAlition S’ – Accelerating the transition to full and immediate Open Access to scientific publications. (n.d.). Retrieved 9 July 2021, from https://www.coalition-s.org/ Poldrack, R. A., Barch, D. M., Mitchell, J. P., Wager, T. D., Wagner, A. D., Devlin, J. T., Cumba, C., Koyejo, O., \u0026amp; Milham, M. P. (2013). Toward open sharing of task-based fMRI data: The OpenfMRI project. Frontiers in Neuroinformatics, 7. https://doi.org/10.3389/fninf.2013.00012 Poldrack, R. A., \u0026amp; Gorgolewski, K. J. (2014). Making big data open: Data sharing in neuroimaging. Nature Neuroscience, 17(11), 1510–1517. https://doi.org/10.1038/nn.3818 Pollet, I. L., \u0026amp; Bond, A. L. (2021). Evaluation and recommendations for greater accessibility of colour figures in ornithology. Ibis, 163(1), 292–295. https://doi.org/10.1111/ibi.12887 Popper, K. (2010). The logic of scientific discovery (Special Indian Edition). Routledge. Posselt, J. R. (2020). Equity in science: Representation, culture, and the dynamics of change in graduate education. Stanford University Press. Pownall, M., Talbot, C. V., Henschel, A., Lautarescu, A., Lloyd, K., Hartmann, H., Darda, K. M., Tang, K. T. Y., Carmichael-Murphy, P., \u0026amp; Siegel, J. A. (2020). Navigating Open Science as Early Career Feminist Researchers [Preprint]. PsyArXiv. https://doi.org/10.31234/osf.io/f9m47 Preregistration pledge. (n.d.). Google Docs. Retrieved 9 July 2021, from https://docs.google.com/forms/d/e/1FAIpQLSf8RflGizFJZamE874o8aDOhyU7UsNByR4dLmzhOtEOiu8KRQ/viewform?embedded=true\u0026amp;usp=embed_facebook Press, W. (2007). Numerical recipes: The art of scientific computing, (3rd ed.). Cambridge University Press. Psychological Science Accelerator. (n.d.). Psychological Science Accelerator. Retrieved 9 July 2021, from https://psysciacc.org/ Publication bias. (2019, May 2). Catalog of Bias. https://catalogofbias.org/biases/publication-bias/ PubPeer—Search publications and join the conversation. (n.d.). Pubpeer. Retrieved 9 July 2021, from https://www.pubpeer.com/ R: The R Project for Statistical Computing. (n.d.). R Project. Retrieved 10 July 2021, from https://www.r-project.org/ Rabagliati, H., Moors, P., \u0026amp; Heyman, T. (2020). Can Item Effects Explain Away the Evidence for Unconscious Sound Symbolism? An Adversarial Commentary on Heyman, Maerten, Vankrunkelsven, Voorspoels, and Moors (2019). Psychological Science, 31(9), 1200–1204. https://doi.org/10.1177/0956797620949461 Rakow, T., Thompson, V., Ball, L., \u0026amp; Markovits, H. (2015). Rationale and guidelines for empirical adversarial collaboration: A Thinking \u0026amp; Reasoning initiative. Thinking \u0026amp; Reasoning, 21(2), 167–175. https://doi.org/10.1080/13546783.2015.975405 Recommended Data Repositories | Scientific Data. (n.d.). Retrieved 10 July 2021, from https://www.nature.com/sdata/policies/repositories Replication Markets – Reliable research replicates…you can bet on it. (n.d.). Retrieved 10 July 2021, from https://www.replicationmarkets.com/ ReproducibiliTea. (n.d.). ReproducibiliTea. Retrieved 10 July 2021, from https://reproducibilitea.org/ Retraction Watch. (n.d.). Retraction Watch. Retrieved 9 July 2021, from https://retractionwatch.com/ RIOT Science Club—Riot Science Club. (n.d.). Reproducible, Interpretable, Open, \u0026amp; Transparent Science. Retrieved 10 July 2021, from http://riotscience.co.uk/ Rogers, A., Castree, N., \u0026amp; Kitchin, R. (2013). Reflexivity. In A Dictionary of Human Geography. Oxford University Press. https://www.oxfordreference.com/view/10.1093/acref/9780199599868.001.0001/acref-9780199599868-e-1530 Rolls, L., \u0026amp; Relf, M. (2006). Bracketing interviews: Addressing methodological challenges in qualitative interviewing in bereavement and palliative care. Mortality, 11(3), 286–305. https://doi.org/10.1080/13576270600774893 Rose, D. (2000). Universal Design for Learning. Journal of Special Education Technology, 15(3), 45–49. https://doi.org/10.1177/016264340001500307 Rose, D. (2018). Participatory research: Real or imagined. Social Psychiatry and Psychiatric Epidemiology, 53(8), 765–771. https://doi.org/10.1007/s00127-018-1549-3 Rose, D. H., \u0026amp; Meyer, A. (2002). Teaching every student in the Digital Age: Universal design for learning. Association for Supervision and Curriculum Development. Ross-Hellauer, T. (2017). What is open peer review? A systematic review. F1000Research, 6, 588. https://doi.org/10.12688/f1000research.11369.2 Rossner, M., Van Epps, H., \u0026amp; Hill, E. (2007). Show me the data. Journal of Cell Biology, 179(6), 1091–1092. https://doi.org/10.1083/jcb.200711140 Rothstein, H. R., Sutton, A. J., \u0026amp; Borenstein, M. (2006). Publication Bias in Meta-Analysis. In H. R. Rothstein, A. J. Sutton, \u0026amp; M. Borenstein (Eds.), Publication Bias in Meta-Analysis (pp. 1–7). John Wiley \u0026amp; Sons, Ltd. https://doi.org/10.1002/0470870168.ch1 Rowhani-Farid, A., Aldcroft, A., \u0026amp; Barnett, A. G. (2020). Did awarding badges increase data sharing in BMJ Open ? A randomized controlled trial. Royal Society Open Science, 7(3), 191818. https://doi.org/10.1098/rsos.191818 Rubin, M. (2021). Explaining the association between subjective social status and mental health among university students using an impact ratings approach. SN Social Sciences, 1(1), 20. https://doi.org/10.1007/s43545-020-00031-3 Rubin, M., Evans, O., \u0026amp; McGuffog, R. (2019). Social Class Differences in Social Integration at University: Implications for Academic Outcomes and Mental Health. In J. Jetten \u0026amp; K. Peters (Eds.), The Social Psychology of Inequality (pp. 87–102). Springer International Publishing. https://doi.org/10.1007/978-3-030-28856-3_6 Sagarin, B. J., Ambler, J. K., \u0026amp; Lee, E. M. (2014). An Ethical Approach to Peeking at Data. Perspectives on Psychological Science, 9(3), 293–304. https://doi.org/10.1177/1745691614528214 Salem, D. N., \u0026amp; Boumil, M. M. (2013). Conflict of Interest in Open-Access Publishing. New England Journal of Medicine, 369(5), 491–491. https://doi.org/10.1056/NEJMc1307577 Sato, T. (1996). Type I and Type II Error in Multiple Comparisons. The Journal of Psychology, 130(3), 293–302. https://doi.org/10.1080/00223980.1996.9915010 Schafersman, S. (1997, January). An Introduction to Science: Scientific Thinking and Scientific Method. An Introduction to Science. https://www.geo.sunysb.edu/esp/files/scientific-method.html Schmidt, Robert. H. (1987). A Worksheet for Authorship of Scientific Articles on JSTOR. Bulletin of the Ecological Society of America, 68(1), 8–10. https://www.jstor.org/stable/20166549 Schneider, J., Merk, S., \u0026amp; Rosman, T. (2020). (Re)Building Trust? Investigating the effects of open science badges on perceived trustworthiness in journal articles. https://doi.org/10.17605/OSF.IO/VGBRS Schönbrodt, F. (2019). Training students for the Open Science future. Nature Human Behaviour, 3(10), 1031–1031. https://doi.org/10.1038/s41562-019-0726-z Schönbrodt, F. D., Wagenmakers, E.-J., Zehetleitner, M., \u0026amp; Perugini, M. (2017). Sequential hypothesis testing with Bayes factors: Efficiently testing mean differences. Psychological Methods, 22(2), 322–339. https://doi.org/10.1037/met0000061 Schulz, K. F., \u0026amp; Grimes, D. A. (2005). Multiplicity in randomised trials I: Endpoints and treatments. The Lancet, 365(9470), 1591–1595. https://doi.org/10.1016/S0140-6736(05)66461-6 Schwarz, N., \u0026amp; Strack, F. (n.d.). Does merely going through the same moves make for a “direct” replication? Concepts, contexts, and operationalizations. Social Psychology, 45(4), 305–306. Science. (n.d.). Open Science Badges. Centre for Open Science. https://www.cos.io/initiatives/badges Scopatz, A., \u0026amp; Huff, K. D. (2015). Effective computation in physics (First Edition). O’Reilly Media. Shadish, W. R., Cook, T. D., \u0026amp; Campbell, D. T. (2001). Experimental and quasi-experimental designs for generalized causal inference. Houghton Mifflin. Sharma, M., Sarin, A., Gupta, P., Sachdeva, S., \u0026amp; Desai, A. (2014). Journal Impact Factor: Its Use, Significance and Limitations. World Journal of Nuclear Medicine, 13(2), 146. https://doi.org/10.4103/1450-1147.139151 Shepard, B. (2015). Community practice as social activism: From direct action to direct services. SAGE Publications, Inc. Siddaway, A. P., Wood, A. M., \u0026amp; Hedges, L. V. (2019). How to Do a Systematic Review: A Best Practice Guide for Conducting and Reporting Narrative Reviews, Meta-Analyses, and Meta-Syntheses. Annual Review of Psychology, 70(1), 747–770. https://doi.org/10.1146/annurev-psych-010418-102803 Sijtsma, K. (2016). Playing with Data—Or How to Discourage Questionable Research Practices and Stimulate Researchers to Do Things Right. Psychometrika, 81(1), 1–15. https://doi.org/10.1007/s11336-015-9446-0 Silberzahn, R., Simonsohn, U., \u0026amp; Uhlmann, E. L. (2014). Matched-Names Analysis Reveals No Evidence of Name-Meaning Effects: A Collaborative Commentary on Silberzahn and Uhlmann (2013). Psychological Science, 25(7), 1504–1505. https://doi.org/10.1177/0956797614533802 Silberzahn, R., Uhlmann, E. L., Martin, D. P., Anselmi, P., Aust, F., Awtrey, E., Bahník, Š., Bai, F., Bannard, C., Bonnier, E., Carlsson, R., Cheung, F., Christensen, G., Clay, R., Craig, M. A., Dalla Rosa, A., Dam, L., Evans, M. H., Flores Cervantes, I., … Nosek, B. A. (2018). Many Analysts, One Data Set: Making Transparent How Variations in Analytic Choices Affect Results. Advances in Methods and Practices in Psychological Science, 1(3), 337–356. https://doi.org/10.1177/2515245917747646 Simmons, J., Nelson, L., \u0026amp; Simonsohn, U. (2021). Pre‐registration: Why and How. Journal of Consumer Psychology, 31(1), 151–162. https://doi.org/10.1002/jcpy.1208 Simmons, J. P., Nelson, L. D., \u0026amp; Simonsohn, U. (2011). False-Positive Psychology: Undisclosed Flexibility in Data Collection and Analysis Allows Presenting Anything as Significant. Psychological Science, 22(11), 1359–1366. https://doi.org/10.1177/0956797611417632 Simons, D. J., Shoda, Y., \u0026amp; Lindsay, D. S. (2017). Constraints on Generality (COG): A Proposed Addition to All Empirical Papers. Perspectives on Psychological Science, 12(6), 1123–1128. https://doi.org/10.1177/1745691617708630 Simonsohn, U., Nelson, L. D., \u0026amp; Simmons, J. P. (2014a). P-curve: A key to the file-drawer. Journal of Experimental Psychology: General, 143(2), 534–547. https://doi.org/10.1037/a0033242 Simonsohn, U., Nelson, L. D., \u0026amp; Simmons, J. P. (2014b). p -Curve and Effect Size: Correcting for Publication Bias Using Only Significant Results. Perspectives on Psychological Science, 9(6), 666–681. https://doi.org/10.1177/1745691614553988 Simonsohn, U., Nelson, L. D., \u0026amp; Simmons, J. P. (2019). P-curve won’t do your laundry, but it will distinguish replicable from non-replicable findings in observational research: Comment on Bruns \u0026amp; Ioannidis (2016). PLOS ONE, 14(3), e0213454. https://doi.org/10.1371/journal.pone.0213454 Simonsohn, U., Simmons, J. P., \u0026amp; Nelson, L. D. (2015). Specification Curve: Descriptive and Inferential Statistics on All Reasonable Specifications. SSRN Electronic Journal. https://doi.org/10.2139/ssrn.2694998 Simonsohn, U., Simmons, J. P., \u0026amp; Nelson, L. D. (2020). Specification curve analysis. Nature Human Behaviour, 4(11), 1208–1214. https://doi.org/10.1038/s41562-020-0912-z Smaldino, P. E., \u0026amp; McElreath, R. (2016). The natural selection of bad science. Royal Society Open Science, 3(9), 160384. https://doi.org/10.1098/rsos.160384 Smith, A. C., Merz, L., Borden, J. B., Gulick, C., Kshirsagar, A. R., \u0026amp; Bruna, E. M. (2020). Assessing the effect of article processing charges on the geographic diversity of authors using Elsevier’s ‘Mirror Journal’ system [Preprint]. MetaArXiv. https://doi.org/10.31222/osf.io/s7cx4 Smith, A. J., Clutton, R. E., Lilley, E., Hansen, K. E. A., \u0026amp; Brattelid, T. (2018). PREPARE: Guidelines for planning animal research and testing. Laboratory Animals, 52(2), 135–141. https://doi.org/10.1177/0023677217724823 Smith, G. T. (2005). On Construct Validity: Issues of Method and Measurement. Psychological Assessment, 17(4), 396–408. https://doi.org/10.1037/1040-3590.17.4.396 Sorsa, M. A., Kiikkala, I., \u0026amp; Åstedt-Kurki, P. (2015). Bracketing as a skill in conducting unstructured qualitative interviews. Nurse Researcher, 22(4), 8–12. https://doi.org/10.7748/nr.22.4.8.e1317 SORTEE. (n.d.). SORTEE. SORTEE. Retrieved 10 July 2021, from https://www.sortee.org/ Spence, J. R., \u0026amp; Stanley, D. J. (2018). Concise, Simple, and Not Wrong: In Search of a Short-Hand Interpretation of Statistical Significance. Frontiers in Psychology, 9, 2185. https://doi.org/10.3389/fpsyg.2018.02185 Spencer, E. A., \u0026amp; Heneghan, C. (2018, April 2). Confirmation bias. Catalog of Bias. https://catalogofbias.org/biases/confirmation-bias/ Steckler, A., \u0026amp; McLeroy, K. R. (2008). The Importance of External Validity. American Journal of Public Health, 98(1), 9–10. https://doi.org/10.2105/AJPH.2007.126847 Steegen, S., Tuerlinckx, F., Gelman, A., \u0026amp; Vanpaemel, W. (2016). Increasing Transparency Through a Multiverse Analysis. Perspectives on Psychological Science, 11(5), 702–712. https://doi.org/10.1177/1745691616658637 Steup, M., \u0026amp; Neta, R. (2020). Epistemology. In E. N. Zalta (Ed.), The Stanford Encyclopedia of Philosophy (Fall 2020). Metaphysics Research Lab, Stanford University. https://plato.stanford.edu/archives/fall2020/entries/epistemology/ Stewart, N., Chandler, J., \u0026amp; Paolacci, G. (2017). Crowdsourcing Samples in Cognitive Science. Trends in Cognitive Sciences, 21(10), 736–748. https://doi.org/10.1016/j.tics.2017.06.007 Stodden, V. C. (2011). Trust Your Science? Open Your Data and Code. https://doi.org/10.7916/D8CJ8Q0P Strathern, M. (1997). ‘Improving ratings’: Audit in the British University system. European Review, 5(3), 305–321. https://doi.org/10.1002/(SICI)1234-981X(199707)5:3\u0026lt;305::AID-EURO184\u0026gt;3.0.CO;2-4 Suber, P. (2004, February 4). It’s the authors, stupid! SPARC Open Access Newsletter. https://dash.harvard.edu/bitstream/handle/1/4391161/suber_authors.htm?sequence=1\u0026amp;isAllowed=y SwissRN. (n.d.). Retrieved 10 July 2021, from http://www.swissrn.org/ Syed, M. (2019). The Open Science Movement is For All of Us [Preprint]. PsyArXiv. https://doi.org/10.31234/osf.io/cteyb Syed, M., \u0026amp; Kathawalla, U.-K. (2020). Cultural Psychology, Diversity, and Representation in Open Science [Preprint]. PsyArXiv. https://doi.org/10.31234/osf.io/t7hp2 Szollosi, A., \u0026amp; Donkin, C. (2021). Arrested Theory Development: The Misguided Distinction Between Exploratory and Confirmatory Research. Perspectives on Psychological Science, 174569162096679. https://doi.org/10.1177/1745691620966796 Team, psyTeachR. (n.d.). P | Glossary. Retrieved 9 July 2021, from https://psyteachr.github.io/glossary Tennant, J., Beamer, J. E., Bosman, J., Brembs, B., Chung, N. C., Clement, G., Crick, T., Dugan, J., Dunning, A., Eccles, D., Enkhbayar, A., Graziotin, D., Harding, R., Havemann, J., Katz, D. S., Khanal, K., Kjaer, J. N., Koder, T., Macklin, P., … Turner, A. (2019). Foundations for Open Scholarship Strategy Development [Preprint]. MetaArXiv. https://doi.org/10.31222/osf.io/b4v8p Tennant, J., Bielczyk, N. Z., Greshake Tzovaras, B., Masuzzo, P., \u0026amp; Steiner, T. (2019). Introducing Massively Open Online Papers (MOOPs) [Preprint]. MetaArXiv. https://doi.org/10.31222/osf.io/et8ak Tenny, S., \u0026amp; Abdelgawad, I. (2021). Statistical Significance. In StatPearls [Internet]. StatPearls Publishing. https://www.ncbi.nlm.nih.gov/books/NBK459346/ The Committee on Publication Ethics. (n.d.). Transparency \u0026amp; best practice – DOAJ. DOAJ. https://doaj.org/apply/transparency/ the CONSORT Group, Schulz, K. F., Altman, D. G., \u0026amp; Moher, D. (2010). CONSORT 2010 Statement: Updated guidelines for reporting parallel group randomised trials. Trials, 11(1), 32. https://doi.org/10.1186/1745-6215-11-32 The European Code of Conduct for Research Integrity | ALLEA. (n.d.). Retrieved 10 July 2021, from https://allea.org/code-of-conduct/ The Open Definition—Open Definition—Defining Open in Open Data, Open Content and Open Knowledge. (n.d.). Open Knowledge Foundation. Retrieved 9 July 2021, from https://opendefinition.org/ The Open Source Definition | Open Source Initiative. (n.d.). Open Source Initative. Retrieved 9 July 2021, from https://opensource.org/osd The Slow Science Academy. (2010). The Slow Science Manifesto. SLOW-SCIENCE.Org — Bear with Us, While We Think. http://slow-science.org/ Thombs, B. D., Levis, A. W., Razykov, I., Syamchandra, A., Leentjens, A. F. G., Levenson, J. L., \u0026amp; Lumley, M. A. (2015). Potentially coercive self-citation by peer reviewers: A cross-sectional study. Journal of Psychosomatic Research, 78(1), 1–6. https://doi.org/10.1016/j.jpsychores.2014.09.015 Tierney, W., Hardy, J., Ebersole, C. R., Viganola, D., Clemente, E. G., Gordon, M., Hoogeveen, S., Haaf, J., Dreber, A., Johannesson, M., Pfeiffer, T., Huang, J. L., Vaughn, L. A., DeMarree, K., Igou, E. R., Chapman, H., Gantman, A., Vanaman, M., Wylie, J., … Uhlmann, E. L. (2021). A creative destruction approach to replication: Implicit work and sex morality across cultures. Journal of Experimental Social Psychology, 93, 104060. https://doi.org/10.1016/j.jesp.2020.104060 Tierney, W., Hardy, J. H., Ebersole, C. R., Leavitt, K., Viganola, D., Clemente, E. G., Gordon, M., Dreber, A., Johannesson, M., Pfeiffer, T., \u0026amp; Uhlmann, E. L. (2020). Creative destruction in science. Organizational Behavior and Human Decision Processes, 161, 291–309. https://doi.org/10.1016/j.obhdp.2020.07.002 Tiokhin, L., Yan, M., \u0026amp; Morgan, T. J. H. (2021). Competition for priority harms the reliability of science, but reforms can help. Nature Human Behaviour. https://doi.org/10.1038/s41562-020-01040-1 Topor, M., Pickering, J. S., Barbosa Mendes, A., Bishop, D. V. M., Büttner, F. C., Elsherif, M. M., Evans, T. R., Henderson, E. L., Kalandadze, T., Nitschke, F. T., Staaks, J., Van den Akker, O., Yeung, S. K., Zaneva, M., Lam, A., Madan, C. R., Moreau, D., O’Mahony, A., Parker, A. J., … Westwood, S. J. (2020). An integrative framework for planning and conducting Non-Intervention, Reproducible, and Open Systematic Reviews (NIRO-SR) [Preprint]. MetaArXiv. https://doi.org/10.31222/osf.io/8gu5z Transparency: The Emerging Third Dimension of Open Science and Open Data. (2016). LIBER QUARTERLY, 25(4), 153–171. https://doi.org/10.18352/lq.10113 Tscharntke, T., Hochberg, M. E., Rand, T. A., Resh, V. H., \u0026amp; Krauss, J. (2007). Author Sequence and Credit for Contributions in Multiauthored Publications. PLoS Biology, 5(1), e18. https://doi.org/10.1371/journal.pbio.0050018 Tufte, E. R. (2001). The visual display of quantitative information (2nd ed). Graphics Press. Tukey, J. W. (1977). Exploratory data analysis. Addison-Wesley Pub. Co. Tvina, A., Spellecy, R., \u0026amp; Palatnik, A. (2019). Bias in the Peer Review Process: Can We Do Better? Obstetrics \u0026amp; Gynecology, 133(6), 1081–1083. https://doi.org/10.1097/AOG.0000000000003260 Uhlmann, E. L., Ebersole, C. R., Chartier, C. R., Errington, T. M., Kidwell, M. C., Lai, C. K., McCarthy, R. J., Riegelman, A., Silberzahn, R., \u0026amp; Nosek, B. A. (2019). Scientific Utopia III: Crowdsourcing Science. Perspectives on Psychological Science, 14(5), 711–733. https://doi.org/10.1177/1745691619850561 UK Reproducibility Network. (n.d.). UK Reproducibility Network. Retrieved 10 July 2021, from https://www.ukrn.org/ University of Illinois at Urbana-Champaign, Burnette, M., Williams, S., University of Illinois at Urbana-Champaign, Imker, H., \u0026amp; University of Illinois at Urbana-Champaign. (2016). From Plan to Action: Successful Data Management Plan Implementation in a Multidisciplinary Project. Journal of EScience Librarianship, 5(1), e1101. https://doi.org/10.7191/jeslib.2016.1101 van de Schoot, R., Depaoli, S., King, R., Kramer, B., Märtens, K., Tadesse, M. G., Vannucci, M., Gelman, A., Veen, D., Willemsen, J., \u0026amp; Yau, C. (2021). Bayesian statistics and modelling. Nature Reviews Methods Primers, 1(1), 1. https://doi.org/10.1038/s43586-020-00001-2 Vazire, S. (2018). Implications of the Credibility Revolution for Productivity, Creativity, and Progress. Perspectives on Psychological Science, 13(4), 411–417. https://doi.org/10.1177/1745691617751884 Vazire, S., Schiavone, S. R., \u0026amp; Bottesini, J. G. (2020). Credibility Beyond Replicability: Improving the Four Validities in Psychological Science [Preprint]. PsyArXiv. https://doi.org/10.31234/osf.io/bu4d3 Villum, C. (2014, March 10). “Open-washing” – The difference between opening your data and simply making them available – Open Knowledge Foundation blog. Open Knowledge Foundation. https://blog.okfn.org/2014/03/10/open-washing-the-difference-between-opening-your-data-and-simply-making-them-available/ Vlaeminck, S., \u0026amp; Podkrajac, F. (2017). Journals in Economic Sciences: Paying Lip Service to Reproducible Research? IASSIST Quarterly, 41(1–4), 16. https://doi.org/10.29173/iq6 Voracek, M., Kossmeier, M., \u0026amp; Tran, U. S. (2019). Which Data to Meta-Analyze, and How?: A Specification-Curve and Multiverse-Analysis Approach to Meta-Analysis. Zeitschrift Für Psychologie, 227(1), 64–82. https://doi.org/10.1027/2151-2604/a000357 Vuorre, M., \u0026amp; Curley, J. P. (2018). Curating Research Assets: A Tutorial on the Git Version Control System. Advances in Methods and Practices in Psychological Science, 1(2), 219–236. https://doi.org/10.1177/2515245918754826 Wacker, J. G. (1998). A definition of theory: Research guidelines for different theory-building research methods in operations management. Journal of Operations Management, 16(4), 361–385. https://doi.org/10.1016/S0272-6963(98)00019-9 Wagenmakers, E.-J., Marsman, M., Jamil, T., Ly, A., Verhagen, J., Love, J., Selker, R., Gronau, Q. F., Šmíra, M., Epskamp, S., Matzke, D., Rouder, J. N., \u0026amp; Morey, R. D. (2018). Bayesian inference for psychology. Part I: Theoretical advantages and practical ramifications. Psychonomic Bulletin \u0026amp; Review, 25(1), 35–57. https://doi.org/10.3758/s13423-017-1343-3 Wagenmakers, E.-J., Wetzels, R., Borsboom, D., van der Maas, H. L. J., \u0026amp; Kievit, R. A. (2012). An Agenda for Purely Confirmatory Research. Perspectives on Psychological Science, 7(6), 632–638. https://doi.org/10.1177/1745691612463078 Wagge, J. R., Baciu, C., Banas, K., Nadler, J. T., Schwarz, S., Weisberg, Y., IJzerman, H., Legate, N., \u0026amp; Grahe, J. (2019). A Demonstration of the Collaborative Replication and Education Project: Replication Attempts of the Red-Romance Effect. Collabra: Psychology, 5(1), 5. https://doi.org/10.1525/collabra.177 Walker, P., \u0026amp; Miksa, T. (2019, November 26). RDA-DMP-Common/RDA-DMP-Common-Standard. GitHub. https://github.com/RDA-DMP-Common/RDA-DMP-Common-Standard Wason, P. C. (1960). On the Failure to Eliminate Hypotheses in a Conceptual Task. Quarterly Journal of Experimental Psychology, 12(3), 129–140. https://doi.org/10.1080/17470216008416717 Wasserstein, R. L., \u0026amp; Lazar, N. A. (2016). The ASA Statement on p -Values: Context, Process, and Purpose. The American Statistician, 70(2), 129–133. https://doi.org/10.1080/00031305.2016.1154108 Webster, M. M., \u0026amp; Rutz, C. (2020). How STRANGE are your study animals? Nature, 582(7812), 337–340. https://doi.org/10.1038/d41586-020-01751-5 Welcome to Sherpa Romeo—V2.sherpa. (n.d.). Sherpa Romeo. Retrieved 10 July 2021, from https://v2.sherpa.ac.uk/romeo/ Wendl, M. C. (2007). H-index: However ranked, citations need context. Nature, 449(7161), 403–403. https://doi.org/10.1038/449403b What is a Codebook? (n.d.). ICPSR. Retrieved 9 July 2021, from https://www.icpsr.umich.edu/icpsrweb/content/shared/ICPSR/faqs/what-is-a-codebook.html What is a reporting guideline? | The EQUATOR Network. (n.d.). Retrieved 10 July 2021, from https://www.equator-network.org/about-us/what-is-a-reporting-guideline/ What is Crowdsourcing? (2021, April 29). Crowdsourcing Week. https://crowdsourcingweek.com/what-is-crowdsourcing/ What is data sharing? | Support Centre for Data Sharing. (n.d.). Support Centre for Data Sharing. Retrieved 11 July 2021, from https://eudatasharing.eu/what-data-sharing What is impact? - Economic and Social Research Council. (n.d.). Economic and Social Research Council. Retrieved 8 July 2021, from https://esrc.ukri.org/research/impact-toolkit/what-is-impact/ What is Open Data? (n.d.). Open Data Handbook. Retrieved 9 July 2021, from https://opendatahandbook.org/guide/en/what-is-open-data/ What is open education? (n.d.). Opensource.Com. Retrieved 9 July 2021, from https://opensource.com/resources/what-open-education Whitaker, K., \u0026amp; Guest, O. (2020). #bropenscience is broken science. The Psychologist, 33, 34–37. Wicherts, J. M., Veldkamp, C. L. S., Augusteijn, H. E. M., Bakker, M., van Aert, R. C. M., \u0026amp; van Assen, M. A. L. M. (2016). Degrees of Freedom in Planning, Running, Analyzing, and Reporting Psychological Studies: A Checklist to Avoid p-Hacking. Frontiers in Psychology, 7. https://doi.org/10.3389/fpsyg.2016.01832 Wilkinson, M. D., Dumontier, M., Aalbersberg, Ij. J., Appleton, G., Axton, M., Baak, A., Blomberg, N., Boiten, J.-W., da Silva Santos, L. B., Bourne, P. E., Bouwman, J., Brookes, A. J., Clark, T., Crosas, M., Dillo, I., Dumon, O., Edmunds, S., Evelo, C. T., Finkers, R., … Mons, B. (2016). The FAIR Guiding Principles for scientific data management and stewardship. Scientific Data, 3(1), 160018. https://doi.org/10.1038/sdata.2016.18 Wilson, B., \u0026amp; Fenner, M. (2012, May 9). Open Researcher \u0026amp;amp; Contributor ID (ORCID): Solving the Name Ambiguity Problem. https://er.educause.edu/articles/2012/5/open-researcher--contributor-id-orcid-solving-the-name-ambiguity-problem Wilson, R. C., \u0026amp; Collins, A. G. (2019). Ten simple rules for the computational modeling of behavioral data. ELife, 8, e49547. https://doi.org/10.7554/eLife.49547 Wingen, T., Berkessel, J. B., \u0026amp; Englich, B. (2020). No Replication, No Trust? How Low Replicability Influences Trust in Psychology. Social Psychological and Personality Science, 11(4), 454–463. https://doi.org/10.1177/1948550619877412 Woelfle, M., Olliaro, P., \u0026amp; Todd, M. H. (2011). Open science is a research accelerator. Nature Chemistry, 3(10), 745–748. https://doi.org/10.1038/nchem.1149 Working Group 1 of the Joint Committee for Guides in Metrology JCGM. (2008). Evaluation of measurement data—Guide to the expression of uncertainty in measurement (pp. 1–120). JCGM. https://www.bipm.org/documents/20126/2071204/JCGM_100_2008_E.pdf/cb0ef43f-baa5-11cf-3f85-4dcd86f77bd6 World Wide Web Consortium. (n.d.). Home | Web Accessibility Initiative (WAI) | W3C. Web Accessibility Initiative. Retrieved 9 July 2021, from https://www.w3.org/WAI/ Wuchty, S., Jones, B. F., \u0026amp; Uzzi, B. (2007). The Increasing Dominance of Teams in Production of Knowledge. Science, 316(5827), 1036–1039. https://doi.org/10.1126/science.1136099 Xia, J., Harmon, J. L., Connolly, K. G., Donnelly, R. M., Anderson, M. R., \u0026amp; Howard, H. A. (2015). Who publishes in “predatory” journals? Journal of the Association for Information Science and Technology, 66(7), 1406–1417. https://doi.org/10.1002/asi.23265 Yamada, Y. (2018). How to Crack Pre-registration: Toward Transparent and Open Science. Frontiers in Psychology, 9, 1831. https://doi.org/10.3389/fpsyg.2018.01831 Yarkoni, T. (2020). The generalizability crisis. Behavioral and Brain Sciences, 1–37. https://doi.org/10.1017/S0140525X20001685 Yeung, S. K., Feldman, G., Fillon, A., Protzko, J., Elsherif, M. M., Xiao, Q., \u0026amp; Pickering, J. (n.d.). Experimental Studies Meta-Analysis\u0026nbsp; Registered Report template: Main manuscript [Preprint]. Hong Kong University. https://docs.google.com/document/d/1z3QBDYr86S9FxGjptZP94jJnZeeN4aQaBQP3VVT89Ec/edit# Zenodo—Research. Shared. (n.d.). Zenodo. Retrieved 9 July 2021, from https://www.zenodo.org/ Zurn, P., Bassett, D. S., \u0026amp; Rust, N. C. (2020). The Citation Diversity Statement: A Practice of Transparency, A Way of Life. Trends in Cognitive Sciences, 24(9), 669–672. https://doi.org/10.1016/j.tics.2020.06.009 Zwaan, R. A., Etz, A., Lucas, R. E., \u0026amp; Donnellan, M. B. (2018). Making replication mainstream. Behavioral and Brain Sciences, 41, e120. https://doi.org/10.1017/S0140525X17001972 ","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1626197826,"objectID":"2ac98b5f20df3a8af2b46c9b9ff0cd26","permalink":"https://forrt.org/glossary/references/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/references/","section":"glossary","summary":"You can find the list of all references that were used to create the Glossary.\nWe are currently working on a better way to display and cross-link the references with the terms they are used for. A free and open platform for sharing MRI, MEG, EEG, iEEG, ECoG, ASL, and PET data—OpenNeuro.","tags":null,"title":"List of References","type":"glossary"},{"authors":null,"categories":null,"content":"You can find the list of all references that were used to create the Glossary.\nWe are currently working on a better way to display and cross-link the references with the terms they are used for. A free and open platform for sharing MRI, MEG, EEG, iEEG, ECoG, ASL, and PET data—OpenNeuro. (n.d.). OpenNeuro. Retrieved 9 July 2021, from https://openneuro.org/ Abele-Brehm, A. E., Gollwitzer, M., Steinberg, U., \u0026amp; Schönbrodt, F. D. (2019). Attitudes Toward Open Science and Public Data Sharing: A Survey Among Members of the German Psychological Society. Social Psychology, 50(4), 252–260. https://doi.org/10.1027/1864-9335/a000384 Aczel, B., Szaszi, B., Nilsonne, G., Van den Akker, O., Albers, C. J., van Assen, M. A. L. M., Bastiaansen, J. A., Benjamin, D. J., Boehm, U., Botvinik-Nezer, R., Bringmann, L. F., Busch, N., Caruyer, E., Cataldo, A. M., Cowan, N., Delios, A., van Dongen, N. N. N., Donkin, C., van Doorn, J., … Wagenmakers, E.-J. (2021). Guidance for conducting and reporting multi-analyst studies [Preprint]. MetaArXiv. https://doi.org/10.31222/osf.io/5ecnh Aczel, B., Szaszi, B., Sarafoglou, A., Kekecs, Z., Kucharský, Š., Benjamin, D., Chambers, C. D., Fisher, A., Gelman, A., Gernsbacher, M. A., Ioannidis, J. P., Johnson, E., Jonas, K., Kousta, S., Lilienfeld, S. O., Lindsay, D. S., Morey, C. C., Munafò, M., Newell, B. R., … Wagenmakers, E.-J. (2020). A consensus-based transparency checklist. Nature Human Behaviour, 4(1), 4–6. https://doi.org/10.1038/s41562-019-0772-6 Albayrak-Aydemir, N. (2018a, April 16). Diversity helps but decolonisation is the key to equality in higher education. Contemporary Issues in Teaching and Learning. https://lsepgcertcitl.wordpress.com/2018/04/16/diversity-helps-but-decolonisation-is-the-key-to-equality-in-higher-education/ Albayrak-Aydemir, N. (2018b, November 29). Academics’ role on the future of higher education: Important but unrecognised. Contemporary Issues in Teaching and Learning. https://lsepgcertcitl.wordpress.com/2018/11/29/academics-role-on-the-future-of-higher-education-important-but-unrecognised/ Albayrak-Aydemir, N. (2020, February 20). ‘The hidden costs of being a scholar from the Global South’ is locked The hidden costs of being a scholar from the Global South. LSE Higher Education. https://blogs.lse.ac.uk/highereducation/2020/02/20/the-hidden-costs-of-being-a-scholar-from-the-global-south/ Albayrak-Aydemir, N., \u0026amp; Okoroji, C. (n.d.). Facing the challenges of postgraduate study as a minority student (A Guide for Psychology Postgraduates: Surviving Postgraduate Study, pp. 63–66). The British Psychological Society. Ali, M. J. (2021). Understanding the Altmetrics. Seminars in Ophthalmology, 1–3. https://doi.org/10.1080/08820538.2021.1930806 ALLEA - All European Academies. (2017). The European Code of Conduct for Research Integrity (Revised Edition). ALLEA. https://allea.org/code-of-conduct/ American Psychological Association,Task Force on Socioeconomic Status. (2007). Report of the APA task force on Socioeconomic status. American Psychological Association. Anderson, A. A., Scheufele, D. A., Brossard, D., \u0026amp; Corley, E. A. (2012). The Role of Media and Deference to Scientific Authority in Cultivating Trust in Sources of Information about Emerging Technologies. International Journal of Public Opinion Research, 24(2), 225–237. https://doi.org/10.1093/ijpor/edr032 Angrist, J. D., \u0026amp; Pischke, J.-S. (2010). The Credibility Revolution in Empirical Economics: How Better Research Design is Taking the Con out of Econometrics. Journal of Economic Perspectives, 24(2), 3–30. https://doi.org/10.1257/jep.24.2.3 Arslan, R. C. (2019). How to Automatically Document Data With the codebook Package to Facilitate Data Reuse. Advances in Methods and Practices in Psychological Science, 2(2), 169–187. https://doi.org/10.1177/2515245919838783 Australian Reproducibility Network. (n.d.). Australian Reproducibility Network. Retrieved 10 July 2021, from http://www.aus-rn.org/ Authorship \u0026amp; contributorship | The BMJ. (n.d.). The British Medical Journal. https://www.bmj.com/about-bmj/resources-authors/article-submission/authorship-contributorship Azevedo, F. (n.d.). Ideology May Help Explain Anti-Scientific Attitudes | Psychology Today. Retrieved 11 July 2021, from https://www.psychologytoday.com/intl/blog/social-justice-pacifists/202107/ideology-may-help-explain-anti-scientific-attitudes Azevedo, F., \u0026amp; Jost, J. T. (2021). The ideological basis of antiscientific attitudes: Effects of authoritarianism, conservatism, religiosity, social dominance, and system justification. Group Processes \u0026amp; Intergroup Relations, 24(4), 518–549. https://doi.org/10.1177/1368430221990104 Bak, H.-J. (2001). Education and Public Attitudes toward Science: Implications for the ‘Deficit Model’ of Education and Support for Science and Technology. Social Science Quarterly, 82(4), 779–795. https://www.jstor.org/stable/42955760 Banks, G. C., Rogelberg, S. G., Woznyj, H. M., Landis, R. S., \u0026amp; Rupp, D. E. (2016). Editorial: Evidence on Questionable Research Practices: The Good, the Bad, and the Ugly. Journal of Business and Psychology, 31(3), 323–338. https://doi.org/10.1007/s10869-016-9456-7 Barba, L. A. (2018). Terminologies for Reproducible Research. ArXiv:1802.03311 [Cs]. http://arxiv.org/abs/1802.03311 Bardsley, N. (2018). What lessons does the “replication crisis”\u0026nbsp; in psychology hold for experimental economics? In A. Lewis (Ed.), The Cambridge Handbook of Psychology and Economic Behavior (2nd ed.). CAMBRIDGE UNIVERSITY PRESS. Barnes, R. M., Johnston, H. M., MacKenzie, N., Tobin, S. J., \u0026amp; Taglang, C. M. (2018). The effect of ad hominem attacks on the evaluation of claims promoted by scientists. PLOS ONE, 13(1), e0192025. https://doi.org/10.1371/journal.pone.0192025 Bartoš, F., \u0026amp; Schimmack, U. (2020). Z-Curve.2.0: Estimating Replication Rates and Discovery Rates [Preprint]. PsyArXiv. https://doi.org/10.31234/osf.io/urgtn Bateman, I., Kahneman, D., Munro, A., Starmer, C., \u0026amp; Sugden, R. (2005). Testing competing models of loss aversion: An adversarial collaboration. Journal of Public Economics, 89(8), 1561–1580. https://doi.org/10.1016/j.jpubeco.2004.06.013 Baturay, M. H. (2015). An Overview of the World of MOOCs. Procedia - Social and Behavioral Sciences, 174, 427–433. https://doi.org/10.1016/j.sbspro.2015.01.685 Bazeley, P. (2003). Defining ‘Early Career’ in Research. Higher Education, 45(3), 257–279. https://doi.org/10.1023/A:1022698529612 Beffara Bret, B., Beffara Bret, A., \u0026amp; Nalborczyk, L. (2021). A fully automated, transparent, reproducible, and blind protocol for sequential analyses. Meta-Psychology, 5. https://doi.org/10.15626/MP.2018.869 Behrens, J. T. (1997). Principles and procedures of exploratory data analysis. Psychological Methods, 2(2), 131–160. https://doi.org/10.1037/1082-989X.2.2.131 Beller, S., \u0026amp; Bender, A. (2017). Theory, the Final Frontier? A Corpus-Based Analysis of the Role of Theory in Psychological Articles. Frontiers in Psychology, 8, 951. https://doi.org/10.3389/fpsyg.2017.00951 Benoit, K., Conway, D., Lauderdale, B. E., Laver, M., \u0026amp; Mikhaylov, S. (2016). Crowd-sourced Text Analysis: Reproducible and Agile Production of Political Data. American Political Science Review, 110(2), 278–295. https://doi.org/10.1017/S0003055416000058 Bhopal, R., Rankin, J., McColl, E., Thomas, L., Kaner, E., Stacy, R., Pearson, P., Vernon, B., \u0026amp; Rodgers, H. (1997). The vexed question of authorship: Views of researchers in a British medical faculty. BMJ, 314(7086), 1009–1009. https://doi.org/10.1136/bmj.314.7086.1009 BIAS | Definition of BIAS by Oxford Dictionary on Lexico.com also meaning of BIAS. (n.d.). Lexico Dictionaries | English. Retrieved 9 July 2021, from https://www.lexico.com/definition/bias BIDS. (2020a). About BIDS. Brain Imaging Data Structure. https://bids.neuroimaging.io/ BIDS. (2020b). Modality agnostic files—Brain Imaging Data Structure v1.6.0. Brain Imaging Data Structure. https://bids-specification.readthedocs.io/en/stable/03-modality-agnostic-files.html Bik, E. M., Casadevall, A., \u0026amp; Fang, F. C. (2016). The Prevalence of Inappropriate Image Duplication in Biomedical Research Publications. MBio, 7(3). https://doi.org/10.1128/mBio.00809-16 Bilder, G. (2013, September 20). DOIs unambiguously and persistently identify published, trustworthy, citable online scholarly literature. Right? [Website]. Crossref. https://www.crossref.org/blog/dois-unambiguously-and-persistently-identify-published-trustworthy-citable-online-scholarly-literature-right/ Bishop, D. V. (2020). The psychology of experimental psychologists: Overcoming cognitive constraints to improve research: The 47th Sir Frederic Bartlett Lecture. Quarterly Journal of Experimental Psychology, 73(1), 1–19. https://doi.org/10.1177/1747021819886519 Björneborn, L., \u0026amp; Ingwersen, P. (2004). Toward a basic framework for webometrics. Journal of the American Society for Information Science and Technology, 55(14), 1216–1227. https://doi.org/10.1002/asi.20077 Blohowiak, B. B., Cohoon, J., de-Wit, L., Eich, E., Farach, F. J., Hasselman, F., Holcombe, A. O., Humphreys, M., Lewis, M., \u0026amp; Nosek, B. A. (2013). Badges to Acknowledge Open Practices. https://osf.io/tvyxz/ BMJ. (2015, September 22). Introducing ‘How to write and publish a Study Protocol’ using BMJ’s new eLearning programme: Research to Publication. BMJ Open. https://blogs.bmj.com/bmjopen/2015/09/22/introducing-how-to-write-and-publish-a-study-protocol-using-bmjs-new-elearning-programme-research-to-publication/ Boivin, A., Richards, T., Forsythe, L., Grégoire, A., L’Espérance, A., Abelson, J., \u0026amp; Carman, K. L. (2018). Evaluating patient and public involvement in research. BMJ, k5147. https://doi.org/10.1136/bmj.k5147 Bol, T., de Vaan, M., \u0026amp; van de Rijt, A. (2018). The Matthew effect in science funding. Proceedings of the National Academy of Sciences, 115(19), 4887–4890. https://doi.org/10.1073/pnas.1719557115 Bollen, K. A. (1989). Structural equations with latent variables. Wiley. Borenstein, M. (Ed.). (2009). Introduction to meta-analysis. John Wiley \u0026amp; Sons. Bornmann, L., Ganser, C., Tekles, A., \u0026amp; Leydesdorff, L. (2019). Does the $h_\\alpha$ index reinforce the Matthew effect in science? Agent-based simulations using Stata and R. ArXiv:1905.11052 [Physics]. http://arxiv.org/abs/1905.11052 Borsboom, D., Mellenbergh, G. J., \u0026amp; van Heerden, J. (2004). The Concept of Validity. Psychological Review, 111(4), 1061–1071. https://doi.org/10.1037/0033-295X.111.4.1061 Borsboom, D., van der Maas, H., Dalege, J., Kievit, R., \u0026amp; Haig, B. (2020). Theory Construction Methodology: A practical framework for theory formation in psychology [Preprint]. PsyArXiv. https://doi.org/10.31234/osf.io/w5tp8 Bortoli, S. (2021, April 1). NIHR Guidance on co-producing a research project. Learning For Involvement. https://www.learningforinvolvement.org.uk/?opportunity=nihr-guidance-on-co-producing-a-research-project Bourne, P. E., Polka, J. K., Vale, R. D., \u0026amp; Kiley, R. (2017). Ten simple rules to consider regarding preprint submission. PLOS Computational Biology, 13(5), e1005473. https://doi.org/10.1371/journal.pcbi.1005473 Bouvy, J. C., \u0026amp; Mujoomdar, M. (2019). All-Male Panels and Gender Diversity of Issue Panels and Plenary Sessions at ISPOR Europe. PharmacoEconomics - Open, 3(3), 419–422. https://doi.org/10.1007/s41669-019-0153-0 Box, G. E. P. (1976). Science and Statistics. Journal of the American Statistical Association, 71(356), 791–799. https://doi.org/10.1080/01621459.1976.10480949 Bramoulle, Y., \u0026amp; Saint-Paul, G. (2007). Research Cycles. SSRN Electronic Journal. https://doi.org/10.2139/ssrn.965816 Brand, A., Allen, L., Altman, M., Hlava, M., \u0026amp; Scott, J. (2015). Beyond authorship: Attribution, contribution, collaboration, and credit. Learned Publishing, 28(2), 151–155. https://doi.org/10.1087/20150211 Brandt, M. J., IJzerman, H., Dijksterhuis, A., Farach, F. J., Geller, J., Giner-Sorolla, R., Grange, J. A., Perugini, M., Spies, J. R., \u0026amp; van ’t Veer, A. (2014). The Replication Recipe: What makes for a convincing replication? Journal of Experimental Social Psychology, 50, 217–224. https://doi.org/10.1016/j.jesp.2013.10.005 Braun, V., \u0026amp; Clarke, V. (2013). Successful qualitative research: A practical guide for beginners. Sage. https://books.google.co.uk/books?hl=en\u0026amp;lr=\u0026amp;id=nYMQAgAAQBAJ\u0026amp;oi=fnd\u0026amp;pg=PP2\u0026amp;ots=SqJAD7C-5w\u0026amp;sig=6hBnRUj4z31CbylBTRzfIudISME#v=onepage\u0026amp;q\u0026amp;f=false Brembs, B., Button, K., \u0026amp; Munafò, M. (2013). Deep impact: Unintended consequences of journal rank. Frontiers in Human Neuroscience, 7. https://doi.org/10.3389/fnhum.2013.00291 Brewer, P. R., \u0026amp; Ley, B. L. (2013). Whose Science Do You Believe? Explaining Trust in Sources of Scientific Information About the Environment. Science Communication, 35(1), 115–137. https://doi.org/10.1177/1075547012441691 Breznau, N., Rinke, E. M., Wuttke, A., Adem, M., Adriaans, J., Alvarez-Benjumea, A., Andersen, H. K., Auer, D., Azevedo, F., Bahnsen, O., Balzer, D., Bauer, G., Bauer, P., Baumann, M., Baute, S., Benoit, V., Bernauer, J., Berning, C., Berthold, A., … Nguyen, H. H. V. (2021). Observing Many Researchers Using the Same Data and Hypothesis Reveals a Hidden Universe of Uncertainty [Preprint]. MetaArXiv. https://doi.org/10.31222/osf.io/cd5j9 Breznau, N., Rinke, E. M., Wuttke, A., Nguyen, H. H. V., Adem, M., Adriaans, J., Akdeniz, E., Alvarez-Benjumea, A., Andersen, H. K., Auer, D., Azevedo, F., Bahnsen, O., Bai, L., Balzer, D., Bauer, G., Bauer, P., Baumann, M., Baute, S., Benoit, V., … Żółtak, T. (2021). How Many Replicators Does It Take to Achieve Reliability? Investigating Researcher Variability in a Crowdsourced Replication [Preprint]. SocArXiv. https://doi.org/10.31235/osf.io/j7qta Brod, M., Tesler, L. E., \u0026amp; Christensen, T. L. (2009). Qualitative research and content validity: Developing best practices based on science and experience. Quality of Life Research, 18(9), 1263–1278. https://doi.org/10.1007/s11136-009-9540-9 Brooks, T. A. (1985). Private acts and public objects: An investigation of citer motivations. Journal of the American Society for Information Science, 36(4), 223–229. https://doi.org/10.1002/asi.4630360402 Brown, J. (2010). An introduction to overlay journals (Repositories Support Project, pp. 1–6). University College London. Brown, N. J. L., \u0026amp; Heathers, J. A. J. (2017). The GRIM Test: A Simple Technique Detects Numerous Anomalies in the Reporting of Results in Psychology. Social Psychological and Personality Science, 8(4), 363–369. https://doi.org/10.1177/1948550616673876 Brown, N., Thompson, P., \u0026amp; Leigh, J. S. (2018). Making Academia More Accessible. Journal of Perspectives in Applied Academic Practice, 6(2), 82–90. https://doi.org/10.14297/jpaap.v6i2.348 Brulé, J. F., \u0026amp; Blount, A. (1989). Knowledge acquisition. McGraw-Hill. Brunner, J., \u0026amp; Schimmack, U. (2020). Estimating Population Mean Power Under Conditions of Heterogeneity and Selection for Significance. Meta-Psychology, 4. https://doi.org/10.15626/MP.2018.874 Bruns, S. B., \u0026amp; Ioannidis, J. P. A. (2016). P-Curve and p-Hacking in Observational Research. PLOS ONE, 11(2), e0149144. https://doi.org/10.1371/journal.pone.0149144 Budapest Open Access Initiative | Read the Budapest Open Access Initiative. (2002, February 14). https://www.budapestopenaccessinitiative.org/read Busse, C., Kach, A. P., \u0026amp; Wagner, S. M. (2017). Boundary Conditions: What They Are, How to Explore Them, Why We Need Them, and When to Consider Them. Organizational Research Methods, 20(4), 574–609. https://doi.org/10.1177/1094428116641191 Button, K. S., Chambers, C. D., Lawrence, N., \u0026amp; Munafò, M. R. (2020). Grassroots Training for Reproducible Science: A Consortium-Based Approach to the Empirical Dissertation. Psychology Learning \u0026amp; Teaching, 19(1), 77–90. https://doi.org/10.1177/1475725719857659 Button, K. S., Lawrence, N., Chambers, C. D., \u0026amp; Munafò, M. R. (2016). Instilling scientific rigour at the grassroots. The Psychologist, 29(16), 158–167. Byrne, J. A., \u0026amp; Christopher, J. (2020). Digital magic, or the dark arts of the 21 st century—How can journals and peer reviewers detect manuscripts and publications from paper mills? FEBS Letters, 594(4), 583–589. https://doi.org/10.1002/1873-3468.13747 Campbell, D. T. (1957). Factors relevant to the validity of experiments in social settings. Psychological Bulletin, 54(4), 297–312. https://doi.org/10.1037/h0040950 Campbell, D. T., \u0026amp; Stanley, J. C. (2011). Experimental and quasi-experimental designs for research. Wadsworth. Carp, J. (2012). On the Plurality of (Methodological) Worlds: Estimating the Analytic Flexibility of fMRI Experiments. Frontiers in Neuroscience, 6. https://doi.org/10.3389/fnins.2012.00149 Carsey, T. M. (2014). Making DA-RT a Reality. PS: Political Science \u0026amp; Politics, 47(01), 72–77. https://doi.org/10.1017/S1049096513001753 Carter, A., Tilling, K., \u0026amp; Munafo, M. R. (2021). Considerations of sample size and power calculations given a range of analytical scenarios [Preprint]. PsyArXiv. https://doi.org/10.31234/osf.io/tcqrn Case, C. M. (1928). Scholarship in sociology. Sociology and Social Research, 12, 323–340. Cassidy, S. A., Dimova, R., Giguère, B., Spence, J. R., \u0026amp; Stanley, D. J. (2019). Failing Grade: 89% of Introduction-to-Psychology Textbooks That Define or Explain Statistical Significance Do So Incorrectly. Advances in Methods and Practices in Psychological Science, 2(3), 233–239. https://doi.org/10.1177/2515245919858072 Center for Open Science. (n.d.). Registered Reports. Retrieved 10 July 2021, from https://www.cos.io/initiatives/registered-reports Centre for Open Science. (n.d.). Show Your Work. Share Your Work. Centre for Open Science. https://www.cos.io/ Chambers, C. D. (2013). Registered Reports: A new publishing initiative at Cortex. Cortex, 49(3), 609–610. https://doi.org/10.1016/j.cortex.2012.12.016 Chambers, C. D., Dienes, Z., McIntosh, R. D., Rotshtein, P., \u0026amp; Willmes, K. (2015). Registered Reports: Realigning incentives in scientific publishing. Cortex, 66, A1–A2. https://doi.org/10.1016/j.cortex.2015.03.022 Chambers, C. D., \u0026amp; Tzavella, L. (2020). The past, present, and future of Registered Reports [Preprint]. MetaArXiv. https://doi.org/10.31222/osf.io/43298 Chartier, C. R., Riegelman, A., \u0026amp; McCarthy, R. J. (2018). StudySwap: A Platform for Interlab Replication, Collaboration, and Resource Exchange. Advances in Methods and Practices in Psychological Science, 1(4), 574–579. https://doi.org/10.1177/2515245918808767 Chuard, P. J. C., Vrtílek, M., Head, M. L., \u0026amp; Jennions, M. D. (2019). Evidence that nonsignificant results are sometimes preferred: Reverse P-hacking or selective reporting? PLOS Biology, 17(1), e3000127. https://doi.org/10.1371/journal.pbio.3000127 CKAN - The open source data management system. (n.d.). Ckan. Retrieved 9 July 2021, from https://ckan.org/ Claerbout, J. F., \u0026amp; Karrenbach, M. (1992). Electronic documents give reproducible research a new meaning. SEG Technical Program Expanded Abstracts 1992, 601–604. https://doi.org/10.1190/1.1822162 Clark, H., Elsherif, M. M., \u0026amp; Leavens, D. A. (2019). Ontogeny vs. phylogeny in primate/canid comparisons: A meta-analysis of the object choice task. Neuroscience \u0026amp; Biobehavioral Reviews, 105, 178–189. https://doi.org/10.1016/j.neubiorev.2019.06.001 Closed access. (n.d.). CASRAI. Retrieved 9 July 2021, from https://casrai.org/term/closed-access/ Cohen, J. (1962). The statistical power of abnormal-social psychological research: A review. The Journal of Abnormal and Social Psychology, 65(3), 145–153. https://doi.org/10.1037/h0045186 Cohen, J. (1988). Statistical power analysis for the behavioral sciences (2nd ed). L. Erlbaum Associates. Cohn, J. P. (2008). Citizen Science: Can Volunteers Do Real Research? BioScience, 58(3), 192–197. https://doi.org/10.1641/B580303 Collaborative Assessment forTrustworthy Science|The repliCATS project. (n.d.). University of Melbourne. Retrieved 10 July 2021, from https://replicats.research.unimelb.edu.au/ Committee on Reproducibility and Replicability in Science, Board on Behavioral, Cognitive, and Sensory Sciences, Committee on National Statistics, Division of Behavioral and Social Sciences and Education, Nuclear and Radiation Studies Board, Division on Earth and Life Studies, Board on Mathematical Sciences and Analytics, Committee on Applied and Theoretical Statistics, Division on Engineering and Physical Sciences, Board on Research Data and Information, Committee on Science, Engineering, Medicine, and Public Policy, Policy and Global Affairs, \u0026amp; National Academies of Sciences, Engineering, and Medicine. (2019). Reproducibility and Replicability in Science (p. 25303). National Academies Press. https://doi.org/10.17226/25303 Confederation Of Open Access Repositories. (2020). COAR Community Framework for Best Practices in Repositories. (Version 1). Zenodo. https://doi.org/10.5281/ZENODO.4110829 Cook, T. D., \u0026amp; Campbell, D. T. (1979). Quasi-experimentation: Design \u0026amp; analysis issues for field settings. Rand McNally College Pub. Co. Corley, K. G., \u0026amp; Gioia, D. A. (2011). Building Theory about Theory Building: What Constitutes a Theoretical Contribution? Academy of Management Review, 36(1), 12–32. https://doi.org/10.5465/amr.2009.0486 Cornwall, A., \u0026amp; Jewkes, R. (1995). What is participatory research? Social Science \u0026amp; Medicine, 41(12), 1667–1676. https://doi.org/10.1016/0277-9536(95)00127-S Correction or retraction? (2006). Nature, 444(7116), 123–124. https://doi.org/10.1038/444123b Corti, L. (2019). Managing and sharing research data: A guide to good practice (2nd edition). SAGE Publications. Cowan, N., Belletier, C., Doherty, J. M., Jaroslawska, A. J., Rhodes, S., Forsberg, A., Naveh-Benjamin, M., Barrouillet, P., Camos, V., \u0026amp; Logie, R. H. (2020). How Do Scientific Views Change? Notes From an Extended Adversarial Collaboration. Perspectives on Psychological Science, 15(4), 1011–1025. https://doi.org/10.1177/1745691620906415 CRediT - Contributor Roles Taxonomy. (n.d.). Casrai. Retrieved 9 July 2021, from https://casrai.org/credit/ Crenshaw, K. (1989). Demarginalizing the Intersection of Race and Sex: A Black Feminist Critique of Antidiscrimination Doctrine, Feminist Theory and Antiracist Politics. University of Chicago Legal Forum, 1989(1), 8. https://chicagounbound.uchicago.edu/uclf/vol1989/iss1/8 Cronbach, L. J., \u0026amp; Meehl, P. E. (1955). Construct validity in psychological tests. Psychological Bulletin, 52(4), 281–302. https://doi.org/10.1037/h0040957 Cronin, B. (2001). Hyperauthorship: A postmodern perversion or evidence of a structural shift in scholarly communication practices? Journal of the American Society for Information Science and Technology, 52(7), 558–569. https://doi.org/10.1002/asi.1097 Crosetto, P. (2021, April 12). Is MDPI a predatory publisher? Paolo Crosetto. https://paolocrosetto.wordpress.com/2021/04/12/is-mdpi-a-predatory-publisher/ Crutzen, R., Ygram Peters, G.-J., \u0026amp; Mondschein, C. (2019). Why and how we should care about the General Data Protection Regulation. Psychology \u0026amp; Health, 34(11), 1347–1357. https://doi.org/10.1080/08870446.2019.1606222 Crüwell, S., van Doorn, J., Etz, A., Makel, M. C., Moshontz, H., Niebaum, J. C., Orben, A., Parsons, S., \u0026amp; Schulte-Mecklenbeck, M. (2019). Seven Easy Steps to Open Science: An Annotated Reading List. Zeitschrift Für Psychologie, 227(4), 237–248. https://doi.org/10.1027/2151-2604/a000387 Curran, P. J. (2009). The seemingly quixotic pursuit of a cumulative psychological science: Introduction to the special issue. Psychological Methods, 14(2), 77–80. https://doi.org/10.1037/a0015972 Curry, S. (2012, August 13). Sick of Impact Factors | Reciprocal Space. Reciprocal Space. http://occamstypewriter.org/scurry/2012/08/13/sick-of-impact-factors/ d’Espagnat, B. (2008). Is Science Cumulative? A Physicist Viewpoint. In L. Soler, H. Sankey, \u0026amp; P. Hoyningen-Huene (Eds.), Rethinking Scientific Change and Theory Comparison (pp. 145–151). Springer Netherlands. https://doi.org/10.1007/978-1-4020-6279-7_10 Data Management Expert Guide—CESSDA TRAINING. (n.d.). CESSDA. Retrieved 10 July 2021, from https://www.cessda.eu/Training/Training-Resources/Library/Data-Management-Expert-Guide Data management plans | Stanford Libraries. (n.d.). Stanford Libraries. Retrieved 9 July 2021, from https://library.stanford.edu/research/data-management-services/data-management-plans Data protection. (n.d.). [Text]. European Commission - European Commission. Retrieved 9 July 2021, from https://ec.europa.eu/info/law/law-topic/data-protection_en Datacite Metadata Schema. (n.d.). DataCite Schema. Retrieved 9 July 2021, from https://schema.datacite.org/ Davies, G. M., \u0026amp; Gray, A. (2015). Don’t let spurious accusations of pseudoreplication limit our ability to learn from natural experiments (and other messy kinds of ecological monitoring). Ecology and Evolution, 5(22), 5295–5304. https://doi.org/10.1002/ece3.1782 Day, S., Rennie, S., Luo, D., \u0026amp; Tucker, J. D. (2020). Open to the public: Paywalls and the public rationale for open access medical research publishing. Research Involvement and Engagement, 6(1), 8. https://doi.org/10.1186/s40900-020-0182-y Declaration on Research Assessment. (n.d.). Health Research Board. Retrieved 9 July 2021, from https://www.hrb.ie/funding/funding-schemes/before-you-apply/how-we-assess-applications/declaration-on-research-assessment/ Del Giudice, M., \u0026amp; Gangestad, S. W. (2021). A Traveler’s Guide to the Multiverse: Promises, Pitfalls, and a Framework for the Evaluation of Analytic Decisions. Advances in Methods and Practices in Psychological Science, 4(1), 251524592095492. https://doi.org/10.1177/2515245920954925 Deutsche Forschungsgemeinschaft. (2019). Guidelines for Safeguarding Good Research Practice. Code of Conduct. https://doi.org/10.5281/ZENODO.3923602 DeVellis, R. F. (2017). Scale development: Theory and applications (Fourth edition). SAGE. Devezer, B., Navarro, D. J., Vandekerckhove, J., \u0026amp; Ozge Buzbas, E. (2021). The case for formal methodology in scientific reform. Royal Society Open Science, 8(3), rsos.200805, 200805. https://doi.org/10.1098/rsos.200805 Dickersin, K., \u0026amp; Min, Y.-I. (1993). Publication Bias: The Problem That Won’t Go Away. Annals of the New York Academy of Sciences, 703(1 Doing More Go), 135–148. https://doi.org/10.1111/j.1749-6632.1993.tb26343.x Dienes, Z. (2008). Understanding Psychology as a Science: An Introduction to Scientific and Statistical Inference. Palgrave Macmillan. https://books.google.ca/books?id=qCQdBQAAQBAJ Dienes, Z. (2011). Bayesian Versus Orthodox Statistics: Which Side Are You On? Perspectives on Psychological Science, 6(3), 274–290. https://doi.org/10.1177/1745691611406920 Dienes, Z. (2014). Using Bayes to get the most out of non-significant results. Frontiers in Psychology, 5. https://doi.org/10.3389/fpsyg.2014.00781 Dienes, Z. (2016). How Bayes factors change scientific practice. Journal of Mathematical Psychology, 72, 78–89. https://doi.org/10.1016/j.jmp.2015.10.003 Digital Object Identifier System Handbook. (n.d.). DOI. Retrieved 9 July 2021, from https://www.doi.org/hb.html Directory of Open Access Journals. (n.d.). Retrieved 11 July 2021, from https://doaj.org/apply/transparency/ Doll, R., \u0026amp; Hill, A. B. (1954). The Mortality of Doctors in Relation to Their Smoking Habits. BMJ, 1(4877), 1451–1455. https://doi.org/10.1136/bmj.1.4877.1451 Domov | SKRN (Slovak Reproducibility network). (n.d.). SKRN. Retrieved 10 July 2021, from https://slovakrn.wixsite.com/skrn Download JASP. (n.d.). JASP - Free and User-Friendly Statistical Software. Retrieved 9 July 2021, from https://jasp-stats.org/download/ Drost, E. A. (2011). Validity and reliability in social science research. Education Research and Perspectives, 38(1), 105–123. Du Bois, W. E. B. (2018). The souls of Black folk: Essays and sketches. Duval, S., \u0026amp; Tweedie, R. (2000a). A Nonparametric ‘Trim and Fill’ Method of Accounting for Publication Bias in Meta-Analysis. Journal of the American Statistical Association, 95(449), 89. https://doi.org/10.2307/2669529 Duval, S., \u0026amp; Tweedie, R. (2000b). Trim and Fill: A Simple Funnel-Plot-Based Method of Testing and Adjusting for Publication Bias in Meta-Analysis. Biometrics, 56(2), 455–463. https://doi.org/10.1111/j.0006-341X.2000.00455.x Duyx, B., Swaen, G. M. H., Urlings, M. J. E., Bouter, L. M., \u0026amp; Zeegers, M. P. (2019). The strong focus on positive results in abstracts may cause bias in systematic reviews: A case study on abstract reporting bias. Systematic Reviews, 8(1), 174. https://doi.org/10.1186/s13643-019-1082-9 Eagly, A. H., \u0026amp; Riger, S. (2014). Feminism and psychology: Critiques of methods and epistemology. American Psychologist, 69(7), 685–702. https://doi.org/10.1037/a0037372 Easterbrook, S. M. (2014). Open code for open science? Nature Geoscience, 7(11), 779–781. https://doi.org/10.1038/ngeo2283 Ebersole, C. R., Atherton, O. E., Belanger, A. L., Skulborstad, H. M., Allen, J. M., Banks, J. B., Baranski, E., Bernstein, M. J., Bonfiglio, D. B. V., Boucher, L., Brown, E. R., Budiman, N. I., Cairo, A. H., Capaldi, C. A., Chartier, C. R., Chung, J. M., Cicero, D. C., Coleman, J. A., Conway, J. G., … Nosek, B. A. (2016). Many Labs 3: Evaluating participant pool quality across the academic semester via replication. Journal of Experimental Social Psychology, 67, 68–82. https://doi.org/10.1016/j.jesp.2015.10.012 Editorial Director. (2021, May). What is a group author (collaborative author) and does it need an ORCID? JMIR Publications. https://support.jmir.org/hc/en-us/articles/115001449591-What-is-a-group-author-collaborative-author-and-does-it-need-an-ORCID- Eldermire, E. (n.d.). LibGuides: Measuring your research impact: i10-Index. Retrieved 9 July 2021, from https://guides.library.cornell.edu/impact/author-impact-10 Eley, A. R. (Ed.). (2012). Becoming a successful early career researcher. Routledge. Ellemers, N. (2021). Science as collaborative knowledge generation. British Journal of Social Psychology, 60(1), 1–28. https://doi.org/10.1111/bjso.12430 Elliott, K. C., \u0026amp; Resnik, D. B. (2019). Making Open Science Work for Science and Society. Environmental Health Perspectives, 127(7), 075002. https://doi.org/10.1289/EHP4808 Elm, E. von, Altman, D. G., Egger, M., Pocock, S. J., Gøtzsche, P. C., \u0026amp; Vandenbroucke, J. P. (2007). Strengthening the reporting of observational studies in epidemiology (STROBE) statement: Guidelines for reporting observational studies. BMJ, 335(7624), 806–808. https://doi.org/10.1136/bmj.39335.541782.AD Elman, C., Gerring, J., \u0026amp; Mahoney, J. (Eds.). (2020). The production of knowledge: Enhancing progress in social science. Cambridge University Press. Elmore, S. A. (2018). Preprints: What Role Do These Have in Communicating Scientific Results? Toxicologic Pathology, 46(4), 364–365. https://doi.org/10.1177/0192623318767322 Embargo (academic publishing). (2021). In Wikipedia. https://en.wikipedia.org/w/index.php?title=Embargo_(academic_publishing)\u0026amp;oldid=1016895567 Epskamp, S., \u0026amp; Nuijten, M. B. (2018). statcheck: Extract Statistics from Articles and Recompute p Values (1.3.0) [Computer software]. https://CRAN.R-project.org/package=statcheck Esterling, K., Brady, D., \u0026amp; Schwitzgebel, E. (2021). The Necessity of Construct and External Validity for Generalized Causal Claims [Preprint]. Open Science Framework. https://doi.org/10.31219/osf.io/2s8w5 Etz, A., Gronau, Q. F., Dablander, F., Edelsbrunner, P. A., \u0026amp; Baribault, B. (2018). How to become a Bayesian in eight easy steps: An annotated reading list. Psychonomic Bulletin \u0026amp; Review, 25(1), 219–234. https://doi.org/10.3758/s13423-017-1317-5 European Commission. (2021). European Commission. Responsible Research \u0026amp; Innovation | Horizon 2020. https://ec.europa.eu/programmes/horizon2020/en/h2020-section/responsible-research-innovation Evans, G., \u0026amp; Durant, J. (1995). The relationship between knowledge and attitudes in the public understanding of science in Britain. Public Understanding of Science, 4(1), 57–74. https://doi.org/10.1088/0963-6625/4/1/004 Evans, O., \u0026amp; Rubin, M. (2021). In a Class on Their Own: Investigating the Role of Social Integration in the Association Between Social Class and Mental Well-Being. Personality and Social Psychology Bulletin, 014616722110211. https://doi.org/10.1177/01461672211021190 Evidence Synthesis. (n.d.). LSHTM. Retrieved 9 July 2021, from https://www.lshtm.ac.uk/research/centres/centre-evaluation/evidence-synthesis Fanelli, D. (2010). Do Pressures to Publish Increase Scientists’ Bias? An Empirical Support from US States Data. PLoS ONE, 5(4), e10271. https://doi.org/10.1371/journal.pone.0010271 Fanelli, D. (2018). Opinion: Is science really facing a reproducibility crisis, and do we need it to? Proceedings of the National Academy of Sciences, 115(11), 2628–2631. https://doi.org/10.1073/pnas.1708272114 Farrow, R. (2017). Open education and critical pedagogy. Learning, Media and Technology, 42(2), 130–146. https://doi.org/10.1080/17439884.2016.1113991 Faul, F., Erdfelder, E., Buchner, A., \u0026amp; Lang, A.-G. (2009). Statistical power analyses using G*Power 3.1: Tests for correlation and regression analyses. Behavior Research Methods, 41(4), 1149–1160. https://doi.org/10.3758/BRM.41.4.1149 Faul, F., Erdfelder, E., Lang, A.-G., \u0026amp; Buchner, A. (2007). G*Power 3: A flexible statistical power analysis program for the social, behavioral, and biomedical sciences. Behavior Research Methods, 39(2), 175–191. https://doi.org/10.3758/BF03193146 Ferson, S., Joslyn, C. A., Helton, J. C., Oberkampf, W. L., \u0026amp; Sentz, K. (2004). Summary from the epistemic uncertainty workshop: Consensus amid diversity. Reliability Engineering \u0026amp; System Safety, 85(1–3), 355–369. https://doi.org/10.1016/j.ress.2004.03.023 Fiedler, K., Kutzner, F., \u0026amp; Krueger, J. I. (2012). The Long Way From α-Error Control to Validity Proper: Problems With a Short-Sighted False-Positive Debate. Perspectives on Psychological Science, 7(6), 661–669. https://doi.org/10.1177/1745691612462587 Fiedler, K., \u0026amp; Schwarz, N. (2016). Questionable Research Practices Revisited. Social Psychological and Personality Science, 7(1), 45–52. https://doi.org/10.1177/1948550615612150 Filipe, A., Renedo, A., \u0026amp; Marston, C. (2017). The co-production of what? Knowledge, values, and social relations in health care. PLOS Biology, 15(5), e2001403. https://doi.org/10.1371/journal.pbio.2001403 Findley, M. G., Jensen, N. M., Malesky, E. J., \u0026amp; Pepinsky, T. B. (2016). Can Results-Free Review Reduce Publication Bias? The Results and Implications of a Pilot Study. Comparative Political Studies, 49(13), 1667–1703. https://doi.org/10.1177/0010414016655539 Finlay, L., \u0026amp; Gough, B. (Eds.). (2003). Reflexivity: A practical guide for researchers in health and social sciences. Blackwell Science. Flake, J. K., \u0026amp; Fried, E. I. (2020). Measurement Schmeasurement: Questionable Measurement Practices and How to Avoid Them. Advances in Methods and Practices in Psychological Science, 3(4), 456–465. https://doi.org/10.1177/2515245920952393 Fletcher-Watson, S., Adams, J., Brook, K., Charman, T., Crane, L., Cusack, J., Leekam, S., Milton, D., Parr, J. R., \u0026amp; Pellicano, E. (2019). Making the future together: Shaping autism research through meaningful participation. Autism, 23(4), 943–953. https://doi.org/10.1177/1362361318786721 Foreman-Mackey, D., Hogg, D. W., Lang, D., \u0026amp; Goodman, J. (2013). emcee: The MCMC Hammer. Publications of the Astronomical Society of the Pacific, 125(925), 306–312. https://doi.org/10.1086/670067 Forrt. (2019). Introducing a Framework for Open and Reproducible Research Training (FORRT) [Preprint]. Open Science Framework. https://doi.org/10.31219/osf.io/bnh7p FORRT - Framework for Open and Reproducible Research Training. (n.d.). FORRT. Retrieved 9 July 2021, from https://forrt.org/ Foster, MSLS, E. D., \u0026amp; Deardorff, MLIS, A. (2017). Open Science Framework (OSF). Journal of the Medical Library Association, 105(2). https://doi.org/10.5195/JMLA.2017.88 Franco, A., Malhotra, N., \u0026amp; Simonovits, G. (2014). Publication bias in the social sciences: Unlocking the file drawer. Science, 345(6203), 1502–1505. https://doi.org/10.1126/science.1255484 Frank, M. C., Bergelson, E., Bergmann, C., Cristia, A., Floccia, C., Gervain, J., Hamlin, J. K., Hannon, E. E., Kline, M., Levelt, C., Lew-Williams, C., Nazzi, T., Panneton, R., Rabagliati, H., Soderstrom, M., Sullivan, J., Waxman, S., \u0026amp; Yurovsky, D. (2017). A Collaborative Approach to Infant Research: Promoting Reproducibility, Best Practices, and Theory-Building. Infancy, 22(4), 421–435. https://doi.org/10.1111/infa.12182 Franzoni, C., \u0026amp; Sauermann, H. (2014). Crowd science: The organization of scientific research in open collaborative projects. Research Policy, 43(1), 1–20. https://doi.org/10.1016/j.respol.2013.07.005 Fraser, H., Bush, M., Wintle, B., Mody, F., Smith, E. T., Hanea, A., Gould, E., Hemming, V., Hamilton, D. G., Rumpff, L., Wilkinson, D. P., Pearson, R., Singleton Thorn, F., Ashton, raquel, Willcox, A., Gray, C. T., Head, A., Ross, M., Groenewegen, R., … Fidler, F. (2021). Predicting reliability through structured expert elicitation with repliCATS (Collaborative Assessments for Trustworthy Science) [Preprint]. MetaArXiv. https://doi.org/10.31222/osf.io/2pczv Free Our Knowledge. (n.d.). About. Free Our Knowledge. Retrieved 9 July 2021, from https://freeourknowledge.org/about/ Frigg, R., \u0026amp; Hartmann, S. (2020). Models in Science. In E. N. Zalta (Ed.), The Stanford Encyclopedia of Philosophy (Spring 2020). Metaphysics Research Lab, Stanford University. https://plato.stanford.edu/archives/spr2020/entries/models-science/ Frith, U. (2020). Fast Lane to Slow Science. Trends in Cognitive Sciences, 24(1), 1–2. https://doi.org/10.1016/j.tics.2019.10.007 Galligan, F., \u0026amp; Dyas-Correia, S. (2013). Altmetrics: Rethinking the Way We Measure. Serials Review, 39(1), 56–61. https://doi.org/10.1080/00987913.2013.10765486 Garson, G. D. (2012). Testing Statistical Assumptions (2012 edition). North Carolina State University. Gelman, A., \u0026amp; Carlin, J. (2014). Beyond Power Calculations: Assessing Type S (Sign) and Type M (Magnitude) Errors. Perspectives on Psychological Science, 9(6), 641–651. https://doi.org/10.1177/1745691614551642 Gelman, A., \u0026amp; Loken, E. (2013). The garden of forking paths: Why multiple comparisons can be a problem, even when there is no “fishing expedition” or “p-hacking” and the research hypothesis was posited ahead of time [Doctoral dissertation, Columbia University]. http://www.stat.columbia.edu/~gelman/research/unpublished/p_hacking.pdf Gelman, A., \u0026amp; Stern, H. (2006). The Difference Between “Significant” and “Not Significant” is not Itself Statistically Significant. The American Statistician, 60(4), 328–331. https://doi.org/10.1198/000313006X152649 Generalizability. (2018). In B. B. Frey, The SAGE Encyclopedia of Educational Research, Measurement, and\u0026nbsp; \u0026nbsp; \u0026nbsp; Evaluation. SAGE Publications, Inc. https://doi.org/10.4135/9781506326139.n284 Gentleman, R. (2005). Reproducible Research: A Bioinformatics Case Study. Statistical Applications in Genetics and Molecular Biology, 4(1). https://doi.org/10.2202/1544-6115.1034 Get Involved—Creative Commons. (n.d.). Creative Commons. Retrieved 9 July 2021, from https://creativecommons.org/about/get-involved/ Geyer, C., J. (2003). Maximum Likelihood in R (pp. 1–9) [Preprint]. Open Science Framework. Geyer, C., J. (2007). Stat 5102 Notes: Maximum Likelihood (pp. 1–8) [Preprint]. Open Science Framework. Gilroy, P. (2002). The black Atlantic: Modernity and double consciousness (3. impr., reprint). Verso. Giner-Sorolla, R., Carpenter, T., Montoya, A., \u0026amp; Neil Lewis, J. (2019). SPSP Power Analysis Working Group 2019. https://osf.io/9bt5s/ Ginsparg, P. (1997). Winners and Losers in the Global Research Village. The Serials Librarian, 30(3–4), 83–95. https://doi.org/10.1300/J123v30n03_13 Ginsparg, P. (2001, February 20). Creating a global knowledge network. Cornell University. http://www.cs.cornell.edu/~ginsparg/physics/blurb/pg01unesco.html Gioia, D. A., \u0026amp; Pitre, E. (1990). Multiparadigm Perspectives on Theory Building. Academy of Management Review, 15(4), 584–602. https://doi.org/10.5465/amr.1990.4310758 Git—About Version Control. (n.d.). Git. Retrieved 9 July 2021, from https://git-scm.com/book/en/v2/Getting-Started-About-Version-Control Glass, D. J., \u0026amp; Hall, N. (2008). A Brief History of the Hypothesis. Cell, 134(3), 378–381. https://doi.org/10.1016/j.cell.2008.07.033 Gollwitzer, M., Abele-Brehm, A., Fiebach, C., Ramthun, R., Scheel, A. M., Schönbrodt, F. D., \u0026amp; Steinberg, U. (2020). Data Management and Data Sharing in Psychological Science: Revision of the DGPs Recommendations [Preprint]. PsyArXiv. https://doi.org/10.31234/osf.io/24ncs Goodman, S. N., Fanelli, D., \u0026amp; Ioannidis, J. P. A. (2016). What does research reproducibility mean? Science Translational Medicine, 8(341), 341ps12-341ps12. https://doi.org/10.1126/scitranslmed.aaf5027 Goodman, S. W., \u0026amp; Pepinsky, T. B. (2019). Gender Representation and Strategies for Panel Diversity: Lessons from the APSA Annual Meeting. PS: Political Science \u0026amp; Politics, 52(4), 669–676. https://doi.org/10.1017/S1049096519000908 Gorgolewski, K. J., Auer, T., Calhoun, V. D., Craddock, R. C., Das, S., Duff, E. P., Flandin, G., Ghosh, S. S., Glatard, T., Halchenko, Y. O., Handwerker, D. A., Hanke, M., Keator, D., Li, X., Michael, Z., Maumet, C., Nichols, B. N., Nichols, T. E., Pellman, J., … Poldrack, R. A. (2016). The brain imaging data structure, a format for organizing and describing outputs of neuroimaging experiments. Scientific Data, 3(1), 160044. https://doi.org/10.1038/sdata.2016.44 Graham, I. D., McCutcheon, C., \u0026amp; Kothari, A. (2019). Exploring the frontiers of research co-production: The Integrated Knowledge Translation Research Network concept papers. Health Research Policy and Systems, 17(1), 88, s12961-019-0501–0507. https://doi.org/10.1186/s12961-019-0501-7 GRN · German Reproducibility Network. (n.d.). German Reproducibility Network. Retrieved 10 July 2021, from https://reproducibilitynetwork.de/ Grossmann, A., \u0026amp; Brembs, B. (2021). Current market rates for scholarly publishing services. F1000Research, 10, 20. https://doi.org/10.12688/f1000research.27468.1 Grzanka, P. R., Flores, M. J., VanDaalen, R. A., \u0026amp; Velez, G. (2020). Intersectionality in psychology: Translational science for social justice. Translational Issues in Psychological Science, 6(4), 304–313. https://doi.org/10.1037/tps0000276 Guenther, E. A., \u0026amp; Rodriguez, J. K. (2020, October 14). What’s wrong with ‘manels’ and what can we do about them. The Conversation. http://theconversation.com/whats-wrong-with-manels-and-what-can-we-do-about-them-148068 Guest, O. (2017, June 5). @BrianNosek @ctitusbrown @StuartBuck1 @DaniRabaiotti @Julie_B92 @jeroenbosman @blahah404 @OSFramework Thanks! Hopefully this thread \u0026amp; many other similar discussions \u0026amp; blogs will help make it less Bropen Science and more Open Science. *hides* [Tweet]. @o_guest. https://twitter.com/o_guest/status/871675631062458368 Guest, O., \u0026amp; Martin, A. E. (2021). How Computational Modeling Can Force Theory Building in Psychological Science. Perspectives on Psychological Science, 174569162097058. https://doi.org/10.1177/1745691620970585 Guide to the UK General Data Protection Regulation (UK GDPR). (2021, July 1). ICO. https://ico.org.uk/for-organisations/guide-to-data-protection/guide-to-the-general-data-protection-regulation-gdpr/ Haak, L. L., Fenner, M., Paglione, L., Pentz, E., \u0026amp; Ratner, H. (2012). ORCID: A system to uniquely identify researchers. Learned Publishing, 25(4), 259–264. https://doi.org/10.1087/20120404 Hackett, R., \u0026amp; Kelly, S. (2020). Publishing ethics in the era of paper mills. Biology Open, 9(10), bio056556. https://doi.org/10.1242/bio.056556 Hahn, G. J., \u0026amp; Meeker, W. Q. (1993). Assumptions for Statistical Inference. The American Statistician, 47(1), 1–11. https://doi.org/10.1080/00031305.1993.10475924 Hardwicke, T. E., Bohn, M., MacDonald, K., Hembacher, E., Nuijten, M. B., Peloquin, B. N., deMayo, B. E., Long, B., Yoon, E. J., \u0026amp; Frank, M. C. (2021). Analytic reproducibility in articles receiving open data badges at the journal Psychological Science: An observational study. Royal Society Open Science, 8(1), 201494. https://doi.org/10.1098/rsos.201494 Hardwicke, T. E., Jameel, L., Jones, M., Walczak, E. J., \u0026amp; Magis-Weinberg, L. (2014). Only Human: Scientists, Systems, and Suspect Statistics. Opticon1826, 16. https://doi.org/10.5334/opt.ch Harris, C. R., Millman, K. J., van der Walt, S. J., Gommers, R., Virtanen, P., Cournapeau, D., Wieser, E., Taylor, J., Berg, S., Smith, N. J., Kern, R., Picus, M., Hoyer, S., van Kerkwijk, M. H., Brett, M., Haldane, A., del Río, J. F., Wiebe, M., Peterson, P., … Oliphant, T. E. (2020). Array programming with NumPy. Nature, 585(7825), 357–362. https://doi.org/10.1038/s41586-020-2649-2 Hart, D., \u0026amp; Silka, L. (n.d.). Rebuilding the Ivory Tower: A Bottom-Up Experiment in Aligning Research With Societal Needs. Issues in Science and Technology, 36(3), 79–85. https://issues.org/aligning-research-with-societal-needs/ Hartgerink, C. H. J., Wicherts, J. M., \u0026amp; van Assen, M. A. L. M. (2017). Too Good to be False: Nonsignificant Results Revisited. Collabra: Psychology, 3(1), 9. https://doi.org/10.1525/collabra.71 Hayes, B. C., \u0026amp; Tariq, V. N. (2000). Gender differences in scientific knowledge and attitudes toward science: A comparative study of four Anglo-American nations. Public Understanding of Science, 9(4), 433–447. https://doi.org/10.1088/0963-6625/9/4/306 Haynes, S. N., Richard, D. C. S., \u0026amp; Kubany, E. S. (1995). Content validity in psychological assessment: A functional approach to concepts and methods. Psychological Assessment, 7(3), 238–247. https://doi.org/10.1037/1040-3590.7.3.238 Healy, K. (2018). Data visualization: A practical introduction. Princeton University Press. Heathers, J. A., Anaya, J., van der Zee, T., \u0026amp; Brown, N. J. (2018). Recovering data from summary statistics: Sample Parameter Reconstruction via Iterative TEchniques (SPRITE) [Preprint]. PeerJ Preprints. https://doi.org/10.7287/peerj.preprints.26968v1 Hendriks, F., Kienhues, D., \u0026amp; Bromme, R. (2016). Trust in Science and the Science of Trust. In B. Blöbaum (Ed.), Trust and Communication in a Digitized World (pp. 143–159). Springer International Publishing. https://doi.org/10.1007/978-3-319-28059-2_8 Henrich, J., Heine, S. J., \u0026amp; Norenzayan, A. (2010). The weirdest people in the world? Behavioral and Brain Sciences, 33(2–3), 61–83. https://doi.org/10.1017/S0140525X0999152X Henrich, J. P. (2020). The WEIRDest people in the world: How the West Became Psychologically Peculiar and Particularly Prosperous. Farrar, Straus and Giroux. Herrmannova, D., \u0026amp; Knoth, P. (2016). Semantometrics: Towards Fulltext-based Research Evaluation. Proceedings of the 16th ACM/IEEE-CS on Joint Conference on Digital Libraries, 235–236. https://doi.org/10.1145/2910896.2925448 Heyman, T., Moors, P., \u0026amp; Rabagliati, H. (2020). The benefits of adversarial collaboration for commentaries. Nature Human Behaviour, 4(12), 1217–1217. https://doi.org/10.1038/s41562-020-00978-6 Higgins, J. P. T., \u0026amp; Cochrane Collaboration (Eds.). (2020). Cochrane handbook for systematic reviews of interventions (Second edition). Wiley-Blackwell. Himmelstein, D. S., Rubinetti, V., Slochower, D. R., Hu, D., Malladi, V. S., Greene, C. S., \u0026amp; Gitter, A. (2019). Open collaborative writing with Manubot. PLOS Computational Biology, 15(6), e1007128. https://doi.org/10.1371/journal.pcbi.1007128 Hirsch, J. E. (2005). An index to quantify an individual’s scientific research output. Proceedings of the National Academy of Sciences, 102(46), 16569–16572. https://doi.org/10.1073/pnas.0507655102 Hitchcock, C., Meyer, A., Rose, D., \u0026amp; Jackson, R. (2002). Providing New Access to the General Curriculum: Universal Design for Learning. TEACHING Exceptional Children, 35(2), 8–17. https://doi.org/10.1177/004005990203500201 Hoekstra, R., Kiers, H., \u0026amp; Johnson, A. (2012). Are Assumptions of Well-Known Statistical Techniques Checked, and Why (Not)? Frontiers in Psychology, 3. https://doi.org/10.3389/fpsyg.2012.00137 Hogg, D. W., Bovy, J., \u0026amp; Lang, D. (2010). Data analysis recipes: Fitting a model to data. ArXiv:1008.4686 [Astro-Ph, Physics:Physics]. http://arxiv.org/abs/1008.4686 Hoijtink, H., Mulder, J., van Lissa, C., \u0026amp; Gu, X. (2019). A tutorial on testing hypotheses using the Bayes factor. Psychological Methods, 24(5), 539–556. https://doi.org/10.1037/met0000201 Holcombe, A. O. (2019). Contributorship, Not Authorship: Use CRediT to Indicate Who Did What. Publications, 7(3), 48. https://doi.org/10.3390/publications7030048 Holden, R. R. (2010). Face Validity. In I. B. Weiner \u0026amp; W. E. Craighead (Eds.), The Corsini Encyclopedia of Psychology (p. corpsy0341). John Wiley \u0026amp; Sons, Inc. https://doi.org/10.1002/9780470479216.corpsy0341 Home | re3data.org. (n.d.). DataCite Schema. Retrieved 10 July 2021, from https://www.re3data.org/ Homepage. (n.d.). Open Science MOOC. Retrieved 9 July 2021, from https://opensciencemooc.eu/ Houtkoop, B. L., Chambers, C., Macleod, M., Bishop, D. V. M., Nichols, T. E., \u0026amp; Wagenmakers, E.-J. (2018). Data Sharing in Psychology: A Survey on Barriers and Preconditions. Advances in Methods and Practices in Psychological Science, 1(1), 70–85. https://doi.org/10.1177/2515245917751886 How to Make Inclusivity More Than Just an Office Buzzword. (n.d.). Kellogg Insight. Retrieved 9 July 2021, from https://insight.kellogg.northwestern.edu/article/how-to-make-inclusivity-more-than-just-an-office-buzzword Https://improvingpsych.org/. (n.d.). Retrieved 10 July 2021, from https://improvingpsych.org/ Huber, B., Barnidge, M., Gil de Zúñiga, H., \u0026amp; Liu, J. (2019). Fostering public trust in science: The role of social media. Public Understanding of Science, 28(7), 759–777. https://doi.org/10.1177/0963662519869097 Huber, C. (2016a, November 1). The Stata Blog » Introduction to Bayesian statistics, part 1: The basic concepts. The Stata Blog. https://blog.stata.com/2016/11/01/introduction-to-bayesian-statistics-part-1-the-basic-concepts/ Huber, C. (2016b, November 15). Introduction to Bayesian statistics, part 2: MCMC and the Metropolis–Hastings algorithm. The Stata Blog. https://blog.stata.com/2016/11/15/introduction-to-bayesian-statistics-part-2-mcmc-and-the-metropolis-hastings-algorithm/ Huelin, R., Iheanacho, I., Payne, K., \u0026amp; Sandman, K. (2015). What’s in a Name? Systematic and Non-Systematic Literature Reviews, and Why the Distinction Matters—Evidera (The Evidence Forum, pp. 34–37). https://www.evidera.com/resource/whats-in-a-name-systematic-and-non-systematic-literature-reviews-and-why-the-distinction-matters/ Hüffmeier, J., Mazei, J., \u0026amp; Schultze, T. (2016). Reconceptualizing replication as a sequence of different studies: A replication typology. Journal of Experimental Social Psychology, 66, 81–92. https://doi.org/10.1016/j.jesp.2015.09.009 Hunter, J. E., \u0026amp; Schmidt, F. L. (2015). Methods of meta-analysis: Correcting error and bias in research findings (Third edition). SAGE. Hurlbert, S. H. (1984). Pseudoreplication and the Design of Ecological Field Experiments. Ecological Monographs, 54(2), 187–211. https://doi.org/10.2307/1942661 ICMJE | Home. (n.d.). International Committee of Medical Journal Editors. Retrieved 11 July 2021, from http://www.icmje.org/ Ikeda A., Xu H., Fuji N., Zhu S., \u0026amp; Yamada Y. (2019). Questionable research practices following pre-registration. 心理学評論刊行会. https://doi.org/10.24602/sjpr.62.3_281 Initial revision of ‘git’, the information manager from hell · git/git@e83c516. (n.d.). GitHub. Retrieved 9 July 2021, from https://github.com/git/git/commit/e83c5163316f89bfbde7d9ab23ca2e25604af290 International Committee of Medical Journal Editors. (n.d.). ICMJE | Recommendations | Author Responsibilities—Disclosure of Financial and Non-Financial Relationships and Activities, and Conflicts of Interest. ICJME. http://www.icmje.org/recommendations/browse/roles-and-responsibilities/author-responsibilities--conflicts-of-interest.html INVOLVE – INVOLVE Supporting public involvement in NHS, public health and social care research. (n.d.). Retrieved 9 July 2021, from https://www.invo.org.uk/ Ioannidis, J. P. A. (2005). Why Most Published Research Findings Are False. PLoS Medicine, 2(8), e124. https://doi.org/10.1371/journal.pmed.0020124 Ioannidis, J. P. A., Fanelli, D., Dunne, D. D., \u0026amp; Goodman, S. N. (2015). Meta-research: Evaluation and Improvement of Research Methods and Practices. PLOS Biology, 13(10), e1002264. https://doi.org/10.1371/journal.pbio.1002264 JabRef—Free Reference Manager—Stay on top of your Literature. (n.d.). JabRef. Retrieved 9 July 2021, from https://www.jabref.org/ Jacobson, D., \u0026amp; Mustafa, N. (2019). Social Identity Map: A Reflexivity Tool for Practicing Explicit Positionality in Critical Qualitative Research. International Journal of Qualitative Methods, 18, 160940691987007. https://doi.org/10.1177/1609406919870075 Jafar, A. J. N. (2018). What is positionality and should it be expressed in quantitative studies? Emergency Medicine Journal, emermed-2017-207158. https://doi.org/10.1136/emermed-2017-207158 James, K. L., Randall, N. P., \u0026amp; Haddaway, N. R. (2016). A methodology for systematic mapping in environmental sciences. Environmental Evidence, 5(1), 7. https://doi.org/10.1186/s13750-016-0059-6 Jamovi—Stats. Open. Now. (n.d.). Jamovi. Retrieved 9 July 2021, from https://www.jamovi.org/ Jannot, A.-S., Agoritsas, T., Gayet-Ageron, A., \u0026amp; Perneger, T. V. (2013). Citation bias favoring statistically significant studies was present in medical research. Journal of Clinical Epidemiology, 66(3), 296–301. https://doi.org/10.1016/j.jclinepi.2012.09.015 John, L. K., Loewenstein, G., \u0026amp; Prelec, D. (2012). Measuring the Prevalence of Questionable Research Practices With Incentives for Truth Telling. Psychological Science, 23(5), 524–532. https://doi.org/10.1177/0956797611430953 Jones, A., Worrall, S., Rudin, L., Duckworth, J. J., \u0026amp; Christiansen, P. (2021). May I have your attention, please? Methodological and analytical flexibility in the addiction stroop. Addiction Research \u0026amp; Theory, 1–14. https://doi.org/10.1080/16066359.2021.1876847 Joseph, T. D., \u0026amp; Hirshfield, L. E. (2011). ‘Why don’t you get somebody new to do it?’ Race and cultural taxation in the academy. Ethnic and Racial Studies, 34(1), 121–141. https://doi.org/10.1080/01419870.2010.496489 Kalliamvakou, E., Gousios, G., Blincoe, K., Singer, L., German, D. M., \u0026amp; Damian, D. (2014). The promises and perils of mining GitHub. Proceedings of the 11th Working Conference on Mining Software Repositories - MSR 2014, 92–101. https://doi.org/10.1145/2597073.2597074 kamraro. (2014, April 1). Responsible research \u0026amp; innovation [Text]. Horizon 2020 - European Commission. https://ec.europa.eu/programmes/horizon2020/en/h2020-section/responsible-research-innovation Kathawalla, U.-K., Silverstein, P., \u0026amp; Syed, M. (2021). Easing Into Open Science: A Guide for Graduate Students and Their Advisors. Collabra: Psychology, 7(1), 18684. https://doi.org/10.1525/collabra.18684 Kelley, T. (1927). Interpretation of educational measurements. World Book Co. Kerr, J. R., \u0026amp; Wilson, M. S. (2021). Right-wing authoritarianism and social dominance orientation predict rejection of science and scientists. Group Processes \u0026amp; Intergroup Relations, 24(4), 550–567. https://doi.org/10.1177/1368430221992126 Kerr, N. L. (1998). HARKing: Hypothesizing After the Results are Known. Personality and Social Psychology Review, 2(3), 196–217. https://doi.org/10.1207/s15327957pspr0203_4 Kerr, N. L., Ao, X., Hogg, M. A., \u0026amp; Zhang, J. (2018). Addressing replicability concerns via adversarial collaboration: Discovering hidden moderators of the minimal intergroup discrimination effect. Journal of Experimental Social Psychology, 78, 66–76. https://doi.org/10.1016/j.jesp.2018.05.001 Kidwell, M. C., Lazarević, L. B., Baranski, E., Hardwicke, T. E., Piechowski, S., Falkenberg, L.-S., Kennett, C., Slowik, A., Sonnleitner, C., Hess-Holden, C., Errington, T. M., Fiedler, S., \u0026amp; Nosek, B. A. (2016). Badges to Acknowledge Open Practices: A Simple, Low-Cost, Effective Method for Increasing Transparency. PLOS Biology, 14(5), e1002456. https://doi.org/10.1371/journal.pbio.1002456 Kienzler, H., \u0026amp; Fontanesi, C. (2017). Learning through inquiry: A Global Health Hackathon. Teaching in Higher Education, 22(2), 129–142. https://doi.org/10.1080/13562517.2016.1221805 Kiernan, C. (1999). Participation in Research by People with Learning Disability: Origins and Issues. British Journal of Learning Disabilities, 27(2), 43–47. https://doi.org/10.1111/j.1468-3156.1999.tb00084.x King, G. (1995). Replication, Replication. PS: Political Science and Politics, 28(3), 444. https://doi.org/10.2307/420301 Kitzes, J., Turek, D., \u0026amp; Deniz, F. (Eds.). (2018). The practice of reproducible research: Case studies and lessons from the data-intensive sciences. University of California Press. Kiureghian, A. D., \u0026amp; Ditlevsen, O. (2009). Aleatory or epistemic? Does it matter? Structural Safety, 31(2), 105–112. https://doi.org/10.1016/j.strusafe.2008.06.020 Klein, O., Hardwicke, T. E., Aust, F., Breuer, J., Danielsson, H., Mohr, A. H., IJzerman, H., Nilsonne, G., Vanpaemel, W., \u0026amp; Frank, M. C. (2018). A Practical Guide for Transparency in Psychological Science. Collabra: Psychology, 4(1), 20. https://doi.org/10.1525/collabra.158 Klein, R. A., Ratliff, K. A., Vianello, M., Adams, R. B., Bahník, Š., Bernstein, M. J., Bocian, K., Brandt, M. J., Brooks, B., Brumbaugh, C. C., Cemalcilar, Z., Chandler, J., Cheong, W., Davis, W. E., Devos, T., Eisner, M., Frankowska, N., Furrow, D., Galliani, E. M., … Nosek, B. A. (2014). Investigating Variation in Replicability: A “Many Labs” Replication Project. Social Psychology, 45(3), 142–152. https://doi.org/10.1027/1864-9335/a000178 Klein, R. A., Vianello, M., Hasselman, F., Adams, B. G., Adams, R. B., Alper, S., Aveyard, M., Axt, J. R., Babalola, M. T., Bahník, Š., Batra, R., Berkics, M., Bernstein, M. J., Berry, D. R., Bialobrzeska, O., Binan, E. D., Bocian, K., Brandt, M. J., Busching, R., … Nosek, B. A. (2018). Many Labs 2: Investigating Variation in Replicability Across Samples and Settings. Advances in Methods and Practices in Psychological Science, 1(4), 443–490. https://doi.org/10.1177/2515245918810225 Kleinberg, B., Mozes, M., van der Toolen, Y., \u0026amp; Verschuere, B. (2017). NETANOS - Named entity-based Text Anonymization for Open Science [Preprint]. Open Science Framework. https://doi.org/10.31219/osf.io/w9nhb Knoth, P., \u0026amp; Herrmannova, D. (n.d.). Towards Semantometrics: A New Semantic Similarity Based Measure for Assessing a Research Publication’s Contribution. D-Lib Magazine, 20(11/12), 8. https://doi.org/10.1045/november14-knoth Koole, S. L., \u0026amp; Lakens, D. (2012). Rewarding Replications: A Sure and Simple Way to Improve Psychological Science. Perspectives on Psychological Science, 7(6), 608–614. https://doi.org/10.1177/1745691612462586 Kreuter, F. (Ed.). (2013). Improving Surveys with Paradata: Analytic Uses of Process Information. John Wiley \u0026amp; Sons, Inc. https://doi.org/10.1002/9781118596869 Kruschke, J. K. (2015). Doing Bayesian data analysis: A tutorial with R, JAGS, and Stan (2nd ed.). Academic Press. Kuhn, T. S. (1996). The structure of scientific revolutions (3rd ed). University of Chicago Press. Kukull, W. A., \u0026amp; Ganguli, M. (2012). Generalizability: The trees, the forest, and the low-hanging fruit. Neurology, 78(23), 1886–1891. https://doi.org/10.1212/WNL.0b013e318258f812 L. Haven, T., \u0026amp; Van Grootel, Dr. L. (2019). Preregistering qualitative research. Accountability in Research, 26(3), 229–244. https://doi.org/10.1080/08989621.2019.1580147 Laakso, M., \u0026amp; Björk, B.-C. (2013). Delayed open access: An overlooked high-impact category of openly available scientific literature. Journal of the American Society for Information Science and Technology, 64(7), 1323–1329. https://doi.org/10.1002/asi.22856 Laine, H. (2017). Afraid of Scooping – Case Study on Researcher Strategies against Fear of Scooping in the Context of Open Science. Data Science Journal, 16, 29. https://doi.org/10.5334/dsj-2017-029 Lakatos, I. (1978). The Methodology of Scientiﬁc Research Programs: Vol. I. Cambridge University Press. Lakens, D. (2014). Performing high-powered studies efficiently with sequential analyses: Sequential analyses. European Journal of Social Psychology, 44(7), 701–710. https://doi.org/10.1002/ejsp.2023 Lakens, D. (2020a, May 11). The 20% Statistician: Red Team Challenge. The 20% Statistician. http://daniellakens.blogspot.com/2020/05/red-team-challenge.html Lakens, D. (2020b). Pandemic researchers—Recruit your own best critics. Nature, 581(7807), 121–121. https://doi.org/10.1038/d41586-020-01392-8 Lakens, D. (2021a). Sample Size Justification [Preprint]. PsyArXiv. https://doi.org/10.31234/osf.io/9d3yf Lakens, D. (2021b). The Practical Alternative to the p Value Is the Correctly Used p Value. Perspectives on Psychological Science, 16(3), 639–648. https://doi.org/10.1177/1745691620958012 Lakens, D., McLatchie, N., Isager, P. M., Scheel, A. M., \u0026amp; Dienes, Z. (2020). Improving Inferences About Null Effects With Bayes Factors and Equivalence Tests. The Journals of Gerontology: Series B, 75(1), 45–57. https://doi.org/10.1093/geronb/gby065 Lakens, D., Scheel, A. M., \u0026amp; Isager, P. M. (2018). Equivalence Testing for Psychological Research: A Tutorial. Advances in Methods and Practices in Psychological Science, 1(2), 259–269. https://doi.org/10.1177/2515245918770963 Largent, E. A., \u0026amp; Snodgrass, R. T. (2016). Blind Peer Review by Academic Journals. In Blinding as a Solution to Bias (pp. 75–95). Elsevier. https://doi.org/10.1016/B978-0-12-802460-7.00005-X Larivière, V., Desrochers, N., Macaluso, B., Mongeon, P., Paul-Hus, A., \u0026amp; Sugimoto, C. R. (2016). Contributorship and division of labor in knowledge production. Social Studies of Science, 46(3), 417–435. https://doi.org/10.1177/0306312716650046 Lazic, S. E. (2019, September 16). Genuine replication and pseudoreplication: What’s the difference? | BMJ Open Science. BMJ Open Science. https://blogs.bmj.com/openscience/2019/09/16/genuine-replication-and-pseudoreplication-whats-the-difference/ Leavens, D. A., Bard, K. A., \u0026amp; Hopkins, W. D. (2010). BIZARRE chimpanzees do not represent “the chimpanzee”. Behavioral and Brain Sciences, 33(2–3), 100–101. https://doi.org/10.1017/S0140525X10000166 Leavy, P. (2017). Research design: Quantitative, qualitative, mixed methods, arts-based, and community-based participatory research approaches. Guilford Press. LeBel, E. P., McCarthy, R. J., Earp, B. D., Elson, M., \u0026amp; Vanpaemel, W. (2018). A Unified Framework to Quantify the Credibility of Scientific Findings. Advances in Methods and Practices in Psychological Science, 1(3), 389–402. https://doi.org/10.1177/2515245918787489 LeBel, E. P., Vanpaemel, W., Cheung, I., \u0026amp; Campbell, L. (2019). A Brief Guide to Evaluate Replications. Meta-Psychology, 3. https://doi.org/10.15626/MP.2018.843 Ledgerwood, A., Hudson, S. T. J., Lewis, N. A., Maddox, K. B., Pickett, C., Remedios, J. D., Cheryan, S., Diekman, A., Dutra, N. B., Goh, J. X., Goodwin, S., Munakata, Y., Navarro, D., Onyeador, I. N., Srivastava, S., \u0026amp; Wilkins, C. L. (2021). The Pandemic as a Portal: Reimagining Psychological Science as Truly Open and Inclusive [Preprint]. PsyArXiv. https://doi.org/10.31234/osf.io/gdzue Lee, R. M. (1993). Doing research on sensitive topics. Sage Publications. Lewandowsky, S., \u0026amp; Bishop, D. (2016). Research integrity: Don’t let transparency damage science. Nature, 529(7587), 459–461. https://doi.org/10.1038/529459a Lewandowsky, S., \u0026amp; Oberauer, K. (2021). Worldview-motivated rejection of science and the norms of science. Cognition, 215, 104820. https://doi.org/10.1016/j.cognition.2021.104820 Licenses \u0026amp; Standards | Open Source Initiative. (n.d.). Open Source Initative. Retrieved 9 July 2021, from https://opensource.org/licenses Lin, D., Crabtree, J., Dillo, I., Downs, R. R., Edmunds, R., Giaretta, D., De Giusti, M., L’Hours, H., Hugo, W., Jenkyns, R., Khodiyar, V., Martone, M. E., Mokrane, M., Navale, V., Petters, J., Sierman, B., Sokolova, D. V., Stockhause, M., \u0026amp; Westbrook, J. (2020). The TRUST Principles for digital repositories. Scientific Data, 7(1), 144. https://doi.org/10.1038/s41597-020-0486-7 Lind, F., Gruber, M., \u0026amp; Boomgaarden, H. G. (2017). Content Analysis by the Crowd: Assessing the Usability of Crowdsourcing for Coding Latent Constructs. Communication Methods and Measures, 11(3), 191–209. https://doi.org/10.1080/19312458.2017.1317338 Lindsay, D. S. (2015). Replication in Psychological Science. Psychological Science, 26(12), 1827–1832. https://doi.org/10.1177/0956797615616374 Lindsay, D. S. (2020). Seven steps toward transparency and replicability in psychological science. Canadian Psychology/Psychologie Canadienne, 61(4), 310–317. https://doi.org/10.1037/cap0000222 Lintott, C. J., Schawinski, K., Slosar, A., Land, K., Bamford, S., Thomas, D., Raddick, M. J., Nichol, R. C., Szalay, A., Andreescu, D., Murray, P., \u0026amp; Vandenberg, J. (2008). Galaxy Zoo: Morphologies derived from visual inspection of galaxies from the Sloan Digital Sky Survey ★. Monthly Notices of the Royal Astronomical Society, 389(3), 1179–1189. https://doi.org/10.1111/j.1365-2966.2008.13689.x Liu, H., \u0026amp; Priest, S. (2009). Understanding public support for stem cell research: Media communication, interpersonal communication and trust in key actors. Public Understanding of Science, 18(6), 704–718. https://doi.org/10.1177/0963662508097625 Liu, Y., Gordon, M., Wang, J., Bishop, M., Chen, Y., Pfeiffer, T., Twardy, C., \u0026amp; Viganola, D. (2020). Replication Markets: Results, Lessons, Challenges and Opportunities in AI Replication. ArXiv:2005.04543 [Cs]. http://arxiv.org/abs/2005.04543 Longino, H. E. (1990). Science as social knowledge: Values and objectivity in scientific inquiry. Princeton University Press. Longino, H. E. (1992). Taking Gender Seriously in Philosophy of Science. PSA: Proceedings of the Biennial Meeting of the Philosophy of Science Association, 1992(2), 333–340. https://doi.org/10.1086/psaprocbienmeetp.1992.2.192847 Lu, J., Qiu, Y., \u0026amp; Deng, A. (2019). A note on Type S/M errors in hypothesis testing. British Journal of Mathematical and Statistical Psychology, 72(1), 1–17. https://doi.org/10.1111/bmsp.12132 Lüdtke, O., Ulitzsch, E., \u0026amp; Robitzsch, A. (2020). A Comparison of Penalized Maximum Likelihood Estimation and Markov Chain Monte Carlo Techniques for Estimating Confirmatory Factor Analysis Models with Small Sample Sizes [Preprint]. PsyArXiv. https://doi.org/10.31234/osf.io/u3qag Lutz, M. (2019). Programming Python (Fourth edition). O’Reilly. Lynch, Jr., J. G. (1982). On the External Validity of Experiments in Consumer Research. Journal of Consumer Research, 9(3), 225. https://doi.org/10.1086/208919 Macfarlane, B., \u0026amp; Cheng, M. (2008). Communism, Universalism and Disinterestedness: Re-examining Contemporary Support among Academics for Merton’s Scientific Norms. Journal of Academic Ethics, 6(1), 67–78. https://doi.org/10.1007/s10805-008-9055-y Makowski, D., Ben-Shachar, M. S., Chen, S. H. A., \u0026amp; Lüdecke, D. (2019). Indices of Effect Existence and Significance in the Bayesian Framework. Frontiers in Psychology, 10, 2767. https://doi.org/10.3389/fpsyg.2019.02767 Martinez-Acosta, V. G., \u0026amp; Favero, C. B. (2018). A Discussion of Diversity and Inclusivity at the Institutional Level: The Need for a Strategic Plan. Journal of Undergraduate Neuroscience Education: JUNE: A Publication of FUN, Faculty for Undergraduate Neuroscience, 16(3), A252–A260. Marwick, B., Boettiger, C., \u0026amp; Mullen, L. (2018). Packaging Data Analytical Work Reproducibly Using R (and Friends). The American Statistician, 72(1), 80–88. https://doi.org/10.1080/00031305.2017.1375986 Masur, P. K. (2020). Understanding the Effects of Analytical Choices on Finding the Privacy Paradox: A Specification Curve Analysis of Large-Scale Survey Data [Preprint]. Open Science Framework. https://osf.io/m72gb/ McElreath, R. (2020). Statistical rethinking: A Bayesian course with examples in R and Stan (2nd ed.). Taylor and Francis, CRC Press. McNutt, M. K., Bradford, M., Drazen, J. M., Hanson, B., Howard, B., Jamieson, K. H., Kiermer, V., Marcus, E., Pope, B. K., Schekman, R., Swaminathan, S., Stang, P. J., \u0026amp; Verma, I. M. (2018). Transparency in authors’ contributions and responsibilities to promote integrity in scientific publication. Proceedings of the National Academy of Sciences, 115(11), 2557–2560. https://doi.org/10.1073/pnas.1715374115 Medical Research Centre. (2019). Identifiability, anonymisation and pseudonymisation. Medical Research Centre. https://mrc.ukri.org/documents/pdf/gdpr-guidance-note-5-identifiability-anonymisation-and-pseudonymisation/ Medin, D. L. (2012, February 1). Rigor Without Rigor Mortis: The APS Board Discusses Research Integrity [Blog]. Association for Psychological Science. https://www.psychologicalscience.org/observer/scientific-rigor Melissa S. Anderson, Emily A. Ronning, Raymond De Vries, \u0026amp; Brian C. Martinson. (2010). Extending the Mertonian Norms: Scientists’ Subscription to Norms of Research. The Journal of Higher Education, 81(3), 366–393. https://doi.org/10.1353/jhe.0.0095 Mellers, B., Hertwig, R., \u0026amp; Kahneman, D. (2001). Do Frequency Representations Eliminate Conjunction Effects? An Exercise in Adversarial Collaboration. Psychological Science, 12(4), 269–275. https://doi.org/10.1111/1467-9280.00350 Menke, C. (2015). A Note on Science and Democracy? Robert K. Mertons Ethos of Science. In R. Klausnitzer, C. Spoerhase, \u0026amp; D. Werle (Eds.), Ethos und Pathos der Geisteswissenschaften. DE GRUYTER. https://doi.org/10.1515/9783110375008-013 Mertens, G., \u0026amp; Krypotos, A.-M. (2019). Preregistration of Analyses of Preexisting Data. Psychologica Belgica, 59(1), 338–352. https://doi.org/10.5334/pb.493 Merton, R. K. (1938). Science and the Social Order. Philosophy of Science, 5(3), 321–337. https://doi.org/10.1086/286513 Merton, R. K. (1968). The Matthew Effect in Science: The reward and communication systems of science are considered. Science, 159(3810), 56–63. https://doi.org/10.1126/science.159.3810.56 Meslin, E. M. (2008). Achieving global justice in health through global research ethics: Supplementing Macklin’s ‘top-down’ approach with one from the ‘ground up’. In R. M. Green, A. Donovan, \u0026amp; S. A. Jauss (Eds.), Global bioethics: Issues of conscience for the twenty-first century (pp. 163–177). Clarendon Press ; Oxford University Press. Michener, W. K. (2015). Ten Simple Rules for Creating a Good Data Management Plan. PLOS Computational Biology, 11(10), e1004525. https://doi.org/10.1371/journal.pcbi.1004525 Mischel, W. (2009, January 1). Becoming a Cumulative Science. Association for Psychological Science. https://www.psychologicalscience.org/observer/becoming-a-cumulative-science Moher, D., Bouter, L., Kleinert, S., Glasziou, P., Sham, M. H., Barbour, V., Coriat, A.-M., Foeger, N., \u0026amp; Dirnagl, U. (2020). The Hong Kong Principles for assessing researchers: Fostering research integrity. PLOS Biology, 18(7), e3000737. https://doi.org/10.1371/journal.pbio.3000737 Moher, D., Liberati, A., Tetzlaff, J., Altman, D. G., \u0026amp; The PRISMA Group. (2009). Preferred Reporting Items for Systematic Reviews and Meta-Analyses: The PRISMA Statement. PLoS Medicine, 6(7), e1000097. https://doi.org/10.1371/journal.pmed.1000097 Moher, D., Naudet, F., Cristea, I. A., Miedema, F., Ioannidis, J. P. A., \u0026amp; Goodman, S. N. (2018). Assessing scientists for hiring, promotion, and tenure. PLOS Biology, 16(3), e2004089. https://doi.org/10.1371/journal.pbio.2004089 Monroe, K. R. (2018). The Rush to Transparency: DA-RT and the Potential Dangers for Qualitative Research. Perspectives on Politics, 16(1), 141–148. https://doi.org/10.1017/S153759271700336X Morabia, A., Have, T. T., \u0026amp; Landis, J. R. (1997). Interaction Fallacy. Journal of Clinical Epidemiology, 50(7), 809–812. https://doi.org/10.1016/S0895-4356(97)00053-X Moran, H., Karlin, L., Lauchlan, E., Rappaport, S. J., Bleasdale, B., Wild, L., \u0026amp; Dorr, J. (2020). Understanding Research Culture: What researchers think about the culture they work in. Wellcome Open Research, 5, 201. https://doi.org/10.12688/wellcomeopenres.15832.1 Moretti, M. (2020, August 12). Beyond Open-washing: Are Narratives the Future of Open Data Portals? | by matteo moretti | Nightingale | Medium. Nightingale. https://medium.com/nightingale/beyond-open-washing-are-stories-and-narratives-the-future-of-open-data-portals-93228d8882f3 Morey, R. D., Chambers, C. D., Etchells, P. J., Harris, C. R., Hoekstra, R., Lakens, D., Lewandowsky, S., Morey, C. C., Newman, D. P., Schönbrodt, F. D., Vanpaemel, W., Wagenmakers, E.-J., \u0026amp; Zwaan, R. A. (2016). The Peer Reviewers’ Openness Initiative: Incentivizing open research practices through peer review. Royal Society Open Science, 3(1), 150547. https://doi.org/10.1098/rsos.150547 Morgan, C. (1998). The DOI (Digital Object Identifier). Serials: The Journal for the Serials Community, 11(1), 47–51. https://doi.org/10.1629/1147 Moshontz, H., Campbell, L., Ebersole, C. R., IJzerman, H., Urry, H. L., Forscher, P. S., Grahe, J. E., McCarthy, R. J., Musser, E. D., Antfolk, J., Castille, C. M., Evans, T. R., Fiedler, S., Flake, J. K., Forero, D. A., Janssen, S. M. J., Keene, J. R., Protzko, J., Aczel, B., … Chartier, C. R. (2018). The Psychological Science Accelerator: Advancing Psychology Through a Distributed Collaborative Network. Advances in Methods and Practices in Psychological Science, 1(4), 501–515. https://doi.org/10.1177/2515245918797607 Moshontz, H., Ebersole, C. R., Weston, S. J., \u0026amp; Klein, R. A. (2021). A guide for many authors: Writing manuscripts in large collaborations. Social and Personality Psychology Compass, 15(4). https://doi.org/10.1111/spc3.12590 Mourby, M., Mackey, E., Elliot, M., Gowans, H., Wallace, S. E., Bell, J., Smith, H., Aidinlis, S., \u0026amp; Kaye, J. (2018). Are ‘pseudonymised’ data always personal data? Implications of the GDPR for administrative data research in the UK. Computer Law \u0026amp; Security Review, 34(2), 222–233. https://doi.org/10.1016/j.clsr.2018.01.002 Muller, J. Z. (2018). The tyranny of metrics. Princeton University Press. Munn, Z., Peters, M. D. J., Stern, C., Tufanaru, C., McArthur, A., \u0026amp; Aromataris, E. (2018). Systematic review or scoping review? Guidance for authors when choosing between a systematic or scoping review approach. BMC Medical Research Methodology, 18(1), 143. https://doi.org/10.1186/s12874-018-0611-x Muthukrishna, M., Bell, A. V., Henrich, J., Curtin, C. M., Gedranovich, A., McInerney, J., \u0026amp; Thue, B. (2020). Beyond Western, Educated, Industrial, Rich, and Democratic (WEIRD) Psychology: Measuring and Mapping Scales of Cultural and Psychological Distance. Psychological Science, 31(6), 678–701. https://doi.org/10.1177/0956797620916782 Naudet, F., Ioannidis, J. P. A., Miedema, F., Cristea, I. A., Goodman, Steven N., J., \u0026amp; Moher, D. (2018, June 4). Six principles for assessing scientists for hiring, promotion, and tenure. Impact of Social Sciences Blog. http://eprints.lse.ac.uk/90753/ Navarro, D. (2020). Paths in strange spaces: A comment on preregistration [Preprint]. PsyArXiv. https://doi.org/10.31234/osf.io/wxn58 Nelson, L. D., Simmons, J. P., \u0026amp; Simonsohn, U. (2012). Let’s Publish Fewer Papers. Psychological Inquiry, 23(3), 291–293. https://doi.org/10.1080/1047840X.2012.705245 Neuroskeptic. (2012). The Nine Circles of Scientific Hell. Perspectives on Psychological Science, 7(6), 643–644. https://doi.org/10.1177/1745691612459519 Nichols, T. E., Das, S., Eickhoff, S. B., Evans, A. C., Glatard, T., Hanke, M., Kriegeskorte, N., Milham, M. P., Poldrack, R. A., Poline, J.-B., Proal, E., Thirion, B., Van Essen, D. C., White, T., \u0026amp; Yeo, B. T. T. (2017). Best practices in data analysis and sharing in neuroimaging using MRI. Nature Neuroscience, 20(3), 299–303. https://doi.org/10.1038/nn.4500 Nickerson, R. S. (1998). Confirmation Bias: A Ubiquitous Phenomenon in Many Guises. Review of General Psychology, 2(2), 175–220. https://doi.org/10.1037/1089-2680.2.2.175 Nieuwenhuis, S., Forstmann, B. U., \u0026amp; Wagenmakers, E.-J. (2011). Erroneous analyses of interactions in neuroscience: A problem of significance. Nature Neuroscience, 14(9), 1105–1107. https://doi.org/10.1038/nn.2886 Nimon, K. F. (2012). Statistical Assumptions of Substantive Analyses Across the General Linear Model: A Mini-Review. Frontiers in Psychology, 3. https://doi.org/10.3389/fpsyg.2012.00322 Nisbet, M. C., Scheufele, D. A., Shanahan, J., Moy, P., Brossard, D., \u0026amp; Lewenstein, B. V. (2002). Knowledge, Reservations, or Promise?: A Media Effects Model for Public Perceptions of Science and Technology. Communication Research, 29(5), 584–608. https://doi.org/10.1177/009365002236196 Nittrouer, C. L., Hebl, M. R., Ashburn-Nardo, L., Trump-Steele, R. C. E., Lane, D. M., \u0026amp; Valian, V. (2018). Gender disparities in colloquium speakers at top universities. Proceedings of the National Academy of Sciences, 115(1), 104–108. https://doi.org/10.1073/pnas.1708414115 Nosek, B. A. (2019, June 11). Strategy for Culture Change. Center for Open Science. https://www.cos.io/blog/strategy-for-culture-change Nosek, B. A., \u0026amp; Bar-Anan, Y. (2012). Scientific Utopia: I. Opening Scientific Communication. Psychological Inquiry, 23(3), 217–243. https://doi.org/10.1080/1047840X.2012.692215 Nosek, B. A., Ebersole, C. R., DeHaven, A. C., \u0026amp; Mellor, D. T. (2018). The preregistration revolution. Proceedings of the National Academy of Sciences, 115(11), 2600–2606. https://doi.org/10.1073/pnas.1708274114 Nosek, B. A., \u0026amp; Errington, T. M. (2020). What is replication? PLOS Biology, 18(3), e3000691. https://doi.org/10.1371/journal.pbio.3000691 Nosek, B. A., \u0026amp; Lakens, D. (2014). Registered Reports: A Method to Increase the Credibility of Published Results. Social Psychology, 45(3), 137–141. https://doi.org/10.1027/1864-9335/a000192 Nosek, B. A., Spies, J. R., \u0026amp; Motyl, M. (2012). Scientific Utopia: II. Restructuring Incentives and Practices to Promote Truth Over Publishability. Perspectives on Psychological Science, 7(6), 615–631. https://doi.org/10.1177/1745691612459058 Noy, N. F., \u0026amp; Guinness, D. L. (2001). Ontology Development 101: A Guide to Creating Your First Ontology. Stanford Knowledge Systems Laboratory Technical Report\u0026nbsp; KSL-01-05 and Stanford Medical Informatics Technical Report SMI-2001-0880. https://protege.stanford.edu/publications/ontology_development/ontology101.pdf Nuijten, M. B., Hartgerink, C. H. J., van Assen, M. A. L. M., Epskamp, S., \u0026amp; Wicherts, J. M. (2016). The prevalence of statistical reporting errors in psychology (1985–2013). Behavior Research Methods, 48(4), 1205–1226. https://doi.org/10.3758/s13428-015-0664-2 Nüst, D., Boettiger, C., \u0026amp; Marwick, B. (2018). How to Read a Research Compendium. ArXiv:1806.09525 [Cs]. http://arxiv.org/abs/1806.09525 Obels, P., Lakens, D., Coles, N. A., Gottfried, J., \u0026amp; Green, S. A. (2020). Analysis of Open Data and Computational Reproducibility in Registered Reports in Psychology. Advances in Methods and Practices in Psychological Science, 3(2), 229–237. https://doi.org/10.1177/2515245920918872 Oberauer, K., \u0026amp; Lewandowsky, S. (2019). Addressing the theory crisis in psychology. Psychonomic Bulletin \u0026amp; Review, 26(5), 1596–1618. https://doi.org/10.3758/s13423-019-01645-2 OER Commons. (n.d.). OER Commons. Retrieved 9 July 2021, from https://www.oercommons.org/ Open Aire. (n.d.). Amnesia Anonymization Tool—Data anonymization made easy. High Accuracy Data Anonymisation. Retrieved 9 July 2021, from https://amnesia.openaire.eu/ Open Educational Resources (OER). (2017, July 20). UNESCO. https://en.unesco.org/themes/building-knowledge-societies/oer Open Scholarship Knowledge Base | OER Commons. (n.d.). OER Commons. Retrieved 9 July 2021, from https://www.oercommons.org/hubs/OSKB Open Science Collaboration. (2015). Estimating the reproducibility of psychological science. Science, 349(6251), aac4716–aac4716. https://doi.org/10.1126/science.aac4716 Open Source in Open Science | FOSTER. (n.d.). Foster. Retrieved 9 July 2021, from https://www.fosteropenscience.eu/foster-taxonomy/open-source-open-science Orben, A. (2019). A journal club to fix science. Nature, 573(7775), 465–465. https://doi.org/10.1038/d41586-019-02842-8 ORCID. (n.d.). ORCID. Retrieved 9 July 2021, from https://orcid.org/ OSF. (n.d.). Open Science Framework. Retrieved 9 July 2021, from https://osf.io/ OSF | StudySwap: A platform for interlab replication, collaboration, and research resource exchange. (n.d.). Retrieved 10 July 2021, from https://osf.io/meetings/StudySwap/ Ottmann, G., Laragy, C., Allen, J., \u0026amp; Feldman, P. (2011). Coproduction in Practice: Participatory Action Research to Develop a Model of Community Aged Care. Systemic Practice and Action Research, 24(5), 413–427. https://doi.org/10.1007/s11213-011-9192-x Our Approach | Co-Production Collective. (n.d.). Co-Production Collective. Retrieved 9 July 2021, from https://www.coproductioncollective.co.uk/what-is-co-production/our-approach Padilla, A. M. (1994). Research news and Comment: Ethnic Minority Scholars; Research, and Mentoring: Current and Future Issues. Educational Researcher, 23(4), 24–27. https://doi.org/10.3102/0013189X023004024 Page, M. J., Moher, D., Bossuyt, P. M., Boutron, I., Hoffmann, T. C., Mulrow, C. D., Shamseer, L., Tetzlaff, J. M., Akl, E. A., Brennan, S. E., Chou, R., Glanville, J., Grimshaw, J. M., Hróbjartsson, A., Lalu, M. M., Li, T., Loder, E. W., Mayo-Wilson, E., McDonald, S., … McKenzie, J. E. (2021). PRISMA 2020 explanation and elaboration: Updated guidance and exemplars for reporting systematic reviews. BMJ, n160. https://doi.org/10.1136/bmj.n160 Patience, G. S., Galli, F., Patience, P. A., \u0026amp; Boffito, D. C. (2019). Intellectual contributions meriting authorship: Survey results from the top cited authors across all science categories. PLOS ONE, 14(1), e0198117. https://doi.org/10.1371/journal.pone.0198117 Pautasso, M. (2013). Ten Simple Rules for Writing a Literature Review. PLoS Computational Biology, 9(7), e1003149. https://doi.org/10.1371/journal.pcbi.1003149 Pavlov, Y. G., Adamian, N., Appelhoff, S., Arvaneh, M., Benwell, C., Beste, C., Bland, A., Bradford, D. E., Bublatzky, F., Busch, N., Clayson, P. E., Cruse, D., Czeszumski, A., Dreber, A., Dumas, G., Ehinger, B. V., Ganis, G., He, X., Hinojosa, J. A., … Mushtaq, F. (2020). #EEGManyLabs: Investigating the Replicability of Influential EEG Experiments [Preprint]. PsyArXiv. https://doi.org/10.31234/osf.io/528nr PCI Registered Reports. (n.d.). PCI. Retrieved 9 July 2021, from https://rr.peercommunityin.org/about/about Peer Community In – A free recommendation process of scientific preprints based on peer-reviews. (n.d.). Retrieved 9 July 2021, from https://peercommunityin.org/ Peer, E., Brandimarte, L., Samat, S., \u0026amp; Acquisti, A. (2017). Beyond the Turk: Alternative platforms for crowdsourcing behavioral research. Journal of Experimental Social Psychology, 70, 153–163. https://doi.org/10.1016/j.jesp.2017.01.006 Peng, R. D. (2011). Reproducible Research in Computational Science. Science, 334(6060), 1226–1227. https://doi.org/10.1126/science.1213847 Percie du Sert, N., Hurst, V., Ahluwalia, A., Alam, S., Avey, M. T., Baker, M., Browne, W. J., Clark, A., Cuthill, I. C., Dirnagl, U., Emerson, M., Garner, P., Holgate, S. T., Howells, D. W., Karp, N. A., Lazic, S. E., Lidster, K., MacCallum, C. J., Macleod, M., … Würbel, H. (2020). The ARRIVE guidelines 2.0: Updated guidelines for reporting animal research. PLOS Biology, 18(7), e3000410. https://doi.org/10.1371/journal.pbio.3000410 Pernet, C. (2016). Null hypothesis significance testing: A short tutorial. F1000Research, 4, 621. https://doi.org/10.12688/f1000research.6963.3 Pernet, C., Garrido, M. I., Gramfort, A., Maurits, N., Michel, C. M., Pang, E., Salmelin, R., Schoffelen, J. M., Valdes-Sosa, P. A., \u0026amp; Puce, A. (2020). Issues and recommendations from the OHBM COBIDAS MEEG committee for reproducible EEG and MEG research. Nature Neuroscience, 23(12), 1473–1483. https://doi.org/10.1038/s41593-020-00709-0 Pernet, C. R., Appelhoff, S., Gorgolewski, K. J., Flandin, G., Phillips, C., Delorme, A., \u0026amp; Oostenveld, R. (2019). EEG-BIDS, an extension to the brain imaging data structure for electroencephalography. Scientific Data, 6(1), 103. https://doi.org/10.1038/s41597-019-0104-8 Peterson, D., \u0026amp; Panofsky, A. (2020). Metascience as a scientific social movement [Preprint]. SocArXiv. https://doi.org/10.31235/osf.io/4dsqa Petre, M., \u0026amp; Wilson, G. (2014). Code Review For and By Scientists. ArXiv:1407.5648 [Cs]. http://arxiv.org/abs/1407.5648 ‘Plan S’ and ‘cOAlition S’ – Accelerating the transition to full and immediate Open Access to scientific publications. (n.d.). Retrieved 9 July 2021, from https://www.coalition-s.org/ Poldrack, R. A., Barch, D. M., Mitchell, J. P., Wager, T. D., Wagner, A. D., Devlin, J. T., Cumba, C., Koyejo, O., \u0026amp; Milham, M. P. (2013). Toward open sharing of task-based fMRI data: The OpenfMRI project. Frontiers in Neuroinformatics, 7. https://doi.org/10.3389/fninf.2013.00012 Poldrack, R. A., \u0026amp; Gorgolewski, K. J. (2014). Making big data open: Data sharing in neuroimaging. Nature Neuroscience, 17(11), 1510–1517. https://doi.org/10.1038/nn.3818 Pollet, I. L., \u0026amp; Bond, A. L. (2021). Evaluation and recommendations for greater accessibility of colour figures in ornithology. Ibis, 163(1), 292–295. https://doi.org/10.1111/ibi.12887 Popper, K. (2010). The logic of scientific discovery (Special Indian Edition). Routledge. Posselt, J. R. (2020). Equity in science: Representation, culture, and the dynamics of change in graduate education. Stanford University Press. Pownall, M., Talbot, C. V., Henschel, A., Lautarescu, A., Lloyd, K., Hartmann, H., Darda, K. M., Tang, K. T. Y., Carmichael-Murphy, P., \u0026amp; Siegel, J. A. (2020). Navigating Open Science as Early Career Feminist Researchers [Preprint]. PsyArXiv. https://doi.org/10.31234/osf.io/f9m47 Preregistration pledge. (n.d.). Google Docs. Retrieved 9 July 2021, from https://docs.google.com/forms/d/e/1FAIpQLSf8RflGizFJZamE874o8aDOhyU7UsNByR4dLmzhOtEOiu8KRQ/viewform?embedded=true\u0026amp;usp=embed_facebook Press, W. (2007). Numerical recipes: The art of scientific computing, (3rd ed.). Cambridge University Press. Psychological Science Accelerator. (n.d.). Psychological Science Accelerator. Retrieved 9 July 2021, from https://psysciacc.org/ Publication bias. (2019, May 2). Catalog of Bias. https://catalogofbias.org/biases/publication-bias/ PubPeer—Search publications and join the conversation. (n.d.). Pubpeer. Retrieved 9 July 2021, from https://www.pubpeer.com/ R: The R Project for Statistical Computing. (n.d.). R Project. Retrieved 10 July 2021, from https://www.r-project.org/ Rabagliati, H., Moors, P., \u0026amp; Heyman, T. (2020). Can Item Effects Explain Away the Evidence for Unconscious Sound Symbolism? An Adversarial Commentary on Heyman, Maerten, Vankrunkelsven, Voorspoels, and Moors (2019). Psychological Science, 31(9), 1200–1204. https://doi.org/10.1177/0956797620949461 Rakow, T., Thompson, V., Ball, L., \u0026amp; Markovits, H. (2015). Rationale and guidelines for empirical adversarial collaboration: A Thinking \u0026amp; Reasoning initiative. Thinking \u0026amp; Reasoning, 21(2), 167–175. https://doi.org/10.1080/13546783.2015.975405 Recommended Data Repositories | Scientific Data. (n.d.). Retrieved 10 July 2021, from https://www.nature.com/sdata/policies/repositories Replication Markets – Reliable research replicates…you can bet on it. (n.d.). Retrieved 10 July 2021, from https://www.replicationmarkets.com/ ReproducibiliTea. (n.d.). ReproducibiliTea. Retrieved 10 July 2021, from https://reproducibilitea.org/ Retraction Watch. (n.d.). Retraction Watch. Retrieved 9 July 2021, from https://retractionwatch.com/ RIOT Science Club—Riot Science Club. (n.d.). Reproducible, Interpretable, Open, \u0026amp; Transparent Science. Retrieved 10 July 2021, from http://riotscience.co.uk/ Rogers, A., Castree, N., \u0026amp; Kitchin, R. (2013). Reflexivity. In A Dictionary of Human Geography. Oxford University Press. https://www.oxfordreference.com/view/10.1093/acref/9780199599868.001.0001/acref-9780199599868-e-1530 Rolls, L., \u0026amp; Relf, M. (2006). Bracketing interviews: Addressing methodological challenges in qualitative interviewing in bereavement and palliative care. Mortality, 11(3), 286–305. https://doi.org/10.1080/13576270600774893 Rose, D. (2000). Universal Design for Learning. Journal of Special Education Technology, 15(3), 45–49. https://doi.org/10.1177/016264340001500307 Rose, D. (2018). Participatory research: Real or imagined. Social Psychiatry and Psychiatric Epidemiology, 53(8), 765–771. https://doi.org/10.1007/s00127-018-1549-3 Rose, D. H., \u0026amp; Meyer, A. (2002). Teaching every student in the Digital Age: Universal design for learning. Association for Supervision and Curriculum Development. Ross-Hellauer, T. (2017). What is open peer review? A systematic review. F1000Research, 6, 588. https://doi.org/10.12688/f1000research.11369.2 Rossner, M., Van Epps, H., \u0026amp; Hill, E. (2007). Show me the data. Journal of Cell Biology, 179(6), 1091–1092. https://doi.org/10.1083/jcb.200711140 Rothstein, H. R., Sutton, A. J., \u0026amp; Borenstein, M. (2006). Publication Bias in Meta-Analysis. In H. R. Rothstein, A. J. Sutton, \u0026amp; M. Borenstein (Eds.), Publication Bias in Meta-Analysis (pp. 1–7). John Wiley \u0026amp; Sons, Ltd. https://doi.org/10.1002/0470870168.ch1 Rowhani-Farid, A., Aldcroft, A., \u0026amp; Barnett, A. G. (2020). Did awarding badges increase data sharing in BMJ Open ? A randomized controlled trial. Royal Society Open Science, 7(3), 191818. https://doi.org/10.1098/rsos.191818 Rubin, M. (2021). Explaining the association between subjective social status and mental health among university students using an impact ratings approach. SN Social Sciences, 1(1), 20. https://doi.org/10.1007/s43545-020-00031-3 Rubin, M., Evans, O., \u0026amp; McGuffog, R. (2019). Social Class Differences in Social Integration at University: Implications for Academic Outcomes and Mental Health. In J. Jetten \u0026amp; K. Peters (Eds.), The Social Psychology of Inequality (pp. 87–102). Springer International Publishing. https://doi.org/10.1007/978-3-030-28856-3_6 Sagarin, B. J., Ambler, J. K., \u0026amp; Lee, E. M. (2014). An Ethical Approach to Peeking at Data. Perspectives on Psychological Science, 9(3), 293–304. https://doi.org/10.1177/1745691614528214 Salem, D. N., \u0026amp; Boumil, M. M. (2013). Conflict of Interest in Open-Access Publishing. New England Journal of Medicine, 369(5), 491–491. https://doi.org/10.1056/NEJMc1307577 Sato, T. (1996). Type I and Type II Error in Multiple Comparisons. The Journal of Psychology, 130(3), 293–302. https://doi.org/10.1080/00223980.1996.9915010 Schafersman, S. (1997, January). An Introduction to Science: Scientific Thinking and Scientific Method. An Introduction to Science. https://www.geo.sunysb.edu/esp/files/scientific-method.html Schmidt, Robert. H. (1987). A Worksheet for Authorship of Scientific Articles on JSTOR. Bulletin of the Ecological Society of America, 68(1), 8–10. https://www.jstor.org/stable/20166549 Schneider, J., Merk, S., \u0026amp; Rosman, T. (2020). (Re)Building Trust? Investigating the effects of open science badges on perceived trustworthiness in journal articles. https://doi.org/10.17605/OSF.IO/VGBRS Schönbrodt, F. (2019). Training students for the Open Science future. Nature Human Behaviour, 3(10), 1031–1031. https://doi.org/10.1038/s41562-019-0726-z Schönbrodt, F. D., Wagenmakers, E.-J., Zehetleitner, M., \u0026amp; Perugini, M. (2017). Sequential hypothesis testing with Bayes factors: Efficiently testing mean differences. Psychological Methods, 22(2), 322–339. https://doi.org/10.1037/met0000061 Schulz, K. F., \u0026amp; Grimes, D. A. (2005). Multiplicity in randomised trials I: Endpoints and treatments. The Lancet, 365(9470), 1591–1595. https://doi.org/10.1016/S0140-6736(05)66461-6 Schwarz, N., \u0026amp; Strack, F. (n.d.). Does merely going through the same moves make for a “direct” replication? Concepts, contexts, and operationalizations. Social Psychology, 45(4), 305–306. Science. (n.d.). Open Science Badges. Centre for Open Science. https://www.cos.io/initiatives/badges Scopatz, A., \u0026amp; Huff, K. D. (2015). Effective computation in physics (First Edition). O’Reilly Media. Shadish, W. R., Cook, T. D., \u0026amp; Campbell, D. T. (2001). Experimental and quasi-experimental designs for generalized causal inference. Houghton Mifflin. Sharma, M., Sarin, A., Gupta, P., Sachdeva, S., \u0026amp; Desai, A. (2014). Journal Impact Factor: Its Use, Significance and Limitations. World Journal of Nuclear Medicine, 13(2), 146. https://doi.org/10.4103/1450-1147.139151 Shepard, B. (2015). Community practice as social activism: From direct action to direct services. SAGE Publications, Inc. Siddaway, A. P., Wood, A. M., \u0026amp; Hedges, L. V. (2019). How to Do a Systematic Review: A Best Practice Guide for Conducting and Reporting Narrative Reviews, Meta-Analyses, and Meta-Syntheses. Annual Review of Psychology, 70(1), 747–770. https://doi.org/10.1146/annurev-psych-010418-102803 Sijtsma, K. (2016). Playing with Data—Or How to Discourage Questionable Research Practices and Stimulate Researchers to Do Things Right. Psychometrika, 81(1), 1–15. https://doi.org/10.1007/s11336-015-9446-0 Silberzahn, R., Simonsohn, U., \u0026amp; Uhlmann, E. L. (2014). Matched-Names Analysis Reveals No Evidence of Name-Meaning Effects: A Collaborative Commentary on Silberzahn and Uhlmann (2013). Psychological Science, 25(7), 1504–1505. https://doi.org/10.1177/0956797614533802 Silberzahn, R., Uhlmann, E. L., Martin, D. P., Anselmi, P., Aust, F., Awtrey, E., Bahník, Š., Bai, F., Bannard, C., Bonnier, E., Carlsson, R., Cheung, F., Christensen, G., Clay, R., Craig, M. A., Dalla Rosa, A., Dam, L., Evans, M. H., Flores Cervantes, I., … Nosek, B. A. (2018). Many Analysts, One Data Set: Making Transparent How Variations in Analytic Choices Affect Results. Advances in Methods and Practices in Psychological Science, 1(3), 337–356. https://doi.org/10.1177/2515245917747646 Simmons, J., Nelson, L., \u0026amp; Simonsohn, U. (2021). Pre‐registration: Why and How. Journal of Consumer Psychology, 31(1), 151–162. https://doi.org/10.1002/jcpy.1208 Simmons, J. P., Nelson, L. D., \u0026amp; Simonsohn, U. (2011). False-Positive Psychology: Undisclosed Flexibility in Data Collection and Analysis Allows Presenting Anything as Significant. Psychological Science, 22(11), 1359–1366. https://doi.org/10.1177/0956797611417632 Simons, D. J., Shoda, Y., \u0026amp; Lindsay, D. S. (2017). Constraints on Generality (COG): A Proposed Addition to All Empirical Papers. Perspectives on Psychological Science, 12(6), 1123–1128. https://doi.org/10.1177/1745691617708630 Simonsohn, U., Nelson, L. D., \u0026amp; Simmons, J. P. (2014a). P-curve: A key to the file-drawer. Journal of Experimental Psychology: General, 143(2), 534–547. https://doi.org/10.1037/a0033242 Simonsohn, U., Nelson, L. D., \u0026amp; Simmons, J. P. (2014b). p -Curve and Effect Size: Correcting for Publication Bias Using Only Significant Results. Perspectives on Psychological Science, 9(6), 666–681. https://doi.org/10.1177/1745691614553988 Simonsohn, U., Nelson, L. D., \u0026amp; Simmons, J. P. (2019). P-curve won’t do your laundry, but it will distinguish replicable from non-replicable findings in observational research: Comment on Bruns \u0026amp; Ioannidis (2016). PLOS ONE, 14(3), e0213454. https://doi.org/10.1371/journal.pone.0213454 Simonsohn, U., Simmons, J. P., \u0026amp; Nelson, L. D. (2015). Specification Curve: Descriptive and Inferential Statistics on All Reasonable Specifications. SSRN Electronic Journal. https://doi.org/10.2139/ssrn.2694998 Simonsohn, U., Simmons, J. P., \u0026amp; Nelson, L. D. (2020). Specification curve analysis. Nature Human Behaviour, 4(11), 1208–1214. https://doi.org/10.1038/s41562-020-0912-z Smaldino, P. E., \u0026amp; McElreath, R. (2016). The natural selection of bad science. Royal Society Open Science, 3(9), 160384. https://doi.org/10.1098/rsos.160384 Smith, A. C., Merz, L., Borden, J. B., Gulick, C., Kshirsagar, A. R., \u0026amp; Bruna, E. M. (2020). Assessing the effect of article processing charges on the geographic diversity of authors using Elsevier’s ‘Mirror Journal’ system [Preprint]. MetaArXiv. https://doi.org/10.31222/osf.io/s7cx4 Smith, A. J., Clutton, R. E., Lilley, E., Hansen, K. E. A., \u0026amp; Brattelid, T. (2018). PREPARE: Guidelines for planning animal research and testing. Laboratory Animals, 52(2), 135–141. https://doi.org/10.1177/0023677217724823 Smith, G. T. (2005). On Construct Validity: Issues of Method and Measurement. Psychological Assessment, 17(4), 396–408. https://doi.org/10.1037/1040-3590.17.4.396 Sorsa, M. A., Kiikkala, I., \u0026amp; Åstedt-Kurki, P. (2015). Bracketing as a skill in conducting unstructured qualitative interviews. Nurse Researcher, 22(4), 8–12. https://doi.org/10.7748/nr.22.4.8.e1317 SORTEE. (n.d.). SORTEE. SORTEE. Retrieved 10 July 2021, from https://www.sortee.org/ Spence, J. R., \u0026amp; Stanley, D. J. (2018). Concise, Simple, and Not Wrong: In Search of a Short-Hand Interpretation of Statistical Significance. Frontiers in Psychology, 9, 2185. https://doi.org/10.3389/fpsyg.2018.02185 Spencer, E. A., \u0026amp; Heneghan, C. (2018, April 2). Confirmation bias. Catalog of Bias. https://catalogofbias.org/biases/confirmation-bias/ Steckler, A., \u0026amp; McLeroy, K. R. (2008). The Importance of External Validity. American Journal of Public Health, 98(1), 9–10. https://doi.org/10.2105/AJPH.2007.126847 Steegen, S., Tuerlinckx, F., Gelman, A., \u0026amp; Vanpaemel, W. (2016). Increasing Transparency Through a Multiverse Analysis. Perspectives on Psychological Science, 11(5), 702–712. https://doi.org/10.1177/1745691616658637 Steup, M., \u0026amp; Neta, R. (2020). Epistemology. In E. N. Zalta (Ed.), The Stanford Encyclopedia of Philosophy (Fall 2020). Metaphysics Research Lab, Stanford University. https://plato.stanford.edu/archives/fall2020/entries/epistemology/ Stewart, N., Chandler, J., \u0026amp; Paolacci, G. (2017). Crowdsourcing Samples in Cognitive Science. Trends in Cognitive Sciences, 21(10), 736–748. https://doi.org/10.1016/j.tics.2017.06.007 Stodden, V. C. (2011). Trust Your Science? Open Your Data and Code. https://doi.org/10.7916/D8CJ8Q0P Strathern, M. (1997). ‘Improving ratings’: Audit in the British University system. European Review, 5(3), 305–321. https://doi.org/10.1002/(SICI)1234-981X(199707)5:3\u0026lt;305::AID-EURO184\u0026gt;3.0.CO;2-4 Suber, P. (2004, February 4). It’s the authors, stupid! SPARC Open Access Newsletter. https://dash.harvard.edu/bitstream/handle/1/4391161/suber_authors.htm?sequence=1\u0026amp;isAllowed=y SwissRN. (n.d.). Retrieved 10 July 2021, from http://www.swissrn.org/ Syed, M. (2019). The Open Science Movement is For All of Us [Preprint]. PsyArXiv. https://doi.org/10.31234/osf.io/cteyb Syed, M., \u0026amp; Kathawalla, U.-K. (2020). Cultural Psychology, Diversity, and Representation in Open Science [Preprint]. PsyArXiv. https://doi.org/10.31234/osf.io/t7hp2 Szollosi, A., \u0026amp; Donkin, C. (2021). Arrested Theory Development: The Misguided Distinction Between Exploratory and Confirmatory Research. Perspectives on Psychological Science, 174569162096679. https://doi.org/10.1177/1745691620966796 Team, psyTeachR. (n.d.). P | Glossary. Retrieved 9 July 2021, from https://psyteachr.github.io/glossary Tennant, J., Beamer, J. E., Bosman, J., Brembs, B., Chung, N. C., Clement, G., Crick, T., Dugan, J., Dunning, A., Eccles, D., Enkhbayar, A., Graziotin, D., Harding, R., Havemann, J., Katz, D. S., Khanal, K., Kjaer, J. N., Koder, T., Macklin, P., … Turner, A. (2019). Foundations for Open Scholarship Strategy Development [Preprint]. MetaArXiv. https://doi.org/10.31222/osf.io/b4v8p Tennant, J., Bielczyk, N. Z., Greshake Tzovaras, B., Masuzzo, P., \u0026amp; Steiner, T. (2019). Introducing Massively Open Online Papers (MOOPs) [Preprint]. MetaArXiv. https://doi.org/10.31222/osf.io/et8ak Tenny, S., \u0026amp; Abdelgawad, I. (2021). Statistical Significance. In StatPearls [Internet]. StatPearls Publishing. https://www.ncbi.nlm.nih.gov/books/NBK459346/ The Committee on Publication Ethics. (n.d.). Transparency \u0026amp; best practice – DOAJ. DOAJ. https://doaj.org/apply/transparency/ the CONSORT Group, Schulz, K. F., Altman, D. G., \u0026amp; Moher, D. (2010). CONSORT 2010 Statement: Updated guidelines for reporting parallel group randomised trials. Trials, 11(1), 32. https://doi.org/10.1186/1745-6215-11-32 The European Code of Conduct for Research Integrity | ALLEA. (n.d.). Retrieved 10 July 2021, from https://allea.org/code-of-conduct/ The Open Definition—Open Definition—Defining Open in Open Data, Open Content and Open Knowledge. (n.d.). Open Knowledge Foundation. Retrieved 9 July 2021, from https://opendefinition.org/ The Open Source Definition | Open Source Initiative. (n.d.). Open Source Initative. Retrieved 9 July 2021, from https://opensource.org/osd The Slow Science Academy. (2010). The Slow Science Manifesto. SLOW-SCIENCE.Org — Bear with Us, While We Think. http://slow-science.org/ Thombs, B. D., Levis, A. W., Razykov, I., Syamchandra, A., Leentjens, A. F. G., Levenson, J. L., \u0026amp; Lumley, M. A. (2015). Potentially coercive self-citation by peer reviewers: A cross-sectional study. Journal of Psychosomatic Research, 78(1), 1–6. https://doi.org/10.1016/j.jpsychores.2014.09.015 Tierney, W., Hardy, J., Ebersole, C. R., Viganola, D., Clemente, E. G., Gordon, M., Hoogeveen, S., Haaf, J., Dreber, A., Johannesson, M., Pfeiffer, T., Huang, J. L., Vaughn, L. A., DeMarree, K., Igou, E. R., Chapman, H., Gantman, A., Vanaman, M., Wylie, J., … Uhlmann, E. L. (2021). A creative destruction approach to replication: Implicit work and sex morality across cultures. Journal of Experimental Social Psychology, 93, 104060. https://doi.org/10.1016/j.jesp.2020.104060 Tierney, W., Hardy, J. H., Ebersole, C. R., Leavitt, K., Viganola, D., Clemente, E. G., Gordon, M., Dreber, A., Johannesson, M., Pfeiffer, T., \u0026amp; Uhlmann, E. L. (2020). Creative destruction in science. Organizational Behavior and Human Decision Processes, 161, 291–309. https://doi.org/10.1016/j.obhdp.2020.07.002 Tiokhin, L., Yan, M., \u0026amp; Morgan, T. J. H. (2021). Competition for priority harms the reliability of science, but reforms can help. Nature Human Behaviour. https://doi.org/10.1038/s41562-020-01040-1 Topor, M., Pickering, J. S., Barbosa Mendes, A., Bishop, D. V. M., Büttner, F. C., Elsherif, M. M., Evans, T. R., Henderson, E. L., Kalandadze, T., Nitschke, F. T., Staaks, J., Van den Akker, O., Yeung, S. K., Zaneva, M., Lam, A., Madan, C. R., Moreau, D., O’Mahony, A., Parker, A. J., … Westwood, S. J. (2020). An integrative framework for planning and conducting Non-Intervention, Reproducible, and Open Systematic Reviews (NIRO-SR) [Preprint]. MetaArXiv. https://doi.org/10.31222/osf.io/8gu5z Transparency: The Emerging Third Dimension of Open Science and Open Data. (2016). LIBER QUARTERLY, 25(4), 153–171. https://doi.org/10.18352/lq.10113 Tscharntke, T., Hochberg, M. E., Rand, T. A., Resh, V. H., \u0026amp; Krauss, J. (2007). Author Sequence and Credit for Contributions in Multiauthored Publications. PLoS Biology, 5(1), e18. https://doi.org/10.1371/journal.pbio.0050018 Tufte, E. R. (2001). The visual display of quantitative information (2nd ed). Graphics Press. Tukey, J. W. (1977). Exploratory data analysis. Addison-Wesley Pub. Co. Tvina, A., Spellecy, R., \u0026amp; Palatnik, A. (2019). Bias in the Peer Review Process: Can We Do Better? Obstetrics \u0026amp; Gynecology, 133(6), 1081–1083. https://doi.org/10.1097/AOG.0000000000003260 Uhlmann, E. L., Ebersole, C. R., Chartier, C. R., Errington, T. M., Kidwell, M. C., Lai, C. K., McCarthy, R. J., Riegelman, A., Silberzahn, R., \u0026amp; Nosek, B. A. (2019). Scientific Utopia III: Crowdsourcing Science. Perspectives on Psychological Science, 14(5), 711–733. https://doi.org/10.1177/1745691619850561 UK Reproducibility Network. (n.d.). UK Reproducibility Network. Retrieved 10 July 2021, from https://www.ukrn.org/ University of Illinois at Urbana-Champaign, Burnette, M., Williams, S., University of Illinois at Urbana-Champaign, Imker, H., \u0026amp; University of Illinois at Urbana-Champaign. (2016). From Plan to Action: Successful Data Management Plan Implementation in a Multidisciplinary Project. Journal of EScience Librarianship, 5(1), e1101. https://doi.org/10.7191/jeslib.2016.1101 van de Schoot, R., Depaoli, S., King, R., Kramer, B., Märtens, K., Tadesse, M. G., Vannucci, M., Gelman, A., Veen, D., Willemsen, J., \u0026amp; Yau, C. (2021). Bayesian statistics and modelling. Nature Reviews Methods Primers, 1(1), 1. https://doi.org/10.1038/s43586-020-00001-2 Vazire, S. (2018). Implications of the Credibility Revolution for Productivity, Creativity, and Progress. Perspectives on Psychological Science, 13(4), 411–417. https://doi.org/10.1177/1745691617751884 Vazire, S., Schiavone, S. R., \u0026amp; Bottesini, J. G. (2020). Credibility Beyond Replicability: Improving the Four Validities in Psychological Science [Preprint]. PsyArXiv. https://doi.org/10.31234/osf.io/bu4d3 Villum, C. (2014, March 10). “Open-washing” – The difference between opening your data and simply making them available – Open Knowledge Foundation blog. Open Knowledge Foundation. https://blog.okfn.org/2014/03/10/open-washing-the-difference-between-opening-your-data-and-simply-making-them-available/ Vlaeminck, S., \u0026amp; Podkrajac, F. (2017). Journals in Economic Sciences: Paying Lip Service to Reproducible Research? IASSIST Quarterly, 41(1–4), 16. https://doi.org/10.29173/iq6 Voracek, M., Kossmeier, M., \u0026amp; Tran, U. S. (2019). Which Data to Meta-Analyze, and How?: A Specification-Curve and Multiverse-Analysis Approach to Meta-Analysis. Zeitschrift Für Psychologie, 227(1), 64–82. https://doi.org/10.1027/2151-2604/a000357 Vuorre, M., \u0026amp; Curley, J. P. (2018). Curating Research Assets: A Tutorial on the Git Version Control System. Advances in Methods and Practices in Psychological Science, 1(2), 219–236. https://doi.org/10.1177/2515245918754826 Wacker, J. G. (1998). A definition of theory: Research guidelines for different theory-building research methods in operations management. Journal of Operations Management, 16(4), 361–385. https://doi.org/10.1016/S0272-6963(98)00019-9 Wagenmakers, E.-J., Marsman, M., Jamil, T., Ly, A., Verhagen, J., Love, J., Selker, R., Gronau, Q. F., Šmíra, M., Epskamp, S., Matzke, D., Rouder, J. N., \u0026amp; Morey, R. D. (2018). Bayesian inference for psychology. Part I: Theoretical advantages and practical ramifications. Psychonomic Bulletin \u0026amp; Review, 25(1), 35–57. https://doi.org/10.3758/s13423-017-1343-3 Wagenmakers, E.-J., Wetzels, R., Borsboom, D., van der Maas, H. L. J., \u0026amp; Kievit, R. A. (2012). An Agenda for Purely Confirmatory Research. Perspectives on Psychological Science, 7(6), 632–638. https://doi.org/10.1177/1745691612463078 Wagge, J. R., Baciu, C., Banas, K., Nadler, J. T., Schwarz, S., Weisberg, Y., IJzerman, H., Legate, N., \u0026amp; Grahe, J. (2019). A Demonstration of the Collaborative Replication and Education Project: Replication Attempts of the Red-Romance Effect. Collabra: Psychology, 5(1), 5. https://doi.org/10.1525/collabra.177 Walker, P., \u0026amp; Miksa, T. (2019, November 26). RDA-DMP-Common/RDA-DMP-Common-Standard. GitHub. https://github.com/RDA-DMP-Common/RDA-DMP-Common-Standard Wason, P. C. (1960). On the Failure to Eliminate Hypotheses in a Conceptual Task. Quarterly Journal of Experimental Psychology, 12(3), 129–140. https://doi.org/10.1080/17470216008416717 Wasserstein, R. L., \u0026amp; Lazar, N. A. (2016). The ASA Statement on p -Values: Context, Process, and Purpose. The American Statistician, 70(2), 129–133. https://doi.org/10.1080/00031305.2016.1154108 Webster, M. M., \u0026amp; Rutz, C. (2020). How STRANGE are your study animals? Nature, 582(7812), 337–340. https://doi.org/10.1038/d41586-020-01751-5 Welcome to Sherpa Romeo—V2.sherpa. (n.d.). Sherpa Romeo. Retrieved 10 July 2021, from https://v2.sherpa.ac.uk/romeo/ Wendl, M. C. (2007). H-index: However ranked, citations need context. Nature, 449(7161), 403–403. https://doi.org/10.1038/449403b What is a Codebook? (n.d.). ICPSR. Retrieved 9 July 2021, from https://www.icpsr.umich.edu/icpsrweb/content/shared/ICPSR/faqs/what-is-a-codebook.html What is a reporting guideline? | The EQUATOR Network. (n.d.). Retrieved 10 July 2021, from https://www.equator-network.org/about-us/what-is-a-reporting-guideline/ What is Crowdsourcing? (2021, April 29). Crowdsourcing Week. https://crowdsourcingweek.com/what-is-crowdsourcing/ What is data sharing? | Support Centre for Data Sharing. (n.d.). Support Centre for Data Sharing. Retrieved 11 July 2021, from https://eudatasharing.eu/what-data-sharing What is impact? - Economic and Social Research Council. (n.d.). Economic and Social Research Council. Retrieved 8 July 2021, from https://esrc.ukri.org/research/impact-toolkit/what-is-impact/ What is Open Data? (n.d.). Open Data Handbook. Retrieved 9 July 2021, from https://opendatahandbook.org/guide/en/what-is-open-data/ What is open education? (n.d.). Opensource.Com. Retrieved 9 July 2021, from https://opensource.com/resources/what-open-education Whitaker, K., \u0026amp; Guest, O. (2020). #bropenscience is broken science. The Psychologist, 33, 34–37. Wicherts, J. M., Veldkamp, C. L. S., Augusteijn, H. E. M., Bakker, M., van Aert, R. C. M., \u0026amp; van Assen, M. A. L. M. (2016). Degrees of Freedom in Planning, Running, Analyzing, and Reporting Psychological Studies: A Checklist to Avoid p-Hacking. Frontiers in Psychology, 7. https://doi.org/10.3389/fpsyg.2016.01832 Wilkinson, M. D., Dumontier, M., Aalbersberg, Ij. J., Appleton, G., Axton, M., Baak, A., Blomberg, N., Boiten, J.-W., da Silva Santos, L. B., Bourne, P. E., Bouwman, J., Brookes, A. J., Clark, T., Crosas, M., Dillo, I., Dumon, O., Edmunds, S., Evelo, C. T., Finkers, R., … Mons, B. (2016). The FAIR Guiding Principles for scientific data management and stewardship. Scientific Data, 3(1), 160018. https://doi.org/10.1038/sdata.2016.18 Wilson, B., \u0026amp; Fenner, M. (2012, May 9). Open Researcher \u0026amp;amp; Contributor ID (ORCID): Solving the Name Ambiguity Problem. https://er.educause.edu/articles/2012/5/open-researcher--contributor-id-orcid-solving-the-name-ambiguity-problem Wilson, R. C., \u0026amp; Collins, A. G. (2019). Ten simple rules for the computational modeling of behavioral data. ELife, 8, e49547. https://doi.org/10.7554/eLife.49547 Wingen, T., Berkessel, J. B., \u0026amp; Englich, B. (2020). No Replication, No Trust? How Low Replicability Influences Trust in Psychology. Social Psychological and Personality Science, 11(4), 454–463. https://doi.org/10.1177/1948550619877412 Woelfle, M., Olliaro, P., \u0026amp; Todd, M. H. (2011). Open science is a research accelerator. Nature Chemistry, 3(10), 745–748. https://doi.org/10.1038/nchem.1149 Working Group 1 of the Joint Committee for Guides in Metrology JCGM. (2008). Evaluation of measurement data—Guide to the expression of uncertainty in measurement (pp. 1–120). JCGM. https://www.bipm.org/documents/20126/2071204/JCGM_100_2008_E.pdf/cb0ef43f-baa5-11cf-3f85-4dcd86f77bd6 World Wide Web Consortium. (n.d.). Home | Web Accessibility Initiative (WAI) | W3C. Web Accessibility Initiative. Retrieved 9 July 2021, from https://www.w3.org/WAI/ Wuchty, S., Jones, B. F., \u0026amp; Uzzi, B. (2007). The Increasing Dominance of Teams in Production of Knowledge. Science, 316(5827), 1036–1039. https://doi.org/10.1126/science.1136099 Xia, J., Harmon, J. L., Connolly, K. G., Donnelly, R. M., Anderson, M. R., \u0026amp; Howard, H. A. (2015). Who publishes in “predatory” journals? Journal of the Association for Information Science and Technology, 66(7), 1406–1417. https://doi.org/10.1002/asi.23265 Yamada, Y. (2018). How to Crack Pre-registration: Toward Transparent and Open Science. Frontiers in Psychology, 9, 1831. https://doi.org/10.3389/fpsyg.2018.01831 Yarkoni, T. (2020). The generalizability crisis. Behavioral and Brain Sciences, 1–37. https://doi.org/10.1017/S0140525X20001685 Yeung, S. K., Feldman, G., Fillon, A., Protzko, J., Elsherif, M. M., Xiao, Q., \u0026amp; Pickering, J. (n.d.). Experimental Studies Meta-Analysis\u0026nbsp; Registered Report template: Main manuscript [Preprint]. Hong Kong University. https://docs.google.com/document/d/1z3QBDYr86S9FxGjptZP94jJnZeeN4aQaBQP3VVT89Ec/edit# Zenodo—Research. Shared. (n.d.). Zenodo. Retrieved 9 July 2021, from https://www.zenodo.org/ Zurn, P., Bassett, D. S., \u0026amp; Rust, N. C. (2020). The Citation Diversity Statement: A Practice of Transparency, A Way of Life. Trends in Cognitive Sciences, 24(9), 669–672. https://doi.org/10.1016/j.tics.2020.06.009 Zwaan, R. A., Etz, A., Lucas, R. E., \u0026amp; Donnellan, M. B. (2018). Making replication mainstream. Behavioral and Brain Sciences, 41, e120. https://doi.org/10.1017/S0140525X17001972 ","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1635961284,"objectID":"b8e8415e839f2c4160e96dd99e92774b","permalink":"https://forrt.org/glossary/vbeta/references/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/vbeta/references/","section":"glossary","summary":"You can find the list of all references that were used to create the Glossary.\nWe are currently working on a better way to display and cross-link the references with the terms they are used for. A free and open platform for sharing MRI, MEG, EEG, iEEG, ECoG, ASL, and PET data—OpenNeuro.","tags":null,"title":"List of References","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1721390377,"objectID":"b9cad5b8b0008e24b03083955189689a","permalink":"https://forrt.org/glossary/english/literature_review/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/english/literature_review/","section":"glossary","summary":"","tags":null,"title":"Literature Review","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1635961284,"objectID":"b427a5d28c8374b35877bff22965bf99","permalink":"https://forrt.org/glossary/vbeta/literature-review/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/vbeta/literature-review/","section":"glossary","summary":"","tags":null,"title":"Literature Review","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1720865851,"objectID":"7adc2282b154f96b6b1a52f2c207b3b8","permalink":"https://forrt.org/glossary/german/literature_review/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/german/literature_review/","section":"glossary","summary":"","tags":null,"title":"Literature Review (Literaturzusammenfassung)","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"561a938e30ecf989bd0fc317fc7dcd40","permalink":"https://forrt.org/curated_resources/local-grassroots-networks-engaging-open/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/local-grassroots-networks-engaging-open/","section":"curated_resources","summary":"This recorded webinar features insights from international panelists currently nurturing culture change in research among their local communities.Representat...","tags":["Open Science","Reproducibility","Reproducible Research","Research"],"title":"Local Grassroots Networks Engaging Open Science in Their Communities","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1689122843,"objectID":"2cdc35f8708a7f2b89610b0250da721c","permalink":"https://forrt.org/curated_resources/logical-and-methodological-issues-affect/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/logical-and-methodological-issues-affect/","section":"curated_resources","summary":"Genetics and neuroscience are two areas of science that pose particular methodological problems because they involve detecting weak signals (i.e., small effects) in noisy data. In recent years, increasing numbers of studies have attempted to bridge these disciplines by looking for genetic factors associated with individual differences in behavior, cognition, and brain structure or function. However, different methodological approaches to guarding against false positives have evolved in the two disciplines. To explore methodological issues affecting neurogenetic studies, we conducted an in-depth analysis of 30 consecutive articles in 12 top neuroscience journals that reported on genetic associations in nonclinical human samples. It was often difficult to estimate effect sizes in neuroimaging paradigms. Where effect sizes could be calculated, the studies reporting the largest effect sizes tended to have two features: (i) they had the smallest samples and were generally underpowered to detect genetic effects, and (ii) they did not fully correct for multiple comparisons. Furthermore, only a minority of studies used statistical methods for multiple comparisons that took into account correlations between phenotypes or genotypes, and only nine studies included a replication sample or explicitly set out to replicate a prior finding. Finally, presentation of methodological information was not standardized and was often distributed across Methods sections and Supplementary Material, making it challenging to assemble basic information from many studies. Space limits imposed by journals could mean that highly complex statistical methods were described in only a superficial fashion. In summary, methods that have become standard in the genetics literature—stringent statistical standards, use of large samples, and replication of findings—are not always adopted when behavioral, cognitive, or neuroimaging phenotypes are used, leading to an increased risk of false-positive findings. Studies need to correct not just for the number of phenotypes collected but also for the number of genotypes examined, genetic models tested, and subsamples investigated. The field would benefit from more widespread use of methods that take into account correlations between the factors corrected for, such as spectral decomposition, or permutation approaches. Replication should become standard practice; this, together with the need for larger sample sizes, will entail greater emphasis on collaboration between research groups. We conclude with some specific suggestions for standardized reporting in this area.","tags":["Reproducibility Crisis and Credibility Revolution","Open Science"],"title":"Logical and methodological issues affecting genetic studies of humans reported in top neuroscience journals. ","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1689122843,"objectID":"65c7e245239714c5894bdd23bfa18f01","permalink":"https://forrt.org/curated_resources/longitudinal-structural-equation-modelin/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/longitudinal-structural-equation-modelin/","section":"curated_resources","summary":"Featuring actual datasets as illustrative examples, this book reveals numerous ways to apply structural equation modeling (SEM) to any repeated-measures study. Initial chapters lay the groundwork for modeling a longitudinal change process, from measurement, design, and specification issues to model evaluation and interpretation. Covering both big-picture ideas and technical \"how-to-do-it\" details, the author deftly walks through when and how to use longitudinal confirmatory factor analysis, longitudinal panel models (including the multiple-group case), multilevel models, growth curve models, and complex factor models, as well as models for mediation and moderation. User-friendly features include equation boxes that clearly explain the elements in every equation, end-of-chapter glossaries, and annotated suggestions for further reading. The companion website (www.guilford.com/little-materials) provides datasets for all of the examples--which include studies of bullying, adolescent students' emotions, and healthy aging--with syntax and output from LISREL, Mplus, and R (lavaan).","tags":["Book"],"title":"Longitudinal Structural Equation Modeling (Methodology in the Social Sciences)","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"8cd0d41e636aca1a07f8ad7aee7f2516","permalink":"https://forrt.org/curated_resources/low-research-data-availability-in-educat/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/low-research-data-availability-in-educat/","section":"curated_resources","summary":"Research-data availability contributes to the transparency of the research process and the credibility of educational-psychology research and science in general. Recently, there have been many initiatives to increase the availability and quality of research data. Many research institutions have adopted research-data policies. This increased awareness might have raised the sharing of research data in empirical articles. To test this idea, we coded 1,242 publications from six educational-psychology journals and the psychological journal Cognition (as a baseline) published in 2018 and 2020. Research-data availability was low (3.85% compared with 62.74% in Cognition) but has increased from 0.32% (2018) to 7.16% (2020). However, neither the data-transparency level of the journal nor the existence of an official research-data policy on the level of the corresponding author’s institution was related to research-data availability. We discuss the consequences of these findings for institutional research-data-management processes.","tags":["Educational Psychology","Research","Data Sharing","Data Policy","FAIR Data Principles","Data Transparency Levels","Open Data"],"title":"Low Research-Data Availability in Educational-Psychology Journals: No Indication of Effective Research-Data Policies","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"7681a8d3cdccb14e46ed0a8717e33edf","permalink":"https://forrt.org/curated_resources/making-erp-research-more-transparent-gui/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/making-erp-research-more-transparent-gui/","section":"curated_resources","summary":"A combination of confirmation bias, hindsight bias, and pressure to publish may prompt the (unconscious) exploration of various methodological options and reporting only the ones that lead to a (statistically) significant outcome. This undisclosed analytic flexibility is particularly relevant in EEG research, where a myriad of preprocessing and analysis pipelines can be used to extract information from complex multidimensional data. One solution to limit confirmation and hindsight bias by disclosing analytic choices is preregistration: researchers write a time-stamped, publicly accessible research plan with hypotheses, data collection plan, and the intended preprocessing and statistical analyses before the start of a research project. In this manuscript, we present an overview of the problems associated with undisclosed analytic flexibility, discuss why and how EEG researchers would benefit from adopting preregistration, provide guidelines and examples on how to preregister data preprocessing and analysis steps in typical ERP studies, and conclude by discussing possibilities and limitations of this open science practice.","tags":["Preregistration","EEG","Bias","Analytic Flexibility","ERP"],"title":"Making ERP research more transparent: Guidelines for preregistration","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1689122843,"objectID":"32c2ceadf9c7ee2c6c2685227796f38a","permalink":"https://forrt.org/curated_resources/making-prospective-registration-of-obser/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/making-prospective-registration-of-obser/","section":"curated_resources","summary":"The vast majority of health-related observational studies are not prospectively registered and the advantages of registration have not been fully appreciated. Nonetheless, international standards require approval of study protocols by an independent ethics committee before the study can begin. We suggest that there is an ethical and scientific imperative to publicly preregister key information from newly approved protocols, which should be required by funders. Ultimately, more complete information may be publicly available by disclosing protocols, analysis plans, data sets, and raw data.","tags":["Reproducibility Crisis and Credibility Revolution","Open Science"],"title":"Making Prospective Registration of Observational Research a Reality","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"65d0c25c125f9bca3cb2111aa278035f","permalink":"https://forrt.org/curated_resources/making-qualitative-data-reusable-a-short/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/making-qualitative-data-reusable-a-short/","section":"curated_resources","summary":"This guidebook aims to give an overview of the challenges associated with making qualitative data reusable as well as providing guidance on how reusability can be improved and addressed at all stages of the research data life cycle.\n\nThe guide includes a decision tree that researchers and data stewards can use to evaluate the options for making qualitative data reusable that are most suited for their projects.","tags":["Qualitative Data","Secondary Analysis"],"title":"Making Qualitative Data Reusable - A Short Guidebook For Researchers And Data Stewards Working With Qualitative Data","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1689122843,"objectID":"7c5e0db825f909a73f3d6e1b24370c4e","permalink":"https://forrt.org/curated_resources/making-replication-mainstream/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/making-replication-mainstream/","section":"curated_resources","summary":"Many philosophers of science and methodologists have argued that the ability to repeat studies and obtain similar results is an essential component of science. A finding is elevated from single observation to scientific evidence when the procedures that were used to obtain it can be reproduced and the finding itself can be replicated. Recent replication attempts show that some high profile results – most notably in psychology, but in many other disciplines as well – cannot be replicated consistently. These replication attempts have generated a considerable amount of controversy, and the issue of whether direct replications have value has, in particular, proven to be contentious. However, much of this discussion has occurred in published commentaries and social media outlets, resulting in a fragmented discourse. To address the need for an integrative summary, we review various types of replication studies and then discuss the most commonly voiced concerns about direct replication. We provide detailed responses to these concerns and consider different statistical ways to evaluate replications. We conclude there are no theoretical or statistical obstacles to making direct replication a routine aspect of psychological science.","tags":["Reproducibility Crisis and Credibility Revolution","Open Science"],"title":"Making replication mainstream","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"0197d63b1a8d31146790deb9103ccd86","permalink":"https://forrt.org/curated_resources/making-science-public-a-review-of-journa/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/making-science-public-a-review-of-journa/","section":"curated_resources","summary":"Science journalists are uniquely positioned to increase the societal impact of open science by contextualizing and communicating research findings in ways that highlight their relevance and implications for non-specialist audiences. Through engagement with and coverage of open research outputs, journalists can help align the ideals of openness, transparency, and accountability with the wider public sphere and its democratic potential. Yet, it is unclear to what degree journalists use open research outputs in their reporting, what factors motivate or constrain this use, and how the recent surge in openly available research seen during the COVID-19 pandemic has affected the relationship between open science and science journalism. This literature review thus examines journalists’ use of open research outputs, specifically open access publications and preprints. We focus on literature published from 2018 onwards—particularly literature relating to the COVID-19 pandemic—but also include seminal articles outside the search dates. We find that, despite journalists’ potential to act as critical brokers of open access knowledge, their use of open research outputs is hampered by an overreliance on traditional criteria for evaluating scientific quality; concerns about the trustworthiness of open research outputs; and challenges using and verifying the findings. We also find that, while the COVID-19 pandemic encouraged journalists to explore open research outputs such as preprints, the extent to which these explorations will become established journalistic practices remains unclear. Furthermore, we note that current research is overwhelmingly authored and focused on the Global North, and the United States specifically. Finally, given the dearth of research in this area, we conclude with recommendations for future research that attend to issues of equity and diversity, and more explicitly examine the intersections of open science and science journalism.","tags":["Open Science","Journalism","COVID-19"],"title":"Making science public: a review of journalists’ use of Open Science research","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"ff0a565af8b72ddfc1ff753ba3cb6828","permalink":"https://forrt.org/curated_resources/managing-a-personal-research-archive/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/managing-a-personal-research-archive/","section":"curated_resources","summary":"A class on setting up and managing research materials; caring for digital files to enable collaboration, sharing, and re-use; and helpful software/digital tools for organizing personal research files.","tags":["Analysis","Materials","Open Scholarship Tools and Technologies","Research Data Management","Researchers"],"title":"Managing a Personal Research Archive","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"e649136c1a9c4a735c5af96e65612d41","permalink":"https://forrt.org/curated_resources/managing-qualitative-social-science-data/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/managing-qualitative-social-science-data/","section":"curated_resources","summary":"A website with modules and lessons on qualitative research and data management","tags":["Qualitative research"],"title":"Managing Qualitative Social Science Data","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1721390377,"objectID":"3655dee28672b17ab824f583a17dcaef","permalink":"https://forrt.org/glossary/english/manel/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/english/manel/","section":"glossary","summary":"","tags":null,"title":"Manel","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1720865851,"objectID":"31f127a414a3be4b644eea6190ca88b9","permalink":"https://forrt.org/glossary/german/manel/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/german/manel/","section":"glossary","summary":"","tags":null,"title":"Manel","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1635961284,"objectID":"12577dca1aa0db3fa9809355d7fc4aa4","permalink":"https://forrt.org/glossary/vbeta/manel/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/vbeta/manel/","section":"glossary","summary":"","tags":null,"title":"Manel","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1721390377,"objectID":"030e9cd94daeb8d5bd573426f18b21e9","permalink":"https://forrt.org/glossary/english/many_authors/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/english/many_authors/","section":"glossary","summary":"","tags":null,"title":"Many authors","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1635961284,"objectID":"0094ce42fb10c96b21215454ec16da4b","permalink":"https://forrt.org/glossary/vbeta/many-authors/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/vbeta/many-authors/","section":"glossary","summary":"","tags":null,"title":"Many authors","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1720865851,"objectID":"a53b9261776c98e2c93b4b49c91a18ee","permalink":"https://forrt.org/glossary/german/many_authors/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/german/many_authors/","section":"glossary","summary":"","tags":null,"title":"Many Authors","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1721390377,"objectID":"6378aa1ed934bad49ab22bce5ed499cd","permalink":"https://forrt.org/glossary/english/many_labs/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/english/many_labs/","section":"glossary","summary":"","tags":null,"title":"Many Labs","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1720865851,"objectID":"a4116d7532f8528d246c2706cb5a4c38","permalink":"https://forrt.org/glossary/german/many_labs/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/german/many_labs/","section":"glossary","summary":"","tags":null,"title":"Many Labs","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1635961284,"objectID":"73cb3488ec9e489ac93a0a3fb0040e37","permalink":"https://forrt.org/glossary/vbeta/many-labs/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/vbeta/many-labs/","section":"glossary","summary":"","tags":null,"title":"Many Labs","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1689122843,"objectID":"a2a3fd4ee54f796f7943dd52221f4a7e","permalink":"https://forrt.org/curated_resources/many-labs-3-evaluating-participant-pool/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/many-labs-3-evaluating-participant-pool/","section":"curated_resources","summary":"The university participant pool is a key resource for behavioral research, and data quality is believed to vary over the course of the academic semester. This crowdsourced project examined time of semester variation in 10 known effects, 10 individual differences, and 3 data quality indicators over the course of the academic semester in 20 participant pools (N = 2696) and with an online sample (N = 737). Weak time of semester effects were observed on data quality indicators, participant sex, and a few individual differences-conscientiousness, mood, and stress. However, there was little evidence for time of semester qualifying experimental or correlational effects. The generality of this evidence is unknown because only a subset of the tested effects demonstrated evidence for the original result in the whole sample. Mean characteristics of pool samples change slightly during the semester, but these data suggest that those changes are mostly irrelevant for detecting effects.","tags":["Reproducibility Crisis and Credibility Revolution","Open Science"],"title":"Many Labs 3: Evaluating participant pool quality across the academic semester via replication.","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"b495d0b0f65292835754e2ecd018a184","permalink":"https://forrt.org/curated_resources/mapping-the-universe-of-registered-repor/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/mapping-the-universe-of-registered-repor/","section":"curated_resources","summary":"Registered reports present a substantial departure from traditional publishing models with the goal of enhancing the transparency and credibility of the scientific literature. We map the evolving universe of registered reports to assess their growth, implementation and shortcomings at journals across scientific disciplines.","tags":["Preregistration","Publishing","Registered Reports"],"title":"Mapping the universe of registered reports","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1721390377,"objectID":"2a2f9e56845f43717f2c22a1219ea6fe","permalink":"https://forrt.org/glossary/english/massive_open_online_courses/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/english/massive_open_online_courses/","section":"glossary","summary":"","tags":null,"title":"Massive Open Online Courses (MOOCs)","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1720865851,"objectID":"e2b9847d7f6d8e708573c2c2d1699cb9","permalink":"https://forrt.org/glossary/german/massive_open_online_courses/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/german/massive_open_online_courses/","section":"glossary","summary":"","tags":null,"title":"Massive Open Online Courses (MOOCs)","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1635961284,"objectID":"47c5e4b2eb12a96ba7cafea40401dc85","permalink":"https://forrt.org/glossary/vbeta/massive-open-online-courses-moocs/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/vbeta/massive-open-online-courses-moocs/","section":"glossary","summary":"","tags":null,"title":"Massive Open Online Courses (MOOCs)","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1721390377,"objectID":"df337c5e96a11a8eb847d50b69e804df","permalink":"https://forrt.org/glossary/english/massively_open_online_papers/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/english/massively_open_online_papers/","section":"glossary","summary":"","tags":null,"title":"Massively Open Online Papers (MOOPs)","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1720865851,"objectID":"b96d4920b5be94b7359a094b9329f4ef","permalink":"https://forrt.org/glossary/german/massively_open_online_papers/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/german/massively_open_online_papers/","section":"glossary","summary":"","tags":null,"title":"Massively Open Online Papers (MOOPs)","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1635961284,"objectID":"69d1444a2f69cc156670883884d54760","permalink":"https://forrt.org/glossary/vbeta/massively-open-online-papers-moops/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/vbeta/massively-open-online-papers-moops/","section":"glossary","summary":"","tags":null,"title":"Massively Open Online Papers (MOOPs)","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"df66705c1886d41dc8f8e5d5e370bb17","permalink":"https://forrt.org/curated_resources/materials-for-the-webinar-helping-scienc/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/materials-for-the-webinar-helping-scienc/","section":"curated_resources","summary":"Headlines and scholarly publications portray a crisis in biomedical and health sciences. In this webinar, you will learn what the crisis is and the vital role of librarians in addressing it. You will see how you can directly and immediately support reproducible and rigorous research using your expertise and your library services. You will explore reproducibility guidelines and recommendations and develop an action plan for engaging researchers and stakeholders at your institution. #MLAReproducibilityLearning OutcomesBy the end of this webinar, participants will be able to: describe the basic history of the “reproducibility crisis” and define reproducibility and replicability explain why librarians have a key role in addressing concerns about reproducibility, specifically in terms of the packaging of science explain 3-4 areas where librarians can immediately and directly support reproducible research through existing expertise and services start developing an action plan to engage researchers and stakeholders at their institution about how they will help address research reproducibility and rigorAudienceLibrarians who work with researchers; librarians who teach, conduct, or assist with evidence-synthesis or critical appraisal, and managers and directors who are interested in allocating resources toward supporting research rigor. No prior knowledge or skills required. Basic knowledge of scholarly research and publishing helpful.","tags":["Librarians","Reproducibility"],"title":"Materials for the Webinar \"Helping Science Succeed: The Librarian’s Role in Addressing the Reproducibility Crisis\"","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1721390377,"objectID":"8397f82fb1e9b75011c9347ac812464e","permalink":"https://forrt.org/glossary/english/matthew_effect/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/english/matthew_effect/","section":"glossary","summary":"","tags":null,"title":"Matthew effect (in science)","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1635961284,"objectID":"901bc24dc572b5d7332583524573dc84","permalink":"https://forrt.org/glossary/vbeta/matthew-effect-in-science/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/vbeta/matthew-effect-in-science/","section":"glossary","summary":"","tags":null,"title":"Matthew effect (in science)","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1720865851,"objectID":"c7e90d35e3144b8156c5b298899252c1","permalink":"https://forrt.org/glossary/german/matthew_effect/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/german/matthew_effect/","section":"glossary","summary":"","tags":null,"title":"Matthew effect (in science) (Matthew-Effekt in der Wissenschaft)","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1689122843,"objectID":"60a24f2e80fc3070ed9796bce59432eb","permalink":"https://forrt.org/curated_resources/maximizing-the-reproducibility-of-your-r/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/maximizing-the-reproducibility-of-your-r/","section":"curated_resources","summary":"A chapter to discuss maximising the reproducibility of research","tags":["Reproducibility Crisis and Credibility Revolution","Open Science"],"title":"Maximizing the Reproducibility of Your Research","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1689122843,"objectID":"6a75b54476da4f60068afba16e6c731b","permalink":"https://forrt.org/curated_resources/measurement-error-and-the-replication-cr/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/measurement-error-and-the-replication-cr/","section":"curated_resources","summary":"The assumption that measurement error always reduces effect sizes is false","tags":[""],"title":"Measurement error and the replication crisis. ","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1689122843,"objectID":"ec7d8a47abb797d17d0d49f114586053","permalink":"https://forrt.org/curated_resources/measurement-matters/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/measurement-matters/","section":"curated_resources","summary":"This resource list contains reading material on the topic of measurement in psychological sciences. We hope the list will be a useful tool in helping researchers to improve measurement practices, and inspire debates about measurement in psychology. We initiated the repository originally as accompanying material to our piece in the APS Observer entitled “Measurement Matters”. We consider it to be a preliminary, active, living document, and plan to update it regularly. We also want to acknowledge that the list is the outcome of many different sources, such as the SIPS pre-conference at SPSP 2018. If you have other papers you would like to see included here, please let us know (eikofried@gmail.com \u0026 kayflake@gmail.com).\nThis is not a complete overview of all relevant papers on measurement in psychology, but a selection of useful papers. We don’t agree with all positions put forward, but believe the papers and books provide a healthy balance of viewpoints. We intend this list as a resource for researchers at all levels of measurement expertise, and marked a few papers with * that we consider to be exceptionally suitable introductory papers for beginners. You can find the list on the Open Science Framework at https://osf.io/zrkd4. ","tags":["Collection","Reproducibility Knowledge"],"title":"Measurement Matters","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1689122843,"objectID":"bbb30fd1e5078b3614d88c3d18028a53","permalink":"https://forrt.org/curated_resources/measuring-the-prevalence-of-questionable/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/measuring-the-prevalence-of-questionable/","section":"curated_resources","summary":"Cases of clear scientific misconduct have received significant media attention recently, but less flagrantly questionable research practices may be more prevalent and, ultimately, more damaging to the academic enterprise. Using an anonymous elicitation format supplemented by incentives for honest reporting, we surveyed over 2,000 psychologists about their involvement in questionable research practices. The impact of truth-telling incentives on self-admissions of questionable research practices was positive, and this impact was greater for practices that respondents judged to be less defensible. Combining three different estimation methods, we found that the percentage of respondents who have engaged in questionable practices was surprisingly high. This finding suggests that some questionable practices may constitute the prevailing research norm.","tags":["Reproducibility Crisis and Credibility Revolution","Open Science"],"title":"Measuring the Prevalence of Questionable Research Practices With Incentives for Truth Telling","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"2064c7bf9e2aae63916ec05745089e42","permalink":"https://forrt.org/curated_resources/medical-product-industry-ties-to-patient/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/medical-product-industry-ties-to-patient/","section":"curated_resources","summary":"Patient advocacy organizations (PAOs) advance patient interests through promotion of disease awareness, engagement with policymakers, and partnership with medical product manufacturers in research and development.1 However, there are concerns that, in addition to industry financial support—which nearly half of PAOs accept2—having individuals formerly or currently based in industry serve on PAOs’ boards of directors and as senior leadership may influence PAOs’ priorities, advocacy, and recommendations.3 We identified the 50 highest-revenue US-based PAOs and examined whether their senior leadership had been or were currently employed by the pharmaceutical and medical device industries.","tags":["Transparency","Conflicts of Interest","Patient Advocacy Organizations","Industry"],"title":"Medical Product Industry Ties to Patient Advocacy Organizations’ Executive Leadership","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"7d94d2895f63be1817fe01446d2f5859","permalink":"https://forrt.org/curated_resources/medicines-and-healthcare-products-regula/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/medicines-and-healthcare-products-regula/","section":"curated_resources","summary":"In the UK, the Medicines and Healthcare products Regulatory Agency consulted on proposals “to improve and strengthen the UK clinical trials legislation to help us make the UK the best place to research and develop safe and innovative medicines”. The purpose of the consultation was to help finalise the proposals and contribute to the drafting of secondary legislation. We discussed these proposals as members of the Trials Methodology Research Partnership Adaptive Designs Working Group, which is jointly funded by the Medical Research Council and the National Institute for Health and Care Research. Two topics arose frequently in the discussion: the emphasis on legislation, and the absence of questions on data sharing. It is our opinion that the proposals rely heavily on legislation to change practice. However, clinical trials are heterogeneous, and as a result some trials will struggle to comply with all of the proposed legislation. Furthermore, adaptive design clinical trials are even more heterogeneous than their non-adaptive counterparts, and face more challenges. Consequently, it is possible that increased legislation could have a greater negative impact on adaptive designs than non-adaptive designs. Overall, we are sceptical that the introduction of legislation will achieve the desired outcomes, with some exceptions. Meanwhile the topic of data sharing — making anonymised individual-level clinical trial data available to other investigators for further use — is entirely absent from the proposals and the consultation in general. However, as an aspect of the wider concept of open science and reproducible research, data sharing is an increasingly important aspect of clinical trials. The benefits of data sharing include faster innovation, improved surveillance of drug safety and effectiveness and decreasing participant exposure to unnecessary risk. There are already a number of UK-focused documents that discuss and encourage data sharing, for example, the Concordat on Open Research Data and the Medical Research Council’s Data Sharing Policy. We strongly suggest that data sharing should be the norm rather than the exception, and hope that the forthcoming proposals on clinical trials invite discussion on this important topic.","tags":["Consultation","Data Sharing","Legislation"],"title":"Medicines and Healthcare products Regulatory Agency’s “Consultation on proposals for legislative changes for clinical trials”: a response from the Trials Methodology Research Partnership Adaptive Designs Working Group, with a focus on data sharing","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"64e2fa8e14d2b53735d8da52a4464d78","permalink":"https://forrt.org/curated_resources/meeting-the-requirements-of-funders-arou/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/meeting-the-requirements-of-funders-arou/","section":"curated_resources","summary":"Expectations by funders for transparent and reproducible methods are on the rise. This session covers expectations for preregistration, data sharing, and open access results of three key funders of education research including the Institute of Education Sciences, the National Science Foundation, and Arnold Ventures. Presenters cover practical resources for meeting these requirements such as the Registry for Efficacy and Effectiveness Studies (REES), the Open Science Framework (OSF), and EdArXiv. Presenters: Jessaca Spybrook, Western Michigan University Bryan Cook, University of Virginia David Mellor, Center for Open Science","tags":["Analysis","Data","Education","Materials","Open Scholarship Tools and Technologies","OSF","Publishing","Reproducibility","Research Data Management Tools","Researchers"],"title":"Meeting the Requirements of Funders Around Open Science: Open Resources and Processes for Education","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1689122843,"objectID":"f48d2db64014868c3c3ac78336bc555e","permalink":"https://forrt.org/curated_resources/meta-analyses-are-no-substitute-for-regi/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/meta-analyses-are-no-substitute-for-regi/","section":"curated_resources","summary":"According to a recent meta-analysis, religious priming has a positive effect on prosocial behavior (Shariff et al., 2015). We first argue that this meta-analysis suffers from a number of methodological shortcomings that limit the conclusions that can be drawn about the potential benefits of religious priming. Next we present a re-analysis of the religious priming data using two different meta-analytic techniques. A Precision-Effect Testing–Precision-Effect-Estimate with Standard Error (PET-PEESE) meta-analysis suggests that the effect of religious priming is driven solely by publication bias. In contrast, an analysis using Bayesian bias correction suggests the presence of a religious priming effect, even after controlling for publication bias. These contradictory statistical results demonstrate that meta-analytic techniques alone may not be sufficiently robust to firmly establish the presence or absence of an effect. We argue that a conclusive resolution of the debate about the effect of religious priming on prosocial behavior – and about theoretically disputed effects more generally – requires a large-scale, preregistered replication project, which we consider to be the sole remedy for the adverse effects of experimenter bias and publication bias.","tags":["Reproducibility Crisis and Credibility Revolution","Open Science"],"title":"Meta-analyses are no substitute for registered replications: a skeptical perspective on religious priming","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1721390377,"objectID":"edf282ae236d78b8c5508d2a378bf23a","permalink":"https://forrt.org/glossary/english/meta_analysis/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/english/meta_analysis/","section":"glossary","summary":"","tags":null,"title":"Meta-analysis","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1635961284,"objectID":"36a0676f6637c6f24fbcdca6027d3e1a","permalink":"https://forrt.org/glossary/vbeta/meta-analysis/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/vbeta/meta-analysis/","section":"glossary","summary":"","tags":null,"title":"Meta-analysis","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1720865851,"objectID":"71001f530783820c08bbebcc6e297143","permalink":"https://forrt.org/glossary/german/meta_analysis/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/german/meta_analysis/","section":"glossary","summary":"","tags":null,"title":"Meta-analysis (Meta-Analyse)","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"1659548c51996b93994520634bec6e11","permalink":"https://forrt.org/curated_resources/meta-assessment-of-bias-in-science/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/meta-assessment-of-bias-in-science/","section":"curated_resources","summary":"Numerous biases are believed to affect the scientific literature, but their actual prevalence across disciplines is unknown. To gain a comprehensive picture of the potential imprint of bias in science, we probed for the most commonly postulated bias-related patterns and risk factors, in a large random sample of meta-analyses taken from all disciplines. The magnitude of these biases varied widely across fields and was overall relatively small. However, we consistently observed a significant risk of small, early, and highly cited studies to overestimate effects and of studies not published in peer-reviewed journals to underestimate them. We also found at least partial confirmation of previous evidence suggesting that US studies and early studies might report more extreme effects, although these effects were smaller and more heterogeneously distributed across meta-analyses and disciplines. Authors publishing at high rates and receiving many citations were, overall, not at greater risk of bias. However, effect sizes were likely to be overestimated by early-career researchers, those working in small or long-distance collaborations, and those responsible for scientific misconduct, supporting hypotheses that connect bias to situational factors, lack of mutual control, and individual integrity. Some of these patterns and risk factors might have modestly increased in intensity over time, particularly in the social sciences. Our findings suggest that, besides one being routinely cautious that published small, highly-cited, and earlier studies may yield inflated results, the feasibility and costs of interventions to attenuate biases in the literature might need to be discussed on a discipline-specific and topic-specific basis.","tags":["Bias","Integrity","Meta-analysis","Meta-research","Metascience","Meta-Science","Misconduct","Publishing","Reproducibility","Research Methods"],"title":"Meta-assessment of bias in science","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1721390377,"objectID":"516725992825fb6d5819e0fb0755beb1","permalink":"https://forrt.org/glossary/english/meta_science_or_meta_research/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/english/meta_science_or_meta_research/","section":"glossary","summary":"","tags":null,"title":"Meta-science or Meta-research","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1635961284,"objectID":"c7c66f357bfaf71fe2612e258b8830ec","permalink":"https://forrt.org/glossary/vbeta/meta-science-or-meta-research/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/vbeta/meta-science-or-meta-research/","section":"glossary","summary":"","tags":null,"title":"Meta-science or Meta-research","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1720865851,"objectID":"e39ff55ef8a7b174738a72440a12de71","permalink":"https://forrt.org/glossary/german/meta_science_or_meta_research/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/german/meta_science_or_meta_research/","section":"glossary","summary":"","tags":null,"title":"Meta-science or Meta-research (Meta-Wissenschaft oder Meta-Forschung)","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1689122843,"objectID":"fcb330ae3db0d8013259a028a449fd9b","permalink":"https://forrt.org/curated_resources/meta-regression-approximations-to-reduce/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/meta-regression-approximations-to-reduce/","section":"curated_resources","summary":"Publication selection bias is a serious challenge to the integrity of all empirical sciences. We derive meta-regression approximations to reduce this bias. Our approach employs Taylor polynomial approximations tothe conditional mean of a truncated distribution. A quadratic approximation without a linear term, precision-effect estimate with standard error (PEESE), is shown to have the smallest bias and mean squared error in mostcases and to outperform conventional meta-analysis estimators, often by a great deal. Monte Carlo simulationsalso demonstrate how a new hybrid estimator that conditionally combines PEESE and the Egger regressionintercept can provide a practical solution to publication selection bias. PEESE is easily expanded to accom-modate systematic heterogeneity along with complex and differential publication selection bias that is relatedto moderator variables. By p roviding an intuitive reason for these approximations, we can also explain why theEgger regression works so well and when it does not. These meta-regression methods are applied to severalpolicy-relevant areas of research including antidepressant effectiveness, the value of a statistical life, theminimum wage, and nicotine replacement therapy.","tags":["Reproducibility Crisis and Credibility Revolution","Open Science"],"title":"Meta‐regression approximations to reduce publication selection bias.","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1721390377,"objectID":"625db96952089a6e3f2bbf5139d5f591","permalink":"https://forrt.org/glossary/english/metadata/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/english/metadata/","section":"glossary","summary":"","tags":null,"title":"Metadata","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1635961284,"objectID":"f15e99eb16a75acc52bdec8c845cb620","permalink":"https://forrt.org/glossary/vbeta/metadata/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/vbeta/metadata/","section":"glossary","summary":"","tags":null,"title":"Metadata","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1720865851,"objectID":"06295e982b1074affbe8b66d785919d4","permalink":"https://forrt.org/glossary/german/metadata/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/german/metadata/","section":"glossary","summary":"","tags":null,"title":"Metadata (Metadaten)","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"5324631e99aadca45ffb88f4b7e9a7f2","permalink":"https://forrt.org/curated_resources/metascience-forum-2020/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/metascience-forum-2020/","section":"curated_resources","summary":"In his talk, Professor Nosek defines replication as gathering evidence that tests an empirical claim made in an original paper. This intent influences the design and interpretation of a replication study and addresses confusion between conceptual and direct replications.\n---\nAre you a funder interested in supporting research on the scientific process? Learn more about the communities mobilizing around the emerging field of metascience by visiting metascience.com. Funders are encouraged to review and adopt the practices overviewed at cos.io/top-funders as part of the solution to issues discussed during the Funders Forum.","tags":["Reproducibiity","Research","Scientific Process"],"title":"Metascience Forum 2020","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"d7265226b6192fd7a73a1ad5e7cbf0f7","permalink":"https://forrt.org/curated_resources/methodological-advances-in-behavioral-re/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/methodological-advances-in-behavioral-re/","section":"curated_resources","summary":"The results of many published studies across many scientific domains are not easily reproduced by independent laboratories. For example, an initiative by Bayer Healthcare to replicate 67 pre-clinical studies led to a reproducibility rate of 20-25% (Prinz et al., 2011), and researchers at Amgen were only able to replicate 6 of 53 influential cancer biology studies (Begley \u0026 Ellis, 2012). Similar replication failures have been reported in social and cognitive psychology (Ebersole et al., 2015; Klein et al., 2014; Open Science Collaboration, 2015). This PhD Boot camp introduces PhD students in the behavioral sciences to 1) the ongoing “crisis of confidence” in science, 2) typical methodological challenges of conducting replications, 3) the philosophy of science and statistical background of replications, 4) highly collaborative approaches to replication, in which findings are replicated in independent laboratories before (rather than after) they are published. As part of the boot camp, students will be organized into replication teams and take part in a crowdsourced pre-publication independent replication project on which they will be creditedas co-authors. Participation in the bootcamp is free, the replications will be funded by a grant from INSEAD, and the infrastructure for data collection is already in place. Crowdsourcing research involves recruiting numerous scientific teams to achieve large-scale projects no single team could feasibly carry out. Leveraging crowds of researchers increases the statistical power and generalizability of research designs, reduces investigator error and bias, and enhances scientific transparency. Actively participating in a large-scale replication effort provides an opportunity for students to experience the power of a crowd of researchers firsthand. Lecture topics will include the scientific crisis caused by high-profile replication failures, publication bias, questionable research practices, the open data movement, and crowdsourced replication efforts, among others.","tags":["Reproducibility Crisis and Credibility Revolution","Open Science","Conceptual and statistical knowledge"],"title":"Methodological Advances in Behavioral Research: Crowdsourcing Science","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1689122843,"objectID":"0946197e249659066dc0839200abb18b","permalink":"https://forrt.org/curated_resources/methods-for-reliable-transparent-and-ope/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/methods-for-reliable-transparent-and-ope/","section":"curated_resources","summary":"A syllabus about the methods for reliable, transparent and open science","tags":["Syllabus"],"title":"Methods for Reliable, Transparent, and Open Science","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1689122843,"objectID":"6539cf644ae904d2625671aa519e4093","permalink":"https://forrt.org/curated_resources/mindless-statistics/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/mindless-statistics/","section":"curated_resources","summary":"Statistical rituals largely eliminate statistical thinking in the social sciences. Rituals are indispensable for identification with social groups, but they should be the subject rather than the procedure of science. What I call the “null ritual” consists of three steps: (1) set up a statistical null hypothesis, but do not specify your own hypothesis nor any alternative hypothesis, (2) use the 5% significance level for rejecting the null and accepting your hypothesis, and (3) always perform this procedure. I report evidence of the resulting collective confusion and fears about sanctions on the part of students and teachers, researchers and editors, as well as textbook writers.","tags":[""],"title":"Mindless statistics","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"f9ed57df7f715218a14eed6bcba4e447","permalink":"https://forrt.org/curated_resources/mini-course-on-reproducibility-open-rese/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/mini-course-on-reproducibility-open-rese/","section":"curated_resources","summary":"This mini course was designed to introduce Psychology undergraduate students at the University of Edinburgh (who are working on their dissertation in the academic year 2021-2022) to open research principles and practices. \nThe course contains 5 videos introducing several core principles of reproducible working which we clustered into the Three T's: Think before you do (about principles of pre-registration), Trace your steps (about detailed record-keeping and version control) and Be Transparent (about adopting a transparent working style and why this is important). \n\nThe videos were created by Dr. Alex Mitchell (Teaching Fellow in Psychology, University of Edinburgh), Niamh MacSweeney (PhD student at the University of Edinburgh \u0026 co-organiser Edinburgh ReproducibiliTea) and Laura Klinkhamer (PhD student at the University of Edinburgh \u0026 co-organiser Edinburgh ReproducibiliTea). ","tags":["INTRODUCTION","INTRODUCTORY","REPRODUCIBLE","PRINCIPLES","UNDERGRADUATE","DISSERTATION","THESIS","LECTURE","SERIES","VIDEOS","THREE TS","THREE T'S","PSYCHOLOGY"],"title":"Mini course on reproducibility \u0026 open research","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1689122843,"objectID":"7e5843b0918401231a6233d539f4f0ed","permalink":"https://forrt.org/curated_resources/mini-meta-analysis-of-your-own-studies-s/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/mini-meta-analysis-of-your-own-studies-s/","section":"curated_resources","summary":"We outline the need to, and provide a guide on how to, conduct a meta-analysis on one’s own studies within a manuscript. Although conducting a “mini meta” within one’s manuscript has been argued for in the past, this practice is still relatively rare and adoption is slow. We believe two deterrents are responsible. First, researchers may not think that it is legitimate to do a meta-analysis on a small number of studies. Second, researchers may think a meta-analysis is too complicated to do without expert knowledge or guidance. We dispel these two misconceptions by (1) offering arguments on why researchers should be encouraged to do mini metas, (2) citing previous articles that have conducted such analyses to good effect, and (3) providing a user-friendly guide on calculating some meta-analytic procedures that are appropriate when there are only a few studies. We provide formulas for calculating effect sizes and converting effect sizes from one metric to another (e.g., from Cohen’s d to r), as well as annotated Excel spreadsheets and a step-by-step guide on how to conduct a simple meta-analysis. A series of related studies can be strengthened and better understood if accompanied by a mini meta-analysis.","tags":[""],"title":"Mini Meta-Analysis of Your Own Studies: Some Arguments on Why and a Primer on How","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"0649802872b5c58c9af00565f046cef6","permalink":"https://forrt.org/curated_resources/misaligned-incentives-hurt-science-but-w/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/misaligned-incentives-hurt-science-but-w/","section":"curated_resources","summary":"In this talk, Professor Corker shows how researchers are typically evaluated and contrasts that with ideal ways to evaluate the process of scientific output. Funding for open practices, infrastructure, and publication decisions made regardless of outcome incentivize the type of science we want to see occur. \n---\nAre you a funder interested in supporting research on the scientific process? Learn more about the communities mobilizing around the emerging field of metascience by visiting metascience.com. Funders are encouraged to review and adopt the practices overviewed at cos.io/top-funders as part of the solution to issues discussed during the Funders Forum.","tags":["Research"],"title":"Misaligned Incentives Hurt Science, but We Can Fix Them","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1689122843,"objectID":"6d00a8d9f9a5ef46e7aa58afafa0d66d","permalink":"https://forrt.org/curated_resources/mission-p-curve/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/mission-p-curve/","section":"curated_resources","summary":"A blog about the p-curve","tags":["Blog"],"title":"Mission: P-curve","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"9b0ae78cfa24ba73fe98cc3fd3376b03","permalink":"https://forrt.org/curated_resources/mitigating-illusory-results-through-prer/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/mitigating-illusory-results-through-prer/","section":"curated_resources","summary":"Like performance-enhancing drugs inflating apparent athletic achievements, several common social science practices contribute to the production of illusory results. In this article, we examine the processes that lead to illusory findings and describe their consequences. We borrow from an approach used increasingly by other disciplines—the norm of preregistering studies. Specifically, we examine how this practice of publicly posting documentation of one's prespecified hypotheses and other key decisions of a study prior to study implementation or data analysis could improve scientific integrity within education. In an attempt to develop initial guidelines to facilitate preregistrations in education, we discuss the types of studies that ought to be preregistered and the logistics of how educational researchers might execute preregistrations. We conclude with ideas for how researchers, reviewers, and the field of education more broadly might speed the adoption of this new norm.","tags":["Educational Research","P-Hacking","Preregistration","Replication","Research Methods"],"title":"Mitigating Illusory Results through Preregistration in Education","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1635961284,"objectID":"271b630e92ba26304f032710c17a7ded","permalink":"https://forrt.org/glossary/vbeta/model-computational/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/vbeta/model-computational/","section":"glossary","summary":"","tags":null,"title":"Model (computational)","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1721390377,"objectID":"28b4c762bd8137654701f96b01681ccc","permalink":"https://forrt.org/glossary/english/model/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/english/model/","section":"glossary","summary":"","tags":null,"title":"Model (philosophy)","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1635961284,"objectID":"9c91d26d79de08407aeb96cbe085ee40","permalink":"https://forrt.org/glossary/vbeta/model-philosophy/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/vbeta/model-philosophy/","section":"glossary","summary":"","tags":null,"title":"Model (philosophy) ","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1720865851,"objectID":"2379047a16f005b9670c4d3410158ebe","permalink":"https://forrt.org/glossary/german/model/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/german/model/","section":"glossary","summary":"","tags":null,"title":"Model (philosophy) (Modell","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1635961284,"objectID":"b9286348cc58937a68ceeed9a7b9c604","permalink":"https://forrt.org/glossary/vbeta/model-statistical/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/vbeta/model-statistical/","section":"glossary","summary":"","tags":null,"title":"Model (statistical)","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1721390377,"objectID":"c93c1807a762cd330cf868892495cdd3","permalink":"https://forrt.org/glossary/english/multi_analyst_studies/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/english/multi_analyst_studies/","section":"glossary","summary":"","tags":null,"title":"Multi-Analyst Studies","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1635961284,"objectID":"759a3fcb9cfb6cd763aa586049b5c3fb","permalink":"https://forrt.org/glossary/vbeta/multi-analyst-studies/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/vbeta/multi-analyst-studies/","section":"glossary","summary":"","tags":null,"title":"Multi-Analyst Studies","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1720865851,"objectID":"c4a5316edfc3f734613c86d4c957ccd4","permalink":"https://forrt.org/glossary/german/multi_analyst_studies/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/german/multi_analyst_studies/","section":"glossary","summary":"","tags":null,"title":"Multi-Analyst Studies (Multi-Analytiker:innen-Studien)","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1689122843,"objectID":"fd2e450a98b1c04e6f7538a890b4f395","permalink":"https://forrt.org/curated_resources/multilevel-analysis-an-introduction-to-b/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/multilevel-analysis-an-introduction-to-b/","section":"curated_resources","summary":"The main methods, techniques and issues for carrying out multilevel modeling and analysis are covered in this book. The book is an applied introduction to the topic, providing a clear conceptual understanding of the issues involved in multilevel analysis and will be a useful reference tool. Information on designing multilevel studies, sampling, testing and model specification and interpretation of models is provided. A comprehensive guide to the software available is included. Multilevel Analysis is the ideal guide for researchers and applied statisticians in the social sciences, including education, but will also interest researchers in economics, and biological, medical and health disciplines.","tags":["Book"],"title":"Multilevel Analysis: An Introduction to Basic and Advanced Multilevel Modeling","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1721390377,"objectID":"589392c9bf2bcf494eddfce28bdecf9a","permalink":"https://forrt.org/glossary/english/multiplicity/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/english/multiplicity/","section":"glossary","summary":"","tags":null,"title":"Multiplicity","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1635961284,"objectID":"5bf227460d5f45a2e7e56ac0a13548d8","permalink":"https://forrt.org/glossary/vbeta/multiplicity/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/vbeta/multiplicity/","section":"glossary","summary":"","tags":null,"title":"Multiplicity","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1720865851,"objectID":"ff4280c6a1a84ef58cc6b79b17fd2694","permalink":"https://forrt.org/glossary/german/multiplicity/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/german/multiplicity/","section":"glossary","summary":"","tags":null,"title":"Multiplicity (Multiplizität)","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1721390377,"objectID":"7afc18ebf8899d68c7ffe0577dd1264c","permalink":"https://forrt.org/glossary/english/multiverse_analysis/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/english/multiverse_analysis/","section":"glossary","summary":"","tags":null,"title":"Multiverse analysis","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1635961284,"objectID":"2f4506b32bd330cbe79f12594a95473b","permalink":"https://forrt.org/glossary/vbeta/multiverse-analysis/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/vbeta/multiverse-analysis/","section":"glossary","summary":"","tags":null,"title":"Multiverse analysis","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1720865851,"objectID":"1c5bc3c76164f6154efab58c3cd323c9","permalink":"https://forrt.org/glossary/german/multiverse_analysis/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/german/multiverse_analysis/","section":"glossary","summary":"","tags":null,"title":"Multiverse analysis (Multiversumsanalyse)","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1721390377,"objectID":"6daa90b4df010e222088bda35d118b2a","permalink":"https://forrt.org/glossary/english/name_ambiguity_problem/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/english/name_ambiguity_problem/","section":"glossary","summary":"","tags":null,"title":"Name Ambiguity Problem","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1635961284,"objectID":"18e34b2f1972f652b9ba111e10f1b36f","permalink":"https://forrt.org/glossary/vbeta/name-ambiguity-problem/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/vbeta/name-ambiguity-problem/","section":"glossary","summary":"","tags":null,"title":"Name Ambiguity Problem","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1720865851,"objectID":"9aa0c62cc5494ca2dbae186770fee546","permalink":"https://forrt.org/glossary/german/name_ambiguity_problem/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/german/name_ambiguity_problem/","section":"glossary","summary":"","tags":null,"title":"Name Ambiguity Problem (Problem mehrdeutiger Namen)","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1721390377,"objectID":"a7feb30faee7df51f43c35a5fde92000","permalink":"https://forrt.org/glossary/english/named_entity_based_text_anonymization_for_open_science/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/english/named_entity_based_text_anonymization_for_open_science/","section":"glossary","summary":"","tags":null,"title":"Named entity-based Text Anonymization for Open Science (NETANOS)","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1720865851,"objectID":"548ebac88e958cfaddbf9f462cd76627","permalink":"https://forrt.org/glossary/german/named_entity_based_text_anonymization_for_open_science/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/german/named_entity_based_text_anonymization_for_open_science/","section":"glossary","summary":"","tags":null,"title":"Named entity-based Text Anonymization for Open Science (NETANOS)","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1635961284,"objectID":"f91468723a55fe36eea4cc20a5511311","permalink":"https://forrt.org/glossary/vbeta/named-entity-based-text-anonymizati/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/vbeta/named-entity-based-text-anonymizati/","section":"glossary","summary":"","tags":null,"title":"Named entity-based Text Anonymization for Open Science (NETANOS)","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"2badb9b5b2802fe426487d92171195e2","permalink":"https://forrt.org/curated_resources/negative-results-are-disappearing-from-m/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/negative-results-are-disappearing-from-m/","section":"curated_resources","summary":"Concerns that the growing competition for funding and citations might distort science are frequently discussed, but have not been verified directly. Of the hypothesized problems, perhaps the most worrying is a worsening of positive-outcome bias. A system that disfavours negative results not only distorts the scientific literature directly, but might also discourage high-risk projects and pressure scientists to fabricate and falsify their data. This study analysed over 4,600 papers published in all disciplines between 1990 and 2007, measuring the frequency of papers that, having declared to have “tested” a hypothesis, reported a positive support for it. The overall frequency of positive supports has grown by over 22% between 1990 and 2007, with significant differences between disciplines and countries. The increase was stronger in the social and some biomedical disciplines. The United States had published, over the years, significantly fewer positive results than Asian countries (and particularly Japan) but more than European countries (and in particular the United Kingdom). Methodological artefacts cannot explain away these patterns, which support the hypotheses that research is becoming less pioneering and/or that the objectivity with which results are produced and published is decreasing.","tags":["Reproducibility Crisis and Credibility Revolution","Open Science"],"title":"Negative results are disappearing from most disciplines and countries","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1689122843,"objectID":"5390a2754c8f1309aa9c7c7726dd07cb","permalink":"https://forrt.org/curated_resources/negativity-towards-negative-results-a-di/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/negativity-towards-negative-results-a-di/","section":"curated_resources","summary":"Science is often romanticised as a flawless system of knowledge building, where scientists work together to systematically find answers. In reality, this is not always the case. Dissemination of results are straightforward when the findings are positive, but what happens when you obtain results that support the null hypothesis, or do not fit with the current scientific thinking? In this Editorial, we discuss the issues surrounding publication bias and the difficulty in communicating negative results. Negative findings are a valuable component of the scientific literature because they force us to critically evaluate and validate our current thinking, and fundamentally move us towards unabridged science.","tags":["Reproducibility Crisis and Credibility Revolution","Open Science"],"title":"Negativity towards negative results: a discussion of the disconnect between scientific worth and scientific culture.","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1689122843,"objectID":"ece99f752fe6759bc4e8dfa87e8207b4","permalink":"https://forrt.org/curated_resources/neuroscientist-explains/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/neuroscientist-explains/","section":"curated_resources","summary":"Daniel Glaser apprehensively revisits an article of his that saw some fallout due to a study he cited. But that study was not the only one involved in what is now being called a crisis for psychology and further afield","tags":["Podcast","Reproducibility Knowledge","Reproducibility Crisis and Credibility Revolution"],"title":"Neuroscientist Explains","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"61f79eb2fd1c5849da35d0a1a82ed37f","permalink":"https://forrt.org/curated_resources/new-trends-in-science-communication-fost/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/new-trends-in-science-communication-fost/","section":"curated_resources","summary":"TRESCA – Trustworthy, Reliable and Engaging Scientific Communication Approaches – is a research project aimed at understanding how science communication can help re-build trust in science and scientists. The project wants to create positive changes through common research activities with various stakeholders, e.g., the general public, scientists, journalists, and policymakers.\n\nThus, TRESCA also aimed to identify the most important actual trends how communication between scientific experts and policymakers changed in the last decades in the field of innovation and digitalisation policy. We looked at how these trends might influence the way policymakers receive, interpret, and use scientific evidence during their daily work.\n\nThe partners first checked various scientific and non-scientific documents concerning potential new communication trends between scientists and policymakers. The partners conducted interviews with policymakers working in four European countries (Austria, Hungary, Italy, the Netherlands) and at the international/EU level. The interviews investigated the scientific data sources, data collection processes, science communication topics, channels, and formats frequently used by policymakers.\n\nWe found that at least three new trends had strengthened in the last decades: (1) increasingly often more permanent formal relationships are developed between scientists and policymakers to cope with the more frequent and intense communication; (2) to enhance trust between scientists and policymakers, more transparent and reliable communication channels and formats are used; (3) policymakers need to understand more scientific information in less time therefore visual and digital communication formats are getting more widespread.\n\nAfter an online consultation process, practical recommendations were provided to policymakers on how to support more effective communication with scientists. This included the creation of more training opportunities, the increased use of communication guides, the promotion of fact-checking websites, or ways to motivate scientists to communicate with policymakers. These steps might support a novel communication process built on trust and the understanding of each other’s perspective.","tags":["Science Communication","Evidence-Informed Policy-Making","Trust","Dialogue Model","Open Science","Visualisation","Digitalisation"],"title":"New trends in science communication fostering evidence-informed policymaking","type":"curated_resources"},{"authors":null,"categories":null,"content":" Stay updated by subscribing to the FORRT Newsletter * indicates required Email Address * Archives \u0026#x1f4c6; 2024\n| June 2024\n| May 2024\n| April 2024\n| March 2024\n| February 2024\n| January 2024\n\u0026#x1f4c6; 2023\n| November 2023\n| October 2023\n| September 2023\n| January 13th 2023\n\u0026#x1f4c6; 2022\n| August 19th 2022\n| April 20th 2022\n","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1726832212,"objectID":"233d9b1767997bd99122be7aff35b525","permalink":"https://forrt.org/newsletters/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/newsletters/","section":"","summary":"Stay updated by subscribing to the FORRT Newsletter * indicates required Email Address * Archives \u0026#x1f4c6; 2024\n| June 2024\n| May 2024\n| April 2024\n| March 2024\n| February 2024\n| January 2024\n\u0026#x1f4c6; 2023\n| November 2023\n| October 2023\n| September 2023\n| January 13th 2023","tags":null,"title":"Newsletter Archive","type":"page"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"e9e1e9b0ab4badb0107cab2b737249b9","permalink":"https://forrt.org/curated_resources/niaid-centers-of-excellence-for-influenz/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/niaid-centers-of-excellence-for-influenz/","section":"curated_resources","summary":"The goal of the NIAID Centers of Excellence for Influenza Research and Response (CEIRR) awards is to provide the federal government, scientists, and public health personnel with information and public health tools and strategies needed to control and lessen the impact of epidemic influenza and the increasing threat of pandemic influenza. Additional information about NIAID’s guiding principles of data sharing can be found here.\n\nNIAID recognizes that the data and information obtained through the CEIRR Program will be extremely valuable research resources and that the rapid sharing of these data will be essential in advancing research on influenza and developing therapeutics, vaccines, and diagnostics. One of the major roles of the iDPCC is to deposit data on behalf of CEIRR to public databases. These include: Bacterial and Viral Bioinformatics Resource Center (BV-BRC), NCBI GenBank, and NCBI SRA.","tags":["Influenza","Flu","Respiratory Illness","Data Standards"],"title":"NIAID Centers of Excellence for Influenza Research and Response (CEIRR) Data Standards","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"8a3a0e46de38bbd9dc09ed0ed8cdfe85","permalink":"https://forrt.org/curated_resources/nigms-clearinghouse-for-training-modules/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/nigms-clearinghouse-for-training-modules/","section":"curated_resources","summary":"In January 2014, NIH launched a series of initiatives to enhance rigor and reproducibility in research. As a part of this initiative, NIGMS, along with nine other NIH institutes and centers, issued a funding opportunity announcement (FOA) RFA-GM-15-006 to develop, pilot, and disseminate training modules to enhance data reproducibility. This FOA was reissued in 2018 (RFA-GM-18-002).For the benefit of the scientific community, we will post the products of grants funded by these FOAs on this website as they become available. In addition, we are sharing here other relevant training modules developed, including courses developed from administrative supplements to NIGMS predoctoral T32 grants.","tags":["Reproducibility","Research Administration","Research Data Management","Researchers"],"title":"NIGMS Clearinghouse for Training Modules to Enhance Data Reproducibility","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"cd1d32e40bf098e5d475d83b87157ed7","permalink":"https://forrt.org/curated_resources/nih-scientific-data-sharing/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/nih-scientific-data-sharing/","section":"curated_resources","summary":"This webpage describes the NIH's data management and sharing policies, including policies for genomic data sharing, and sharing data about model organisms. It also has guides for research tools, clinical trials, research publications, and accessing data.","tags":["data sharing","federal policy"],"title":"NIH Scientific Data Sharing","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"dcaf54c23bede6899564ce9e45b769cc","permalink":"https://forrt.org/curated_resources/no-evidence-of-publication-bias-in-clima/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/no-evidence-of-publication-bias-in-clima/","section":"curated_resources","summary":"Non-significant results are less likely to be reported by authors and, when submitted for peer review, are less likely to be published by journal editors. This phenomenon, known collectively as publication bias, is seen in a variety of scientific disciplines and can erode public trust in the scientific method and the validity of scientific theories. Public trust in science is especially important for fields like climate change science, where scientific consensus can influence state policies on a global scale, including strategies for industrial and agricultural management and development. Here, we used meta-analysis to test for biases in the statistical results of climate change articles, including 1154 experimental results from a sample of 120 articles. Funnel plots revealed no evidence of publication bias given no pattern of non-significant results being under-reported, even at low sample sizes. However, we discovered three other types of systematic bias relating to writing style, the relative prestige of journals, and the apparent rise in popularity of this field: First, the magnitude of statistical effects was significantly larger in the abstract than the main body of articles. Second, the difference in effect sizes in abstracts versus main body of articles was especially pronounced in journals with high impact factors. Finally, the number of published articles about climate change and the magnitude of effect sizes therein both increased within 2 years of the seminal report by the Intergovernmental Panel on Climate Change 2007.","tags":["Analysis","Publication Bias","Reporting","Statistics"],"title":"No evidence of publication bias in climate change science","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1721390377,"objectID":"841abc455c6c6bb770c48c5416319855","permalink":"https://forrt.org/glossary/english/non_intervention__reproducible__and_open_systematic_reviews/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/english/non_intervention__reproducible__and_open_systematic_reviews/","section":"glossary","summary":"","tags":null,"title":"Non-Intervention, Reproducible, and Open Systematic Reviews (NIRO-SR)","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1720865851,"objectID":"fd81dc5c4c727e48b95708b1954e49d1","permalink":"https://forrt.org/glossary/german/non_intervention__reproducible__and_open_systematic_reviews/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/german/non_intervention__reproducible__and_open_systematic_reviews/","section":"glossary","summary":"","tags":null,"title":"Non-Intervention, Reproducible, and Open Systematic Reviews (NIRO-SR)","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1635961284,"objectID":"e90cb6a3774708d1b63db759a5ffbe19","permalink":"https://forrt.org/glossary/vbeta/non-intervention-reproducible-and-o/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/vbeta/non-intervention-reproducible-and-o/","section":"glossary","summary":"","tags":null,"title":"Non-Intervention, Reproducible, and Open Systematic Reviews (NIRO-SR)","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1689122843,"objectID":"fc86b03959fd421c6b2c1e9712d4c71b","permalink":"https://forrt.org/curated_resources/novel-methods-to-deal-with-publication-b/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/novel-methods-to-deal-with-publication-b/","section":"curated_resources","summary":"Objective: To assess the performance of novel contour enhanced funnel plots and a regression based adjustment method to detect and adjust for publication biases. Design: Secondary analysis of a published systematic literature review. Data sources: Placebo controlled trials of antidepressants previously submitted to the US Food and Drug Administration (FDA) and matching journal publications. Methods: Publication biases were identified using novel contour enhanced funnel plots, a regression based adjustment method, Egger's test, and the trim and fill method. Results were compared with a meta-analysis of the gold standard data submitted to the FDA. Results: Severe asymmetry was observed in the contour enhanced funnel plot that appeared to be heavily influenced by the statistical significance of results, suggesting publication biases as the cause of the asymmetry. Applying the regression based adjustment method to the journal data produced a similar pooled effect to that observed by a meta-analysis of the FDA data. Contrasting journal and FDA results suggested that, in addition to other deviations from study protocol, switching from an intention to treat analysis to a per protocol one would contribute to the observed discrepancies between the journal and FDA results. Conclusion: Novel contour enhanced funnel plots and a regression based adjustment method worked convincingly and might have an important part to play in combating publication biases.","tags":["Reproducibility Crisis and Credibility Revolution","Open Science"],"title":"Novel methods to deal with publication biases: secondary analysis of antidepressant trials in the FDA trial registry database and related journal publications.","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1720865851,"objectID":"1a4440f89a7ea4d8a1a1c6dc7ee7f9f7","permalink":"https://forrt.org/glossary/german/null_hypothesis_significance_testing/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/german/null_hypothesis_significance_testing/","section":"glossary","summary":"","tags":null,"title":"Null Hypothesis Significance Testing (NHST, Nullhypothesen-Signifikanztestung)","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1721390377,"objectID":"49878e73e36afb6a25028aa3f580c6c1","permalink":"https://forrt.org/glossary/english/null_hypothesis_significance_testing/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/english/null_hypothesis_significance_testing/","section":"glossary","summary":"","tags":null,"title":"Null Hypothesis Significance Testing (NHST)","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1635961284,"objectID":"8b014ac66f8ac327a83a2945622fea67","permalink":"https://forrt.org/glossary/vbeta/null-hypothesis-significance-testin/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/vbeta/null-hypothesis-significance-testin/","section":"glossary","summary":"","tags":null,"title":"Null Hypothesis Significance Testing (NHST)","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1689122843,"objectID":"16c43cf677b0cb61bf2b16941a62eb8a","permalink":"https://forrt.org/curated_resources/null-hypothesis-significance-testing-a-r/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/null-hypothesis-significance-testing-a-r/","section":"curated_resources","summary":"Null hypothesis significance testing (NHST) is arguably the most widely used approach to hypothesis evaluation among behavioral and social scientists. It is also very controversial. A major concern expressed by critics is that such testing is misunderstood by many of those who use it. Several other objections to its use have also been raised. In this article the author reviews and comments on the claimed misunderstandings as well as on other criticisms of the approach, and he notes arguments that have been advanced in support of NHST. Alternatives and supplements to NHST are considered, as are several related recommendations regarding the interpretation of experimental data. The concluding opinion is that NHST is easily misunderstood and misused but that when applied with good judgment it can be an effective aid to the interpretation of experimental data","tags":[""],"title":"Null hypothesis significance testing: a review of an old and continuing controversy","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1689122843,"objectID":"6a91ccf5e78b99163139a198653b4210","permalink":"https://forrt.org/curated_resources/null-hypothesis-significance-testing-on/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/null-hypothesis-significance-testing-on/","section":"curated_resources","summary":"Null hypothesis significance testing (NHST) is the researcher's workhorse for making inductive inferences. This method has often been challenged, has occasionally been defended, and has persistently been used through most of the history of scientific psychology. This article reviews both the criticisms of NHST and the arguments brought to its defense. The review shows that the criticisms address the logical validity of inferences arising from NHST, whereas the defenses stress the pragmatic value of these inferences. The author suggests that both critics and apologists implicitly rely on Bayesian assumptions. When these assumptions are made explicit, the primary challenge for NHST--and any system of induction--can be confronted. The challenge is to find a solution to the question of replicability.","tags":[""],"title":"Null Hypothesis Significance Testing. On the Survival of a Flawed Method","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"fe374da145292cb2a668fb49c94633eb","permalink":"https://forrt.org/curated_resources/number-of-countries-with-open-science-po/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/number-of-countries-with-open-science-po/","section":"curated_resources","summary":"Eleven countries have introduced open science policies, strategies and legislative frameworks since the adoption of the UNESCO Recommendation on Open Science two years ago, according to the UNESCO Open Science Outlook 1: Status and Trends Around the World. This means that the the total number of countries with open science policies has almost doubled. Other countries have adopted policies that pertain to one aspect of open science, such as for publications and/ or research data. The Outlook was launched on 14 December in Geneva at the European Organization for Nuclear Research (CERN) as a prelude to the closing ceremony of the International Year of Basic Sciences for Sustainable Development.","tags":["UNESCO","Open Science Policies"],"title":"Number of countries with open science policies has almost doubled since adoption of UNESCO Recommendation","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1721390377,"objectID":"69ec8f17320f438053d6e92356baa09f","permalink":"https://forrt.org/glossary/english/objectivity/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/english/objectivity/","section":"glossary","summary":"","tags":null,"title":"Objectivity","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1635961284,"objectID":"68759def268d499cb819cf7572bed1c1","permalink":"https://forrt.org/glossary/vbeta/objectivity/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/vbeta/objectivity/","section":"glossary","summary":"","tags":null,"title":"Objectivity","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1720865851,"objectID":"082ebba03f05adce791d273a709e9a9a","permalink":"https://forrt.org/glossary/german/objectivity/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/german/objectivity/","section":"glossary","summary":"","tags":null,"title":"Objectivity (Objektivität)","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"dd99e8ebaa62c9f71eab76258eb29286","permalink":"https://forrt.org/curated_resources/on-not-confusing-the-tree-of-trustworthy/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/on-not-confusing-the-tree-of-trustworthy/","section":"curated_resources","summary":"In this commentary on Simmons, Nelson, and Simonsohn (this issue), we examine their rationale for pre-registration within the broader perspective of what good science is. We agree that there is potential benefit in a system of pre-registration if implemented selectively. However, we believe that other tools of open science such as the full sharing of study materials and open access to underlying data, provide most of the same benefits—and more (i.e., the prevention of outright fraud)—without risking the potential adverse consequences of a system of pre-registration. This is why we favor these other means of controlling type I error and fostering transparency. Direct replication, as opposed to conceptual replication, should be encouraged as well.","tags":["Preregistration","Open Science","Reproducibility Crisis","Consumer Research","Philosophy of Science"],"title":"On Not Confusing the Tree of Trustworthy Statistics with the Greater Forest of Good Science: A Comment on Simmons et al.’s Perspective on Pre-registration","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"241c66907465461aa2fedaafafc8c0da","permalink":"https://forrt.org/curated_resources/on-the-challenges-of-drawing-conclusions/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/on-the-challenges-of-drawing-conclusions/","section":"curated_resources","summary":"In recent years, researchers have attempted to provide an indication of the prevalence of inflated Type 1 error rates by analyzing the distribution of p-values in the published literature. De Winter \u0026 Dodou (2015) analyzed the distribution (and its change over time) of a large number of p-values automatically extracted from abstracts in the scientific literature. They concluded there is a ‘surge of p-values between 0.041–0.049 in recent decades’ which ‘suggests (but does not prove) questionable research practices have increased over the past 25 years.’ I show the changes in the ratio of fractions of p-values between 0.041–0.049 over the years are better explained by assuming the average power has decreased over time. Furthermore, I propose that their observation that p-values just below 0.05 increase more strongly than p-values above 0.05 can be explained by an increase in publication bias (or the file drawer effect) over the years (cf. Fanelli, 2012; Pautasso, 2010, which has led to a relative decrease of ‘marginally significant’ p-values in abstracts in the literature (instead of an increase in p-values just below 0.05). I explain why researchers analyzing large numbers of p-values need to relate their assumptions to a model of p-value distributions that takes into account the average power of the performed studies, the ratio of true positives to false positives in the literature, the effects of publication bias, and the Type 1 error rate (and possible mechanisms through which it has inflated). Finally, I discuss why publication bias and underpowered studies might be a bigger problem for science than inflated Type 1 error rates, and explain the challenges when attempting to draw conclusions about inflated Type 1 error rates from a large heterogeneous set of p-values.","tags":[""],"title":"On the challenges of drawing conclusions from p-values just below 0.05","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1689122843,"objectID":"3bcd048aa046baabf0c25d02f8a30cdf","permalink":"https://forrt.org/curated_resources/on-the-origins-of-the-05-level-of-statis/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/on-the-origins-of-the-05-level-of-statis/","section":"curated_resources","summary":"Examination of the literature in statistics and probability that predates Fisher's Statistical Methods for Research Workers indicates that although Fisher is responsible for the first formal statement of the .05 criterion for statistical significance, the concept goes back much further. The move toward conventional levels for the rejection of the hypothesis of chance dates from the turn of the century. Early statements about statistical significance were given in terms of the probable error. These earlier conventions were adopted and restated by Fisher.","tags":[""],"title":"On the origins of the .05 level of statistical significance","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1689122843,"objectID":"a35aab6fcb1344696431c72a77e60693","permalink":"https://forrt.org/curated_resources/on-the-plurality-of-methodological-world/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/on-the-plurality-of-methodological-world/","section":"curated_resources","summary":"How likely are published findings in the functional neuroimaging literature to be false? According to a recent mathematical model, the potential for false positives increases with the flexibility of analysis methods. Functional MRI (fMRI) experiments can be analyzed using a large number of commonly used tools, with little consensus on how, when, or whether to apply each one. This situation may lead to substantial variability in analysis outcomes. Thus, the present study sought to estimate the flexibility of neuroimaging analysis by submitting a single event-related fMRI experiment to a large number of unique analysis procedures. Ten analysis steps for which multiple strategies appear in the literature were identified, and two to four strategies were enumerated for each step. Considering all possible combinations of these strategies yielded 6,912 unique analysis pipelines. Activation maps from each pipeline were corrected for multiple comparisons using five thresholding approaches, yielding 34,560 significance maps. While some outcomes were relatively consistent across pipelines, others showed substantial methods-related variability in activation strength, location, and extent. Some analysis decisions contributed to this variability more than others, and different decisions were associated with distinct patterns of variability across the brain. Qualitative outcomes also varied with analysis parameters: many contrasts yielded significant activation under some pipelines but not others. Altogether, these results reveal considerable flexibility in the analysis of fMRI experiments. This observation, when combined with mathematical simulations linking analytic flexibility with elevated false positive rates, suggests that false positive results may be more prevalent than expected in the literature. This risk of inflated false positive rates may be mitigated by constraining the flexibility of analytic choices or by abstaining from selective analysis reporting.","tags":["Reproducibility Crisis and Credibility Revolution","Open Science"],"title":"On the plurality of (methodological) worlds: estimating the analytic flexibility of fMRI experiments","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1689122843,"objectID":"d823f8caf4d3f71905eb954b00cdc5d0","permalink":"https://forrt.org/curated_resources/on-the-reproducibility-of-meta-analyses/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/on-the-reproducibility-of-meta-analyses/","section":"curated_resources","summary":"Background: Meta-analyses play an important role in cumulative science by combining information across multiple studies and attempting to provide effect size estimates corrected for publication bias. Research on the reproducibility of meta-analyses reveals that errors are common, and the percentage of effect size calculations that cannot be reproduced is much higher than is desirable. Furthermore, the flexibility in inclusion criteria when performing a meta-analysis, combined with the many conflicting conclusions drawn by meta-analyses of the same set of studies performed by different researchers, has led some people to doubt whether meta-analyses can provide objective conclusions. Discussion: The present article highlights the need to improve the reproducibility of meta-analyses to facilitate the identification of errors, allow researchers to examine the impact of subjective choices such as inclusion criteria, and update the meta-analysis after several years. Reproducibility can be improved by applying standardized reporting guidelines and sharing all meta-analytic data underlying the meta-analysis, including quotes from articles to specify how effect sizes were calculated. Pre-registration of the research protocol (which can be peer-reviewed using novel ‘registered report’ formats) can be used to distinguish a-priori analysis plans from data-driven choices, and reduce the amount of criticism after the results are known. Summary: The recommendations put forward in this article aim to improve the reproducibility of meta-analyses. In addition, they have the benefit of “future-proofing” meta-analyses by allowing the shared data to be re-analyzed as new theoretical viewpoints emerge or as novel statistical techniques are developed. Adoption of these practices will lead to increased credibility of meta-analytic conclusions, and facilitate cumulative scientific knowledge.","tags":["Reproducibility Crisis and Credibility Revolution","Open Science"],"title":"On the reproducibility of meta-analyses: six practical recommendations","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1689122843,"objectID":"180b8be2f6a3388938ba30acd7d84105","permalink":"https://forrt.org/curated_resources/on-the-reproducibility-of-psychological/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/on-the-reproducibility-of-psychological/","section":"curated_resources","summary":"Investigators from a large consortium of scientists recently performed a multi-year study in which they replicated 100 psychology experiments. Although statistically significant results were reported in 97% of the original studies, statistical significance was achieved in only 36% of the replicated studies. This article presents a reanalysis of these data based on a formal statistical model that accounts for publication bias by treating outcomes from unpublished studies as missing data, while simultaneously estimating the distribution of effect sizes for those studies that tested non-null effects. The resulting model suggests that more than 90% of tests performed in eligible psychology experiments tested negligible effects, and that publication biases based on p-values caused the observed rates of nonreproducibility. The results of this reanalysis provide a compelling argument for both increasing the threshold required for declaring scientific discoveries and for adopting statistical summaries of evidence that account for the high proportion of tested hypotheses that are false. Supplementary materials for this article are available online.","tags":["Reproducibility Crisis and Credibility Revolution","Open Science"],"title":"On the reproducibility of psychological science","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"8700e81902f31b4d6fbc8ecd69b6dae0","permalink":"https://forrt.org/curated_resources/on-the-reproducibility-of-science-unique/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/on-the-reproducibility-of-science-unique/","section":"curated_resources","summary":"Scientific reproducibility has been at the forefront of many news stories and there exist numerous initiatives to help address this problem. We posit that a contributor is simply a lack of specificity that is required to enable adequate research reproducibility. In particular, the inability to uniquely identify research resources, such as antibodies and model organisms, makes it difficult or impossible to reproduce experiments even where the science is otherwise sound. In order to better understand the magnitude of this problem, we designed an experiment to ascertain the “identifiability” of research resources in the biomedical literature. We evaluated recent journal articles in the fields of Neuroscience, Developmental Biology, Immunology, Cell and Molecular Biology and General Biology, selected randomly based on a diversity of impact factors for the journals, publishers, and experimental method reporting guidelines. We attempted to uniquely identify model organisms (mouse, rat, zebrafish, worm, fly and yeast), antibodies, knockdown reagents (morpholinos or RNAi), constructs, and cell lines. Specific criteria were developed to determine if a resource was uniquely identifiable, and included examining relevant repositories (such as model organism databases, and the Antibody Registry), as well as vendor sites. The results of this experiment show that 54% of resources are not uniquely identifiable in publications, regardless of domain, journal impact factor, or reporting requirements. For example, in many cases the organism strain in which the experiment was performed or antibody that was used could not be identified. Our results show that identifiability is a serious problem for reproducibility. Based on these results, we provide recommendations to authors, reviewers, journal editors, vendors, and publishers. Scientific efficiency and reproducibility depend upon a research-wide improvement of this substantial problem in science today.","tags":["Data","Reproducibility"],"title":"On the reproducibility of science: unique identification of research resources in the biomedical literature","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1689122843,"objectID":"d397db1da24ae931930cf794d4d2658f","permalink":"https://forrt.org/curated_resources/on-the-surprising-longevity-of-flogged-h/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/on-the-surprising-longevity-of-flogged-h/","section":"curated_resources","summary":"Criticisms of null-hypothesis significance tests (NHSTs) are reviewed. Used as formal, two-valued decision procedures, they often generate misleading conclusions. However, critics who argue that NHSTs are totally meaningless because the null hypothesis is virtually always false are overstating their case. Critics also neglect the whole class of valuable significance tests that assess goodness of fit of models to data. Even as applied to simple mean differences, NHSTs can be rhetorically useful in defending research against criticisms that random factors adequately explain the results, or that the direction of mean difference was not demonstrated convincingly. Principled argument and counterargument produce the lore, or communal understanding, in a field, which in turn helps guide new research. Alternative procedures--confidence intervals, effect sizes, and meta-analysis--are discussed. Although these alternatives are not totally free from criticism either, they deserve more frequent use, without an unwise ban on NHSTs.","tags":[""],"title":"On the Surprising Longevity of Flogged Horses: Why There Is a Case for the Significance Test","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1689122843,"objectID":"5da3db37445fe5d77cd30b910b63c37b","permalink":"https://forrt.org/curated_resources/one-cheer-for-null-hypothesis-significan/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/one-cheer-for-null-hypothesis-significan/","section":"curated_resources","summary":"Null hypothesis testing as a tool in research is defended. Six examples are offered of situations in which, if all the researcher could do was \"reject H₀ at α = .05\" the scientific contribution would still be substantial. The examples are drawn from physics, cosmology, psychology, geophysics, career counseling and theology. ","tags":[""],"title":"One cheer for null hypothesis significance testing","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"609610918ee2592fac43dbcb9df8a018","permalink":"https://forrt.org/curated_resources/online-experimentation-and-sampling-in-c/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/online-experimentation-and-sampling-in-c/","section":"curated_resources","summary":"Online data collection methods have become increasingly popular in many domains of psychology, but their use in cognitive aging studies remains relatively limited. Is it time for cognitive aging researchers to embrace these methods? Here, we weigh potential advantages and disadvantages of conducting online studies with young and older adults, relative to lab-based studies, with a particular focus on the study of human memory and aging. With online studies, it may be possible to assess whether age-related effects on cognition obtained in the laboratory generalize to other situations with different environmental or subject characteristics. However, there are many open questions about the representativeness of older adults on online data collection platforms, and issues surrounding data quality, selection effects, and other biasing characteristics, which must be carefully handled in cognitive aging studies which recruit young and older adult participants online. We consider the benefit of conducting experimentation both in the lab and online in providing converging evidence on a research question, and we offer an example of an experiment on adult age differences in associative recognition that was conducted in the laboratory and online. We also provide practical recommendations for ways to maximize the potential for online studies to contribute to our understanding of cognitive aging. v","tags":["Online Experiments","Sampling","Cognitive Aging"],"title":"Online experimentation and sampling in cognitive aging research","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"5899c30362380ca62d71a2cdba5118df","permalink":"https://forrt.org/curated_resources/online-experiments-for-language-scientis/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/online-experiments-for-language-scientis/","section":"curated_resources","summary":"Many areas in the language sciences rely on collecting data from human participants, from grammaticality judgments to behavioural responses (key presses, mouse clicks, spoken responses). While data collection traditionally takes place face-to-face, recent years have seen an explosion in the use of online data collection: participants take part remotely, providing responses through a survey tool or custom experimental software running in their web browser, with surveys or experiments often being advertised on crowdsourcing websites like Amazon Mechanical Turk (MTurk) or Prolific. Online methods potentially allow rapid and low-effort collection of large samples, and are particularly useful in situations where face-to-face data collection is not possible (e.g. during a pandemic); however, building and running these experiments poses challenges that differ from lab-based methods.\n\nThis course will provide a rapid tour of online experimental methods in the language sciences, covering a range of paradigms, from survey-like responses (e.g. as required for grammaticality judgments) through more standard psycholinguistic methods (button presses, mouse clicks) up to more ambitious and challenging techniques (e.g. voice recording, real-time interaction through text and/or streaming audio, iterated learning). Each week we will read a paper detailing a study using online methods, and look at code (written in javascript using jspsych) to implement a similar experiment - the examples will skew towards the topics I am interested in (language learning, communication, language evolution), but we’ll cover more standard paradigms too (grammaticality judgments, self-paced reading) and the techniques are fairly general anyway. We’ll also look at the main platforms for reaching paid participants, e.g. MTurk and Prolific, and discuss some of the challenges around data quality and the ethics of running on those platforms.\n\nNo prior experience in coding is assumed, but you have to be prepared to dive in and try things out; the assessment will involve elements of both literature review and coding.","tags":["Programming"],"title":"Online Experiments for Language Scientists","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1721390377,"objectID":"71c4acc06de4bca851fb8c34ae109457","permalink":"https://forrt.org/glossary/english/ontology/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/english/ontology/","section":"glossary","summary":"","tags":null,"title":"Ontology (Artificial Intelligence)","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1635961284,"objectID":"12ff4efe94ef0f7b37a135d49d4f22de","permalink":"https://forrt.org/glossary/vbeta/ontology-artificial-intelligence/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/vbeta/ontology-artificial-intelligence/","section":"glossary","summary":"","tags":null,"title":"Ontology (Artificial Intelligence)","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1720865851,"objectID":"cb3943a2bd4cf56de1d8253fde670a53","permalink":"https://forrt.org/glossary/german/ontology/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/german/ontology/","section":"glossary","summary":"","tags":null,"title":"Ontology (Artificial Intelligence) (Ontologie in der Künstlichen Intelligenz)","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"639d5daf43a9bfb8de49718bea62596c","permalink":"https://forrt.org/curated_resources/open-reproducible-research-workshop/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/open-reproducible-research-workshop/","section":"curated_resources","summary":"Topics covered:\n\nUnderstanding reproducible research\nSetting up a reproducible project\nUnderstanding power\nPreregistering your study\nKeeping track of things\nContaining bias\nSharing your work","tags":["Documentation","Organizing","Reproducibility","Version Control"],"title":"Open + Reproducible Research Workshop","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1721390377,"objectID":"21b5f57b7968e6d18151bdf9f7e3438f","permalink":"https://forrt.org/glossary/english/open_access/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/english/open_access/","section":"glossary","summary":"","tags":null,"title":"Open access","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1720865851,"objectID":"3d027b890ada60a597f372a8416ba92f","permalink":"https://forrt.org/glossary/german/open_access/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/german/open_access/","section":"glossary","summary":"","tags":null,"title":"Open access","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1635961284,"objectID":"c2fae4dc4eafa923e6669ff03f30bd5d","permalink":"https://forrt.org/glossary/vbeta/open-access/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/vbeta/open-access/","section":"glossary","summary":"","tags":null,"title":"Open access","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"3d6bbf07ab77987c5c46ecb243e2333b","permalink":"https://forrt.org/curated_resources/open-access-directory/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/open-access-directory/","section":"curated_resources","summary":"The Open Access Directory is an online compendium of factual lists about open access to science and scholarship, maintained by the community at large. It exists as a wiki hosted by the School of Library and Information Science at Simmons University in Boston, USA. The goal is for the open access community itself to enlarge and correct the lists with little intervention from the editors or editorial board. For quality control, editing privileges are granted to registered users. As far as possible, lists are limited to brief factual statements without narrative or opinion.","tags":["Governmental Policies","Institutional Policies","Open Access","Open Access Policies","Organizational Change","Preprints","Publishing Models","Research Administration","Researchers"],"title":"Open Access Directory","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"5200b914077cd22ca206bee0d766d670","permalink":"https://forrt.org/curated_resources/open-access-journal-publication-in-healt/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/open-access-journal-publication-in-healt/","section":"curated_resources","summary":"Scientific progress, including in evidence-based medicine, requires all available evidence to be accessed, scrutinised, interpreted and used. Missing or incomplete evidence creates biases and errors in later research. Open science practices are movements and procedures that aim to increase transparency in science production. They aim to make scientific knowledge available, accessible and reusable, benefitting scientific collaboration and all society. Open access is a core component of open science that aims to help solve the problem of accessibility.\n\nTraditional publication behind a paywall can hide evidence from the public, clinicians, policymakers and other researchers. Whether online or print, traditional scientific journals maintain their content behind a paywall, with only abstracts freely available to read. Readers access articles by purchasing the individual article, the entire journal issue or through a subscription. These journal subscriptions are purchased by institutions like universities and libraries. However, readers whose institutions cannot afford these subscriptions or who are not affiliated to an institution are often unable to pay to access every article they need. Members of the public and readers in low-resourced countries are disproportionately affected.\n\nOpen access is defined as making a document freely available for anyone to read and, depending on the license model, share and use. Scholarly publishers now offer open access routes for publishing journal articles such as protocols, commentaries, reviews and result articles. The academic community expects these publishers to adhere to the same quality standards as in traditional closed access publication, such as peer review, indexing and permanent archiving. Biomedical research has progressively adopted open access, with yearly increases in the percentage of articles available as open access publications and the number of countries and policies mandating open access. Online supplemental text 1 summarizes national and international open access mandates.","tags":["Open Access","Health Research","Medical Research"],"title":"Open access journal publication in health and medical research and open science: Benefits, challenges and limitations","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"fb936e687046648e6663c3890d091fd8","permalink":"https://forrt.org/curated_resources/open-access-target-validation-is-a-more/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/open-access-target-validation-is-a-more/","section":"curated_resources","summary":"There is a scarcity of novel treatments to address many unmet medical needs. Industry and academia are finally coming to terms with the fact that the prevalent models and incentives for innovation in early stage drug discovery are failing to promote progress quickly enough. Here we will examine how an open model of precompetitive public–private research partnership is enabling efficient derisking and acceleration in the early stages of drug discovery, whilst also widening the range of communities participating in the process, such as patient and disease foundations.","tags":["Acute Myeloid Leukemia","Drug Discovery","Drug Research and Development","Drug Therapy","Leukemias","Lymphomas","Open Access Publishing","Phase I Clinical Investigation"],"title":"Open Access Target Validation Is a More Efficient Way to Accelerate Drug Discovery","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"d5d7d483fe98b605562c37d01c320ad6","permalink":"https://forrt.org/curated_resources/open-access-tracking-project/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/open-access-tracking-project/","section":"curated_resources","summary":"OATP is a crowd-sourced social-tagging project running on free software to capture new developments on open access to research. Its mission is (1) to create real-time alerts for OA-related news and comment, and (2) to organize knowledge of the field, by tag or subtopic, for easy searching and sharing.","tags":["Open Access","Researchers"],"title":"Open Access Tracking Project","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"d802c0ab613e403b36b7184b9a93240c","permalink":"https://forrt.org/curated_resources/open-and-reproducible-methods-syllabus/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/open-and-reproducible-methods-syllabus/","section":"curated_resources","summary":"This course will examine current controversies and new developments in research methods in psychology. The goal of the course is to learn to think critically about how psychological science is conducted and how conclusions are drawn. We will cover both methodological and statistical issues that affect the validity of research in psychology, with an emphasis on social and personality psychology.We will cover the debate about Null Hypothesis Significance Testing (NHST) and alternatives to NHST (e.g., effectestimation).We will also discuss the recent controversy in psychology about the replicability of scientific results.This course is most suited for students who plan to pursue graduate school in psychology and are preparing for a career conducting research in psychological science","tags":["Reproducibility Crisis and Credibility Revolution","Open Science"],"title":"Open and Reproducible Methods Syllabus","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"2bb454ef2569893518cc9635b0347b6b","permalink":"https://forrt.org/curated_resources/open-and-reproducible-research-in-psycho/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/open-and-reproducible-research-in-psycho/","section":"curated_resources","summary":"Psychological research is in the midst of what has been called the “replication crisis”, in which questions have been raised about the validity of even seminal research findings. How can we tell if the research we’re reading and doing is rigorous and reproducible? In this course, we will discuss the controversy around and the replicability of our results, while learning about new initiatives that are reinventing our day-to-day scientific workflow. These include preregistration, open sharing of data and materials, new models of scientific publishing, large-scale open collaborations, and statistical reform. The course will involve hands-on practice implementing cutting-edge tools for conducting open and reproducible research in psychology. \n---We acknowledge that Concordia University is located on unceded Indigenous lands. The Kanien’kehá:ka Nation is recognized as the custodians of the lands and waters on which we gather today. Tiohtià:ke/Montréal is historically known as a gathering place for many First Nations. Today, it is home to a diverse population of Indigenous and other peoples. We respect the continued connections with the past, present and future in our ongoing relationships with Indigenous and other peoples within the Montreal community.","tags":["Transparency","Reproducibility Crisis and Credibility Revolution","Open Science"],"title":"Open and Reproducible Research in Psychology","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"c97c66168f30c1c66335779a4ce84e40","permalink":"https://forrt.org/curated_resources/open-and-reproducible-science/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/open-and-reproducible-science/","section":"curated_resources","summary":"This course is designed to be delivered to master and doctoral students at the Universidade Federaldo Rio Grande do Sul (UFRGS). The structure is organized to (1) provide basic skills to enhance overall transparency and methodological reproducibility, and (2) promote reflection and discussion about open science-not only for simple and good things but also for barriers and some complex issues. Each class requires preparation through reading papers and watching videos. In an ambitious sense, this course aims to compose topics about the philosophy of science, research methods, and useful resources, ultimately targeting more efficient and robust research projects.","tags":["Reproducibility Crisis and Credibility Revolution","Open Science"],"title":"Open and reproducible science","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1721390377,"objectID":"f7016f3007ac479907607afa331716b4","permalink":"https://forrt.org/glossary/english/open_code/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/english/open_code/","section":"glossary","summary":"","tags":null,"title":"Open Code","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1635961284,"objectID":"b0632421aae676d6b00224e3dbf1c354","permalink":"https://forrt.org/glossary/vbeta/open-code/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/vbeta/open-code/","section":"glossary","summary":"","tags":null,"title":"Open Code","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1720865851,"objectID":"842c7b44f97d3893885a7d995c7f8506","permalink":"https://forrt.org/glossary/german/open_code/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/german/open_code/","section":"glossary","summary":"","tags":null,"title":"Open Code (Offener Code)","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1721390377,"objectID":"c4a38888d02105846f798b74cbe95320","permalink":"https://forrt.org/glossary/english/open_data/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/english/open_data/","section":"glossary","summary":"","tags":null,"title":"Open Data","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1635961284,"objectID":"88f6e898771a64c9c50e608d416382b7","permalink":"https://forrt.org/glossary/vbeta/open-data/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/vbeta/open-data/","section":"glossary","summary":"","tags":null,"title":"Open Data","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1720865851,"objectID":"45485a1adcf0080a0636434b8aa9e4ff","permalink":"https://forrt.org/glossary/german/open_data/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/german/open_data/","section":"glossary","summary":"","tags":null,"title":"Open Data (Offene Daten)","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"a21c189a882f3a93603b3e90aa5295d6","permalink":"https://forrt.org/curated_resources/open-developmental-science-an-overview-a/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/open-developmental-science-an-overview-a/","section":"curated_resources","summary":"The increasing adoption of open science practices in the last decade has been changing the scientific landscape across fields. However, developmental science has been relatively slow in adopting open science practices. To address this issue, we followed the format of Crüwell et al., (2019) and created summaries and an annotated list of informative and actionable resources discussing ten topics in developmental science: Open science; Reproducibility and replication; Open data, materials and code; Open access; Preregistration; Registered reports; Replication; Incentives; Collaborative developmental science. This article offers researchers and students in developmental science a starting point for understanding how open science intersects with developmental science. After getting familiarized with this article, the developmental scientist should understand the core tenets of open and reproducible developmental science, and feel motivated to start applying open science practices in their workflow.","tags":[""],"title":"Open Developmental Science: An Overview and Annotated Reading List","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"9daae2d697401644817c04d5b0be3cac","permalink":"https://forrt.org/curated_resources/open-education-in-promotion-tenure-and-f/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/open-education-in-promotion-tenure-and-f/","section":"curated_resources","summary":"This resource was developed by a working group from the Iowa Open Education Action Team (Iowa OER). Our team built upon DOERS3's OER in Tenure \u0026 Promotion Matrix to help faculty and staff advocate for the inclusion of open educational practices (OEP) in promotion, tenure, and faculty evaluation practices at their institutions. Below, you can find our main document, directions for interacting with the text, and handouts you can use or adapt for your own advocacy work.","tags":["Faculty Development","Promotion","Tenure"],"title":"Open Education in Promotion, Tenure, and Faculty Development","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1721390377,"objectID":"276729cf6084927c9bd7c05bbbcc5930","permalink":"https://forrt.org/glossary/english/open_educational_resources/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/english/open_educational_resources/","section":"glossary","summary":"","tags":null,"title":"Open Educational Resources (OER) Commons","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1720865851,"objectID":"6834a25e4bb435eee58d2dc3ab7a00eb","permalink":"https://forrt.org/glossary/german/open_educational_resources/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/german/open_educational_resources/","section":"glossary","summary":"","tags":null,"title":"Open Educational Resources (OER) Commons","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1635961284,"objectID":"a58b5d5d622c8e9251ebc6458ca0b18c","permalink":"https://forrt.org/glossary/vbeta/open-educational-resources-oer-comm/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/vbeta/open-educational-resources-oer-comm/","section":"glossary","summary":"","tags":null,"title":"Open Educational Resources (OER) Commons ","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1635961284,"objectID":"31c833ad305a8aae84adf40e7a49cf7c","permalink":"https://forrt.org/glossary/vbeta/open-educational-resources-oers/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/vbeta/open-educational-resources-oers/","section":"glossary","summary":"","tags":null,"title":"Open Educational Resources (OERs)","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"8940bc8e875b988a027ed8ec8b70cca5","permalink":"https://forrt.org/curated_resources/open-educational-resources-equitable-and/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/open-educational-resources-equitable-and/","section":"curated_resources","summary":"This research report describes findings from a study conducted to learn students’ experiences using open educational resources (OER) in an online nursing research course. Twenty-three students participated in a pilot study in which traditional textbooks were replaced with OER as the primary course material. Data were collected by administering open-ended and closed-ended questions about students’ experiences and demographic characteristics. Findings indicated that OER were cost-effective, accessible, easy to navigate, and interesting. The resources were adequate to meet student learning outcomes. Challenges experienced were related to poor Internet access for a few students.","tags":["Accelerated Nursing Education","Barriers to Higher Education","Higher Education Costs","Open Educational Resources","Textbooks"],"title":"Open Educational Resources: Equitable and Affordable Nursing Education","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"daecfada120738b444adc81ea5581e34","permalink":"https://forrt.org/curated_resources/open-for-insight-experimental-methods-op/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/open-for-insight-experimental-methods-op/","section":"curated_resources","summary":"A syllabus about open science and experimental methods","tags":["Reproducibility Crisis and Credibility Revolution","Open Science","Conceptual and statistical knowledge"],"title":"Open for Insight (Experimental Methods + Open Science)","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"d61c2c63027ee0e9835b2fdaa6062b0d","permalink":"https://forrt.org/curated_resources/open-hardware-in-microscopy/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/open-hardware-in-microscopy/","section":"curated_resources","summary":"The field of microscopy has been empowering humankind for many centuries by enabling the observation of objects that are otherwise too small to detect for the naked human eye. Microscopy techniques can be loosely divided into three main branches, namely photon-based optical microscopy, electron microscopy, and scanning probe microscopy with optical microscopy being the most prominent one. On the high-end level, optical microscopy nowadays enables nanometer resolution covering many scientific disciplines ranging from material sciences over the natural sciences and life sciences to the food sciences. On the lower-end level, simplified hardware and openly available description and blueprints have helped to make powerful microscopes widely available to interested scientists and researchers. For this special issue, we invited contributions from the community to share their latest ideas, designs, and research results on open-source hardware in microscopy. With this collection of articles, we hope to inspire the community to further increase the accessibility, interoperability, and reproducibility of microscopy. We further touch on the standardization of methodologies and devices including the use of computerized control of data acquisition and data analysis to achieve high quality and efficiency in research and development.","tags":["Open Microscopy","Open Source","Open Science","Open Hardware","Microscopy"],"title":"Open hardware in microscopy ","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"2d292e3a72499942b8c2b7b0413a5c99","permalink":"https://forrt.org/curated_resources/open-intro-statistics/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/open-intro-statistics/","section":"curated_resources","summary":"OpenIntro Statistics is a dynamic take on the traditional curriculum, being successfully used at Community Colleges to the Ivy League","tags":["Textbook","Open Textbook"],"title":"Open Intro Statistics\n","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1721390377,"objectID":"3bdb95803f0f715244c72081968bb7a6","permalink":"https://forrt.org/glossary/english/open_licenses/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/english/open_licenses/","section":"glossary","summary":"","tags":null,"title":"Open Licenses","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1635961284,"objectID":"fc145d434a5b0cc00228d043816fb44f","permalink":"https://forrt.org/glossary/vbeta/open-licenses/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/vbeta/open-licenses/","section":"glossary","summary":"","tags":null,"title":"Open Licenses","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1720865851,"objectID":"2fd08c737ef0d3652e7f520004a01609","permalink":"https://forrt.org/glossary/german/open_licenses/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/german/open_licenses/","section":"glossary","summary":"","tags":null,"title":"Open Licenses (Offene Lizenzen)","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1721390377,"objectID":"0cf32ea6d81ac2eb489c3df633ff7505","permalink":"https://forrt.org/glossary/english/open_material/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/english/open_material/","section":"glossary","summary":"","tags":null,"title":"Open Material","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1635961284,"objectID":"a0e3413fc7ebd7aabdb0588fb7f7d5a1","permalink":"https://forrt.org/glossary/vbeta/open-material/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/vbeta/open-material/","section":"glossary","summary":"","tags":null,"title":"Open Material","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1720865851,"objectID":"01ab0b2578b2b6065a19aecdfd29ac3f","permalink":"https://forrt.org/glossary/german/open_material/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/german/open_material/","section":"glossary","summary":"","tags":null,"title":"Open Material (offene Materialien)","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1721390377,"objectID":"e46ab6afc036d831d4079349c923e3c5","permalink":"https://forrt.org/glossary/english/open_peer_review/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/english/open_peer_review/","section":"glossary","summary":"","tags":null,"title":"Open Peer Review","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1635961284,"objectID":"e0f02cf42e39a5c857aa44d928dc3c65","permalink":"https://forrt.org/glossary/vbeta/open-peer-review/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/vbeta/open-peer-review/","section":"glossary","summary":"","tags":null,"title":"Open Peer Review","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1720865851,"objectID":"5ee0910e2bf280701a9007fa53ebf495","permalink":"https://forrt.org/glossary/german/open_peer_review/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/german/open_peer_review/","section":"glossary","summary":"","tags":null,"title":"Open Peer Review (Offene Begutachtung)","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"8f34843a5b1cecf89b74bcc310609e10","permalink":"https://forrt.org/curated_resources/open-peer-review-pros-and-cons-from-the/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/open-peer-review-pros-and-cons-from-the/","section":"curated_resources","summary":"Peer review is considered by many to be a fundamental component of scientific publishing. In this context, open peer review (OPR) has gained popularity in recent years as a tool to increase transparency, rigor, and inclusivity in research. But how does OPR really affect the review process? How does OPR impact specific groups, such as early career researchers? This editorial explores and discusses these aspects as well as some suggested actions for journals.","tags":["Open Peer Review","Early Career Researcher","Academic Life"],"title":"Open peer review, pros and cons from the perspective of an early career researcher","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"5680286c3c68b6156fd9dbb5c7d6cdea","permalink":"https://forrt.org/curated_resources/open-research-examples-of-good-practice/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/open-research-examples-of-good-practice/","section":"curated_resources","summary":"This is a document about open science across disciplines, together with resources to these disciplines.","tags":["Open science"],"title":"Open Research: Examples of good practice, and resources across disciplines","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1721390377,"objectID":"73f0cefdd036b0888e554d4024052643","permalink":"https://forrt.org/glossary/english/open_scholarship/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/english/open_scholarship/","section":"glossary","summary":"","tags":null,"title":"Open Scholarship","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1720865851,"objectID":"040254aaa4b2af48c4f7da99215bb26b","permalink":"https://forrt.org/glossary/german/open_scholarship/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/german/open_scholarship/","section":"glossary","summary":"","tags":null,"title":"Open Scholarship","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1635961284,"objectID":"5aee660b1a8119dd5c4735a9e35056fd","permalink":"https://forrt.org/glossary/vbeta/open-scholarship/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/vbeta/open-scholarship/","section":"glossary","summary":"","tags":null,"title":"Open Scholarship","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1721390377,"objectID":"f6c66c00f048b04eaed51cfd48922945","permalink":"https://forrt.org/glossary/english/open_scholarship_knowledge_base/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/english/open_scholarship_knowledge_base/","section":"glossary","summary":"","tags":null,"title":"Open Scholarship Knowledge Base","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1635961284,"objectID":"cddecd91c922ff8249ff24414491ba76","permalink":"https://forrt.org/glossary/vbeta/open-scholarship-knowledge-base/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/vbeta/open-scholarship-knowledge-base/","section":"glossary","summary":"","tags":null,"title":"Open Scholarship Knowledge Base ","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1720865851,"objectID":"2918b0a8255ad21989009d2567827ddf","permalink":"https://forrt.org/glossary/german/open_scholarship_knowledge_base_/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/german/open_scholarship_knowledge_base_/","section":"glossary","summary":"","tags":null,"title":"Open Scholarship Knowledge Base  (translated, reviewed)","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"d6ec1d96dc8179be23fbef461099183e","permalink":"https://forrt.org/curated_resources/open-scholarship-where-are-the-self-corr/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/open-scholarship-where-are-the-self-corr/","section":"curated_resources","summary":"In her talk, Professor Vazire covers the role of open practices in allowing science to work the way it was intended to work—through self correction. By being able to “look under the hood,” we are able to evaluate scientific claims in the way that they should be.","tags":["Research"],"title":"Open Scholarship: Where are the Self-Correcting Mechanisms of Science?","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1721390377,"objectID":"05fb0873ba557520ddf809c70508b4fd","permalink":"https://forrt.org/glossary/english/open_science/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/english/open_science/","section":"glossary","summary":"","tags":null,"title":"Open Science","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1635961284,"objectID":"1be61154bf6ff6c6aa7a3d78a12849f2","permalink":"https://forrt.org/glossary/vbeta/open-science/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/vbeta/open-science/","section":"glossary","summary":"","tags":null,"title":"Open Science","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1720865851,"objectID":"5c41565d9435843fd0fa6e2088080e27","permalink":"https://forrt.org/glossary/german/open_science/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/german/open_science/","section":"glossary","summary":"","tags":null,"title":"Open Science (Offene Forschung)","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"092a2c3857c41f03410530af0188d3e7","permalink":"https://forrt.org/curated_resources/open-science-inclusive-psychology/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/open-science-inclusive-psychology/","section":"curated_resources","summary":"This course is an introduction to the Open Science approach to psychology. We will investigate if how the field has experienced a “replicability crisis” and explore the potential structural and methodological factors that may be creating false positives within the psychological literature, using case studies of particular research topics in social/personality and cognitive psychology. Students will learn about advances in methods and novel approaches to conducting research that have been developed in response to critiques of past practices. The process of scientific publishing and alternative models of disseminating knowledge will be examined, along with issues of civil and productive scientific discourse and community-building in the social media era. We will discuss issues of inclusivity and accessibility in psychological science, as well as pathways to conducting research in academic and industry settings.","tags":["Reproducibility Crisis and Credibility Revolution","Open Science","Inclusivity","Diversity","Equity"],"title":"Open Science \u0026 Inclusive Psychology","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"80e58ad220b2c751ecc5ed1a40e7d846","permalink":"https://forrt.org/curated_resources/open-science-2-0-towards-a-truly-collabo/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/open-science-2-0-towards-a-truly-collabo/","section":"curated_resources","summary":"Conversations about open science have reached the mainstream, yet many open science practices such as data sharing remain uncommon. Our efforts towards openness therefore need to increase in scale and aim for a more ambitious target. We need an ecosystem not only where research outputs are openly shared but also in which transparency permeates the research process from the start and lends itself to more rigorous and collaborative research. To support this vision, this Essay provides an overview of a selection of open science initiatives from the past 2 decades, focusing on methods transparency, scholarly communication, team science, and research culture, and speculates about what the future of open science could look like. It then draws on these examples to provide recommendations for how funders, institutions, journals, regulators, and other stakeholders can create an environment that is ripe for improvement.","tags":["Open Science","Ecosystems","Reproducibility","Quality Control","Clinical Trials","Science Policy","Open Data","Research Quality Assessment"],"title":"Open Science 2.0: Towards a truly collaborative research ecosystem","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"b58d549fbe7c9bfb64f0457c5faa2adb","permalink":"https://forrt.org/curated_resources/open-science-and-methodological-improvem/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/open-science-and-methodological-improvem/","section":"curated_resources","summary":"In recent years, meta-research methods issues (e.g., reproducibility, open science, etc.) have yielded provocative findings, vigorous discussions, and novel innovations. This course will engage with these issues in ways that are primarily practical, i.e., useful to current graduate students’ research. However, doing this will require addressing some basic issues in measurement, statistical inference, ethics, and philosophy of science.Readings, discussions, and activities will provide tools that will help you improve your own research practices, and to better interpret the evidence in published reports. The standards, requirements, and values of publishing high quality research in psychology are changing quickly, and we will engage with the very latest developments in ways that will likely increase the quality and impact of your own work. We will also discuss the advantages and disadvantages of potential reforms to the way psychologists (and otherscientists) conduct and report on research.","tags":["Reproducibility Crisis and Credibility Revolution","Open Science"],"title":"Open Science and Methodological Improvements","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"c0b4008755f53023efbfaa4b2a9e8f16","permalink":"https://forrt.org/curated_resources/open-science-challenges-benefits-and-tip/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/open-science-challenges-benefits-and-tip/","section":"curated_resources","summary":"The movement towards open science is a consequence of seemingly pervasive failures to replicate previous research. This transition comes with great benefits but also significant challenges that are likely to affect those who carry out the research, usually early career researchers (ECRs). Here, we describe key benefits, including reputational gains, increased chances of publication, and a broader increase in the reliability of research. The increased chances of publication are supported by exploratory analyses indicating null findings are substantially more likely to be published via open registered reports in comparison to more conventional methods. These benefits are balanced by challenges that we have encountered and that involve increased costs in terms of flexibility, time, and issues with the current incentive structure, all of which seem to affect ECRs acutely. Although there are major obstacles to the early adoption of open science, overall open science practices should benefit both the ECR and improve the quality of research. We review 3 benefits and 3 challenges and provide suggestions from the perspective of ECRs for moving towards open science practices, which we believe scientists and institutions at all levels would do well to consider.","tags":["Careers","Data","Experimental Design","Neuroimaging","Open Data","Open Science","Peer Review","Reproducibility","Statistical Data"],"title":"Open science challenges, benefits and tips in early career and beyond","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1721390377,"objectID":"6f6d603cc856b73d63a45826c3a29583","permalink":"https://forrt.org/glossary/english/open_science_framework/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/english/open_science_framework/","section":"glossary","summary":"","tags":null,"title":"Open Science Framework","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1720865851,"objectID":"f05d07247272156c724ef5505045dc81","permalink":"https://forrt.org/glossary/german/open_science_framework/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/german/open_science_framework/","section":"glossary","summary":"","tags":null,"title":"Open Science Framework","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1635961284,"objectID":"1fe3a473b882c97c5b7ea74cced0ca8b","permalink":"https://forrt.org/glossary/vbeta/open-science-framework/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/vbeta/open-science-framework/","section":"glossary","summary":"","tags":null,"title":"Open Science Framework","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"f55258a8589559b334038395d5d68e45","permalink":"https://forrt.org/curated_resources/open-science-in-dementia-care-embedded-p/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/open-science-in-dementia-care-embedded-p/","section":"curated_resources","summary":"Few dementia care interventions have been translated to healthcare contexts for those who need them. Embedded pragmatic clinical trials (ePCTs) are one design that can expedite the timeframe of research translation to clinical practice. As the National Institutes of Health (NIH) and other funders commit immense new resources to increasing the nation’s capacity to conduct dementia care ePCTs, we call on psychologists to employ their extensive expertise in open science to improve the quality of dementia care ePCTs. This article provides several recommendations to enhance the transparency and reporting rigor of ePCTs in dementia care and other chronic disease contexts. We illustrate these recommendations in the context of a recent pilot pragmatic trial known as the Porchlight Project. Porchlight provided training to volunteers who serve clients and caregivers to help them provide more “dementia capable” support. Notably this trial did not include a special effort to make use of open science practices. We discuss the benefits and costs had the Porchlight Project incorporated open science principles.","tags":["Embedded Pragmatic Clinical Trials","Open Science","Dementia Care"],"title":"Open science in dementia care embedded pragmatic clinical trials","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"a58cb0ccf9d302b025fce214c7ba49ac","permalink":"https://forrt.org/curated_resources/open-science-in-latin-america/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/open-science-in-latin-america/","section":"curated_resources","summary":"Note: This webinar was presented in Spanish. The slides presented during this webinar can be found here:https://osf.io/6qnse/ The slides presented during this seminar can be found here: https://osf.io/6qnse/ Este seminario web se centrará en el estado de la ciencia abierta en América Latina, desde los esfuerzos de los investigadores individuales para abrir sus flujos de trabajo, herramientas para ayudar a los investigadores a ser abiertos y nuevas redes e iniciativas prometedoras en ciencia abierta. Ricardo Hartley (@ametodico) es profesor de metodología de la investigación de la Universidad Central de Chile, investigador en biología de la reproducción y en comunicación - valoración del conocimiento. Organizador de las OpenCon Santiago 2016 y 2017 y embajador COS. Erin McKiernan es profesora del Departamento de Física, Programa de Física Biomédica de la Universidad Nacional Autónoma de México. También es la fundadora del Why Open Research? proyecto, un sitio educativo para que los investigadores aprendan cómo compartir su trabajo, financiado en parte por la Fundación Shuttleworth. Fernan Federici Noe es profesor asistente e investigador de la Universidad Católica de Chile y fellow internacional del OpenPlant Synthetic Biology Center, University of Cambridge. Fernan es miembro del Global For Open Science Hardware (GOSH) y TECNOx (www.tecnox.org).","tags":["Analysis","Data","Education","Materials","Open Scholarship Tools and Technologies","OSF","Publishing","Reproducibility","Research Data Management Tools","Researchers"],"title":"Open Science in Latin America","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1689122843,"objectID":"416bd8407c744ae74aafcd7229528f5c","permalink":"https://forrt.org/curated_resources/open-science-in-sport-and-exercise-psych/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/open-science-in-sport-and-exercise-psych/","section":"curated_resources","summary":"Open science practices including open access (OA) publication, open methods, study preregistration, and open data are gaining acceptance across diverse fields of research. These practices are promoted as strategies to improve the reproducibility of research findings and the replicability of studies to accumulate knowledge and advance science. However, these arguments may raise concerns for qualitative researchers, and open science practices pose several challenges for qualitative researchers. The purpose of this paper is: (1) to review the state of open science practices within sport and exercise psychology, and (2) to discuss the implications of open science for qualitative inquiry. We examined open science practices across quantitative and qualitative articles in 11 sport and exercise psychology journals. While OA publication is a relatively recent phenomenon, OA articles were cited slightly more often than non-OA articles, although this difference was not significant. Some researchers provided supplementary materials alongside published articles, but researchers do not appear to be openly sharing the methods and data from their studies. No articles were published as preregistered studies at the time of our review. Some benefits of open science practices for qualitative inquiry include transparent documentation of the research process, opportunities for collaborative and pluralistic analyses, access to data across multiple research sites and from difficult-to-access settings and participants, and opportunities for teaching qualitative inquiry. We conclude by addressing several key questions including participant consent, confidentiality and anonymity, analyzing de-contextualized qualitative data, storing and accessing data, study preregistration, and the principle of emergent design within qualitative inquiry.","tags":["Reproducibility Crisis and Credibility Revolution","Open Science"],"title":"Open Science in Sport and Exercise Psychology: Review of Current Approaches and Considerations for Qualitative Inquiry","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"3fdad08dc6b3ebd949df25cbf2724e29","permalink":"https://forrt.org/curated_resources/open-science-is-for-aging-research-too/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/open-science-is-for-aging-research-too/","section":"curated_resources","summary":"In response to concerns about the replicability of published research, some disciplines have used open science practices to try to enhance the credibility of published findings. Gerontology has been slow to embrace these changes. We argue that open science is important for aging research, both to reduce questionable research practices that may also be prevalent in the field (such as too many reported significant age differences in the literature, underpowered studies, hypothesizing after the results are known, and lack of belief updating when findings do not support theories), as well as to make research in the field more transparent overall. To ensure the credibility of gerontology research moving forward, we suggest concrete ways to incorporate open science into gerontology research: for example, by using available preregistration templates adaptable to a variety of study designs typical for aging research (even secondary analyses of existing data). Larger sample sizes may be achieved by many-lab collaborations. Though using open science practices may make some aspects of gerontology research more challenging, we believe that gerontology needs open science to ensure credibility now and in the future.","tags":["Aging Science"],"title":"Open Science is for Aging Research, Too","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"7cfea7095434d74f1bfa2375e61176b0","permalink":"https://forrt.org/curated_resources/open-science-made-easy/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/open-science-made-easy/","section":"curated_resources","summary":"7 steps towards transparent and reproducible research","tags":["Analysis","Reproducibility","Researchers"],"title":"Open Science Made Easy","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"642189349c012faffa30c9e8b8d50c1a","permalink":"https://forrt.org/curated_resources/open-science-manual/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/open-science-manual/","section":"curated_resources","summary":"About This Document: This manual was assembled and is being updated by Professor Benjamin Le (@benjaminle), who is on the faculty in the Department of Psychology at Haverford College. The primary goal of this text is to provide guidance to his senior thesis students on how to conduct research in his lab by working within general principles that promote research transparency using the specific open science practices described here. While it is aimed at undergraduate psychology students, hopefully it will be of use to other faculty/researchers/students who are interested in adopting open science practices in their labs.","tags":["Data","Materials","Policy","Publishing","Reproducibility","Researchers","Students"],"title":"Open Science Manual","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"d6146fb56ba060f5513cc9fff3ed6147","permalink":"https://forrt.org/curated_resources/open-science-practices-are-on-the-rise-t/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/open-science-practices-are-on-the-rise-t/","section":"curated_resources","summary":"Has there been meaningful movement toward open science practices within the social sciences in recent years? Discussions about changes in practices such as posting data and pre-registering analyses have been marked by controversy—including controversy over the extent to which change has taken place. This study, based on the State of Social Science (3S) Survey, provides the first comprehensive assessment of awareness of, attitudes towards, perceived norms regarding, and adoption of open science practices within a broadly representative sample of scholars from four major social science disciplines: economics, political science, psychology, and sociology. We observe a steep increase in adoption: as of 2017, over 80% of scholars had used at least one such practice, rising from one quarter a decade earlier. Attitudes toward research transparency are on average similar between older and younger scholars, but the paceof change differs by field and methodology. According with theories of normal science and scientific change, the timing of increases in adoption coincides with technological innovations and institutional policies. Patterns are consistent with most scholars underestimating the trend toward open science in their discipline.","tags":["Data"],"title":"Open Science Practices are on the Rise: The State of Social Science (3S) Survey","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"22ff39e610e8fd98d4623bab67b94ada","permalink":"https://forrt.org/curated_resources/open-science-practices-need-substantial/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/open-science-practices-need-substantial/","section":"curated_resources","summary":"Objective: To describe the frequency of open science practices in a contemporary sample of studies developing prognostic models using machine-learning methods in the field of oncology.   \n\nStudy design and setting: We conducted a systematic review, searching the MEDLINE database between 01/12/2022 and 31/12/2022 for studies developing a multivariable prognostic model using machine-learning methods (as defined by the authors) in oncology. Two authors independently screened records and extracted open science practices.  \n\nResults: We identified 46 publications describing the development of a multivariable prognostic model. The adoption of open science principles was poor. Only one study reported availability of a study protocol, and only one study was registered. Funding statements and conflicts of interest statements were common. Thirty-five studies (76%) provided data-sharing statements, with 21 (46%) indicating data were available on request to the authors and 7 declaring data sharing was not applicable. Two studies (4%) shared data. Only 12 studies (26%) provided code-sharing statements, including 2 (4%) that indicated the code was available on request to the authors. Only 11 studies (24%) provided sufficient information to allow their to model to be used in practice. The use of reporting guidelines was rare: 8 studies (18%) mentioning using a reporting guideline, with 4 (10%) using the TRIPOD statement, 1 (2%) using MI-CLAIM and CONSORT-AI, 1 (2%) using STROBE, 1 (2%) using STARD, and 1 (2%) using TREND.  \n\nConclusion: The adoption of open science principles in oncology studies developing prognostic models using machine-learning methods is poor. Guidance and an increased awareness of benefits and best practices of open science is needed for prediction research in oncology. ","tags":["Open Science","Prognosis","Machine Learning","Reporting","Data Sharing","Code Sharing"],"title":"Open Science Practices Need Substantial Improvement in Prognostic Model Studies in Oncology Using Machine Learning","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"2002e197e85d8682e3c70d5c7718b333","permalink":"https://forrt.org/curated_resources/open-science-saves-lives-lessons-from-th/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/open-science-saves-lives-lessons-from-th/","section":"curated_resources","summary":"In the last decade Open Science principles, such as Open Access, study preregistration, use of preprints, making available data and code, and open peer review, have been successfully advocated for and are being slowly adopted in many different research communities. In response to the COVID-19 pandemic many publishers and researchers have sped up their adoption of some of these Open Science practices, sometimes embracing them fully and sometimes partially or in a sub-optimal manner. In this article, we express concerns about the violation of some of the Open Science principles and its potential impact on the quality of research output. We provide evidence of the misuses of these principles at different stages of the scientific process. We call for a wider adoption of Open Science practices in the hope that this work will encourage a broader endorsement of Open Science principles and serve as a reminder that science should always be a rigorous process, reliable and transparent, especially in the context of a pandemic where research findings are being translated into practice even more rapidly. We provide all data and scripts at https://osf.io/renxy/.","tags":["Open science"],"title":"Open Science Saves Lives: Lessons from the COVID-19 Pandemic","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"5d0cd630ddae6882aa120826f0a4cf3b","permalink":"https://forrt.org/curated_resources/open-science-seminar-how-to-do-credible/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/open-science-seminar-how-to-do-credible/","section":"curated_resources","summary":"Students have an overview about the \"historical\" developments and debates of the replication movement in the last years, understand questionable research practices (QRPs) and publication bias and how their prevalence distorts the scientific record.•can judge the quality of (set of) studies based on cues and formally (p-curve, R-index, power, etc.).•know best practices for a reproducible workflow using Rmarkdown, Open Science Framework, and other tools.","tags":["Reproducibility Crisis and Credibility Revolution","Open Science","Conceptual and statistical knowledge"],"title":"Open Science Seminar: How to do credible research with a high informational value (and how not to do it)","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"3c464f5b3d9c28c9ff89cbe69e278bde","permalink":"https://forrt.org/curated_resources/open-science-session-gsa-2019/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/open-science-session-gsa-2019/","section":"curated_resources","summary":"Open Science Session GSA 2019","tags":["Aging","Gerontology","Open Science"],"title":"Open Science Session GSA 2019","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"5a36f6d402822cef43e302a6120740f1","permalink":"https://forrt.org/curated_resources/open-science-takes-on-the-coronavirus-pa/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/open-science-takes-on-the-coronavirus-pa/","section":"curated_resources","summary":"Data sharing, open-source designs for medical equipment, and hobbyists are all being harnessed to combat COVID-19.","tags":["Open Science","COVID-19","Pandemic"],"title":"Open science takes on the coronavirus pandemic","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1689122843,"objectID":"316d2e646a5fe19ed66949bb7683f229","permalink":"https://forrt.org/curated_resources/open-science-toolbox/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/open-science-toolbox/","section":"curated_resources","summary":"There is a vast body of helpful tools that can be used in order to foster Open Science practices. For reasons of clarity, this toolbox aims at providing only a selection of links to these resources and tools. Our goal is to give a short overview on possibilities of how to enhance your Open Science practices without consuming too much of your time.","tags":["Data","Materials","Open Scholarship Guidelines","Policy","Publishing","Reproducibility","Researchers"],"title":"Open Science Toolbox","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"75318e96518269e08188d484dc7b992d","permalink":"https://forrt.org/curated_resources/open-science-training-in-triple/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/open-science-training-in-triple/","section":"curated_resources","summary":"This case study focuses on the online training activities on Open Science delivered within the H2020 project Transforming Research through Innovative Practices for Linked Interdisciplinary Exploration (TRIPLE, Grant Agreement 863420). The project is dedicated to building a discovery platform for the Social Sciences and Humanities (SSH) and is committed to promoting and supporting the uptake of Open Science within research practices.\n\nIn order to address SSH research and training communities’ needs for enhanced competencies on Open Science and for stronger support in the Findable, Accessible, Interoperable, Reusable (FAIR) management of digital training materials, two reusable outputs were produced. The work carried out is presented as a novel approach to tackle the issues related to FAIRifying research and training practices and to create training resources whose reusability and relevance reaches beyond the project lifetime and framework. The case study presents the methods by which the results were produced so as to encourage and enable their future adaptation and reuse.\n\nThe TRIPLE Open Science training series (result 1) targets SSH researchers, research support personnel and infrastructure developers in need of practical tools and specific skills to integrate Open Science practices in their workflows. The training series provides 12 competence-oriented online training events in Open Access whose training materials are available as Open Educational Resources (OER).\n\nThe TRIPLE Training Toolkit (result 2) targets training organisers and research performing organisations who wish to design and manage training events as OERs and increase the impact of their training following good practice. The Toolkit is an easily reproducible workflow designed to help trainers minimise the time they spend in managing training events following FAIR practice. The workflow follows a FAIR-by-design method to address the frequent findability and reusability issues related to the management of digital training resources.","tags":["Open Science","Training and Education","FAIR data","EOSC","Scholarly Practice","Project Management"],"title":"Open Science Training in TRIPLE","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"f08029f0e3c2d51a74a60dce27411ebb","permalink":"https://forrt.org/curated_resources/open-science-workshop-materials-of-the-l/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/open-science-workshop-materials-of-the-l/","section":"curated_resources","summary":"Open Science workshop materials created at LMU Munich, available for everyone under a CC-BY license. Look in the README folder for more information.","tags":["Reproducibility Crisis and Credibility Revolution","Open Science","Conceptual and statistical knowledge"],"title":"Open Science Workshop Materials of the LMU Open Science Center","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1689122843,"objectID":"1060ab2b07080585ae989a22af5b2275","permalink":"https://forrt.org/curated_resources/open-science-many-hands-make-light-work/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/open-science-many-hands-make-light-work/","section":"curated_resources","summary":"Open science has been highlighted as one of the priorities of the Dutch presidency of the European Union in 2016. Matthew Dovey discusses the motivations behind the open science movement and why initiatives to support it are more important than ever. Read his original blog.","tags":["Podcast","Reproducibility Crisis and Credibility Revolution"],"title":"Open science: many hands make light work","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1689122843,"objectID":"07b4ee5ccb70f75bb0630e5201b08484","permalink":"https://forrt.org/curated_resources/open-science-what-why-and-how/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/open-science-what-why-and-how/","section":"curated_resources","summary":"Open Science is a collection of actions designed to make scientific processes more transparent and results more accessible. Its goal is to build a more replicable and robust science; it does so using new technologies, altering incentives, and changing attitudes. The current movement towards open science was spurred, in part, by a recent “series of unfortunate events” within psychology and other sciences. These events include the large number of studies that have failed to replicate and the prevalence of common research and publication procedures that could explain why. Many journals and funding agencies now encourage, require, or reward some open science practices, including pre-registration, providing full materials, posting data, distinguishing between exploratory and confirmatory analyses, and running replication studies. Individuals can practice and encourage open science in their many roles as researchers, authors, reviewers, editors, teachers, and members of hiring, tenure, promotion, and awards committees. A plethora of resources are available to help scientists, and science, achieve these goals.","tags":["Reproducibility Crisis and Credibility Revolution","Open Science"],"title":"Open Science: What, Why, and How","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1689122843,"objectID":"d71e890d1ee967e314510d1507092f71","permalink":"https://forrt.org/curated_resources/open-sharing-of-data-on-close-relationsh/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/open-sharing-of-data-on-close-relationsh/","section":"curated_resources","summary":"This article reports on an adversarial (but friendly) collaboration examining the issues that lie at the intersection of confidentiality and open-data practices. We describe the process we followed to share our data for a speed-dating article we recently published in Psychological Science (Joel, Eastwick, \u0026 Finkel, 2017) and provide a summary of the issues we considered and addressed along the way. As we drafted the present article, the third author became unsure, in retrospect, about some of the procedures we had followed, especially if our approach were to be perceived as a model for open-data decisions in other, more typical cases involving nonindependent data. This article addresses these concerns, but also identifies areas of consensus. All three authors agree that there remains an unmet need for guidelines and other resources to help researchers address the challenges of sharing data that cover sensitive topics, particularly nonindependent data collected from pairs and groups (e.g., romantic couples, work teams, therapy groups). We conclude with a discussion of new tools that could be developed to help scholars who have collected such data to increase the transparency of their research while simultaneously protecting the confidentiality of the participants.","tags":["Reproducibility Crisis and Credibility Revolution","Open Science"],"title":"Open sharing of data on close relationships and other sensitive social psychological topics: Challenges, tools, and future directions","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"50bc40f716074ee781f375bcae8ddf71","permalink":"https://forrt.org/curated_resources/open-source-guides/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/open-source-guides/","section":"curated_resources","summary":"Open Source Guides (https://opensource.guide/) are a collection of resources for individuals, communities, and companies who want to learn how to run and contribute to an open source project.\n\nBackground: Open Source Guides were created and are curated by GitHub, along with input from outside community reviewers, but they are not exclusive to GitHub products. One reason we started this project is because we felt that there weren't enough resources for people creating open source projects.\n\nOur goal is to aggregate community best practices, not what GitHub (or any other individual or entity) thinks is best. Therefore, we try to use examples and quotations from others to illustrate our points.","tags":["Analysis","Open Source","Researchers","Software Engineers"],"title":"Open Source Guides","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1721390377,"objectID":"e1ad4cddcd31071ab2a6f45b0db8054a","permalink":"https://forrt.org/glossary/english/open_source_software/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/english/open_source_software/","section":"glossary","summary":"","tags":null,"title":"Open Source software","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1635961284,"objectID":"24d056e5faf51cd8c4b06d0426e814ac","permalink":"https://forrt.org/glossary/vbeta/open-source-software/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/vbeta/open-source-software/","section":"glossary","summary":"","tags":null,"title":"Open Source software","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1720865851,"objectID":"85b84b39610b3990c4f58ee450c1ae5d","permalink":"https://forrt.org/glossary/german/open_source_software/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/german/open_source_software/","section":"glossary","summary":"","tags":null,"title":"Open Source software (Software mit offenem Quellcode)","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1689122843,"objectID":"c032abc6c8ac7577dde4dc3ed458ac6a","permalink":"https://forrt.org/curated_resources/open-stats-lab/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/open-stats-lab/","section":"curated_resources","summary":"A website about open statistic labs with data and activities","tags":["Website"],"title":"open stats lab","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1721390377,"objectID":"eaab812a5c3b2506fba5efc58bd07e9d","permalink":"https://forrt.org/glossary/english/open_washing/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/english/open_washing/","section":"glossary","summary":"","tags":null,"title":"Open washing","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1720865851,"objectID":"80705559f82ee8e6a1a45fe74064564a","permalink":"https://forrt.org/glossary/german/open_washing/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/german/open_washing/","section":"glossary","summary":"","tags":null,"title":"Open washing","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1635961284,"objectID":"d6ed33ff3436b8b1bc760d2ac0664e6c","permalink":"https://forrt.org/glossary/vbeta/open-washing/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/vbeta/open-washing/","section":"glossary","summary":"","tags":null,"title":"Open washing","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"08fc9c8e94c175c18f61dacb975f628c","permalink":"https://forrt.org/curated_resources/open-rigorous-and-reproducible-research/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/open-rigorous-and-reproducible-research/","section":"curated_resources","summary":"This book starts from the premise that there is a lot we can all do to increase the benefits of research.\n\nLet’s consider the main limitations of research that is not carried out and shared in an open, transparent, and reproducible way:\n\nIf papers are published in venues that are only available to those who pay for access, the vast majority of the world will not be able to see the output of all the work that went into producing it; this limits the potential reach and benefit to others.\n\nBecause of the complexity involved in many analyses, it is nearly impossible to describe every detail and choice that went into an analysis in the main paper; without accompanying code, it can be very very difficult for others to be certain about exactly what was done.\n\nEven if code is made available, there can be additional challenges to reproducing or re-analyzing past work, such as inaccessible data or deprecated software.\n\nIf others are not able to easily re-analyze past work, that limits the ability of the community to explore other analysis pathways, combine datasets, attempt to generalize experiments to new settings, etc.\n\nIf experiments are carried out without proper care in experiment design and analysis, there are likely to be more erroneous findings in the literature, making it harder for everyone to make sense of the object of study.\n\nThe more that new researchers have to wade through results that may not be credible, they more they are delayed from making genuine advances\n\nOf course, there are numerous reasons why people don’t put more effort into making their work open, transparent, and reproducible:\n\nPerhaps most importantly, doing so does require some additional work, and current incentive structures do not necessarily reward these efforts; however, this is changing in many fields, and certain communities place a lot of value on such things. Moreover, the cost of mistakes can be high, and this sort of openness helps to avoid them.\n\nSome data is legitimately not possible to share, due to concerns about privacy, copyright, or other considerations. Using such data will generally be less useful to the world than using more open data, but some work will of course require it. However, there are still things that can be done to avoid the worst problems, including being transparent about the analyses carried out, the protocol for collecting data, and other techniques such as pre-registration, which can bolster people’s confidence in a piece of work.\n\nMany people worry that making their data and code open to the world will expose them to risk or ridicule, either because they fear they have made mistakes, or they think it will reveal them to be a poor coder. This is understandable, but generally misplaced. It is better to catch errors early. Moreover, most people will be happy if you share any code, no matter how bad it is, and doing so is one of the best ways to improve, especially if you begin with the end in mind.\n\nFinally, many people don’t know where to start. Most guides to open science and reproducibility take the form of complete books or corpus, and try to teach an entire philosophy and comprehensive approach to research, which can be overwhelming.\n\nIn this document, we take a different approach. Our main goal here is to show how there are many ways to make your research more open, transparent, and reproducible on the margin, and that each step in that direction may bring some benefit. While there will always be nuances and requirements specific to each field, in general there is a great deal that we can learn from each other, and most ideas can be applied to any domain.\n\nIn summary, this handbook is a guide to making science more open, transparent, and reproducible by presenting best practices in a way that is:\n\nmodular: individual ideas can be used separately or combined\npractical: focused on the most tractable and impactful practices\ngeneral: applicable to any field that works with data and statistical analysis\nconcise: aimed at the busy scientists who doesn’t have time to take a full course right now\nWe break this guide down into three mains sections. Each section contains many modular components, each of which can be considered and used independently or in combination with the others:\n\nSection 1: Careful study design to help ensure and demonstrate that results and conclusions are valid and useful:\n\nThoughtful determination of experimental parameters, such as using power analysis to estimate an appropriate sample size\nDistinguishing between exploratory and confirmatory research\nPre-analysis planning of statistical analyses\nEnsuring that all relevant data is collected in order to be comparable with past work\nAdditional considerations, such as pre-registration, planning for potential problems, and consideration of ethical implications.\nSection 2: Adopting best practices in analyzing data and reporting results:\n\nPreliminary: decisions and considerations before working with any data.\nStatistical analysis plan: plan your analytic approach beforehand.\nData generation: generate an appropriate set of data.\nData preparation: transparently prepare your data for data analysis.\nData visualization: visualize all data using informative visualizations.\nData summarization: summarize all data using appropriate statistics.\nData analysis: analyze all data and avoid common blunders.\nData analysis - medicine: a few more considerations for medical research.\nStatistical analysis report: report transparently and comprehensively.\nExamples: published literature exemplifying principles of this manual.\nSection 3: Making relevant research materials available to all:\n\nOpen Data: making the raw data available for further research and replication\nOpen Source Code: making the analysis pipeline transparent and available for others to borrow or verify\nReproducible Environments: making not just the data and code available for others, but making it easy for them to re-run the analysis in an easily reproducible manner\nOpen Publication Models such that anyone can see the scholarly output associated with the work\nDocumenting Processes and Decisions: making it clear to interested parties not only what was done and how, but also why, by mechanisms such as open lab notebooks\nIn addition, appendices cover additional resources, such as frequently asked questions, discipline-specific considerations and linked to additional resources (of which there are plenty!)\n","tags":["Textbook","Open Textbook"],"title":"Open, rigorous and reproducible research: A practitioner’s handbook","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"5bd164add7f1dec0b6d811a2c2b9eac7","permalink":"https://forrt.org/curated_resources/openaccess-net/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/openaccess-net/","section":"curated_resources","summary":"The open-access.net platform provides comprehensive information on the subject of Open Access (OA) and offers practical advice on its implementation. Developed collaboratively by the Freie UniversitÃ¤t Berlin and the Universities of Goettingen, Konstanz, and Bielefeld, open-access.net first went online at the beginning of May 2007. The platform's target groups include all relevant stakeholders in the science sector, especially the scientists and scholars themselves, university and research institution managers, infrastructure service providers such as libraries and data centres, and funding agencies and policy makers. open-access.net provides easy, one-stop access to comprehensive information on OA. \n \n Aspects covered include OA concepts, legal, organisational and technical frameworks, concrete implementation experiences, initiatives, services, service providers, and position papers. The target-group-oriented and discipline-specific presentation of the content enables users to access relevant themes quickly and efficiently. Moreover, the platform offers practical implementation advice and answers to fundamental questions regarding OA.\n In collaboration with cooperation partners in Austria (the University of Vienna) and Switzerland (the University of Zurich), country-specific web pages for these two countries have been integrated into the platform - especially in the Legal Issues section.\n \n Each year since 2007, the information platform has organised the \"Open Access Days\" at alternating venues in collaboration with local partners. This event is the key conference on OA and Open Science in the German-speaking area.\n \n With funding from the Ministry of Science, Research and the Arts (MWK) of the State of Baden-WÃ¼rttemberg, the platform underwent a complete technical and substantive overhaul in 2015.","tags":["Funders","Librarians","Open Access","Open Access Policies","Policy","Policy Makers","Publishers","Publishing","Research Administration","Researchers"],"title":"OpenAccess.net","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1689122843,"objectID":"fd5e3f587fae4c1f62036611a7f55572","permalink":"https://forrt.org/curated_resources/opening-science/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/opening-science/","section":"curated_resources","summary":"The Evolving Guide on How the Internet is Changing Research, Collaboration and Scholarly Publishing","tags":["Reproducibility Crisis and Credibility Revolution","Open Science"],"title":"Opening Science","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1721390377,"objectID":"e9fea32688cf22c621c6952783ce7ac3","permalink":"https://forrt.org/glossary/english/openneuro/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/english/openneuro/","section":"glossary","summary":"","tags":null,"title":"OpenNeuro","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1720865851,"objectID":"d0e00d2353d574bfe31e949cff388afd","permalink":"https://forrt.org/glossary/german/openneuro/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/german/openneuro/","section":"glossary","summary":"","tags":null,"title":"OpenNeuro","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1635961284,"objectID":"d0787a44a9cda5a3803dccaa356e7e58","permalink":"https://forrt.org/glossary/vbeta/openneuro/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/vbeta/openneuro/","section":"glossary","summary":"","tags":null,"title":"OpenNeuro","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"f9f5880eaa2ef4b2c64ca0f3298ce67a","permalink":"https://forrt.org/curated_resources/openrefine-for-social-science-data/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/openrefine-for-social-science-data/","section":"curated_resources","summary":"Lesson on OpenRefine for social scientists. A part of the data workflow is preparing the data for analysis. Some of this involves data cleaning, where errors in the data are identifed and corrected or formatting made consistent. This step must be taken with the same care and attention to reproducibility as the analysis. OpenRefine (formerly Google Refine) is a powerful free and open source tool for working with messy data: cleaning it and transforming it from one format into another. This lesson will teach you to use OpenRefine to effectively clean and format data and automatically track any changes that you make. Many people comment that this tool saves them literally months of work trying to make these edits by hand.","tags":["Analysis","Data","Education","OpenRefine","Open Scholarship Tools and Technologies","Reproducibility","Research Data Management Tools","Researchers"],"title":"OpenRefine for Social Science Data","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"4873313235c4f4ce845b0404dba2a94f","permalink":"https://forrt.org/curated_resources/optimizing-research-collaboration/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/optimizing-research-collaboration/","section":"curated_resources","summary":"In this webinar, we demonstrate the OSF tools available for contributors, labs, centers, and institutions that support stronger collaborations. The demo includes useful practices like: contributor management, the OSF wiki as an electronic lab notebook, using OSF to manage online courses and syllabi, and more. Finally, we look at how OSF Institutions can provide discovery and intelligence gathering infrastructure so that you can focus on conducting and supporting exceptional research. The Center for Open Science’s ongoing mission is to provide community and technical resources to support your commitments to rigorous, transparent research practices. Visit cos.io/institutions to learn more.","tags":["Analysis","Data","Education","Materials","Open Scholarship Tools and Technologies","OSF","Publishing","Reproducibility","Research Data Management Tools","Researchers"],"title":"Optimizing Research Collaboration","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"d782f23515252ca4bb1c73720d485d8b","permalink":"https://forrt.org/curated_resources/optimizing-research-collaboration-for-re/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/optimizing-research-collaboration-for-re/","section":"curated_resources","summary":"Many in the global research community are adapting to conducting work remotely while exploring the best ways to maintain collaboration with colleagues across teams and institutions.\n\nJoin us as we discuss the OSF tools available for contributors, labs, centers, and institutions that support stronger collaborations.\n\nWe demonstrate: contributor management, the OSF wiki as an electronic lab notebook, how to affiliate research projects for institution-wide discovery, using OSF to manage online courses and syllabi, and more. Plus, see examples from the research teams optimizing their workflows for inclusive collaboration and efficient data management.","tags":["Research"],"title":"Optimizing Research Collaboration for Remote Teams","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1689122843,"objectID":"a9065dd49b7b7892c7a5c55e66fe2b7e","permalink":"https://forrt.org/curated_resources/optimizing-research-payoff/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/optimizing-research-payoff/","section":"curated_resources","summary":"In this article, we present a model for determining how total research payoff depends on researchers’ choices of sample sizes, α levels, and other parameters of the research process. The model can be used to quantify various tradeoffs inherent in the research process and thus to balance competing goals, such as (a) maximizing both the number of studies carried out and also the statistical power of each study, (b) minimizing the rates of both false positive and false negative findings, and (c) maximizing both replicability and research efficiency. Given certain necessary information about a research area, the model can be used to determine the optimal values of sample size, statistical power, rate of false positives, rate of false negatives, and replicability, such that overall research payoff is maximized. More specifically, the model shows how the optimal values of these quantities depend upon the size and frequency of true effects within the area, as well as the individual payoffs associated with particular study outcomes. The model is particularly relevant within current discussions of how to optimize the productivity of scientific research, because it shows which aspects of a research area must be considered and how these aspects combine to determine total research payoff.","tags":[""],"title":"Optimizing Research Payoff","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"ecd283a92cb33042aae6b43d22494827","permalink":"https://forrt.org/curated_resources/optimizing-the-methodology-of-human-slee/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/optimizing-the-methodology-of-human-slee/","section":"curated_resources","summary":"Understanding the complex relationship between sleep and memory consolidation is a major challenge in cognitive neuroscience and psychology. Many studies suggest that sleep triggers off-line memory processes, resulting in less forgetting of declarative memory and performance stabilization in non-declarative memory. However, the role of sleep in human memory consolidation is still under considerable debate, and numerous contradictory and non-replicable findings have been reported. Methodological issues related to experimental designs, task characteristics and measurements, and data-analysis practices all influence the effects that are observed and their interpretation. In this Perspective, we review methodological issues in sleep and memory studies and suggest constructive solutions to address them. We believe that implementing these solutions in future sleep and memory research will substantially advance the field and improve understanding of the specific role of sleep in memory consolidation.","tags":["Sleep","Memory","Consolidation","Methodology"],"title":"Optimizing the methodology of human sleep and memory research","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1721390377,"objectID":"d6875a11bcc208d560ea79a6ced72e2c","permalink":"https://forrt.org/glossary/english/optional_stopping/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/english/optional_stopping/","section":"glossary","summary":"","tags":null,"title":"Optional Stopping","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1635961284,"objectID":"81aa86f450412adfbca0df04178240c7","permalink":"https://forrt.org/glossary/vbeta/optional-stopping/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/vbeta/optional-stopping/","section":"glossary","summary":"","tags":null,"title":"Optional Stopping","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1720865851,"objectID":"1c5f840cc73f94c6d954fdaf2bb7e3ef","permalink":"https://forrt.org/glossary/german/optional_stopping/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/german/optional_stopping/","section":"glossary","summary":"","tags":null,"title":"Optional Stopping (Optionales Stoppen)","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"3f19f78a8bb7683354b0881c6027786c","permalink":"https://forrt.org/curated_resources/orcc-ukrn-primer-on-working-in-open-rese/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/orcc-ukrn-primer-on-working-in-open-rese/","section":"curated_resources","summary":"This is an introductory guide for those working and considering working in the area of open research. It was drafted by members of the Open Research Competencies Coalition. There are many resources available on the topic of open research either aimed at those working in open research roles or more generally on open research practices. A list is included in the document.\n\n","tags":["collaboration","data management","introductory guide","open research","outputs reproducibility research","UKRN"],"title":"ORCC UKRN Primer on Working in Open Research","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1721390377,"objectID":"e2ca471c47c8432191e1980a32476a85","permalink":"https://forrt.org/glossary/english/orcid/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/english/orcid/","section":"glossary","summary":"","tags":null,"title":"ORCID (Open Researcher and Contributor ID)","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1720865851,"objectID":"d5b33381edb6268694df9bac68897803","permalink":"https://forrt.org/glossary/german/orcid/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/german/orcid/","section":"glossary","summary":"","tags":null,"title":"ORCID (Open Researcher and Contributor ID)","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1635961284,"objectID":"86bf554fc5c0a974858ab2467c15c0c6","permalink":"https://forrt.org/glossary/vbeta/orcid-open-researcher-and-contribut/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/vbeta/orcid-open-researcher-and-contribut/","section":"glossary","summary":"","tags":null,"title":"ORCID (Open Researcher and Contributor ID)","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"3002945aa9597b19658b809e7ca12998","permalink":"https://forrt.org/curated_resources/osf-collections-supporting-research-disc/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/osf-collections-supporting-research-disc/","section":"curated_resources","summary":"The OSF Collections repository platform supports the discoverability and reuse of research by enabling the aggregation of related projects across OSF. With OSF Collections, any funder, journal, society, or research community can show their commitment to scientific integrity by aggregating the open outputs from their disciplines, grantees, journal articles, or more. Learn how research collections can foster new norms for sharing, collaboration, and reproducibility.\n\nWe also provide a demo of how OSF Collections aggregates and hosts your research by discipline, funded outcomes, project type, journal issue, and more.\n\nVisit cos.io/collections to learn more.","tags":["Center for Open Science","Open Science","Open Science Framework","OSF","OSF Collections","Reproducability","Reproducibility","Research","Research Best Practices","Research Tools","Research Transparency","TOP Guidelines"],"title":"OSF Collections: supporting research discoverability and reuse","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"ca03ed895a55ef1d0b673651f5b0eebe","permalink":"https://forrt.org/curated_resources/osf-deep-dive/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/osf-deep-dive/","section":"curated_resources","summary":"This video will introduce users of the Open Science Framework into some of the more advanced features.","tags":["Analysis","Data","Education","Materials","Open Scholarship Tools and Technologies","OSF","Publishing","Reproducibility","Research Data Management Tools","Researchers"],"title":"OSF Deep Dive","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"f07085d4c90b50f83907dac0c47f1731","permalink":"https://forrt.org/curated_resources/osf-in-the-classroom/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/osf-in-the-classroom/","section":"curated_resources","summary":"This webinar will introduce how to use the Open Science Framework (OSF; https://osf.io) in a Classroom. The OSF is a free, open source web application built to help researchers manage their workflows. The OSF is part collaboration tool, part version control software, and part data archive. The OSF connects to popular tools researchers already use, like Dropbox, Box, Github and Mendeley, to streamline workflows and increase efficiency. This webinar will discuss how to introduce reproducible research practices to students, show ways of tracking student activity, and introduce the use of Templates and Forks on the OSF to allow students to easily make new class projects. The OSF is the flagship product of the Center for Open Science, a non-profit technology start-up dedicated to improving the alignment between scientific values and scientific practices. Learn more at cos.io and osf.io, or email contact@cos.io.","tags":["Analysis","Data","Education","Materials","Open Scholarship Tools and Technologies","OSF","Publishing","Reproducibility","Research Data Management Tools","Researchers"],"title":"OSF in the Classroom","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"359fa00dcfae62bba11306022eca1be4","permalink":"https://forrt.org/curated_resources/osf-in-the-lab-organizing-related-projec/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/osf-in-the-lab-organizing-related-projec/","section":"curated_resources","summary":"Files for this webinar are available at: https://osf.io/ewhvq/ This webinar focuses on how to use the Open Science Framework (OSF) to tie together and organize multiple projects. We look at example structures appropriate for organizing classroom projects, a line of research, or a whole lab's activity. We discuss the OSF's capabilities for using projects as templates, linking projects, and forking projects as well as some considerations for using each of those capabilities when designing a structure for your own project. The OSF is a free, open source web application built to help researchers manage their workflows. The OSF is part collaboration tool, part version control software, and part data archive. The OSF connects to popular tools researchers already use, like Dropbox, Box, Github and Mendeley, to streamline workflows and increase efficiency.","tags":["Analysis","Data","Education","Inside Your Classroom","Materials","Open Scholarship Tools and Technologies","OSF","Publishing","Reproducibility","Research Data Management Tools","Researchers"],"title":"OSF In The Lab: Organizing related projects with Links, Forks, and Templates","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"53abbd2febb2f114a31953a9d86ffe13","permalink":"https://forrt.org/curated_resources/osf101/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/osf101/","section":"curated_resources","summary":"This webinar walks you through the basics of creating an OSF project, structuring it to fit your research needs, adding collaborators, and tying your favorite online tools into your project structure. OSF is a free, open source web application built by the Center for Open Science, a non-profit dedicated to improving the alignment between scientific values and scientific practices. OSF is part collaboration tool, part version control software, and part data archive. It is designed to connect to popular tools researchers already use, like Dropbox, Box, Github, and Mendeley, to streamline workflows and increase efficiency.","tags":["Analysis","Data","Education","Materials","Open Scholarship Tools and Technologies","OSF","Publishing","Reproducibility","Research Data Management Tools","Researchers"],"title":"OSF101","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"fda934abc22d596a06e7c4cfa9ba6f9f","permalink":"https://forrt.org/curated_resources/our-data-ourselves-a-framework-for-using/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/our-data-ourselves-a-framework-for-using/","section":"curated_resources","summary":"Qualitative training rarely acknowledges the role of emotions in both data collection and analysis. While bracketing emotions is an important part of reflexivity, emotions are both a source of data and a source of ‘work’ (Hochschild, Citation1983). Accordingly, mentoring junior qualitative scholars also requires emotion work. Issues of race, gender, and power come into play when we think critically about the role and importance of recognizing emotion work in the field and the academy. This piece draws on data from a year-long ethnographic multicase study of three schools using restorative practices, focusing on one interview with one participant that raised significant emotions for the principal investigator. I demonstrate and propose a framework for what I call ‘emotional coding’: noting data that give rise to strong emotions, and then identifying what these emotions say about our positionally; our participants; and the research topic. Implications for scholarship and mentorship are discussed.","tags":["Reflexivity","Positionality","Race and Education","Whiteness","Restorative Practices"],"title":"Our data, ourselves: A framework for using emotion in qualitative analysis","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"f6ff2c2cba74b95744ec33d8f933d18f","permalink":"https://forrt.org/curated_resources/outcome-reporting-bias-in-randomized-con/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/outcome-reporting-bias-in-randomized-con/","section":"curated_resources","summary":"Recent literature hints that outcomes of clinical trials in medicine are selectively reported. If applicable to psychotic disorders, such bias would jeopardize the reliability of randomized clinical trials (RCTs) investigating antipsychotics and thus their extrapolation to clinical practice. We therefore comprehensively examined outcome reporting bias in RCTs of antipsychotic drugs by a systematic review of prespecified outcomes on ClinicalTrials.gov records of RCTs investigating antipsychotic drugs in schizophrenia and schizoaffective disorder between 1 January 2006 and 31 December 2013. These outcomes were compared with outcomes published in scientific journals. Our primary outcome measure was concordance between prespecified and published outcomes; secondary outcome measures included outcome modifications on ClinicalTrials.gov after trial inception and the effects of funding source and directionality of results on record adherence. Of the 48 RCTs, 85% did not fully adhere to the prespecified outcomes. Discrepancies between prespecified and published outcomes were found in 23% of RCTs for primary outcomes, whereas 81% of RCTs had at least one secondary outcome non-reported, newly introduced, or changed to a primary outcome in the respective publication. In total, 14% of primary and 44% of secondary prespecified outcomes were modified after trial initiation. Neither funding source (P=0.60) nor directionality of the RCT results (P=0.10) impacted ClinicalTrials.gov record adherence. Finally, the number of published safety endpoints (N=335) exceeded the number of prespecified safety outcomes by 5.5 fold. We conclude that RCTs investigating antipsychotic drugs suffer from substantial outcome reporting bias and offer suggestions to both monitor and limit such bias in the future.","tags":["Publishing","Reporting","Reporting Bias","Reproducibility"],"title":"Outcome reporting bias in randomized-controlled trials investigating antipsychotic drugs","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"a64ece728ee24cc2380d341f7a5eba2a","permalink":"https://forrt.org/curated_resources/overcoming-the-knowledge-barrier-in-open/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/overcoming-the-knowledge-barrier-in-open/","section":"curated_resources","summary":"Getting started with open science and knowing where to go. This webinar will introduce participants to major practices in open science and then dive into the resources available to learn how to use these in your own work.","tags":["Reproducibility Crisis and Credibility Revolution","Open Science","Conceptual and statistical knowledge"],"title":"Overcoming the Knowledge Barrier in Open Science","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1721390377,"objectID":"23b42cff64beef06e89917decc417ba6","permalink":"https://forrt.org/glossary/english/overlay_journal/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/english/overlay_journal/","section":"glossary","summary":"","tags":null,"title":"Overlay Journal","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1635961284,"objectID":"96d9c5b682be1ad062fe41d4478a8836","permalink":"https://forrt.org/glossary/vbeta/overlay-journal/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/vbeta/overlay-journal/","section":"glossary","summary":"","tags":null,"title":"Overlay Journal","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1720865851,"objectID":"70d0075f4b34114dda08c92798e863d5","permalink":"https://forrt.org/glossary/german/overlay_journal/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/german/overlay_journal/","section":"glossary","summary":"","tags":null,"title":"Overlay Journal (Overlay Zeitschrift)","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"43d83c70e6036857e7caadc5ee565209","permalink":"https://forrt.org/curated_resources/oxford-berlin-open-research-summer-schoo/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/oxford-berlin-open-research-summer-schoo/","section":"curated_resources","summary":"This projects contains materials from lectures and workshops associated with the Oxford-Berlin Open Research Summer School 2019.","tags":["Open Research"],"title":"Oxford-Berlin Open Research summer school 2019","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1689122843,"objectID":"699786ab2d49802939fa80a8f67023ce","permalink":"https://forrt.org/curated_resources/p-values-and-statistical-practice/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/p-values-and-statistical-practice/","section":"curated_resources","summary":"An article about P Values and Statistical Practice","tags":[""],"title":"P Values and Statistical Practice","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"4fa5dbfda582697dc866a499ff979ad5","permalink":"https://forrt.org/curated_resources/p-values-in-display-items-are-ubiquitous/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/p-values-in-display-items-are-ubiquitous/","section":"curated_resources","summary":"P values represent a widely used, but pervasively misunderstood and fiercely contested method of scientific inference. Display items, such as figures and tables, often containing the main results, are an important source of P values. We conducted a survey comparing the overall use of P values and the occurrence of significant P values in display items of a sample of articles in the three top multidisciplinary journals (Nature, Science, PNAS) in 2017 and, respectively, in 1997. We also examined the reporting of multiplicity corrections and its potential influence on the proportion of statistically significant P values. Our findings demonstrated substantial and growing reliance on P values in display items, with increases of 2.5 to 14.5 times in 2017 compared to 1997. The overwhelming majority of P values (94%, 95% confidence interval [CI] 92% to 96%) were statistically significant. Methods to adjust for multiplicity were almost non-existent in 1997, but reported in many articles relying on P values in 2017 (Nature 68%, Science 48%, PNAS 38%). In their absence, almost all reported P values were statistically significant (98%, 95% CI 96% to 99%). Conversely, when any multiplicity corrections were described, 88% (95% CI 82% to 93%) of reported P values were statistically significant. Use of Bayesian methods was scant (2.5%) and rarely (0.7%) articles relied exclusively on Bayesian statistics. Overall, wider appreciation of the need for multiplicity corrections is a welcome evolution, but the rapid growth of reliance on P values and implausibly high rates of reported statistical significance are worrisome.","tags":["Analysis","Analysis of Variance","Bayesian Method","Bayesian Statistics","Computer Software","Data","Meta-analysis","Publishing","Scientific Publishing","Software Tools","Statistical Data","Statistics"],"title":"P values in display items are ubiquitous and almost invariably significant: A survey of top science journals","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1689122843,"objectID":"790093081b9ea0365fe9f4274e618728","permalink":"https://forrt.org/curated_resources/p-curve/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/p-curve/","section":"curated_resources","summary":"An abstract about the p-curve","tags":["Blog"],"title":"P-curve","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1721390377,"objectID":"87ace27822a821a16726f2c15e2561eb","permalink":"https://forrt.org/glossary/english/p_curve/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/english/p_curve/","section":"glossary","summary":"","tags":null,"title":"P-curve","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1635961284,"objectID":"d1a0af8dd42ac13c2c748433e61a4520","permalink":"https://forrt.org/glossary/vbeta/p-curve/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/vbeta/p-curve/","section":"glossary","summary":"","tags":null,"title":"P-curve","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1720865851,"objectID":"86fd93e03c146589a4cdf066c2fca0e0","permalink":"https://forrt.org/glossary/german/p_curve/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/german/p_curve/","section":"glossary","summary":"","tags":null,"title":"P-curve (P-Kurve)","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1689122843,"objectID":"4b611543c0efed2927b736491a99dced","permalink":"https://forrt.org/curated_resources/p-curve-visualization-updated-with-log-x/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/p-curve-visualization-updated-with-log-x/","section":"curated_resources","summary":"My p-curve tool now lets you show the x-axis on a log₁₀ scale, which makes it a lot easier to look at really small p-values","tags":["Blog","Interaction"],"title":"P-curve visualization updated with log x-axis","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1689122843,"objectID":"3b59ae4f8eabe9eb5ec866b1181fd76d","permalink":"https://forrt.org/curated_resources/p-curve-a-key-to-the-file-drawer/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/p-curve-a-key-to-the-file-drawer/","section":"curated_resources","summary":"Because scientists tend to report only studies (publication bias) or analyses (p-hacking) that “work,” readers must ask, “Are these effects true, or do they merely reflect selective reporting?” We introduce p-curve as a way to answer this question. P-curve is the distribution of statistically significant p values for a set of studies (ps \u003c .05). Because only true effects are expected to generate right-skewed p-curves—containing more low (.01s) than high (.04s) significant p values—only right-skewed p-curves are diagnostic of evidential value. By telling us whether we can rule out selective reporting as the sole explanation for a set of findings, p-curve offers a solution to the age-old inferential problems caused by file-drawers of failed studies and analyses. ","tags":["Reproducibility Crisis and Credibility Revolution","Open Science"],"title":"P-curve: A key to the file-drawer.","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1721390377,"objectID":"79a37c4cbce76ebcf97e6bbb702b18c2","permalink":"https://forrt.org/glossary/english/p_hacking/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/english/p_hacking/","section":"glossary","summary":"","tags":null,"title":"P-hacking","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1720865851,"objectID":"3f54bc4c35fe68425fdaa61dfa7ffb49","permalink":"https://forrt.org/glossary/german/p_hacking/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/german/p_hacking/","section":"glossary","summary":"","tags":null,"title":"P-hacking","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1635961284,"objectID":"7d652854c891404f4e787937a94b76b6","permalink":"https://forrt.org/glossary/vbeta/p-hacking/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/vbeta/p-hacking/","section":"glossary","summary":"","tags":null,"title":"P-hacking","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1689122843,"objectID":"9dbefdd07c79cfbf51988390e56c3919","permalink":"https://forrt.org/curated_resources/p-hacking-and-other-problems-in-psycholo/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/p-hacking-and-other-problems-in-psycholo/","section":"curated_resources","summary":"What's wrong with the social sciences? In this episode, Massimo and Julia are joined by Professor Daniel Lakens from the Eindhoven University of Technology, who studies psychology and blogs about research methods and open science. The three discuss why so many psychology papers can't be trusted, and what solutions might exist for the problem (including how to fix the skewed incentives in the field).","tags":["Podcast","Reproducibility Crisis and Credibility Revolution"],"title":"P-Hacking and Other Problems in Psychology Research","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1721390377,"objectID":"e0c3c4661287e2fb34917d91ccd4a4bb","permalink":"https://forrt.org/glossary/english/p_value/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/english/p_value/","section":"glossary","summary":"","tags":null,"title":"p-value","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1635961284,"objectID":"9c27a0472596fa468e3972ffcdbe7934","permalink":"https://forrt.org/glossary/vbeta/p-value/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/vbeta/p-value/","section":"glossary","summary":"","tags":null,"title":"p-value","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1720865851,"objectID":"86ab7d6ed89795cf6b4e3978843644ba","permalink":"https://forrt.org/glossary/german/p_value/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/german/p_value/","section":"glossary","summary":"","tags":null,"title":"P-value (p-Wert)","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1689122843,"objectID":"ea681b50d084dc228af8144870c81c26","permalink":"https://forrt.org/curated_resources/pangea/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/pangea/","section":"curated_resources","summary":"PANGEA is the first power analysis program for general ANOVA designs (e.g., Winer, Brown, \u0026 Michels, 1991). PANGEA can handle designs with any number of factors, each with any number of levels; any factor can be treated as fixed or random; and any valid pattern of nesting or crossing of the factors is allowed.","tags":["Reproducibility Knowledge","Power Analysis Tool"],"title":"PANGEA","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1721390377,"objectID":"ea577c4244b3b25224d968400287ba36","permalink":"https://forrt.org/glossary/english/papermill/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/english/papermill/","section":"glossary","summary":"","tags":null,"title":"Papermill","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1635961284,"objectID":"c0a1c86188139b017444cc412b487e16","permalink":"https://forrt.org/glossary/vbeta/papermill/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/vbeta/papermill/","section":"glossary","summary":"","tags":null,"title":"Papermill","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1720865851,"objectID":"732ee9764a2851bf5a5fb694a75e5a02","permalink":"https://forrt.org/glossary/german/papermill/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/german/papermill/","section":"glossary","summary":"","tags":null,"title":"Papermill (Papierfabrik)","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1721390377,"objectID":"e95e197d0f9ee1fd8091599071b5fb9f","permalink":"https://forrt.org/glossary/english/paradata/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/english/paradata/","section":"glossary","summary":"","tags":null,"title":"Paradata","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1635961284,"objectID":"fdc190d360341ac61dd55e657016c8fa","permalink":"https://forrt.org/glossary/vbeta/paradata/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/vbeta/paradata/","section":"glossary","summary":"","tags":null,"title":"Paradata","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1720865851,"objectID":"313897d6eaf59c1ed73935370da65bb1","permalink":"https://forrt.org/glossary/german/paradata/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/german/paradata/","section":"glossary","summary":"","tags":null,"title":"Paradata (Paradaten)","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1721390377,"objectID":"3603c13642f853f5f19815115378ffe3","permalink":"https://forrt.org/glossary/english/parking/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/english/parking/","section":"glossary","summary":"","tags":null,"title":"PARKing","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1720865851,"objectID":"28702fc73bcc981a15d48e7f3dcf1681","permalink":"https://forrt.org/glossary/german/parking/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/german/parking/","section":"glossary","summary":"","tags":null,"title":"PARKing","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1635961284,"objectID":"94fcf4203ef5ff5232bc6c5fc32d5a15","permalink":"https://forrt.org/glossary/vbeta/parking/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/vbeta/parking/","section":"glossary","summary":"","tags":null,"title":"PARKing","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1721390377,"objectID":"7387b92272c74be2eefa397079823da4","permalink":"https://forrt.org/glossary/english/participatory_research/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/english/participatory_research/","section":"glossary","summary":"","tags":null,"title":"Participatory Research","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1635961284,"objectID":"3f0d5563066121b61ddd5d8bac126a2d","permalink":"https://forrt.org/glossary/vbeta/participatory-research/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/vbeta/participatory-research/","section":"glossary","summary":"","tags":null,"title":"Participatory Research","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1720865851,"objectID":"013ea6a8e4df719a4715e1691b11ad4c","permalink":"https://forrt.org/glossary/german/participatory_research/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/german/participatory_research/","section":"glossary","summary":"","tags":null,"title":"Participatory Research (Partizipative Forschung)","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"089b10a44b678a15477e86e00d277571","permalink":"https://forrt.org/curated_resources/participatory-research-approaches-in-alz/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/participatory-research-approaches-in-alz/","section":"curated_resources","summary":"Background and Objectives\nGiven the increase in methodological pluralism in research on brain health, cognitive aging, and neurodegenerative diseases, this scoping review aims to provide a descriptive overview and qualitative content analysis of studies stating the use of participatory research approaches within Alzheimer’s disease and related dementias (ADRD) literature globally.\n\nResearch Design and Methods\nWe conducted a systematic search across four multidisciplinary databases (CINAHL, SCOPUS, PsycInfo, PubMed) for peer-reviewed, English-language studies addressing ADRD that explicitly described their use of a participatory research approach. We employed a systematic process for selecting articles that yielded a final sample of 163 studies. Data from articles were analyzed to chart trends from 1990 to 2022 in terminology, descriptions, application of participatory approaches, and the extent and nature of partnerships with nonacademics.\n\nResults\nResults demonstrated geographic differences in the use of stated approaches between North America—where community-based participatory research predominates—and Europe, where Action Research is most common. We further found that only 73% of papers in this systematic review had identifiable definitions or descriptions of the participatory approach used. Findings also showed that 14% of articles demonstrated no evidence of engaged partnership beyond activities typical of research participants, while 23% of articles identified partnering with people with dementia, and an additional 16% reported partnerships with members from Indigenous, Black, Asian, or Latinx communities.\n\nDiscussion and Implications\nThis scoping review identifies three areas in need of greater attention in ADRD literature using participatory research approaches. First, findings indicate the importance of strengthening the use, transparency, and rigor of participatory methods. Second, results suggest the need for greater inclusion of historically marginalized groups who are most affected by ADRD as research partners. Finally, the findings highlight the need for integrating social justice values of participatory approaches into research project designs.","tags":["Aging","Dementia","Lived Experience","Participatory Research","Scoping Review"],"title":"Participatory Research Approaches in Alzheimer’s Disease and Related Dementias Literature: A Scoping Review","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"3416f473dbd0dc2b6c762aa136f2e180","permalink":"https://forrt.org/curated_resources/paths-in-strange-spaces-a-comment-on-pre/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/paths-in-strange-spaces-a-comment-on-pre/","section":"curated_resources","summary":"This is an archived version of a blog post on preregistration. The first half of the post argues that there is not a strong justification for preregistration as a tool to solve problems with statistical inference (p-hacking); the second half argues that preregistration has a stronger justification as one tool (among many) that can aid scientists in documenting our projects. [Note that this archival version exists only because the blog itself no longer does, and as the original has been cited multiple times there is value in ensuring that some version of the blog post remains accessible.]","tags":["Preregistration"],"title":"Paths in strange spaces: A comment on preregistration","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1720865851,"objectID":"ce71dbf9d77924d8cd98bf02d38f073c","permalink":"https://forrt.org/glossary/german/patient_and_public_involvement/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/german/patient_and_public_involvement/","section":"glossary","summary":"","tags":null,"title":"Patient and Public Involvement (PPI, Einbindung von Patient:innen und Öffentlichkeit)","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1721390377,"objectID":"1a39905edf3c4c7e3a37d1f1d70c9980","permalink":"https://forrt.org/glossary/english/patient_and_public_involvement/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/english/patient_and_public_involvement/","section":"glossary","summary":"","tags":null,"title":"Patient and Public Involvement (PPI)","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1635961284,"objectID":"4ef8b040ce5aa2bbe2c9e28b153d5e67","permalink":"https://forrt.org/glossary/vbeta/patient-and-public-involvement-ppi/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/vbeta/patient-and-public-involvement-ppi/","section":"glossary","summary":"","tags":null,"title":"Patient and Public Involvement (PPI)","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1721390377,"objectID":"a2d2775815d8865acbd14f063cb81925","permalink":"https://forrt.org/glossary/english/paywall/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/english/paywall/","section":"glossary","summary":"","tags":null,"title":"Paywall","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1635961284,"objectID":"0e469ab7fafbbae4c16f93cac5073af4","permalink":"https://forrt.org/glossary/vbeta/paywall/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/vbeta/paywall/","section":"glossary","summary":"","tags":null,"title":"Paywall","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1720865851,"objectID":"5366c276becf1f7c663335dd9867bfb4","permalink":"https://forrt.org/glossary/german/paywall/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/german/paywall/","section":"glossary","summary":"","tags":null,"title":"Paywall (Bezahlschranke)","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"dc94ad9520bad07a99c2ed8f93119746","permalink":"https://forrt.org/curated_resources/pbfj-editorial-engaging-with-responsible/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/pbfj-editorial-engaging-with-responsible/","section":"curated_resources","summary":"This editorial note is an “action-based” follow-up to the previous PBFJ Editorial published in September 2021 … “A Declaration about Responsible Science” (Faff, 2021a). In that prior editorial, I made a very clear statement that “PBFJ is moving on a pathway toward more effectively embracing and fostering the principles of responsible science, captured by three central pillars: (1) Credible research; (2) Relevant research; and (3) Independent research.” To be very clear, this new initiative is designed to both complement and augment our well-established and traditional process of publication PBFJ. In my view, now it is time for real action … we invite expressions of interest, as the first step in a 4-phase process of “pre-registration” studies, as outlined in this Editorial.","tags":["Finance","Business","Preregistration","Responsible Science"],"title":"PBFJ Editorial … Engaging with responsible science. “OPEN FOR BUSINESS” – Launching the PBFJ pre-registration publication initiative","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1721390377,"objectID":"f943b8b447472b4e3926d957fa72a623","permalink":"https://forrt.org/glossary/english/pci/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/english/pci/","section":"glossary","summary":"","tags":null,"title":"PCI (Peer Community In)","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1720865851,"objectID":"88d63c2d621a2a2d3379c809a176428f","permalink":"https://forrt.org/glossary/german/pci/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/german/pci/","section":"glossary","summary":"","tags":null,"title":"PCI (Peer Community In)","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1635961284,"objectID":"e82f4993907599a29c059e93a362b03a","permalink":"https://forrt.org/glossary/vbeta/pci-peer-community-in/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/vbeta/pci-peer-community-in/","section":"glossary","summary":"","tags":null,"title":"PCI (Peer Community In)","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1721390377,"objectID":"5d0bf2183f9144bf897723904bcb2733","permalink":"https://forrt.org/glossary/english/pci_registered_reports/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/english/pci_registered_reports/","section":"glossary","summary":"","tags":null,"title":"PCI Registered Reports","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1720865851,"objectID":"b9ea8fe880b53894f908ea14e36ac267","permalink":"https://forrt.org/glossary/german/pci_registered_reports/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/german/pci_registered_reports/","section":"glossary","summary":"","tags":null,"title":"PCI Registered Reports","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1635961284,"objectID":"4c6d5d6de91e61ebf5ac3623614ee84f","permalink":"https://forrt.org/glossary/vbeta/pci-registered-reports/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/vbeta/pci-registered-reports/","section":"glossary","summary":"","tags":null,"title":"PCI Registered Reports","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"4a1e5138a014f3a4d9ff5cf8a71ff904","permalink":"https://forrt.org/curated_resources/peer-review-decisions-decisions/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/peer-review-decisions-decisions/","section":"curated_resources","summary":"Journals are exploring new approaches to peer review in order to reduce bias, increase transparency and respond to author preferences. Funders are also getting involved. If you start reading about the subject of peer review, it won't be long before you encounter articles with titles like Can we trust peer review?, Is peer review just a crapshoot? and It's time to overhaul the secretive peer review process. Read some more and you will learn that despite its many shortcomings – it is slow, it is biased, and it lets flawed papers get published while rejecting work that goes on to win Nobel Prizes – the practice of having your work reviewed by your peers before it is published is still regarded as the 'gold standard' of scientific research. Carry on reading and you will discover that peer review as currently practiced is a relatively new phenomenon and that, ironically, there have been remarkably few peer-reviewed studies of peer review.","tags":["Bias","Data","Funding Agencies","Peer Review","Publishing"],"title":"Peer Review: Decisions, decisions","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1689122843,"objectID":"6d14f31abd512d49ff724aa1a62c15be","permalink":"https://forrt.org/curated_resources/peer-review-practices-of-psychological-j/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/peer-review-practices-of-psychological-j/","section":"curated_resources","summary":"A growing interest in and concern about the adequacy and fairness of modern peer-review practices in publication and funding are apparent across a wide range of scientific disciplines. Although questions about reliability, accountability, reviewer bias, and competence have been raised, there has been very little direct research on these variables. The present investigation was an attempt to study the peer-review process directly, in the natural setting of actual journal referee evaluations of submitted manuscripts. As test materials we selected 12 already published research articles by investigators from prestigious and highly productive American psychology departments, one article from each of 12 highly regarded and widely read American psychology journals with high rejection rates (80%) and nonblind refereeing practices. With fictitious names and institutions substituted for the original ones (e.g., Tri-Valley Center for Human Potential), the altered manuscripts were formally resubmitted to the journals that had originally refereed and published them 18 to 32 months earlier. Of the sample of 38 editors and reviewers, only three (8%) detected the resubmissions. This result allowed nine of the 12 articles to continue through the review process to receive an actual evaluation: eight of the nine were rejected. Sixteen of the 18 referees (89%) recommended against publication and the editors concurred. The grounds for rejection were in many cases described as “serious methodological flaws.” A number of possible interpretations of these data are reviewed and evaluated.","tags":["Open Science","Open Review"],"title":"Peer-review practices of psychological journals: The fate of published articles, submitted again. ","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1689122843,"objectID":"4d446fa08a9a17907c5b23da60eeeaa2","permalink":"https://forrt.org/curated_resources/performing-high-powered-studies-efficien/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/performing-high-powered-studies-efficien/","section":"curated_resources","summary":"Running studies with high statistical power, while effect size estimates in psychology are often inaccurate, leads to a practical challenge when designing an experiment. This challenge can be addressed by performing sequential analyses while the data collection is still in progress. At an interim analysis, data collection can be stopped whenever the results are convincing enough to conclude that an effect is present, more data can be collected, or the study can be terminated whenever it is extremely unlikely that the predicted effect will be observed if data collection would be continued. Such interim analyses can be performed while controlling the Type 1 error rate. Sequential analyses can greatly improve the efficiency with which data are collected. Additional flexibility is provided by adaptive designs where sample sizes are increased on the basis of the observed effect size. The need for pre‐registration, ways to prevent experimenter bias, and a comparison between Bayesian approaches and null‐hypothesis significance testing (NHST) are discussed. Sequential analyses, which are widely used in large‐scale medical trials, provide an efficient way to perform high‐powered informative experiments. I hope this introduction will provide a practical primer that allows researchers to incorporate sequential analyses in their research. ","tags":[""],"title":"Performing high‐powered studies efficiently with sequential analyses","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"e7f340062d5c246cfb51b90e0b709bb3","permalink":"https://forrt.org/curated_resources/perspectives-on-improving-methods-in-psy/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/perspectives-on-improving-methods-in-psy/","section":"curated_resources","summary":"A syllabus about open science","tags":["Reproducibility Crisis and Credibility Revolution","Open Science"],"title":"Perspectives on Improving Methods in Psychological Science","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"372fc1abe0e10ab7575da3de2d5d1a8a","permalink":"https://forrt.org/curated_resources/perspectives-replication-is-more-than-me/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/perspectives-replication-is-more-than-me/","section":"curated_resources","summary":"Drawing on recent research and debates in social sciences, this paper situates replication in an advertising research context. We clarify the role of replication in the field and outline the challenges inherent in replication studies in advertising research. We further elaborate on how researchers should engage in replication research to increase the truth value of advertising research while overcoming the obstacles to replication research. Finally, we discuss how advertising scholars, reviewers, and editors can facilitate replication research to reduce the share of false-positive results and accumulate knowledge in the discipline. We see replication as critical in advertising research, given the high variability of experimental factors and the applied nature of the field. Therefore, a better understanding of replications and the challenges of advertising research should inspire scholars to engage in more replication attempts and reviewers and editors to consider it for publication.","tags":["Methodology","Marketing","Advertising","Replication","Exact Replication","Close Replication","Constructive Replication","Conceptual Replication","Truth Value"],"title":"Perspectives: Replication is more than meets the eye","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1689122843,"objectID":"928ddfb35fe6b34c443abb2a25f93434","permalink":"https://forrt.org/curated_resources/philosophical-psychology-1989/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/philosophical-psychology-1989/","section":"curated_resources","summary":"P. E. Meehl did first 10 sessions (Winter Quarter, Jan–Mar 1989). In the Spring Quarter, several other department members lectured on various topics. Then PEM did last two sessions (5/25/89 and 6/1/89).","tags":["Course","Video"],"title":"Philosophical Psychology 1989","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1721390377,"objectID":"ff1f1faf1b6aa48273c6570f43ffb9d1","permalink":"https://forrt.org/glossary/english/plan_s/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/english/plan_s/","section":"glossary","summary":"","tags":null,"title":"Plan S","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1720865851,"objectID":"e03e246a88e9de110ed50e054c875a28","permalink":"https://forrt.org/glossary/german/plan_s/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/german/plan_s/","section":"glossary","summary":"","tags":null,"title":"Plan S","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1635961284,"objectID":"5e5fce64ff7bb5c66012de45dcf0bb5d","permalink":"https://forrt.org/glossary/vbeta/plan-s/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/vbeta/plan-s/","section":"glossary","summary":"","tags":null,"title":"Plan S","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"767edc9976999d3eb7e1c8cadeaef0a1","permalink":"https://forrt.org/curated_resources/platform-controlled-social-media-apis-th/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/platform-controlled-social-media-apis-th/","section":"curated_resources","summary":"Social media data enable insights into human behaviour. Researchers can access these data via platform-provided application programming interfaces (APIs), but these come with restrictive usage terms that mean studies cannot be reproduced or replicated. Platform-owned APIs hinder access, transparency and scientific knowledge.","tags":["Human Behaviour","Science","Technology","Society","Social Sciences"],"title":"Platform-controlled social media APIs threaten open science","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"61a76c63f926875900e5fa36fe6abf57","permalink":"https://forrt.org/curated_resources/plotting-and-programming-in-python/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/plotting-and-programming-in-python/","section":"curated_resources","summary":"This lesson is part of Software Carpentry workshops and teach an introduction to plotting and programming using python. This lesson is an introduction to programming in Python for people with little or no previous programming experience. It uses plotting as its motivating example, and is designed to be used in both Data Carpentry and Software Carpentry workshops. This lesson references JupyterLab, but can be taught using a regular Python interpreter as well. Please note that this lesson uses Python 3 rather than Python 2.","tags":["Analysis","Data","Education","Open Scholarship Tools and Technologies","Python","Reproducibility","Research Data Management Tools","Researchers"],"title":"Plotting and Programming in Python","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1689122843,"objectID":"e1df69640fb6324c3844d1607a363f67","permalink":"https://forrt.org/curated_resources/point-of-view-how-open-science-helps-res/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/point-of-view-how-open-science-helps-res/","section":"curated_resources","summary":"Open access, open data, open source and other open scholarship practices are growing in popularity and necessity. However, widespread adoption of these practices has not yet been achieved. One reason is that researchers are uncertain about how sharing their work will affect their careers. We review literature demonstrating that open research is associated with increases in citations, media attention, potential collaborators, job opportunities and funding opportunities. These findings are evidence that open research practices bring significant benefits to researchers relative to more traditional closed practices.","tags":["Reproducibility Crisis and Credibility Revolution","Open Science"],"title":"Point of View: How open science helps researchers succeed","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"440e6fba7ad5dfe540edf7ee30adb66a","permalink":"https://forrt.org/curated_resources/policy-recommendations-to-ensure-that-re/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/policy-recommendations-to-ensure-that-re/","section":"curated_resources","summary":"Research data is optimized when it can be freely accessed and reused. To maximize research equity, transparency, and reproducibility, policymakers should take concrete steps to ensure that research software is openly accessible and reusable.","tags":["research","software","open software"],"title":"Policy recommendations to ensure that research software is openly accessible and reusable","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"6649d50aa111aa9603d99fd8d451d1ff","permalink":"https://forrt.org/curated_resources/poor-replication-validity-of-biomedical/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/poor-replication-validity-of-biomedical/","section":"curated_resources","summary":"Objective To investigate the replication validity of biomedical association studies covered by newspapers. Methods We used a database of 4723 primary studies included in 306 meta-analysis articles. These studies associated a risk factor with a disease in three biomedical domains, psychiatry, neurology and four somatic diseases. They were classified into a lifestyle category (e.g. smoking) and a non-lifestyle category (e.g. genetic risk). Using the database Dow Jones Factiva, we investigated the newspaper coverage of each study. Their replication validity was assessed using a comparison with their corresponding meta-analyses. Results Among the 5029 articles of our database, 156 primary studies (of which 63 were lifestyle studies) and 5 meta-analysis articles were reported in 1561 newspaper articles. The percentage of covered studies and the number of newspaper articles per study strongly increased with the impact factor of the journal that published each scientific study. Newspapers almost equally covered initial (5/39 12.8%) and subsequent (58/600 9.7%) lifestyle studies. In contrast, initial non-lifestyle studies were covered more often (48/366 13.1%) than subsequent ones (45/3718 1.2%). Newspapers never covered initial studies reporting null findings and rarely reported subsequent null observations. Only 48.7% of the 156 studies reported by newspapers were confirmed by the corresponding meta-analyses. Initial non-lifestyle studies were less often confirmed (16/48) than subsequent ones (29/45) and than lifestyle studies (31/63). Psychiatric studies covered by newspapers were less often confirmed (10/38) than the neurological (26/41) or somatic (40/77) ones. This is correlated to an even larger coverage of initial studies in psychiatry. Whereas 234 newspaper articles covered the 35 initial studies that were later disconfirmed, only four press articles covered a subsequent null finding and mentioned the refutation of an initial claim. Conclusion Journalists preferentially cover initial findings although they are often contradicted by meta-analyses and rarely inform the public when they are disconfirmed.","tags":["ADHD","Analysis","Bibliometrics","Biomarkers","Breast Cancer","Data","Health Risk Analysis","Mental Health and Psychiatry","Meta-analysis","Publishing","Scientific Publishing"],"title":"Poor replication validity of biomedical association studies reported by newspapers","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"626ec113fbe77fd3a2cf111edb065e36","permalink":"https://forrt.org/curated_resources/poor-statistical-reporting-inadequate-da/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/poor-statistical-reporting-inadequate-da/","section":"curated_resources","summary":"The Journal of Physiology and British Journal of Pharmacology jointly published an editorial series in 2011 to improve standards in statistical reporting and data analysis. It is not known whether reporting practices changed in response to the editorial advice. We conducted a cross-sectional analysis of reporting practices in a random sample of research papers published in these journals before (n = 202) and after (n = 199) publication of the editorial advice. Descriptive data are presented. There was no evidence that reporting practices improved following publication of the editorial advice. Overall, 76-84% of papers with written measures that summarized data variability used standard errors of the mean, and 90-96% of papers did not report exact p-values for primary analyses and post-hoc tests. 76-84% of papers that plotted measures to summarize data variability used standard errors of the mean, and only 2-4% of papers plotted raw data used to calculate variability. Of papers that reported p-values between 0.05 and 0.1, 56-63% interpreted these as trends or statistically significant. Implied or gross spin was noted incidentally in papers before (n = 10) and after (n = 9) the editorial advice was published. Overall, poor statistical reporting, inadequate data presentation and spin were present before and after the editorial advice was published. While the scientific community continues to implement strategies for improving reporting practices, our results indicate stronger incentives or enforcements are needed.","tags":["Bioassays and Physiological Analysis","Clinical Trials","Data","Data Processing","Drug Research and Development","Medical Journals","Pharmacology","Research Integrity","Research Reporting Guidelines","Research Validity","Scientific Publishing","Statistical Data"],"title":"Poor statistical reporting, inadequate data presentation and spin persist despite editorial advice","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1721390377,"objectID":"6f11fe36b53a26d63d4a6ee004fa51a0","permalink":"https://forrt.org/glossary/english/positionality/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/english/positionality/","section":"glossary","summary":"","tags":null,"title":"Positionality","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1635961284,"objectID":"7ebf1908779aeaa11059e111b4d322d1","permalink":"https://forrt.org/glossary/vbeta/positionality/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/vbeta/positionality/","section":"glossary","summary":"","tags":null,"title":"Positionality","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1720865851,"objectID":"d0cc087854098e31bae373191b6e4b6b","permalink":"https://forrt.org/glossary/german/positionality/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/german/positionality/","section":"glossary","summary":"","tags":null,"title":"Positionality (Positionalität)","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1721390377,"objectID":"e6e97671070345b8abfdc68a99c17324","permalink":"https://forrt.org/glossary/english/positionality_map/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/english/positionality_map/","section":"glossary","summary":"","tags":null,"title":"Positionality Map","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1635961284,"objectID":"886ccc4ce1dfcd4832ef3bae20df655d","permalink":"https://forrt.org/glossary/vbeta/positionality-map/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/vbeta/positionality-map/","section":"glossary","summary":"","tags":null,"title":"Positionality Map","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1720865851,"objectID":"d997f5beb9c0b11473987df3ea39ffad","permalink":"https://forrt.org/glossary/german/positionality_map/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/german/positionality_map/","section":"glossary","summary":"","tags":null,"title":"Positionality Map (Positionalitätskarte)","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"831b37578f77a929204155818c5c97d6","permalink":"https://forrt.org/curated_resources/positionality-statements-are-just-the-ti/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/positionality-statements-are-just-the-ti/","section":"curated_resources","summary":"In July 2020, Journal of Women and Minorities in Science and Engineering (JWM)\nstarted requiring a positionality statement in each paper published. To the best of our\nknowledge, requiring a positionality statement at JWM is a first for a journal in science,\ntechnology, engineering, and mathematics (STEM) education, although some other\njournals are beginning to encourage authors to include one.\n\nWith this editorial, we, the JWM editorial staff, want to clarify several aspects about\npositionality statements. We understand positionality statements to be only one part of\na larger process of reflexivity. We also understand that for many JWM authors, professional and personal risks are associated with revealing identities in positionality statements. It is our hope that positionality statements—when understood in these more\ncomplex ways—will result in higher quality, more socially just research. And that is\nwhy JWM has committed to them.","tags":["Positionality","Author Guidelines","Reviewer Guidelines","Reflexivity","Transparency"],"title":"Positionality Statements are just the tip of the Iceberg: Moving toward a Reflexive Process","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1721390377,"objectID":"c3eac4e0eba23fb55a48b154f224c91f","permalink":"https://forrt.org/glossary/english/post_hoc/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/english/post_hoc/","section":"glossary","summary":"","tags":null,"title":"Post Hoc","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1635961284,"objectID":"5f5bb39208ed6b15bbf411b671bf7457","permalink":"https://forrt.org/glossary/vbeta/post-hoc/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/vbeta/post-hoc/","section":"glossary","summary":"","tags":null,"title":"Post Hoc","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1720865851,"objectID":"ccd1f25b510ca20ae7bf0ac6cf36f6fd","permalink":"https://forrt.org/glossary/german/post_hoc/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/german/post_hoc/","section":"glossary","summary":"","tags":null,"title":"Post Hoc (Im Nachhinein)","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1721390377,"objectID":"8777c97f564150c11c3fd6287ea91687","permalink":"https://forrt.org/glossary/english/post_publication_peer_review/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/english/post_publication_peer_review/","section":"glossary","summary":"","tags":null,"title":"Post Publication Peer Review","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1635961284,"objectID":"52822538efe9d9db7c1d8c99548f96bc","permalink":"https://forrt.org/glossary/vbeta/post-publication-peer-review/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/vbeta/post-publication-peer-review/","section":"glossary","summary":"","tags":null,"title":"Post Publication Peer Review ","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1720865851,"objectID":"9f494dc8e54ddc9906b8dbf17a4a899b","permalink":"https://forrt.org/glossary/german/post_publication_peer_review/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/german/post_publication_peer_review/","section":"glossary","summary":"","tags":null,"title":"Post Publication Peer Review (Peer-Review nach der Veröffentlichung)","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1635961284,"objectID":"84feeb72c02b0439d27da6b98321962a","permalink":"https://forrt.org/glossary/vbeta/posterior-distribution/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/vbeta/posterior-distribution/","section":"glossary","summary":"","tags":null,"title":"Posterior distribution","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"632e9a7f9843715cdcd42617b8a090ba","permalink":"https://forrt.org/glossary/english/posterior-distribution/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/english/posterior-distribution/","section":"glossary","summary":"","tags":null,"title":"Posterior distribution","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"181354f94ba6b671491d274c1fe3de25","permalink":"https://forrt.org/glossary/german/posterior-distribution/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/german/posterior-distribution/","section":"glossary","summary":"","tags":null,"title":"Posterior distribution","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1689122843,"objectID":"d3ca96c33e523d8c40b76defd279bd96","permalink":"https://forrt.org/curated_resources/power-analysis-and-effect-size-in-mixed/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/power-analysis-and-effect-size-in-mixed/","section":"curated_resources","summary":"In psychology, attempts to replicate published findings are less successful than expected. For properly powered studies replication rate should be around 80%, whereas in practice less than 40% of the studies selected from different areas of psychology can be replicated. Researchers in cognitive psychology are hindered in estimating the power of their studies, because the designs they use present a sample of stimulus materials to a sample of participants, a situation not covered by most power formulas. To remedy the situation, we review the literature related to the topic and introduce recent software packages, which we apply to the data of two masked priming studies with high power. We checked how we could estimate the power of each study and how much they could be reduced to remain powerful enough. On the basis of this analysis, we recommend that a properly powered reaction time experiment with repeated measures has at least 1,600 word observations per condition (e.g., 40 participants, 40 stimuli). This is considerably more than current practice. We also show that researchers must include the number of observations in meta-analyses because the effect sizes currently reported depend on the number of stimuli presented to the participants. Our analyses can easily be applied to new datasets gathered.","tags":[""],"title":"Power Analysis and Effect Size in Mixed Effects Models: A Tutorial. ","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1689122843,"objectID":"f58040ffafe154b0807084246fdfe3df","permalink":"https://forrt.org/curated_resources/power-failure-why-small-sample-size-unde/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/power-failure-why-small-sample-size-unde/","section":"curated_resources","summary":"A study with low statistical power has a reduced chance of detecting a true effect, but it is less well appreciated that low power also reduces the likelihood that a statistically significant result reflects a true effect. Here, we show that the average statistical power of studies in the neurosciences is very low. The consequences of this include overestimates of effect size and low reproducibility of results. There are also ethical dimensions to this problem, as unreliable research is inefficient and wasteful. Improving reproducibility in neuroscience is a key priority and requires attention to well-established but often ignored methodological principles.","tags":["Reproducibility Crisis and Credibility Revolution","Open Science"],"title":"Power failure: why small sample size undermines the reliability of neuroscience","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1689122843,"objectID":"c7d02a70adb7f29f4df07cb42abfd7fe","permalink":"https://forrt.org/curated_resources/power-posing-reassessing-the-evidence-be/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/power-posing-reassessing-the-evidence-be/","section":"curated_resources","summary":"A recent paper in Psych Science (.pdf) reports a failure to replicate the study that inspired a TED Talk that has been seen 25 million times. [1] The talk invited viewers to do better in life by assuming high-power poses, just like Wonder Woman's below, but the replication found that power-posing was inconsequential. If an original finding is a false positive then its replication is likely to fail, but a failed replication need not imply that the original was a false positive. In this post we try to figure out why the replication failed.","tags":["Blog"],"title":"Power Posing: Reassessing The Evidence Behind The Most Popular TED Talk","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1689122843,"objectID":"b085a521367b13e3e9966a6f4047eb70","permalink":"https://forrt.org/curated_resources/power-up-a-reanalysis-of-power-failure-i/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/power-up-a-reanalysis-of-power-failure-i/","section":"curated_resources","summary":"Recently, evidence for endemically low statistical power has cast neuroscience findings into doubt. If low statistical power plagues neuroscience, then this reduces confidence in the reported effects. However, if statistical power is not uniformly low, then such blanket mistrust might not be warranted. Here, we provide a different perspective on this issue, analyzing data from an influential study reporting a median power of 21% across 49 meta-analyses (Button et al., 2013). We demonstrate, using Gaussian mixture modeling, that the sample of 730 studies included in that analysis comprises several subcomponents so the use of a single summary statistic is insufficient to characterize the nature of the distribution. We find that statistical power is extremely low for studies included in meta-analyses that reported a null result and that it varies substantially across subfields of neuroscience, with particularly low power in candidate gene association studies. Therefore, whereas power in neuroscience remains a critical issue, the notion that studies are systematically underpowered is not the full story: low power is far from a universal problem.","tags":[""],"title":"Power-up: A reanalysis of ‘power failure’ in neuroscience using mixture modeling.","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"cec5176951ed1d963856ab8d53aeab23","permalink":"https://forrt.org/curated_resources/practical-considerations-for-navigating/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/practical-considerations-for-navigating/","section":"curated_resources","summary":"Practical Considerations for Navigating Registered Reports","tags":["Research"],"title":"Practical Considerations for Navigating Registered Reports","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"dbd7fc4d0cadde9c68646648b4faee2e","permalink":"https://forrt.org/curated_resources/practical-guides-on-open-science-for-res/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/practical-guides-on-open-science-for-res/","section":"curated_resources","summary":"A recently published series of open science guides help researchers in the Netherlands navigate open science. They cover topics such as pre-prints, open licenses, questionable publication practices (forthcoming) and how to apply open science in practice.\n\nAll guides are open access and are available via the open repository Zenodo. For ease of reference, we have gathered the guides below. The list will be updated as more guides are published in the future. ","tags":["Early Career Researcher","Preprints","Creative Commons","Open Educational Resources"],"title":"Practical guides on open science for researchers in the Netherlands","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1689122843,"objectID":"19a380410f025c41688dbcab69250807","permalink":"https://forrt.org/curated_resources/practical-solutions-for-sharing-data-and/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/practical-solutions-for-sharing-data-and/","section":"curated_resources","summary":"Widespread sharing of data and materials (including displays and text- and video-based descriptions of experimental procedures) will improve the reproducibility of psychological science and accelerate the pace of discovery. In this article, we discuss some of the challenges to open sharing and offer practical solutions for researchers who wish to share more of the products—and process—of their research. Many of these solutions were devised by the Databrary.org data library for storing and sharing video, audio, and other forms of sensitive or personally identifiable data. We also discuss ways in which researchers can make shared data and materials easier for others to find and reuse. Widely adopted, these solutions and practices will increase transparency and speed progress in psychological science.","tags":["Open Science"],"title":"Practical Solutions for Sharing Data and Materials From Psychological Research","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"0ffa849c081fd7062ac599012eae3ecf","permalink":"https://forrt.org/curated_resources/practical-steps-for-increasing-opennes-a/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/practical-steps-for-increasing-opennes-a/","section":"curated_resources","summary":"Reproducibility, Research Management Planning, Structuring a study, Preregistration + Analysis Plan, Files and Version Control, Sharing on the OSF, Incentives (Badges, RR)","tags":["Research"],"title":"Practical steps for increasing opennes and reproducibility: an introduction to OSF, Workshop Manual","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"fb3117c7fbf828df31327354ca6a3cd2","permalink":"https://forrt.org/curated_resources/pre-analysis-plans-have-limited-upside-e/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/pre-analysis-plans-have-limited-upside-e/","section":"curated_resources","summary":"The social sciences—including economics—have long called for transparency in research to counter threats to producing robust and replicable results. In this paper, we discuss the pros and cons of three of the more prominent proposed approaches: pre-analysis plans, hypothesis registries, and replications. They have been primarily discussed for experimental research, both in the field including randomized control trials and the laboratory, so we focus on these areas. A pre-analysis plan is a credibly fixed plan of how a researcher will collect and analyze data, which is submitted before a project begins. Though pre-analysis plans have been lauded in the popular press and across the social sciences, we will argue that enthusiasm for pre-analysis plans should be tempered for several reasons. Hypothesis registries are a database of all projects attempted; the goal of this promising mechanism is to alleviate the \"file drawer problem,\" which is that statistically significant results are more likely to be published, while other results are consigned to the researcher's \"file drawer.\" Finally, we evaluate the efficacy of replications. We argue that even with modest amounts of researcher bias—either replication attempts bent on proving or disproving the published work, or poor replication attempts—replications correct even the most inaccurate beliefs within three to five replications. We offer practical proposals for how to increase the incentives for researchers to carry out replications.","tags":["Preregistration","Critique"],"title":"Pre-analysis Plans Have Limited Upside, Especially Where Replications Are Feasible","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"b9fdd2e9fd765e814dc51b6d7316d6d6","permalink":"https://forrt.org/curated_resources/pre-analysis-plans-a-stocktaking/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/pre-analysis-plans-a-stocktaking/","section":"curated_resources","summary":"The evidence-based community has championed the public registration of pre-analysis plans (PAPs) as a solution to the problem of research credibility, but without any evidence that PAPs actually bolster the credibility of research. We analyze a representative sample of 195 pre-analysis plans (PAPs) from the American Economic Association (AEA) and Evidence in Governance and Politics (EGAP) registration platforms to assess whether PAPs are sufficiently clear, precise and comprehensive to be able to achieve their objectives of preventing “fishing” and reducing the scope for post-hoc adjustment of research hypotheses. We also analyze a subset of 93 PAPs from projects that have resulted in publicly available papers to ascertain how faithfully they adhere to their pre-registered specifications and hypotheses. We find significant variation in the extent to which PAPs are accomplishing the goals they were designed to achieve","tags":["Analysis","Policy"],"title":"Pre-analysis Plans: A Stocktaking","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"036dbe7b142937412ae818742b257a51","permalink":"https://forrt.org/curated_resources/pre-analysis-plans-an-early-stocktaking/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/pre-analysis-plans-an-early-stocktaking/","section":"curated_resources","summary":"Pre-analysis plans (PAPs) have been championed as a solution to the problem of research credibility, but without any evidence that PAPs actually bolster the credibility of research. We analyze a representative sample of 195 PAPs registered on the Evidence in Governance and Politics (EGAP) and American Economic Association (AEA) registration platforms to assess whether PAPs registered in the early days of pre-registration (2011–2016) were sufficiently clear, precise, and comprehensive to achieve their objective of preventing “fishing” and reducing the scope for post-hoc adjustment of research hypotheses. We also analyze a subset of ninety-three PAPs from projects that resulted in publicly available papers to ascertain how faithfully they adhere to their pre-registered specifications and hypotheses. We find significant variation in the extent to which PAPs registered during this period accomplished the goals they were designed to achieve. We discuss these findings in light of both the costs and benefits of pre-registration, showing how our results speak to the various arguments that have been made in support of and against PAPs. We also highlight the norms and institutions that will need to be strengthened to augment the power of PAPs to improve research credibility and to create incentives for researchers to invest in both producing and policing them.","tags":["Preregistration","Pre-analysis Plans"],"title":"Pre-Analysis Plans: An Early Stocktaking","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"2ca2c15d378e225aa63689807a221bfb","permalink":"https://forrt.org/curated_resources/pre-registration-and-results-free-review/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/pre-registration-and-results-free-review/","section":"curated_resources","summary":"Amidst rising concern about publication bias, pre-registration and results-blind review have grown rapidly in use. Yet discussion of both the problem of publication bias and of potential solutions has been remarkably narrow in scope: publication bias has been understood largely as a problem afflicting quantitative studies, while pre-registration and results-blind review have been almost exclusively applied to experimental or otherwise prospective research. This chapter examines the potential contributions of pre-registration and results-blind review to qualitative and quantitative retrospective research. First, the chapter provides an empirical assessment of the degree of publication bias in qualitative political science research. Second, it elaborates a general analytic framework for evaluating the feasbility and utility of pre-registration and results-blind review for confirmatory studies. Third, through a review of published studies, the paper demonstrates that much observational—and, especially, qualitative—political science research displays features that would make for credible pre-registration. The paper concludes that pre-registration and results-blind review have the potential to enhance the validity of confirmatory research across a range of empirical methods, while elevating exploratory work by making it harder to disguise discovery as testing.","tags":["Preregistration","Publication Bias","Qualitative Research","Political Science","Observational Research","Results-Blind Review"],"title":"Pre-registration and Results-Free Review in Observational and Qualitative Research","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1689122843,"objectID":"97d819ed67d368d62f9943445ee69fc2","permalink":"https://forrt.org/curated_resources/pre-registration-in-social-psychology-a/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/pre-registration-in-social-psychology-a/","section":"curated_resources","summary":"Pre-registration of studies before they are conducted has recently become more feasible for researchers, and is encouraged by an increasing number of journals. However, because the practice of pre-registration is relatively new to psychological science, specific guidelines for the content of registrations are still in a formative stage. After giving a brief history of pre-registration in medical and psychological research, we outline two different models that can be applied—reviewed and unreviewed pre-registration—and discuss the advantages of each model to science as a whole and to the individual scientist, as well as some of their drawbacks and limitations. Finally, we present and justify a proposed standard template that can facilitate pre-registration. Researchers can use the template before and during the editorial process to meet article requirements and enhance the robustness of their scholarly efforts. (","tags":["Transparency"],"title":"Pre-registration in social psychology—A discussion and suggested template","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"aea6bbb6ca535a908e931faca09b5212","permalink":"https://forrt.org/curated_resources/pre-registration-in-the-undergraduate-di/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/pre-registration-in-the-undergraduate-di/","section":"curated_resources","summary":"Over recent years, psychology has become increasingly concerned with reproducibility and replicability of research findings (Munafò et al., 2017). One method of ensuring that research is hypothesis driven, as opposed to data driven, is the process of publicly pre-registering a study's hypotheses, data analysis plan, and procedure prior to data collection (Nosek et al., 2018). This paper discusses the potential benefits of introducing pre-registration to the undergraduate dissertation. The utility of pre-registration as a pedagogic practice within dissertation supervision is also critically appraised, with reference to open science literature. Here, it is proposed that encouraging pre-registration of undergraduate dissertation work may alleviate some pedagogic challenges, such as statistics anxiety, questionable research practices, and research clarity and structure. Perceived barriers, such as time and resource constraints, are also discussed.","tags":["Psychology","Undergraduate Students","Student Research","Teaching Methods","Research Reports","Supervision","Barriers","Role","Research Design","Foreign Countries"],"title":"Pre-Registration in the Undergraduate Dissertation: A Critical Discussion","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1689122843,"objectID":"21b5155c8778cbb3c94ed86508f4bc25","permalink":"https://forrt.org/curated_resources/pre-registration-of-clinical-trials-is-a/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/pre-registration-of-clinical-trials-is-a/","section":"curated_resources","summary":"Pre-registration of clinical trials is associated with fewer positive findings","tags":["Blog","Open Science","Reproducibility Crisis and Credibility Revolution"],"title":"Pre-registration of clinical trials is associated with fewer positive findings","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"0c7ba70f8d46a93feefe1a0a1a2f240d","permalink":"https://forrt.org/curated_resources/pre-registration-of-mathematical-models/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/pre-registration-of-mathematical-models/","section":"curated_resources","summary":"Pre-registration is a research practice where a protocol is deposited in a repository before a scientific project is performed. The protocol may be publicly visible immediately upon deposition or it may remain hidden until the work is completed/published. It may include the analysis plan, outcomes, and/or information about how evaluation of performance (e.g. forecasting ability) will be made, Pre-registration aims to enhance the trust one can put on scientific work. Deviations from the original plan, may still often be desirable, but pre-registration makes them transparent. While pre-registration has been advocated and used to variable extent in diverse types of research, there has been relatively little attention given to the possibility of pre-registration for mathematical modeling studies. Feasibility of pre-registration depends on the type of modeling and the ability to pre-specify processes and outcomes. In some types of modeling, in particular those that involve forecasting or other outcomes that can be appraised in the future, trust in model performance would be enhanced through pre-registration. Pre-registration can also be seen as a component of a larger suite of research practices that aim to improve documentation, transparency, and sharing—eventually allowing better reproducibility of the research work. The current commentary discusses the evolving landscape of the concept of pre-registration as it relates to different mathematical modeling activities, the potential advantages and disadvantages, feasibility issues, and realistic goals.","tags":["Mathematical Modeling","Preregistration","Bias","Reproducibility"],"title":"Pre-registration of mathematical models","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"da78bfc77c8f3894d1e7ce282e459139","permalink":"https://forrt.org/curated_resources/pre-registration-weighing-costs-and-bene/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/pre-registration-weighing-costs-and-bene/","section":"curated_resources","summary":"In the past decade, the social and behavioral sciences underwent a methodological revolution, offering practical prescriptions for improving the replicability and reproducibility of research results. One key to reforming science is a simple and scalable practice: pre-registration. Pre-registration constitutes pre-specifying an analysis plan prior to data collection. A growing chorus of articles discusses the prescriptive, field-wide benefits of pre-registration. To increase adoption, however, scientists need to know who currently pre-registers and understand perceived barriers to doing so. Thus, we weigh costs and benefits of pre-registration. Our survey of researchers reveals generational differences in who pre-registers and uncertainty regarding how pre-registration benefits individual researchers. We leverage these data to directly address researchers’ uncertainty by clarifying why pre-registration improves the research process itself. Finally, we discuss how to pre-register and compare available resources. The present work examines the who, why, and how of pre-registration in order to weigh the costs and benefits of pre-registration to researchers and motivate continued adoption.","tags":["Preregistration"],"title":"Pre-registration: Weighing costs and benefits for researchers","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"194806d5582aa443123659efb9d32cc2","permalink":"https://forrt.org/curated_resources/pre-registration-why-and-how/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/pre-registration-why-and-how/","section":"curated_resources","summary":"In this article, we (1) discuss the reasons why pre-registration is a good idea, both for the field and individual researchers, (2) respond to arguments against pre-registration, (3) describe how to best write and review a pre-registration, and (4) comment on pre-registration’s rapidly accelerating popularity. Along the way, we describe the (big) problem that pre-registration can solve (i.e., false positives caused by p-hacking), while also offering viable solutions to the problems that pre-registration cannot solve (e.g., hidden confounds or fraud). Pre-registration does not guarantee that every published finding will be true, but without it you can safely bet that many more will be false. It is time for our field to embrace pre-registration, while taking steps to ensure that it is done right.","tags":["Preregistration","Consumer Psychology","Advertising"],"title":"Pre-registration: Why and How","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"dab77434e48176dfda7539f55410570d","permalink":"https://forrt.org/curated_resources/pre-results-review-in-economics-lessons/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/pre-results-review-in-economics-lessons/","section":"curated_resources","summary":"Hear from Andrew Foster, editor at the Journal of Development Economics, and Irenaeus Wolff, a guest editor for Experimental Economics, as they discuss their experiences with implementing the Registered Reports format, how it was received by authors, and the trends they noticed after adoption. Aleksandar Bogdanoski of BITSS also joins us to explore pre-results review, how to facilitate the process at journals, and best practices for supporting authors and reviewers.","tags":["Academic Publishing","Center for Open Science","Data","Publishing Formats","Registered Reports","Research","Research Methods","Research Transparency"],"title":"Pre-results Review in Economics: Lessons Learned from Setting up Registered Reports","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1721390377,"objectID":"ee1b08c2f1749506dcfe3e0b7f3c41de","permalink":"https://forrt.org/glossary/english/predatory_publishing/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/english/predatory_publishing/","section":"glossary","summary":"","tags":null,"title":"Predatory Publishing","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1635961284,"objectID":"c32c7a3a08d9a2149f5c018a821ea9a8","permalink":"https://forrt.org/glossary/vbeta/predatory-publishing/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/vbeta/predatory-publishing/","section":"glossary","summary":"","tags":null,"title":"Predatory Publishing","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1720865851,"objectID":"3a4e182665bbaeea95c67e8dc9529f1e","permalink":"https://forrt.org/glossary/german/predatory_publishing/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/german/predatory_publishing/","section":"glossary","summary":"","tags":null,"title":"Predatory Publishing (Predatory Verlagswesen)","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1689122843,"objectID":"4d3df2ed8a1910de0fac2bb0eebfdcda","permalink":"https://forrt.org/curated_resources/prediction-interval-what-to-expect-when/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/prediction-interval-what-to-expect-when/","section":"curated_resources","summary":"A challenge when interpreting replications is determining whether the results of a replication “successfully” replicate the original study. Looking for consistency between two studies is challenging because individual studies are susceptible to many sources of error that can cause study results to deviate from each other and the population effect in unpredictable directions and magnitudes. In the current paper, we derive methods to compute a prediction interval, a range of results that can be expected in a replication due to chance (i.e., sampling error), for means and commonly used indexes of effect size: correlations and d-values. The prediction interval is calculable based on objective study characteristics (i.e., effect size of the original study and sample sizes of the original study and planned replication) even when sample sizes across studies are unequal. The prediction interval provides an a priori method for assessing if the difference between an original and replication result is consistent with what can be expected due to sample error alone. We provide open-source software tools that allow researchers, reviewers, replicators, and editors to easily calculate prediction intervals.","tags":[""],"title":"Prediction Interval: What to Expect When You’re Expecting … A Replication","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"9c55c103c554d6ba22eae22b9edc08b6","permalink":"https://forrt.org/curated_resources/preliminary-analysis-of-covid-19-academi/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/preliminary-analysis-of-covid-19-academi/","section":"curated_resources","summary":"The Pandemic of COVID-19, an infectious disease caused by SARS-CoV-2 motivated the scientific community to work together in order to gather, organize, process and distribute data on the novel biomedical hazard. Here, we analyzed how the scientific community responded to this challenge by quantifying distribution and availability patterns of the academic information related to COVID-19. The aim of this study was to assess the quality of the information flow and scientific collaboration, two factors we believe to be critical for finding new solutions for the ongoing pandemic. The RISmed R package, and a custom Python script were used to fetch metadata on articles indexed in PubMed and published on Rxiv preprint server. Scopus was manually searched and the metadata was exported in BibTex file. Publication rate and publication status, affiliation and author count per article, and submission-to-publication time were analysed in R. Biblioshiny application was used to create a world collaboration map. Preliminary data suggest that COVID-19 pandemic resulted in generation of a large amount of scientific data, and demonstrates potential problems regarding the information velocity, availability, and scientific collaboration in the early stages of the pandemic. More specifically, the results indicate precarious overload of the standard publication systems, significant problems with data availability and apparent deficient collaboration. In conclusion, we believe the scientific community could have used the data more efficiently in order to create proper foundations for finding new solutions for the COVID-19 pandemic. Moreover, we believe we can learn from this on the go and adopt open science principles and a more mindful approach to COVID-19-related data to accelerate the discovery of more efficient solutions. We take this opportunity to invite our colleagues to contribute to this global scientific collaboration by publishing their findings with maximal transparency.","tags":["COVID-19","Open Science","Data","Bibliometric","Pandemic"],"title":"Preliminary analysis of COVID-19 academic information patterns: a call for open science in the times of closed borders","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"64e047cef41693c4d9c10aa4a4d7d63d","permalink":"https://forrt.org/curated_resources/premiering-pre-registration-at-plos-biol/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/premiering-pre-registration-at-plos-biol/","section":"curated_resources","summary":"Pre-registration promises to address some of the problems with traditional peer-review. As we publish our first Registered Report, we take stock of two years of submissions and the future possibilities of this approach.","tags":["Peer Review","Research Grants","Neuroscience","Research Assessment","Medical Facies","Pandemics","Publication Ethics","Research Design"],"title":"Premiering pre-registration at PLOS Biology","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1721390377,"objectID":"d1273f3946a53ece41316455c4fb7d24","permalink":"https://forrt.org/glossary/english/prepare_guidelines/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/english/prepare_guidelines/","section":"glossary","summary":"","tags":null,"title":"PREPARE Guidelines","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1635961284,"objectID":"78c436f8e16f09a1cbc9eb5d0589f5f8","permalink":"https://forrt.org/glossary/vbeta/prepare-guidelines/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/vbeta/prepare-guidelines/","section":"glossary","summary":"","tags":null,"title":"PREPARE Guidelines","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1720865851,"objectID":"a3ff45928e16c77931560eb29b3e04bf","permalink":"https://forrt.org/glossary/german/prepare_guidelines/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/german/prepare_guidelines/","section":"glossary","summary":"","tags":null,"title":"PREPARE Guidelines (PREPARE Leitlinien)","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"af41a99957331de977b38bcf3fb664ec","permalink":"https://forrt.org/curated_resources/preparing-code-and-data-for-computationa/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/preparing-code-and-data-for-computationa/","section":"curated_resources","summary":"Computational analyses are playing an increasingly central role in research. Journals, funders, and researchers are calling for published research to include associated data and code. However, many involved in research have not received training in best practices and tools for sharing code and data. This course aims to address this gap in training while also providing those who support researchers with curated best practices guidance and tools.This course is unique compared to other reproducibility courses due to its practical, step-by-step design. It is comprised of hands-on exercises to prepare research code and data for computationally reproducible publication. Although the course starts with some brief introductory information about computational reproducibility, the bulk of the course is guided work with data and code. Participants move through preparing research for reuse, organization, documentation, automation, and submitting their code and data to share. Tools that support reproducibility will be introduced (Code Ocean), but all lessons will be platform agnostic.Level: IntermediateIntended audience: The course is targeted at researchers and research support staff who are involved in the preparation and publication of research materials. Anyone with an interest in reproducible publication is welcome. The course is especially useful for those looking to learn practical steps for improving the computational reproducibility of their own research.","tags":["Data","Librarians","Materials","Reproducibility","Research Data Management Tools","Researchers"],"title":"Preparing code and data for computationally reproducible collaboration and publication: a hands-on workshop","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1721390377,"objectID":"7ee03b70b1a9b356fe7c5bed3205c964","permalink":"https://forrt.org/glossary/english/preprint/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/english/preprint/","section":"glossary","summary":"","tags":null,"title":"Preprint","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1635961284,"objectID":"9299b43929a92df24daf10c2e16aa3da","permalink":"https://forrt.org/glossary/vbeta/preprint/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/vbeta/preprint/","section":"glossary","summary":"","tags":null,"title":"Preprint","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1720865851,"objectID":"4e0d9e967f4fab03dc592116558e9b64","permalink":"https://forrt.org/glossary/german/preprint/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/german/preprint/","section":"glossary","summary":"","tags":null,"title":"Preprint (Vorabdruck)","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"d6f26c2ef9580dc3c35455e95d097025","permalink":"https://forrt.org/curated_resources/preregistering-qualitative-research/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/preregistering-qualitative-research/","section":"curated_resources","summary":"The threat to reproducibility and awareness of current rates of research misbehavior sparked initiatives to better academic science. One initiative is preregistration of quantitative research. We investigate whether the preregistration format could also be used to boost the credibility of qualitative research. A crucial distinction underlying preregistration is that between prediction and postdiction. In qualitative research, data are used to decide which way interpretation should move forward, using data to generate hypotheses and new research questions. Qualitative research is thus a real-life example of postdiction research. Some may object to the idea of preregistering qualitative studies because qualitative research generally does not test hypotheses, and because qualitative research design is typically flexible and subjective. We rebut these objections, arguing that making hypotheses explicit is just one feature of preregistration, that flexibility can be tracked using preregistration, and that preregistration would provide a check on subjectivity. We then contextualize preregistrations alongside another initiative to enhance credibility in qualitative research: the confirmability audit. Besides, preregistering qualitative studies is practically useful to combating dissemination bias and could incentivize qualitative researchers to report constantly on their study's development. We conclude with suggested modifications to the Open Science Framework preregistration form to tailor it for qualitative studies.","tags":["Preregistration","Qualitative Research"],"title":"Preregistering qualitative research","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"82bab901e8ad9c28951c5afd8b5cce04","permalink":"https://forrt.org/curated_resources/preregistering-qualitative-research-a-de/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/preregistering-qualitative-research-a-de/","section":"curated_resources","summary":"Preregistrations—records made a priori about study designs and analysis plans and placed in open repositories—are thought to strengthen the credibility and transparency of research. Different authors have put forth arguments in favor of introducing this practice in qualitative research and made suggestions for what to include in a qualitative preregistration form. The goal of this study was to gauge and understand what parts of preregistration templates qualitative researchers would find helpful and informative. We used an online Delphi study design consisting of two rounds with feedback reports in between. In total, 48 researchers participated (response rate: 16%). In round 1, panelists considered 14 proposed items relevant to include in the preregistration form, but two items had relevance scores just below our predefined criterion (68%) with mixed argument and were put forth again. We combined items where possible, leading to 11 revised items. In round 2, panelists agreed on including the two remaining items. Panelists also converged on suggested terminology and elaborations, except for two terms for which they provided clear arguments. The result is an agreement-based form for the preregistration of qualitative studies that consists of 13 items. The form will be made available as a registration option on Open Science Framework (osf.io). We believe it is important to assure that the strength of qualitative research, which is its flexibility to adapt, adjust and respond, is not lost in preregistration. The preregistration should provide a systematic starting point.","tags":["Preregistration","Qualitative Research"],"title":"Preregistering Qualitative Research: A Delphi Study","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1721390377,"objectID":"b623632d776c69231e1fe55115890102","permalink":"https://forrt.org/glossary/english/preregistration/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/english/preregistration/","section":"glossary","summary":"","tags":null,"title":"Preregistration","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1635961284,"objectID":"c81138524690daffe4fd3a1f7c490033","permalink":"https://forrt.org/glossary/vbeta/preregistration/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/vbeta/preregistration/","section":"glossary","summary":"","tags":null,"title":"Preregistration","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1720865851,"objectID":"bb43d5934e3507cc94bbbc9af64dd387","permalink":"https://forrt.org/glossary/german/preregistration/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/german/preregistration/","section":"glossary","summary":"","tags":null,"title":"Preregistration (Präregistrierung)","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"c83d6371453d275736c91bcda86a1c4b","permalink":"https://forrt.org/curated_resources/preregistration-and-incentives/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/preregistration-and-incentives/","section":"curated_resources","summary":"Preregistering study designs is broadly supported as improving scientific credibility but criticized for limiting the scope of what can be learned. The paper investigates this tradeoff in a model where a researcher conducts a study and aims to convince an evaluator that the results are worth publishing. When both begin equally informed, the aim to publish is closely aligned with producing informative research, leaving preregistration redundant. When better informed, the researcher can credibly signal confidence by committing to a hypothesis. Thus, whether preregistration should be the norm in a field depends on how critically private information plays in designing studies.","tags":["Preregistration","Information Acquisition","Verifiable Disclosure"],"title":"Preregistration and Incentives","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"1f6960f2e6099e16703fd588b8a3f60e","permalink":"https://forrt.org/curated_resources/preregistration-and-registered-reports-i/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/preregistration-and-registered-reports-i/","section":"curated_resources","summary":"Both within and outside of sociology, there are conversations about methods to reduce error and improve research quality—one such method is preregistration and its counterpart, registered reports. Preregistration is the process of detailing research questions, variables, analysis plans, etc. before conducting research. Registered reports take this one step further, with a paper being reviewed on the merit of these plans, not its findings. In this manuscript, I detail preregistration’s and registered reports’ strengths and weaknesses for improving the quality of sociological research. I conclude by considering the implications of a structural-level adoption of preregistration and registered reports. Importantly, I do not recommend that all sociologists use preregistration and registered reports for all studies. Rather, I discuss the potential benefits and genuine limitations of preregistration and registered reports for the individual sociologist and the discipline.","tags":["Preregistration","Open Science","Registered Reports","Reproducibility","Transparency"],"title":"Preregistration and Registered Reports in Sociology: Strengths, Weaknesses, and Other Considerations","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"e6900958f7b5f0439f52ea30cb6bc6fe","permalink":"https://forrt.org/curated_resources/preregistration-and-reproducibility/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/preregistration-and-reproducibility/","section":"curated_resources","summary":"Many view preregistration as a promising way to improve research credibility. However, scholars have argued that using pre-analysis plans in Experimental Economics has limited benefits. This paper argues that preregistration of studies is likely to improve research credibility. I show that in a setting with selective reporting and low statistical power, effect sizes are highly inflated, and this translates into low reproducibility. Preregistering the original studies could avoid such inflation of effect sizes—through increasing the share of “frequentist” researchers—and would lead to more credible power analyses for replication studies. Numerical applications of the model indicate that the inflation bias could be very large in practice, and available empirical evidence is in line with the central assumptions of the model.","tags":["Experimental Economics","Preregistration","Estimation","Reproducibility"],"title":"Preregistration and reproducibility","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"5c4511f5130fa3d5cbba9843d85e478f","permalink":"https://forrt.org/curated_resources/preregistration-as-a-way-to-limit-questi/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/preregistration-as-a-way-to-limit-questi/","section":"curated_resources","summary":"This paper discusses two phenomena that threaten the credibility of scientific research and suggests an approach to limiting the extent of their use in advertising research. HARKing (hypothesizing after the results are known) refers to when hypotheses are formulated or modified after the results of a study are known. P-hacking refers to various practices (e.g., adding respondents, introducing control variables) that increase the likelihood of obtaining statistically significant results from a study. Both of these practices increase the risk of false positives (Type I errors) in research results and it is in the interest of the advertising research field that they are limited. Voluntary preregistration, where researchers commit to and register their research design and analytical approach before conducting the study, is put forward as a means to limiting both HARKing and p-hacking.","tags":["Preregistration","Advertising","Questionable Research Practices"],"title":"Preregistration as a way to limit questionable research practice in advertising research","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1689122843,"objectID":"3e7fcc454ea4828944c337adf36c54b3","permalink":"https://forrt.org/curated_resources/preregistration-becoming-the-norm-in-psy/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/preregistration-becoming-the-norm-in-psy/","section":"curated_resources","summary":"A blog about Preregistration Becoming the Norm in Psychological Science","tags":["Blog"],"title":"Preregistration Becoming the Norm in Psychological Science","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"a81bf415cbaaf71f1bdc8d40df8aa305","permalink":"https://forrt.org/curated_resources/preregistration-for-quantitative-researc/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/preregistration-for-quantitative-researc/","section":"curated_resources","summary":"A template to use for pre-registration","tags":["Transparency"],"title":"Preregistration for Quantitative Research in Psychology Template","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"f554fb069d646ffe73e1c8082ebc24c1","permalink":"https://forrt.org/curated_resources/preregistration-in-complex-contexts-a-pr/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/preregistration-in-complex-contexts-a-pr/","section":"curated_resources","summary":"In recent years, open science practices have become increasingly popular in psychology and related sciences. These practices aim to increase rigour and transparency in science as a potential response to the challenges posed by the replication crisis. Many of these reforms -- including the highly influential preregistration -- have been designed for experimental work that tests simple hypotheses with standard statistical analyses, such as assessing whether an experimental manipulation has an effect on a variable of interest. However, psychology is a diverse field of research, and the somewhat narrow focus of the prevalent discussions surrounding and templates for preregistration has led to debates on how appropriate these reforms are for areas of research with more diverse hypotheses and more complex methods of analysis, such as cognitive modelling research within mathematical psychology. Our article attempts to bridge the gap between open science and mathematical psychology, focusing on the type of cognitive modelling that Crüwell, Stefan, \u0026 Evans (2019) labelled model application, where researchers apply a cognitive model as a measurement tool to test hypotheses about parameters of the cognitive model. Specifically, we (1) discuss several potential researcher degrees of freedom within model application, (2) provide the first preregistration template for model application, and (3) provide an example of a preregistered model application using our preregistration template. More broadly, we hope that our discussions and proposals constructively advance the debate surrounding preregistration in cognitive modelling, and provide a guide for how preregistration templates may be developed in other diverse or complex research contexts.","tags":["Analysis","Preregistration","Publishing","Reproducibility","Researchers"],"title":"Preregistration in Complex Contexts: A Preregistration Template for the Application of Cognitive Models","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"b1423887ba3524c73e11ee2c1049357f","permalink":"https://forrt.org/curated_resources/preregistration-in-diverse-contexts-a-pr/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/preregistration-in-diverse-contexts-a-pr/","section":"curated_resources","summary":"In recent years, open science practices have become increasingly popular in psychology and related sciences. These practices aim to increase rigour and transparency in science as a potential response to the challenges posed by the replication crisis. Many of these reforms—including the increasingly used preregistration—have been designed for purely experimental work that tests straightforward hypotheses with standard inferential statistical analyses, such as assessing whether an experimental manipulation has an effect on a variable of interest. But psychology is a diverse field of research. The somewhat narrow focus of the prevalent discussions surrounding and templates for preregistration has led to debates on how appropriate these reforms are for areas of research with more diverse hypotheses and more intricate methods of analysis, such as cognitive modelling research within mathematical psychology. Our article attempts to bridge the gap between open science and mathematical psychology, focusing on the type of cognitive modelling that Crüwell et al. (Crüwell S, Stefan AM, Evans NJ. 2019 Robust standards in cognitive science. Comput. Brain Behav. 2, 255–265) labelled model application, where researchers apply a cognitive model as a measurement tool to test hypotheses about parameters of the cognitive model. Specifically, we (i) discuss several potential researcher degrees of freedom within model application, (ii) provide the first preregistration template for model application and (iii) provide an example of a preregistered model application using our preregistration template. More broadly, we hope that our discussions and concrete proposals constructively advance the mostly abstract current debate surrounding preregistration in cognitive modelling, and provide a guide for how preregistration templates may be developed in other diverse or intricate research contexts.","tags":["Reproducibility","Transparency","Open Science","Cognitive Modelling","Preregistration"],"title":"Preregistration in diverse contexts: A preregistration template for the application of cognitive models","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"4564e7c983e85c12a7f3a9c84471192f","permalink":"https://forrt.org/curated_resources/preregistration-in-experimental-linguist/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/preregistration-in-experimental-linguist/","section":"curated_resources","summary":"The current publication system neither incentivizes publishing null results nor direct replication attempts, which biases the scientific record toward novel findings that appear to support presented hypotheses (referred to as “publication bias”). Moreover, flexibility in data collection, measurement, and analysis (referred to as “researcher degrees of freedom”) can lead to overconfident beliefs in the robustness of a statistical relationship. One way to systematically decrease publication bias and researcher degrees of freedom is preregistration. A preregistration is a time-stamped document that specifies how data is to be collected, measured, and analyzed prior to data collection. While preregistration is a powerful tool to reduce bias, it comes with certain challenges and limitations which have to be evaluated for each scientific discipline individually. This paper discusses the application, challenges and limitations of preregistration for experimental linguistic research.","tags":["Confirmatory","Exploratory","Preregistration","Publication Bias","Registered Report","Researcher Degrees of Freedom"],"title":"Preregistration in experimental linguistics: Applications, challenges, and limitations","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"eb97df1c8df922c78f7ffa56861c8be5","permalink":"https://forrt.org/curated_resources/preregistration-in-infant-research-a-pri/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/preregistration-in-infant-research-a-pri/","section":"curated_resources","summary":"Preregistration, the act of specifying a research plan in advance, is becoming more common in scientific research. Infant researchers contend with unique problems that might make preregistration particularly challenging. Infants are a hard-to-reach population, usually yielding small sample sizes, they can only complete a limited number of trials, and they can be excluded based on hard-to-predict complications (e.g., parental interference, fussiness). In addition, as effects themselves potentially change with age and population, it is hard to calculate an a priori effect size. At the same time, these very factors make preregistration in infant studies a valuable tool. A priori examination of the planned study, including the hypotheses, sample size, and resulting statistical power, increases the credibility of single studies and adds value to the field. Preregistration might also improve explicit decision making to create better studies. We present an in-depth discussion of the issues uniquely relevant to infant researchers, and ways to contend with them in preregistration and study planning. We provide recommendations to researchers interested in following current best practices.","tags":["Infant Research","Preregistration"],"title":"Preregistration in infant research—A primer","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"6c50755640bb89e8405d381b4a1e9a67","permalink":"https://forrt.org/curated_resources/preregistration-in-single-case-design-re/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/preregistration-in-single-case-design-re/","section":"curated_resources","summary":"To draw informed conclusions from research studies, research consumers need full and accurate descriptions of study methods and procedures. Preregistration has been proposed as a means to clarify reporting of research methods and procedures, with the goal of reducing bias in research. However, preregistration has been applied primarily to research studies utilizing group designs. In this article, we discuss general issues in preregistration and consider the use of preregistration in single-case design research, particularly as it relates to differing applications of this methodology. We then provide a rationale and make specific recommendations for preregistering single-case design research, including guidelines for preregistering basic descriptive information, research questions, participant characteristics, baseline conditions, independent and dependent variables, hypotheses, and phase-change decisions.","tags":["Preregistration","Single Case Design"],"title":"Preregistration in Single-Case Design Research","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"d5caf0585e4b31dde42d2165b127b7b9","permalink":"https://forrt.org/curated_resources/preregistration-is-hard-and-worthwhile/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/preregistration-is-hard-and-worthwhile/","section":"curated_resources","summary":"Preregistration clarifies the distinction between planned and unplanned research by reducing unnoticed flexibility. This improves credibility of findings and calibration of uncertainty. However, making decisions before conducting analyses requires practice. During report writing, respecting both what was planned and what actually happened requires good judgment and humility in making claims.","tags":["Preregistration"],"title":"Preregistration Is Hard, And Worthwhile","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"f558f5a6d4f1be4b581aab732074a520","permalink":"https://forrt.org/curated_resources/preregistration-is-neither-sufficient-no/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/preregistration-is-neither-sufficient-no/","section":"curated_resources","summary":"To address widespread perceptions of a reproducibility crisis in the social sciences, a growing number of scholars recommend the systematic preregistration of empirical studies. The purpose of this article is to contribute to an epistemological dialogue on the value of preregistration in consumer research by identifying the limitations, drawbacks, and potential adverse effects of a preregistration system. After a brief review of some of the implementation challenges that commonly arise with preregistration, we raise three levels of issues with a system of preregistration. First, we identify its limitations as a means of advancing consumer knowledge, thus questioning the sufficiency of preregistration in promoting good consumer science. Second, we elaborate on why consumer science can progress even in the absence of preregistration, thereby also questioning the necessity of preregistration in promoting good consumer science. Third, we discuss serious potential adverse effects of preregistration, both at the individual researcher level and at the level of the field as a whole. We conclude by offering a broader perspective on the narrower role that preregistration can play within the general pursuit of building robust and useful knowledge about consumers.","tags":["Preregistration","Open Science","Reproducibility Crisis","Consumer Research","Critiques"],"title":"Preregistration Is Neither Sufficient nor Necessary for Good Science","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"b7c21ff7203bdf994b9b8bad7c45878f","permalink":"https://forrt.org/curated_resources/preregistration-of-analyses-of-preexisti/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/preregistration-of-analyses-of-preexisti/","section":"curated_resources","summary":"The preregistration of a study’s hypotheses, methods, and data-analyses steps is becoming a popular psychological research practice. To date, most of the discussion on study preregistration has focused on the preregistration of studies that include the collection of original data. However, much of the research in psychology relies on the (re-)analysis of preexisting data. Importantly, this type of studies is different from original studies as researchers cannot change major aspects of the study (e.g., experimental manipulations, sample size). Here, we provide arguments as to why it is useful to preregister analyses of preexisting data, discuss practical considerations, consider potential concerns, and introduce a preregistration template tailored for studies focused on the analyses of preexisting data. We argue that the preregistration of hypotheses and data-analyses for analyses of preexisting data is an important step towards more transparent psychological research.","tags":["Replicability","Transparency","Open Science","Archival Data","Preregistration"],"title":"Preregistration of Analyses of Preexisting Data","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"d17d9673bb5e51d31c616e5745b84775","permalink":"https://forrt.org/curated_resources/preregistration-of-animal-research-proto/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/preregistration-of-animal-research-proto/","section":"curated_resources","summary":"Open, prospective registration of a study protocol can improve research rigour in a number of ways. Through preregistration, key features of the study’s methodology are recorded and maintained as a permanent record, enabling comparison of the completed study with what was planned. By recording the study hypothesis and planned outcomes a priori, preregistration creates transparency and can reduce the risk of several common biases, such as hypothesising after results are known and outcome switching or selective outcome reporting. Second, preregistration raises awareness of measures to reduce bias, such as randomisation and blinding. Third, preregistration provides a comprehensive listing of planned studies, which can prevent unnecessary duplication and reduce publication bias. Although commonly acknowledged and applied in clinical research since 2000, preregistration of animal studies is not yet the norm. In 2018 we launched the first dedicated, open, online register for animal study protocols: www.preclinicaltrials.eu. Here, we provide insight in the development of preclinicaltrials.eu (PCT) and evaluate its use during the first 3 years after its launch. Furthermore, we elaborate on ongoing developments such as the rise of comparable registries, increasing support for preregistration in the Netherlands—which led to the funding of PCT by the Dutch government—and pilots of mandatory preregistration by several funding bodies. We show the international coverage of currently registered protocols but with the overall low number of (pre)registered protocols.","tags":["Biomedical Research","Research Design"],"title":"Preregistration of animal research protocols: development and 3-year overview of preclinicaltrials.eu","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"866eabf199f5c889113b8599376a2b85","permalink":"https://forrt.org/curated_resources/preregistration-of-epidemiologic-studies/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/preregistration-of-epidemiologic-studies/","section":"curated_resources","summary":"The idea for the registration of observational epidemiologic studies (with their protocols, hypotheses, and analysis plans) is modeled on the registration of randomized clinical trials (RCTs). The movement that led to the registration of RCTs was fed by distrust that important findings would be withheld—against the best interests of future patients. Randomized controlled trials test the end-product of a scientific process: whether a therapy should be applied to patients. Indeed, the application of a new therapy to hundreds of thousands of future patients should not depend on the whim of whomever decides what to publish and not to publish.","tags":["Preregistration","Epidemiology"],"title":"Preregistration of Epidemiologic Studies: An Ill-founded Mix of Ideas","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"bb30efc18d2d200ad04a3643b6f85f55","permalink":"https://forrt.org/curated_resources/preregistration-of-epidemiology-protocol_3/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/preregistration-of-epidemiology-protocol_3/","section":"curated_resources","summary":"Professors Kogevinas and Stayner do not address the central point of my commentary but they do pursue an issue on which I welcome the opportunity to elucidate. As a student of epidemiology myself, I appreciate professors who are more critical of their discipline. Self-satisfaction and complacency cannot fuel the necessary innovation and evolution needed for the new challenges in the field of epidemiology. By all means, celebrate success but do learn from the mistakes. If the professoriate does not raise questions, then others, including thoughtful science writers, will do it for them. Fortunately, awareness of the many problems facing observational epidemiology is increasing.","tags":["Preregistration","Epidemiology"],"title":"Preregistration of Epidemiology Protocols (Author Response)","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"af221d201138c1d89aa4d7a823c781d0","permalink":"https://forrt.org/curated_resources/preregistration-of-epidemiology-protocol_2/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/preregistration-of-epidemiology-protocol_2/","section":"curated_resources","summary":"There must be something wrong when a professor of epidemiology includes the following statement in the concluding remarks of a commentary he has written: “Whether observational epidemiology has, on balance, contributed more to the good of public health than not is an open question.”1 This is a false statement and actually dangerous for public health. There is no question that the health of the population in this world is far better because of observational epidemiology and the resulting public health action. The public health impact is clearly huge if one considers just a few of the risk factors that have been identified by epidemiologic studies, such as the hazards associated with exposure to mainstream and second-hand smoke, air pollution, viral infections (HIV, HPV, and other), asbestos, ionizing radiation, alcohol and substance abuse, as well as the benefits associated with physical exercise and reduced cholesterol and the influence of obesity and socioeconomic factors. We really do not believe it is necessary to even defend this position. Bracken should have supported his statement by something better than a news article2 that is a standard for superficial and biased criticisms toward epidemiology. Epidemiologists who believe our work has not given much to society are unlikely to provide us with a good diagnosis of the problems, or a prescription for how epidemiologic research should be improved.","tags":["Preregistration","Epidemiology"],"title":"Preregistration of Epidemiology Protocols (Response)","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"59b54c649d9e381e8d7e632f770b1938","permalink":"https://forrt.org/curated_resources/preregistration-of-epidemiology-protocol/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/preregistration-of-epidemiology-protocol/","section":"curated_resources","summary":"It has been proposed that observational epidemiology protocols, just like those for randomized controlled trials (RCTs), should be preregistered. Unlike the editors of the Lancet and BMJ who have endorsed the proposal, the Epidemiology editors and 5 commentators all resisted. One does not have to agree with all the details of the workshop document to believe that protocol registration has substantial advantages in epidemiologic research. Some of them are reported in this article.","tags":["Preregistration","Epidemiology"],"title":"Preregistration of Epidemiology Protocols: A Commentary in Support","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"f438c0d5a5eb8daf356283f4db2a5ece","permalink":"https://forrt.org/curated_resources/preregistration-of-exploratory-research/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/preregistration-of-exploratory-research/","section":"curated_resources","summary":"Preregistration of study protocols and, in particular, Registered Reports are novel publishing formats that are currently gaining substantial traction. Besides rating the research question and soundness of methodology over outstanding significance of the results, they can help with antagonizing inadequate statistical power, selective reporting of results, undisclosed analytic flexibility, as well as publication bias. Preregistration works well when a clear hypothesis, primary outcome, and mode of analysis can be formulated. But is it also applicable and useful in discovery research, which develops theories and hypotheses, measurement techniques, and generates evidence that justifies further research? I will argue that only slight modifications are needed to harness the potential of preregistration and make exploratory research more trustworthy and useful.","tags":["Preregistration","Study Protocols","Registered Reports","Exploratory Research","Confirmatory Research"],"title":"Preregistration of exploratory research: Learning from the golden age of discovery","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"9ef99eec963290e9398def905b8648ef","permalink":"https://forrt.org/curated_resources/preregistration-of-information-systems-r/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/preregistration-of-information-systems-r/","section":"curated_resources","summary":"In this paper, we introduce the preregistration concept for experiments in the information systems (IS) discipline. Preregistration constitutes a way to commit to analytic steps before collecting or observing data and, thus, mitigate any biases authors may have (consciously or not) towards reporting significant findings. We explain why preregistration matters, how to preregister a study, the benefits of preregistration, and common arguments against preregistration. We also offer a call to action for authors to conduct more preregistered work in IS.","tags":["Preregistration","Information Systems"],"title":"Preregistration of Information Systems Research","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"6c4a7f87ff80e5f2f6629612adef2f66","permalink":"https://forrt.org/curated_resources/preregistration-of-machine-learning-rese/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/preregistration-of-machine-learning-rese/","section":"curated_resources","summary":"It is interesting to note that human intelligence thrives on what Peirce called abductive inferences (Peirce and Turrisi 1997, 241-56), which are neither inductive nor deductive. Abductive inferencing basically entails an informed guess as to the explanation of a set of observations. Building on Peirce, scientific research can be framed as starting with an abduction based on observation, generating an explanation (theory) from which a hypothesis (prediction) is deduced about subsequent observations, after which the prediction can be inductively tested against new observations. Building on Popper’s theory of falsification,1 hypotheses should be developed in a way that enables the rejection of the explanation – not merely its verification. A theory that explains why all swans are white should not just be verified by detecting ever more white swans, but tested against its potential falsification by searching for black swans.","tags":["Preregistration","Machine Learning","P-Hacking"],"title":"Preregistration of Machine Learning Research ","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"adadbf367c0934da2e1597e46e5855c6","permalink":"https://forrt.org/curated_resources/preregistration-of-modeling-exercises-ma/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/preregistration-of-modeling-exercises-ma/","section":"curated_resources","summary":"This is a commentary on Lee et al.’s (2019) article encouraging preregistration of model development, fitting, and evaluation. While we are in general agreement with Lee et al.’s characterization of the modeling process, we disagree on whether preregistration of this process will move the scientific enterprise forward. We emphasize the subjective and exploratory nature of model development, and point out that “under-modeling” of data (relying on black-box approaches applied to data without data exploration) is as big a problem as “over-modeling” (fitting noise, resulting in models that generalize poorly). We also note the potential long-run negative impact of preregistration on future generations of cognitive scientists. It is our opinion that preregistration of model development will lead to less, and to less creative, exploratory analysis (i.e., to more under-modeling), and that Lee at al.’s primary goals can be achieved by requiring publication of raw data and code. We conclude our commentary with suggestions on how to move forward.","tags":["Preregistration","Modeling","Replication Crisis","Open Science","Exploratory","Robust Practices"],"title":"Preregistration of Modeling Exercises May Not Be Useful","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"596d41852175a07dfa2fa1e3eef49263","permalink":"https://forrt.org/curated_resources/preregistration-of-qualitative-research/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/preregistration-of-qualitative-research/","section":"curated_resources","summary":"In this webinar, Tamarinde Haven provides an overview of the process of preregistration in qualitative research. We review the process of preregistration, how we partnered with a community of qualitative researchers to develop a template for qualitative research through a Delphi study, and a guide to the fields that were included in the final form.","tags":["Open Science","OSF","Preregistration","Qualitative Methods","Qualitative Research","Research","Research Best Practices"],"title":"Preregistration of Qualitative Research","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"f4a5747f7ec8aeb0a34eb129310acddd","permalink":"https://forrt.org/curated_resources/preregistration-of-randomized-controlled/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/preregistration-of-randomized-controlled/","section":"curated_resources","summary":"Randomized controlled trials (RCTs) are designed to answer causal questions with internal validity. However, threats to internal validity exist for even well-designed RCTs. In this article, we focus on how preregistration can help address some specific threats to internal validity related to the reporting of results. Preregistration involves researchers publicly posting critical decision points in a study prior to conducting it for the purpose of making researcher plans transparent, making deviations from those plans discoverable, and improving the validity of tests of significance. We provide a brief overview of null-hypothesis significance testing; consider how questionable research practices (e.g., p-hacking) and conducting data-dependent analysis threaten the validity of significance tests; discuss how preregistration can help address these threats and how preregistration works for RCTs; note limitations and challenges to preregistration; and provide recommendations for increasing the use of preregistration by researchers conducting RCTs in social work, education, and related fields.","tags":["Preregistration","Randomized Experiment","Outcome Study"],"title":"Preregistration of Randomized Controlled Trials","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"35e0f19cde6ec48f6dcc80125af115f1","permalink":"https://forrt.org/curated_resources/preregistration-of-secondary-data-analys/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/preregistration-of-secondary-data-analys/","section":"curated_resources","summary":"Preregistration has been lauded as one of the solutions to the so-called ‘crisis of confidence’ in the social sciences and has therefore gained popularity in recent years. However, the current guidelines for preregistration have been developed primarily for studies where new data will be collected. Yet, preregistering secondary data analyses--- where new analyses are proposed for existing data---is just as important, given that researchers’ hypotheses and analyses may be biased by their prior knowledge of the data. The need for proper guidance in this area is especially desirable now that data is increasingly shared publicly. In this tutorial, we present a template specifically designed for the preregistration of secondary data analyses and provide comments and a worked example that may help with using the template effectively. Through this illustration, we show that completing such a template is feasible, helps limit researcher degrees of freedom, and may make researchers more deliberate in their data selection and analysis efforts.","tags":["Preregistration","Secondary Data Analysis"],"title":"Preregistration of secondary data analysis: A template and tutorial","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"8e535f82b4df213a00c09b08fa7aa6e0","permalink":"https://forrt.org/curated_resources/preregistration-of-studies-with-existing/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/preregistration-of-studies-with-existing/","section":"curated_resources","summary":"Preregistration of research plans is becoming an increasingly popular and common tool to enhance the transparency of a study’s methodology. In a preregistration, researchers document their research plans and register them to a public repository prior to conducting their research. In this chapter, we provide arguments for why preregistration can protect scientific findings against questionable research practices (QRPs), such as outcome swapping, selective reporting of conditions, unwarranted data exclusions, and post hoc changing of hypotheses. Furthermore, we place particular emphasis on preregistering research plans when using existing data, and we give an overview of preregistration templates and public repositories for different types of research designs. We conclude this chapter with highlighting some of the common criticisms of preregistration and our counter-arguments and provide future reflections.","tags":["Replicability","Transparency","Questionable Research Practices","Open Science","Existing Data","Secondary Data Analysis","Preregistration"],"title":"Preregistration of Studies with Existing Data","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"1fcf8268791b774fb9b42506fd95641e","permalink":"https://forrt.org/curated_resources/preregistration-of-study-protocols-is-un/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/preregistration-of-study-protocols-is-un/","section":"curated_resources","summary":"A recent workshop focused on the idea that protocols for nonrandomized studies should be registered in advance of their conduct. In reviewing the workshop report and publications citing it, I note a number of unfounded assumptions and little attention to competing alternatives.","tags":["Epidemiology","Preregistration"],"title":"Preregistration of Study Protocols Is Unlikely to Improve the Yield From Our Science, But Other Strategies Might","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"e5874ae2a1fd2ded1aafe677d07ff8dc","permalink":"https://forrt.org/curated_resources/preregistration-overview-page/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/preregistration-overview-page/","section":"curated_resources","summary":"What is Preregistration? When you preregister your research, you're simply specifying your research plan in advance of your study and submitting it to a registry. Preregistration separates hypothesis-generating (exploratory) from hypothesis-testing (confirmatory) research. Both are important. But the same data cannot be used to generate and test a hypothesis, which can happen unintentionally and reduce the credibility of your results. Addressing this problem through planning improves the quality and transparency of your research. This helps you clearly report your study and helps others who may wish to build on it.","tags":["Funders","Preregistration","Publishers","Reproducibility","Researchers"],"title":"Preregistration Overview page","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1721390377,"objectID":"7889510a72aeb30f0c300883efe286ab","permalink":"https://forrt.org/glossary/english/preregistration_pledge/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/english/preregistration_pledge/","section":"glossary","summary":"","tags":null,"title":"Preregistration Pledge","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1635961284,"objectID":"0169b3c2a4ff497d069bb5923fbdcc4a","permalink":"https://forrt.org/glossary/vbeta/preregistration-pledge/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/vbeta/preregistration-pledge/","section":"glossary","summary":"","tags":null,"title":"Preregistration Pledge","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1720865851,"objectID":"77a5dbdee99abd9b3e4c467ea457ed6f","permalink":"https://forrt.org/glossary/german/preregistration_pledge/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/german/preregistration_pledge/","section":"glossary","summary":"","tags":null,"title":"Preregistration Pledge (Präregistrierungs-Versprechen)","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"1bc2c2f2bc670c3f538d12d354f610d0","permalink":"https://forrt.org/curated_resources/preregistration-specificity-adherence-a/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/preregistration-specificity-adherence-a/","section":"curated_resources","summary":"Study preregistration is one of several “open science” practices (e.g., open data, preprints) that researchers use to improve the transparency and rigour of their research. As more researchers adopt preregistration as a regular research practice, examining the nature and content of preregistrations can help identify strengths and weaknesses of current practices. The value of preregistration, in part, relates to the specificity of the study plan and the extent to which investigators adhere to this plan. We identified 53 preregistrations from the gambling studies field meeting our predefined eligibility criteria and scored their level of specificity using a 23-item protocol developed to measure the extent to which a clear and exhaustive preregistration plan restricts various researcher degrees of freedom(RDoF; i.e., the many methodological choices available to researchers when collecting and analysing data, and when reporting their findings). We also scored studies on a 32-item protocol that measured adherence to the preregistered plan in the study manuscript. We found that gambling preregistrations had low specificity levels on most RDoF. However, a comparison with a sample of cross-disciplinary preregistrations (N = 52; Bakker et al., 2020) indicated that gambling preregistrations scored higher on 12 (of 29) items. Thirteen (65%)of the 20 associated published articles or preprints deviated from the protocol without declaring as much (the mean number of undeclared deviations per article was 2.25,SD= 2.34). Overall, while we found improvements in specificity and adherence over time (2017-2020), our findings suggest the purported benefits of preregistration—including increasing transparency and reducing RDoF—are not fully achieved by current practices. Using our findings, we provide 10 practical recommendations that can be used to support and refine preregistration practices.","tags":["Preregistration","Gambling Studies"],"title":"Preregistration specificity \u0026 adherence: A review of preregistered gambling studies \u0026 cross-disciplinary comparison","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"67dd6c89594e005ef698c6d0057d492a","permalink":"https://forrt.org/curated_resources/preregistration-replication-and-nonexper/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/preregistration-replication-and-nonexper/","section":"curated_resources","summary":"In last month’s column, I worried about whether encouraging us to preregister our hypotheses and analysis plan before running studies would stifle discovery. I came to the conclusion that it needn’t — but that we need to guard against letting the practice run away with itself. In this column, I take up a second concern about preregistration: That it seems to apply only to certain types of studies, and thus runs the risk of marginalizing studies for which preregistration is less fitting.","tags":["Preregistration","Longitudinal Research"],"title":"Preregistration, Replication, and Nonexperimental Studies","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"e9e6a90d8a9dfb7d2888a4840d6d76e6","permalink":"https://forrt.org/curated_resources/preregistration-definition-advantages-di/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/preregistration-definition-advantages-di/","section":"curated_resources","summary":"Questionable research practices (QRPs), such as p-hacking (i.e., the inappropriate manipulation of data analysis to find statistical significance) and post hoc hypothesizing, are threats to the replicability of research findings. One key solution to the problem of QRPs is preregistration. This refers to time-stamped documentation that describes the methodology and statistical analyses of a study before the data are collected or inspected. As such, readers of the study’s report can evaluate whether the described research is in line with the planned methods and analyses or whether there are deviations from these (e.g., analyses performed so that the research hypotheses is confirmed). Here, we aim to describe what preregistration entails and why it is useful for psychology research. In this vein, we present the key elements of a sufficient preregistration file, its advantages as well as its disadvantages, and why preregistration is a key, yet partially insufficient, solution against QRPs. By the end of this chapter, we hope that readers are convinced that there is little reason not to preregister their research.","tags":["Questionable Research Practices","Preregistration","Psychological Science","Clinical Science"],"title":"Preregistration: Definition, Advantages, Disadvantages, and How It Can Help Against Questionable Research Practices","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"90b05a2a6058f159a956fa3a579a294d","permalink":"https://forrt.org/curated_resources/preregistration-improve-research-rigor-r/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/preregistration-improve-research-rigor-r/","section":"curated_resources","summary":"In this webinar Professor Brian Nosek, Executive Director of the Center for Open Science (https://cos.io), outlines the practice of Preregistration and how it can aid in increasing the rigor and reproducibility of research. The webinar is co-hosted by the Health Research Alliance, a collaborative member organization of nonprofit research funders. Slides available at: https://osf.io/9m6tx/","tags":["Analysis","Data","Education","Materials","Open Scholarship Tools and Technologies","OSF","Preregistation","Publishing","Reproducibility","Research Data Management Tools","Researchers"],"title":"Preregistration: Improve Research Rigor, Reduce Bias","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"4c24bc3b7d61c24dbedbc31f1054b5ca","permalink":"https://forrt.org/curated_resources/preregistration-practical-considerations/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/preregistration-practical-considerations/","section":"curated_resources","summary":"Purpose: In the last decade, psychology and other sciences have implemented numerous reforms to improve the robustness of our research, many of which are based on increasing transparency throughout the research process. Among these reforms is the practice of preregistration, in which researchers create a time-stamped and uneditable document before data collection that describes the methods of the study, how the data will be analyzed, the sample size, and many other decisions. The current article highlights the benefits of preregistration with a focus on the specific issues that speech, language, and hearing researchers are likely to encounter, and additionally provides a tutorial for writing preregistrations.\n\nConclusions: Although rates of preregistration have increased dramatically in recent years, the practice is still relatively uncommon in research on speech, language, and hearing. Low rates of adoption may be driven by a lack of understanding of the benefits of preregistration (either generally or for our discipline in particular) or uncertainty about how to proceed if it becomes necessary to deviate from the preregistered plan. Alternatively, researchers may see the benefits of preregistration but not know where to start, and gathering this information from a wide variety of sources is arduous and time consuming. This tutorial addresses each of these potential roadblocks to preregistration and equips readers with tools to facilitate writing preregistrations for research on speech, language, and hearing.","tags":["Speech","Language","Hearing","Preregistration"],"title":"Preregistration: Practical Considerations for Speech, Language, and Hearing Research","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"f496a65c330916f1f37d6ee5a3c8c6c8","permalink":"https://forrt.org/curated_resources/preregistration-the-good-the-bad-and-the/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/preregistration-the-good-the-bad-and-the/","section":"curated_resources","summary":"Preregistration is a tool to enhance the reliability of science that has been promoted as a normative requirement for the award of grants or the acceptance of publications. I argue that: (a) preregistration addresses an important need, (b) it offers considerable benefits, (c) those benefits partially cover the need, (d) they are accompanied by costs and side effects. The decision to make preregistration a normative requirement should be carefully assessed for its potential side-effects, and alternative models and norms should be considered. I discuss factors that affect the reliability of science and how preregistration can influence them, and I make a few suggestions to enhance its efficacy while limiting its risks.","tags":["Meta-science","Neuroscience","Social and Behavioral Sciences","Life Sciences"],"title":"Preregistration: the good, the bad, and the confusing","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"f2ca56e2818324101f7fa407c4870a77","permalink":"https://forrt.org/curated_resources/presentations-given-by-center-for-open-s/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/presentations-given-by-center-for-open-s/","section":"curated_resources","summary":"A collection of slides for virtually all presentations given by Center for Open Science staff since its founding in 2013.","tags":["Data","Funders","Librarians","Materials","Open Science Policy","Policy","Policy Makers","Publishers","Publishing","Reproducibility","Research Administration","Researchers","Workflow Tools"],"title":"Presentations Given by Center for Open Science","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"0b02b1109e0757bbbfcb670ef78bdcae","permalink":"https://forrt.org/curated_resources/preserving-privacy-in-the-era-of-opennes/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/preserving-privacy-in-the-era-of-opennes/","section":"curated_resources","summary":"Psychological science journals are increasingly adopting open science (OS) policies (e.g., Transparency and Openness Promotion) requiring researchers to make all data and materials publicly available in an effort to drive research toward greater transparency and accessibility. These policies certainly have many benefits to the scientific community and public in helping ensure the quality of published research. However, the Center for Open Science has not offered any explicit guidelines regarding when exceptions to OS policies should be made, with only vague guidelines offered such as “when ethical or legal constraints prevent it.” We argue that these ambiguous policies may create bias in decisions made by journal editors as to whom and what type of research is granted exceptions. When journals are too rigid in their exception policies, this may unintentionally contradict OS’s goals to create a more valid and ethical science. We argue that journals should never mandate identifiable data to be posted publicly as a publication prerequisite. Maintaining participant anonymity should always come before OS policies to (a) align with psychologists’ primary obligation of maintaining participant confidentiality, (b) encourage participation from the broader population and more specifically from marginalized communities, and (c) maintain unbiased, representative, and valid data. From empirical and ethical insights, we offer several solutions to ease the tensions between OS and participant privacy during the data collection and publication process.","tags":["open data","transparency and openness promotion","TOP factor","privacy"],"title":"Preserving privacy in the era of openness: Commentary on open science requirements for identifiable data in psychological science journals","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1689122843,"objectID":"f69066d55e9cbd667ea8efb1c64d6ee2","permalink":"https://forrt.org/curated_resources/priming-replication-and-the-hardest-scie/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/priming-replication-and-the-hardest-scie/","section":"curated_resources","summary":"Concerns have been raised recently about the replicability of behavioral priming effects, and calls have been issued to identify priming methodologies with effects that can be obtained in any context and with any population. I argue that such expectations are misguided and inconsistent with evolutionary understandings of the brain as a computational organ. Rather, we should expect priming effects to be highly sensitive to variations in experimental features and subject populations. Such variation does not make priming effects frivolous or capricious but instead can be predicted a priori. However, absent theories specifying the precise contingencies that lead to such variation, failures to replicate another researcher’s findings will necessarily be ambiguous with respect to the inferences that can be made. Priming research is not yet at the stage where such theories exist, and therefore failures are uninformative at the current time. Ultimately, priming researchers themselves must provide direct replications of their own effects; researchers have been deficient in meeting this responsibility and have contributed to the current state of confusion. The recommendations issued in this article reflect concerns both with the practice of priming researchers and with the inappropriate expectations of researchers who have failed to replicate others’ priming effects.","tags":["Reproducibility Crisis and Credibility Revolution","Open Science"],"title":"Priming, Replication, and the Hardest Science","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"87015cbf5367eca719fc6a84eefd026f","permalink":"https://forrt.org/glossary/english/prior-distribution/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/english/prior-distribution/","section":"glossary","summary":"","tags":null,"title":"Prior distribution","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1635961284,"objectID":"570a234e8af469a42047dd9a4355dcb5","permalink":"https://forrt.org/glossary/vbeta/prior-distribution/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/vbeta/prior-distribution/","section":"glossary","summary":"","tags":null,"title":"Prior distribution ","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"1f24303efcf797a20d25f3e366cce468","permalink":"https://forrt.org/glossary/german/prior-distribution/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/german/prior-distribution/","section":"glossary","summary":"","tags":null,"title":"Prior distribution (a-priori Verteilung)","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1689122843,"objectID":"1e82a055ca9bd2a94fec84f9c3142539","permalink":"https://forrt.org/curated_resources/privacy-and-data-based-research/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/privacy-and-data-based-research/","section":"curated_resources","summary":"What can we, as users of microdata, formally guarantee to the individuals (or firms) in our dataset, regarding their privacy? We retell a few stories, well-known in data-privacy circles, of failed anonymization attempts in publicly released datasets. We then provide a mostly informal introduction to several ideas from the literature on differential privacy, an active literature in computer science that studies formal approaches to preserving the privacy of individuals in statistical databases. We apply some of its insights to situations routinely faced by applied economists, emphasizing big-data contexts.","tags":[""],"title":"Privacy and Data-Based Research","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1721390377,"objectID":"f2de71db8746f89971aae7efe27c5adf","permalink":"https://forrt.org/glossary/english/pro/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/english/pro/","section":"glossary","summary":"","tags":null,"title":"PRO (peer review openness) initiative","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1635961284,"objectID":"8ca1ed2c56c6437d10611f68c13c631f","permalink":"https://forrt.org/glossary/vbeta/pro-peer-review-openness-initiative/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/vbeta/pro-peer-review-openness-initiative/","section":"glossary","summary":"","tags":null,"title":"PRO (peer review openness) initiative","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1720865851,"objectID":"dd387fd7ef05fe474cacfd4f0a85eb1e","permalink":"https://forrt.org/glossary/german/pro/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/german/pro/","section":"glossary","summary":"","tags":null,"title":"PRO (peer review openness) initiative (PRO Initiative)","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1689122843,"objectID":"a90f2c58dfa5c5d7473a312afac2ed03","permalink":"https://forrt.org/curated_resources/probing-birth-order-effects-on-narrow-tr/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/probing-birth-order-effects-on-narrow-tr/","section":"curated_resources","summary":"The idea that birth-order position has a lasting impact on personality has been discussed for the past 100 years. Recent large-scale studies have indicated that birth-order effects on the Big Five personality traits are negligible. In the current study, we examined a variety of more narrow personality traits in a large representative sample ( n = 6,500-10,500 in between-family analyses; n = 900-1,200 in within-family analyses). We used specification-curve analysis to assess evidence for birth-order effects across a range of models implementing defensible yet arbitrary analytical decisions (e.g., whether to control for age effects or to exclude participants on the basis of sibling spacing). Although specification-curve analysis clearly confirmed the previously reported birth-order effect on intellect, we found no meaningful effects on life satisfaction, locus of control, interpersonal trust, reciprocity, risk taking, patience, impulsivity, or political orientation. The lack of meaningful birth-order effects on self-reports of personality was not limited to broad traits but also held for more narrowly defined characteristics.","tags":["Reproducibility Crisis and Credibility Revolution","Open Science"],"title":"Probing Birth-Order Effects on Narrow Traits Using Specification-Curve Analysis","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1689122843,"objectID":"c690c6983416f12a2d3b9d8db591e026","permalink":"https://forrt.org/curated_resources/problems-in-using-p-curve-analysis-and-t/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/problems-in-using-p-curve-analysis-and-t/","section":"curated_resources","summary":"Background. The p-curve is a plot of the distribution of p-values reported in a set of scientific studies. Comparisons between ranges of p-values have been used to evaluate fields of research in terms of the extent to which studies have genuine evidential value, and the extent to which they suffer from bias in the selection of variables and analyses for publication, p-hacking. Methods. p-hacking can take various forms. Here we used R code to simulate the use of ghost variables, where an experimenter gathers data on several dependent variables but reports only those with statistically significant effects. We also examined a text-mined dataset used by Head et al. (2015) and assessed its suitability for investigating p-hacking. Results. We show that when there is ghost p-hacking, the shape of the p-curve depends on whether dependent variables are intercorrelated. For uncorrelated variables, simulated p-hacked data do not give the \"p-hacking bump\" just below .05 that is regarded as evidence of p-hacking, though there is a negative skew when simulated variables are inter-correlated. The way p-curves vary according to features of underlying data poses problems when automated text mining is used to detect p-values in heterogeneous sets of published papers. Conclusions. The absence of a bump in the p-curve is not indicative of lack of p-hacking. Furthermore, while studies with evidential value will usually generate a right-skewed p-curve, we cannot treat a right-skewed p-curve as an indicator of the extent of evidential value, unless we have a model specific to the type of p-values entered into the analysis. We conclude that it is not feasible to use the p-curve to estimate the extent of p-hacking and evidential value unless there is considerable control over the type of data entered into the analysis. In particular, p-hacking with ghost variables is likely to be missed.","tags":["Reproducibility Crisis and Credibility Revolution","Open Science"],"title":"Problems in using p-curve analysis and text-mining to detect rate of p-hacking.","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"fb74dd213f32dcdb5cdcf1f6673f98cf","permalink":"https://forrt.org/curated_resources/procedimentos-para-investigacao-cientifi/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/procedimentos-para-investigacao-cientifi/","section":"curated_resources","summary":"O profissional formado pelo Curso de Graduação em Psicologia da Unifesspa caracterizar-se-á porpossuir uma formação pluralista e generalista, preparado para atuação multiprofissional pelaformação interdisciplinar com enfoque crítico, científico e reflexivo visando à promoção da Saúde edo bem-estar humano, nos seus mais variados aspectos.","tags":["Reproducibility Crisis and Credibility Revolution","Open Science","Conceptual and statistical knowledge"],"title":"Procedimentos para investigação científica","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"bcce32eb1e035b1b693842a887c231ff","permalink":"https://forrt.org/curated_resources/programming-psychological-experiments-us/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/programming-psychological-experiments-us/","section":"curated_resources","summary":"Using this material, you can learn how to code in python, build experiments using OpenSesame, and do basic data preprocessing in python.","tags":["Python","Experiments","OpenSesame"],"title":"Programming Psychological Experiments using python and OpenSesame","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"dee61f34682139987d2568e278dbace2","permalink":"https://forrt.org/curated_resources/programming-with-matlab/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/programming-with-matlab/","section":"curated_resources","summary":"The best way to learn how to program is to do something useful, so this introduction to MATLAB is built around a common scientific task: data analysis. Our real goal isn’t to teach you MATLAB, but to teach you the basic concepts that all programming depends on. We use MATLAB in our lessons because: we have to use something for examples; it’s well-documented; it has a large (and growing) user base among scientists in academia and industry; and it has a large library of packages available for performing diverse tasks. But the two most important things are to use whatever language your colleagues are using, so that you can share your work with them easily, and to use that language well.","tags":["Analysis","Data","Education","MATLAB","Open Scholarship Tools and Technologies","Reproducibility","Research Data Management Tools","Researchers"],"title":"Programming with MATLAB","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"4b6936b687ea6e4f50d545c0043a5c29","permalink":"https://forrt.org/curated_resources/programming-with-python/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/programming-with-python/","section":"curated_resources","summary":"The best way to learn how to program is to do something useful, so this introduction to Python is built around a common scientific task: data analysis. Arthritis Inflammation We are studying inflammation in patients who have been given a new treatment for arthritis, and need to analyze the first dozen data sets of their daily inflammation. The data sets are stored in comma-separated values (CSV) format: each row holds information for a single patient, columns represent successive days. The first three rows of our first file look like this: 0,0,1,3,1,2,4,7,8,3,3,3,10,5,7,4,7,7,12,18,6,13,11,11,7,7,4,6,8,8,4,4,5,7,3,4,2,3,0,0 0,1,2,1,2,1,3,2,2,6,10,11,5,9,4,4,7,16,8,6,18,4,12,5,12,7,11,5,11,3,3,5,4,4,5,5,1,1,0,1 0,1,1,3,3,2,6,2,5,9,5,7,4,5,4,15,5,11,9,10,19,14,12,17,7,12,11,7,4,2,10,5,4,2,2,3,2,2,1,1 Each number represents the number of inflammation bouts that a particular patient experienced on a given day. For example, value “6” at row 3 column 7 of the data set above means that the third patient was experiencing inflammation six times on the seventh day of the clinical study. So, we want to: Calculate the average inflammation per day across all patients. Plot the result to discuss and share with colleagues. To do all that, we’ll have to learn a little bit about programming.","tags":["Analysis","Data","Education","Open Scholarship Tools and Technologies","Python","Reproducibility","Research Data Management Tools","Researchers"],"title":"Programming with Python","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"c7b13194ea62680e5e6958177cfdd287","permalink":"https://forrt.org/curated_resources/programming-with-r/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/programming-with-r/","section":"curated_resources","summary":"The best way to learn how to program is to do something useful, so this introduction to R is built around a common scientific task: data analysis. Our real goal isn’t to teach you R, but to teach you the basic concepts that all programming depends on. We use R in our lessons because: we have to use something for examples; it’s free, well-documented, and runs almost everywhere; it has a large (and growing) user base among scientists; and it has a large library of external packages available for performing diverse tasks. But the two most important things are to use whatever language your colleagues are using, so you can share your work with them easily, and to use that language well. We are studying inflammation in patients who have been given a new treatment for arthritis, and need to analyze the first dozen data sets of their daily inflammation. The data sets are stored in CSV format (comma-separated values): each row holds information for a single patient, and the columns represent successive days. The first few rows of our first file look like this: 0,0,1,3,1,2,4,7,8,3,3,3,10,5,7,4,7,7,12,18,6,13,11,11,7,7,4,6,8,8,4,4,5,7,3,4,2,3,0,0 0,1,2,1,2,1,3,2,2,6,10,11,5,9,4,4,7,16,8,6,18,4,12,5,12,7,11,5,11,3,3,5,4,4,5,5,1,1,0,1 0,1,1,3,3,2,6,2,5,9,5,7,4,5,4,15,5,11,9,10,19,14,12,17,7,12,11,7,4,2,10,5,4,2,2,3,2,2,1,1 0,0,2,0,4,2,2,1,6,7,10,7,9,13,8,8,15,10,10,7,17,4,4,7,6,15,6,4,9,11,3,5,6,3,3,4,2,3,2,1 0,1,1,3,3,1,3,5,2,4,4,7,6,5,3,10,8,10,6,17,9,14,9,7,13,9,12,6,7,7,9,6,3,2,2,4,2,0,1,1 We want to: load that data into memory, calculate the average inflammation per day across all patients, and plot the result. To do all that, we’ll have to learn a little bit about programming.","tags":["Analysis","Data","Education","Open Scholarship Tools and Technologies","R","Reproducibility","Research Data Management Tools","Researchers"],"title":"Programming with R","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"7db7983c7b2239c4ed96ca7855a0ccc9","permalink":"https://forrt.org/curated_resources/project-organization-and-management-for/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/project-organization-and-management-for/","section":"curated_resources","summary":"Data Carpentry Genomics workshop lesson to learn how to structure your metadata, organize and document your genomics data and bioinformatics workflow, and access data on the NCBI sequence read archive (SRA) database. Good data organization is the foundation of any research project. It not only sets you up well for an analysis, but it also makes it easier to come back to the project later and share with collaborators, including your most important collaborator - future you. Organizing a project that includes sequencing involves many components. There’s the experimental setup and conditions metadata, measurements of experimental parameters, sequencing preparation and sample information, the sequences themselves and the files and workflow of any bioinformatics analysis. So much of the information of a sequencing project is digital, and we need to keep track of our digital records in the same way we have a lab notebook and sample freezer. In this lesson, we’ll go through the project organization and documentation that will make an efficient bioinformatics workflow possible. Not only will this make you a more effective bioinformatics researcher, it also prepares your data and project for publication, as grant agencies and publishers increasingly require this information. In this lesson, we’ll be using data from a study of experimental evolution using E. coli. More information about this dataset is available here. In this study there are several types of files: Spreadsheet data from the experiment that tracks the strains and their phenotype over time Spreadsheet data with information on the samples that were sequenced - the names of the samples, how they were prepared and the sequencing conditions The sequence data Throughout the analysis, we’ll also generate files from the steps in the bioinformatics pipeline and documentation on the tools and parameters that we used. In this lesson you will learn: How to structure your metadata, tabular data and information about the experiment. The metadata is the information about the experiment and the samples you’re sequencing. How to prepare for, understand, organize and store the sequencing data that comes back from the sequencing center How to access and download publicly available data that may need to be used in your bioinformatics analysis The concepts of organizing the files and documenting the workflow of your bioinformatics analysis","tags":["Analysis","Data","Education","Genomics","Metadata","Open Scholarship Tools and Technologies","Reproducibility","Research Data Management Tools","Researchers"],"title":"Project Organization and Management for Genomics","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"82e29a8a179501b8340c6d4fd62ab6f4","permalink":"https://forrt.org/curated_resources/project-teaching-integrity-in-empirical/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/project-teaching-integrity-in-empirical/","section":"curated_resources","summary":"The Project Teaching Integrity in Empirical Research (TIER) develops methods and tools for enhancing research transparency through teaching. These can be used by faculty who teach quantitative methods or supervise student research. TIER further provides guidance to students who want to adopt transparent and replicable research practices independently.","tags":["Education","Educators","Open Scholarship Guidelines","Researchers","Workflow Tools"],"title":"Project Teaching Integrity in Empirical Research (TIER)","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"4386ccb5abf7ea529a83e044c02b3e34","permalink":"https://forrt.org/curated_resources/promises-and-perils-of-pre-analysis-plan/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/promises-and-perils-of-pre-analysis-plan/","section":"curated_resources","summary":"The purpose of this paper is to help think through the advantages and costs of rigorous pre-specification of statistical analysis plans in economics. A pre-analysis plan pre-specifies in a precise way the analysis to be run before examining the data. A researcher can specify variables, data cleaning procedures, regression specifications, and so on. If the regressions are pre-specified in advance and researchers are required to report all the results they pre-specify, data-mining problems are greatly reduced. I begin by laying out the basics of what a statistical analysis plan actually contains so those researchers unfamiliar with it can better understand how it is done. In so doing, I have drawn both on standards used in clinical trials, which are clearly specified by the Food and Drug Administration, as well as my own practical experience from writing these plans in economics contexts. I then lay out some of the advantages of pre-specified analysis plans, both for the scientific community as a whole and also for the researcher. I also explore some of the limitations and costs of such plans. I then review a few pieces of evidence that suggest that, in many contexts, the benefits of using pre-specified analysis plans may not be as high as one might have expected initially. For the most part, I will focus on the relatively narrow issue of pre-analysis for randomized controlled trials.","tags":["Preregistration","Pre-Analysis Plans","Economics"],"title":"Promises and Perils of Pre-analysis Plans","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1689122843,"objectID":"a262945ccdaf764629d03edfd8ef1efe","permalink":"https://forrt.org/curated_resources/promoting-an-open-research-culture/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/promoting-an-open-research-culture/","section":"curated_resources","summary":"Author guidelines for journals could help to promote transparency, openness, and reproducibility","tags":["Reproducibility Crisis and Credibility Revolution","Open Science"],"title":"Promoting an open research culture","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1689122843,"objectID":"7da9cab233f42cab298d53ea55b80599","permalink":"https://forrt.org/curated_resources/promoting-transparency-in-social-science/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/promoting-transparency-in-social-science/","section":"curated_resources","summary":"Social scientists should adopt higher transparency standards to improve the quality and credibility of research.","tags":["Reproducibility Crisis and Credibility Revolution","Open Science"],"title":"Promoting Transparency in Social Science Research","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"c838a51c6613aa9ff7d0714a831dae43","permalink":"https://forrt.org/curated_resources/promotion-tenure-aligning-incentives-wit/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/promotion-tenure-aligning-incentives-wit/","section":"curated_resources","summary":"This repository provides resources for faculty and academic units to re-imagine faculty rewards and incentives, and in particular how faculty are evaluated for promotion and tenure, with the goal of maximizing inclusivity in science by changing how we share our scholarship, data, and research material. \"Impact as Access\" is realized when scholars have access to all aspects of the research pipeline, whether that be through open sharing of research, being directly involved (access as a researcher), or access to education (open education resources). Impact as Access reduces costs by making research reusable, reproducible, and replicable. Impact as Access empowers those from traditionally underserved communities to benefit from publicly funded research. Impact as Access reduces barriers (both financial and social) to participating in advancing science and helping to solve the world's problems.","tags":["Incentives","Tenure","Institutional Values","Inclusion"],"title":"Promotion \u0026 Tenure: Aligning incentives with institutional values and open science","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1721390377,"objectID":"0b283072f3f5d06ed1b4ac30987c44bb","permalink":"https://forrt.org/glossary/english/pseudonymisation/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/english/pseudonymisation/","section":"glossary","summary":"","tags":null,"title":"Pseudonymisation","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1635961284,"objectID":"0b67b66f9d6532952de45806ffc5c6fe","permalink":"https://forrt.org/glossary/vbeta/pseudonymisation/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/vbeta/pseudonymisation/","section":"glossary","summary":"","tags":null,"title":"Pseudonymisation","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1720865851,"objectID":"4904ee4e9f36395350377240aaf061a4","permalink":"https://forrt.org/glossary/german/pseudonymisation/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/german/pseudonymisation/","section":"glossary","summary":"","tags":null,"title":"Pseudonymisation (Pseudonymisierung)","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1721390377,"objectID":"62a24d59687e440097a39f9fca152178","permalink":"https://forrt.org/glossary/english/pseudoreplication/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/english/pseudoreplication/","section":"glossary","summary":"","tags":null,"title":"Pseudoreplication","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1635961284,"objectID":"d503766da77ad87ef5ada3ffc8ce767a","permalink":"https://forrt.org/glossary/vbeta/pseudoreplication/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/vbeta/pseudoreplication/","section":"glossary","summary":"","tags":null,"title":"Pseudoreplication","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1720865851,"objectID":"affc4469e32bd4f74152f81e57defec4","permalink":"https://forrt.org/glossary/german/pseudoreplication/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/german/pseudoreplication/","section":"glossary","summary":"","tags":null,"title":"Pseudoreplication (Pseudoreplikation)","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1689122843,"objectID":"fa59f6e52fb0053bf655dccac380e1ee","permalink":"https://forrt.org/curated_resources/psychological-testing-and-psychological/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/psychological-testing-and-psychological/","section":"curated_resources","summary":"This article summarizes evidence and issues associated with psychological assessment. Data from more than 125 meta-analyses on test validity and 800 samples examining multimethod assessment suggest 4 general conclusions: (a) Psychological test validity is strong and compelling, (b) psychological test validity is comparable to medical test validity, (c) distinct assessment methods provide unique sources of information, and (d) clinicians who rely exclusively on interviews are prone to incomplete understandings. Following principles for optimal nomothetic research, the authors suggest that a multimethod assessment battery provides a structured means for skilled clinicians to maximize the validity of individualized assessments. Future investigations should move beyond an examination of test scales to focus more on the role of psychologists who use tests as helpful tools to furnish patients and referral sources with professional consultatio","tags":[""],"title":"Psychological testing and psychological assessment: A review of evidence and issues.","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1689122843,"objectID":"2c49d676d1ee4bf5960a3994bc48a2ae","permalink":"https://forrt.org/curated_resources/psychologists-are-open-to-change-yet-war/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/psychologists-are-open-to-change-yet-war/","section":"curated_resources","summary":"Psychologists must change the way they conduct and report their research—this notion has been the topic of much debate in recent years. One article recently published in Psychological Science proposing six requirements for researchers concerning data collection and reporting practices as well as four guidelines for reviewers aimed at improving the publication process has recently received much attention (Simmons, Nelson, \u0026 Simonsohn, 2011). We surveyed 1,292 psychologists to address two questions: Do psychologists support these concrete changes to data collection, reporting, and publication practices, and if not, what are their reasons? Respondents also indicated the percentage of print and online journal space that should be dedicated to novel studies and direct replications as well as the percentage of published psychological research that they believed would be confirmed if direct replications were conducted. We found that psychologists are generally open to change. Five requirements for researchers and three guidelines for reviewers were supported as standards of good practice, whereas one requirement was even supported as a publication condition. Psychologists appear to be less in favor of mandatory conditions of publication than standards of good practice. We conclude that the proposal made by Simmons, Nelson \u0026 Simonsohn (2011) is a starting point for such standards.","tags":["Reproducibility Crisis and Credibility Revolution","Open Science"],"title":"Psychologists Are Open to Change, yet Wary of Rules","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"aac8bfaa48764222a7b1d99fa87a4bbf","permalink":"https://forrt.org/curated_resources/psychology-as-a-robust-science/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/psychology-as-a-robust-science/","section":"curated_resources","summary":"Is psychology a robust science? To answer such a question, this course will encourage you to think critically about how psychological research is conducted and how conclusions are drawn. \nTo enable you to truly understand how psychology functions as a science, however, this course will also need to discuss how psychologists are incentivised, how they publish and how their beliefs influence the inferences they make. By engaging with such issues, this course will probe and challenge the basic features and functions of our discipline. We will uncover multiple methodological, statistical and systematic issues that could impair the robustness of scientific claims we encounter every day. We will discuss the controversy around psychology and the replicability of its results, while learning about new initiatives that are currently reinventing the basic foundations of our field. \nThe course will equip you with some of the basic tools necessary to conduct robust psychological research fit for the 21st century.The course will be based on a mix of set readings, class discussions and lectures. Readings will include a diverse range of journal articles, reviews, editorials, blog posts, newspaper articles, commentaries, podcasts,videos, and tweets. No exams or papers will be set; but come along with a critical eye and a willingness to discuss some difficult and controversial issues.","tags":["Reproducibility Crisis and Credibility Revolution","Open Science","Conceptual and statistical knowledge"],"title":"Psychology as a Robust Science","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1689122843,"objectID":"b1c95259729d192e12c1038db4ad3319","permalink":"https://forrt.org/curated_resources/psychology-science-and-knowledge-constru/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/psychology-science-and-knowledge-constru/","section":"curated_resources","summary":"Psychology advances knowledge by testing statistical hypotheses using empirical observations and data. The expectation is that most statistically significant findings can be replicated in new data and in new laboratories, but in practice many findings have replicated less often than expected, leading to claims of a replication crisis. We review recent methodological literature on questionable research practices, meta-analysis, and power analysis to explain the apparently high rates of failure to replicate. Psychologists can improve research practices to advance knowledge in ways that improve replicability. We recommend that researchers adopt open science conventions of preregi-stration and full disclosure and that replication efforts be based on multiple studies rather than on a single replication attempt. We call for more sophisticated power analyses, careful consideration of the various influences on effect sizes, and more complete disclosure of nonsignificant as well as statistically significant findings.","tags":["Reproducibility Crisis and Credibility Revolution","Open Science"],"title":"Psychology, Science, and Knowledge Construction: Broadening Perspectives from the Replication Crisis","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1689122843,"objectID":"37aa10f83be8a16d5878172cbb5726de","permalink":"https://forrt.org/curated_resources/psychology-s-renaissance/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/psychology-s-renaissance/","section":"curated_resources","summary":"In 2010–2012, a few largely coincidental events led experimental psychologists to realize that their approach to collecting, analyzing, and reporting data made it too easy to publish false-positive findings. This sparked a period of methodological reflection that we review here and call Psychology's Renaissance. We begin by describing how psychologists’ concerns with publication bias shifted from worrying about file-drawered studies to worrying about p-hacked analyses. We then review the methodological changes that psychologists have proposed and, in some cases, embraced. In describing how the renaissance has unfolded, we attempt to describe different points of view fairly but not neutrally, so as to identify the most promising paths forward. In so doing, we champion disclosure and preregistration, express skepticism about most statistical solutions to publication bias, take positions on the analysis and interpretation of replication failures, and contend that meta-analytical thinking increases the prevalence of false positives. Our general thesis is that the scientific practices of experimental psychologists have improved dramatically.","tags":["Reproducibility Crisis and Credibility Revolution","Open Science"],"title":"Psychology's renaissance","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1689122843,"objectID":"7ba9ee5fbb1e0b9aea64431b8102fd26","permalink":"https://forrt.org/curated_resources/psychology-s-replication-crisis-and-the/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/psychology-s-replication-crisis-and-the/","section":"curated_resources","summary":"The past several years have been a time for soul searching in psychology, as we have gradually come to grips with the reality that some of our cherished findings are less robust than we had assumed. Nevertheless, the replication crisis highlights the operation of psychological science at its best, as it reflects our growing humility. At the same time, institutional variables, especially the growing emphasis on external funding as an expectation or de facto requirement for faculty tenure and promotion, pose largely unappreciated hazards for psychological science, including (a) incentives for engaging in questionable research practices, (b) a single-minded focus on programmatic research, (c) intellectual hyperspecialization, (d) disincentives for conducting direct replications, (e) stifling of creativity and intellectual risk taking, (f) researchers promising more than they can deliver, and (g) diminished time for thinking deeply. Preregistration should assist with (a), but will do little about (b) through (g). Psychology is beginning to right the ship, but it will need to confront the increasingly deleterious impact of the grant culture on scientific inquiry.","tags":["Reproducibility Crisis and Credibility Revolution","Open Science"],"title":"Psychology's Replication Crisis and the Grant Culture: Righting the Ship","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"5c5bda24d9b8c6a5d370b8c6bd8a70d4","permalink":"https://forrt.org/curated_resources/psychology-s-credibility-revolution/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/psychology-s-credibility-revolution/","section":"curated_resources","summary":"In 2011, Daryl Bem published a paper that seemed to demonstrate evidence for extra sensory perception (ESP). Four years later, the Open Science Collaboration failed to replicate 67 of 100 published psychological studies. These results and others have rocked the field of Psychology (and science more generally) and caused many to re- examine how research is designed, analyzed, and reported. In this seminar, we will explore the factors that contribute to false positives in the literature, including questionable research practices like p-hacking and selective reporting, as well as publication bias, and the incentive structure of science. Along the way, we’ll also discuss the strategies being used to improve the discipline and how to apply them to your own research and consumption of science.","tags":["Reproducibility Crisis and Credibility Revolution","Open Science"],"title":"Psychology’s Credibility Revolution","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1721390377,"objectID":"c57d3d7f400d986e23f865f9516e658f","permalink":"https://forrt.org/glossary/english/psychometric_meta_analysis/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/english/psychometric_meta_analysis/","section":"glossary","summary":"","tags":null,"title":"Psychometric meta-analysis","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1635961284,"objectID":"32eb634a291e8a160a124467004f07bd","permalink":"https://forrt.org/glossary/vbeta/psychometric-meta-analysis/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/vbeta/psychometric-meta-analysis/","section":"glossary","summary":"","tags":null,"title":"Psychometric meta-analysis","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1720865851,"objectID":"aa8aad48ca8d0216ab30d01e9bd1ac23","permalink":"https://forrt.org/glossary/german/psychometric_meta_analysis/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/german/psychometric_meta_analysis/","section":"glossary","summary":"","tags":null,"title":"Psychometric meta-analysis (Psychometrische Metaanalyse)","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"b16e51a3b69e763de2c5e5785f415688","permalink":"https://forrt.org/curated_resources/psyteachr/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/psyteachr/","section":"curated_resources","summary":"Materials for the University of Glasgow Institute of Neuroscience and Psychology’s undergraduate and MSc methods courses + Experiences, insights, and materials for teaching R across all undergraduate and postgraduate levels.","tags":["Curriculum Change","Education","Educators","Open Education","Open Scholarship Tools and Technologies","Reproducibility","Students"],"title":"PsyTeachR","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"1284023c514e470c16917a2fa2033b7b","permalink":"https://forrt.org/curated_resources/public-availability-of-published-researc/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/public-availability-of-published-researc/","section":"curated_resources","summary":"Background There is increasing interest to make primary data from published research publicly available. We aimed to assess the current status of making research data available in highly-cited journals across the scientific literature. Methods and Results We reviewed the first 10 original research papers of 2009 published in the 50 original research journals with the highest impact factor. For each journal we documented the policies related to public availability and sharing of data. Of the 50 journals, 44 (88%) had a statement in their instructions to authors related to public availability and sharing of data. However, there was wide variation in journal requirements, ranging from requiring the sharing of all primary data related to the research to just including a statement in the published manuscript that data can be available on request. Of the 500 assessed papers, 149 (30%) were not subject to any data availability policy. Of the remaining 351 papers that were covered by some data availability policy, 208 papers (59%) did not fully adhere to the data availability instructions of the journals they were published in, most commonly (73%) by not publicly depositing microarray data. The other 143 papers that adhered to the data availability instructions did so by publicly depositing only the specific data type as required, making a statement of willingness to share, or actually sharing all the primary data. Overall, only 47 papers (9%) deposited full primary raw data online. None of the 149 papers not subject to data availability policies made their full primary data publicly available. Conclusion A substantial proportion of original research papers published in high-impact journals are either not subject to any data availability policies, or do not adhere to the data availability instructions in their respective journals. This empiric evaluation highlights opportunities for improvement.","tags":["Bibliometrics","Clinical Trials","Data","Data Processing","Microarrays","Policy","Public Policy","Publishing","Reproducibility","Science Policy","Scientific Publishing"],"title":"Public Availability of Published Research Data in High-Impact Journals","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"7df7e755201d6a7a6706598e6798e2ae","permalink":"https://forrt.org/curated_resources/public-data-archiving-in-ecology-and-evo/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/public-data-archiving-in-ecology-and-evo/","section":"curated_resources","summary":"Policies that mandate public data archiving (PDA) successfully increase accessibility to data underlying scientific publications. However, is the data quality sufficient to allow reuse and reanalysis? We surveyed 100 datasets associated with nonmolecular studies in journals that commonly publish ecological and evolutionary research and have a strong PDA policy. Out of these datasets, 56% were incomplete, and 64% were archived in a way that partially or entirely prevented reuse. We suggest that cultural shifts facilitating clearer benefits to authors are necessary to achieve high-quality PDA and highlight key guidelines to help authors increase their data’s reuse potential and compliance with journal data policies.","tags":["Archives","Data","Data Processing","Evolutionary Biology","Evolutionary Genetics","Public Policy","Reproducibility","Science Policy","Scientific Publishing"],"title":"Public Data Archiving in Ecology and Evolution: How Well Are We Doing?","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1721390377,"objectID":"1a2456b594c19f6146747dffec813ad9","permalink":"https://forrt.org/glossary/english/public_trust_in_science/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/english/public_trust_in_science/","section":"glossary","summary":"","tags":null,"title":"Public Trust in Science","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1635961284,"objectID":"3d6598c574c8319542a675d5d4cb589f","permalink":"https://forrt.org/glossary/vbeta/public-trust-in-science/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/vbeta/public-trust-in-science/","section":"glossary","summary":"","tags":null,"title":"Public Trust in Science","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1720865851,"objectID":"2dbe8a55c3ed6b46f4acfc8a09fae9c5","permalink":"https://forrt.org/glossary/german/public_trust_in_science/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/german/public_trust_in_science/","section":"glossary","summary":"","tags":null,"title":"Public Trust in Science (öffentliches Vertrauen in die Wissenschaft)","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1721390377,"objectID":"46abf6f44eb3f3dde5e7724ea825fd20","permalink":"https://forrt.org/glossary/english/publication_bias/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/english/publication_bias/","section":"glossary","summary":"","tags":null,"title":"Publication bias (File Drawer Problem)","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1635961284,"objectID":"7c6a8085f2e7b6595f5211a55beddbec","permalink":"https://forrt.org/glossary/vbeta/publication-bias-file-drawer-proble/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/vbeta/publication-bias-file-drawer-proble/","section":"glossary","summary":"","tags":null,"title":"Publication bias (File Drawer Problem)","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1720865851,"objectID":"fb62631196b5970ea9a0a60e72a953b0","permalink":"https://forrt.org/glossary/german/publication_bias/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/german/publication_bias/","section":"glossary","summary":"","tags":null,"title":"Publication bias (File Drawer Problem) (Publikationsverzerrung; Aktenschubladenproblem)","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1689122843,"objectID":"47cb39f9e30ae288a525e13eebc54592","permalink":"https://forrt.org/curated_resources/publication-bias-and-the-canonization-of/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/publication-bias-and-the-canonization-of/","section":"curated_resources","summary":"Science is facing a “replication crisis” in which many experimental findings cannot be replicated and are likely to be false. Does this imply that many scientific facts are false as well? To find out, we explore the process by which a claim becomes fact. We model the community’s confidence in a claim as a Markov process with successive published results shifting the degree of belief. Publication bias in favor of positive findings influences the distribution of published results. We find that unless a sufficient fraction of negative results are published, false claims frequently can become canonized as fact. Data-dredging, p-hacking, and similar behaviors exacerbate the problem. Should negative results become easier to publish as a claim approaches acceptance as a fact, however, true and false claims would be more readily distinguished. To the degree that the model reflects the real world, there may be serious concerns about the validity of purported facts in some disciplines.","tags":["Reproducibility Crisis and Credibility Revolution","Open Science"],"title":"Publication bias and the canonization of false facts. ","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1689122843,"objectID":"e1272f219501333d3dbca1f8b302aac0","permalink":"https://forrt.org/curated_resources/publication-bias-and-the-limited-strengt/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/publication-bias-and-the-limited-strengt/","section":"curated_resources","summary":"Few models of self-control have generated as much scientific interest as has the limited strength model. One of the entailments of this model, the depletion effect, is the expectation that acts of self-control will be less effective when they follow prior acts of self-control. Results from a previous meta-analysis concluded that the depletion effect is robust and medium in magnitude (d = 0.62). However, when we applied methods for estimating and correcting for small-study effects (such as publication bias) to the data from this previous meta-analysis effort, we found very strong signals of publication bias, along with an indication that the depletion effect is actually no different from zero. We conclude that until greater certainty about the size of the depletion effect can be established, circumspection about the existence of this phenomenon is warranted, and that rather than elaborating on the model, research efforts should focus on establishing whether the basic effect exists. We argue that the evidence for the depletion effect is a useful case study for illustrating the dangers of small-study effects as well as some of the possible tools for mitigating their influence in psychological science.","tags":["Reproducibility Crisis and Credibility Revolution","Open Science"],"title":"Publication bias and the limited strength model of self-control: has the evidence for ego depletion been overestimated?","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"0fc104cf2458f42928dcf97c06e00696","permalink":"https://forrt.org/curated_resources/publication-bias-in-clinical-research/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/publication-bias-in-clinical-research/","section":"curated_resources","summary":"In a retrospective survey, 487 research projects approved by the Central Oxford Research Ethics Committee between 1984 and 1987, were studied for evidence of publication bias. As of May, 1990, 285 of the studies had been analysed by the investigators, and 52% of these had been published. Studies with statistically significant results were more likely to be published than those finding no difference between the study groups (adjusted odds ratio [OR] 2·32; 95% confidence interval [Cl] 1·25-4·28). Studies with significant results were also more likely to lead to a greater number of publications and presentations and to be published in journals with a high citation impact factor. An increased likelihood of publication was also associated with a high rating by the investigator of the importance of the study results, and with increasing sample size. The tendency towards publication bias was greater with observational and laboratory-based experimental studies (OR=3·79; 95% CI=1·47-9·76) than with randomised clinical trials (OR=0·84; 95% CI=0·34-2·09). We have confirmed the presence of publication bias in a cohort of clinical research studies. These findings suggest that conclusions based only on a review of published data should be interpreted cautiously, especially for observational studies. Improved strategies are needed to identify the results of unpublished as well as published studies.","tags":["Publication Bias","Clinical Research"],"title":"Publication bias in clinical research","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1689122843,"objectID":"f6605c7ae2ffa869447557840f682269","permalink":"https://forrt.org/curated_resources/publication-bias-in-psychology-a-diagnos/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/publication-bias-in-psychology-a-diagnos/","section":"curated_resources","summary":"Background: The p value obtained from a significance test provides no information about the magnitude or importance of the underlying phenomenon. Therefore, additional reporting of effect size is often recommended. Effect sizes are theoretically independent from sample size. Yet this may not hold true empirically: non-independence could indicate publication bias. Methods: We investigate whether effect size is independent from sample size in psychological research. We randomly sampled 1,000 psychological articles from all areas of psychological research. We extracted p values, effect sizes, and sample sizes of all empirical papers, and calculated the correlation between effect size and sample size, and investigated the distribution of p values. Results: We found a negative correlation of r = −.45 [95% CI: −.53; −.35] between effect size and sample size. In addition, we found an inordinately high number of p values just passing the boundary of significance. Additional data showed that neither implicit nor explicit power analysis could account for this pattern of findings. Conclusion: The negative correlation between effect size and samples size, and the biased distribution of p values indicate pervasive publication bias in the entire field of psychology.","tags":["Reproducibility Crisis and Credibility Revolution","Open Science"],"title":"Publication Bias in Psychology: A Diagnosis Based on the Correlation between Effect Size and Sample Size","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1689122843,"objectID":"0eb5c876e4e3c05d87c7def7aef94b2c","permalink":"https://forrt.org/curated_resources/publication-bias-in-the-social-sciences/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/publication-bias-in-the-social-sciences/","section":"curated_resources","summary":"We studied publication bias in the social sciences by analyzing a known population of conducted studies—221 in total—in which there is a full accounting of what is published and unpublished. We leveraged Time-sharing Experiments in the Social Sciences (TESS), a National Science Foundation–sponsored program in which researchers propose survey-based experiments to be run on representative samples of American adults. Because TESS proposals undergo rigorous peer review, the studies in the sample all exceed a substantial quality threshold. Strong results are 40 percentage points more likely to be published than are null results and 60 percentage points more likely to be written up. We provide direct evidence of publication bias and identify the stage of research production at which publication bias occurs: Authors do not write up and submit null findings","tags":["Reproducibility Crisis and Credibility Revolution","Open Science"],"title":"Publication bias in the social sciences: Unlocking the file drawer","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1689122843,"objectID":"4913ef032ac20933a80804a24972d107","permalink":"https://forrt.org/curated_resources/publication-decisions-and-their-possible/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/publication-decisions-and-their-possible/","section":"curated_resources","summary":"There is some evidence that in fields where statistical tests of significance are commonly used, research which yields nonsignificant results is not published. Such research being unknown to other investigators may be repeated independently until eventually by chance a significant result occurs-an \"error of the first kind\"-and is published. Significant results published in these fields are seldom verified by independent replication. The possibility thus arises that the literature of such a field consists in substantial part of false conclusions resulting from errors of the first kind in statistical tests of significance","tags":[""],"title":"Publication Decisions and their Possible Effects on Inferences Drawn from Tests of Significance—or Vice Versa","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"825af891baddf0cb2208263d29730569","permalink":"https://forrt.org/curated_resources/publication-of-nih-funded-trials-registe/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/publication-of-nih-funded-trials-registe/","section":"curated_resources","summary":"Objective To review patterns of publication of clinical trials funded by US National Institutes of Health (NIH) in peer reviewed biomedical journals indexed by Medline. Design Cross sectional analysis. Setting Clinical trials funded by NIH and registered within ClinicalTrials.gov (clinicaltrials.gov), a trial registry and results database maintained by the US National Library of Medicine, after 30 September 2005 and updated as having been completed by 31 December 2008, allowing at least 30 months for publication after completion of the trial. Main outcome measures Publication and time to publication in the biomedical literature, as determined through Medline searches, the last of which was performed in June 2011. Results Among 635 clinical trials completed by 31 December 2008, 294 (46%) were published in a peer reviewed biomedical journal, indexed by Medline, within 30 months of trial completion. The median period of follow-up after trial completion was 51 months (25th-75th centiles 40-68 months), and 432 (68%) were published overall. Among published trials, the median time to publication was 23 months (14-36 months). Trials completed in either 2007 or 2008 were more likely to be published within 30 months of study completion compared with trials completed before 2007 (54% (196/366) v 36% (98/269); P\u003c0.001). Conclusions Despite recent improvement in timely publication, fewer than half of trials funded by NIH are published in a peer reviewed biomedical journal indexed by Medline within 30 months of trial completion. Moreover, after a median of 51 months after trial completion, a third of trials remained unpublished.","tags":["NIH","Clinical Research","Preregistration"],"title":"Publication of NIH funded trials registered in ClinicalTrials.gov: Cross sectional analysis","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1689122843,"objectID":"baad960ca0e5dac12543f69ef4cffa33","permalink":"https://forrt.org/curated_resources/publication-prejudices-an-experimental-s/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/publication-prejudices-an-experimental-s/","section":"curated_resources","summary":"Confirmatory bias is the tendency to emphasize and believe experiences which support one's views and to ignore or discredit those which do not. The effects of this tendency have been repeatedly documented in clinical research. However, its ramifications for the behavior of scientists have yet to be adequately explored. For example, although publication is a critical element in determining the contribution and impact of scientific findings, little research attention has been devoted to the variables operative in journal review policies. In the present study, 75 journal reviewers were asked to referee manuscripts which described identical experimental procedures but which reported positive, negative, mixed, or no results. In addition to showing poor interrater agreement, reviewers were strongly biased against manuscripts which reported results contrary to their theoretical perspective. The implications of these findings for epistemology and the peer review system are briefly addressed.","tags":["Reproducibility Crisis and Credibility Revolution","Open Science"],"title":"Publication Prejudices: An Experimental Study of Confirmatory Bias in the Peer Review System ","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"2ae0978899adcb1d43d92bc938e5c449","permalink":"https://forrt.org/curated_resources/publication-pressure-and-scientific-misc/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/publication-pressure-and-scientific-misc/","section":"curated_resources","summary":"There is increasing evidence that scientific misconduct is more common than previously thought. Strong emphasis on scientific productivity may increase the sense of publication pressure. We administered a nationwide survey to Flemish biomedical scientists on whether they had engaged in scientific misconduct and whether they had experienced publication pressure. A total of 315 scientists participated in the survey; 15% of the respondents admitted they had fabricated, falsified, plagiarized, or manipulated data in the past 3 years. Fraud was more common among younger scientists working in a university hospital. Furthermore, 72% rated publication pressure as “too high.” Publication pressure was strongly and significantly associated with a composite scientific misconduct severity score.","tags":["Reproducibility Crisis and Credibility Revolution","Open Science"],"title":"Publication pressure and scientific misconduct in medical scientists","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"354e81188ad82cd3acb1c6c413258116","permalink":"https://forrt.org/curated_resources/publication-rate-in-preclinical-research/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/publication-rate-in-preclinical-research/","section":"curated_resources","summary":"Objectives\nThe ultimate goal of biomedical research is the development of new treatment options for patients. Animal models are used if questions cannot be addressed otherwise. Currently, it is widely believed that a large fraction of performed studies are never published, but there are no data that directly address this question.\n\nMethods\nWe have tracked a selection of animal study protocols approved in the University Medical Center Utrecht in the Netherlands, to assess whether these have led to a publication with a follow-up period of 7 years.\n\nResults\nWe found that 60% of all animal study protocols led to at least one publication (full text or abstract). A total of 5590 animals were used in these studies, of which 26% was reported in the resulting publications.\n\nConclusions\nThe data presented here underline the need for preclinical preregistration, in view of the risk of reporting and publication bias in preclinical research. We plea that all animal study protocols should be prospectively registered on an online, accessible platform to increase transparency and data sharing. To facilitate this, we have developed a platform dedicated to animal study protocol registration: www.preclinicaltrials.eu.","tags":["Preregistration","Translational Research","Publication Rate","Publication Bias"],"title":"Publication rate in preclinical research: A plea for preregistration","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1721390377,"objectID":"7e36a881fbf013bb570b1806fc028bf8","permalink":"https://forrt.org/glossary/english/publish_or_perish/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/english/publish_or_perish/","section":"glossary","summary":"","tags":null,"title":"Publish or Perish","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1635961284,"objectID":"447a4ce86f1f624b14f59b3593cf0c56","permalink":"https://forrt.org/glossary/vbeta/publish-or-perish/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/vbeta/publish-or-perish/","section":"glossary","summary":"","tags":null,"title":"Publish or Perish","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1720865851,"objectID":"30fa8a884fb3b4a21976b9ed9d8fa15e","permalink":"https://forrt.org/glossary/german/publish_or_perish/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/german/publish_or_perish/","section":"glossary","summary":"","tags":null,"title":"Publish or Perish (Publizieren oder untergehen)","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1721390377,"objectID":"7f8c15e34c3d3376a03fd78018eb588f","permalink":"https://forrt.org/glossary/english/pubpeer/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/english/pubpeer/","section":"glossary","summary":"","tags":null,"title":"PubPeer","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1720865851,"objectID":"ac1cdbdcdcac15156c08ccdcdcee48f9","permalink":"https://forrt.org/glossary/german/pubpeer/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/german/pubpeer/","section":"glossary","summary":"","tags":null,"title":"PubPeer","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1635961284,"objectID":"6e92fa9af75860162a0e03dfe2df43ad","permalink":"https://forrt.org/glossary/vbeta/pubpeer/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/vbeta/pubpeer/","section":"glossary","summary":"","tags":null,"title":"PubPeer ","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1721390377,"objectID":"1e9bbf0b72af025d843f33a0a856f21b","permalink":"https://forrt.org/glossary/english/python/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/english/python/","section":"glossary","summary":"","tags":null,"title":"Python","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1720865851,"objectID":"b7d2ea037c23c8bd03855f5792fd46f5","permalink":"https://forrt.org/glossary/german/python/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/german/python/","section":"glossary","summary":"","tags":null,"title":"Python","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1635961284,"objectID":"25e454b86d33cf29b5b019f87807f8fa","permalink":"https://forrt.org/glossary/vbeta/python/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/vbeta/python/","section":"glossary","summary":"","tags":null,"title":"Python","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"e606c3ede2dd3e2a9215b2c79d9440eb","permalink":"https://forrt.org/curated_resources/python-for-harvesting-data-on-the-web/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/python-for-harvesting-data-on-the-web/","section":"curated_resources","summary":"This session is an intermediate-to-advanced level class that offers some ideas for how to approach the following common data wrangling needs in research: 1) Obtain data and load it into a suitable data \"container\" for analysis, often via a web interface, especially an API, 2) parse the data retrieved via an API and turn it into a useful object for manipulation and analysis, and 3) perform some basic summary counts of records in a dataset and work up a quick visualization.","tags":["Analysis","Open Scholarship Tools and Technologies","Research Data Management Tools","Researchers"],"title":"Python for Harvesting Data on the Web","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"a7b1d22adb4d1c0de6a7d83211354369","permalink":"https://forrt.org/curated_resources/python-for-humanities/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/python-for-humanities/","section":"curated_resources","summary":"Python is a general purpose programming language that is useful for writing scripts to work effectively and reproducibly with data. This is an introduction to Python designed for participants with no programming experience. These lessons can be taught in a day (~ 6 hours). They start with some basic information about Python syntax, the Jupyter notebook interface, and move through how to import CSV files, using the pandas package to work with data frames, how to calculate summary information from a data frame, and a brief introduction to plotting. The last lesson demonstrates how to work with databases directly from Python.","tags":["Analysis","Data","Education","Open Scholarship Tools and Technologies","Python","Reproducibility","Research Data Management Tools","Researchers"],"title":"Python for Humanities","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"3f89922526266507d02089304030cb22","permalink":"https://forrt.org/curated_resources/qdr-data-for-teaching/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/qdr-data-for-teaching/","section":"curated_resources","summary":"As part of its mission, QDR promotes the pedagogical use of published qualitative and multi-method data projects. Shared qualitative data can play a crucial role for both substantive and methods teaching. Below is a non-exhaustive list of projects we want to highlight as being particularly well-suited for secondary uses by undergraduate and graduate students. They all\n\n- Allow unrestricted access to data (after free website registration)\n- Are well documented so data is understandable\n- Are rich in content to allow deep engagement and the pursuit of a variety of questions\n- Are mostly in English for widest usability.\n- The projects are listed by data type and include a short description. For additional details, simply click through to the project page in our data catalog.","tags":[""],"title":"QDR Data for Teaching","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1721390377,"objectID":"7f852d867275be6950d96fec2523a688","permalink":"https://forrt.org/glossary/english/qualitative_research/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/english/qualitative_research/","section":"glossary","summary":"","tags":null,"title":"Qualitative research","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1635961284,"objectID":"9a105b739cf6640ec8e1d26b1561e102","permalink":"https://forrt.org/glossary/vbeta/qualitative-research/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/vbeta/qualitative-research/","section":"glossary","summary":"","tags":null,"title":"Qualitative research","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1720865851,"objectID":"3a6fc2644fee413368306de0c345fd12","permalink":"https://forrt.org/glossary/german/qualitative_research/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/german/qualitative_research/","section":"glossary","summary":"","tags":null,"title":"Qualitative research (Qualitative Forschung)","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"711ddca3cf0b273d32a169152276e32a","permalink":"https://forrt.org/curated_resources/qualitative-research-using-open-tools/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/qualitative-research-using-open-tools/","section":"curated_resources","summary":"Qualitative research has long suffered from a lack of free tools for analysis, leaving no options for researchers without significant funds for software licenses. This presents significant challenges for equity. This panel discussion will explore the first two free/libre open source qualitative analysis tools out there: qcoder (R package) and Taguette (desktop application). Drawing from the diverse backgrounds of the presenters (social science, library \u0026 information science, software engineering), we will discuss what openness and extensibility means for qualitative research, and how the two tools we've built facilitate equitable, open sharing.","tags":["Analysis","Librarians","Open Scholarship Tools and Technologies","Qualitative","R","Research Data Management Tools","Researchers","Taguette"],"title":"Qualitative Research Using Open Tools","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"02d7f32b95e16e093a5691a510078b64","permalink":"https://forrt.org/curated_resources/quality-indicators-of-secondary-data-ana/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/quality-indicators-of-secondary-data-ana/","section":"curated_resources","summary":"Secondary data analyses occur when new analyses are proposed for existing data. Although they are prevalent in special education research, there is little guidance on how to prepare secondary data analyses studies. Preregistration of secondary data analyses studies provides a nice opportunity and structure for fellow researchers to share innovative questions and analytic approaches to existing data sets as well as increase transparency. In this manuscript, we (a) describe quality indicators of secondary data analyses consistent with open science practices and (b) provide applied examples of these indicators from a sampling of published studies based on two iterations of data from the National Longitudinal Transition Study (NLTS2 and NLTS2012) with the overall goals to provide guidance to authors and peer reviewers and promote collaboration among fellow researchers engaged in secondary analyses for a range of purposes.","tags":["Secondary Data Analysis","Special Education","Preregistration"],"title":"Quality Indicators of Secondary Data Analyses in Special Education Research: A Preregistration Guide","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1689122843,"objectID":"a1fbc3eefc4dc2de57f5491e3e9fd816","permalink":"https://forrt.org/curated_resources/quality-uncertainty-erodes-trust-in-scie/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/quality-uncertainty-erodes-trust-in-scie/","section":"curated_resources","summary":"When consumers of science (readers and reviewers) lack relevant details about the study design, data, and analyses, they cannot adequately evaluate the strength of a scientific study. Lack of transparency is common in science, and is encouraged by journals that place more emphasis on the aesthetic appeal of a manuscript than the robustness of its scientific claims. In doing this, journals are implicitly encouraging authors to do whatever it takes to obtain eye-catching results. To achieve this, researchers can use common research practices that beautify results at the expense of the robustness of those results (e.g., p-hacking). The problem is not engaging in these practices, but failing to disclose them. A car whose carburetor is duct-taped to the rest of the car might work perfectly fine, but the buyer has a right to know about the duct-taping. Without high levels of transparency in scientific publications, consumers of scientific manuscripts are in a similar position as buyers of used cars – they cannot reliably tell the difference between lemons and high quality findings. This phenomenon – quality uncertainty – has been shown to erode trust in economic markets, such as the used car market. The same problem threatens to erode trust in science. The solution is to increase transparency and give consumers of scientific research the information they need to accurately evaluate research. Transparency would also encourage researchers to be more careful in how they conduct their studies and write up their results. To make this happen, we must tie journals’ reputations to their practices regarding transparency. Reviewers hold a great deal of power to make this happen, by demanding the transparency needed to rigorously evaluate scientific manuscripts. The public expects transparency from science, and appropriately so – we should be held to a higher standard than used car salespeople","tags":["Reproducibility Crisis and Credibility Revolution","Open Science"],"title":"Quality Uncertainty Erodes Trust in Science","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"088e71da7dfcb5b97e871626c97a3dd6","permalink":"https://forrt.org/curated_resources/quantitative-emerging-practices-and-meth/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/quantitative-emerging-practices-and-meth/","section":"curated_resources","summary":"Recently, a reporter in the Chronicle of Higher Education wrote that “psychology is having an uneasy moment” (Zamudio-Suarez, 2016). The “uneasy moment” to which she referred is a movementof field self-criticism that has gained incredible steam and features media coverage, twitter wars, and a numerous methods-focused blogs.The purpose of this course is to sort through the criticisms and recommendations that have emerged in our field. We will address a wide swath of topics including historical and emerging concerns about our practices, modern journal requirements, and recommendations on the horizon. The course will center primarily around three types of topics: 1) philosophy of psychological science, 2) criticisms of past and present practices, 3) practical (andimpractical) solutions.The course will be driven by both discussion and hands-on practice. On most days, we will spend the first half of class discussing readings and the second half of class learning hands-on practices.","tags":["Reproducibility Crisis and Credibility Revolution","Open Science"],"title":"Quantitative Emerging Practices and Methods in Psychology","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1721390377,"objectID":"78f0b081898ed11b602360b75adef028","permalink":"https://forrt.org/glossary/english/quantitative_research/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/english/quantitative_research/","section":"glossary","summary":"","tags":null,"title":"Quantitative research","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1635961284,"objectID":"a14cec528db5b454453d052c5d736d07","permalink":"https://forrt.org/glossary/vbeta/quantitative-research/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/vbeta/quantitative-research/","section":"glossary","summary":"","tags":null,"title":"Quantitative research ","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1720865851,"objectID":"76fd81facd65740d3fbe5779a5719f9f","permalink":"https://forrt.org/glossary/german/quantitative_research/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/german/quantitative_research/","section":"glossary","summary":"","tags":null,"title":"Quantitative research (Quantitative Forschung)  (translated, reviewed)","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"562471556f271a5eb3887bc03267ae8f","permalink":"https://forrt.org/curated_resources/questionable-and-open-research-practices/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/questionable-and-open-research-practices/","section":"curated_resources","summary":"Discussions of how to improve research quality are predominant in a number of fields, including education. But how prevalent are the use of problematic practices and the improved practices meant to counter them? This baseline information will be a critical data source as education researchers seek to improve our research practices. In this preregistered study, we replicated and extended previous studies from other fields by asking education researchers about 10 questionable research practices and 5 open research practices. We asked them to estimate the prevalence of the practices in the field, self-report their own use of such practices, and estimate the appropriateness of these behaviors in education research. We made predictions under four umbrella categories: comparison to psychology, geographic location, career stage, and quantitative orientation. Broadly, our results suggest that both questionable and open research practices are part of the typical research practices of many educational researchers. Preregistration, code, and data can be found at https://osf.io/83mwk/.","tags":["Data","Open Data","Preregistration","Research Methods"],"title":"Questionable and Open Research Practices in Education Research","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1720865851,"objectID":"d471575674271460eb8a724013727038","permalink":"https://forrt.org/glossary/german/questionable_measurement_practices/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/german/questionable_measurement_practices/","section":"glossary","summary":"","tags":null,"title":"Questionable Measurement Practices (QMP; Fragwürdige Forschungspraktiken))","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1721390377,"objectID":"2dea6995ed39b97ed16b2d5a03151c11","permalink":"https://forrt.org/glossary/english/questionable_measurement_practices/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/english/questionable_measurement_practices/","section":"glossary","summary":"","tags":null,"title":"Questionable Measurement Practices (QMP)","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1635961284,"objectID":"1dd5395fbe322fe3a10cd9d850e8a725","permalink":"https://forrt.org/glossary/vbeta/questionable-measurement-practices-/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/vbeta/questionable-measurement-practices-/","section":"glossary","summary":"","tags":null,"title":"Questionable Measurement Practices (QMP)","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"9b363fcd89b39902f017a397bce9472c","permalink":"https://forrt.org/curated_resources/questionable-research-practices-among-it/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/questionable-research-practices-among-it/","section":"curated_resources","summary":"A survey in the United States revealed that an alarmingly large percentage of university psychologists admitted having used questionable research practices that can contaminate the research literature with false positive and biased findings. We conducted a replication of this study among Italian research psychologists to investigate whether these findings generalize to other countries. All the original materials were translated into Italian, and members of the Italian Association of Psychology were invited to participate via an online survey. The percentages of Italian psychologists who admitted to having used ten questionable research practices were similar to the results obtained in the United States although there were small but significant differences in self-admission rates for some QRPs. Nearly all researchers (88%) admitted using at least one of the practices, and researchers generally considered a practice possibly defensible if they admitted using it, but Italian researchers were much less likely than US researchers to consider a practice defensible. Participants’ estimates of the percentage of researchers who have used these practices were greater than the self-admission rates, and participants estimated that researchers would be unlikely to admit it. In written responses, participants argued that some of these practices are not questionable and they have used some practices because reviewers and journals demand it. The similarity of results obtained in the United States, this study, and a related study conducted in Germany suggest that adoption of these practices is an international phenomenon and is likely due to systemic features of the international research and publication processes.","tags":["Analysis","Behavior","Experimental Psychology","Italian People","Metrics","Psychologists","Psychology","Psychometrics","Questionnaires","Reproducibility","Research Methods","United States"],"title":"Questionable research practices among Italian research psychologists","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"47da8380dd8f720bbafebb9bab4a826e","permalink":"https://forrt.org/curated_resources/questionable-research-practices-followin/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/questionable-research-practices-followin/","section":"curated_resources","summary":"The credibility of psychological findings can be undermined by a history of questionable research practices (QRPs) by researchers. One remedy for this problem is the pre-registration of a study in which a research protocol is registered before beginning an experiment. However, the current style of pre-registration can be negatively affected by other QRPs. The purpose of this study was to demonstrate that researchers can engage in QRPs, even after a study has been preregistered. In this demonstration study, we used eight QRPs to obtain statistically meaningful results that supported an ad hoc hypothesis. Major system updates such as pre-registration, peer review, and evaluation are required to address these harmful practices. We hope that the present demonstration study provides momentum for further discussions on next-generation research practices.","tags":["Open Science","Preregistration","Transparency","Questionable Research Practices","Embodied Cognition"],"title":"Questionable research practices following pre-registration","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"e4dc97d1a263f6c68b7a17b06c3dfc76","permalink":"https://forrt.org/curated_resources/questionable-research-practices-in-ecolo/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/questionable-research-practices-in-ecolo/","section":"curated_resources","summary":"We surveyed 807 researchers (494 ecologists and 313 evolutionary biologists) about their use of Questionable Research Practices (QRPs), including cherry picking statistically significant results, p hacking, and hypothesising after the results are known (HARKing). We also asked them to estimate the proportion of their colleagues that use each of these QRPs. Several of the QRPs were prevalent within the ecology and evolution research community. Across the two groups, we found 64% of surveyed researchers reported they had at least once failed to report results because they were not statistically significant (cherry picking); 42% had collected more data after inspecting whether results were statistically significant (a form of p hacking) and 51% had reported an unexpected finding as though it had been hypothesised from the start (HARKing). Such practices have been directly implicated in the low rates of reproducible results uncovered by recent large scale replication studies in psychology and other disciplines. The rates of QRPs found in this study are comparable with the rates seen in psychology, indicating that the reproducibility problems discovered in psychology are also likely to be present in ecology and evolution.","tags":["Analysis","Behavioral Ecology","Community Ecology","Data","Ecology and Environmental Sciences","Evolutionary Biology","Evolutionary Ecology","HARKing","Psychology","Publication Ethics","Statistical Data","Statistics"],"title":"Questionable research practices in ecology and evolution","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1635961284,"objectID":"625f58c64ca6e71b1fb0f5cbe27ee3b3","permalink":"https://forrt.org/glossary/vbeta/questionable-research-practices-or-/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/vbeta/questionable-research-practices-or-/","section":"glossary","summary":"","tags":null,"title":"Questionable Research Practices or Questionable Reporting Practices (QRPs)","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"f85642e675ec43a4f8ce5ccbf752421e","permalink":"https://forrt.org/glossary/english/questionable_research-practices-or_questionable_reporting_practices/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/english/questionable_research-practices-or_questionable_reporting_practices/","section":"glossary","summary":"","tags":null,"title":"Questionable Research Practices or Questionable Reporting Practices (QRPs)","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"1690b3a0451642d8ce1f483facfa1a86","permalink":"https://forrt.org/glossary/german/questionable_research-practices-or_questionable_reporting_practices/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/german/questionable_research-practices-or_questionable_reporting_practices/","section":"glossary","summary":"","tags":null,"title":"Questionable Research Practices or Questionable Reporting Practices (QRPs) (Fragwürdige Forschungs- oder Veröffentlichungspraktiken)","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1689122843,"objectID":"296912b0d3ab5a6a74b2599c2e5de3cc","permalink":"https://forrt.org/curated_resources/questionable-research-practices-revisite/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/questionable-research-practices-revisite/","section":"curated_resources","summary":"The current discussion of questionable research practices (QRPs) is meant to improve the quality of science. It is, however, important to conduct QRP studies with the same scrutiny as all research. We note problems with overestimates of QRP prevalence and the survey methods used in the frequently cited study by John, Loewenstein, and Prelec. In a survey of German psychologists, we decomposed QRP prevalence into its two multiplicative components, proportion of scientists who ever committed a behavior and, if so, how frequently they repeated this behavior across all their research. The resulting prevalence estimates are lower by order of magnitudes. We conclude that inflated prevalence estimates, due to problematic interpretation of survey data, can create a descriptive norm (QRP is normal) that can counteract the injunctive norm to minimize QRPs and unwantedly damage the image of behavioral sciences, which are essential to dealing with many societal problems.","tags":["Reproducibility Crisis and Credibility Revolution","Open Science"],"title":"Questionable Research Practices Revisited","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1721390377,"objectID":"337ecbbd1fdb9f5153be707ca37f4864","permalink":"https://forrt.org/glossary/english/r/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/english/r/","section":"glossary","summary":"","tags":null,"title":"R","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1720865851,"objectID":"8a3508e1479f0c98cd860b1c3ae99a0c","permalink":"https://forrt.org/glossary/german/r/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/german/r/","section":"glossary","summary":"","tags":null,"title":"R","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1635961284,"objectID":"218e610d7918af9ce91055f0ecb861b0","permalink":"https://forrt.org/glossary/vbeta/r/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/vbeta/r/","section":"glossary","summary":"","tags":null,"title":"R","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"6c35182ba206d67a6f9aec69aa40b0ba","permalink":"https://forrt.org/curated_resources/r-for-reproducible-scientific-analysis/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/r-for-reproducible-scientific-analysis/","section":"curated_resources","summary":"This lesson in part of Software Carpentry workshop and teach novice programmers to write modular code and best practices for using R for data analysis. an introduction to R for non-programmers using gapminder data The goal of this lesson is to teach novice programmers to write modular code and best practices for using R for data analysis. R is commonly used in many scientific disciplines for statistical analysis and its array of third-party packages. We find that many scientists who come to Software Carpentry workshops use R and want to learn more. The emphasis of these materials is to give attendees a strong foundation in the fundamentals of R, and to teach best practices for scientific computing: breaking down analyses into modular units, task automation, and encapsulation. Note that this workshop will focus on teaching the fundamentals of the programming language R, and will not teach statistical analysis. The lesson contains more material than can be taught in a day. The instructor notes page has some suggested lesson plans suitable for a one or half day workshop. A variety of third party packages are used throughout this workshop. These are not necessarily the best, nor are they comprehensive, but they are packages we find useful, and have been chosen primarily for their usability.","tags":["Analysis","Data","Education","Open Scholarship Tools and Technologies","R","Reproducibility","Research Data Management Tools","Researchers"],"title":"R for Reproducible Scientific Analysis","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"d7ca97d7c8bd41a09de30a5ed0ae03cd","permalink":"https://forrt.org/curated_resources/r-for-social-scientists/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/r-for-social-scientists/","section":"curated_resources","summary":"Data Carpentry lesson part of the Social Sciences curriculum. This lesson teaches how to analyse and visualise data used by social scientists. Data Carpentry’s aim is to teach researchers basic concepts, skills, and tools for working with data so that they can get more done in less time, and with less pain. The lessons below were designed for those interested in working with social sciences data in R. This is an introduction to R designed for participants with no programming experience. These lessons can be taught in a day (~ 6 hours). They start with some basic information about R syntax, the RStudio interface, and move through how to import CSV files, the structure of data frames, how to deal with factors, how to add/remove rows and columns, how to calculate summary statistics from a data frame, and a brief introduction to plotting.","tags":["Analysis","Data","Education","Open Scholarship Tools and Technologies","R","Reproducibility","Research Data Management Tools","Researchers","RStudio"],"title":"R for Social Scientists","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1689122843,"objectID":"fc3f002fef071317abb92e792a3c68c5","permalink":"https://forrt.org/curated_resources/r-graphics-cookbook-practical-recipes-fo/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/r-graphics-cookbook-practical-recipes-fo/","section":"curated_resources","summary":"This practical guide provides more than 150 recipes to help you generate high-quality graphs quickly, without having to comb through all the details of R’s graphing systems. Each recipe tackles a specific problem with a solution you can apply to your own project, and includes a discussion of how and why the recipe works.","tags":["Book","Software","R"],"title":"R Graphics Cookbook: Practical Recipes for Visualizing Data ","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"19e6ae381e1ceaa082f7488c63140aeb","permalink":"https://forrt.org/curated_resources/r-para-analisis-cientificos-reproducible/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/r-para-analisis-cientificos-reproducible/","section":"curated_resources","summary":"Una introducción a R utilizando los datos de Gapminder. El objetivo de esta lección es enseñar a las programadoras principiantes a escribir códigos modulares y adoptar buenas prácticas en el uso de R para el análisis de datos. R nos provee un conjunto de paquetes desarrollados por terceros que se usan comúnmente en diversas disciplinas científicas para el análisis estadístico. Encontramos que muchos científicos que asisten a los talleres de Software Carpentry utilizan R y quieren aprender más. Nuestros materiales son relevantes ya que proporcionan a los asistentes una base sólida en los fundamentos de R y enseñan las mejores prácticas del cómputo científico: desglose del análisis en módulos, automatización tareas y encapsulamiento. Ten en cuenta que este taller se enfoca en los fundamentos del lenguaje de programación R y no en el análisis estadístico. A lo largo de este taller se utilizan una variedad de paquetes desarrolados por terceros, los cuales no son necesariamente los mejores ni se encuentran explicadas todas sus funcionalidades, pero son paquetes que consideramos útiles y han sido elegidos principalmente por su facilidad de uso.","tags":["Analysis","Data","Education","Gapminder","Open Scholarship Tools and Technologies","R","Reproducibility","Research Data Management Tools","Researchers"],"title":"R para Análisis Científicos Reproducibles","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"2d3c31f135932938090d33e9f114900c","permalink":"https://forrt.org/curated_resources/race-to-the-bottom-competition-and-quali/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/race-to-the-bottom-competition-and-quali/","section":"curated_resources","summary":"This paper investigates how competition to publish first and thereby establish priority impacts the quality of scientific research. We begin by developing a model where scientists decide whether and how long to work on a given project. When deciding how long to let their projects mature, scientists trade off the marginal benefit of higher quality research against the marginal risk of being preempted. The most important (highest potential) projects are the most competitive because they induce the most entry. Therefore, the model predicts these projects are also the most rushed and lowest quality. We test the predictions of this model in the field of structural biology using data from the Protein Data Bank (PDB), a repository for structures of large macromolecules. An important feature of the PDB is that it assigns objective measures of scientific quality to each structure. As suggested by the model, we find that structures with higher ex-ante potential generate more competition, are completed faster, and are lower quality. Consistent with the model, and with a causal interpretation of our empirical results, these relationships are mitigated when we focus on structures deposited by scientists who – by nature of their employment position – are less focused on publication and priority.","tags":["Publish or perish culture","Quality of publication"],"title":"Race to the Bottom: Competition and Quality in Science","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"b0925f85c6e1823fc92e774191efbf52","permalink":"https://forrt.org/curated_resources/raiders-of-the-lost-hark-a-reproducible/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/raiders-of-the-lost-hark-a-reproducible/","section":"curated_resources","summary":"Hypothesizing after the results are known (HARK) has been disparaged as data dredging, and safeguards including hypothesis preregistration and statistically rigorous oversight have been recommended. Despite potential drawbacks, HARK has deepened thinking about complex causal processes. Some of the HARK precautions can conflict with the modern reality of researchers' obligations to use big, â€˜organic' data sourcesâ€”from high-throughput genomics to social media streams. We here propose a HARK-solid, reproducible inference framework suitable for big data, based on models that represent formalization of hypotheses. Reproducibility is attained by employing two levels of model validation: internal (relative to data collated around hypotheses) and external (independent to the hypotheses used to generate data or to the data used to generate hypotheses). With a model-centered paradigm, the reproducibility focus changes from the ability of others to reproduce both data and specific inferences from a study to the ability to evaluate models as representation of reality. Validation underpins â€˜natural selection' in a knowledge base maintained by the scientific community. The community itself is thereby supported to be more productive in generating and critically evaluating theories that integrate wider, complex systems.","tags":["Analysis","Data","HARKing","Reproducibility"],"title":"Raiders of the lost HARK: a reproducible inference framework for big data science","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1689122843,"objectID":"5e10eed1042c3152fed9f3733ea160ee","permalink":"https://forrt.org/curated_resources/raise-standards-for-preclinical-cancer-r/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/raise-standards-for-preclinical-cancer-r/","section":"curated_resources","summary":"C. Glenn Begley and Lee M. Ellis propose how methods, publications and incentives must change if patients are to benefit.","tags":["Reproducibility Crisis and Credibility Revolution","Open Science"],"title":"Raise standards for preclinical cancer research","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1689122843,"objectID":"6ffd867b408bf11771326c5f484aec57","permalink":"https://forrt.org/curated_resources/randomization-does-not-help-much-compara/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/randomization-does-not-help-much-compara/","section":"curated_resources","summary":"According to R.A. Fisher, randomization “relieves the experimenter from the anxiety of considering innumerable causes by which the data may be disturbed.” Since, in particular, it is said to control for known and unknown nuisance factors that may considerably challenge the validity of a result, it has become very popular. This contribution challenges the received view. First, looking for quantitative support, we study a number of straightforward, mathematically simple models. They all demonstrate that the optimism surrounding randomization is questionable: In small to medium-sized samples, random allocation of units to treatments typically yields a considerable imbalance between the groups, i.e., confounding due to randomization is the rule rather than the exception. In the second part of this contribution, the reasoning is extended to a number of traditional arguments in favour of randomization. This discussion is rather non-technical, and sometimes touches on the rather fundamental Frequentist/Bayesian debate. However, the result of this analysis turns out to be quite similar: While the contribution of randomization remains doubtful, comparability contributes much to a compelling conclusion. Summing up, classical experimentation based on sound background theory and the systematic construction of exchangeable groups seems to be advisable","tags":[""],"title":"Randomization Does Not Help Much, Comparability Does","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"4c75684b024639333ffeadfeda6d2632","permalink":"https://forrt.org/curated_resources/rate-and-success-of-study-replication-in/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/rate-and-success-of-study-replication-in/","section":"curated_resources","summary":"The recent replication crisis has caused several scientific disciplines to self-reflect on the frequency with which they replicate previously published studies and to assess their success in such endeavours. The rate of replication, however, has yet to be assessed for ecology and evolution. Here, I survey the open-access ecology and evolution literature to determine how often ecologists and evolutionary biologists replicate, or at least claim to replicate, previously published studies. I found that approximately 0.023% of ecology and evolution studies are described by their authors as replications. Two of the 11 original-replication study pairs provided sufficient statistical detail for three effects so as to permit a formal analysis of replication success. Replicating authors correctly concluded that they replicated an original effect in two cases; in the third case, my analysis suggests that the finding by the replicating authors was consistent with the original finding, contrary the conclusion of “replication failure” by the authors.","tags":["Reproducibility"],"title":"Rate and success of study replication in ecology and evolution","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"b2c322ca4f37aa1513bb0c6d3d57e521","permalink":"https://forrt.org/curated_resources/rationalizing-pre-analysis-plans-statist/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/rationalizing-pre-analysis-plans-statist/","section":"curated_resources","summary":"Pre-analysis plans (PAPs) are a potential remedy to the publication of spurious findings in empirical research, but they have been criticized for their costs and for preventing valid discoveries. In this article, we analyze the costs and benefits of pre-analysis plans by casting pre-commitment in empirical research as a mechanism-design problem. In our model, a decision-maker commits to a decision rule. Then an analyst chooses a PAP, observes data, and reports selected statistics to the decision-maker, who applies the decision rule. With conflicts of interest and private information, not all decision rules are implementable. We provide characterizations of implementable decision rules, where PAPs are optimal when there are many analyst degrees of freedom and high communication costs. These PAPs improve welfare by enlarging the space of implementable decision functions. This stands in contrast to single-agent statistical decision theory, where commitment devices are unnecessary if preferences are consistent across time.","tags":["Implementability","Pre-Analysis Plans","Statistical Decisions","Economics"],"title":"Rationalizing pre-analysis plans: Statistical decisions subject to implementability","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"42a8121e143dfb9fbe1a1e87790f5e22","permalink":"https://forrt.org/curated_resources/re-run-repeat-reproduce-reuse-replicate/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/re-run-repeat-reproduce-reuse-replicate/","section":"curated_resources","summary":"Scientific code is different from production software. Scientific code, by producing results that are then analyzed and interpreted, participates in the elaboration of scientific conclusions. This imposes specific constraints on the code that are often overlooked in practice. We articulate, with a small example, five characteristics that a scientific code in computational science should possess: re-runnable, repeatable, reproducible, reusable and replicable. The code should be executable (re-runnable) and produce the same result more than once (repeatable); it should allow an investigator to reobtain the published results (reproducible) while being easy to use, understand and modify (reusable), and it should act as an available reference for any ambiguity in the algorithmic descriptions of the article (replicable).","tags":["Best Practices","Computational Science","Replicability","Reproducibility of Results","Reproducible Research","Reproducible Science","Software Development"],"title":"Re-run, Repeat, Reproduce, Reuse, Replicate: Transforming Code into Scientific Contributions","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1689122843,"objectID":"7b3f9820ca08cbb86d9a48a90d44a940","permalink":"https://forrt.org/curated_resources/recommendations-for-increasing-replicabi/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/recommendations-for-increasing-replicabi/","section":"curated_resources","summary":"Replicability of findings is at the heart of any empirical science. The aim of this article is to move the current replicability debate in psychology towards concrete recommendations for improvement. We focus on research practices but also offer guidelines for reviewers, editors, journal management, teachers, granting institutions, and university promotion committees, highlighting some of the emerging and existing practical solutions that can facilitate implementation of these recommendations. The challenges for improving replicability in psychological science are systemic. Improvement can occur only if changes are made at many levels of practice, evaluation, and reward. ","tags":[""],"title":"Recommendations for Increasing Replicability in Psychology","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1689122843,"objectID":"9ce4a7eeff46483cae98b34233efaae0","permalink":"https://forrt.org/curated_resources/reconceptualizing-replication-as-a-seque/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/reconceptualizing-replication-as-a-seque/","section":"curated_resources","summary":"In contrast to the truncated view that replications have only a little to offer beyond what is already known, we suggest a broader understanding of replications: We argue that replications are better conceptualized as a process of conducting consecutive studies that increasingly consider alternative explanations, critical contingencies, and real-world relevance. To reflect this understanding, we collected and summarized the existing literature on replications and combined it into a comprehensive overall typology that simplifies and restructures existing approaches. The resulting typology depicts how multiple, hierarchically structured replication studies guide the integration of laboratory and field research and advance theory. It can be applied to (a) evaluate a theory's current status, (b) guide researchers' decisions, (c) analyze and argue for the necessity of certain types of replication studies, and (d) assess the added value of a replication study at a given state of knowledge. We conclude with practical recommendations for different protagonists in the field (e.g., authors, reviewers, editors, and funding agencies). Together, our comprehensive typology and the related recommendations will contribute to an enhanced replication culture in social psychology and to a stronger real-world impact of the discipline.","tags":["Reproducibility Crisis and Credibility Revolution","Open Science"],"title":"Reconceptualizing replication as a sequence of different studies: A replication typology","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1721390377,"objectID":"3704bdf2956838181f844f1d8dfa5f5c","permalink":"https://forrt.org/glossary/english/red_teams/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/english/red_teams/","section":"glossary","summary":"","tags":null,"title":"Red Teams","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1635961284,"objectID":"7a13aa8af2cc5d9e17fb1ab96ef488d7","permalink":"https://forrt.org/glossary/vbeta/red-teams/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/vbeta/red-teams/","section":"glossary","summary":"","tags":null,"title":"Red Teams","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1720865851,"objectID":"6df16470e600f543cb7d59a730f0a692","permalink":"https://forrt.org/glossary/german/red_teams/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/german/red_teams/","section":"glossary","summary":"","tags":null,"title":"Red Teams (Rote Teams)","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1689122843,"objectID":"278eb6a91c9de206d50d03667bdffd85","permalink":"https://forrt.org/curated_resources/redefine-statistical-signifcance/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/redefine-statistical-signifcance/","section":"curated_resources","summary":"We propose to change the default P-value threshold for statistical signifcance from 0.05 to 0.005 for claims of new discoveries.","tags":[""],"title":"Redefine statistical signifcance","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"ef442d6a49ae3aee27b4e3175db63731","permalink":"https://forrt.org/curated_resources/reducing-bias-increasing-transparency-an/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/reducing-bias-increasing-transparency-an/","section":"curated_resources","summary":"Flexibility in the design, analysis and interpretation of scientific studies creates a multiplicity of possible research outcomes. Scientists are granted considerable latitude to selectively use and report the hypotheses, variables and analyses that create the most positive, coherent and attractive story while suppressing those that are negative or inconvenient. This creates a risk of bias that can lead to scientists fooling themselves and fooling others. Preregistration involves declaring a research plan (for example, hypotheses, design and statistical analyses) in a public registry before the research outcomes are known. Preregistration (1) reduces the risk of bias by encouraging outcome-independent decision-making and (2) increases transparency, enabling others to assess the risk of bias and calibrate their confidence in research outcomes. In this Perspective, we briefly review the historical evolution of preregistration in medicine, psychology and other domains, clarify its pragmatic functions, discuss relevant meta-research, and provide recommendations for scientists and journal editors.","tags":["Science","Technology","Society","Preregistration","Scientific Community"],"title":"Reducing bias, increasing transparency and calibrating confidence with preregistration","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1721390377,"objectID":"e2f7dea49bd1338a0e1ee4ae99c94734","permalink":"https://forrt.org/glossary/english/reflexivity/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/english/reflexivity/","section":"glossary","summary":"","tags":null,"title":"Reflexivity","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1635961284,"objectID":"c41bdfe3ff904dcfe04a176e913f3d42","permalink":"https://forrt.org/glossary/vbeta/reflexivity/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/vbeta/reflexivity/","section":"glossary","summary":"","tags":null,"title":"Reflexivity","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1720865851,"objectID":"942560486f26dc44dd66ffa0aca41b6e","permalink":"https://forrt.org/glossary/german/reflexivity/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/german/reflexivity/","section":"glossary","summary":"","tags":null,"title":"Reflexivity (Reflexivität)","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1689122843,"objectID":"c63dc13b462e704fb586b271de084f7c","permalink":"https://forrt.org/curated_resources/registered-replication-report-hart-albar/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/registered-replication-report-hart-albar/","section":"curated_resources","summary":"Language can be viewed as a complex set of cues that shape people’s mental representations of situations. For example, people think of behavior described using imperfective aspect (i.e., what a person was doing) as a dynamic, unfolding sequence of actions, whereas the same behavior described using perfective aspect (i.e., what a person did) is perceived as a completed whole. A recent study found that aspect can also influence how we think about a person’s intentions (Hart \u0026 Albarracín, 2011). Participants judged actions described in imperfective as being more intentional (d between 0.67 and 0.77) and they imagined these actions in more detail (d = 0.73). The fact that this finding has implications for legal decision making, coupled with the absence of other direct replication attempts, motivated this registered replication report (RRR). Multiple laboratories carried out 12 direct replication studies, including one MTurk study. A meta-analysis of these studies provides a precise estimate of the size of this effect free from publication bias. This RRR did not find that grammatical aspect affects intentionality (d between 0 and −0.24) or imagery (d = −0.08). We discuss possible explanations for the discrepancy between these results and those of the original study.","tags":["Transparency","Open Science","Registered Reports"],"title":"Registered replication report: Hart \u0026 Albarracín (2011).","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1689122843,"objectID":"0f3a6a09b03ada174e3226ae398d1d30","permalink":"https://forrt.org/curated_resources/registered-replication-report-rand-green/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/registered-replication-report-rand-green/","section":"curated_resources","summary":"In an anonymous 4-person economic game, participants contributed more money to a common project (i.e., cooperated) when required to decide quickly than when forced to delay their decision (Rand, Greene \u0026 Nowak, 2012), a pattern consistent with the social heuristics hypothesis proposed by Rand and colleagues. The results of studies using time pressure have been mixed, with some replication attempts observing similar patterns (e.g., Rand et al., 2014) and others observing null effects (e.g., Tinghög et al., 2013; Verkoeijen \u0026 Bouwmeester, 2014). This Registered Replication Report (RRR) assessed the size and variability of the effect of time pressure on cooperative decisions by combining 21 separate, preregistered replications of the critical conditions from Study 7 of the original article (Rand et al., 2012). The primary planned analysis used data from all participants who were randomly assigned to conditions and who met the protocol inclusion criteria (an intent-to-treat approach that included the 65.9% of participants in the time-pressure condition and 7.5% in the forced-delay condition who did not adhere to the time constraints), and we observed a difference in contributions of -0.37 percentage points compared with an 8.6 percentage point difference calculated from the original data. Analyzing the data as the original article did, including data only for participants who complied with the time constraints, the RRR observed a 10.37 percentage point difference in contributions compared with a 15.31 percentage point difference in the original study. In combination, the results of the intent-to-treat analysis and the compliant-only analysis are consistent with the presence of selection biases and the absence of a causal effect of time pressure on cooperation.","tags":["Reproducibility Crisis and Credibility Revolution","Open Science"],"title":"Registered Replication Report: Rand, Greene, and Nowak (2012). ","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1689122843,"objectID":"948dce6bd75193df17c69aca91f91e49","permalink":"https://forrt.org/curated_resources/registered-replication-report-schooler-a/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/registered-replication-report-schooler-a/","section":"curated_resources","summary":"Trying to remember something now typically improves your ability to remember it later. However, after watching a video of a simulated bank robbery, participants who verbally described the robber were 25% worse at identifying the robber in a lineup than were participants who instead listed U.S. states and capitals—this has been termed the “verbal overshadowing” effect (Schooler \u0026 Engstler-Schooler, 1990). More recent studies suggested that this effect might be substantially smaller than first reported. Given uncertainty about the effect size, the influence of this finding in the memory literature, and its practical importance for police procedures, we conducted two collections of preregistered direct replications (RRR1 and RRR2) that differed only in the order of the description task and a filler task. In RRR1, when the description task immediately followed the robbery, participants who provided a description were 4% less likely to select the robber than were those in the control condition. In RRR2, when the description was delayed by 20 min, they were 16% less likely to select the robber. These findings reveal a robust verbal overshadowing effect that is strongly influenced by the relative timing of the tasks. The discussion considers further implications of these replications for our understanding of verbal overshadowing.","tags":["Transparency","Open Science","Registered Reports"],"title":"Registered replication report: Schooler and engstler-schooler (1990). ","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1689122843,"objectID":"519f090b0bbd8adca43a85014f4d51cf","permalink":"https://forrt.org/curated_resources/registered-replication-report-strack-mar/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/registered-replication-report-strack-mar/","section":"curated_resources","summary":"According to the facial feedback hypothesis, people’s affective responses can be influenced by their own facial expression (e.g., smiling, pouting), even when their expression did not result from their emotional experiences. For example, Strack, Martin, and Stepper (1988) instructed participants to rate the funniness of cartoons using a pen that they held in their mouth. In line with the facial feedback hypothesis, when participants held the pen with their teeth (inducing a “smile”), they rated the cartoons as funnier than when they held the pen with their lips (inducing a “pout”). This seminal study of the facial feedback hypothesis has not been replicated directly. This Registered Replication Report describes the results of 17 independent direct replications of Study 1 from Strack et al. (1988), all of which followed the same vetted protocol. A meta-analysis of these studies examined the difference in funniness ratings between the “smile” and “pout” conditions. The original Strack et al. (1988) study reported a rating difference of 0.82 units on a 10-point Likert scale. Our meta-analysis revealed a rating difference of 0.03 units with a 95% confidence interval ranging from −0.11 to 0.16","tags":["Reproducibility Crisis and Credibility Revolution","Open Science"],"title":"Registered Replication Report: Strack, Martin, \u0026 Stepper (1988)","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1689122843,"objectID":"f7138b134ee269094db091a79ebe6c2f","permalink":"https://forrt.org/curated_resources/registered-replication-report-study-1-fr/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/registered-replication-report-study-1-fr/","section":"curated_resources","summary":"Finkel, Rusbult, Kumashiro, and Hannon (2002, Study 1) demonstrated a causal link between subjective commitment to a relationship and how people responded to hypothetical betrayals of that relationship. Participants primed to think about their commitment to their partner (high commitment) reacted to the betrayals with reduced exit and neglect responses relative to those primed to think about their independence from their partner (low commitment). The priming manipulation did not affect constructive voice and loyalty responses. Although other studies have demonstrated a correlation between subjective commitment and responses to betrayal, this study provides the only experimental evidence that inducing changes to subjective commitment can causally affect forgiveness responses. This Registered Replication Report (RRR) meta-analytically combines the results of 16 new direct replications of the original study, all of which followed a standardized, vetted, and preregistered protocol. The results showed little effect of the priming manipulation on the forgiveness outcome measures, but it also did not observe an effect of priming on subjective commitment, so the manipulation did not work as it had in the original study. We discuss possible explanations for the discrepancy between the findings from this RRR and the original study.","tags":["Reproducibility Crisis and Credibility Revolution","Open Science"],"title":"Registered Replication Report: Study 1 From Finkel, Rusbult, Kumashiro, \u0026 Hannon (2002).","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1721390377,"objectID":"ca0292e0b05ed5724bf299adabbfebec","permalink":"https://forrt.org/glossary/english/registered_report/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/english/registered_report/","section":"glossary","summary":"","tags":null,"title":"Registered Report","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1720865851,"objectID":"2e9f3264e108b696a3408415ac5ac555","permalink":"https://forrt.org/glossary/german/registered_report/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/german/registered_report/","section":"glossary","summary":"","tags":null,"title":"Registered Report","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1635961284,"objectID":"13b8ece246ccf94c999e138eb8e5bc9d","permalink":"https://forrt.org/glossary/vbeta/registered-report/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/vbeta/registered-report/","section":"glossary","summary":"","tags":null,"title":"Registered Report","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"1e396efbc3ace0e021ee09f4b0ba7bcb","permalink":"https://forrt.org/curated_resources/registered-report-protocol-survey-on-att/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/registered-report-protocol-survey-on-att/","section":"curated_resources","summary":"Background\nPreregistration, the open science practice of specifying and registering details of a planned study prior to knowing the data, increases the transparency and reproducibility of research. Large-scale replication attempts for psychological results yielded shockingly low success rates and contributed to an increasing demand for open science practices among psychologists. However, preregistering one’s studies is still not the norm in the field. Here, we propose a study to explore possible reasons for this discrepancy.\n\nMethods\nIn a mixed-methods approach, an online survey will be conducted, assessing attitudes, motivations, and perceived obstacles with respect to preregistration. Participants will be psychological researchers that will be recruited by scanning research articles on Web of Science, PubMed, PSYNDEX, and PsycInfo, and preregistrations on OSF Registries (targeted sample size: N = 296). Based on the theory of planned behavior, we predict that positive attitudes (moderated by the perceived importance of preregistration) as well as a favorable subjective norm and higher perceived behavioral control positively influence researchers’ intention to preregister (hypothesis 1). Furthermore, we expect an influence of research experience on attitudes and perceived motivations and obstacles regarding preregistration (hypothesis 2). We will analyze these hypotheses with multiple regression models, and will include preregistration experience as control variable.","tags":["Psychological Attitudes","Psychology","Behavior","Surveys","Open Science","Motivation","Pilot Studies","Psychometrics"],"title":"Registered Report Protocol: Survey on attitudes and experiences regarding preregistration in psychological research","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"82a0a6b71a5759b6b36ff13bb56eceb5","permalink":"https://forrt.org/curated_resources/registered-reports-factsheet-for-editors/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/registered-reports-factsheet-for-editors/","section":"curated_resources","summary":"Registered Reports Factsheet for Editors","tags":["Research"],"title":"Registered Reports Factsheet for Editors","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"8769e029995908a3685f7c18d0426668","permalink":"https://forrt.org/curated_resources/registered-reports-faq/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/registered-reports-faq/","section":"curated_resources","summary":"Registered Reports FAQ","tags":["Research"],"title":"Registered Reports FAQ","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"2865173682096651a5e952a8e537599e","permalink":"https://forrt.org/curated_resources/registered-reports-q-a/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/registered-reports-q-a/","section":"curated_resources","summary":"This webinar addresses questions related to writing, reviewing, editing, or funding a study using the Registered Report format, featuring Chris Chambers and ...","tags":["Open Science","Peer Review","Publishing","Registered Reports","Research","Research Best Practices"],"title":"Registered Reports Q\u0026A","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1689122843,"objectID":"275cbb462099073989fbb57b77855c26","permalink":"https://forrt.org/curated_resources/registered-reports-a-method-to-increase/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/registered-reports-a-method-to-increase/","section":"curated_resources","summary":"Registered reports a Method to Increase the Credibility of Published Results","tags":["Reproducibility Crisis and Credibility Revolution","Open Science"],"title":"Registered reports: A method to increase the credibility of published results","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1689122843,"objectID":"33d572671d464d529b4005d9590dd039","permalink":"https://forrt.org/curated_resources/registered-reports-a-new-publishing-init/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/registered-reports-a-new-publishing-init/","section":"curated_resources","summary":"An article about registered Reports: A new publishing initiative at Cortex","tags":["Reproducibility Crisis and Credibility Revolution","Open Science"],"title":"Registered Reports: A New Publishing Initiative at Cortex","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"3f8d200ecf8be66efef70f2b73381050","permalink":"https://forrt.org/curated_resources/registered-reports-an-early-example-and/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/registered-reports-an-early-example-and/","section":"curated_resources","summary":"The recent ‘replication crisis’ in psychology has focused attention on ways of increasing methodological rigor within the behavioral sciences. Part of this work has involved promoting ‘Registered Reports’, wherein journals peer review papers prior to data collection and publication. Although this approach is usually seen as a relatively recent development, we note that a prototype of this publishing model was initiated in the mid-1970s by parapsychologist Martin Johnson in the European Journal of Parapsychology (EJP). A retrospective and observational comparison of Registered and non-Registered Reports published in the EJP during a seventeen-year period provides circumstantial evidence to suggest that the approach helped to reduce questionable research practices. This paper aims both to bring Johnson’s pioneering work to a wider audience, and to investigate the positive role that Registered Reports may play in helping to promote higher methodological and statistical standards.","tags":["Data","Preregistation","Publishing","Registered Reports"],"title":"Registered reports: an early example and analysis","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1721390377,"objectID":"91df7d081bb5e1f447696d89e2e6b708","permalink":"https://forrt.org/glossary/english/registry_of_research_data_repositories/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/english/registry_of_research_data_repositories/","section":"glossary","summary":"","tags":null,"title":"Registry of Research Data Repositories","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1720865851,"objectID":"cf6b3b9ebc13ccd56a16d645d5fee9ff","permalink":"https://forrt.org/glossary/german/registry_of_research_data_repositories/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/german/registry_of_research_data_repositories/","section":"glossary","summary":"","tags":null,"title":"Registry of Research Data Repositories","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1635961284,"objectID":"b8c0c6c3fdd749a4f9d30811275f34ff","permalink":"https://forrt.org/glossary/vbeta/registry-of-research-data-repositor/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/vbeta/registry-of-research-data-repositor/","section":"glossary","summary":"","tags":null,"title":"Registry of Research Data Repositories","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"1d735d7f9dadd027fdff474055a1f25a","permalink":"https://forrt.org/curated_resources/reimagining-peer-review-the-emergence-of/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/reimagining-peer-review-the-emergence-of/","section":"curated_resources","summary":"The traditional peer review process in medicine faces a crisis marked by publication delays, potential bias, and a lack of transparency. These issues hinder scientific progress and undermine the credibility of published medical findings, which inform healthcare practices and clinical decision-making. In response, innovative models like the Peer Community In (PCI) Research Reports (RR) have emerged, providing a rapid, transparent, and collaborative evaluation process for scientific research. Peer Community In Research Reports aims to address the issues within the traditional peer review system by focusing on preprints and fostering trust and credibility in published research. The success of the PCI RR model depends on the active engagement and support of researchers, publishers, and other stakeholders. Ideally, researchers should submit their work to PCI RR, while publishers should be open to embracing innovative peer review models. Policymakers and funding agencies should promote the adoption of open science principles and practices at a systemic level. In conclusion, addressing the crisis in the peer review process requires the collective efforts of the entire scientific community. By embracing and supporting innovative alternatives like the PCI RR model, stakeholders can help establish a more efficient and credible peer review system, ultimately benefiting scientific progress and patient care.","tags":["Medical Publishing","Peer Community In Research Reports","Peer Review","Referees","Publication"],"title":"Reimagining peer review: the emergence of peer community in registered reports system","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"6a1d99902a0499cf32393ea5139fb376","permalink":"https://forrt.org/curated_resources/releasing-a-preprint-is-associated-with/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/releasing-a-preprint-is-associated-with/","section":"curated_resources","summary":"Preprints in biology are becoming more popular, but only a small fraction of the articles published in peer-reviewed journals have previously been released as preprints. To examine whether releasing a preprint on bioRxiv was associated with the attention and citations received by the corresponding peer-reviewed article, we assembled a dataset of 74,239 articles, 5,405 of which had a preprint, published in 39 journals. Using log-linear regression and random-effects meta-analysis, we found that articles with a preprint had, on average, a 49% higher Altmetric Attention Score and 36% more citations than articles without a preprint. These associations were independent of several other article- and author-level variables (such as scientific subfield and number of authors), and were unrelated to journal-level variables such as access model and Impact Factor. This observational study can help researchers and publishers make informed decisions about how to incorporate preprints into their work.","tags":["Preprints","Publishing"],"title":"Releasing a preprint is associated with more attention and citations for the peer-reviewed article","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1721390377,"objectID":"145a67f52e2a60c6ecd18d6a032c8303","permalink":"https://forrt.org/glossary/english/reliability/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/english/reliability/","section":"glossary","summary":"","tags":null,"title":"Reliability","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1635961284,"objectID":"489bf5dccbed1ca8ca52659966c87fd1","permalink":"https://forrt.org/glossary/vbeta/reliability/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/vbeta/reliability/","section":"glossary","summary":"","tags":null,"title":"Reliability","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1720865851,"objectID":"00b76b266bee0b2e084a7b95f7ae7c72","permalink":"https://forrt.org/glossary/german/reliability/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/german/reliability/","section":"glossary","summary":"","tags":null,"title":"Reliability (Reliabilität)","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1721390377,"objectID":"41b7f5667c2fb3f41765dd08844b0f09","permalink":"https://forrt.org/glossary/english/repeatability/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/english/repeatability/","section":"glossary","summary":"","tags":null,"title":"Repeatability","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1635961284,"objectID":"295efe37b9a7e958f2ec0ba79e9ceaa3","permalink":"https://forrt.org/glossary/vbeta/repeatability/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/vbeta/repeatability/","section":"glossary","summary":"","tags":null,"title":"Repeatability","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1720865851,"objectID":"c1c28ca01b88611544636d03b7d955a0","permalink":"https://forrt.org/glossary/german/repeatability/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/german/repeatability/","section":"glossary","summary":"","tags":null,"title":"Repeatability (Wiederholbarkeit)","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1689122843,"objectID":"dcbfd0a63e1b778ed075f590cae92b57","permalink":"https://forrt.org/curated_resources/replacing-p-values-with-bayes-factors-a/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/replacing-p-values-with-bayes-factors-a/","section":"curated_resources","summary":"A blog post about choosing Bayes Factor over p-value","tags":["Blog"],"title":"Replacing p-values with Bayes-Factors: A Miracle Cure for the Replicability Crisis in Psychological Science","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1721390377,"objectID":"4d50bfaaca4c8c64d82d0f530fc0fd26","permalink":"https://forrt.org/glossary/english/replicability/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/english/replicability/","section":"glossary","summary":"","tags":null,"title":"Replicability","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1635961284,"objectID":"6d6bd1bf453e3f1b35d0d88f47828edf","permalink":"https://forrt.org/glossary/vbeta/replicability/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/vbeta/replicability/","section":"glossary","summary":"","tags":null,"title":"Replicability","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1720865851,"objectID":"58ac9e4527c3042431428fff5583912d","permalink":"https://forrt.org/glossary/german/replicability/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/german/replicability/","section":"glossary","summary":"","tags":null,"title":"Replicability (Replikabilität)","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"c75c2fb1116402d122788cbffa02f878","permalink":"https://forrt.org/curated_resources/replicability-and-reproducibility/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/replicability-and-reproducibility/","section":"curated_resources","summary":"A video about replicability and reproducibility debate","tags":["Video","Replication Research"],"title":"Replicability and reproducibility","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"83360e2aa97475f8e4e2f0ebb807874c","permalink":"https://forrt.org/curated_resources/replicability-and-reproducibility-in-com/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/replicability-and-reproducibility-in-com/","section":"curated_resources","summary":"Replicability and Reproducibility in Comparative Psychology Psychology faces a replication crisis. The Reproducibility Project: Psychology sought to replicate the effects of 100 psychology studies. Though 97% of the original studies produced statistically significant results, only 36% of the replication studies did so (Open Science Collaboration, 2015). This inability to replicate previously published results, however, is not limited to psychology (Ioannidis, 2005). Replication projects in medicine (Prinz et al., 2011) and behavioral economics (Camerer et al., 2016) resulted in replication rates of 25 and 61%, respectively, and analyses in genetics (Munafò, 2009) and neuroscience (Button et al., 2013) question the validity of studies in those fields. Science, in general, is reckoning with challenges in one of its basic tenets: replication. Comparative psychology also faces the grand challenge of producing replicable research. Though social psychology has born the brunt of most of the critique regarding failed replications, comparative psychology suffers from some of the same problems faced by social psychology (e.g., small sample sizes). Yet, comparative psychology follows the methods of cognitive psychology by often using within-subjects designs, which may buffer it from replicability problems (Open Science Collaboration, 2015). In this Grand Challenge article, I explore the shared and unique challenges of and potential solutions for replication and reproducibility in comparative psychology.","tags":["Animal Research","Comparative Psychology","Pre-registration","Replication","Reproducibility","Reproducible Research"],"title":"Replicability and Reproducibility in Comparative Psychology","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"08a59f8f07db3fe04a8f0a7e5a213754","permalink":"https://forrt.org/curated_resources/replicability-syllabus-2018/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/replicability-syllabus-2018/","section":"curated_resources","summary":"A syllabus about replicability seminar","tags":["Reproducibility Crisis and Credibility Revolution","Open Science"],"title":"Replicability syllabus (2018)","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"25549b6ee4c68f17d1f498022f347b0b","permalink":"https://forrt.org/curated_resources/replicability-syllabus-2020/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/replicability-syllabus-2020/","section":"curated_resources","summary":"This course will examine current controversies and new developments in research methods in psychology. The goal of the course is to learn to think critically about how psychological science is conducted and how conclusions are drawn. We will cover both methodological and statistical issues that affect the validity of research in psychology, with an emphasis on social and personality psychology. We will discuss the research process from designing a study to how a study gets published. We will also discuss the recent controversy in psychology about the replicability of scientific results. This course is most suited for students who plan to pursue graduate school in psychology and are preparing for a career conducting research in psychological science.","tags":["Reproducibility Crisis and Credibility Revolution","Open Science"],"title":"Replicability Syllabus (2020)","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1689122843,"objectID":"eb6432dbc4f675e3259fafb503473478","permalink":"https://forrt.org/curated_resources/replicating-studies-in-which-samples-of/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/replicating-studies-in-which-samples-of/","section":"curated_resources","summary":"In a direct replication, the typical goal is to reproduce a prior experimental result with a new but comparable sample of participants in a high-powered replication study. Often in psychology, the research to be replicated involves a sample of participants responding to a sample of stimuli. In replicating such studies, we argue that the same criteria should be used in sampling stimuli as are used in sampling participants. Namely, a new but comparable sample of stimuli should be used to ensure that the original results are not due to idiosyncrasies of the original stimulus sample, and the stimulus sample must often be enlarged to ensure high statistical power. In support of the latter point, we discuss the fact that in experiments involving samples of stimuli, statistical power typically does not approach 1 as the number of participants goes to infinity. As an example of the importance of sampling new stimuli, we discuss the bygone literature on the risky shift phenomenon, which was almost entirely based on a single stimulus sample that was later discovered to be highly unrepresentative. We discuss the use of both resampled and expanded stimulus sets, that is, stimulus samples that include the original stimuli plus new stimuli.","tags":["Reproducibility Crisis and Credibility Revolution"],"title":"Replicating Studies in Which Samples of Participants Respond to Samples of Stimuli","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"3266e419506f7b0ba4623c5682544e2b","permalink":"https://forrt.org/curated_resources/replication-and-bias-in-special-educatio/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/replication-and-bias-in-special-educatio/","section":"curated_resources","summary":"This is a clearinghouse for resources related to open science in Special Education. If you find a good resource that has not been included, please email it to marcy@cos.io.\n","tags":["Open Practices","Replication","Research","Special Education"],"title":"Replication and Bias in (Special) Education Research Base","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1689122843,"objectID":"b6b90f292c44d0677c2fe0a7037a7a90","permalink":"https://forrt.org/curated_resources/replication-and-open-science-for-undergr/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/replication-and-open-science-for-undergr/","section":"curated_resources","summary":"Blog post going over the replication crisis and how it has led to the open science movement.","tags":["Blog","Open Science","Reproducibility Crisis and Credibility Revolution"],"title":"Replication and Open Science for Undergraduates","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1689122843,"objectID":"db06e4ac8604f97cc4808efc1f1761a4","permalink":"https://forrt.org/curated_resources/replication-and-robustness-in-developmen/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/replication-and-robustness-in-developmen/","section":"curated_resources","summary":"Replications and robustness checks are key elements of the scientific method and a staple in many disciplines. However, leading journals in developmental psychology rarely include explicit replications of prior research conducted by different investigators, and few require authors to establish in their articles or online appendices that their key results are robust across estimation methods, data sets, and demographic subgroups. This article makes the case for prioritizing both explicit replications and, especially, within-study robustness checks in developmental psychology. It provides evidence on variation in effect sizes in developmental studies and documents strikingly different replication and robustness-checking practices in a sample of journals in developmental psychology and a sister behavioral science-applied economics. Our goal is not to show that any one behavioral science has a monopoly on best practices, but rather to show how journals from a related discipline address vital concerns of replication and generalizability shared by all social and behavioral sciences. We provide recommendations for promoting graduate training in replication and robustness-checking methods and for editorial policies that encourage these practices. Although some of our recommendations may shift the form and substance of developmental research articles, we argue that they would generate considerable scientific benefits for the field.","tags":["Reproducibility Crisis and Credibility Revolution","Open Science"],"title":"Replication and Robustness in Developmental Research","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1689122843,"objectID":"58820f0686b94ab25d7928cbf4cc2df2","permalink":"https://forrt.org/curated_resources/replication-and-the-manufacture-of-scien/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/replication-and-the-manufacture-of-scien/","section":"curated_resources","summary":"The field of replication studies remains a controversial, misunderstood, and unappreciated piñata of 18 replication typologies spanning 79 replication types. To help bring order to the chaos, I contribute a theory of manufactured inferences. The theory is built on three pillars: (1) replication causal diagrams (or r-dags for short), (2) a formal conceptualization of study procedures, and (3) the use of Bayesian inference to update our beliefs about the natural phenomenon under investigation and the operating characteristics of the study procedures used to study it. I use this theory to motivate a formal typology of replication types, explaining how they are done and for what purpose. Finally, I discuss some implications of this theory, including the importance of an analytical approach to robustness and generalizability replications, the need to avoid conceptual replications, the possibility of legitimate (unplanned) specification searches, the limitations of meta-analysis, and the false dichotomy between so-called successful and failed replications.","tags":["Reproducibility Crisis and Credibility Revolution","Open Science"],"title":"Replication and the Manufacture of Scientific Inferences: A Formal Approach","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1689122843,"objectID":"d41563d9acf302590f6728d605d44fa3","permalink":"https://forrt.org/curated_resources/replication-in-psychology-a-historical-p/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/replication-in-psychology-a-historical-p/","section":"curated_resources","summary":"The reproducibility of psychological findings has generated much discussion of late. However, the question of replication is not a new one for psychologists. Psychologist have long debated how best to measure their phenomena, how to design their studies, how best to interpret their data, and whether their findings have a broader relevance.","tags":["Blog","Reproducibility Knowledge"],"title":"Replication in Psychology: A historical perspective","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"eb67517d20a0b7d075ec7829071dbbb5","permalink":"https://forrt.org/curated_resources/replication-is-relevant-in-qualitative-r/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/replication-is-relevant-in-qualitative-r/","section":"curated_resources","summary":"Replication has received increasing attention over the last decade. This comes on the heels of prominent instances of data fabrication (National Academies of Sciences, Engineering, and Medicine, 2017) and estimates that few studies attempt to replicate previous findings (Makel \u0026 Plucker, 2014). Replication has been called the Supreme Court of science (Collins, 1985), as well as a basic building block of scholarship. One persistent question in informal conversations that we have not seen addressed in formal writing, is replication’s relevance to qualitative research. Qualitative research is\" a situated activity that locates the observer in the world\" and\" consists of a set of interpretive, material practices that make the world visible\"(Denzin \u0026 Lincoln, 2011, p. 3). Some have argued that replication “missed the point” of qualitative research (Pratt et al., 2020, p. 3). However, in a survey of nearly 1,500 recently published education researchers, less than 10% of qualitative researchers reported that replication should never be used (Makel et al., 2021). Given the prevalence of qualitative research in education, it is important to examine replication’s relevance. In this commentary, we argue that replication is relevant to the qualitative lens in at least three ways. First, replication supports the established values in qualitative research of transparency and intentionality. Second, replication can be used to assess the well-established tradition of transferability. Third, replication can evaluate connections between reflexivity, as evidenced by positionality statements, and qualitative research findings.","tags":["Replication","Qualitative Research"],"title":"Replication is Relevant in Qualitative Research","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1721390377,"objectID":"8f08e35183ce19559cbcc879fac13890","permalink":"https://forrt.org/glossary/english/replication_markets/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/english/replication_markets/","section":"glossary","summary":"","tags":null,"title":"Replication Markets","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1635961284,"objectID":"a22558f4def6c7565bdd54aca3026d65","permalink":"https://forrt.org/glossary/vbeta/replication-markets/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/vbeta/replication-markets/","section":"glossary","summary":"","tags":null,"title":"Replication Markets ","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1720865851,"objectID":"23f77487e130a585645eb022142c1558","permalink":"https://forrt.org/glossary/german/replication_markets/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/german/replication_markets/","section":"glossary","summary":"","tags":null,"title":"Replication Markets (Replikationsmärkte)","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1689122843,"objectID":"9cf9c1070e855c02694b137788cf4e75","permalink":"https://forrt.org/curated_resources/replication-communication-and-the-popula/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/replication-communication-and-the-popula/","section":"curated_resources","summary":"Many published research results are false (Ioannidis, 2005), and controversy continues over the roles of replication and publication policy in improving the reliability of research. Addressing these problems is frustrated by the lack of a formal framework that jointly represents hypothesis formation, replication, publication bias, and variation in research quality. We develop a mathematical model of scientific discovery that combines all of these elements. This model provides both a dynamic model of research as well as a formal framework for reasoning about the normative structure of science. We show that replication may serve as a ratchet that gradually separates true hypotheses from false, but the same factors that make initial findings unreliable also make replications unreliable. The most important factors in improving the reliability of research are the rate of false positives and the base rate of true hypotheses, and we offer suggestions for addressing each. Our results also bring clarity to verbal debates about the communication of research. Surprisingly, publication bias is not always an obstacle, but instead may have positive impacts—suppression of negative novel findings is often beneficial. We also find that communication of negative replications may aid true discovery even when attempts to replicate have diminished power. The model speaks constructively to ongoing debates about the design and conduct of science, focusing analysis and discussion on precise, internally consistent models, as well as highlighting the importance of population dynamics","tags":["Reproducibility Crisis and Credibility Revolution","Open Science"],"title":"Replication, Communication, and the Population Dynamics of Scientific Discovery","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1689122843,"objectID":"f5ee7601b453bdd30fd8ff136432c3fb","permalink":"https://forrt.org/curated_resources/replications-in-psychology-research-how/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/replications-in-psychology-research-how/","section":"curated_resources","summary":"Recent controversies in psychology have spurred conversations about the nature and quality of psychological research. One topic receiving substantial attention is the role of replication in psychological science. Using the complete publication history of the 100 psychology journals with the highest 5-year impact factors, the current article provides an overview of replications in psychological research since 1900. This investigation revealed that roughly 1.6% of all psychology publications used the term replication in text. A more thorough analysis of 500 randomly selected articles revealed that only 68% of articles using the term replication were actual replications, resulting in an overall replication rate of 1.07%. Contrary to previous findings in other fields, this study found that the majority of replications in psychology journals reported similar findings to their original studies (i.e., they were successful replications). However, replications were significantly less likely to be successful when there was no overlap in authorship between the original and replicating articles. Moreover, despite numerous systemic biases, the rate at which replications are being published has increased in recent decades.","tags":["Reproducibility Crisis and Credibility Revolution","Open Science"],"title":"Replications in Psychology Research: How Often Do They Really Occur?","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1721390377,"objectID":"ef8947ed068204ec82b48947c550d336","permalink":"https://forrt.org/glossary/english/replicats_project/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/english/replicats_project/","section":"glossary","summary":"","tags":null,"title":"RepliCATs project","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1720865851,"objectID":"3eedde561aef625da7c521f1c09a54f7","permalink":"https://forrt.org/glossary/german/replicats_project/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/german/replicats_project/","section":"glossary","summary":"","tags":null,"title":"RepliCATs project","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1635961284,"objectID":"74ae0faed4cd283d85eb2b9f76a03d0e","permalink":"https://forrt.org/glossary/vbeta/replicats-project/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/vbeta/replicats-project/","section":"glossary","summary":"","tags":null,"title":"RepliCATs project","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"008581c1476f86e6d06446e71e5fc5d0","permalink":"https://forrt.org/curated_resources/repligate-reliability-and-reproducibilit/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/repligate-reliability-and-reproducibilit/","section":"curated_resources","summary":"This\tseminar\twill\taddress\tissues\trelevant\tto\tthe\tcurrent\tcontroversy\tover the\treliability\tof\tpsychological\tresearch,\ta\tcontroversy\twhich\tseems\tto\tbe\twhite-hot\tright\tnow. Topics\twill\tinclude\t(but\tnot\tbe\tlimited\tto) \n- Critiques\tof\tthe\tcurrent\tscience,\tincluding\tthe\tclaim\tthat\t\"most\tpublished\tresearch findings\tare\tfalse\"\n- Controversies\tover\tthe\treplicability\tof\tparticular\tfindings,\tincluding\tbehavioral\tpriming\tand\tESP\n- The\tway\tpractices\tby\tjournal\teditors,\tgranting\tagencies,\tand\thiring\tcommittees\tdo\tand\tdo\tnot\t encourage\treliable,\treplicable\tresearch\n- Recommendations\tfor\timproving\tconduct\tand\treporting\tof\tresearch,\tincluding\tstatements\tby\t professional\tsocieties,\tjournals,\tand\tgovernment\tagencies\n- Related\tmethodological\tissues\tincluding\no Null-hypothesis\tstatistical\ttesting\noThe\t\"new\tstatistics\"\temphasizing\teffect\tsizes\tand\tconfidence\tintervals\no Exploratory\tvs.\tconfirmatory\tresearch\no p-curving,\tthe\ttest\tfor\texcess\tsignificance,\tand\tother\tstatistical\ttools\tintended\tto\tdetect\tquestionable research\n- Defenses\tof\tthe\tcurrent\tstate\tof\tpsychological\tresearch,\tand\tthe\tvarious\tkinds\tof\tpush\tback\tagainst\twhat\tsome\tcall\tthe\t\"anti-false-positives\tmovement\"\tand\tothers\tsimply\tcall\t\"shameless\tlittle\tbullies.\" Readings\twill\tinclude\tjournal\tarticles,\teditorials,\tblog\tposts\tand\tprobably\teven\ta\ttweet\tor\ttwo. While a\t lengthy\treading\tlist\tis provided,\twe\twill\tbe\tselective\tin\twhat\twe\tactually\tread\tin\tdepth;\tskimming\twill\tbe\t encouraged\twhen\tappropriate. The\tcourse\tstructure\twill\tbe\tdiscussion\tof\tthe\treadings. That's\tall. No\t exams\tor\tpapers. But\tdo\tcome\tto\tclass\tready\tto\ttalk.","tags":["Reproducibility Crisis and Credibility Revolution","Open Science"],"title":"Repligate: Reliability and Reproducibility in Psychology Syllabus","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"5c72fa8985169c3c46a752f32f571a45","permalink":"https://forrt.org/curated_resources/reply-to-ledgerwood-predictions-without/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/reply-to-ledgerwood-predictions-without/","section":"curated_resources","summary":"Ledgerwood (1) argues that there are two independent uses of preregistration that are conflated in Nosek et al. (2) and elsewhere: “Preregistering theoretical predictions enables theory falsifiability. Preregistering analysis plans enables type I error control.” We appreciate that the comment elevates the complementary roles of prediction and analysis plans in preregistration. We disagree that they are conflated in the sense of being “two types of preregistration.” To enable theory falsification, we agree that a preregistration should offer a prediction derived from theory and provide the theoretical context. However, a prediction without an analysis plan is inert for falsification. An analysis plan is necessary to specify how the prediction will be tested with the observed data. So, the position that prediction and analysis plans are conflated is misleading—theory testing requires both. Here is a somewhat different way to describe the distinct roles of prediction and analysis plans in preregistration to elaborate on Ledgerwood’s (1) points. Researcher 1 preregisters that they will conduct a two-tailed t test with alpha = 0.05 to evaluate whether subjects randomly assigned to condition A versus condition B will differ on a single outcome. Researcher 1 does not preregister a directional prediction. Researcher 2 observes the study design and analysis plan and preregisters a prediction of A \u003e B based on their theoretical framework. Researcher 3 does the same and predicts B \u003e A. The observed outcome is P = 0.01 for B \u003e A, opposite of researcher 2’s prediction and consistent with researcher 3’s. Is this an exploratory result because researcher 1 did not make a directional prediction? Is it a theoretical falsification because researcher 2’s prediction was not supported? Is it a theoretical confirmation because researcher 3’s prediction was supporting? “Yes” and “no” are defensible assertions for all three questions. The statistical outcomes do not know what the researchers predicted. The P value is interpretable the same way for all researchers (Ledgerwood’s error control). This might imply that all three researchers should now believe that B \u003e A to the same degree based on the statistical evidence. Not so. Predictions are an informal way that prior beliefs are incorporated into null hypothesis significance testing. Falsification is rarely a discrete event, nor is it a consensus event. All three researchers should be responsive to the new evidence, but the preexisting beliefs supporting their predictions will shape how much their beliefs change with the evidence. This updating is not quantified directly by the P value. Preregistering predictions is also useful for the social aspect of scientific communication. In basketball, hitting a bank shot is much more impressive if it is called before shooting. Otherwise, people presume that it was luck. Researcher 3’s theoretical perspective for B \u003e A may gain greater credibility because of its successful prediction compared with generating an explanation after the fact. This may invite greater empirical scrutiny to assess whether the theoretical perspective survives more prediction scenarios. Echoing Ledgerwood (1), preregistration without predictions is fine for exploration. However, all preregistered predictions need analysis plans specifying how the prediction will be evaluated when the outcomes are observed.","tags":["Preregistration","Reply","Prediction","Analyses"],"title":"Reply to Ledgerwood: Predictions without analysis plans are inert","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1689122843,"objectID":"a6e4923d8c78180faef799b644088347","permalink":"https://forrt.org/curated_resources/reply-to-uri-simonsohn-s-critique-of-def/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/reply-to-uri-simonsohn-s-critique-of-def/","section":"curated_resources","summary":"A blog that summarises a reply to Uri Simonsohn's Critique of Default Bayesian Tests","tags":["Blog"],"title":"Reply to Uri Simonsohn's Critique of Default Bayesian Tests","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1689122843,"objectID":"bc85a5ee9fed088561d58f859b6aca9c","permalink":"https://forrt.org/curated_resources/reporting-effect-sizes-in-original-psych/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/reporting-effect-sizes-in-original-psych/","section":"curated_resources","summary":"Statistical practice in psychological science is undergoing reform which is reflected in part by strong recommendations for reporting and interpreting effect sizes and their confidence intervals. We present principles and recommendations for research reporting and emphasize the variety of ways effect sizes can be reported. Additionally, we emphasize interpreting and reporting unstandardized effect sizes because of common misconceptions regarding standardized effect sizes which we elucidate. Effect sizes should directly answer their motivating research questions, be comprehensible to the average reader, and be based on meaningful metrics of their constituent variables. We illustrate our recommendations with empirical examples involving a One-way ANOVA, a categorical variable analysis, an interaction effect in linear regression, and a simple mediation model, emphasizing the interpretation of effect sizes.","tags":[""],"title":"Reporting Effect Sizes in Original Psychological Research: A Discussion and Tutorial","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1721390377,"objectID":"92ccd6a515e273157bf32c8a4b92a62e","permalink":"https://forrt.org/glossary/english/reporting_guideline/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/english/reporting_guideline/","section":"glossary","summary":"","tags":null,"title":"Reporting Guideline","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1635961284,"objectID":"2df0fe45d938288a31eee6cde7a2f214","permalink":"https://forrt.org/glossary/vbeta/reporting-guideline/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/vbeta/reporting-guideline/","section":"glossary","summary":"","tags":null,"title":"Reporting Guideline","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1720865851,"objectID":"2481c0a198c9d8defdb3bccc94f8918a","permalink":"https://forrt.org/glossary/german/reporting_guideline/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/german/reporting_guideline/","section":"glossary","summary":"","tags":null,"title":"Reporting Guideline (Berichtleitlinien)","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"cbd74c81635463a6f587cfddb4a28ef2","permalink":"https://forrt.org/curated_resources/reporting-in-experimental-philosophy-cur/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/reporting-in-experimental-philosophy-cur/","section":"curated_resources","summary":"Recent replication crises in psychology and other fields have led to intense reflection about the validity of common research practices. Much of this reflection has focussed on reporting standards, and how they may be related to the questionable research practices that could underlie a high proportion of irreproducible findings in the published record. As a developing field, it is particularly important for Experimental Philosophy to avoid some of the pitfalls that have beset other disciplines. To this end, here we provide a detailed, comprehensive assessment of current reporting practices in Experimental Philosophy. We focus on the quality of statistical reporting and the disclosure of information about study methodology. We assess all the articles using quantitative methods (n = 134) that were published over the years 2013–2016 in 29 leading philosophy journals. We find that null hypothesis significance testing is the prevalent statistical practice in Experimental Philosophy, although relying solely on this approach has been criticised in the psychological literature. To augment this approach, various additional measures have become commonplace in other fields, but we find that Experimental Philosophy has adopted these only partially: 53% of the papers report an effect size, 28% confidence intervals, 1% examined prospective statistical power and 5% report observed statistical power. Importantly, we find no direct relation between an article’s reporting quality and its impact (numbers of citations). We conclude with recommendations for authors, reviewers and editors in Experimental Philosophy, to facilitate making research statistically-transparent and reproducible.","tags":["Reproducibility"],"title":"Reporting in Experimental Philosophy: Current Standards and Recommendations for Future Practice","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1721390377,"objectID":"fbd0e201f2d1c8eac333f16c77dbb85f","permalink":"https://forrt.org/glossary/english/repository/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/english/repository/","section":"glossary","summary":"","tags":null,"title":"Repository","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1635961284,"objectID":"dde15bc7c5a894aece005286b544f4a0","permalink":"https://forrt.org/glossary/vbeta/repository/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/vbeta/repository/","section":"glossary","summary":"","tags":null,"title":"Repository","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1720865851,"objectID":"d141c74d9c41b651e966e96d19c44e85","permalink":"https://forrt.org/glossary/german/repository/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/german/repository/","section":"glossary","summary":"","tags":null,"title":"Repository (Repositorium)","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"156d73220fe43b396cc9615fee140cc1","permalink":"https://forrt.org/curated_resources/repository-of-psychological-instruments/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/repository-of-psychological-instruments/","section":"curated_resources","summary":"An Open-Access repository of psychological measures, scales, tests, and other research instruments in Serbian. Documented are psychological instruments that have been translated into Serbian and/or adapted for the Serbian population. Hosted on the Open Science Framework and maintained by researchers at the Laboratory for Research of Individual Differences (LIRA) at the University of Belgrade, Serbia.","tags":["Digital Repository","Research Data","Psychological Measures","Open Repository","Scales","Translation","Adaptation"],"title":"Repository of Psychological Instruments in Serbian (REPOPSI)","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1689122843,"objectID":"60f758bda935e4fed93d7776c118678e","permalink":"https://forrt.org/curated_resources/reproducibilitea/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/reproducibilitea/","section":"curated_resources","summary":"Serving mugs of Reproducibili☕️: Blends include transparency, openness and robustness + a spoonful of science.","tags":["Podcast","Reproducibility Crisis and Credibility Revolution","Open Science"],"title":"ReproducibiliTea","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1721390377,"objectID":"b8225864cb0b20f0d28ba177a121a859","permalink":"https://forrt.org/glossary/english/reproducibilitea/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/english/reproducibilitea/","section":"glossary","summary":"","tags":null,"title":"ReproducibiliTea","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1720865851,"objectID":"2f87b9ce956f38cefe385b1c525036d1","permalink":"https://forrt.org/glossary/german/reproducibilitea/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/german/reproducibilitea/","section":"glossary","summary":"","tags":null,"title":"ReproducibiliTea","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1635961284,"objectID":"8729793a6bd4872a27dd2200e74a407b","permalink":"https://forrt.org/glossary/vbeta/reproducibilitea/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/vbeta/reproducibilitea/","section":"glossary","summary":"","tags":null,"title":"ReproducibiliTea","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1721390377,"objectID":"01abf679de7789a9d549ea99d2b7bd9c","permalink":"https://forrt.org/glossary/english/reproducibility/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/english/reproducibility/","section":"glossary","summary":"","tags":null,"title":"Reproducibility","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1635961284,"objectID":"5f6b08f1c24a937b599c62aef4bea949","permalink":"https://forrt.org/glossary/vbeta/reproducibility/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/vbeta/reproducibility/","section":"glossary","summary":"","tags":null,"title":"Reproducibility","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1720865851,"objectID":"94d1ea842d245c7e51b6f2bd4664ea97","permalink":"https://forrt.org/glossary/german/reproducibility/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/german/reproducibility/","section":"glossary","summary":"","tags":null,"title":"Reproducibility (Reproduzierbarkeit)","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1721390377,"objectID":"d7cd5a0c20b429469351f4e8b289f527","permalink":"https://forrt.org/glossary/english/reproducibility_crisis/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/english/reproducibility_crisis/","section":"glossary","summary":"","tags":null,"title":"Reproducibility crisis (aka Replicability or replication crisis)","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1635961284,"objectID":"1075a95715412dcd65a54094c715c54f","permalink":"https://forrt.org/glossary/vbeta/reproducibility-crisis-aka-replicab/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/vbeta/reproducibility-crisis-aka-replicab/","section":"glossary","summary":"","tags":null,"title":"Reproducibility crisis (aka Replicability or replication crisis)","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1720865851,"objectID":"e586a3e90eff3ae54ba197aa3bf503e5","permalink":"https://forrt.org/glossary/german/reproducibility_crisis/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/german/reproducibility_crisis/","section":"glossary","summary":"","tags":null,"title":"Reproducibility crisis (aka Replicability or replication crisis) (Reproduzierbarkeitskrise, auch Replizierbarkeitskrise oder Replikationskrise)","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"063cba9ea9c8053cee138f37f5dc8832","permalink":"https://forrt.org/curated_resources/reproducibility-for-everyone/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/reproducibility-for-everyone/","section":"curated_resources","summary":"Reproducibility for Everyone produces resources to help grow researchers' awareness of and ability to do reproducible research.","tags":["Reproducibility","Research Data Management Tools","Researchers"],"title":"Reproducibility for Everyone","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"b87e01b0c70158412aec6973cc2cfa00","permalink":"https://forrt.org/curated_resources/reproducibility-immersive-course/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/reproducibility-immersive-course/","section":"curated_resources","summary":"Various fields in the natural and social sciences face a ‘crisis of confidence’. Broadly, this crisis amounts to a pervasiveness of non-reproducible results in the published literature. For example, in the field of biomedicine, Amgen published findings that out of 53 landmark published results of pre-clinical studies, only 11% could be replicated successfully. This crisis is not confined to biomedicine. Areas that have recently received attention for non-reproducibility include biomedicine, economics, political science, psychology, as well as philosophy. Some scholars anticipate the expansion of this crisis to other disciplines.This course explores the state of reproducibility. After giving a brief historical perspective, case studies from different disciplines (biomedicine, psychology, and philosophy) are examined to understand the issues concretely. Subsequently, problems that lead to non-reproducibility are discussed as well as possible solutions and paths forward.","tags":["Analysis","Git","GitHub","GitLab","Jupyter Notebooks","Open Scholarship Tools and Technologies","Open Science Framework","R","Reproducibility","ReproZip","Research Data Management Tools","Researchers","RStudio","Version Control"],"title":"Reproducibility Immersive Course","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"0744a494312ba095bcb7af2fd75df20a","permalink":"https://forrt.org/curated_resources/reproducibility-in-cancer-biology-the-ch/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/reproducibility-in-cancer-biology-the-ch/","section":"curated_resources","summary":"Interpreting the first results from the Reproducibility Project: Cancer Biology requires a highly nuanced approach. Reproducibility is a cornerstone of science, and the development of new drugs and medical treatments relies on the results of preclinical research being reproducible. In recent years, however, the validity of published findings in a number of areas of scientific research, including cancer research, have been called into question (Begley and Ellis, 2012; Baker, 2016). One response to these concerns has been the launch of a project to repeat selected experiments from a number of high-profile papers in cancer biology (Morrison, 2014; Errington et al., 2014). The aim of the Reproducibility Project: Cancer Biology, which is a collaboration between the Center for Open Science and Science Exchange, is two-fold: to provide evidence about reproducibility in preclinical cancer research, and to identify the factors that influence reproducibility more generally.","tags":["Metascience","Methodology","Open Science","Replication","Reproducibility","Reproducibility Project: Cancer Biology"],"title":"Reproducibility in Cancer Biology: The challenges of replication","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"507a92d1f66497d7313ade460c98bac4","permalink":"https://forrt.org/curated_resources/reproducibility-in-psychology-syllabus/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/reproducibility-in-psychology-syllabus/","section":"curated_resources","summary":"Psychological science has been going through a crisis of confidence concerning theveracity of the findings that serve as the foundation to our field. This seminar willcover the problems of reproducibility and replicability in psychological science, aswell as the proposed solutions to this crisis.","tags":["Reproducibility Crisis and Credibility Revolution","Open Science","Conceptual and statistical knowledge"],"title":"Reproducibility in Psychology Syllabus","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"0c971a92ed8d3309e7e10e2dae3f505a","permalink":"https://forrt.org/curated_resources/reproducibility-in-the-psychological-sci/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/reproducibility-in-the-psychological-sci/","section":"curated_resources","summary":"Science is undergoing a paradigm shift in the way research is conducted and what is considered evidence. Psychology is leading other disciplines in innovative methods to address concerns related to replication and transparency by creating meta-science projects (i.e., Reproducibility Project, Many Labs) and other tools (Transparency of Publication Guidelines, Open Data Badges) to maximize reproducibility in psychology and other sciences. In addition to reading primary articles reporting findings emerging from these efforts, students will engage in the projects through critical evaluation of research protocols, reanalysis of data, and writing APA research reports of findings.","tags":["Reproducibility Crisis and Credibility Revolution","Open Science"],"title":"Reproducibility in the Psychological Sciences","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"f0b14b7552ea2e8e188aa8ebdb5d2183","permalink":"https://forrt.org/curated_resources/reproducibility-librarianship-in-practic/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/reproducibility-librarianship-in-practic/","section":"curated_resources","summary":"As research across domains of study has become increasingly reliant on digital tools (librarianship included), the challenges in reproducibility have grown. Alongside this reproducibility challenge are the demands for open scholarship, such as releasing code, data, and articles under an open license.Before, researchers out in the field used to capture their environments through observation, drawings, photographs, and videos; now, researchers and the librarians who work alongside them must capture digital environments and what they contain (e.g. code and data) to achieve reproducibility. Librarians are well-positioned to help patrons open their scholarship, and it’s time to build in reproducibility as a part of our services.Librarians are already engaged with research data management, open access publishing, grant compliance, pre-registration, and it’s time we as a profession add reproducibility to that repertoire. In this webinar, organised by LIBER’s Research Data Management Working Group, speaker Vicky Steeves discusses how she’s built services around reproducibility as a dual appointment between the Libraries and the Center for Data Science at New York University.","tags":["Institutional Policies","Librarians","Librarianship","Reproducibility"],"title":"Reproducibility Librarianship in Practice","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1721390377,"objectID":"e52e6e3bcad8b45de337877b569037cf","permalink":"https://forrt.org/glossary/english/reproducibility_network/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/english/reproducibility_network/","section":"glossary","summary":"","tags":null,"title":"Reproducibility Network","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1635961284,"objectID":"d7ee4a559a94570ad174788ebf72e151","permalink":"https://forrt.org/glossary/vbeta/reproducibility-network/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/vbeta/reproducibility-network/","section":"glossary","summary":"","tags":null,"title":"Reproducibility Network","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1720865851,"objectID":"15fa757acad79d050eff274887fd1995","permalink":"https://forrt.org/glossary/german/reproducibility_network/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/german/reproducibility_network/","section":"glossary","summary":"","tags":null,"title":"Reproducibility Network (Reproduzierbarkeitsnetzwerk)","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"eca69319a20d5b23abcd60eb484e3179","permalink":"https://forrt.org/curated_resources/reproducibility-preservation-and-access/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/reproducibility-preservation-and-access/","section":"curated_resources","summary":"The adoption of reproducibility remains low, despite incentives becoming increasingly common in different domains, conferences, and journals. The truth is, reproducibility is technically difficult to achieve due to the complexities of computational environments.To address these technical challenges, we created ReproZip, an open-source tool that packs research along with all the necessary information to reproduce it, including data files, software, OS version, and environment variables. Everything is then bundled into an .rpz file, which users can use to reproduce the work with ReproUnzip and an unpacker (Docker, Vagrant, and Singularity). The .rpz file is general and contains rich metadata: more unpackers can be added as needed, better guaranteeing long-term preservation.However, installing the unpackers can still be burdensome for secondary users of ReproZip bundles. In this paper, we will discuss how ReproZip and our new tool ReproServer can be used together to facilitate access to well-preserved, reproducible work. ReproServer is a cloud application that allows users to upload or provide a link to a ReproZip bundle, and then interact with/reproduce the contents from the comfort of their browser. Users are then provided a stable link to the unpacked work on ReproServer they can share with reviewers or colleagues.","tags":["Open Scholarship Tools and Technologies","Reproducibility","Research Data Management Tools","Researchers"],"title":"Reproducibility, Preservation, and Access to Research with ReproZip and ReproServer","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"a1a0c3bfc30b843b62b3cfacbd079ba8","permalink":"https://forrt.org/curated_resources/reproducibility-open-science-and-clinica/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/reproducibility-open-science-and-clinica/","section":"curated_resources","summary":"A reading list for reproducibility in clinical psychology","tags":["Clinical psychology"],"title":"Reproducibility/Open Science and Clinical Psych Reading List","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"6bd70a64b266d5c43bdcc2d1220bb3c9","permalink":"https://forrt.org/curated_resources/reproducible-and-reusable-research-are-j/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/reproducible-and-reusable-research-are-j/","section":"curated_resources","summary":"Background There is wide agreement in the biomedical research community that research data sharing is a primary ingredient for ensuring that science is more transparent and reproducible. Publishers could play an important role in facilitating and enforcing data sharing; however, many journals have not yet implemented data sharing policies and the requirements vary widely across journals. This study set out to analyze the pervasiveness and quality of data sharing policies in the biomedical literature. Methods The online author’s instructions and editorial policies for 318 biomedical journals were manually reviewed to analyze the journal’s data sharing requirements and characteristics. The data sharing policies were ranked using a rubric to determine if data sharing was required, recommended, required only for omics data, or not addressed at all. The data sharing method and licensing recommendations were examined, as well any mention of reproducibility or similar concepts. The data was analyzed for patterns relating to publishing volume, Journal Impact Factor, and the publishing model (open access or subscription) of each journal. Results A total of 11.9% of journals analyzed explicitly stated that data sharing was required as a condition of publication. A total of 9.1% of journals required data sharing, but did not state that it would affect publication decisions. 23.3% of journals had a statement encouraging authors to share their data but did not require it. A total of 9.1% of journals mentioned data sharing indirectly, and only 14.8% addressed protein, proteomic, and/or genomic data sharing. There was no mention of data sharing in 31.8% of journals. Impact factors were significantly higher for journals with the strongest data sharing policies compared to all other data sharing criteria. Open access journals were not more likely to require data sharing than subscription journals. Discussion Our study confirmed earlier investigations which observed that only a minority of biomedical journals require data sharing, and a significant association between higher Impact Factors and journals with a data sharing requirement. Moreover, while 65.7% of the journals in our study that required data sharing addressed the concept of reproducibility, as with earlier investigations, we found that most data sharing policies did not provide specific guidance on the practices that ensure data is maximally available and reusable.","tags":["Data","Inside Your Classroom","Reproducibility"],"title":"Reproducible and reusable research: are journal data sharing policies meeting the mark?","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"b90b1df2d7090397cd600dba0130ab59","permalink":"https://forrt.org/curated_resources/reproducible-and-transparent-research-pr/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/reproducible-and-transparent-research-pr/","section":"curated_resources","summary":"The objective of this study was to evaluate the nature and extent of reproducible and transparent research practices in neurology publications. Methods The NLM catalog was used to identify MEDLINE-indexed neurology journals. A PubMed search of these journals was conducted to retrieve publications over a 5-year period from 2014 to 2018. A random sample of publications was extracted. Two authors conducted data extraction in a blinded, duplicate fashion using a pilot-tested Google form. This form prompted data extractors to determine whether publications provided access to items such as study materials, raw data, analysis scripts, and protocols. In addition, we determined if the publication was included in a replication study or systematic review, was preregistered, had a conflict of interest declaration, specified funding sources, and was open access. Results Our search identified 223,932 publications meeting the inclusion criteria, from which 400 were randomly sampled. Only 389 articles were accessible, yielding 271 publications with empirical data for analysis. Our results indicate that 9.4% provided access to materials, 9.2% provided access to raw data, 0.7% provided access to the analysis scripts, 0.7% linked the protocol, and 3.7% were preregistered. A third of sampled publications lacked funding or conflict of interest statements. No publications from our sample were included in replication studies, but a fifth were cited in a systematic review or meta-analysis. Conclusions Currently, published neurology research does not consistently provide information needed for reproducibility. The implications of poor research reporting can both affect patient care and increase research waste. Collaborative intervention by authors, peer reviewers, journals, and funding sources is needed to mitigate this problem.","tags":["Data","Inside Your Classroom","Reproducibility"],"title":"Reproducible and transparent research practices in published neurology research","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"1e8cc380ca80504a70d6e01354c9d3db","permalink":"https://forrt.org/curated_resources/reproducible-research/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/reproducible-research/","section":"curated_resources","summary":"Modern scientific research takes advantage of programs such as Python and R that are open source. As such, they can be modified and shared by the wider community. Additionally, there is added functionality through additional programs and packages, such as IPython, Sweave, and Shiny. These packages can be used to not only execute data analyses, but also to present data and results consistently across platforms (e.g., blogs, websites, repositories and traditional publishing venues).\n\nThe goal of the course is to show how to implement analyses and share them using IPython for Python, Sweave and knitr for RStudio to create documents that are shareable and analyses that are reproducible.\n\nCourse outline is as follows:\n1) Use of IPython notebooks to demonstrate and explain code, visualize data, and display analysis results\n2) Applications of Python modules such as SymPy, NumPy, pandas, and SciPy\n3) Use of Sweave to demonstrate and explain code, visualize data, display analysis results, and create documents and presentations\n4) Integration and execution of IPython and R code and analyses using the IPython notebook","tags":["Analysis","Literate Programming","Reproducibility","Version Control"],"title":"Reproducible Research","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1689122843,"objectID":"ef830fb9ff32dc85ca3ba3202bb4857e","permalink":"https://forrt.org/curated_resources/reproducible-research-in-sport-and-exerc/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/reproducible-research-in-sport-and-exerc/","section":"curated_resources","summary":"Objectives: We aim to introduce the discussion on the crisis of confidence to sport and exercise psychology. We focus on an important aspect of this debate, the impact of sample sizes, by assessing sample sizes within sport and exercise psychology. Researchers have argued that publications in psychological research contain numerous false-positive findings and inflated effect sizes due to small sample sizes. Method: We analyse the four leading journals in sport and exercise psychology regarding sample sizes of all quantitative studies published in these journals between 2009 and 2013. Subsequently, we conduct power analyses. Results: A substantial proportion of published studies does not have sufficient power to detect effect sizes typical for psychological research. Sample sizes and power vary between research designs. Although many correlational studies have adequate sample sizes, experimental studies are often underpowered to detect small-to-medium effects. Conclusions: As sample sizes are small, research in sport and exercise psychology may suffer from false-positive results and inflated effect sizes, while at the same time failing to detect meaningful small effects. Larger sample sizes are warranted, particularly in experimental studies.","tags":[""],"title":"Reproducible research in sport and exercise psychology: The role of sample sizes.","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"9d0dfe57befb5c17c442f1a079068e44","permalink":"https://forrt.org/curated_resources/reproducible-research-methods/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/reproducible-research-methods/","section":"curated_resources","summary":"This is the website for the Autumn 2014 course “Reproducible Research Methods” taught by Eric C. Anderson at NOAA’s Southwest Fisheries Science Center. The course meets on Tuesdays and Thursdays from 3:30 to 4:30 PM in Room 188 of the Fisheries Ecology Division.\nIt runs from Oct 7 to December 18.\n\nThe goal of this course is for scientists, researchers, and students to learn:\n\nto write programs in the R language to manipulate and analyze data,\nto integrate data analysis with report generation and article preparation using knitr,\nto work fluently within the Rstudio integrated development environment for R,\nto use git version control software and GitHub to effectively manage source code, collaborate efficiently with other researchers, and neatly package their research.","tags":["Analysis","Literate Programming","Reproducibility","Version Control"],"title":"Reproducible Research Methods","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"c64ac3692867f98581f280cd8ea68ad1","permalink":"https://forrt.org/curated_resources/reproducible-research-practices-transpar/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/reproducible-research-practices-transpar/","section":"curated_resources","summary":"Currently, there is a growing interest in ensuring the transparency and reproducibility of the published scientific literature. According to a previous evaluation of 441 biomedical journals articles published in 2000–2014, the biomedical literature largely lacked transparency in important dimensions. Here, we surveyed a random sample of 149 biomedical articles published between 2015 and 2017 and determined the proportion reporting sources of public and/or private funding and conflicts of interests, sharing protocols and raw data, and undergoing rigorous independent replication and reproducibility checks. We also investigated what can be learned about reproducibility and transparency indicators from open access data provided on PubMed. The majority of the 149 studies disclosed some information regarding funding (103, 69.1% [95% confidence interval, 61.0% to 76.3%]) or conflicts of interest (97, 65.1% [56.8% to 72.6%]). Among the 104 articles with empirical data in which protocols or data sharing would be pertinent, 19 (18.3% [11.6% to 27.3%]) discussed publicly available data; only one (1.0% [0.1% to 6.0%]) included a link to a full study protocol. Among the 97 articles in which replication in studies with different data would be pertinent, there were five replication efforts (5.2% [1.9% to 12.2%]). Although clinical trial identification numbers and funding details were often provided on PubMed, only two of the articles without a full text article in PubMed Central that discussed publicly available data at the full text level also contained information related to data sharing on PubMed; none had a conflicts of interest statement on PubMed. Our evaluation suggests that although there have been improvements over the last few years in certain key indicators of reproducibility and transparency, opportunities exist to improve reproducible research practices across the biomedical literature and to make features related to reproducibility more readily visible in PubMed.","tags":["Conflicts of Interest","Data","Medical Journals","Open Access Publishing","Open Science","PublishingGovernment Funding of Science","Replication Studies","Reproducibility","Scientific Publishing","Sequence Databases","Systematic Reviews"],"title":"Reproducible research practices, transparency, and open access data in the biomedical literature, 2015–2017","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"14789834f8de2ad15364c1e686dd38b0","permalink":"https://forrt.org/curated_resources/reproducible-research-true-or-false/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/reproducible-research-true-or-false/","section":"curated_resources","summary":"A video detailing whether reproducible research is true or false","tags":["Video"],"title":"Reproducible Research: True or False?","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"85b98471e56f5a36b30fe55a12336e70","permalink":"https://forrt.org/curated_resources/reproducible-research-walking-the-walk/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/reproducible-research-walking-the-walk/","section":"curated_resources","summary":"Description\n\nThis hands-on tutorial will train reproducible research warriors on the practices and tools that make experimental verification possible with an end-to-end data analysis workflow. The tutorial will expose attendees to open science methods during data gathering, storage, analysis, up to publication into a reproducible article.\n\nAttendees are expected to have basic familiarity with scientific Python and Git.","tags":["Analysis","Organizing","Report Writing","Reproducibility","Version Control"],"title":"Reproducible Research: Walking the Walk","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"511e7d998b1299c179806593bc1c20a8","permalink":"https://forrt.org/curated_resources/reproducible-science-curriculum-lesson-f/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/reproducible-science-curriculum-lesson-f/","section":"curated_resources","summary":"Workshop goals\n- Why are we teaching this\n- Why is this important\n- For future and current you\n- For research as a whole\n- Lack of reproducibility in research is a real problem\n\nMaterials and how we'll use them\n- Workshop landing page, with\n\n- links to the Materials\n- schedule\n\nStructure oriented along the Four Facets of Reproducibility:\n\n- Documentation\n- Organization\n- Automation\n- Dissemination\n\nWill be available after the Workshop\n\nHow this workshop is run\n- This is a Carpentries Workshop\n- that means friendly learning environment\n- Code of Conduct\n- active learning\n- work with the people next to you\n- ask for help","tags":["Analysis","Report Writing","Reproducibility"],"title":"Reproducible Science Curriculum Lesson for Automation","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"3763b6fd7e5ef993ebc2ab6d82b537d4","permalink":"https://forrt.org/curated_resources/reproducible-science-curriculum-lesson-f_2/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/reproducible-science-curriculum-lesson-f_2/","section":"curated_resources","summary":"Workshop goals\n- Why are we teaching this\n- Why is this important\n- For future and current you\n- For research as a whole\n- Lack of reproducibility in research is a real problem\n\nMaterials and how we'll use them\n- Workshop landing page, with\n\n- links to the Materials\n- schedule\n\nStructure oriented along the Four Facets of Reproducibility:\n\n- Documentation\n- Organization\n- Automation\n- Dissemination\n\nWill be available after the Workshop\n\nHow this workshop is run\n- This is a Carpentries Workshop\n- that means friendly learning environment\n- Code of Conduct\n- active learning\n- work with the people next to you\n- ask for help","tags":["Literate Programming","Reproducibility"],"title":"Reproducible Science Curriculum Lesson for Literate Programming","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"fdd7e8e895c233b1ed39d67f7f0a1615","permalink":"https://forrt.org/curated_resources/reproducible-science-curriculum-lesson-f_3/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/reproducible-science-curriculum-lesson-f_3/","section":"curated_resources","summary":"Workshop goals\n- Why are we teaching this\n- Why is this important\n- For future and current you\n- For research as a whole\n- Lack of reproducibility in research is a real problem\n\nMaterials and how we'll use them\n- Workshop landing page, with\n\n- links to the Materials\n- schedule\n\nStructure oriented along the Four Facets of Reproducibility:\n\n- Documentation\n- Organization\n- Automation\n- Dissemination\n\nWill be available after the Workshop\n\nHow this workshop is run\n- This is a Carpentries Workshop\n- that means friendly learning environment\n- Code of Conduct\n- active learning\n- work with the people next to you\n- ask for help","tags":["Organizing","Reproducibility"],"title":"Reproducible Science Curriculum Lesson for Organization","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"f6c15456bb48d437bc8c797a8aa77dca","permalink":"https://forrt.org/curated_resources/reproducible-science-curriculum-lesson-f_4/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/reproducible-science-curriculum-lesson-f_4/","section":"curated_resources","summary":"Workshop goals\n- Why are we teaching this\n- Why is this important\n- For future and current you\n- For research as a whole\n- Lack of reproducibility in research is a real problem\n\nMaterials and how we'll use them\n- Workshop landing page, with\n\n- links to the Materials\n- schedule\n\nStructure oriented along the Four Facets of Reproducibility:\n\n- Documentation\n- Organization\n- Automation\n- Dissemination\n\nWill be available after the Workshop\n\nHow this workshop is run\n- This is a Carpentries Workshop\n- that means friendly learning environment\n- Code of Conduct\n- active learning\n- work with the people next to you\n- ask for help","tags":["Report Writing","Reproducibility"],"title":"Reproducible Science Curriculum Lesson for Publication","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"18015f91cc6270c5513b006ca0168ffa","permalink":"https://forrt.org/curated_resources/reproducible-science-curriculum-lesson-f_5/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/reproducible-science-curriculum-lesson-f_5/","section":"curated_resources","summary":"Workshop goals\n- Why are we teaching this\n- Why is this important\n- For future and current you\n- For research as a whole\n- Lack of reproducibility in research is a real problem\n\nMaterials and how we'll use them\n- Workshop landing page, with\n\n- links to the Materials\n- schedule\n\nStructure oriented along the Four Facets of Reproducibility:\n\n- Documentation\n- Organization\n- Automation\n- Dissemination\n\nWill be available after the Workshop\n\nHow this workshop is run\n- This is a Carpentries Workshop\n- that means friendly learning environment\n- Code of Conduct\n- active learning\n- work with the people next to you\n- ask for help","tags":["Reproducibility","Version Control"],"title":"Reproducible Science Curriculum Lesson for Version Control","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"505dc4f69022040a0b41e56c7705d47e","permalink":"https://forrt.org/curated_resources/reproducible-science-workshop/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/reproducible-science-workshop/","section":"curated_resources","summary":"Workshop goals\n- Why are we teaching this\n- Why is this important\n- For future and current you\n- For research as a whole\n- Lack of reproducibility in research is a real problem\n\nMaterials and how we'll use them\n- Workshop landing page, with\n\n- links to the Materials\n- schedule\n\nStructure oriented along the Four Facets of Reproducibility:\n\n- Documentation\n- Organization\n- Automation\n- Dissemination\n\nWill be available after the Workshop\n\nHow this workshop is run\n- This is a Carpentries Workshop\n- that means friendly learning environment\n- Code of Conduct\n- active learning\n- work with the people next to you\n- ask for help","tags":["Documentation","Organizing","Reproducibility"],"title":"Reproducible Science Workshop","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"4624308b1eb81357d919bcfa6c8ab8ea","permalink":"https://forrt.org/curated_resources/reproducible-statistics-for-psychologist/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/reproducible-statistics-for-psychologist/","section":"curated_resources","summary":"This is a series of labs/tutorials currently under development (2020-2021) for a two-semester graduate-level statistics sequence in Psychology @ Brooklyn College of CUNY. The goal of these tutorials is to 1) develop a deeper conceptual understanding of the principles of statistical analysis and inference; and 2) develop practical skills for data-analysis, such as using the increasingly popular statistical software environment R to code reproducible analyses. The first set of 13 labs roughly track chapters in “Thinking with Data” (Vokey \u0026 Allen, 2018), and the second set of labs (to be written on a weekly basis during the Spring 2021 semester) will roughly track chapters in “Experimental Design and Analysis for Psychology” (Abdi et al., 2009). Although the primary aim is to create lab exercises that reinforce stats concepts and also train basic R coding skills for data-analysis, there are many side goals, including showing students the advantages of using R markdown and Github for creating and communicating research products. For example, aside from these tutorials, I have been developing an R package called vertical (Vuorre \u0026 Crump, 2020), that highlights the advantages of learning R for researchers in psychology. And, where possible, I hope to inject some of this broader discussion about awesome R tools and how to use them into the labs (at the same time, a deep-dive requires a separate course…maybe coming soon to a browser near you).","tags":["Statistics"],"title":"Reproducible statistics for psychologists with R Lab Tutorials","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1721390377,"objectID":"d05e844f618f155317ca0dc2c74c525a","permalink":"https://forrt.org/glossary/english/research_contribution_metric/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/english/research_contribution_metric/","section":"glossary","summary":"","tags":null,"title":"Research Contribution Metric (p)","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1720865851,"objectID":"f1b7535369a0fb07a306690d21a31d98","permalink":"https://forrt.org/glossary/german/research_contribution_metric/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/german/research_contribution_metric/","section":"glossary","summary":"","tags":null,"title":"Research Contribution Metric (p)","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1635961284,"objectID":"67f91c601cae16255212b2f37b592fbe","permalink":"https://forrt.org/glossary/vbeta/research-contribution-metric-p/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/vbeta/research-contribution-metric-p/","section":"glossary","summary":"","tags":null,"title":"Research Contribution Metric (p) ","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1721390377,"objectID":"2129e4dce94127acff6dd2c098ee4730","permalink":"https://forrt.org/glossary/english/research_cycle/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/english/research_cycle/","section":"glossary","summary":"","tags":null,"title":"Research Cycle","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1635961284,"objectID":"ec13b0c5278434963d9a79538a6c33ec","permalink":"https://forrt.org/glossary/vbeta/research-cycle/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/vbeta/research-cycle/","section":"glossary","summary":"","tags":null,"title":"Research Cycle","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1720865851,"objectID":"7aeaa63bd2650b18db9440051af90e73","permalink":"https://forrt.org/glossary/german/research_cycle/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/german/research_cycle/","section":"glossary","summary":"","tags":null,"title":"Research Cycle (Forschungszyklus)","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1721390377,"objectID":"b2844063aeb9af027de633d037a4dd60","permalink":"https://forrt.org/glossary/english/research_data_management/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/english/research_data_management/","section":"glossary","summary":"","tags":null,"title":"Research Data Management","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1635961284,"objectID":"77477c1fba6149f4d99e545658d7588f","permalink":"https://forrt.org/glossary/vbeta/research-data-management/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/vbeta/research-data-management/","section":"glossary","summary":"","tags":null,"title":"Research Data Management","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1720865851,"objectID":"a88904adea785872b0cd9489c96070e5","permalink":"https://forrt.org/glossary/german/research_data_management/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/german/research_data_management/","section":"glossary","summary":"","tags":null,"title":"Research Data Management (Forschungsdatenmanagement)","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1721390377,"objectID":"d28542865182ae83712d47a66d5619af","permalink":"https://forrt.org/glossary/english/research_integrity/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/english/research_integrity/","section":"glossary","summary":"","tags":null,"title":"Research integrity","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1635961284,"objectID":"8118dd933c87478b14892569b9798caa","permalink":"https://forrt.org/glossary/vbeta/research-integrity/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/vbeta/research-integrity/","section":"glossary","summary":"","tags":null,"title":"Research integrity","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1720865851,"objectID":"fea72a7e54efd9ef479bf2ddb796049a","permalink":"https://forrt.org/glossary/german/research_integrity/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/german/research_integrity/","section":"glossary","summary":"","tags":null,"title":"Research integrity (Forschungsintegrität)","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"9ac5b8e96a5cc0205837abf3ad941ca0","permalink":"https://forrt.org/curated_resources/research-integrity-and-responsible-schol/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/research-integrity-and-responsible-schol/","section":"curated_resources","summary":"Manual for a mandatory course for PhD candidates enrolled in the Graduate School for Social Sciences and Master students in the Social Sciences for a Digital Society program at VU Amsterdam. The course seeks to contribute to a reflection and discussion on the normative consequences of the abstract ideals of science, and an awareness of standards of good conduct and the responsibility of researchers in the social sciences. ","tags":["ETHICS REVIEW","DATA MANAGEMENT","RESEARCH INTEGRITY"],"title":"Research Integrity and Responsible Scholarship","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"4a468c77e11f4b0c158aa69c7e1e0399","permalink":"https://forrt.org/curated_resources/research-methods-2020/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/research-methods-2020/","section":"curated_resources","summary":"Broad-based philosophical and methodological perspectives on conducting and interpreting psychological research; considers basic, applied, and translational research, laboratory- and field-based research, and experimental, quasi-experimental, correlational, and longitudinal research designs.","tags":["Reproducibility Crisis and Credibility Revolution","Open Science"],"title":"Research Methods 2020","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"789e3f47a1b6a93bf66d89cedacd2334","permalink":"https://forrt.org/curated_resources/research-methods-in-psychology/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/research-methods-in-psychology/","section":"curated_resources","summary":" Broad-based philosophical and methodological perspectives on conducting and interpreting psychological research; considers basic, applied, and translational research, laboratory- and field-based research, and experimental, quasi-experimental, correlational, and longitudinal research designs.","tags":["Reproducibility Crisis and Credibility Revolution","Open Science"],"title":"Research Methods in Psychology","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"dcccf086f156fd15442b0c3c5b0253a9","permalink":"https://forrt.org/curated_resources/research-methods-in-social-psychology/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/research-methods-in-social-psychology/","section":"curated_resources","summary":"This course will acquaint students with the major research designs and procedures in social psychology, as well as explore recent methodological innovations that were designed to address issues unique to social psychological research. The objectives are to develop a firm grasp of the research methods available, including the application of these methods in research settings, and statistical considerations of these methods. A consideration of how we “do” science is a theme permeating the entire class (e.g., research transparency, replicability of research findings). \nTopics to be covered include, but are not limited to, transparency of the research process and the replicability of research findings, validity and reliability, mediation and moderation, field research, modelling interdependence (data from groups of 2 or more), multi-level modelling, methods for the study of social cognition, and meta-analysis. Half course; one term.","tags":["Reproducibility Crisis and Credibility Revolution","Open Science"],"title":"Research Methods in Social Psychology ","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"a178ffb47dfcb0a76ec59570d6aa0140","permalink":"https://forrt.org/curated_resources/research-methods-syllabi/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/research-methods-syllabi/","section":"curated_resources","summary":"A syllabus about evaluating research methods","tags":["Syllabus"],"title":"Research Methods Syllabi","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"1b72816c3753aa51048258a6ae1f7634","permalink":"https://forrt.org/curated_resources/research-practices-and-statistical-repor/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/research-practices-and-statistical-repor/","section":"curated_resources","summary":"The replicability of research findings has recently been disputed across multiple scientific disciplines. In constructive reaction, the research culture in psychology is facing fundamental changes, but investigations of research practices that led to these improvements have almost exclusively focused on academic researchers. By contrast, we investigated the statistical reporting quality and selected indicators of questionable research practices (QRPs) in psychology students' master's theses. In a total of 250 theses, we investigated utilization and magnitude of standardized effect sizes, along with statistical power, the consistency and completeness of reported results, and possible indications of p-hacking and further testing. Effect sizes were reported for 36% of focal tests (median r = 0.19), and only a single formal power analysis was reported for sample size determination (median observed power 1 − β = 0.67). Statcheck revealed inconsistent p-values in 18% of cases, while 2% led to decision errors. There were no clear indications of p-hacking or further testing. We discuss our findings in the light of promoting open science standards in teaching and student supervision.","tags":["Reproducibility"],"title":"Research practices and statistical reporting quality in 250 economic psychology master's theses: a meta-research investigation","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"06a5b8b35f26461652e5f931d433e153","permalink":"https://forrt.org/curated_resources/research-practices-for-a-robust-psycholo/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/research-practices-for-a-robust-psycholo/","section":"curated_resources","summary":"This first issue of 2022 marks the transition of Psychology and Aging in adopting a transparency and openness promotion (TOP) framework. The journal has always had high standards for theoretically meaningful research conducted with methodological and analytic rigor. As the Open Science movement has gathered steam, authors are increasingly submitting papers that fully meet TOP standards at Levels 1 or 2, and those who do not, have generally been quite happy to respond to the gentle nudges of the journal's editors. Thus, in practical terms, the changes at this point are actually quite modest. In what follows, Stine-Morrow addresses questions about the new standards: (a) Why now? and (b) What are they?","tags":["TOP Factor"],"title":"Research practices for a robust psychological science of adult development and aging","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1689122843,"objectID":"15c46a4e3254de587946b7a893d639d0","permalink":"https://forrt.org/curated_resources/research-practices-that-can-prevent-an-i/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/research-practices-that-can-prevent-an-i/","section":"curated_resources","summary":"Recent studies have indicated that research practices in psychology may be susceptible to factors that increase false-positive rates, raising concerns about the possible prevalence of false-positive findings. The present article discusses several practices that may run counter to the inflation of false-positive rates. Taking these practices into account would lead to a more balanced view on the false-positive issue. Specifically, we argue that an inflation of false-positive rates would diminish, sometimes to a substantial degree, when researchers (a) have explicit a priori theoretical hypotheses, (b) include multiple replication studies in a single paper, and (c) collect additional data based on observed results. We report findings from simulation studies and statistical evidence that support these arguments. Being aware of these preventive factors allows researchers not to overestimate the pervasiveness of false-positives in psychology and to gauge the susceptibility of a paper to possible false positives in practical and fair ways.","tags":["Reproducibility Crisis and Credibility Revolution","Open Science"],"title":"Research Practices That Can Prevent an Inflation of False-Positive Rates","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1689122843,"objectID":"c4fe7c5406ee487c5ceb3e3586055e4a","permalink":"https://forrt.org/curated_resources/research-preregistration-101/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/research-preregistration-101/","section":"curated_resources","summary":"A blog about pre-registration","tags":["Blog"],"title":"Research preregistration 101","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"b1737e758f217eb4478430090ed25ef2","permalink":"https://forrt.org/curated_resources/research-preregistration-as-a-teaching-a/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/research-preregistration-as-a-teaching-a/","section":"curated_resources","summary":"The preregistration of research plans and hypotheses may prevent publication bias and questionable research practices. We incorporated a modified version of the preregistration process into an undergraduate capstone research course. Students completed a standard preregistration form during the planning stages of their research projects as well as surveys about their knowledge of preregistration. Based on survey results, our senior-level psychology students lacked knowledge of importance of the preregistration movement in the sciences but could anticipate some of its benefits. Our review of the completed preregistration assignment suggested that students struggle with data analysis decision-making but generally perceive preregistration as a helpful planning tool. We discuss the value of a preregistration assignment for generating discussions of research practice and ethics.","tags":["Preregistration","Questionable Research Practices","Undergraduate Preregistration Assignment"],"title":"Research Preregistration as a Teaching and Learning Tool in Undergraduate Psychology Courses","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"3a5438e974388ce2f4aec0fd8130ead1","permalink":"https://forrt.org/curated_resources/research-preregistration-in-political-sc/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/research-preregistration-in-political-sc/","section":"curated_resources","summary":"This article describes the current debate on the practice of preregistration in political science—that is, publicly releasing a research design before observing outcome data. The case in favor of preregistration maintains that it can restrain four potential causes of publication bias, clearly distinguish deductive and inductive studies, add transparency regarding a researcher’s motivation, and liberate researchers who may be pressured to find specific results. Concerns about preregistration maintain that it is less suitable for the study of historical data, could reduce data exploration, may not allow for contextual problems that emerge in field research, and may increase the difficulty of finding true positive results. This article makes the case that these concerns can be addressed in preregistered studies, and it offers advice to those who would like to pursue study registration in their own work.","tags":["Political Science","Preregistration"],"title":"Research Preregistration in Political Science: The Case, Counterarguments, and a Response to Critiques","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"2f2d6ec44782b48972a5c7f08c6262c0","permalink":"https://forrt.org/curated_resources/research-project-initialization-and-orga/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/research-project-initialization-and-orga/","section":"curated_resources","summary":"Workshop goals\n- Why are we teaching this\n- Why is this important\n- For future and current you\n- For research as a whole\n- Lack of reproducibility in research is a real problem\n\nMaterials and how we'll use them\n- Workshop landing page, with\n\n- links to the Materials\n- schedule\n\nStructure oriented along the Four Facets of Reproducibility:\n\n- Documentation\n- Organization\n- Automation\n- Dissemination\n\nWill be available after the Workshop\n\nHow this workshop is run\n- This is a Carpentries Workshop\n- that means friendly learning environment\n- Code of Conduct\n- active learning\n- work with the people next to you\n- ask for help","tags":["Organizing","Reproducibility"],"title":"Research project initialization and organization following reproducible research guidelines","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"148a29af4b4a543c7a74ccd0b9a65302","permalink":"https://forrt.org/curated_resources/research-project-management-using-the-op/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/research-project-management-using-the-op/","section":"curated_resources","summary":"An introduction to managing, annotating, organizing, archiving, and publishing research data using the Open Science Framework.","tags":["Analysis","Materials","Open Scholarship Tools and Technologies","Research Data Management Tools","Researchers"],"title":"Research Project Management Using the Open Science Framework","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1721390377,"objectID":"e015f352eec518e84743666d8b8f8955","permalink":"https://forrt.org/glossary/english/research_protocol/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/english/research_protocol/","section":"glossary","summary":"","tags":null,"title":"Research Protocol","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1635961284,"objectID":"7c92efd7a573c3aaa8db0b9704724abe","permalink":"https://forrt.org/glossary/vbeta/research-protocol/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/vbeta/research-protocol/","section":"glossary","summary":"","tags":null,"title":"Research Protocol","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1720865851,"objectID":"66d636638e16eeef5f3a4ba2f2077e22","permalink":"https://forrt.org/glossary/german/research_protocol/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/german/research_protocol/","section":"glossary","summary":"","tags":null,"title":"Research Protocol (Forschungsprotokoll)","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1721390377,"objectID":"aa700f1c0eefbb26622d087b9f4af738","permalink":"https://forrt.org/glossary/english/research_workflow/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/english/research_workflow/","section":"glossary","summary":"","tags":null,"title":"Research workflow","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1635961284,"objectID":"e567c10e2ec02a58bd553d2db4685bd5","permalink":"https://forrt.org/glossary/vbeta/research-workflow/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/vbeta/research-workflow/","section":"glossary","summary":"","tags":null,"title":"Research workflow","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1720865851,"objectID":"37914dc74c7b551679fbbd0e064d499d","permalink":"https://forrt.org/glossary/german/research_workflow/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/german/research_workflow/","section":"glossary","summary":"","tags":null,"title":"Research workflow (Forschungs-Arbeitsablauf)","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1721390377,"objectID":"68bff44179ba4a1abc2013ff0a2de072","permalink":"https://forrt.org/glossary/english/researcher_degrees_of_freedom/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/english/researcher_degrees_of_freedom/","section":"glossary","summary":"","tags":null,"title":"Researcher degrees of freedom","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1635961284,"objectID":"aec632c49cf5edf0b0acff0546728924","permalink":"https://forrt.org/glossary/vbeta/researcher-degrees-of-freedom/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/vbeta/researcher-degrees-of-freedom/","section":"glossary","summary":"","tags":null,"title":"Researcher degrees of freedom","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1720865851,"objectID":"c955f1c22e1a9f2baa54a17cb7ffc240","permalink":"https://forrt.org/glossary/german/researcher_degrees_of_freedom/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/german/researcher_degrees_of_freedom/","section":"glossary","summary":"","tags":null,"title":"Researcher degrees of freedom (Freiheitsgrade von Forschenden)","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1689122843,"objectID":"f19679150296d6396190cacbd1d9251a","permalink":"https://forrt.org/curated_resources/researchers-intuitions-about-power-in-ps/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/researchers-intuitions-about-power-in-ps/","section":"curated_resources","summary":"Many psychology studies are statistically underpowered. In part, this may be because many researchers rely on intuition, rules of thumb, and prior practice (along with practical considerations) to determine the number of subjects to test. In Study 1, we surveyed 291 published research psychologists and found large discrepancies between their reports of their preferred amount of power and the actual power of their studies (calculated from their reported typical cell size, typical effect size, and acceptable alpha). Furthermore, in Study 2, 89% of the 214 respondents overestimated the power of specific research designs with a small expected effect size, and 95% underestimated the sample size needed to obtain .80 power for detecting a small effect. Neither researchers’ experience nor their knowledge predicted the bias in their self-reported power intuitions. Because many respondents reported that they based their sample sizes on rules of thumb or common practice in the field, we recommend that researchers conduct and report formal power analyses for their studies.","tags":[""],"title":"Researchers’ Intuitions About Power in Psychological Research","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"2a18ea24fdc067ed63ea86f4b1a3cc05","permalink":"https://forrt.org/curated_resources/resolving-the-tension-between-exploratio/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/resolving-the-tension-between-exploratio/","section":"curated_resources","summary":"Confirmation through competent replication is a founding principle of modern science. However, biomedical researchers are rewarded for innovation, and not for confirmation, and confirmatory research is often stigmatized as unoriginal and as a consequence faces barriers to publication. As a result, the current biomedical literature is dominated by exploration, which to complicate matters further is often disguised as confirmation. Only recently scientists and the public have begun to realize that high-profile research results in biomedicine can often not be replicated. Consequently, confirmation has become central stage in the quest to safeguard the robustness of research findings. Research which is pushing the boundaries of or challenges what is currently known must necessarily result in a plethora of false positive results. Thus, since discovery, the driving force of scientific progress, is unavoidably linked to high false positive rates and cannot support confirmatory inference, dedicated confirmatory investigation is needed for pivotal results. In this chapter I will argue that the tension between the two modes of research, exploration and confirmation, can be resolved if we conceptually and practically separate them. I will discuss the idiosyncrasies of exploratory and confirmatory studies, with a focus on the specific features of their design, analysis, and interpretation.","tags":["False Negative","False Positive","Preclinical Randomized Controlled Trial","Replication","Reproducibility","Statistics"],"title":"Resolving the Tension Between Exploration and Confirmation in Preclinical Biomedical Research","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1689122843,"objectID":"88883886baea3e8f4e7d433c61346d05","permalink":"https://forrt.org/curated_resources/resources-tutorials-papers-analysis-scri/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/resources-tutorials-papers-analysis-scri/","section":"curated_resources","summary":"A website about moderation and mediation","tags":["Website"],"title":"Resources (tutorials, papers, analysis scripts, utilities) for testing moderation and mediation","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"f956f347d881eeca0dae36c073654bde","permalink":"https://forrt.org/curated_resources/resources-for-practicing-open-science-wi/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/resources-for-practicing-open-science-wi/","section":"curated_resources","summary":"This list of resources consists of resources for researchers, editors, and reviewers interested in practicing open science principles, particularly in education research. This list is not exhaustive but meant as a starting point for individuals wanting to learn more about doing open science work specifically for qualitative research.\nThis list was compiled by the following contributors: Rachel Renbarger, Sondra Stegenga, Thomas, Sebastian Karcher, and\u0026nbsp;Crystal Steltenpohl.\u0026nbsp;This resource list grew out of a hackathon at the Virtual Unconference on Open Scholarship Practices in Education Research. ","tags":["Data Sharing","Education Research","Methodology","Open Science","Open Science Training","Posititionality","Qualitative","Research","Transparency"],"title":"Resources for Practicing Open Science with Qualitative Research in Education","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1689122843,"objectID":"787b9a545a998790ba3aa646f7ada095","permalink":"https://forrt.org/curated_resources/response-to-comment-on-estimating-the-re/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/response-to-comment-on-estimating-the-re/","section":"curated_resources","summary":"Gilbert et al. conclude that evidence from the Open Science Collaboration’s Reproducibility Project: Psychology indicates high reproducibility, given the study methodology. Their very optimistic assessment is limited by statistical misconceptions and by causal inferences from selectively interpreted, correlational data. Using the Reproducibility Project: Psychology data, both optimistic and pessimistic conclusions about reproducibility are possible, and neither are yet warranted.","tags":["Reproducibility Crisis and Credibility Revolution","Open Science"],"title":"Response to Comment on “Estimating the reproducibility of psychological science”","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1721390377,"objectID":"61161f26b7f4dd1eb113747dd25d2af8","permalink":"https://forrt.org/glossary/english/responsible_research_and_innovation/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/english/responsible_research_and_innovation/","section":"glossary","summary":"","tags":null,"title":"Responsible Research and Innovation","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1635961284,"objectID":"b154602873f14970b60cd087af180d11","permalink":"https://forrt.org/glossary/vbeta/responsible-research-and-innovation/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/vbeta/responsible-research-and-innovation/","section":"glossary","summary":"","tags":null,"title":"Responsible Research and Innovation","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1720865851,"objectID":"790ca9c184efea9307ec492b32d58da2","permalink":"https://forrt.org/glossary/german/responsible_research_and_innovation/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/german/responsible_research_and_innovation/","section":"glossary","summary":"","tags":null,"title":"Responsible Research and Innovation (Verantwortungsvolle Forschung und Innovation)","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"513008edb02625bce633d65d71fd6dbb","permalink":"https://forrt.org/curated_resources/rethinking-research-assessment-addressin/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/rethinking-research-assessment-addressin/","section":"curated_resources","summary":"In our final installment we get into how incumbent processes and perceptions have the advantage.","tags":["Diversity","Equity and Inclusion"],"title":"Rethinking Research Assessment:Addressing Institutional Biases in Review, Promotion, and Tenure Decision-Making (part IV)","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"6414fedf3f91226272347600181bf2a9","permalink":"https://forrt.org/curated_resources/rethinking-transparency-and-rigor-from-a/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/rethinking-transparency-and-rigor-from-a/","section":"curated_resources","summary":"Discussions around transparency in open science focus primarily on sharing data, materials, and coding schemes, especially as these practices relate to reproducibility. This fairly quantitative perspective of transparency does not align with all scientific methodologies. Indeed, qualitative researchers also care deeply about how knowledge is produced, what factors influence the research process, and how to share this information. Explicating a researcher’s background and role allows researchers to consider their impact on the research process and interpretation of the data, thereby increasing both transparency and rigor. Researchers may engage in positionality and reflexivity in a variety of ways, and transparently sharing these steps allows readers to draw their own informed conclusions about the results and study as a whole. Imposing a limited, quantitatively-informed set of standards on all research can cause harm to researchers and the communities they work with if researchers are not careful in considering the impact of such standards. Our paper will argue the importance of avoiding strong defaults around transparency (e.g., always share data) and build upon previous work around qualitative open science. We explore how transparency in all aspects of our research can lend itself toward projecting and confirming the rigor of our work.","tags":["Open Science","Transparency","Rigor","Qualitative","Quantitative"],"title":"Rethinking transparency and rigor from a qualitative open science perspective","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1689122843,"objectID":"0f2cf79a8da36aec367a8639381a91aa","permalink":"https://forrt.org/curated_resources/retraction-watch-michael-lacour-archives/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/retraction-watch-michael-lacour-archives/","section":"curated_resources","summary":"How easy is it to change people’s minds? In 2014, a Science study suggested that a short conversation could have a lasting impact on people’s opinions about gay marriage – but left readers disappointed when it was retracted only months later, after the first author admitted to falsifying some of the details of the study, including data collection. We found out about the problems with the paper thanks to Joshua Kalla at the University of California, Berkeley and David Broockman at Stanford University, who tried to repeat the remarkable findings. Last week, Kalla and Broockman published a Science paper suggesting what the 2014 paper showed was, in fact, correct – they found that 10-minute conversations about the struggles facing transgender people reduced prejudices against them for months afterwards. We spoke with Kalla and Broockman about the remarkable results from their paper, and the shadow of the earlier retraction.","tags":["Blog"],"title":"Retraction Watch: Michael LaCour archives","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1721390377,"objectID":"c76d657375624ab745f917e13c2ef44f","permalink":"https://forrt.org/glossary/english/reverse_p_hacking/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/english/reverse_p_hacking/","section":"glossary","summary":"","tags":null,"title":"Reverse p-hacking","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1720865851,"objectID":"801e6e0e3c37e9846f7cda6f21c37857","permalink":"https://forrt.org/glossary/german/reverse_p_hacking/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/german/reverse_p_hacking/","section":"glossary","summary":"","tags":null,"title":"Reverse p-hacking","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1635961284,"objectID":"9b7333872c12b47d732f351f363b15f7","permalink":"https://forrt.org/glossary/vbeta/reverse-p-hacking/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/vbeta/reverse-p-hacking/","section":"glossary","summary":"","tags":null,"title":"Reverse p-hacking","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"6dd3ccb7dd9f52791634390952feee73","permalink":"https://forrt.org/curated_resources/review-of-four-preregistration-registrie/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/review-of-four-preregistration-registrie/","section":"curated_resources","summary":"Preregistration involves researchers publicly registering key study elements before conducting a study to increase the transparency of research and limit the use and impact of questionable research practices. To support special education researchers’ engagement with preregistrations, in this article we provide an overview of preregistration and systematically review four preregistration registries (Open Science Framework, Registry of Efficacy and Effectiveness Studies, AsPredicted, and ClinicalTrials). Each of the registry templates reviewed effectively addresses most questionable research practices and can accommodate a variety of research designs, with the exception of mixed-methods research. Among the benefits of Registry of Efficacy and Effectiveness Studies, Open Science Framework, and ClinicalTrials are the provision of highly specific templates and allowing users to update the preregistration when changes to the original study occur. Researchers can use this review to help select and use a registry that is appropriate for their study design and purpose.","tags":["Change","Innovation","Legal Issues","Policy Issues","Research Methodology"],"title":"Review of Four Preregistration Registries for Special Education Researchers","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1689122843,"objectID":"089bbbff0f2eab3f8b73b35b195d8721","permalink":"https://forrt.org/curated_resources/reviewer-bias-against-replication-resear/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/reviewer-bias-against-replication-resear/","section":"curated_resources","summary":"Social science journal reviewers (N=8) responded to questionnaires regarding their reviewing history, and attitudes towards and perception of replication studies. Results indicate that reviewers are biased against replication studies and toward studies demonstrating some new effects. Several reviewers indicated that replications are a waste of time and journal space.","tags":["Reproducibility Crisis and Credibility Revolution","Open Science"],"title":"Reviewer Bias Against Replication Research","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1689122843,"objectID":"9f076c3c936d85952953e6a3034fa0ab","permalink":"https://forrt.org/curated_resources/rewarding-replications-a-sure-and-simple/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/rewarding-replications-a-sure-and-simple/","section":"curated_resources","summary":"Although replications are vital to scientific progress, psychologists rarely engage in systematic replication efforts. In this article, we consider psychologists’ narrative approach to scientific publications as an underlying reason for this neglect and propose an incentive structure for replications within psychology. First, researchers need accessible outlets for publishing replications. To accomplish this, psychology journals could publish replication reports in files that are electronically linked to reports of the original research. Second, replications should get cited. This can be achieved by cociting replications along with original research reports. Third, replications should become a valued collaborative effort. This can be realized by incorporating replications in teaching programs and by stimulating adversarial collaborations. The proposed incentive structure for replications can be developed in a relatively simple and cost-effective manner. By promoting replications, this incentive structure may greatly enhance the dependability of psychology’s knowledge base.","tags":["Reproducibility Crisis and Credibility Revolution","Open Science"],"title":"Rewarding Replications: A Sure and Simple Way to Improve Psychological Science","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"6598f1a81b512ea31e30b476e5502ceb","permalink":"https://forrt.org/curated_resources/richard-mcelreath/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/richard-mcelreath/","section":"curated_resources","summary":"A series of youtube videos about his book on statistical rethinking and Bayesian statistics","tags":["Bayesian statistics"],"title":"Richard McElreath","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"3cfea38b54c1017897488e6de3d4dadd","permalink":"https://forrt.org/curated_resources/rigor-and-reproducibility/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/rigor-and-reproducibility/","section":"curated_resources","summary":"The information provided on this website is designed to assist the extramural community in addressing rigor and transparency in NIH grant applications and progress reports. Scientific rigor and transparency in conducting biomedical research is key to the successful application of knowledge toward improving health outcomes.\n \nDefinition\nScientific rigor is the strict application of the scientific method to ensure unbiased and well-controlled experimental design, methodology, analysis, interpretation and reporting of results. \n \nGoals\nThe NIH strives to exemplify and promote the highest level of scientific integrity, public accountability, and social responsibility in the conduct of science. Grant applications instructions and the criteria by which reviewers are asked to evaluate the scientific merit of the application are intended to:\n \nâ€¢ ensure that NIH is funding the best and most rigorous science,\nâ€¢ highlight the need for applicants to describe details that may have been previously overlooked,\nâ€¢ highlight the need for reviewers to consider such details in their reviews through updated review language, and\nâ€¢ minimize additional burden.","tags":["Policy","Reproducibility"],"title":"Rigor and Reproducibility","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"03125e18a1455e16fcc2d44c3262519e","permalink":"https://forrt.org/curated_resources/rigor-champions-and-resources/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/rigor-champions-and-resources/","section":"curated_resources","summary":"Efforts to Instill the Fundamental Principles of Rigorous ResearchRigorous experimental procedures and transparent reporting of research results are vital to the continued success of the biomedical enterprise at both the preclinical and the clinical levels; therefore, NINDS convened major stakeholders in October 2018 to discuss how best to encourage rigorous biomedical research practices. The attendees discussed potential improvements to current training resources meant to instill the principles of rigorous research in current and future scientists, ideal attributes of a potential new educational resource, and cultural factors needed to ensure the success of such training. Please see the event website for more information about this workshop, including video recordings of the discussion, or the recent publication summarizing the workshop.Rigor ChampionsAs described in this publication, enthusiastic individuals (\"champions\") who want to drive improvements in rigorous research practices, transparent reporting, and comprehensive education may come from all career stages and sectors, including undergraduate students, graduate students, postdoctoral fellows, researchers, educators, institutional leaders, journal editors, scientific societies, private industry, and funders. We encouraged champions to organize themselves into intra- and inter-institutional communities to effect change within and across scientific institutions. These communities can then share resources and best practices, propose changes to current training and research infrastructure, build new tools to support better research practices, and support rigorous research on a daily basis.If you are interested learning more, you can join this grassroots online workspace or email us at RigorChampions@nih.gov.Rigor ResourcesIn order to understand the current landscape of training in the principles of rigorous research, NINDS is gathering a list of public resources that are, or can be made, freely accessible to the scientific community and beyond. We hope that compiling these resources will help identify gaps in training and stimulate discussion about proposed improvements and the building of new resources that facilitate training in transparency and other rigorous research practices. Please peruse the resources compiled thus far below, and contact us at RigorChampions@nih.gov to let us know about other potential resources.NINDS does not endorse any of these resources and leaves it to the scientific community to judge their quality.Resources TableCategories of resources listed in the table include Books and Articles, Guidelines and Protocols, Organizations and Training Programs, Software and Other Digital Resources, and Videos and Courses.","tags":["Reproducibility","Research Administration","Research Integrity","Rigor"],"title":"Rigor Champions and Resources","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"6ffc5fe5d4781e4b1141ba2fae55a52e","permalink":"https://forrt.org/curated_resources/rigorous-reproducible-research-practices/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/rigorous-reproducible-research-practices/","section":"curated_resources","summary":"How do you know whether the quantitative research you’re consuming and producing is rigorous and reproducible? This course will draw on contemporary perspectives to help you answer this question. We'll discuss the whys and hows of statistical inference and transparent research practices (e.g., sample size planning, preregistration, sharing data and materials), and best practices for reporting and evaluating research. We'll also consider thorny issues surrounding conducting and evaluating replication research, giving and responding to scientific criticism, and the everyday incentives that shape scientists' behavior. Students will come away having developed a principled understanding of relevant concepts and a set of concrete tools for producing and consuming high quality science. ","tags":["Reproducibility Crisis and Credibility Revolution","Open Science","Ethics"],"title":"Rigorous \u0026 Reproducible Research Practices","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"b411b7fec4337454134da2eb7cf0a133","permalink":"https://forrt.org/curated_resources/riot-science-club/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/riot-science-club/","section":"curated_resources","summary":"A videos that discusses world-leading open science","tags":["Open science"],"title":"RIOT science club","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1721390377,"objectID":"40c556ef2cb3ed634810f35ce7fa8b1c","permalink":"https://forrt.org/glossary/english/riot_science_club/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/english/riot_science_club/","section":"glossary","summary":"","tags":null,"title":"RIOT Science Club","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1720865851,"objectID":"d0305675922fa8e501fe13b86b4ade6c","permalink":"https://forrt.org/glossary/german/riot_science_club/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/german/riot_science_club/","section":"glossary","summary":"","tags":null,"title":"RIOT Science Club","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1635961284,"objectID":"1b96a9060db64305c2b6f7ee80afe066","permalink":"https://forrt.org/glossary/vbeta/riot-science-club/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/vbeta/riot-science-club/","section":"glossary","summary":"","tags":null,"title":"RIOT Science Club","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1689122843,"objectID":"b5944e45483f9a7f2ea14c1d94fd92d3","permalink":"https://forrt.org/curated_resources/riposte-a-framework-for-improving-the-de/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/riposte-a-framework-for-improving-the-de/","section":"curated_resources","summary":"Lack of reproducibility is an ongoing problem in some areas of the biomedical sciences. Poor experimental design and a failure to engage with experienced statisticians at key stages in the design and analysis of experiments are two factors that contribute to this problem. The RIPOSTE (Reducing IrreProducibility in labOratory STudiEs) framework has been developed to support early and regular discussions between scientists and statisticians in order to improve the design, conduct and analysis of laboratory studies and, therefore, to reduce irreproducibility. This framework is intended for use during the early stages of a research project, when specific questions or hypotheses are proposed. The essential points within the framework are explained and illustrated using three examples (a medical equipment test, a macrophage study and a gene expression study). Sound study design minimises the possibility of bias being introduced into experiments and leads to higher quality research with more reproducible results.","tags":["Reproducibility Crisis and Credibility Revolution","Open Science"],"title":"RIPOSTE: A Framework for Improving the Design and Analysis of Laboratory-Based Research","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"0e3f4cda1f726c525101a3083d5fa17b","permalink":"https://forrt.org/curated_resources/risk-of-bias-in-reports-of-in-vivo-resea/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/risk-of-bias-in-reports-of-in-vivo-resea/","section":"curated_resources","summary":"The reliability of experimental findings depends on the rigour of experimental design. Here we show limited reporting of measures to reduce the risk of bias in a random sample of life sciences publications, significantly lower reporting of randomisation in work published in journals of high impact, and very limited reporting of measures to reduce the risk of bias in publications from leading United Kingdom institutions. Ascertainment of differences between institutions might serve both as a measure of research quality and as a tool for institutional efforts to improve research quality.","tags":["Bibliometrics","Experimental Design","Publishing","Research Assessment","Research Quality Assessment","Research Validity","Scientific Publishing","Scientists","Systematic Reviews"],"title":"Risk of Bias in Reports of In Vivo Research: A Focus for Improvement","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"ccb3fc406cfe304c03ceecbe26657dcb","permalink":"https://forrt.org/curated_resources/robust-modeling-in-cognitive-science/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/robust-modeling-in-cognitive-science/","section":"curated_resources","summary":"In an attempt to increase the reliability of empirical findings, psychological scientists have recently proposed a number of changes in the practice of experimental psychology. Most current reform efforts have focused on the analysis of data and the reporting of findings for empirical studies. However, a large contingent of psychologists build models that explain psychological processes and test psychological theories using formal psychological models. Some, but not all, recommendations borne out of the broader reform movement bear upon the practice of behavioral or cognitive modeling. In this article, we consider which aspects of the current reform movement are relevant to psychological modelers, and we propose a number of techniques and practices aimed at making psychological modeling more transparent, trusted, and robust.","tags":["Robustness","Cognitive Modeling","Open Science","Reproducibility"],"title":"Robust Modeling in Cognitive Science","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1721390377,"objectID":"929ff4f2e342df956f2ffe353d3628f0","permalink":"https://forrt.org/glossary/english/robustness/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/english/robustness/","section":"glossary","summary":"","tags":null,"title":"Robustness (analyses)","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1635961284,"objectID":"9f677820d4df731cce9bc76442056ab9","permalink":"https://forrt.org/glossary/vbeta/robustness-analyses/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/vbeta/robustness-analyses/","section":"glossary","summary":"","tags":null,"title":"Robustness (analyses)","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1720865851,"objectID":"8f45473dc341e3f1b231200b0081a2ae","permalink":"https://forrt.org/glossary/german/robustness/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/german/robustness/","section":"glossary","summary":"","tags":null,"title":"Robustness (analyses) (Robustheit(-sanalysen))","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"51ccd8d4c5220ff82061347e57e14777","permalink":"https://forrt.org/curated_resources/rpm-an-open-source-rotation-platform-for/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/rpm-an-open-source-rotation-platform-for/","section":"curated_resources","summary":"Head fixation allows the recording and presentation of controlled stimuli and is used to study neural processes underlying spatial navigation. However, it disrupts the head direction system because of the lack of vestibular stimulation. To overcome this limitation, we developed a novel rotation platform which can be driven by the experimenter (open-loop) or by animal movement (closed-loop). The platform is modular, affordable, easy to build and open source. Additional modules presented here include cameras for monitoring eye movements, visual virtual reality, and a micro-manipulator for positioning various probes for recording or optical interference. We demonstrate the utility of the platform by recording eye movements and showing the robust activation of head-direction cells. This novel experimental apparatus combines the advantages of head fixation and intact vestibular activity in the horizontal plane. The open-loop mode can be used to study e.g., vestibular sensory representation and processing, while the closed-loop mode allows animals to navigate in rotational space, providing a better substrate for 2-D navigation in virtual environments. The full build documentation is maintained at https://ranczlab.github.io/RPM/.","tags":["Vestibular","Virtual Reality","Navigation","Sensory Processing","Sensorimotor Processing","Eye Movements","Open Science","Open Hardware"],"title":"RPM: An open-source Rotation Platform for open- and closed-loop vestibular stimulation in head-fixed Mice","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"db29e2764b07f50b50ee4e4e91897458","permalink":"https://forrt.org/curated_resources/rstudio-cheatsheets/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/rstudio-cheatsheets/","section":"curated_resources","summary":"RStudio Cheatsheets\n\nThe cheatsheets below make it easy to use some of our favorite packages. Cheatsheets include the following topics: \n\nPython with R and Reticulate Cheatsheet: The reticulate package provides a comprehensive set of tools for interoperability between Python and R. With reticulate, you can call Python from R in a variety of ways including importing Python modules into R scripts, writing R Markdown Python chunks, sourcing Python scripts, and using Python interactively within the RStudio IDE. This cheatsheet will remind you how. \n\nFactors with forcats Cheatsheet: Factors are R’s data structure for categorical data. The forcats package makes it easy to work with factors. This cheatsheet reminds you how to make factors, reorder their levels, recode their values, and more.\n\nTidy Evaluation with rlang Cheatsheet: Tidy Evaluation (Tidy Eval) is a framework for doing non-standard evaluation in R that makes it easier to program with tidyverse functions. Non-standard evaluation, better thought of as “delayed evaluation,” lets you capture a user’s R code to run later in a new environment or against a new data frame. The tidy evaluation framework is implemented by the rlang package and used by functions throughout the tidyverse. \n\nDeep Learning with Keras Cheatsheet: Keras is a high-level neural networks API developed with a focus on enabling fast experimentation. Keras supports both convolution based networks and recurrent networks (as well as combinations of the two), runs seamlessly on both CPU and GPU devices, and is capable of running on top of multiple back-ends including TensorFlow, CNTK, and Theano. \n\nDates and Times Cheatsheet: Lubridate makes it easier to work with dates and times in R. This lubridate cheatsheet covers how to round dates, work with time zones, extract elements of a date or time, parse dates into R and more. The back of the cheatsheet describes lubridate’s three timespan classes: periods, durations, and intervals; and explains how to do math with date-times. \n\nWork with Strings Cheatsheet: The stringr package provides an easy to use toolkit for working with strings, i.e. character data, in R. This cheatsheet guides you through stringr’s functions for manipulating strings. The back page provides a concise reference to regular expresssions, a mini-language for describing, finding, and matching patterns in strings. \n\nApply Functions Cheatsheet: The purrr package makes it easy to work with lists and functions. This cheatsheet will remind you how to manipulate lists with purrr as well as how to apply functions iteratively to each element of a list or vector. The back of the cheatsheet explains how to work with list-columns. With list columns, you can use a simple data frame to organize any collection of objects in R. \n\nData Import Cheatsheet: The Data Import cheatsheet reminds you how to read in flat files with http://readr.tidyverse.org/, work with the results as tibbles, and reshape messy data with tidyr. Use tidyr to reshape your tables into tidy data, the data format that works the most seamlessly with R and the tidyverse. \n\nData Transformation Cheatsheet: dplyr provides a grammar for manipulating tables in R. This cheatsheet will guide you through the grammar, reminding you how to select, filter, arrange, mutate, summarise, group, and join data frames and tibbles. \n\nSparklyr Cheatsheet: Sparklyr provides an R interface to Apache Spark, a fast and general engine for processing Big Data. With sparklyr, you can connect to a local or remote Spark session, use dplyr to manipulate data in Spark, and run Spark’s built in machine learning algorithms. \n\nR Markdown Cheatsheet: R Markdown is an authoring format that makes it easy to write reusable reports with R. You combine your R code with narration written in markdown (an easy-to-write plain text format) and then export the results as an html, pdf, or Word file. You can even use R Markdown to build interactive documents and slideshows. \n\nRStudio IDE Cheatsheet: The RStudio IDE is the most popular integrated development environment for R. Do you want to write, run, and debug your own R code? Work collaboratively on R projects with version control? Build packages or create documents and apps? No matter what you do with R, the RStudio IDE can help you do it faster. This cheatsheet will guide you through the most useful features of the IDE, as well as the long list of keyboard shortcuts built into the RStudio IDE. \n\nShiny Cheatsheet: If you’re ready to build interactive web apps with R, say hello to Shiny. This cheatsheet provides a tour of the Shiny package and explains how to build and customize an interactive app. Be sure to follow the links on the sheet for even more information. \n\nData Visualization Cheatsheet: The ggplot2 package lets you make beautiful and customizable plots of your data. It implements the grammar of graphics, an easy to use system for building plots. See docs.ggplot2.org for detailed examples. \n\nPackage Development Cheatsheet: The devtools package makes it easy to build your own R packages, and packages make it easy to share your R code. Supplement this cheatsheet with r-pkgs.had.co.nz, Hadley’s book on package development.","tags":["Inside Your Classroom","Open Scholarship Tools and Technologies","R","Reproducibility","Researchers","RStudio","Workflow Tools"],"title":"RStudio Cheatsheets","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1689122843,"objectID":"9181e83d0e7fefcc29583719942c8349","permalink":"https://forrt.org/curated_resources/safeguard-power-as-a-protection-against/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/safeguard-power-as-a-protection-against/","section":"curated_resources","summary":"An essential first step in planning a confirmatory or a replication study is to determine the sample size necessary to draw statistically reliable inferences using power analysis. A key problem, however, is that what is available is the sample-size estimate of the effect size, and its use can lead to severely underpowered studies when the effect size is overestimated. As a potential remedy, we introduce safeguard power analysis, which uses the uncertainty in the estimate of the effect size to achieve a better likelihood of correctly identifying the population effect size. Using a lower-bound estimate of the effect size, in turn, allows researchers to calculate a sample size for a replication study that helps protect it from being underpowered. We show that in most common instances, compared with nominal power, safeguard power is higher whereas standard power is lower. We additionally recommend the use of safeguard power analysis to evaluate the strength of the evidence provided by the original study.","tags":[""],"title":"Safeguard Power as a Protection Against Imprecise Power Estimates","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1689122843,"objectID":"6f9162379d5ef1065f6384035a8f06c3","permalink":"https://forrt.org/curated_resources/sailing-from-the-seas-of-chaos-into-the/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/sailing-from-the-seas-of-chaos-into-the/","section":"curated_resources","summary":"Recent events have led psychologists to acknowledge that the inherent uncertainty encapsulated in an inductive science is amplified by problematic research practices. In this article, we provide a practical introduction to recently developed statistical tools that can be used to deal with these uncertainties when performing and evaluating research. In Part 1, we discuss the importance of accurate and stable effect size estimates as well as how to design studies to reach a corridor of stability around effect size estimates. In Part 2, we explain how, given uncertain effect size estimates, well-powered studies can be designed with sequential analyses. In Part 3, we (a) explain what p values convey about the likelihood that an effect is true, (b) illustrate how the v statistic can be used to evaluate the accuracy of individual studies, and (c) show how the evidential value of multiple studies can be examined with a p-curve analysis. We end by discussing the consequences of incorporating our recommendations in terms of a reduced quantity, but increased quality, of the research output. We hope that the practical recommendations discussed in this article will provide researchers with the tools to make important steps toward a psychological science that allows researchers to differentiate among all possible truths on the basis of their likelihood.","tags":[""],"title":"Sailing From the Seas of Chaos Into the Corridor of Stability: Practical Recommendations to Increase the Informational Value of Studies","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1721390377,"objectID":"d3cec9b5ce9c052be6b998e20b1cd2c2","permalink":"https://forrt.org/glossary/english/salami_slicing/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/english/salami_slicing/","section":"glossary","summary":"","tags":null,"title":"Salami slicing","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1635961284,"objectID":"f7594ae785901fec291f3333b25aa422","permalink":"https://forrt.org/glossary/vbeta/salami-slicing/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/vbeta/salami-slicing/","section":"glossary","summary":"","tags":null,"title":"Salami slicing","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1720865851,"objectID":"bb4c0f50f46b684eef23d666809d7e3a","permalink":"https://forrt.org/glossary/german/salami_slicing/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/german/salami_slicing/","section":"glossary","summary":"","tags":null,"title":"Salami slicing (Salamischneiden)","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1689122843,"objectID":"32118d3c26ef9e79788935ed3b30e5bb","permalink":"https://forrt.org/curated_resources/sample-size-in-psychological-research-ov/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/sample-size-in-psychological-research-ov/","section":"curated_resources","summary":"The American Psychological Association (APA) Task Force on Statistical Inference was formed in 1996 in response to a growing body of research demonstrating methodological issues that threatened the credibility of psychological research, and made recommendations to address them. One issue was the small, even dramatically inadequate, size of samples used in studies published by leading journals. The present study assessed the progress made since the Task Force's final report in 1999. Sample sizes reported in four leading APA journals in 1955, 1977, 1995, and 2006 were compared using nonparametric statistics, while data from the last two waves were fit to a hierarchical generalized linear growth model for more in-depth analysis. Overall, results indicate that the recommendations for increasing sample sizes have not been integrated in core psychological research, although results slightly vary by field. This and other implications are discussed in the context of current methodological critique and practice.","tags":[""],"title":"Sample Size in Psychological Research Over the Past 30 Years","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"d9bd788250e1565b5a244a9358f450bf","permalink":"https://forrt.org/curated_resources/sample-size-justification-shiny-app/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/sample-size-justification-shiny-app/","section":"curated_resources","summary":"This Shiny app accompanies the paper 'Sample Size Justification' by Daniël Lakens. You can download the pre-print of this article at PsyArXiV and any sections in this online form that are unclear are explained in the paper. You can help to improve this app by providing feedback or suggest additions by filling out this feedback form . Note that this app will not store the information you enter if you close or refresh you browser. You might want to write down answers in a local text file first. For a completed example, see here .\n\nThe main goal of this app and the accompanying paper is to guide you through an evaluation of the informational value of a planned study. After filling out this form you can download a report of your sample size justification.","tags":["Sample Size Justification","Study Planning"],"title":"Sample Size Justification Shiny App","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1689122843,"objectID":"0167bde78665b84f3f7cd25137e284f8","permalink":"https://forrt.org/curated_resources/sample-size-planning-for-statistical-pow/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/sample-size-planning-for-statistical-pow/","section":"curated_resources","summary":"This review examines recent advances in sample size planning, not only from the perspective of an individual researcher, but also with regard to the goal of developing cumulative knowledge. Psychologists have traditionally thought of sample size planning in terms of power analysis. Although we review recent advances in power analysis, our main focus is the desirability of achieving accurate parameter estimates, either instead of or in addition to obtaining sufficient power. Accuracy in parameter estimation (AIPE) has taken on increasing importance in light of recent emphasis on effect size estimation and formation of confidence intervals. The review provides an overview of the logic behind sample size planning for AIPE and summarizes recent advances in implementing this approach in designs commonly used in psychological research.","tags":[""],"title":"Sample Size Planning for Statistical Power and Accuracy in Parameter Estimation","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1689122843,"objectID":"bf6b0780a2a09912670f0152d53c09f4","permalink":"https://forrt.org/curated_resources/sample-size-planning-for-the-standardize/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/sample-size-planning-for-the-standardize/","section":"curated_resources","summary":"Methods for planning sample size (SS) for the standardized mean difference so that a narrow confidence interval (CI) can be obtained via the accuracy in parameter estimation (AIPE) approach are developed. One method plans SS so that the expected width of the CI is sufficiently narrow. A modification adjusts the SS so that the obtained CI is no wider than desired with some specified degree of certainty (e.g., 99% certain the 95% CI will be no wider than omega). The rationale of the AIPE approach to SS planning is given, as is a discussion of the analytic approach to CI formation for the population standardized mean difference. Tables with values of necessary SS are provided. The freely available Methods for the Behavioral, Educational, and Social Sciences (K. Kelley, 2006a) R (R Development Core Team, 2006) software package easily implements the methods discussed.","tags":[""],"title":"Sample Size Planning for the Standardized Mean Difference: Accuracy in Parameter Estimation via Narrow Confidence Intervals","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1689122843,"objectID":"36bfd48c4a03a9dd5e66561350d7b703","permalink":"https://forrt.org/curated_resources/sample-size-planning-for-more-accurate-s/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/sample-size-planning-for-more-accurate-s/","section":"curated_resources","summary":"The sample size necessary to obtain a desired level of statistical power depends in part on the population value of the effect size, which is, by definition, unknown. A common approach to sample-size planning uses the sample effect size from a prior study as an estimate of the population value of the effect to be detected in the future study. Although this strategy is intuitively appealing, effect-size estimates, taken at face value, are typically not accurate estimates of the population effect size because of publication bias and uncertainty. We show that the use of this approach often results in underpowered studies, sometimes to an alarming degree. We present an alternative approach that adjusts sample effect sizes for bias and uncertainty, and we demonstrate its effectiveness for several experimental designs. Furthermore, we discuss an open-source R package, BUCSS, and user-friendly Web applications that we have made available to researchers so that they can easily implement our suggested methods.","tags":[""],"title":"Sample-size planning for more accurate statistical power: A method adjusting sample effect sizes for publication bias and uncertainty. ","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1689122843,"objectID":"6d9294d1f195b6048a2bceaf253e4f88","permalink":"https://forrt.org/curated_resources/scanning-the-horizon-towards-transparent/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/scanning-the-horizon-towards-transparent/","section":"curated_resources","summary":"Functional neuroimaging techniques have transformed our ability to probe the neurobiological basis of behaviour and are increasingly being applied by the wider neuroscience community. However, concerns have recently been raised that the conclusions that are drawn from some human neuroimaging studies are either spurious or not generalizable. Problems such as low statistical power, flexibility in data analysis, software errors and a lack of direct replication apply to many fields, but perhaps particularly to functional MRI. Here, we discuss these problems, outline current and suggested best practices, and describe how we think the field should evolve to produce the most meaningful and reliable answers to neuroscientific questions.","tags":["Reproducibility Crisis and Credibility Revolution","Open Science"],"title":"Scanning the horizon: towards transparent and reproducible neuroimaging research","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1689122843,"objectID":"1db06bb248b8a4c93bfd24ab434d274a","permalink":"https://forrt.org/curated_resources/science-and-pseudoscience-bbc-radio-talk/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/science-and-pseudoscience-bbc-radio-talk/","section":"curated_resources","summary":"A podcast about science and pseudoscience","tags":["Podcast","Reproducibility Crisis and Credibility Revolution"],"title":"Science and Pseudoscience BBC Radio Talk","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"dd11050c8389f0347baf9f1606264aa7","permalink":"https://forrt.org/curated_resources/science-isn-t-broken/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/science-isn-t-broken/","section":"curated_resources","summary":"If you follow the headlines, your confidence in science may have taken a hit lately. Peer review? More like self-review. An investigation in November uncovered a scam in which researchers were rubber-stamping their own work, circumventing peer review at five high-profile publishers. Scientific journals? Not exactly a badge of legitimacy, given that the International Journal of Advanced Computer Technology recently accepted for publication a paper titled “Get Me Off Your Fucking Mailing List,” whose text was nothing more than those seven words, repeated over and over for 10 pages. Two other journals allowed an engineer posing as Maggie Simpson and Edna Krabappel to publish a paper, “Fuzzy, Homogeneous Configurations.” Revolutionary findings? Possibly fabricated. In May, a couple of University of California, Berkeley, grad students discovered irregularities in Michael LaCour’s influential paper suggesting that an in-person conversation with a gay person could change how people felt about same-sex marriage. The journal Science retracted the paper shortly after, when LaCour’s co-author could find no record of the data. Taken together, headlines like these might suggest that science is a shady enterprise that spits out a bunch of dressed-up nonsense. But I’ve spent months investigating the problems hounding science, and I’ve learned that the headline-grabbing cases of misconduct and fraud are mere distractions. The state of our science is strong, but it’s plagued by a universal problem: Science is hard — really fucking hard.","tags":["Metascience","Scientific Method","P-values","Science","Health"],"title":"Science isn't Broken","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"9c586392f42e97bd5c5022d8edc32c61","permalink":"https://forrt.org/curated_resources/science-literacy/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/science-literacy/","section":"curated_resources","summary":"Fake news or good science? In a world where we have access to unlimited information, it is hard to sift through the echo chamber of opinions fueled by emotions and personal biases, rather than scientific evidence. Science Literacy will teach you about the process of science, how to think critically, how to differentiate science from pseudoscience, how indigenous wisdom can inform science, how to understand and design a scientific study, and how to critically evaluate scientific communication in the media. Every module will build your new skill-base with real life examples, and at the end of each module you will have to apply these skills to scientific questions, talking points and controversies in the world. Warning: this course requires an open mind and the ability to self-reflect.","tags":["Scientific communication"],"title":"Science Literacy","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1689122843,"objectID":"79318c52e321f34fd83d5f59fed4bc53","permalink":"https://forrt.org/curated_resources/science-or-art-how-aesthetic-standards-g/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/science-or-art-how-aesthetic-standards-g/","section":"curated_resources","summary":"The current crisis in psychological research involves issues of fraud, replication, publication bias, and false positive results. I argue that this crisis follows the failure of widely adopted solutions to psychology’s similar crisis of the 1970s. The untouched root cause is an information-economic one: Too many studies divided by too few publication outlets equals a bottleneck. Articles cannot pass through just by showing theoretical meaning and methodological rigor; their results must appear to support the hypothesis perfectly. Consequently, psychologists must master the art of presenting perfect-looking results just to survive in the profession. This favors aesthetic criteria of presentation in a way that harms science’s search for truth. Shallow standards of statistical perfection distort analyses and undermine the accuracy of cumulative data; narrative expectations encourage dishonesty about the relationship between results and hypotheses; criteria of novelty suppress replication attempts. Concerns about truth in research are emerging in other sciences and may eventually descend on our heads in the form of difficult and insensitive regulations. I suggest a more palatable solution: to open the bottleneck, putting structures in place to reward broader forms of information sharing beyond the exquisite art of present-day journal publication.","tags":["Reproducibility Crisis and Credibility Revolution","Open Science"],"title":"Science or art? How aesthetic standards grease the way through the publication bottleneck but undermine science.","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1689122843,"objectID":"4ec6c1a9579ff2d1c8f8dd8b1fdcc36f","permalink":"https://forrt.org/curated_resources/scientific-apophenia-in-strategic-manage/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/scientific-apophenia-in-strategic-manage/","section":"curated_resources","summary":"This article uses distributional matching and posterior predictive checks to estimate the extent of false and inflated findings in empirical research on strategic management. Based on a sample of 300 papers in top outlets for research on strategic management, we estimate that if each study were repeated, 24–40 percent of significant coefficients would become insignificant at the five percent level. Our best guess is that for about half of these, the true coefficient is very close to 0. The remaining coefficients are likely directionally correct but inflated in magnitude. We offer several practical individual and field level suggestions for reducing scientific apophenia, that is, our tendency to find and publish evidence of order where none exists.","tags":["Reproducibility Crisis and Credibility Revolution","Open Science"],"title":"Scientific apophenia in strategic management research: Significance tests \u0026 mistaken inference","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"5c9b2563e5771cf3ab5a10e1dfade45f","permalink":"https://forrt.org/curated_resources/scientific-utopia-ii-restructuring-incen_2/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/scientific-utopia-ii-restructuring-incen_2/","section":"curated_resources","summary":"An academic scientist’s professional success depends on publishing. Publishing norms emphasize novel, positive results. As such, disciplinary incentives encourage design, analysis, and reporting decisions that elicit positive results and ignore negative results. Prior reports demonstrate how these incentives inflate the rate of false effects in published science. When incentives favor novelty over replication, false results persist in the literature unchallenged, reducing efficiency in knowledge accumulation. Previous suggestions to address this problem are unlikely to be effective. For example, a journal of negative results publishes otherwise unpublishable reports. This enshrines the low status of the journal and its content. The persistence of false findings can be meliorated with strategies that make the fundamental but abstract accuracy motive—getting it right—competitive with the more tangible and concrete incentive—getting it published. This article develops strategies for improving scientific practices and knowledge accumulation that account for ordinary human motivations and biases","tags":["Reproducibility Crisis and Credibility Revolution","Open Science"],"title":"Scientific Utopia II. Restructuring incentives and practices to promote truth over publishability.","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1689122843,"objectID":"686039fd4131c94e5e5d20d1824f3007","permalink":"https://forrt.org/curated_resources/scientific-utopia-i-opening-scientific-c/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/scientific-utopia-i-opening-scientific-c/","section":"curated_resources","summary":"Existing norms for scientific communication are rooted in anachronistic practices of bygone eras making them needlessly inefficient. We outline a path that moves away from the existing model of scientific communication to improve the efficiency in meeting the purpose of public science—knowledge accumulation. We call for six changes: (a) full embrace of digital communication; (b) open access to all published research; (c) disentangling publication from evaluation; (d) breaking the “one article, one journal” model with a grading system for evaluation and diversified dissemination outlets; (e) publishing peer review; and (f) allowing open, continuous peer review. We address conceptual and practical barriers to change and provide examples showing how the suggested practices are being used already. The critical barriers to change are not technical or financial; they are social. Although scientists guard the status quo, they also have the power to change it.","tags":["Reproducibility Crisis and Credibility Revolution","Open Science"],"title":"Scientific Utopia: I. Opening Scientific Communication","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1689122843,"objectID":"1c3d43dc4c76ae865fee2cbe94b875e7","permalink":"https://forrt.org/curated_resources/scientific-utopia-ii-restructuring-incen/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/scientific-utopia-ii-restructuring-incen/","section":"curated_resources","summary":"An academic scientist’s professional success depends on publishing. Publishing norms emphasize novel, positive results. As such, disciplinary incentives encourage design, analysis, and reporting decisions that elicit positive results and ignore negative results. Prior reports demonstrate how these incentives inflate the rate of false effects in published science. When incentives favor novelty over replication, false results persist in the literature unchallenged, reducing efficiency in knowledge accumulation. Previous suggestions to address this problem are unlikely to be effective. For example, a journal of negative results publishes otherwise unpublishable reports. This enshrines the low status of the journal and its content. The persistence of false findings can be meliorated with strategies that make the fundamental but abstract accuracy motive—getting it right—competitive with the more tangible and concrete incentive—getting it published. This article develops strategies for improving scientific practices and knowledge accumulation that account for ordinary human motivations and biases.","tags":["Policy","Reproducibility"],"title":"Scientific Utopia: II. Restructuring Incentives and Practices to Promote Truth Over Publishability","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1689122843,"objectID":"83ee715bfb5fbc3ef1ef50de19e76b31","permalink":"https://forrt.org/curated_resources/scientists-reputations-are-based-on-gett/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/scientists-reputations-are-based-on-gett/","section":"curated_resources","summary":"Replication is vital for increasing precision and accuracy of scientific claims. However, when replications “succeed” or “fail,” they could have reputational consequences for the claim’s originators. Surveys of United States adults (N = 4,786), undergraduates (N = 428), and researchers (N = 313) showed that reputational assessments of scientists were based more on how they pursue knowledge and respond to replication evidence, not whether the initial results were true. When comparing one scientist that produced boring but certain results with another that produced exciting but uncertain results, opinion favored the former despite researchers’ belief in more rewards for the latter. Considering idealized views of scientific practices offers an opportunity to address incentives to reward both innovation and verification.","tags":["Reproducibility Crisis and Credibility Revolution","Open Science"],"title":"Scientists’ Reputations Are Based on Getting It Right, Not Being Right","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1721390377,"objectID":"16461faa130d67fb0705bde4cc50066b","permalink":"https://forrt.org/glossary/english/scooping/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/english/scooping/","section":"glossary","summary":"","tags":null,"title":"Scooping","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1635961284,"objectID":"218e2ace184492737841e0670a39e4ab","permalink":"https://forrt.org/glossary/vbeta/scooping/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/vbeta/scooping/","section":"glossary","summary":"","tags":null,"title":"Scooping","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1720865851,"objectID":"f0e8520ea469f04f3288178d8c362a89","permalink":"https://forrt.org/glossary/german/scooping/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/german/scooping/","section":"glossary","summary":"","tags":null,"title":"Scooping (Untergrabung)","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"477e7f257b8dca9e42745a3323e5c280","permalink":"https://forrt.org/curated_resources/secondary-data-preregistration/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/secondary-data-preregistration/","section":"curated_resources","summary":"Preregistration is the process of specifying project details, such as hypotheses, data collection procedures, and analytical decisions, prior to conducting a study. It is designed to make a clearer distinction between data-driven, exploratory work and a-priori, confirmatory work. Both modes of research are valuable, but are easy to unintentionally conflate. See the Preregistration Revolution for more background and recommendations.\n\nFor research that uses existing datasets, there is an increased risk of analysts being biased by preliminary trends in the dataset. However, that risk can be balanced by proper blinding to any summary statistics in the dataset and the use of hold out datasets (where the \"training\" and \"validation\" datasets are kept separate from each other). See this page for specific recommendations about \"split samples\" or \"hold out\" datasets. Finally, if those procedures are not followed, disclosure of possible biases can inform the researcher and her audience about the proper role any results should have (i.e. the results should be deemed mostly exploratory and ideal for additional confirmation).\n\nThis project contains a template for creating your preregistration, designed specifically for research using existing data. In the future, this template will be integrated into the OSF.","tags":["Aging Science","Open Science","Secondary Data Preregistration"],"title":"Secondary Data Preregistration","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1721390377,"objectID":"ae168ba1111955bab12aa6030a02dbbd","permalink":"https://forrt.org/glossary/english/semantometrics/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/english/semantometrics/","section":"glossary","summary":"","tags":null,"title":"Semantometrics","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1635961284,"objectID":"59a32abcefb1ec740ba4fbd6855554dd","permalink":"https://forrt.org/glossary/vbeta/semantometrics/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/vbeta/semantometrics/","section":"glossary","summary":"","tags":null,"title":"Semantometrics ","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1720865851,"objectID":"1bb57a74eac07bc83f9c6780a564d357","permalink":"https://forrt.org/glossary/german/semantometrics/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/german/semantometrics/","section":"glossary","summary":"","tags":null,"title":"Semantometrics (Semantometrie)","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1721390377,"objectID":"cc0542a41c9825a374b4774285a18e97","permalink":"https://forrt.org/glossary/english/sensitive_research/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/english/sensitive_research/","section":"glossary","summary":"","tags":null,"title":"Sensitive research","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1635961284,"objectID":"a1e65dd97c1f0dd10022d5b8ae4fa145","permalink":"https://forrt.org/glossary/vbeta/sensitive-research/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/vbeta/sensitive-research/","section":"glossary","summary":"","tags":null,"title":"Sensitive research","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1720865851,"objectID":"7bc071a03581febc99bcd09dc4a572a2","permalink":"https://forrt.org/glossary/german/sensitive_research/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/german/sensitive_research/","section":"glossary","summary":"","tags":null,"title":"Sensitive research (Sensitive Forschung)","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1721390377,"objectID":"bdb25e8f026fc00c5dbc79b7d66af915","permalink":"https://forrt.org/glossary/english/sequence_determines_credit_approach/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/english/sequence_determines_credit_approach/","section":"glossary","summary":"","tags":null,"title":"Sequence-determines-credit approach (SDC)","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"16719bdb3ce7aa6348e72eb0e30eb340","permalink":"https://forrt.org/glossary/german/sequence_determines_credit-approach/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/german/sequence_determines_credit-approach/","section":"glossary","summary":"","tags":null,"title":"Sequence-determines-credit approach (SDC)","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1635961284,"objectID":"f98e4bbf09fba9d68d81d96966d3d84b","permalink":"https://forrt.org/glossary/vbeta/sequence-determines-credit-approach/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/vbeta/sequence-determines-credit-approach/","section":"glossary","summary":"","tags":null,"title":"Sequence-determines-credit approach (SDC) ","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"80a0d99f7a4ec5219a5686337d792912","permalink":"https://forrt.org/curated_resources/shall-we-really-do-it-again-the-powerful/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/shall-we-really-do-it-again-the-powerful/","section":"curated_resources","summary":"Replication is one of the most important tools for the verification of facts within the empirical sciences. A detailed examination of the notion of replication reveals that there are many different meanings to this concept and the relevant procedures, but hardly any systematic literature. This paper analyzes the concept of replication from a theoretical point of view. It demonstrates that the theoretical demands are scarcely met in everyday work within the social sciences. Some demands are just not feasible, whereas others are constricted by restrictions relating to publication. A new classification scheme based on a functional approach that distinguishes between different types of replication is proposed. Next, it will be argued that replication addresses the important connection between existing and new knowledge. To do so it has to be applied explicitly and systematically. The paper ends with a description of procedures how this could be done and a set of recommendations how to handle the concept of replication in the future to exploit its potential to the full.","tags":["Reproducibility Crisis and Credibility Revolution","Open Science"],"title":"Shall we really do it again? The powerful concept of replication is neglected in the social sciences","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"5dc36e76a3f9b913ddb0587abada4796","permalink":"https://forrt.org/curated_resources/sharing-detailed-research-data-is-associ/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/sharing-detailed-research-data-is-associ/","section":"curated_resources","summary":"Background Sharing research data provides benefit to the general scientific community, but the benefit is less obvious for the investigator who makes his or her data available. Principal Findings We examined the citation history of 85 cancer microarray clinical trial publications with respect to the availability of their data. The 48% of trials with publicly available microarray data received 85% of the aggregate citations. Publicly available data was significantly (p = 0.006) associated with a 69% increase in citations, independently of journal impact factor, date of publication, and author country of origin using linear regression. Significance This correlation between publicly available data and increased literature impact may further motivate investigators to share their detailed research data.","tags":["Bibliometrics","Cancers and Neoplasms","Citation Analysis","Clinical Trials (cancer Treatment)","Data","Internet","Linear Regression Analysis","Microarrays","Open Data","Scientific Publishing"],"title":"Sharing Detailed Research Data Is Associated with Increased Citation Rate","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1721390377,"objectID":"65b75b2e7bd2422fdbc65555104a1bcd","permalink":"https://forrt.org/glossary/english/sherpa_romeo/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/english/sherpa_romeo/","section":"glossary","summary":"","tags":null,"title":"Sherpa Romeo","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1720865851,"objectID":"d4acfb41228beac27f57e3afcf04cc05","permalink":"https://forrt.org/glossary/german/sherpa_romeo/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/german/sherpa_romeo/","section":"glossary","summary":"","tags":null,"title":"Sherpa Romeo","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1635961284,"objectID":"4a00f1be1b73c75fa14b26ca2fefb73a","permalink":"https://forrt.org/glossary/vbeta/sherpa-romeo/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/vbeta/sherpa-romeo/","section":"glossary","summary":"","tags":null,"title":"Sherpa Romeo","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1689122843,"objectID":"07c921be0757dce8d31bb76b3f4342c5","permalink":"https://forrt.org/curated_resources/short-r-script-to-plot-effect-sizes-cohe/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/short-r-script-to-plot-effect-sizes-cohe/","section":"curated_resources","summary":"This blog describes how to plot effect sizes (Cohen's d) and shade overlapping area with R scripts","tags":["Blog","Tutorial"],"title":"Short R script to plot effect sizes (Cohen's d) and share overlapping are","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1689122843,"objectID":"86ad92bae034b5ebbbafdbbd830c1909","permalink":"https://forrt.org/curated_resources/short-sweet-and-problematic-the-rise-of/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/short-sweet-and-problematic-the-rise-of/","section":"curated_resources","summary":"Our field has witnessed a rapid increase in the appeal and prevalence of the short report format over the last two decades. In this article, we discuss both the benefits and drawbacks of the trend toward shorter and faster publications. Although the short report format can help us cope with ever-increasing time constraints; ease the burden on hiring, promotion, and tenure committees; speed the publication of our findings; and promote the dissemination of research beyond the borders of our discipline, it can also exacerbate problems with publication bias and selective reporting, decrease theoretical integration within our science, and risk overemphasizing colorful effects relative to basic processes. In the face of these challenges, we believe it is essential to find ways to preserve the advantages of the short-and-fast approach while minimizing its disadvantages and while acknowledging the complementary and critical importance of longer articles in advancing the field.","tags":["Reproducibility Crisis and Credibility Revolution","Open Science"],"title":"Short, Sweet, and Problematic? The Rise of the Short Report in Psychological Science","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"8f412bd6081e6c038efcd4d2f09a19d5","permalink":"https://forrt.org/curated_resources/should-preregistration-of-epidemiologic/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/should-preregistration-of-epidemiologic/","section":"curated_resources","summary":"There is an ongoing debate regarding preregistration of epidemiologic study protocols. We examine the basic idea that preregistration of study protocols and their associated hypotheses would enhance the reliability of observational research. We define instances in which preregistration would be useful, and we support a counter-proposal: a public registry containing descriptions of collected epidemiologic data.","tags":["Epidemiology","Preregistration"],"title":"Should Preregistration of Epidemiologic Study Protocols Become Compulsory? Reflections and a Counterproposal","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"9cc0e3728b580bd68f27860138dc403d","permalink":"https://forrt.org/curated_resources/signaling-the-trustworthiness-of-science/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/signaling-the-trustworthiness-of-science/","section":"curated_resources","summary":"Trust in science increases when scientists and the outlets certifying their work honor science’s norms. Scientists often fail to signal to other scientists and, perhaps more importantly, the public that these norms are being upheld. They could do so as they generate, certify, and react to each other’s findings: for example, by promoting the use and value of evidence, transparent reporting, self-correction, replication, a culture of critique, and controls for bias. A number of approaches for authors and journals would lead to more effective signals of trustworthiness at the article level. These include article badging, checklists, a more extensive withdrawal ontology, identity verification, better forward linking, and greater transparency.","tags":["OSKB"],"title":"Signaling the trustworthiness of science","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1689122843,"objectID":"d7fc18b719fb8e38434238310c5822e9","permalink":"https://forrt.org/curated_resources/significance/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/significance/","section":"curated_resources","summary":"Visual comics about significance testing","tags":["Comic"],"title":"Significance","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1689122843,"objectID":"92d7ea6cea0c78e844f5dbc1ae47f5cf","permalink":"https://forrt.org/curated_resources/significance-tests-have-their-place/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/significance-tests-have-their-place/","section":"curated_resources","summary":"Null-hypothesis significance tests (NHST), properly used, tell us whether we have sufficient evidence to be confident of the sign of the population effect–but only if we abandon two-valued logic in favor of Kaiser's (1960) three-alternative hypothesis tests Confidence intervals provide a useful addition to NHSTs, and can be used to provide the same sign-determination function as NHST However, when so used, confidence intervals are subject to exactly the same Type I, II, and III error rates as NHST In addition, NHSTs provide two pieces of information about our data–maximum probability of a Type III error and probability of a successful exact replication–that confidence intervals do not The proposed alternative to NHST is just as susceptible to misinterpretation as is NHST The problem of bias due to censoring of data collection or publication can be handled by providing archives for all methodologically sound data sets, but reserving interpretations and conclusions for statistically significant results","tags":[""],"title":"Significance tests have their place","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1721390377,"objectID":"9c856dbbd24a756921b2cf619f0c148b","permalink":"https://forrt.org/glossary/english/single_blind_peer_review/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/english/single_blind_peer_review/","section":"glossary","summary":"","tags":null,"title":"Single-blind peer review","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1635961284,"objectID":"c2858870955ce1a73e95ffc35b65cc27","permalink":"https://forrt.org/glossary/vbeta/single-blind-peer-review/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/vbeta/single-blind-peer-review/","section":"glossary","summary":"","tags":null,"title":"Single-blind peer review","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1720865851,"objectID":"5997203f22303ad47a8e83bb950b1890","permalink":"https://forrt.org/glossary/german/single_blind_peer_review/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/german/single_blind_peer_review/","section":"glossary","summary":"","tags":null,"title":"Single-blind peer review (Einfachblinde Peer Begutachtung)","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1721390377,"objectID":"c3569b819026faf512750b5349482dc3","permalink":"https://forrt.org/glossary/english/slow_science/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/english/slow_science/","section":"glossary","summary":"","tags":null,"title":"Slow science","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1635961284,"objectID":"7fff2abbad330489978530d1b54fb05f","permalink":"https://forrt.org/glossary/vbeta/slow-science/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/vbeta/slow-science/","section":"glossary","summary":"","tags":null,"title":"Slow science","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1720865851,"objectID":"71af2fab41e4cb2ceec671094e3a599d","permalink":"https://forrt.org/glossary/german/slow_science/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/german/slow_science/","section":"glossary","summary":"","tags":null,"title":"Slow science (Langsame Forschung)","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1689122843,"objectID":"88daaf10cb3ac7f6afcdeab24ba58335","permalink":"https://forrt.org/curated_resources/small-telescopes-detectability-and-the-e/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/small-telescopes-detectability-and-the-e/","section":"curated_resources","summary":"This article introduces a new approach for evaluating replication results. It combines effect-size estimation with hypothesis testing, assessing the extent to which the replication results are consistent with an effect size big enough to have been detectable in the original study. The approach is demonstrated by examining replications of three well-known findings. Its benefits include the following: (a) differentiating “unsuccessful” replication attempts (i.e., studies yielding p \u003e .05) that are too noisy from those that actively indicate the effect is undetectably different from zero, (b) “protecting” true findings from underpowered replications, and (c) arriving at intuitively compelling inferences in general and for the revisited replications in particular","tags":["Reproducibility Crisis and Credibility Revolution","Open Science"],"title":"Small telescopes Detectability and the evaluation of replication results. ","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"39d0706f1f13cf082333f5d18d3103f1","permalink":"https://forrt.org/curated_resources/snapshot-reporting-practices-for-publish/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/snapshot-reporting-practices-for-publish/","section":"curated_resources","summary":"This checklist is intended to help scientists, reviewers, and editors prepare and assess manuscripts for inclusion of critical details relevant to work with pluripotent stem cells (PSCs) and tissue stem cells (TSCs) with the goal of increasing the rigor and reproducibility of research through reporting. It is essential that any published paper includes detailed information on the following parameters to increase the transparency of the experimental details and ensure that the published results are reproducible. For additional details on the recommendations, please see the specific sections of the ISSCR’s Standards for Human Stem Cell Use in Research referenced in the checklist (https://www.isscr.org/standards-document). All sections apply to PSCs and TSCs unless otherwise noted. To view this SnapShot, open or download the PDF.","tags":["Pluripotent Stem Cells","Tissue Stem Cells","Metadata","Culture","Genomic Characterization","Molecular Characterization","Experiment","Data"],"title":"SnapShot: Reporting practices for publishing results with human PSCs and tissue stem cells","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1721390377,"objectID":"4a2ae37d380b10c317b87b86dfb748c2","permalink":"https://forrt.org/glossary/english/social_class/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/english/social_class/","section":"glossary","summary":"","tags":null,"title":"Social class","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1635961284,"objectID":"75584520c03ebeecdcfa1e6a8ab4a33d","permalink":"https://forrt.org/glossary/vbeta/social-class/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/vbeta/social-class/","section":"glossary","summary":"","tags":null,"title":"Social class","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1720865851,"objectID":"7821912c5ed43040b94af0d2892b1d26","permalink":"https://forrt.org/glossary/german/social_class/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/german/social_class/","section":"glossary","summary":"","tags":null,"title":"Social class (Soziale Schicht)","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"645570a63774381fead0a2b1a46b1a01","permalink":"https://forrt.org/curated_resources/social-identity-and-morality-lab-teachin/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/social-identity-and-morality-lab-teachin/","section":"curated_resources","summary":"I believe that higher education should be focused on developing passionate, critical, independent and creative thought, and the transmission of knowledge should be in the service of developing these skills. In the classroom, I try to communicate the core issues and controversies within an area in an interactive fashion, drawing students into the material though exercises and debate. This experience provides a deeper understanding of the material and encourages students to think critically about how scientific conclusions depend on the research process. In addition, I try to teach case studies of scientific controversy on topical issues to illustrate that science is not simply an assembly of facts but a method for developing and testing ideas. In my experience, this approach has led naturally to classroom discussions where students share their own insights and critical perspectives.","tags":["Materials","Mentoring"],"title":"Social Identity and morality lab teaching","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1721390377,"objectID":"80ae62c5670aba617acf7d22c27d4584","permalink":"https://forrt.org/glossary/english/social_integration/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/english/social_integration/","section":"glossary","summary":"","tags":null,"title":"Social integration","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1635961284,"objectID":"293f55b5d7d939caec8fe71e5f27c091","permalink":"https://forrt.org/glossary/vbeta/social-integration/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/vbeta/social-integration/","section":"glossary","summary":"","tags":null,"title":"Social integration","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1720865851,"objectID":"3e7b5553582971f611a34ab320e16a2e","permalink":"https://forrt.org/glossary/german/social_integration/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/german/social_integration/","section":"glossary","summary":"","tags":null,"title":"Social integration (Soziale Integration)","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"3de8edb6618fd60ab103a50112cd8765","permalink":"https://forrt.org/curated_resources/social-science-workshop-overview/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/social-science-workshop-overview/","section":"curated_resources","summary":"Workshop overview for the Data Carpentry Social Sciences curriculum. Data Carpentry’s aim is to teach researchers basic concepts, skills, and tools for working with data so that they can get more done in less time, and with less pain. This workshop teaches data management and analysis for social science research including best practices for data organization in spreadsheets, reproducible data cleaning with OpenRefine, and data analysis and visualization in R. This curriculum is designed to be taught over two full days of instruction. Materials for teaching data analysis and visualization in Python and extraction of information from relational databases using SQL are in development. Interested in teaching these materials? We have an onboarding video and accompanying slides available to prepare Instructors to teach these lessons. After watching this video, please contact team@carpentries.org so that we can record your status as an onboarded Instructor. Instructors who have completed onboarding will be given priority status for teaching at centrally-organized Data Carpentry Social Sciences workshops.","tags":["Analysis","Data","Education","Open Scholarship Tools and Technologies","Python","R","Reproducibility","Research Data Management Tools","Researchers","SQL"],"title":"Social Science Workshop Overview","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"2b0b0ef11c67d37a1cafc819a3980332","permalink":"https://forrt.org/curated_resources/social-sciences-research-methods-centre/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/social-sciences-research-methods-centre/","section":"curated_resources","summary":"This workshop will introduce students to the process of reproducing published work. Replicating other scholars’ work is an essential tool for becoming familiar with methods, learning to select suitable models, and getting a chance to publish early during their academic career. This replication workshop will therefore provide students with a deeper understanding of statistical modeling and professionalism in their field. With the right amount of value added, a replication study can be submitted to a journal, as has been done by several students in the past.","tags":["Reproducibility Crisis and Credibility Revolution","Open Science"],"title":"Social Sciences Research Methods Centre Replication Workshop","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1689122843,"objectID":"4209cdfdd3082ee4608ed03adf3db652","permalink":"https://forrt.org/curated_resources/social-behavioral-and-economic-sciences/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/social-behavioral-and-economic-sciences/","section":"curated_resources","summary":"A paper about social, behavioural economic perspective on reproducible science","tags":["Reproducibility Crisis and Credibility Revolution","Open Science"],"title":"Social, Behavioral, and Economic Sciences Perspectives on Robust and Reliable Science","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1721390377,"objectID":"57c4db2624c642ea49a155ebe51111fe","permalink":"https://forrt.org/glossary/english/society_for_open__reliable__and_transparent_ecology_and_evolutionary_biology/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/english/society_for_open__reliable__and_transparent_ecology_and_evolutionary_biology/","section":"glossary","summary":"","tags":null,"title":"Society for Open, Reliable, and Transparent Ecology and Evolutionary biology (SORTEE)","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1720865851,"objectID":"17132f148e81796b28db2e1330315b59","permalink":"https://forrt.org/glossary/german/society_for_open__reliable__and_transparent_ecology_and_evolutionary_biology/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/german/society_for_open__reliable__and_transparent_ecology_and_evolutionary_biology/","section":"glossary","summary":"","tags":null,"title":"Society for Open, Reliable, and Transparent Ecology and Evolutionary biology (SORTEE)","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1635961284,"objectID":"9dda1121169a6393d3f1d2fb4dfa66d3","permalink":"https://forrt.org/glossary/vbeta/society-for-open-reliable-and-trans/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/vbeta/society-for-open-reliable-and-trans/","section":"glossary","summary":"","tags":null,"title":"Society for Open, Reliable, and Transparent Ecology and Evolutionary biology (SORTEE)","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1721390377,"objectID":"f53dd23ced3c144bd80b877355e2dcb6","permalink":"https://forrt.org/glossary/english/society_for_the_improvement_of_psychological_science/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/english/society_for_the_improvement_of_psychological_science/","section":"glossary","summary":"","tags":null,"title":"Society for the Improvement of Psychological Science (SIPS)","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1720865851,"objectID":"3b3cce752b899b151de61bcdf5f8bf75","permalink":"https://forrt.org/glossary/german/society_for_the_improvement_of_psychological_science/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/german/society_for_the_improvement_of_psychological_science/","section":"glossary","summary":"","tags":null,"title":"Society for the Improvement of Psychological Science (SIPS)","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1635961284,"objectID":"62e19de7fd21d4c3c83dc97a12e0a5df","permalink":"https://forrt.org/glossary/vbeta/society-for-the-improvement-of-psyc/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/vbeta/society-for-the-improvement-of-psyc/","section":"glossary","summary":"","tags":null,"title":"Society for the Improvement of Psychological Science (SIPS)","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"7d9c3b0cceac7ea4e34ad00275200fb2","permalink":"https://forrt.org/curated_resources/software-carpentry/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/software-carpentry/","section":"curated_resources","summary":"Since 1998, Software Carpentry has been teaching researchers the computing skills they need to get more done in less time and with less pain. Our volunteer instructors have run hundreds of events for more than 34,000 researchers since 2012. All of our lesson materials are freely reusable under the Creative Commons - Attribution license.","tags":["Analysis","Open Source Software","Reproducibility","Researchers","Version Control","Workflow Tools"],"title":"Software Carpentry","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"50afbedca079e08edd925dfe5c05fc5c","permalink":"https://forrt.org/curated_resources/sources-of-nonreplicability-in-aging-eth/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/sources-of-nonreplicability-in-aging-eth/","section":"curated_resources","summary":"The older adult population in the U.S. is becoming increasingly diverse across a constellation of factors including ethnoracial group, socioeconomic status, and immigration status. However, our understanding of the consequences of this diversity for cognitive and mental health is masked by the lack of inclusion of diverse sample characteristics, the use of assessments that might hold a different meaning for different groups of people, and analytical choices that do not probe the impact of diverse characteristics or assume an unwarranted degree of homogeneity within groups. Each of these factors not only hinders our ability to understand various psychological mechanisms that differ as a function of age but also threatens the likelihood of replicability across aging research studies. This article provides our perspective on three key sources of nonreplicability in ethnoracial health disparities research among older adults: (a) what is lost in creating monolithic groups rather than identifying subgroups of minorities, (b) understanding aging from the perspective of intersecting identities, and (c) biases of research materials. We also provide recommendations to increase replicability in aging research with respect to the challenges outlined. Approaching questions on aging from a health disparities lens can both increase the generalizability of research outcomes and improve initiatives of social justice that are long overdue.","tags":["Health Disparities","Nonreplicability","Replicability","Replication","Subgroup Analyses","Aging","Biases"],"title":"Sources of nonreplicability in aging ethnoracial health disparities research","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"d50579343194be9f6cc3ca1703ea5701","permalink":"https://forrt.org/curated_resources/sparc-popular-resources/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/sparc-popular-resources/","section":"curated_resources","summary":"SPARC is a global coalition committed to making Open the default for research and education. SPARC empowers people to solve big problems and make new discoveries through the adoption of policies and practices that advance Open Access, Open Data, and Open Education.","tags":["Funders","Governmental Policies","Librarians","Open Access","Publishers","Publishing","Researchers"],"title":"SPARC Popular Resources","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"acf4aba6ace75098c843f308d2819657","permalink":"https://forrt.org/curated_resources/special-issue-innovation-in-aging/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/special-issue-innovation-in-aging/","section":"curated_resources","summary":"Special Issue: Innovation in Aging\n","tags":["Aging Science"],"title":"Special Issue: Innovation in Aging","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"215a5549e0d3e293e00981686524fea9","permalink":"https://forrt.org/curated_resources/special-issue-preregistered-studies-of-p/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/special-issue-preregistered-studies-of-p/","section":"curated_resources","summary":"SPECIAL ISSUE: PREREGISTERED STUDIES OF PERSONALITY DEVELOPMENT AND AGING USING EXISTING DATA (Volume 76, Issue 1, January 2021)\n","tags":["Aging Science","Research"],"title":"Special Issue: Preregistered Studies of Personality Development and Aging Using Existing Data ","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1721390377,"objectID":"5f0c16dc5ba71db294d2520879892df4","permalink":"https://forrt.org/glossary/english/specification_curve_analysis/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/english/specification_curve_analysis/","section":"glossary","summary":"","tags":null,"title":"Specification Curve Analysis","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1635961284,"objectID":"ad966fec61ca194ac4574a417bec9838","permalink":"https://forrt.org/glossary/vbeta/specification-curve-analysis/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/vbeta/specification-curve-analysis/","section":"glossary","summary":"","tags":null,"title":"Specification Curve Analysis ","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1720865851,"objectID":"00b4ae906bc2053f3faa635f27ff6d55","permalink":"https://forrt.org/glossary/german/specification_curve_analysis/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/german/specification_curve_analysis/","section":"glossary","summary":"","tags":null,"title":"Specification Curve Analysis (Spezifikationskurvenanalyse)","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1689122843,"objectID":"a0a789943bc268ebd8b1086653d3841c","permalink":"https://forrt.org/curated_resources/spsp-experts-open-science/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/spsp-experts-open-science/","section":"curated_resources","summary":"A video about open science, pre-registration etc.","tags":["Video","Reproducibility Knowledge"],"title":"SPSP experts - open science","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1689122843,"objectID":"430245d518e9c63a3d11013fa67606d0","permalink":"https://forrt.org/curated_resources/spurious-correlations/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/spurious-correlations/","section":"curated_resources","summary":"A website detailing spurious correlations","tags":["Website"],"title":"Spurious Correlations","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1689122843,"objectID":"36d91f7a636ec59918c797eb4691402f","permalink":"https://forrt.org/curated_resources/standard-operating-procedures-a-safety-n/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/standard-operating-procedures-a-safety-n/","section":"curated_resources","summary":"Across the social sciences, growing concerns about research transparency have led to calls for pre-analysis plans (PAPs) that specify in advance how researchers intend to analyze the data they are about to gather. PAPs promote transparency and credibility by helping readers distinguish between exploratory and confirmatory analyses. However, PAPs are time-consuming to write and may fail to anticipate contingencies that arise in the course of data collection. This article proposes the use of “standard operating procedures” (SOPs)—default practices to guide decisions when issues arise that were not anticipated in the PAP. We offer an example of an SOP that can be adapted by other researchers seeking a safety net to support their PAPs.","tags":[""],"title":"Standard Operating Procedures: A Safety Net for Pre-Analysis Plans","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"d063b0fa38677dc7864bd2b856affe84","permalink":"https://forrt.org/curated_resources/star-wars-the-empirics-strike-back/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/star-wars-the-empirics-strike-back/","section":"curated_resources","summary":"Using 50,000 tests published in the AER, JPE, and QJE, we identify a residual in the distribution of tests that cannot be explained solely by journals favoring rejection of the null hypothesis. We observe a two-humped camel shape with missing p-values between 0.25 and 0.10 that can be retrieved just after the 0.05 threshold and represent 10-20 percent of marginally rejected tests. Our interpretation is that researchers inflate the value of just-rejected tests by choosing \"significant\" specifications. We propose a method to measure this residual and describe how it varies by article and author characteristics","tags":["Reproducibility Crisis and Credibility Revolution","Open Science"],"title":"Star wars: The empirics strike back","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1689122843,"objectID":"7afa487f5d2085cb224aa67649f97066","permalink":"https://forrt.org/curated_resources/stat-545/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/stat-545/","section":"curated_resources","summary":"This site is about everything that comes up during data analysis except for statistical modelling and inference. This might strike you as strange, given R’s statistical roots. First, let me assure you we believe that modelling and inference are important. But the world already offers a lot of great resources for doing statistics with R. The design of STAT 545 was motivated by the need to provide more balance in applied statistical training. Data analysts spend a considerable amount of time on project organization, data cleaning and preparation, and communication. These activities can have a profound effect on the quality and credibility of an analysis. Yet these skills are rarely taught, despite how important and necessary they are. STAT 545 aims to address this gap.","tags":["Tutorial"],"title":"Stat 545","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"fa404f60d5838cd25c0ad0de5aa3e404","permalink":"https://forrt.org/curated_resources/statcheck/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/statcheck/","section":"curated_resources","summary":"statcheck is a program that checks for errors in statistical reporting in APA-formatted documents. It was originally written in the R programming language. statcheck/web is a web-based implementation of statcheck. Using statcheck/web, you can check any PDF for statistical errors without installing the R programming language on your computer.","tags":["Reproducibility","Researchers","Software","Workflow Tools"],"title":"Statcheck","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1721390377,"objectID":"37f6a7d497e5c23a937b1cfdb8a78dc5","permalink":"https://forrt.org/glossary/english/statistical_assumptions/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/english/statistical_assumptions/","section":"glossary","summary":"","tags":null,"title":"Statistical Assumptions","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1635961284,"objectID":"392e2479c630be67b7e268823d3c3b2f","permalink":"https://forrt.org/glossary/vbeta/statistical-assumptions/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/vbeta/statistical-assumptions/","section":"glossary","summary":"","tags":null,"title":"Statistical Assumptions","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1720865851,"objectID":"9c0b0a070b7a3554c5f84e1ecb4aedcd","permalink":"https://forrt.org/glossary/german/statistical_assumptions/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/german/statistical_assumptions/","section":"glossary","summary":"","tags":null,"title":"Statistical Assumptions (Statistische Vorannahmen)","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"3d7212255a92e02c3ba79bfde20fc8a7","permalink":"https://forrt.org/curated_resources/statistical-errors-p-values-the-gold-sta/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/statistical-errors-p-values-the-gold-sta/","section":"curated_resources","summary":"P values, the 'gold standard' of statistical validity, are not as reliable as many scientists assume.","tags":[""],"title":"Statistical errors: P values, the ‘gold standard’ of statistical validity, are not as reliable as many scientists assume","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1689122843,"objectID":"5f30888e98e07afa1454bf8f07ad6ca2","permalink":"https://forrt.org/curated_resources/statistical-evidence-a-likelihood-paradi/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/statistical-evidence-a-likelihood-paradi/","section":"curated_resources","summary":"Interpreting statistical data as evidence, Statistical Evidence: A Likelihood Paradigm focuses on the law of likelihood, fundamental to solving many of the problems associated with interpreting data in this way. Statistics has long neglected this principle, resulting in a seriously defective methodology. This book redresses the balance, explaining why science has clung to a defective methodology despite its well-known defects. After examining the strengths and weaknesses of the work of Neyman and Pearson and the Fisher paradigm, the author proposes an alternative paradigm which provides, in the law of likelihood, the explicit concept of evidence missing from the other paradigms. At the same time, this new paradigm retains the elements of objective measurement and control of the frequency of misleading results, features which made the old paradigms so important to science. The likelihood paradigm leads to statistical methods that have a compelling rationale and an elegant simplicity, no longer forcing the reader to choose between frequentist and Bayesian statistics. ","tags":["Book"],"title":"Statistical Evidence: A Likelihood Paradigm","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"61094d8fda40188003927e0b2c00b9c4","permalink":"https://forrt.org/curated_resources/statistical-methods-in-psychological-res/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/statistical-methods-in-psychological-res/","section":"curated_resources","summary":"A statistics book and tutorial about statistics","tags":["Statistical Book","Tutorial"],"title":"Statistical Methods in Psychological Research Syllabus","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1689122843,"objectID":"f33ae7b527864b7a39f752fccb4a8f8e","permalink":"https://forrt.org/curated_resources/statistical-methods-in-psychology-journa/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/statistical-methods-in-psychology-journa/","section":"curated_resources","summary":"In the light of continuing debate over the applications of significance testing in psychology journals and following the publication of J. Cohen's (1994) article, the Board of Scientific Affairs (BSA) of the American Psychological Association (APA) convened a committee called the Task Force on Statistical Interference (TFSI) whose charge was \"to elucidate some of the controversial issues surrounding applications of statistics including significance testing and its alternatives; alternative underlying models and data transformation; and newer methods made possible by powerful computers\" (BSA, personal communication, February 28, 1996). After extensive discussion, the BSA recommended that publishing an article in American Psychologist, as a way to initiate discussion in the field about changes in current practices of data analysis and reporting may be appropriate. This report follows that request. Following each guideline are comments, explanations, or elaborations assembled by L. Wilkinson for the task force and under its review. The report is concerned with the use of statistical methods only and is not meant as an assessment of research methods in general. The title and format of the report are adapted from an article by J. C. Bailar and F. Mosteller (1988).","tags":[""],"title":"Statistical methods in psychology journals: Guidelines and explanations.","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1689122843,"objectID":"8c7eeeb5424fd4a25f98419b29d05527","permalink":"https://forrt.org/curated_resources/statistical-modeling-causal-inference-an/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/statistical-modeling-causal-inference-an/","section":"curated_resources","summary":"A blog about statistics and open science","tags":["Blog","Reproducibility Knowledge"],"title":"Statistical Modeling, Causal Inference, and Social Science","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1721390377,"objectID":"fe9981eb78834a1b74041b6d7439ff2f","permalink":"https://forrt.org/glossary/english/statistical_power/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/english/statistical_power/","section":"glossary","summary":"","tags":null,"title":"Statistical power","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1635961284,"objectID":"7ce2b026570974630c2a29396da4ada2","permalink":"https://forrt.org/glossary/vbeta/statistical-power/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/vbeta/statistical-power/","section":"glossary","summary":"","tags":null,"title":"Statistical power","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1720865851,"objectID":"5fe683291685de109200d941f8941721","permalink":"https://forrt.org/glossary/german/statistical_power/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/german/statistical_power/","section":"glossary","summary":"","tags":null,"title":"Statistical power (Statistische Teststärke)","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"999d92c11bebca8cd11cebf2989ba20c","permalink":"https://forrt.org/curated_resources/statistical-power-analysis/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/statistical-power-analysis/","section":"curated_resources","summary":"A paper about statistical power","tags":[""],"title":"Statistical power analysis","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1689122843,"objectID":"70209ab3b3972cbf85dfff9ef1a3e9f2","permalink":"https://forrt.org/curated_resources/statistical-power-and-optimal-design-in/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/statistical-power-and-optimal-design-in/","section":"curated_resources","summary":"Researchers designing experiments in which a sample of participants responds to a sample of stimuli are faced with difficult questions about optimal study design. The conventional procedures of statistical power analysis fail to provide appropriate answers to these questions because they are based on statistical models in which stimuli are not assumed to be a source of random variation in the data, models that are inappropriate for experiments involving crossed random factors of participants and stimuli. In this article, we present new methods of power analysis for designs with crossed random factors, and we give detailed, practical guidance to psychology researchers planning experiments in which a sample of participants responds to a sample of stimuli. We extensively examine 5 commonly used experimental designs, describe how to estimate statistical power in each, and provide power analysis results based on a reasonable set of default parameter values. We then develop general conclusions and formulate rules of thumb concerning the optimal design of experiments in which a sample of participants responds to a sample of stimuli. We show that in crossed designs, statistical power typically does not approach unity as the number of participants goes to infinity but instead approaches a maximum attainable power value that is possibly small, depending on the stimulus sample. We also consider the statistical merits of designs involving multiple stimulus blocks. Finally, we provide a simple and flexible Web-based power application to aid researchers in planning studies with samples of stimuli.","tags":[""],"title":"Statistical power and optimal design in experiments in which samples of participants respond to samples of stimuli","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1689122843,"objectID":"81a2e24a0e128facff1006ebd437d7b2","permalink":"https://forrt.org/curated_resources/statistical-power-and-the-testing-of-nul/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/statistical-power-and-the-testing-of-nul/","section":"curated_resources","summary":"The purpose of this study is to determine how well contemporary management research fares on the issue of statistical power with regard to studies specifically predicting null relationships between phenomena of interest. This power assessment differs from traditional power studies because it focuses solely on studies that offered and tested null hypotheses. A sample of studies containing hypothesized null relationships was taken from five mainstream management journals over the 1990 to 1999 time period. Results of the power assessment suggest that management researchers’ abilities to affirm null hypotheses are low. On average, the power assessment revealed that for those studies that found nonsignificance of results and consequently affirmed their null hypotheses, the actual Type II error rate was nearly 15 times greater than what is advocated in the literature when failing to reject a false null hypothesis. Recommendations for researchers proposing and testing formal null hypotheses are also discussed","tags":[""],"title":"Statistical Power and the Testing of Null Hypotheses: A Review of Contemporary Management Research and Recommendations for Future Studies","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1689122843,"objectID":"42f036e1a7135e77e0fdae2b63c1a4b0","permalink":"https://forrt.org/curated_resources/statistical-power-in-operations-manageme/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/statistical-power-in-operations-manageme/","section":"curated_resources","summary":"This paper discusses the need and importance of statistical power analysis in field-based empirical research in Production and Operations Management (POM) and related disciplines. The concept of statistical power analysis is explained in detail and its relevance in designing and conducting empirical experiments is discussed. Statistical power reflects the degree to which differences in sample data in a statistical test can be detected. A high power is required to reduce the probability of failing to detect an effect when it is present. This paper also examines the relationship between statistical power, significance level, sample size and effect size. A probability tree analysis further explains the importance of statistical power by showing the relationship between Type II errors and the probability of making wrong decisions in statistical analysis. A power analysis of 28 articles (524 statistical tests) in the Journal of Operations Management and in Decision Sciences shows that 60% of empirical studies do not have high power levels. This means that several of these tests will have a low degree of repeatability. This and other similar issues involving statistical power will become increasingly important as empirical studies in POM study relatively smaller effects.","tags":[""],"title":"Statistical power in operations management research ","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1689122843,"objectID":"cd28a5fef397fc9b16778a4b9d398cd5","permalink":"https://forrt.org/curated_resources/statistical-power-of-psychological-resea/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/statistical-power-of-psychological-resea/","section":"curated_resources","summary":"Power was calculated for 6,155 statistical tests in 221 journal articles published in the 1982 volumes of the Journal of Abnormal Psychology, Journal of Consulting and Clinical Psychology, and Journal of Personality and Social Psychology. Power to detect small, medium, and large effects was .17, .57, and .83, respectively. 20 years after Cohen (1962) conducted the first power survey, the power of psychological research is still low. The implications of these results concerning the proliferation of Type I errors in the published literature, the failure of replication studies, and the interpretation of null (negative) results are emphasized. An example is given of the use of power analysis to help interpret null results by setting probable upper bounds on the magnitudes of effects. Limitations of statistical power analyses, suggestions for future research, sources of computational information, and recommendations for improving power are discussed.","tags":[""],"title":"Statistical Power of Psychological Research: What Have We Gained in 20 Years?","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1689122843,"objectID":"3727a63bec60dcd54544131633d54837","permalink":"https://forrt.org/curated_resources/statistical-power-problems-with-moderate/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/statistical-power-problems-with-moderate/","section":"curated_resources","summary":"Due to the increasing importance of moderating (i.e., interaction) effects, the use of moderated multiple regression (MMR) has become pervasive in numerous management specialties such as organizational behavior, human resources management, and strategy, to name a few. Despite its popularity, recent research on the MMR approach to moderator variable detection has identified several factors that reduce statistical power below acceptable levels and, consequently, lead researchers to erroneously dismiss theoretical models that include moderated relationships. The present article (1) briefly describes MMR, (2) reviews factors that affect the statistical power of hypothesis tests conducted using this technique, (3) proposes solutions to low power situations, and (4) discusses areas and problems related to MMR that are in need of further investigation.","tags":[""],"title":"Statistical Power Problems with Moderated Multiple Regression in Management Research ","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1689122843,"objectID":"7be727bab36c9d56991e2f397b408785","permalink":"https://forrt.org/curated_resources/statistical-power-sample-size-and-their/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/statistical-power-sample-size-and-their/","section":"curated_resources","summary":"Objective. —To describe the pattern over time in the level of statistical power and the reporting of sample size calculations in published randomized controlled trials (RCTs) with negative results. Design. —Our study was a descriptive survey. Power to detect 25% and 50% relative differences was calculated for the subset of trials with negative results in which a simple two-group parallel design was used. Criteria were developed both to classify trial results as positive or negative and to identify the primary outcomes. Power calculations were based on results from the primary outcomes reported in the trials. Population. —We reviewed all 383 RCTs published in JAMA, Lancet, and the New England Journal of Medicine in 1975, 1980, 1985, and 1990. Results. —Twenty-seven percent of the 383 RCTs (n=102) were classified as having negative results. The number of published RCTs more than doubled from 1975 to 1990, with the proportion of trials with negative results remaining fairly stable. Of the simple two-group parallel design trials having negative results with dichotomous or continuous primary outcomes (n=70), only 16% and 36% had sufficient statistical power (80%) to detect a 25% or 50% relative difference, respectively. These percentages did not consistently increase overtime. Overall, only 32% of the trials with negative results reported sample size calculations, but the percentage doing so has improved over time from 0% in 1975 to 43% in 1990. Only 20 of the 102 reports made any statement related to the clinical significance of the observed differences. Conclusions. —Most trials with negative results did not have large enough sample sizes to detect a 25% or a 50% relative difference. This result has not changed over time. Few trials discussed whether the observed differences were clinically important. There are important reasons to change this practice. The reporting of statistical power and sample size also needs to be improved","tags":[""],"title":"Statistical Power, Sample Size, and Their Reporting in Randomized Controlled Trials","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1689122843,"objectID":"7cca70221f13cb68caa1d17db12e7e22","permalink":"https://forrt.org/curated_resources/statistical-procedures-and-the-justifica/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/statistical-procedures-and-the-justifica/","section":"curated_resources","summary":"Justification, in the vernacular language of philosophy of science, refers to the evaluation, defense, and confirmation of claims of truth. In this article, we examine some aspects of the rhetoric of justification, which in part draws on statistical data analysis to shore up facts and inductive inferences. There are a number of problems of methodological spirit and substance that in the past have been resistant to attempts to correct them. The major problems are discussed, and readers are reminded of ways to clear away these obstacles to justification.","tags":["Reproducibility Crisis and Credibility Revolution","Open Science"],"title":"Statistical Procedures and the Justification of Knowledge in Psychological Science ","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1689122843,"objectID":"d1fe9d2bf500c646d9fa4745ade2bfe8","permalink":"https://forrt.org/curated_resources/statistical-reporting-errors-and-collabo/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/statistical-reporting-errors-and-collabo/","section":"curated_resources","summary":"Statistical analysis is error prone. A best practice for researchers using statistics would therefore be to share data among co-authors, allowing double-checking of executed tasks just as co-pilots do in aviation. To document the extent to which this ‘co-piloting’ currently occurs in psychology, we surveyed the authors of 697 articles published in six top psychology journals and asked them whether they had collaborated on four aspects of analyzing data and reporting results, and whether the described data had been shared between the authors. We acquired responses for 49.6% of the articles and found that co-piloting on statistical analysis and reporting results is quite uncommon among psychologists, while data sharing among co-authors seems reasonably but not completely standard. We then used an automated procedure to study the prevalence of statistical reporting errors in the articles in our sample and examined the relationship between reporting errors and co-piloting. Overall, 63% of the articles contained at least one p-value that was inconsistent with the reported test statistic and the accompanying degrees of freedom, and 20% of the articles contained at least one p-value that was inconsistent to such a degree that it may have affected decisions about statistical significance. Overall, the probability that a given p-value was inconsistent was over 10%. Co-piloting was not found to be associated with reporting errors.","tags":[""],"title":"Statistical Reporting Errors and Collaboration on Statistical Analyses in Psychological Science","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1689122843,"objectID":"212a378572ba389828700a03509ed983","permalink":"https://forrt.org/curated_resources/statistical-rethinking-winter-2015/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/statistical-rethinking-winter-2015/","section":"curated_resources","summary":"A video about Statistical Rethinking: A Bayesian Course with R ","tags":["Video","Bayesian"],"title":"Statistical Rethinking Winter 2015","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1721390377,"objectID":"2dcd25f398b65dba4ffd804eaa436ba3","permalink":"https://forrt.org/glossary/english/statistical_significance/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/english/statistical_significance/","section":"glossary","summary":"","tags":null,"title":"Statistical significance","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1635961284,"objectID":"1636166c04b716be9a07bf82c21709de","permalink":"https://forrt.org/glossary/vbeta/statistical-significance/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/vbeta/statistical-significance/","section":"glossary","summary":"","tags":null,"title":"Statistical significance","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1720865851,"objectID":"ce9128621008c4c012ef87b5fcae1fbc","permalink":"https://forrt.org/glossary/german/statistical_significance/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/german/statistical_significance/","section":"glossary","summary":"","tags":null,"title":"Statistical significance (Statistische Signifikanz)","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1689122843,"objectID":"adde6f33022133df8fcb3bba2e70122d","permalink":"https://forrt.org/curated_resources/statistical-significance-and-the-dichoto/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/statistical-significance-and-the-dichoto/","section":"curated_resources","summary":"In light of recent concerns about reproducibility and replicability, the ASA issued a Statement on Statistical Significance and p-values aimed at those who are not primarily statisticians. While the ASA Statement notes that statistical significance and p-values are “commonly misused and misinterpreted,” it does not discuss and document broader implications of these errors for the interpretation of evidence. In this article, we review research on how applied researchers who are not primarily statisticians misuse and misinterpret p-values in practice and how this can lead to errors in the interpretation of evidence. We also present new data showing, perhaps surprisingly, that researchers who are primarily statisticians are also prone to misuse and misinterpret p-values thus resulting in similar errors. In particular, we show that statisticians tend to interpret evidence dichotomously based on whether or not a p-value crosses the conventional 0.05 threshold for statistical significance. We discuss implications and offer recommendations.","tags":[""],"title":"Statistical Significance and the Dichotomization of Evidence","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1689122843,"objectID":"f7561bc5fc56014db74ad2a794ebd70c","permalink":"https://forrt.org/curated_resources/statistical-significance-in-psychologica/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/statistical-significance-in-psychologica/","section":"curated_resources","summary":"MOST THEORIES IN THE AREAS OF PERSONALITY, CLINICAL, AND SOCIAL PSYCHOLOGY PREDICT ONLY THE DIRECTION OF A CORRELATION, GROUP DIFFERENCE, OR TREATMENT EFFECT. SINCE THE NULL HYPOTHESIS IS NEVER STRICTLY TRUE, SUCH PREDICTIONS HAVE ABOUT A 50-50 CHANCE OF BEING CONFIRMED BY EXPERIMENT WHEN THE THEORY IN QUESTION IS FALSE, SINCE THE STATISTICAL SIGNIFICANCE OF THE RESULT IS A FUNCTION OF THE SAMPLE SIZE. CONFIRMATION OF 1 DIRECTIONAL PREDICTION GENERALLY BUILDS LITTLE CONFIDENCE IN THE THEORY BEING TESTED. MOST THEORIES SHOULD BE TESTED BY MULTIPLE CORROBORATION AND MOST EMPIRICAL GENERALIZATIONS BY CONSTRUCTIVE REPLICATION. STATISTICAL SIGNIFICANCE, PERHAPS THE LEAST IMPORTANT ATTRIBUTE OF A GOOD EXPERIMENT, IS NEVER A SUFFICIENT CONDITION FOR CLAIMING THAT (1) A THEORY HAS BEEN USEFULLY CORROBORATED, (2) A MEANINGFUL EMPIRICAL FACT HAS BEEN ESTABLISHED, OR (3) AN EXPERIMENTAL REPORT OUGHT TO BE PUBLISHED","tags":[""],"title":"Statistical significance in psychological research.","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1689122843,"objectID":"ed9818dba293998d3041625609420184","permalink":"https://forrt.org/curated_resources/statistical-significance-testing-and-cum/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/statistical-significance-testing-and-cum/","section":"curated_resources","summary":"Data analysis methods in psychology still emphasize statistical significance testing, despite numerous articles demonstrating its severe deficiencies. It is now possible to use meta-analysis to show that reliance on significance testing retards the development of cumulative knowledge. But reform of teaching and practice will also require that researchers learn that the benefits that they believe flow from use of significance testing are illusory. Teachers must revamp their courses to bring students to understand that (a) reliance on significance testing retards the growth of cumulative research knowledge; (b) benefits widely believed to flow from significance testing do not in fact exist; and (c) significance testing methods must be replaced with point estimates and confidence intervals in individual studies and with meta-analyses in the integration of multiple studies. This reform is essential to the future progress of cumulative knowledge in psychological research.","tags":[""],"title":"Statistical significance testing and cumulative knowledge in psychology: implications for training researchers.","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1689122843,"objectID":"676eb4ed16e300129598777e359043c9","permalink":"https://forrt.org/curated_resources/statistical-tests-p-values-confidence-in/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/statistical-tests-p-values-confidence-in/","section":"curated_resources","summary":"Misinterpretation and abuse of statistical tests, confidence intervals, and statistical power have been decried for decades, yet remain rampant. A key problem is that there are no interpretations of these concepts that are at once simple, intuitive, correct, and foolproof. Instead, correct use and interpretation of these statistics requires an attention to detail which seems to tax the patience of working scientists. This high cognitive demand has led to an epidemic of shortcut definitions and interpretations that are simply wrong, sometimes disastrously so—and yet these misinterpretations dominate much of the scientific literature. In light of this problem, we provide definitions and a discussion of basic statistics that are more general and critical than typically found in traditional introductory expositions. Our goal is to provide a resource for instructors, researchers, and consumers of statistics whose knowledge of statistical theory and technique may be limited but who wish to avoid and spot misinterpretations. We emphasize how violation of often unstated analysis protocols (such as selecting analyses for presentation based on the P values they produce) can lead to small P values even if the declared test hypothesis is correct, and can lead to large P values even if that hypothesis is incorrect. We then provide an explanatory list of 25 misinterpretations of P values, confidence intervals, and power. We conclude with guidelines for improving statistical interpretation and reporting.","tags":[""],"title":"Statistical tests, p values, confidence intervals, and power: a guide to misinterpretations. ","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1721390377,"objectID":"78c62d76b767356dd1b664f930b91545","permalink":"https://forrt.org/glossary/english/statistical_validity/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/english/statistical_validity/","section":"glossary","summary":"","tags":null,"title":"Statistical validity","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1635961284,"objectID":"82c2b37ea4cef088b9e5651e63224747","permalink":"https://forrt.org/glossary/vbeta/statistical-validity/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/vbeta/statistical-validity/","section":"glossary","summary":"","tags":null,"title":"Statistical validity ","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1720865851,"objectID":"c7f14e0b31ffe9a95bd1a22233c753ef","permalink":"https://forrt.org/glossary/german/statistical_validity/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/german/statistical_validity/","section":"glossary","summary":"","tags":null,"title":"Statistical validity (Statistische Validität)","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"d85c49874b8a8a0bb69f075e7bd10a5f","permalink":"https://forrt.org/curated_resources/statistics-and-quantitative-methods-exam/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/statistics-and-quantitative-methods-exam/","section":"curated_resources","summary":"The goal of this repository is to index and host short videos that can be used to supplement the teaching of introductory statistics concepts. The purpose of these videos is to show students examples of statistical concepts being used in real research, to build off of foundational understanding of the concept they were introduced to in class. Let's show students real people, doing real research, and using real baby statistics to solve science! If you are interested to contribute a video to this page, please read the wiki, which explains what is needed in more depth. If you are still interested to contribute at that point, please request access as a contributor for the specific component(s) you would like to contribute to. If you would like to contribute a video for a topic that is not listed as a component, please contact JK Flake.","tags":["Quantitative Methods","Statistics"],"title":"Statistics and Quantitative Methods Example Videos for Teaching","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"ecea3f6901292fefa03aeb08dd91bce0","permalink":"https://forrt.org/curated_resources/statistics-of-doom/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/statistics-of-doom/","section":"curated_resources","summary":"About Stats of DOOM\n\nSupport Statistics of DOOM! This page and the YouTube channel to help people learn statistics by including step-by-step instructions for SPSS, R, Excel, and other programs. Demonstrations are provided including power, data screening, analysis, write up tips, effect sizes, and graphs. Help guides and course materials are also provided!\n\nWhen I originally started posting my videos on YouTube, I never really thought people would be interested in them - minus a few overachieving students. I am glad that I've been able to help so many folks! I have taught many statistics courses - you can view full classes by using the Learn tab in the top right. I have also taught cognitive and language courses, some with coding (see the NLP and Language Modeling courses), and some without (see Other Courses). I hope this website provides structure to all my materials for you to use for yourself or your classroom.\n\nEach page has an example syllabus, video lectures laid out with that syllabus (if I have them!), and links to the appropriate materials. Any broken links can be reported by sending me an email (linked at the bottom). Stats Tools was designed for learning statistics, which morphed into learning coding, open science, statistics, and more! Recommendations, comments, and other questions are welcome with the general suggestion to post on the specific video or page you have a question on. I do my best to answer, but also work a full-time job.\n\nThese resources wouldn't be possible without the help of many fantastic people over the years including:\n\nAll the Help Desk TAs: Rachel E. Monroe, Marshall Beauchamp, Louis Oberdiear, Simone Donaldson, Kim Koch, Jessica Willis, Samantha Hunter, Flora Forbes, Tabatha Hopke\nResearch colleagues: K.D. Valentine, John E. Scofield, Jeff Pavlacic\nAnd more! Pages with specific content made by others are noted on that page.","tags":["Educators","Open Education","Researchers","Statistics"],"title":"Statistics of DOOM","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"ce91975198da5def0475041bb3e21b4d","permalink":"https://forrt.org/curated_resources/statistics-with-jasp-and-the-open-scienc/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/statistics-with-jasp-and-the-open-scienc/","section":"curated_resources","summary":"This webinar will introduce the integration of JASP Statistical Software (https://jasp-stats.org/) with the Open Science Framework (OSF; https://osf.io). The OSF is a free, open source web application built to help researchers manage their workflows. The OSF is part collaboration tool, part version control software, and part data archive. The OSF connects to popular tools researchers already use, like Dropbox, Box, Github, Mendeley, and now is integrated with JASP, to streamline workflows and increase efficiency.","tags":["Analysis","Data","Education","JASP","Materials","Open Scholarship Tools and Technologies","OSF","Publishing","Reproducibility","Research Data Management Tools","Researchers","Statistics"],"title":"Statistics with JASP and the Open Science Framework","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1689122843,"objectID":"624838b08e003928a58834c20d8802e1","permalink":"https://forrt.org/curated_resources/stereotype-threat/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/stereotype-threat/","section":"curated_resources","summary":"A podcast about stereotype threat and replication","tags":["Podcast","Reproducibility Knowledge","Reproducibility Crisis and Credibility Revolution"],"title":"Stereotype threat","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1721390377,"objectID":"53e82397d6834255a47f3cf56233d8b9","permalink":"https://forrt.org/glossary/english/strange/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/english/strange/","section":"glossary","summary":"","tags":null,"title":"STRANGE","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1720865851,"objectID":"bebacfa412d9560c404e62b149c757bd","permalink":"https://forrt.org/glossary/german/strange/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/german/strange/","section":"glossary","summary":"","tags":null,"title":"STRANGE","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1635961284,"objectID":"3dd89fea217e9334712bd36d700e821e","permalink":"https://forrt.org/glossary/vbeta/strange/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/vbeta/strange/","section":"glossary","summary":"","tags":null,"title":"STRANGE","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1689122843,"objectID":"d329b4b46025d3ab36ae8faad579b601","permalink":"https://forrt.org/curated_resources/strong-inference/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/strong-inference/","section":"curated_resources","summary":"certain systematic methods of scientific thinking may produce much more rapid progress than others.","tags":["Reproducibility Crisis and Credibility Revolution","Open Science"],"title":"Strong inference","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"75f89115b1fb77250640afaacdb644ff","permalink":"https://forrt.org/curated_resources/study-preregistration-an-evaluation-of-a/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/study-preregistration-an-evaluation-of-a/","section":"curated_resources","summary":"Study preregistration promotes transparency in scientific research by making a clear distinction between a priori and post hoc procedures or analyses. Management and applied psychology have not embraced preregistration in the way other closely related social science fields have. There may be concerns that preregistration does not add value and prevents exploratory data analyses. Using a mixed-method approach, in Study 1, we compared published preregistered samples against published non-preregistered samples. We found that preregistration effectively facilitated more transparent reporting based on criteria (i.e., confirmed hypotheses and a priori analysis plans). Moreover, consistent with concerns that the published literature contains elevated type I error rates, preregistered samples had fewer statistically significant results (48%) than non-preregistered samples (66%). To learn about the perceived advantages, disadvantages, and misconceptions of study preregistration, in Study 2, we surveyed authors of preregistered studies and authors who had never preregistered a study. Participants in both samples had positive inclinations towards preregistration yet expressed concerns about the process. We conclude with a review of best practices for management and applied psychology stakeholders.","tags":["Preregistration","Open Science","Reproducibility","Questionable Research Practices","Methodology"],"title":"Study Preregistration: An Evaluation of a Method for Transparent Reporting","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1721390377,"objectID":"3aadec7128a450605625ff00e6df82d2","permalink":"https://forrt.org/glossary/english/studyswap/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/english/studyswap/","section":"glossary","summary":"","tags":null,"title":"StudySwap","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1720865851,"objectID":"092fa4e07ab301b198b6ac9cab198511","permalink":"https://forrt.org/glossary/german/studyswap/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/german/studyswap/","section":"glossary","summary":"","tags":null,"title":"StudySwap","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1635961284,"objectID":"dba4228dc05dd79b8b258e33daec590f","permalink":"https://forrt.org/glossary/vbeta/studyswap/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/vbeta/studyswap/","section":"glossary","summary":"","tags":null,"title":"StudySwap","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"60577c20e92e0692a3290ef8aeb7011c","permalink":"https://forrt.org/curated_resources/supporting-open-science-data-curation-pr/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/supporting-open-science-data-curation-pr/","section":"curated_resources","summary":"Openness in research can lead to greater reproducibility, an accelerated pace of discovery, and decreased redundancy of effort. In addition, open research ensures equitable access to knowledge and the ability for any community to assess, interrogate, and build upon prior work. It also requires open infrastructure and distributed access; but few institutions can provide all of these services alone. Providing a trustworthy network for perpetual availability of research data is critical to ensuring reproducibility, transparency, and ongoing inquiry.\n\nIncreased attention on the importance of open research and data sharing has led to a proliferation of platforms to store data, materials, etc., with limited technical integration. This can hinder data sharing, but also complicate coordination with local library expertise and services, thus hampering curation and long-term stewardship.\n\nFor example, the open source OSF enables researchers to directly create and manage research projects and integrates with other tools researchers use (Google Drive, Dropbox, Box, etc.), but lacks the ability to archive that material locally at a researcher’s institution. Long-term stewardship and preservation requires multiple copies of data archived in different locations, and creating archives seamlessly would be ideal.\n\nCOS and IA are working together to address these preservation and stewardship challenges by providing open, cooperative infrastructure to ensure long-term access and connection to research data, and by supporting and promoting adoption of open science practices to enhance research reproducibility as well as data sharing and reuse.\n\nIn this webinar, attendees will learn about both the technical and practical aspects of this collaborative project connecting the researcher tool OSF and the preservation system of Internet Archive. We demonstrate how researchers can improve the openness and reproducibility of their research through preregistration, and how those preregistrations are preserved with Internet Archive. We answer questions and explore use cases for how this powerful workflow can support library curation and stewardship of open research.","tags":["Reproducability","Research"],"title":"Supporting Open Science Data Curation, Preservation, and Access by Libraries","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"b670cd7d8816d87586e89cc1c1de47f2","permalink":"https://forrt.org/curated_resources/supporting-robust-research-on-adult-emot/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/supporting-robust-research-on-adult-emot/","section":"curated_resources","summary":"A criterion for high quality science is to produce findings that are robust and replicable across studies. A potential hinderance to successful replication however is context dependency. To formally address issues of context dependency, context has to be defined and integrated into research and replication practices. Emotion research and particularly research on adult emotional development have long emphasized the importance of context. Drawing on established theories of adult development and existing frameworks of context, we define context as it relates to emotional development in adulthood, highlighting specific aspects of immediate surroundings (familiarity, cognitive demands, and social aspects) as well as sociocultural and socioeconomic context, situated within ontogenetic development and historical time. In order to improve the robustness of research on adult emotional development, we encourage researchers to consider these contextual aspects in formulating and testing research questions as well as when interpreting failed replications. We discuss how to adapt study designs to facilitate more context sensitive adult emotional development research. Considering context not only enables new discoveries in aging research, but also can help clarify significant long-standing research questions and further enhance the robustness of research on adult development in emotion.","tags":["Robustness","Adult Emotional Development","Context","Sociocultural","Socioeconomic"],"title":"Supporting robust research on adult emotional development by considering context","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1689122843,"objectID":"29a5256a6cfc7a4f985cab2482fc0f71","permalink":"https://forrt.org/curated_resources/surrogate-science-the-idol-of-a-universa/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/surrogate-science-the-idol-of-a-universa/","section":"curated_resources","summary":"The application of statistics to science is not a neutral act. Statistical tools have shaped and were also shaped by its objects. In the social sciences, statistical methods fundamentally changed research practice, making statistical inference its centerpiece. At the same time, textbook writers in the social sciences have transformed rivaling statistical systems into an apparently monolithic method that could be used mechanically. The idol of a universal method for scientific inference has been worshipped since the “inference revolution” of the 1950s. Because no such method has ever been found, surrogates have been created, most notably the quest for significant p values. This form of surrogate science fosters delusions and borderline cheating and has done much harm, creating, for one, a flood of irreproducible results. Proponents of the “Bayesian revolution” should be wary of chasing yet another chimera: an apparently universal inference procedure. A better path would be to promote both an understanding of the various devices in the “statistical toolbox” and informed judgment to select among these.","tags":[""],"title":"Surrogate Science: The Idol of a Universal Method for Scientific Inference","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1689122843,"objectID":"dafa8d840a189edde6bea5c91a6e4acf","permalink":"https://forrt.org/curated_resources/swirl/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/swirl/","section":"curated_resources","summary":"Swirl teaches you R programming and data science interactively, at your own pace, and right in the R console!","tags":["Tutorial"],"title":"SWIRL","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1721390377,"objectID":"ed5296297ebf3594b4de73e88693a9c2","permalink":"https://forrt.org/glossary/english/systematic_review/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/english/systematic_review/","section":"glossary","summary":"","tags":null,"title":"Systematic Review","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1635961284,"objectID":"c36aa32f913490a2aef81c06e69e52a7","permalink":"https://forrt.org/glossary/vbeta/systematic-review/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/vbeta/systematic-review/","section":"glossary","summary":"","tags":null,"title":"Systematic Review","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1720865851,"objectID":"af75d1de22ea667f1f747b5e50516b96","permalink":"https://forrt.org/glossary/german/systematic_review/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/german/systematic_review/","section":"glossary","summary":"","tags":null,"title":"Systematic Review (systematisches Review)","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"b1de751a8f82b768736057c7923f379c","permalink":"https://forrt.org/curated_resources/systematic-review-of-the-empirical-evide/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/systematic-review-of-the-empirical-evide/","section":"curated_resources","summary":"Background The increased use of meta-analysis in systematic reviews of healthcare interventions has highlighted several types of bias that can arise during the completion of a randomised controlled trial. Study publication bias and outcome reporting bias have been recognised as a potential threat to the validity of meta-analysis and can make the readily available evidence unreliable for decision making. Methodology/Principal Findings In this update, we review and summarise the evidence from cohort studies that have assessed study publication bias or outcome reporting bias in randomised controlled trials. Twenty studies were eligible of which four were newly identified in this update. Only two followed the cohort all the way through from protocol approval to information regarding publication of outcomes. Fifteen of the studies investigated study publication bias and five investigated outcome reporting bias. Three studies have found that statistically significant outcomes had a higher odds of being fully reported compared to non-significant outcomes (range of odds ratios: 2.2 to 4.7). In comparing trial publications to protocols, we found that 40–62% of studies had at least one primary outcome that was changed, introduced, or omitted. We decided not to undertake meta-analysis due to the differences between studies. Conclusions This update does not change the conclusions of the review in which 16 studies were included. Direct empirical evidence for the existence of study publication bias and outcome reporting bias is shown. There is strong evidence of an association between significant results and publication; studies that report positive or significant results are more likely to be published and outcomes that are statistically significant have higher odds of being fully reported. Publications have been found to be inconsistent with their protocols. Researchers need to be aware of the problems of both types of bias and efforts should be concentrated on improving the reporting of trials.","tags":["Clinical Trials","Drug Discovery","Meta-analysis","Peer Review","Publication Ethics","Publishing","Questionnaires","Research Funding","Scientific Publishing"],"title":"Systematic Review of the Empirical Evidence of Study Publication Bias and Outcome Reporting Bias — An Updated Review","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"6f3d4e8c8d93a6b0a3d420c45cc0c7f4","permalink":"https://forrt.org/curated_resources/systematic-reviews-and-meta-analysis/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/systematic-reviews-and-meta-analysis/","section":"curated_resources","summary":"When used together, systematic review methods and meta-analysis can produce comprehensive, accurate, and useful summaries of empirical evidence to answer questions that are relevant for policy, practice, and future research. Systematic reviews and meta-analysis can also uncover previously-undetected patterns of results across multiple studies, leading to new discoveries. For these reasons, systematic reviews and meta-analysis have become popular tools that are widely used – and misused – in the social, health, and natural sciences. A growing body of meta research has been used to develop evidence-based guidelines for the conduct and reporting of rigorous systematic reviews and meta-analysis. The Campbell Collaboration developed such guidelines for reviews in the social, behavioral, and economic sciences, and these guidelines undergird the content of this course.\n\nA systematic approach is necessary to identify relevant studies and avoid well-documented sources of bias and error in the dissemination, assessment, and synthesis of research results across studies. Meta-analysis provides a set of statistical tools for analysis and synthesis of quantitative data from two or more studies.\n\nThe course provides an introduction to the methods of systematic reviews and meta-analysis. It is appropriate for graduate students, post-doctoral fellows, faculty, and senior researchers in institutions of higher education. It is geared for participants who have already completed introductory graduate level training in research methodology and statistics. \n\nAccess to the Open \u0026 Free version of the course is free of charge. It contains no scored assessment, has no schedule, and no instructor. Use it at your own pace. The content of this course may not be modified or adapted for other uses.","tags":["Systematic Review","Meta-Analysis"],"title":"Systematic Reviews and Meta-Analysis","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"4faa1ab4519e8bc193c615bb2f270cad","permalink":"https://forrt.org/curated_resources/taking-stock-of-the-credibility-revoluti/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/taking-stock-of-the-credibility-revoluti/","section":"curated_resources","summary":"A book about the credibility revolution","tags":["Credibility revolution"],"title":"Taking stock of the credibility revolution: Scientific reform 2011-no","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"2ee61d69186f3bf726f60a61d494ebe5","permalink":"https://forrt.org/curated_resources/teaching-and-mentoring-open-science/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/teaching-and-mentoring-open-science/","section":"curated_resources","summary":"A syllabi about mentoring and teaching open science","tags":["Reproducibility Crisis and Credibility Revolution","Open Science","Conceptual and statistical knowledge"],"title":"Teaching and Mentoring Open Science","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1689122843,"objectID":"077bcb07a9a280a736a1540421e88d02","permalink":"https://forrt.org/curated_resources/teaching-psych-science/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/teaching-psych-science/","section":"curated_resources","summary":"A collection of activities to teach APA writing and statistics","tags":[""],"title":"teaching psych science","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"8b9506accd8df3e15fb91ef7a539b3d5","permalink":"https://forrt.org/curated_resources/teaching-replicable-and-reproducible-sci/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/teaching-replicable-and-reproducible-sci/","section":"curated_resources","summary":"Participants will develop materials for teaching replicability and reproducible science. Possible materials to be generated include syllabi, specific assignments, or single lectures or lesson plans. We will provide existing teaching materials and structured activities designed to help participants define learning goals, develop teaching resources to facilitate those goals, and to create appropriate learning assessments.","tags":["Teaching"],"title":"Teaching replicable and reproducible science","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1689122843,"objectID":"834f24ee62ef66db14c9a4fb0a68ba34","permalink":"https://forrt.org/curated_resources/teaching-replication/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/teaching-replication/","section":"curated_resources","summary":"Replication is held as the gold standard for ensuring the reliability of published scientific literature. But conducting direct replications is expensive, time-consuming, and unrewarded under current publication practices. So who will do them? Our answer is that students in laboratory classes should replicate recent findings as part of their training in experimental methods. In our own courses, we have found that replicating cutting-edge results is exciting and fun, it gives students the opportunity to make real scientific contributions (provided supervision is appropriate), and it provides object lessons about the scientific process, the importance of reporting standards, and the value of openness.","tags":["Reproducibility Crisis and Credibility Revolution"],"title":"Teaching replication","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1689122843,"objectID":"26dd007d13b5efc70ea726ac22f4132d","permalink":"https://forrt.org/curated_resources/teaching-replication-in-psychology-a-gui/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/teaching-replication-in-psychology-a-gui/","section":"curated_resources","summary":"This symposium explores the “replication crisis” from the perspective of teachers and students. Presenters will describe the major issues surrounding replication, explain how students can contribute to replication research both in the classroom and in the lab, and offer curricular models that incorporate replication.","tags":["OSF Project"],"title":"Teaching Replication in Psychology: A Guide for Teachers and Students","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"b64610fb77d85dc4ff97fc5963410de5","permalink":"https://forrt.org/curated_resources/teaching-replication-to-graduate-student/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/teaching-replication-to-graduate-student/","section":"curated_resources","summary":"Replicating published studies promotes active learning of quantitative research skills. Drawing on experiences from a replication course, we provide practical tips and reflections for teachers who consider incorporating replication in their courses. We discuss teaching practices and challenges we encountered at three stages of a replication course: student recruitment, course structure and proceedings, and learning outcomes. We highlight that by engaging in replication, students learn from established scholarly work in a collaborative and reflective manner. Students not only improve their quantitative literacy but also learn more generally about the scientific method and the production of research.\n","tags":["Teaching","Mentoring","Quantitative"],"title":"Teaching Replication to Graduate Students","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1689122843,"objectID":"8fcaf49f7627e7c69a3bc98cd79aea9c","permalink":"https://forrt.org/curated_resources/teaching-resources-spreadsheet/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/teaching-resources-spreadsheet/","section":"curated_resources","summary":"An excel spreadsheet about collection of open science items","tags":["Reproducibility Knowledge"],"title":"Teaching resources spreadsheet","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"31598c48445976ce55dde404685142a4","permalink":"https://forrt.org/curated_resources/template-preregistration-registered-repo/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/template-preregistration-registered-repo/","section":"curated_resources","summary":"If you can answer these TEN questions you will have built the engine of a Stage 1 Registered Report.","tags":["Funders","Librarians","Open Scholarship Guidelines","Publishers","Publishing","Researchers"],"title":"Template preregistration Registered Report","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"8400f6c53230a400b2dd03234c4868a3","permalink":"https://forrt.org/curated_resources/ten-quick-tips-for-building-fair-workflo/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/ten-quick-tips-for-building-fair-workflo/","section":"curated_resources","summary":"Research data is accumulating rapidly and with it the challenge of fully reproducible science. As a consequence, implementation of high-quality management of scientific data has become a global priority. The FAIR (Findable, Accesible, Interoperable and Reusable) principles provide practical guidelines for maximizing the value of research data; however, processing data using workflows—systematic executions of a series of computational tools—is equally important for good data management. The FAIR principles have recently been adapted to Research Software (FAIR4RS Principles) to promote the reproducibility and reusability of any type of research software. Here, we propose a set of 10 quick tips, drafted by experienced workflow developers that will help researchers to apply FAIR4RS principles to workflows. The tips have been arranged according to the FAIR acronym, clarifying the purpose of each tip with respect to the FAIR4RS principles. Altogether, these tips can be seen as practical guidelines for workflow developers who aim to contribute to more reproducible and sustainable computational science, aiming to positively impact the open science and FAIR community.","tags":["Computer Software","Metadata","Source Code","Programming Languages","Reproducibility","Data Management","Research Design","Software Tools"],"title":"Ten quick tips for building FAIR workflows","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1689122843,"objectID":"19eb313db2ab5abce90be0f8b61d150e","permalink":"https://forrt.org/curated_resources/ten-simple-rules-for-effective-statistic/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/ten-simple-rules-for-effective-statistic/","section":"curated_resources","summary":"A paper about Ten Simple Rules for Effective Statistical Practice","tags":[""],"title":"Ten Simple Rules for Effective Statistical Practice","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"0f9f9e9383c60aea6eb32f18af200d71","permalink":"https://forrt.org/curated_resources/ten-simple-rules-for-reproducible-comput/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/ten-simple-rules-for-reproducible-comput/","section":"curated_resources","summary":"Replication is the cornerstone of a cumulative science. However, new tools and technologies, massive amounts of data, interdisciplinary approaches, and the complexity of the questions being asked are complicating replication efforts, as are increased pressures on scientists to advance their research. As full replication of studies on independently collected data is often not feasible, there has recently been a call for reproducible research as an attainable minimum standard for assessing the value of scientific claims. This requires that papers in experimental science describe the results and provide a sufficiently clear protocol to allow successful repetition and extension of analyses based on original data. The importance of replication and reproducibility has recently been exemplified through studies showing that scientific papers commonly leave out experimental details essential for reproduction, studies showing difficulties with replicating published experimental results, an increase in retracted papers, and through a high number of failing clinical trials. This has led to discussions on how individual researchers, institutions, funding bodies, and journals can establish routines that increase transparency and reproducibility. In order to foster such aspects, it has been suggested that the scientific community needs to develop a “culture of reproducibility” for computational science, and to require it for published claims. We want to emphasize that reproducibility is not only a moral responsibility with respect to the scientific field, but that a lack of reproducibility can also be a burden for you as an individual researcher. As an example, a good practice of reproducibility is necessary in order to allow previously developed methodology to be effectively applied on new data, or to allow reuse of code and results for new projects. In other words, good habits of reproducibility may actually turn out to be a time-saver in the longer run. We further note that reproducibility is just as much about the habits that ensure reproducible research as the technologies that can make these processes efficient and realistic. Each of the following ten rules captures a specific aspect of reproducibility, and discusses what is needed in terms of information handling and tracking of procedures. If you are taking a bare-bones approach to bioinformatics analysis, i.e., running various custom scripts from the command line, you will probably need to handle each rule explicitly. If you are instead performing your analyses through an integrated framework (such as GenePattern, Galaxy, LONI pipeline, or Taverna), the system may already provide full or partial support for most of the rules. What is needed on your part is then merely the knowledge of how to exploit these existing possibilities.","tags":["Archives","Computer and Information Sciences","Computer Applications","Data","Habits","Replication Studies","Reproducibility","Sequence Analysis","Source Code"],"title":"Ten Simple Rules for Reproducible Computational Research","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1689122843,"objectID":"3007a32e2e80da4cfcc12bded226ccc6","permalink":"https://forrt.org/curated_resources/ten-simple-rules-for-the-care-and-feedin/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/ten-simple-rules-for-the-care-and-feedin/","section":"curated_resources","summary":"A paper about ten Simple Rules for the Care and Feeding of Scientific Data","tags":["Reproducibility Crisis and Credibility Revolution","Open Science"],"title":"Ten Simple Rules for the Care and Feeding of Scientific Data","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1721390377,"objectID":"e25f0b326b0e4451d0cce45d0b99b725","permalink":"https://forrt.org/glossary/english/tenzing/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/english/tenzing/","section":"glossary","summary":"","tags":null,"title":"Tenzing","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1720865851,"objectID":"2b0087377934379eb81914f77ab1f091","permalink":"https://forrt.org/glossary/german/tenzing/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/german/tenzing/","section":"glossary","summary":"","tags":null,"title":"Tenzing","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1635961284,"objectID":"3117b286c002c35a19871820f8b7ee24","permalink":"https://forrt.org/glossary/vbeta/tenzing/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/vbeta/tenzing/","section":"glossary","summary":"","tags":null,"title":"Tenzing","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1721390377,"objectID":"063a909027602e55b867000234f63404","permalink":"https://forrt.org/glossary/english/term_placeholder/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/english/term_placeholder/","section":"glossary","summary":"","tags":null,"title":"Term placeholder","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1689122843,"objectID":"70a9be78b2f8a4c71cffa44c2e960788","permalink":"https://forrt.org/curated_resources/the-mis-reporting-of-statistical-results/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/the-mis-reporting-of-statistical-results/","section":"curated_resources","summary":" In order to study the prevalence, nature (direction), and causes of reporting errors in psychology, we checked the consistency of reported test statistics, degrees of freedom, and p values in a random sample of high- and low-impact psychology journals. In a second study, we established the generality of reporting errors in a random sample of recent psychological articles. Our results, on the basis of 281 articles, indicate that around 18% of statistical results in the psychological literature are incorrectly reported. Inconsistencies were more common in low-impact journals than in high impact journals. Moreover, around 15% of the articles contained at least one statistical conclusion that proved, upon recalculation, to be incorrect; that is, recalculation rendered the previously significant result insignificant, or vice versa. These errors were often in line with researchers’ expectations. We classified the most common errors and contacted authors to shed light on the origins of the errors.","tags":[""],"title":"The (mis)reporting of statistical results in psychology journals","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1689122843,"objectID":"d4c5f439f6c89795e19402ec9a9ca4e0","permalink":"https://forrt.org/curated_resources/the-52-symptoms-of-major-depression-lack/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/the-52-symptoms-of-major-depression-lack/","section":"curated_resources","summary":"Depression severity is assessed in numerous research disciplines, ranging from the social sciences to genetics, and used as a dependent variable, predictor, covariate, or to enroll participants. The routine practice is to assess depression severity with one particular depression scale, and draw conclusions about depression in general, relying on the assumption that scales are interchangeable measures of depression. The present paper investigates to which degree 7 common depression scales differ in their item content and generalizability. A content analysis is carried out to determine symptom overlap among the 7 scales via the Jaccard index (0=no overlap, 1=full overlap). Per scale, rates of idiosyncratic symptoms, and rates of specific vs. compound symptoms, are computed. The 7 instruments encompass 52 disparate symptoms. Mean overlap among all scales is low (0.36), mean overlap of each scale with all others ranges from 0.27 to 0.40, overlap among individual scales from 0.26 to 0.61. Symptoms feature across a mean of 3 scales, 40% of the symptoms appear in only a single scale, 12% across all instruments. Scales differ regarding their rates of idiosyncratic symptoms (0–33%) and compound symptoms (22–90%). Future studies analyzing more and different scales will be required to obtain a better estimate of the number of depression symptoms; the present content analysis was carried out conservatively and likely underestimates heterogeneity across the 7 scales. The substantial heterogeneity of the depressive syndrome and low overlap among scales may lead to research results idiosyncratic to particular scales used, posing a threat to the replicability and generalizability of depression research. Implications and future research opportunities are discussed.","tags":["Reproducibility Crisis and Credibility Revolution","Open Science"],"title":"The 52 symptoms of major depression: Lack of content overlap among seven common depression scales","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1689122843,"objectID":"3133bd6804ff6ff3320ec618334dad35","permalink":"https://forrt.org/curated_resources/the-amazing-significo-why-researchers-ne/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/the-amazing-significo-why-researchers-ne/","section":"curated_resources","summary":"A post that describes significance values","tags":["Blog","Reproducibility Crisis and Credibility Revolution"],"title":"The Amazing Significo: why researchers need to understand poker","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1689122843,"objectID":"e7d30f7c4bee41b3730e2a0717e027d1","permalink":"https://forrt.org/curated_resources/the-appropriate-use-of-null-hypothesis-t/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/the-appropriate-use-of-null-hypothesis-t/","section":"curated_resources","summary":"The many criticisms of null hypothesis testing suggest when it is not useful and what is should not be used for. This article explores when and why its use is appropriate. Null hypothesis testing is insufficient when size of effect is important, but it is ideal for testing ordinal claims relating the order of conditions, which are common in psychology. Null hypothesis testing also is insufficient for determining beliefs, but it is ideal for demonstrating sufficient evidential strength to support an ordinal claim, with sufficient evidence being 1 criterion for a finding entering the corpus of legitimate findings in psychology. The line between sufficient and insufficient evidence is currently set at p \u003c .05; there is little reason for allowing experimenters to select their own value of alpha. Thus null hypothesis testing is an optimal method for demonstrating sufficient evidence for an ordinal claim.","tags":[""],"title":"The appropriate use of null hypothesis testing. ","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1689122843,"objectID":"686d4d8ddfd402acfec7d55400281f22","permalink":"https://forrt.org/curated_resources/the-asa-statement-on-p-values-context-pr/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/the-asa-statement-on-p-values-context-pr/","section":"curated_resources","summary":"An editorial about p value","tags":[""],"title":"The ASA Statement on p-Values: Context, Process, and Purpose","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1689122843,"objectID":"9c4ab11b6fc04dd47fd2a64468428451","permalink":"https://forrt.org/curated_resources/the-baby-factory-difficult-research-obje/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/the-baby-factory-difficult-research-obje/","section":"curated_resources","summary":"Science studies scholars have shown that the management of natural complexity in lab settings is accomplished through a mixture of technological standardization and tacit knowledge by lab workers. Yet these strategies are not available to researchers who study difficult research objects. Using 16 months of ethnographic data from three laboratories that conduct experiments on infants and toddlers, the author shows how psychologists produce statistically significant results under challenging circumstances by using strategies that enable them to bridge the distance between an uncontrollable research object and a professional culture that prizes methodological rigor. This research raises important questions regarding the value of restrictive evidential cultures in challenging research environments.","tags":["Reproducibility Crisis and Credibility Revolution","Open Science"],"title":"The Baby Factory: Difficult Research Objects, Disciplinary Standards, and the Production of Statistical Significance","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1689122843,"objectID":"11556826cd02afef230964e45fa5d62e","permalink":"https://forrt.org/curated_resources/the-bayes-factor/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/the-bayes-factor/","section":"curated_resources","summary":"In this episode JP and Alex interview Zoltan Dienes. They discuss Zoltan's passion for the martial arts, why Bayesian inference could be more Popperian than you might think, and the easiest way to start using Bayesian statistics in practice.","tags":["Podcast"],"title":"The Bayes Factor","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1689122843,"objectID":"e357a0cf3680cfadc0a48b26f1a7c9b0","permalink":"https://forrt.org/curated_resources/the-bayesfactor-blog/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/the-bayesfactor-blog/","section":"curated_resources","summary":"Blog about Bayesfactor and statistics","tags":["Blog"],"title":"The Bayesfactor blog","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1689122843,"objectID":"dc6f148ab612139a9233d6ca84a8607f","permalink":"https://forrt.org/curated_resources/the-bayesian-reproducibility-project/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/the-bayesian-reproducibility-project/","section":"curated_resources","summary":"An abstract about bayesian reproducibility project","tags":["Blog","R code"],"title":"The Bayesian Reproducibility Project","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"64ec9b6073e746865471e4700b346952","permalink":"https://forrt.org/curated_resources/the-benefits-of-preregistration-for-hypo/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/the-benefits-of-preregistration-for-hypo/","section":"curated_resources","summary":"Preregistration is an open science practice that requires the specification of research hypotheses and analysis plans before the data are inspected. Here, we discuss the benefits of preregistration for hypothesis-driven, confirmatory bilingualism research. Using examples from psycholinguistics and bilingualism, we illustrate how non-peer reviewed preregistrations can serve to implement a clean distinction between hypothesis testing and data exploration. This distinction helps researchers avoid casting post-hoc hypotheses and analyses as confirmatory ones. We argue that, in keeping with current best practices in the experimental sciences, preregistration, along with sharing data and code, should be an integral part of hypothesis-driven bilingualism research.","tags":["Preregistration","Open Science","Bilingualism","Psycholinguistics","Confirmatory Analysis","Exploratory Analysis"],"title":"The benefits of preregistration for hypothesis-driven bilingualism research","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1689122843,"objectID":"3da476909c1ca44618d7d349360357b6","permalink":"https://forrt.org/curated_resources/the-black-goat/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/the-black-goat/","section":"curated_resources","summary":"Three psychologists talk about doing science. Hosted by Sanjay Srivastava, Alexa Tullett, and Simine Vazire.","tags":["Podcast","Reproducibility Crisis and Credibility Revolution"],"title":"The Black Goat","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1689122843,"objectID":"ae43ab2656a52d6b805d7a30f72cf32f","permalink":"https://forrt.org/curated_resources/the-case-against-statistical-significanc/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/the-case-against-statistical-significanc/","section":"curated_resources","summary":"In recent years the use of traditional statistical methods in educational research has increasingly come under attack. In this article, Ronald P. Carver exposes the fantasies often entertained by researchers about the meaning of statistical significance. The author recommends abandoning all statistical significance testing and suggests other ways of evaluating research results. Carver concludes that we should return to the scientific method of examining data and replicating results rather than relying on statistical significance testing to provide equivalent information.","tags":[""],"title":"The case against statistical significance testing","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"d2cf2faef255ad1778942b0864a3fc03","permalink":"https://forrt.org/curated_resources/the-case-for-formal-methodology-in-scien/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/the-case-for-formal-methodology-in-scien/","section":"curated_resources","summary":"Current attempts at methodological reform in sciences come in response to an overall lack of rigor in methodological and scientific practices in experimental sciences. However, most methodological reform attempts suffer from similar mistakes and over-generalizations to the ones they aim to address. We argue that this can be attributed in part to lack of formalism and first principles. Considering the costs of allowing false claims to become canonized, we argue for formal statistical rigor and scientific nuance in methodological reform. To attain this rigor and nuance, we propose a five-step formal approach for solving methodological problems. To illustrate the use and benefits of such formalism, we present a formal statistical analysis of three popular claims in the metascientific literature: (i) that reproducibility is the cornerstone of science; (ii) that data must not be used twice in any analysis; and (iii) that exploratory projects imply poor statistical practice. We show how our formal approach can inform and shape debates about such methodological claims.","tags":["Double-dipping","Exploratory Research","Replication","Scientific Reform","Reproducibility"],"title":"The case for formal methodology in scientific reform","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1689122843,"objectID":"7f343005079f75054921589f0551f815","permalink":"https://forrt.org/curated_resources/the-chrysalis-effect-how-ugly-initial-re/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/the-chrysalis-effect-how-ugly-initial-re/","section":"curated_resources","summary":"The issue of a published literature not representative of the population of research is most often discussed in terms of entire studies being suppressed. However, alternative sources of publication bias are questionable research practices (QRPs) that entail post hoc alterations of hypotheses to support data or post hoc alterations of data to support hypotheses. Using general strain theory as an explanatory framework, we outline the means, motives, and opportunities for researchers to better their chances of publication independent of rigor and relevance. We then assess the frequency of QRPs in management research by tracking differences between dissertations and their resulting journal publications. Our primary finding is that from dissertation to journal article, the ratio of supported to unsupported hypotheses more than doubled (0.82 to 1.00 versus 1.94 to 1.00). The rise in predictive accuracy resulted from the dropping of statistically nonsignificant hypotheses, the addition of statistically significant hypotheses, the reversing of predicted direction of hypotheses, and alterations to data. We conclude with recommendations to help mitigate the problem of an unrepresentative literature that we label the “Chrysalis Effect.”","tags":["Reproducibility Crisis and Credibility Revolution","Open Science"],"title":"The Chrysalis Effect: How Ugly Initial Results Metamorphosize Into Beautiful Articles","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"dc2f47c93dec937bb35c08668380eb39","permalink":"https://forrt.org/curated_resources/the-citation-advantage-of-linking-public/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/the-citation-advantage-of-linking-public/","section":"curated_resources","summary":"Efforts to make research results open and reproducible are increasingly reflected by journal policies encouraging or mandating authors to provide data availability statements. As a consequence of this, there has been a strong uptake of data availability statements in recent literature. Nevertheless, it is still unclear what proportion of these statements actually contain well-formed links to data, for example via a URL or permanent identifier, and if there is an added value in providing them. We consider 531,889 journal articles published by PLOS and BMC which are part of the PubMed Open Access collection, categorize their data availability statements according to their content and analyze the citation advantage of different statement categories via regression. We find that, following mandated publisher policies, data availability statements have become common by now, yet statements containing a link to a repository are still just a fraction of the total. We also find that articles with these statements, in particular, can have up to 25.36% higher citation impact on average: an encouraging result for all publishers and authors who make the effort of sharing their data. All our data and code are made available in order to reproduce and extend our results.","tags":["Computer Science","Data","Data Sharing","Digital Libraries","Policy","Publishing","Reproducibility"],"title":"The citation advantage of linking publications to research data","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"7ab767385579c66e714937006cf7a817","permalink":"https://forrt.org/curated_resources/the-clinicaltrials-gov-results-database/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/the-clinicaltrials-gov-results-database/","section":"curated_resources","summary":"BACKGROUND\nThe ClinicalTrials.gov trial registry was expanded in 2008 to include a database for reporting summary results. We summarize the structure and contents of the results database, provide an update of relevant policies, and show how the data can be used to gain insight into the state of clinical research.\n\nMETHODS\nWe analyzed ClinicalTrials.gov data that were publicly available between September 2009 and September 2010.\n\nRESULTS\nAs of September 27, 2010, ClinicalTrials.gov received approximately 330 new and 2000 revised registrations each week, along with 30 new and 80 revised results submissions. We characterized the 79,413 registry and 2178 results of trial records available as of September 2010. From a sample cohort of results records, 78 of 150 (52%) had associated publications within 2 years after posting. Of results records available publicly, 20% reported more than two primary outcome measures and 5% reported more than five. Of a sample of 100 registry record outcome measures, 61% lacked specificity in describing the metric used in the planned analysis. In a sample of 700 results records, the mean number of different analysis populations per study group was 2.5 (median, 1; range, 1 to 25). Of these trials, 24% reported results for 90% or less of their participants.\n\nCONCLUSIONS\nClinicalTrials.gov provides access to study results not otherwise available to the public. Although the database allows examination of various aspects of ongoing and completed clinical trials, its ultimate usefulness depends on the research community to submit accurate, informative data.","tags":["Clinical Trials","Registry","Clinical Research"],"title":"The ClinicalTrials.gov Results Database — Update and Key Issues","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"34572586f9a141c61c75ddd68b00ba1f","permalink":"https://forrt.org/curated_resources/the-costs-of-harking/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/the-costs-of-harking/","section":"curated_resources","summary":"Kerr ([1998]) coined the term ‘HARKing’ to refer to the practice of ‘hypothesizing after the results are known’. This questionable research practice has received increased attention in recent years because it is thought to have contributed to low replication rates in science. The present article discusses the concept of HARKing from a philosophical standpoint and then undertakes a critical review of Kerr’s ([1998]) twelve potential costs of HARKing. It is argued that these potential costs are either misconceived, misattributed to HARKing, lacking evidence, or that they do not take into account pre- and post-publication peer review and public availability to research materials and data. It is concluded that it is premature to conclude that HARKing has led to low replication rates.","tags":["Preregistration"],"title":"The Costs of HARKing","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1689122843,"objectID":"204165679e71fb1c2d2e3d999b4f9155","permalink":"https://forrt.org/curated_resources/the-crisis-of-confidence-in-social-psych/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/the-crisis-of-confidence-in-social-psych/","section":"curated_resources","summary":"Notes that social psychologists' early enthusiasm has been replaced by serious doubts about the future of their field. Difficulties in conducting research, unfulfilled expectations about research payoffs, and outside pressures had all contributed to a sense of crisis. Relief may come from acceptance of theoretical and methodological pluralism, from reevaluation of research expectations and ethical stances, and from the development of realistic responses to societal demands.","tags":["Reproducibility Crisis and Credibility Revolution","Open Science"],"title":"The crisis of confidence in social psychology.","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"81fccb880e3c8ddcb5126dd6ec3c4707","permalink":"https://forrt.org/curated_resources/the-cumulative-effect-of-reporting-and-c/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/the-cumulative-effect-of-reporting-and-c/","section":"curated_resources","summary":"Evidence-based medicine is the cornerstone of clinical practice, but it is dependent on the quality of evidence upon which it is based. Unfortunately, up to half of all randomized controlled trials (RCTs) have never been published, and trials with statistically significant findings are more likely to be published than those without (Dwan et al., 2013). Importantly, negative trials face additional hurdles beyond study publication bias that can result in the disappearance of non-significant results (Boutron et al., 2010; Dwan et al., 2013; Duyx et al., 2017). Here, we analyze the cumulative impact of biases on apparent efficacy, and discuss possible remedies, using the evidence base for two effective treatments for depression: antidepressants and psychotherapy.","tags":["Antidepressants","Bias","Citation Bias","Depression","Psychotherapy","Reporting Bias"],"title":"The cumulative effect of reporting and citation biases on the apparent efficacy of treatments: the case of depression","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1689122843,"objectID":"8dca46a99c9a964a6c45c06dc38df565","permalink":"https://forrt.org/curated_resources/the-default-bayesian-test-is-prejudiced/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/the-default-bayesian-test-is-prejudiced/","section":"curated_resources","summary":"When considering any statistical tool I think it is useful to answer the following two practical questions: 1. \"Does it give reasonable answers in realistic circumstances?\" and 2. \"Does it answer a question I am interested in?\" In this post I explain why, for me, when it comes to the default Bayesian test that's starting to pop up in some psychology publications, the answer to both questions is \"no.\"","tags":["Blog"],"title":"The default bayesian test is prejudiced against small effects","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"15f26c380bd7be1a46d7896f9b2eb594","permalink":"https://forrt.org/curated_resources/the-earth-is-flat-p-0-05-significance-th/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/the-earth-is-flat-p-0-05-significance-th/","section":"curated_resources","summary":"The widespread use of ‘statistical significance’ as a license for making a claim of a scientific finding leads to considerable distortion of the scientific process (according to the American Statistical Association). We review why degrading p-values into ‘significant’ and ‘nonsignificant’ contributes to making studies irreproducible, or to making them seem irreproducible. A major problem is that we tend to take small p-values at face value, but mistrust results with larger p-values. In either case, p-values tell little about reliability of research, because they are hardly replicable even if an alternative hypothesis is true. Also significance (p ≤ 0.05) is hardly replicable: at a good statistical power of 80%, two studies will be ‘conflicting’, meaning that one is significant and the other is not, in one third of the cases if there is a true effect. A replication can therefore not be interpreted as having failed only because it is nonsignificant. Many apparent replication failures may thus reflect faulty judgment based on significance thresholds rather than a crisis of unreplicable research. Reliable conclusions on replicability and practical importance of a finding can only be drawn using cumulative evidence from multiple independent studies. However, applying significance thresholds makes cumulative knowledge unreliable. One reason is that with anything but ideal statistical power, significant effect sizes will be biased upwards. Interpreting inflated significant results while ignoring nonsignificant results will thus lead to wrong conclusions. But current incentives to hunt for significance lead to selective reporting and to publication bias against nonsignificant findings. Data dredging, p-hacking, and publication bias should be addressed by removing fixed significance thresholds. Consistent with the recommendations of the late Ronald Fisher, p-values should be interpreted as graded measures of the strength of evidence against the null hypothesis. Also larger p-values offer some evidence against the null hypothesis, and they cannot be interpreted as supporting the null hypothesis, falsely concluding that ‘there is no effect’. Information on possible true effect sizes that are compatible with the data must be obtained from the point estimate, e.g., from a sample average, and from the interval estimate, such as a confidence interval. We review how confusion about interpretation of larger p-values can be traced back to historical disputes among the founders of modern statistics. We further discuss potential arguments against removing significance thresholds, for example that decision rules should rather be more stringent, that sample sizes could decrease, or that p-values should better be completely abandoned. We conclude that whatever method of statistical inference we use, dichotomous threshold thinking must give way to non-automated informed judgment.","tags":["Analysis","Data","Reproducibility","Statistics"],"title":"The earth is flat (p \u003e 0.05): significance thresholds and the crisis of unreplicable research","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1689122843,"objectID":"975ba9557075bbab92e4b0e4c263dd12","permalink":"https://forrt.org/curated_resources/the-earth-is-round-p-05/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/the-earth-is-round-p-05/","section":"curated_resources","summary":"After 4 decades of severe criticism, the ritual of null hypothesis significance testing (mechanical dichotomous decisions around a sacred .05 criterion) still persists. This article reviews the problems with this practice, including near universal misinterpretation of p as the probability that H₀ is false, the misinterpretation that its complement is the probability of successful replication, and the mistaken assumption that if one rejects H₀ one thereby affirms the theory that led to the test. Exploratory data analysis and the use of graphic methods, a steady improvement in and a movement toward standardization in measurement, an emphasis on estimating effect sizes using confidence intervals, and the informed use of available statistical methods are suggested. For generalization, psychologists must finally rely, as has been done in all the older sciences, on replication.","tags":[""],"title":"The earth is round (p \u003c .05).","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"240dd120b62da38f9c8faed7df51968e","permalink":"https://forrt.org/curated_resources/the-economics-of-reproducibility-in-prec/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/the-economics-of-reproducibility-in-prec/","section":"curated_resources","summary":"Low reproducibility rates within life science research undermine cumulative knowledge production and contribute to both delays and costs of therapeutic drug development. An analysis of past studies indicates that the cumulative (total) prevalence of irreproducible preclinical research exceeds 50%, resulting in approximately US$28,000,000,000 (US$28B)/year spent on preclinical research that is not reproducible—in the United States alone. We outline a framework for solutions and a plan for long-term improvements in reproducibility rates that will help to accelerate the discovery of life-saving therapies and cures.","tags":["Drug Discovery","Drug Research and Development","Drug Therapy","Economics","Finance","Internet","Peer Review","Reproducibility"],"title":"The Economics of Reproducibility in Preclinical Research","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1689122843,"objectID":"7e7754e595ce7ba5064723dae85a9356","permalink":"https://forrt.org/curated_resources/the-effect-of-horizontal-eye-movements-o/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/the-effect-of-horizontal-eye-movements-o/","section":"curated_resources","summary":"A growing body of research has suggested that horizontal saccadic eye movements facilitate the retrieval of episodic memories in free recall and recognition memory tasks. Nevertheless, a minority of studies have failed to replicate this effect. This article attempts to resolve the inconsistent results by introducing a novel variant of proponent-skeptic collaboration. The proposed approach combines the features of adversarial collaboration and purely confirmatory preregistered research. Prior to data collection, the adversaries reached consensus on an optimal research design, formulated their expectations, and agreed to submit the findings to an academic journal regardless of the outcome. To increase transparency and secure the purely confirmatory nature of the investigation, the 2 parties set up a publicly available adversarial collaboration agreement that detailed the proposed design and all foreseeable aspects of the data analysis. As anticipated by the skeptics, a series of Bayesian hypothesis tests indicated that horizontal eye movements did not improve free recall performance. The skeptics suggested that the nonreplication may partly reflect the use of suboptimal and questionable research practices in earlier eye movement studies. The proponents countered this suggestion and used a p curve analysis to argue that the effect of horizontal eye movements on explicit memory did not merely reflect selective reporting. ","tags":["Reproducibility Crisis and Credibility Revolution","Open Science","Transparency"],"title":"The effect of horizontal eye movements on free recall: A preregistered adversarial collaboration.","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"2336a0777b9e80a602ed1d63168dace4","permalink":"https://forrt.org/curated_resources/the-effect-of-preregistration-on-trust-i/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/the-effect-of-preregistration-on-trust-i/","section":"curated_resources","summary":"The crisis of confidence has undermined the trust that researchers place in the findings of their peers. In order to increase trust in research, initiatives such as preregistration have been suggested, which aim to prevent various questionable research practices. As it stands, however, no empirical evidence exists that preregistration does increase perceptions of trust. The picture may be complicated by a researcher's familiarity with the author of the study, regardless of the preregistration status of the research. This registered report presents an empirical assessment of the extent to which preregistration increases the trust of 209 active academics in the reported outcomes, and how familiarity with another researcher influences that trust. Contrary to our expectations, we report ambiguous Bayes factors and conclude that we do not have strong evidence towards answering our research questions. Our findings are presented along with evidence that our manipulations were ineffective for many participants, leading to the exclusion of 68% of complete datasets, and an underpowered design as a consequence. We discuss other limitations and confounds which may explain why the findings of the study deviate from a previously conducted pilot study. We reflect on the benefits of using the registered report submission format in light of our results. ","tags":["Preregistration","Registered Reporting","Trustworthiness","Questionable Research Practice"],"title":"The effect of preregistration on trust in empirical research findings: results of a registered report","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"46b859537544cc95f68fe54528200c9d","permalink":"https://forrt.org/curated_resources/the-effect-of-publishing-peer-review-rep/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/the-effect-of-publishing-peer-review-rep/","section":"curated_resources","summary":"To increase transparency in science, some scholarly journals are publishing peer review reports. But it is unclear how this practice affects the peer review process. Here, we examine the effect of publishing peer review reports on referee behavior in five scholarly journals involved in a pilot study at Elsevier. By considering 9,220 submissions and 18,525 reviews from 2010 to 2017, we measured changes both before and during the pilot and found that publishing reports did not significantly compromise referees’ willingness to review, recommendations, or turn-around times. Younger and non-academic scholars were more willing to accept to review and provided more positive and objective recommendations. Male referees tended to write more constructive reports during the pilot. Only 8.1% of referees agreed to reveal their identity in the published report. These findings suggest that open peer review does not compromise the process, at least when referees are able to protect their anonymity.","tags":["Peer Review","Publishing"],"title":"The effect of publishing peer review reports on referee behavior in five scholarly journals","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1689122843,"objectID":"cdf2cb39c742eb09eaac0bf32a6a1034","permalink":"https://forrt.org/curated_resources/the-empirical-benefits-of-conceptual-rig/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/the-empirical-benefits-of-conceptual-rig/","section":"curated_resources","summary":"Most discussions of rigor and replication focus on empirical practices (methods used to collect and analyze data). Typically overlooked is the role of conceptual practices: the methods scientists use to arrive at and articulate research hypotheses in the first place. This article discusses how the conceptualization of research hypotheses has implications for methodological decision-making and, consequently, for the replicability of results. The article identifies three ways in which empirical findings may be non-replicable, and shows how all three kinds of non-replicability are more likely to emerge when scientists take an informal conceptual approach, in which personal predictions are equated with scientific hypotheses. The risk of non-replicability may be reduced if scientists adopt more formal conceptual practices, characterized by the rigorous use of “if–then” logic to articulate hypotheses, and to systematically diagnose the plausibility, size, and context-dependence of hypothesized effects. The article identifies benefits that are likely to arise from more rigorous and systematic conceptual practices, and identifies ways in which their use can be encouraged to be more normative within the scholarly culture of the psychological sciences.","tags":["Reproducibility Crisis and Credibility Revolution","Open Science"],"title":"The empirical benefits of conceptual rigor: Systematic articulation of conceptual hypotheses can reduce the risk of non-replicable results (and facilitate novel discoveries too)","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1689122843,"objectID":"1b227c087a56e2019d9714beb5bde963","permalink":"https://forrt.org/curated_resources/the-empirical-march-making-science-bette/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/the-empirical-march-making-science-bette/","section":"curated_resources","summary":"Psychology has been criticized recently for a range of research quality issues. The current article organizes these problems around the actions of the individual researcher and the existing norms of the field. Proposed solutions align the incentives of all those involved in the research process. I recommend moving away from a focus on statistical significance to one of statistical power, renewing an emphasis on prediction and the pre-registration of hypotheses, changing the timing and method of peer-review, and increasing the rate at which replications are conducted and published. These strategies seek to unify incentives toward increased methodological and statistical rigor to more effectively and efficiently reduce bias and error.","tags":["Reproducibility Crisis and Credibility Revolution","Open Science"],"title":"The Empirical March: Making Science Better at Self-Correction","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1689122843,"objectID":"9961d857a4c8dcb1a87727db01993ea1","permalink":"https://forrt.org/curated_resources/the-existence-of-publication-bias-and-ri/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/the-existence-of-publication-bias-and-ri/","section":"curated_resources","summary":"Publication bias is the tendency on the parts of investigators, reviewers, and editors to submit or accept manuscripts for publication based on the direction or strength of the study findings. Much of what has been learned about publication bias comes from the social sciences, less from the field of medicine. In medicine, three studies have provided direct evidence for this bias. Prevention of publication bias is important both from the scientific perspective (complete dissemination of knowledge) and from the perspective of those who combine results from a number of similar studies (meta-analysis). If treatment decisions are based on the published literature, then the literature must include all available data that is of acceptable quality. Currently, obtaining information regarding all studies undertaken in a given field is difficult, even impossible. Registration of clinical trials, and perhaps other types of studies, is the direction in which the scientific community should move.","tags":["Reproducibility Crisis and Credibility Revolution","Open Science"],"title":"The existence of publication bias and risk factors for its occurrence.","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1689122843,"objectID":"feef90a1b70f0ee2d619d906f6a4f2ff","permalink":"https://forrt.org/curated_resources/the-experiment-experiment/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/the-experiment-experiment/","section":"curated_resources","summary":"A few years back, a famous psychologist published a series of studies that found people could predict the future — not all the time, but more often than if they were guessing by chance alone.The paper left psychologists with two options. \"Either we have to conclude that ESP is true,\" says Brian Nosek, a psychologist at the University of Virginia, \"or we have to change our beliefs about the right ways to do science.\" Nosek is going with Option B — and not just for psychology experiments. He thinks there's something wrong with the way we're doing science. And he launched a massive project to try to fix it.","tags":["Podcast","Reproducibility Knowledge","Reproducibility Crisis and Credibility Revolution"],"title":"The Experiment Experiment","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1689122843,"objectID":"a2516b7e02cff4d202e957091f971282","permalink":"https://forrt.org/curated_resources/the-extent-and-consequences-of-p-hacking/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/the-extent-and-consequences-of-p-hacking/","section":"curated_resources","summary":"A focus on novel, confirmatory, and statistically significant results leads to substantial bias in the scientific literature. One type of bias, known as \"p-hacking,\" occurs when researchers collect or select data or statistical analyses until nonsignificant results become significant. Here, we use text-mining to demonstrate that p-hacking is widespread throughout science. We then illustrate how one can test for p-hacking when performing a meta-analysis and show that, while p-hacking is probably common, its effect seems to be weak relative to the real effect sizes being measured. This result suggests that p-hacking probably does not drastically alter scientific consensuses drawn from meta-analyses.","tags":[""],"title":"The Extent and Consequences of P-Hacking in Science","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1689122843,"objectID":"14f6d31222f7e3d9cff7c4bd66f15190","permalink":"https://forrt.org/curated_resources/the-fickle-p-value-generates-irreproduci/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/the-fickle-p-value-generates-irreproduci/","section":"curated_resources","summary":"The reliability and reproducibility of science are under scrutiny. However, a major cause of this lack of repeatability is not being considered: the wide sample-to-sample variability in the P value. We explain why P is fickle to discourage the ill-informed practice of interpreting analyses based predominantly on this statistic.","tags":[""],"title":"The fickle P value generates irreproducible results","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1689122843,"objectID":"afe09b022f05e4f7bafeec4c3eff2f89","permalink":"https://forrt.org/curated_resources/the-file-drawer-problem-and-tolerance-fo/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/the-file-drawer-problem-and-tolerance-fo/","section":"curated_resources","summary":"For any given research area, one cannot tell how many studies have been conducted but never reported. The extreme view of the \"file drawer problem\" is that journals are filled with the 5% of the studies that show Type I errors, while the file drawers are filled with the 95% of the studies that show nonsignificant results. Quantitative procedures for computing the tolerance for filed and future null results are reported and illustrated, and the implications are discussed. ","tags":["Reproducibility Crisis and Credibility Revolution","Open Science"],"title":"The file drawer problem and tolerance for null results.","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1689122843,"objectID":"912051e497c0b02ea37d7dfc137ccf37","permalink":"https://forrt.org/curated_resources/the-frequency-of-excess-success-for-arti/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/the-frequency-of-excess-success-for-arti/","section":"curated_resources","summary":"Recent controversies have questioned the quality of scientific practice in the field of psychology, but these concerns are often based on anecdotes and seemingly isolated cases. To gain a broader perspective, this article applies an objective test for excess success to a large set of articles published in the journal Psychological Science between 2009 and 2012. When empirical studies succeed at a rate much higher than is appropriate for the estimated effects and sample sizes, readers should suspect that unsuccessful findings have been suppressed, the experiments or analyses were improper, or the theory does not properly account for the data. In total, problems appeared for 82 % (36 out of 44) of the articles in Psychological Science that had four or more experiments and could be analyzed.","tags":["Reproducibility Crisis and Credibility Revolution","Open Science"],"title":"The frequency of excess success for articles in Psychological Science.","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1689122843,"objectID":"77d9ae1422e55c75ed0f24a031e00eb6","permalink":"https://forrt.org/curated_resources/the-garden-of-forking-paths-why-multiple/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/the-garden-of-forking-paths-why-multiple/","section":"curated_resources","summary":"Data-dependent analysis—a “garden of forking paths”— explains why many statistically significant comparisons don't hold up. ","tags":[""],"title":"The garden of forking paths: Why multiple comparisons can be a problem, even when there is no “fishing expedition” or “p-hacking” and the research hypothesis was posited ahead of time","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"090dbd2adc083218c88eb0f679af76fe","permalink":"https://forrt.org/curated_resources/the-general-linear-model-semester-2/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/the-general-linear-model-semester-2/","section":"curated_resources","summary":"A syllabi used for general linear model","tags":["Statistics"],"title":"The General Linear Model: Semester 2","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1689122843,"objectID":"c30b2540d8f4e307ac9b8a1575c62397","permalink":"https://forrt.org/curated_resources/the-generalizability-of-survey-experimen/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/the-generalizability-of-survey-experimen/","section":"curated_resources","summary":"Survey experiments have become a central methodology across the social sciences. Researchers can combine experiments’ causal power with the generalizability of population-based samples. Yet, due to the expense of population-based samples, much research relies on convenience samples (e.g. students, online opt-in samples). The emergence of affordable, but non-representative online samples has reinvigorated debates about the external validity of experiments. We conduct two studies of how experimental treatment effects obtained from convenience samples compare to effects produced by population samples. In Study 1, we compare effect estimates from four different types of convenience samples and a population-based sample. In Study 2, we analyze treatment effects obtained from 20 experiments implemented on a population-based sample and Amazon’s Mechanical Turk (MTurk). The results reveal considerable similarity between many treatment effects obtained from convenience and nationally representative population-based samples. While the results thus bolster confidence in the utility of convenience samples, we conclude with guidance for the use of a multitude of samples for advancing scientific knowledge.","tags":["Reproducibility Crisis and Credibility Revolution","Open Science"],"title":"The Generalizability of Survey Experiments","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1689122843,"objectID":"9c20007b104389d2b0153df3e0d833d7","permalink":"https://forrt.org/curated_resources/the-grim-test-a-simple-technique-detects/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/the-grim-test-a-simple-technique-detects/","section":"curated_resources","summary":"We present a simple mathematical technique that we call granularity-related inconsistency of means (GRIM) for verifying the summary statistics of research reports in psychology. This technique evaluates whether the reported means of integer data such as Likert-type scales are consistent with the given sample size and number of items. We tested this technique with a sample of 260 recent empirical articles in leading journals. Of the articles that we could test with the GRIM technique (N = 71), around half (N = 36) appeared to contain at least one inconsistent mean, and more than 20% (N = 16) contained multiple such inconsistencies. We requested the data sets corresponding to 21 of these articles, receiving positive responses in 9 cases. We confirmed the presence of at least one reporting error in all cases, with three articles requiring extensive corrections. The implications for the reliability and replicability of empirical psychology are discussed.","tags":["Reproducibility Crisis and Credibility Revolution","Open Science"],"title":"The GRIM Test: A Simple Technique Detects Numerous Anomalies in the Reporting of Results in Psychology","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1689122843,"objectID":"8037eb2267975dc5e547856eb208bf97","permalink":"https://forrt.org/curated_resources/the-hardest-science/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/the-hardest-science/","section":"curated_resources","summary":"Blogposts about psychology, reproducibility, replication etc.","tags":["Blog","Reproducibility Knowledge"],"title":"The Hardest Science","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1689122843,"objectID":"f3b34bdfe5f502bf2feced25fbcd8a22","permalink":"https://forrt.org/curated_resources/the-harm-done-by-tests-of-significance/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/the-harm-done-by-tests-of-significance/","section":"curated_resources","summary":"Three historical episodes in which the application of null hypothesis significance testing (NHST) led to the mis-interpretation of data are described. It is argued that the pervasive use of this statistical ritual impedes the accumulation of knowledge and is unfit for use.","tags":[""],"title":"The harm done by tests of significance","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"38fba1ee61a46aed7475742aada03847","permalink":"https://forrt.org/curated_resources/the-influence-of-journal-submission-guid/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/the-influence-of-journal-submission-guid/","section":"curated_resources","summary":"From January 2014, Psychological Science introduced new submission guidelines that encouraged the use of effect sizes, estimation, and meta-analysis (the “new statistics”), required extra detail of methods, and offered badges for use of open science practices. We investigated the use of these practices in empirical articles published by Psychological Science and, for comparison, by the Journal of Experimental Psychology: General, during the period of January 2013 to December 2015. The use of null hypothesis significance testing (NHST) was extremely high at all times and in both journals. In Psychological Science, the use of confidence intervals increased markedly overall, from 28% of articles in 2013 to 70% in 2015, as did the availability of open data (3 to 39%) and open materials (7 to 31%). The other journal showed smaller or much smaller changes. Our findings suggest that journal-specific submission guidelines may encourage desirable changes in authors’ practices.","tags":["Data","Data Management","Experimental Psychology","Meta-analysis","Open Data","Open Science","Peer Review","Publishing","Research Reporting Guidelines","Scientific Publishing"],"title":"The influence of journal submission guidelines on authors' reporting of statistics and use of open research practices","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1689122843,"objectID":"71000c3fb266896d61eb63b322edce5e","permalink":"https://forrt.org/curated_resources/the-ironic-effect-of-significant-results/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/the-ironic-effect-of-significant-results/","section":"curated_resources","summary":"Cohen (1962) pointed out the importance of statistical power for psychology as a science, but statistical power of studies has not increased, while the number of studies in a single article has increased. It has been overlooked that multiple studies with modest power have a high probability of producing nonsignificant results because power decreases as a function of the number of statistical tests that are being conducted (Maxwell, 2004). The discrepancy between the expected number of significant results and the actual number of significant results in multiple-study articles undermines the credibility of the reported results, and it is likely that questionable research practices have contributed to the reporting of too many significant results (Sterling, 1959). The problem of low power in multiple-study articles is illustrated using Bem's (2011) article on extrasensory perception and Gailliot et al.'s (2007) article on glucose and self-regulation. I conclude with several recommendations that can increase the credibility of scientific evidence in psychological journals. One major recommendation is to pay more attention to the power of studies to produce positive results without the help of questionable research practices and to request that authors justify sample sizes with a priori predictions of effect sizes. It is also important to publish replication studies with nonsignificant results if these studies have high power to replicate a published finding.","tags":[""],"title":"The ironic effect of significant results on the credibility of multiple-study articles.","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1689122843,"objectID":"fd77ae05def0bd230c519df756a51149","permalink":"https://forrt.org/curated_resources/the-meaning-of-significance-for-differen/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/the-meaning-of-significance-for-differen/","section":"curated_resources","summary":"Adrianus Dingeman de Groot (1914-2006) was one of the most influential Dutch psychologists. He became famous for his work \"Thought and Choice in Chess\", but his main contribution was methodological--De Groot co-founded the Department of Psychological Methods at the University of Amsterdam (together with R. F. van Naerssen), founded one of the leading testing and assessment companies (CITO), and wrote the monograph \"Methodology\" that centers on the empirical-scientific cycle: observation-induction-deduction-testing-evaluation. Here we translate one of De Groot's early articles, published in 1956 in the Dutch journal Nederlands Tijdschrift voor de Psychologie en Haar Grensgebieden. This article is more topical now than it was almost 60years ago. De Groot stresses the difference between exploratory and confirmatory (\"hypothesis testing\") research and argues that statistical inference is only sensible for the latter: \"One 'is allowed' to apply statistical tests in exploratory research, just as long as one realizes that they do not have evidential impact\". De Groot may have also been one of the first psychologists to argue explicitly for preregistration of experiments and the associated plan of statistical analysis. The appendix provides annotations that connect De Groot's arguments to the current-day debate on transparency and reproducibility in psychological science.","tags":["Reproducibility Crisis and Credibility Revolution","Open Science","Transparency"],"title":"The meaning of “significance” for different types of research","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"432d6ae81c7f9d5002e693aee03e4a3e","permalink":"https://forrt.org/curated_resources/the-meaningfulness-of-effect-sizes-in-ps/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/the-meaningfulness-of-effect-sizes-in-ps/","section":"curated_resources","summary":"Effect sizes are the currency of psychological research. They quantify the results of a study to answer the research question and are used to calculate statistical power. The interpretation of effect sizes—when is an effect small, medium, or large?—has been guided by the recommendations Jacob Cohen gave in his pioneering writings starting in 1962: Either compare an effect with the effects found in past research or use certain conventional benchmarks. The present analysis shows that neither of these recommendations is currently applicable. From past publications without pre-registration, 900 effects were randomly drawn and compared with 93 effects from publications with pre-registration, revealing a large difference: Effects from the former (median r = .36) were much larger than effects from the latter (median r = .16). That is, certain biases, such as publication bias or questionable research practices, have caused a dramatic inflation in published effects, making it difficult to compare an actual effect with the real population effects (as these are unknown). In addition, there were very large differences in the mean effects between psychological sub-disciplines and between different study designs, making it impossible to apply any global benchmarks. Many more pre-registered studies are needed in the future to derive a reliable picture of real population effects.","tags":["Cohen","Effect Size","Power","Publication Bias","Replicability","Replication","Sample Size"],"title":"The Meaningfulness of Effect Sizes in Psychological Research: Differences Between Sub-Disciplines and the Impact of Potential Biases","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1689122843,"objectID":"a81799369b68c2269858aed8b700b6d7","permalink":"https://forrt.org/curated_resources/the-missing-semester-of-your-cs-educatio/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/the-missing-semester-of-your-cs-educatio/","section":"curated_resources","summary":"Course on computer sciences skills needed for all scientific research","tags":["Computer Sciences"],"title":"The Missing Semester of Your CS Education","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1689122843,"objectID":"9b5c5a063899de2a030a9fa44662bfb4","permalink":"https://forrt.org/curated_resources/the-n-pact-factor-evaluating-the-quality/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/the-n-pact-factor-evaluating-the-quality/","section":"curated_resources","summary":"The authors evaluate the quality of research reported in major journals in social-personality psychology by ranking those journals with respect to their N-pact Factors (NF)—the statistical power of the empirical studies they publish to detect typical effect sizes. Power is a particularly important attribute for evaluating research quality because, relative to studies that have low power, studies that have high power are more likely to (a) to provide accurate estimates of effects, (b) to produce literatures with low false positive rates, and (c) to lead to replicable findings. The authors show that the average sample size in social-personality research is 104 and that the power to detect the typical effect size in the field is approximately 50%. Moreover, they show that there is considerable variation among journals in sample sizes and power of the studies they publish, with some journals consistently publishing higher power studies than others. The authors hope that these rankings will be of use to authors who are choosing where to submit their best work, provide hiring and promotion committees with a superior way of quantifying journal quality, and encourage competition among journals to improve their NF rankings.","tags":[""],"title":"The N-Pact Factor: Evaluating the Quality of Empirical Journals with Respect to Sample Size and Statistical Power","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1689122843,"objectID":"44b0bce8327c43d0e4e011d887892ce2","permalink":"https://forrt.org/curated_resources/the-natural-selection-of-bad-science/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/the-natural-selection-of-bad-science/","section":"curated_resources","summary":"Poor research design and data analysis encourage false-positive findings. Such poor methods persist despite perennial calls for improvement, suggesting that they result from something more than just misunderstanding. The persistence of poor methods results partly from incentives that favour them, leading to the natural selection of bad science. This dynamic requires no conscious strategizing—no deliberate cheating nor loafing—by scientists, only that publication is a principal factor for career advancement. Some normative methods of analysis have almost certainly been selected to further publication instead of discovery. In order to improve the culture of science, a shift must be made away from correcting misunderstandings and towards rewarding understanding. We support this argument with empirical evidence and computational modelling. We first present a 60-year meta-analysis of statistical power in the behavioural sciences and show that power has not improved despite repeated demonstrations of the necessity of increasing power. To demonstrate the logical consequences of structural incentives, we then present a dynamic model of scientific communities in which competing laboratories investigate novel or previously published hypotheses using culturally transmitted research methods. As in the real world, successful labs produce more ‘progeny,’ such that their methods are more often copied and their students are more likely to start labs of their own. Selection for high output leads to poorer methods and increasingly high false discovery rates. We additionally show that replication slows but does not stop the process of methodological deterioration. Improving the quality of research requires change at the institutional level.","tags":["Reproducibility Crisis and Credibility Revolution","Open Science"],"title":"The natural selection of bad science.","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"a8d0721a36aa06553b93b872692f33db","permalink":"https://forrt.org/curated_resources/the-need-for-public-opinion-and-survey-m/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/the-need-for-public-opinion-and-survey-m/","section":"curated_resources","summary":"Survey researchers take great care to measure respondents’ answers in an unbiased way; but, how successful are we as a field at remedying unintended and intended biases in our research? The validity of inferences drawn from studies has been found to be improved by the implementation of preregistration practices. Despite this, only 3 of the 83 published articles in POQ and IJPOR in 2020 feature explicitly stated preregistered hypotheses or analyses. This manuscript aims to show survey methodologists how preregistration and replication (where possible) are in service to the broader mission of survey methodology. To that end, we present a practical example of how unknown biases in analysis strategies without preregistration or replication inflate type I errors. In an initial data collection, our analysis showed that the visual layout of battery-type questions significantly decreased data quality. But after committing to replicating and preregistering the hypotheses and analysis plans, none of the results replicated successfully, despite keeping the procedure, sample provider, and analyses identical. This manuscript illustrates how preregistration and replication practices might, in the long term, likely help unburden the academic literature from follow-up publications relying on type I errors.","tags":["Preregistration","Replication","Public Opinion"],"title":"The need for public opinion and survey methodology research to embrace preregistration and replication, exemplified by a team’s failure to replicate their own findings on visual cues in grid-type questions","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1689122843,"objectID":"f20ad6b9d4e04dcee9995d4bc389344d","permalink":"https://forrt.org/curated_resources/the-new-statistics-confidence-intervals/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/the-new-statistics-confidence-intervals/","section":"curated_resources","summary":"A video about Confidence Intervals, NHST, and p Values","tags":["Video"],"title":"The New Statistics: Confidence Intervals, NHST, and p Values (Workshop Part 1)","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1689122843,"objectID":"143aa0843ea56ded23370da509d90194","permalink":"https://forrt.org/curated_resources/the-new-statistics-effect-sizes-and-conf/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/the-new-statistics-effect-sizes-and-conf/","section":"curated_resources","summary":"A video about effect sizes and confidence intervals","tags":["Video"],"title":"The New Statistics: Effect Sizes and Confidence Intervals (Workshop Part 3)","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1689122843,"objectID":"5894646316b4e4bebf96453184a9bfd2","permalink":"https://forrt.org/curated_resources/the-new-statistics-meta-analysis-and-met/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/the-new-statistics-meta-analysis-and-met/","section":"curated_resources","summary":"A video about meta analysis and meta-analytical thinking","tags":["Video"],"title":"The New Statistics: Meta-Analysis and Meta-Analytic Thinking (workshop Part 6)","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1689122843,"objectID":"927e87720e4389fb24f923d826e5a183","permalink":"https://forrt.org/curated_resources/the-new-statistics-planning-power-and-pr/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/the-new-statistics-planning-power-and-pr/","section":"curated_resources","summary":"A video about power analysis and precision","tags":["Video"],"title":"The New Statistics: Planning, Power, and Precision (Workshop Part 5)","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1689122843,"objectID":"4ac68762b46831117cf02088f11af1c6","permalink":"https://forrt.org/curated_resources/the-new-statistics-research-integrity-th/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/the-new-statistics-research-integrity-th/","section":"curated_resources","summary":"A video about Research Integrity \u0026 the New Statistics","tags":["Video","Reproducibility Knowledge"],"title":"The New Statistics: Research Integrity \u0026 the New Statistics (Workshop Part 2)","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1689122843,"objectID":"e592a7f0cb9370b759ad799f7a2a1b25","permalink":"https://forrt.org/curated_resources/the-new-statistics-the-new-statistics-in/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/the-new-statistics-the-new-statistics-in/","section":"curated_resources","summary":"A video about New statistics in action","tags":["Video"],"title":"The New Statistics: The New Statistics in Action (Workshop Part 4)","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1689122843,"objectID":"fad172f242274a5e1920b43015e2a90c","permalink":"https://forrt.org/curated_resources/the-new-statistics-why-and-how/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/the-new-statistics-why-and-how/","section":"curated_resources","summary":"We need to make substantial changes to how we conduct research. First, in response to heightened concern that our published research literature is incomplete and untrustworthy, we need new requirements to ensure research integrity. These include prespecification of studies whenever possible, avoidance of selection and other inappropriate data-analytic practices, complete reporting, and encouragement of replication. Second, in response to renewed recognition of the severe flaws of null-hypothesis significance testing (NHST), we need to shift from reliance on NHST to estimation and other preferred techniques. The new statistics refers to recommended practices, including estimation based on effect sizes, confidence intervals, and meta-analysis. The techniques are not new, but adopting them widely would be new for many researchers, as well as highly beneficial. This article explains why the new statistics are important and offers guidance for their use. It describes an eight step new-statistics strategy for research with integrity, which starts with formulation of research questions in estimation terms, has no place for NHST, and is aimed at building a cumulative quantitative discipline.","tags":[""],"title":"The new statistics: Why and how","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1689122843,"objectID":"bba00ad670e375b484ce01dd5f72f003","permalink":"https://forrt.org/curated_resources/the-null-hypothesis-significance-testing/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/the-null-hypothesis-significance-testing/","section":"curated_resources","summary":"A chapter about null hypothesis significance testing in personality research","tags":["Reproducibility Crisis and Credibility Revolution","Open Science"],"title":"The Null Hypothesis Significance-Testing Debate and Its Implications for Personality Research","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1689122843,"objectID":"04d9b4f7c89b12cb1af550bdb7c50e21","permalink":"https://forrt.org/curated_resources/the-null-ritual-what-you-always-wanted-t/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/the-null-ritual-what-you-always-wanted-t/","section":"curated_resources","summary":"A chapter about significance testing","tags":[""],"title":"The Null Ritual What You Always Wanted to Know About Significance Testing but Were Afraid to Ask","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"4735f5e49e04a9384c64388f8fbfbf56","permalink":"https://forrt.org/curated_resources/the-open-research-lifecycle/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/the-open-research-lifecycle/","section":"curated_resources","summary":"Open science reduces waste and accelerates the discovery of knowledge, solutions, and cures for the world's most pressing needs. Shifting research culture toward greater openness, transparency, and reproducibility is challenging, but there are incremental steps at every stage of the research lifecycle that can improve rigor and reduce waste. Visit cos.io to learn more.","tags":["Center for Open Science","Open Science","Open Science Framework","Open Science Research Lifecycle","OSF","Reproducibility","Research","Research Best Practices","Research Integrity","Research Lifecycle","Research Rigor","Research Transparency"],"title":"The Open Research Lifecycle","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1689122843,"objectID":"84a7ccc2980fa745ee8c456c9716bd97","permalink":"https://forrt.org/curated_resources/the-open-science-training-handbook/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/the-open-science-training-handbook/","section":"curated_resources","summary":"A collection about open science","tags":[""],"title":"The Open Science Training Handbook","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"1c2cdc0cb764aaed023269e73075d117","permalink":"https://forrt.org/curated_resources/the-p-value-misconception-eradication-ch/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/the-p-value-misconception-eradication-ch/","section":"curated_resources","summary":"If you have educational material that you think will do a better job at preventing p-value misconceptions than the material in my MOOC, join the p-value misconception eradication challenge by proposing an improvement to my current material in a new A/B test in my MOOC.","tags":["Teaching"],"title":"The p-value misconception eradication challenge","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"fba51f5c22e790b7158197e2bc2a1c85","permalink":"https://forrt.org/curated_resources/the-past-present-and-future-of-registere/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/the-past-present-and-future-of-registere/","section":"curated_resources","summary":"Registered Reports are a form of empirical publication in which study proposals are peer reviewed and pre-accepted before research is undertaken. By deciding which articles are published based on the question, theory and methods, Registered Reports offer a remedy for a range of reporting and publication biases. Here, we reflect on the history, progress and future prospects of the Registered Reports initiative and offer practical guidance for authors, reviewers and editors. We review early evidence that Registered Reports are working as intended, while at the same time acknowledging that they are not a universal solution for irreproducibility. We also consider how the policies and practices surrounding Registered Reports are changing, or must change in the future, to address limitations and adapt to new challenges. We conclude that Registered Reports are promoting reproducibility, transparency and self-correction across disciplines and may help reshape how society evaluates research and researchers.","tags":["Culture","Publishing"],"title":"The past, present and future of Registered Reports","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1689122843,"objectID":"d770da64441bb27cf57aa541e41c8089","permalink":"https://forrt.org/curated_resources/the-peer-reviewers-openness-initiative-i/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/the-peer-reviewers-openness-initiative-i/","section":"curated_resources","summary":"Openness is one of the central values of science. Open scientific practices such as sharing data, materials and analysis scripts alongside published articles have many benefits, including easier replication and extension studies, increased availability of data for theory-building and meta-analysis, and increased possibility of review and collaboration even after a paper has been published. Although modern information technology makes sharing easier than ever before, uptake of open practices had been slow. We suggest this might be in part due to a social dilemma arising from misaligned incentives and propose a specific, concrete mechanism—reviewers withholding comprehensive review—to achieve the goal of creating the expectation of open practices as a matter of scientific principle.","tags":["Transparency","Peer-Review"],"title":"The Peer Reviewers’ Openness Initiative: incentivizing open research practices through peer review. ","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1689122843,"objectID":"38cd950518e6fc388906dfa7e7df9301","permalink":"https://forrt.org/curated_resources/the-persistence-of-underpowered-studies/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/the-persistence-of-underpowered-studies/","section":"curated_resources","summary":"Underpowered studies persist in the psychological literature. This article examines reasons for their persistence and the effects on efforts to create a cumulative science. The \"curse of multiplicities\" plays a central role in the presentation. Most psychologists realize that testing multiple hypotheses in a single study affects the Type I error rate, but corresponding implications for power have largely been ignored. The presence of multiple hypothesis tests leads to 3 different conceptualizations of power. Implications of these 3 conceptualizations are discussed from the perspective of the individual researcher and from the perspective of developing a coherent literature. Supplementing significance tests with effect size measures and confidence intervals is shown to address some but not necessarily all problems associated with multiple testing","tags":["Reproducibility Crisis and Credibility Revolution","Open Science"],"title":"The Persistence of Underpowered Studies in Psychological Research: Causes, Consequences, and Remedies","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1689122843,"objectID":"dd21e92e926a164c77307b6c2ed2334b","permalink":"https://forrt.org/curated_resources/the-pipeline-project-pre-publication-ind/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/the-pipeline-project-pre-publication-ind/","section":"curated_resources","summary":"This crowdsourced project introduces a collaborative approach to improving the reproducibility of scientific research, in which findings are replicated in qualified independent laboratories before (rather than after) they are published. Our goal is to establish a non-adversarial replication process with highly informative final results. To illustrate the Pre-Publication Independent Replication (PPIR) approach, 25 research groups conducted replications of all ten moral judgment effects which the last author and his collaborators had “in the pipeline” as of August 2014. Six findings replicated according to all replication criteria, one finding replicated but with a significantly smaller effect size than the original, one finding replicated consistently in the original culture but not outside of it, and two findings failed to find support. In total, 40% of the original findings failed at least one major replication criterion. Potential ways to implement and incentivize pre-publication independent replication on a large scale are discussed.","tags":["Reproducibility Crisis and Credibility Revolution","Open Science"],"title":"The pipeline project: Pre-publication independent replications of a single laboratory's research pipeline","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1689122843,"objectID":"9e3bb3feca66285134f98a2d259385c6","permalink":"https://forrt.org/curated_resources/the-poor-availability-of-psychological-r/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/the-poor-availability-of-psychological-r/","section":"curated_resources","summary":"The origin of the present comment lies in a failed attempt to obtain, through e-mailed requests, data reported in 141 empirical articles recently published by the American Psychological Association (APA). Our original aim was to reanalyze these data sets to assess the robustness of the research findings to outliers. We never got that far. In June 2005, we contacted the corresponding author of every article that appeared in the last two 2004 issues of four major APA journals. Because their articles had been published in APA journals, we were certain that all of the authors had signed the APA Certification of Compliance With APA Ethical Principles, which includes the principle on sharing data for reanalysis. Unfortunately, 6 months later, after writing more than 400 e-mails--and sending some corresponding authors detailed descriptions of our study aims, approvals of our ethical committee, signed assurances not to share data with others, and even our full resumes-we ended up with a meager 38 positive reactions and the actual data sets from 64 studies (25.7% of the total number of 249 data sets). This means that 73% of the authors did not share their data.","tags":["Reproducibility Crisis and Credibility Revolution","Open Science"],"title":"The poor availability of psychological research data for reanalysis.","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1689122843,"objectID":"6d6c70bd0dd8576924db0b0b4ed4b4ba","permalink":"https://forrt.org/curated_resources/the-poor-availability-of-syntaxes-of-str/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/the-poor-availability-of-syntaxes-of-str/","section":"curated_resources","summary":"The syntax or codes used to fit Structural Equation Models (SEMs) convey valuable information on model specifications and the manner in which SEMs are estimated. We requested SEM syntaxes from a random sample of 229 articles (published in 1998–2013) that ran SEMs using LISREL, AMOS, or Mplus. After exchanging over 500 emails, we ended up obtaining a meagre 57 syntaxes used in these articles (24.9% of syntaxes we requested). Results considering the 129 (corresponding) authors who replied to our request showed that the odds of the syntax being lost increased by 21% per year passed since publication of the article, while the odds of actually obtaining a syntax dropped by 13% per year. So SEM syntaxes that are crucial for reproducibility and for correcting errors in the running and reporting of SEMs are often unavailable and get lost rapidly. The preferred solution is mandatory sharing of SEM syntaxes alongside articles or in data repositories.","tags":[""],"title":"The poor availability of syntaxes of structural equation modeling","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"4fbaeadaa37051cad3455dcb56c693df","permalink":"https://forrt.org/curated_resources/the-post-embargo-open-access-citation-ad/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/the-post-embargo-open-access-citation-ad/","section":"curated_resources","summary":"Many studies show that open access (OA) articles—articles from scholarly journals made freely available to readers without requiring subscription fees—are downloaded, and presumably read, more often than closed access/subscription-only articles. Assertions that OA articles are also cited more often generate more controversy. Confounding factors (authors may self-select only the best articles to make OA; absence of an appropriate control group of non-OA articles with which to compare citation figures; conflation of pre-publication vs. published/publisher versions of articles, etc.) make demonstrating a real citation difference difficult. This study addresses those factors and shows that an open access citation advantage as high as 19% exists, even when articles are embargoed during some or all of their prime citation years. Not surprisingly, better (defined as above median) articles gain more when made OA.","tags":["Careers","Citation Analysis","Institutional Repositories","Medicine and Health Sciences","Open Access Publishing","Peer Review","Physical Sciences","Scientific Publishing"],"title":"The Post-Embargo Open Access Citation Advantage: It Exists (Probably), It’s Modest (Usually), and the Rich Get Richer (of Course)","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"254c1a496601d408f7ef4f36617d2dac","permalink":"https://forrt.org/curated_resources/the-preregistration-challenge-a-how-to-g/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/the-preregistration-challenge-a-how-to-g/","section":"curated_resources","summary":"This video shows interested researchers how to get started on their own preregistration as part of the Preregistration Challenge. Learn how to create a new draft, find example preregistrations from different fields, respond to comments from the preregistration review team, and turn your final draft into a formal preregistration. For more information, check out https://www.cos.io/initiatives/prereg-more-information.","tags":["Analysis Plan","Arnold Foundation","Bias","Center for Open Science","Cos","Data","OSF","Preregistration","Preregistration Challenge","Reproducibility","Research","Research Planning"],"title":"The Preregistration Challenge: A How To Guide","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"f10d2bfca8576f0c2d0f4fe5291f843d","permalink":"https://forrt.org/curated_resources/the-preregistration-prescriptiveness-tra/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/the-preregistration-prescriptiveness-tra/","section":"curated_resources","summary":"I discuss Van Drimmelen’s (2023) Metascience2023 presentation on researchers’ decision making during the research process. In particular, I consider his evidence that researchers’ discretion over research decisions is unavoidable when they follow research plans that are either overdetermined (i.e., too prescriptive) or underdetermined (i.e., too vague). I argue that this evidence points to a prescriptiveness trade-off when writing preregistered plans: All other things being equal, plans that are more prescriptive are more likely to result in deviations that turn their confirmatory tests into exploratory tests, and plans that are less prescriptive are more likely to result in confirmatory tests that are susceptible to questionable research practices. I also consider Van Drimmelen’s idea that researchers may make unconscious, implicit decisions during the research process. I relate these implicit decisions to Rumsfeld’s (2002) concept of unknown unknowns: “the things we don’t know we don’t know”! I argue that scientists can report their known knowns (what they know they did and found), and they can be transparent and speculative about their known unknowns (what they know they didn’t do and may find), but that they can’t say much about their unknown unknowns (including their unconscious, implicit decisions) because, by definition, they don’t know what they are! Nonetheless, I think that it’s important to acknowledge unknown unknowns in science because doing so helps to contextualise research efforts as being highly tentative and fallible.","tags":["Metaresearch","Metascience","Open Science","Preregistration","Questionable Metascience Practices","Researcher Degrees of Freedom","Researcher Discretion","Unknown Unknowns"],"title":"The Preregistration Prescriptiveness Trade-Off and Unknown Unknowns in Science: Comments on Van Drimmelen (2023)","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1689122843,"objectID":"465708142747cb89db8babb1824e4213","permalink":"https://forrt.org/curated_resources/the-preregistration-revolution/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/the-preregistration-revolution/","section":"curated_resources","summary":"Progress in science relies in part on generating hypotheses with existing observations and testing hypotheses with new observations. This distinction between postdiction and prediction is appreciated conceptually but is not respected in practice. Mistaking generation of postdictions with testing of predictions reduces the credibility of research findings. However, ordinary biases in human reasoning, such as hindsight bias, make it hard to avoid this mistake. An effective solution is to define the research questions and analysis plan before observing the research outcomes—a process called preregistration. Preregistration distinguishes analyses and outcomes that result from predictions from those that result from postdictions. A variety of practical strategies are available to make the best possible use of preregistration in circumstances that fall short of the ideal application, such as when the data are preexisting. Services are now available for preregistration across all disciplines, facilitating a rapid increase in the practice. Widespread adoption of preregistration will increase distinctiveness between hypothesis generation and hypothesis testing and will improve the credibility of research findings.","tags":[""],"title":"The preregistration revolution","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"f36dfbd5088354557d49d168af77cf40","permalink":"https://forrt.org/curated_resources/the-preregistration-revolution-needs-to/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/the-preregistration-revolution-needs-to/","section":"curated_resources","summary":"Nosek et al. (1) recently joined others in advocating for “widespread adoption of preregistration” as a tool for advancing science. The language they use in making this important argument, however, creates unnecessary confusion: Like many others discussing these issues, they seem to conflate the goal of theory falsification with the goal of constraining type I error. This masks a crucial distinction between two types of preregistration: preregistering a theoretical, a priori, directional prediction (which serves to clarify how a hypothesis is constructed) and preregistering an analysis plan (which serves to clarify how evidence is produced).\n\nIndeed, philosophers of science have identified elements of both how a hypothesis is constructed and how evidence is produced that are important for scientifically valid inference (2–4). We can distill these to two key, separable questions:\n\ni)\t\nHave these data influenced my theoretical prediction? This question is relevant when researchers want to test existing theory: Rationally speaking, we should only adjust our confidence in a theory in response to evidence that was not itself used to construct the theoretical prediction in question (3). Preregistering theoretical predictions can help researchers distinguish clearly between using evidence to inform versus test theory (3, 5, 6).\n\nii)\t\nHave these data influenced my choice of statistical test (and/or other dataset-construction/analysis decisions)? This question is relevant when researchers want to know the type I error rate of statistical tests: Flexibility in researcher decisions can inflate the risk of false positives (7, 8). Preregistration of analysis plans can help researchers distinguish clearly between data-dependent analyses (which can be interesting but may have unknown type I error) and data-independent analyses (for which P values can be interpreted as diagnostic about the likelihood of a result; refs. 1 and 9).\n\nPut differently, preregistration of theoretical predictions helps researchers know how to correctly calibrate their confidence that a study tests (versus informs) a theory, whereas preregistration of analysis plans helps researchers know how to correctly calibrate their confidence that a specific finding is unlikely to be due to chance.\n\nConflating theoretical predictions and analyses is problematic for multiple reasons. First, it implies, erroneously, that preanalysis plans can only help control type I error when research is in a prediction-making/theory-testing phase (e.g., theory Z predicts a gender difference in trait X; ref. 1). In fact, preanalysis plans can also be useful in the question-asking/discovery/theory-building phase (e.g., is there a gender difference in trait X?). Second, it may lead people to preregister the wrong things (e.g., a researcher attempting to control type I error records careful predictions but omits or only loosely specifies a preanalysis plan). Third, it increases misunderstandings and backlash against preregistration as scientists discuss these issues in everyday life (e.g., students erroneously infer that their results are more robust if they correctly guess them ahead of time; skeptics understandably argue that recording one’s prediction ahead of time has no effect on type I error).\n\nIf we want clear communication, productive debates, and effective strategies for advancing science, we must first pull apart our tangled terminology. Preregistering theoretical predictions enables theory falsifiability. Preregistering analysis plans enables type I error control.","tags":["Preregistration","Predictions","Analyses"],"title":"The preregistration revolution needs to distinguish between predictions and analyses","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1689122843,"objectID":"eb6a13e2ba94ea9c9115bb6a481ef818","permalink":"https://forrt.org/curated_resources/the-prevalence-of-statistical-reporting/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/the-prevalence-of-statistical-reporting/","section":"curated_resources","summary":"This study documents reporting errors in a sample of over 250,000 p-values reported in eight major psychology journals from 1985 until 2013, using the new R package “statcheck.” statcheck retrieved null-hypothesis significance testing (NHST) results from over half of the articles from this period. In line with earlier research, we found that half of all published psychology papers that use NHST contained at least one p-value that was inconsistent with its test statistic and degrees of freedom. One in eight papers contained a grossly inconsistent p-value that may have affected the statistical conclusion. In contrast to earlier findings, we found that the average prevalence of inconsistent p-values has been stable over the years or has declined. The prevalence of gross inconsistencies was higher in p-values reported as significant than in p-values reported as nonsignificant. This could indicate a systematic bias in favor of significant results. Possible solutions for the high prevalence of reporting inconsistencies could be to encourage sharing data, to let co-authors check results in a so-called “co-pilot model,” and to use statcheck to flag possible inconsistencies in one’s own manuscript or during the review process.","tags":[""],"title":"The prevalence of statistical reporting errors in psychology (1985–2013)","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"969af9019780f46d1e4629dbe3694334","permalink":"https://forrt.org/curated_resources/the-problem-of-new-evidence-p-hacking-an/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/the-problem-of-new-evidence-p-hacking-an/","section":"curated_resources","summary":"We provide a novel articulation of the epistemic peril of p-hacking using three resources from philosophy: predictivism, Bayesian confirmation theory, and model selection theory. We defend a nuanced position on p-hacking: p-hacking is sometimes, but not always, epistemically pernicious. Our argument requires a novel understanding of Bayesianism, since a standard criticism of Bayesian confirmation theory is that it cannot represent the influence of biased methods. We then turn to pre-analysis plans, a methodological device used to mitigate p-hacking. Some say that pre-analysis plans are epistemically meritorious while others deny this, and in practice pre-analysis plans are often violated. We resolve this debate with a modest defence of pre-analysis plans. Further, we argue that pre-analysis plans can be epistemically relevant even if the plan is not strictly followed—and suggest that allowing for flexible pre-analysis plans may be the best available policy option.","tags":["Bayesian Confirmation Theory","Pre-Analysis Plans","Replication Crisis","Predictivism","P-hacking","Philosophy"],"title":"The Problem of New Evidence: P-Hacking and Pre-Analysis Plans","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"21163aca4381a5bf735743c0b98ffe41","permalink":"https://forrt.org/curated_resources/the-psychology-of-experimental-psycholog/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/the-psychology-of-experimental-psycholog/","section":"curated_resources","summary":"Like many other areas of science, experimental psychology is affected by a “replication crisis” that is causing concern in many fields of research. Approaches t...","tags":["Reproducibility"],"title":"The psychology of experimental psychologists: Overcoming cognitive constraints to improve research: The 47th Sir Frederic Bartlett Lecture:","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1689122843,"objectID":"7d0ea1f7b12e117251d68e20b5fa010c","permalink":"https://forrt.org/curated_resources/the-relation-between-statistical-power-a/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/the-relation-between-statistical-power-a/","section":"curated_resources","summary":"Statistically underpowered studies can result in experimental failure even when all other experimental considerations have been addressed impeccably. In fMRI the combination of a large number of dependent variables, a relatively small number of observations (subjects), and a need to correct for multiple comparisons can decrease statistical power dramatically. This problem has been clearly addressed yet remains controversial—especially in regards to the expected effect sizes in fMRI, and especially for between-subjects effects such as group comparisons and brain-behavior correlations. We aimed to clarify the power problem by considering and contrasting two simulated scenarios of such possible brain-behavior correlations: weak diffuse effects and strong localized effects. Sampling from these scenarios shows that, particularly in the weak diffuse scenario, common sample sizes (n = 20–30) display extremely low statistical power, poorly represent the actual effects in the full sample, and show large variation on subsequent replications. Empirical data from the Human Connectome Project resembles the weak diffuse scenario much more than the localized strong scenario, which underscores the extent of the power problem for many studies. Possible solutions to the power problem include increasing the sample size, using less stringent thresholds, or focusing on a region-of-interest. However, these approaches are not always feasible and some have major drawbacks. The most prominent solutions that may help address the power problem include model-based (multivariate) prediction methods and meta-analyses with related synthesis-oriented approaches.","tags":["Reproducibility Crisis and Credibility Revolution","Open Science"],"title":"The relation between statistical power and inference in fMRI.","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1689122843,"objectID":"6b4d0c0e372edecd7e056572b3b4aab6","permalink":"https://forrt.org/curated_resources/the-replication-crisis-in-psychology/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/the-replication-crisis-in-psychology/","section":"curated_resources","summary":"In science, replication is the process of repeating research to determine the extent to which findings generalize across time and across situations. Recently, the science of psychology has come under criticism because a number of research findings do not replicate. In this module we discuss reasons for non-replication, the impact this phenomenon has on the field, and suggest solutions to the problem.","tags":["Reproducibility Crisis and Credibility Revolution","Open Science"],"title":"The Replication Crisis in Psychology","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"c034d300d263c5e6fd25a94918c81319","permalink":"https://forrt.org/curated_resources/the-replication-crisis-is-less-of-a-cris/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/the-replication-crisis-is-less-of-a-cris/","section":"curated_resources","summary":"Popper’s (1983, 2002) philosophy of science has enjoyed something of a renaissance in the wake of the replication crisis, offering a philosophical basis for the ensuing science reform movement. However, adherence to Popper’s approach may also be at least partly responsible for the sense of “crisis” that has developed following multiple unexpected replication failures. In this article, I contrast Popper’s approach with Lakatos’ (1978) approach and a related approach called naïve methodological falsificationism (NMF; Lakatos, 1978). The Popperian approach is powerful because it is based on logical refutation, but its theories are noncausal and, therefore, lacking in scientific value. In contrast, the Lakatosian approach tests causal theories, but it concedes that these theories are not logically refutable. Finally, the NMF approach subjects Lakatosian causal theories to Popperian logical refutations. However, its approach of temporarily accepting a ceteris paribus clause during theory testing may be viewed as scientifically inappropriate, epistemically inconsistent, and “completely redundant” (Lakatos, 1978, p. 40). I conclude that a replication “crisis” makes the most sense in the context of the Popperian and NMF approaches because it is only in these two approaches that replication failures represent logical refutations of theories. In contrast, replication failures are less problematic in the Lakatosian approach because they do not logically refute theories. Indeed, in the Lakatosian approach, replication failures can be legitimately ignored or used to motivate theory development.","tags":["Lakatos","Popper","Metaresearch","Metascience","Philosophy of Science","Replication Crisis","Theory Testing","Theory"],"title":"The replication crisis is less of a “crisis” in the Lakatosian approach than it is in the Popperian and naïve methodological falsificationism approaches","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1689122843,"objectID":"dafb3f7c16fbdcf5ff7a33f3e20ceecd","permalink":"https://forrt.org/curated_resources/the-replication-recipe-what-makes-for-a/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/the-replication-recipe-what-makes-for-a/","section":"curated_resources","summary":"Psychological scientists have recently started to reconsider the importance of close replications in building a cumulative knowledge base; however, there is no consensus about what constitutes a convincing close replication study. To facilitate convincing close replication attempts we have developed a Replication Recipe, outlining standard criteria for a convincing close replication. Our Replication Recipe can be used by researchers, teachers, and students to conduct meaningful replication studies and integrate replications into their scholarly habits.","tags":["Reproducibility Crisis and Credibility Revolution","Open Science"],"title":"The Replication Recipe: What makes for a convincing replication? ","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1689122843,"objectID":"a20368dcf3109459e5a3ccc14f0223a6","permalink":"https://forrt.org/curated_resources/the-reputational-consequences-of-failed/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/the-reputational-consequences-of-failed/","section":"curated_resources","summary":"Scientists are dedicating more attention to replication efforts. While the scientific utility of replications is unquestionable, the impact of failed replication efforts and the discussions surrounding them deserve more attention. Specifically, the debates about failed replications on social media have led to worry, in some scientists, regarding reputation. In order to gain data-informed insights into these issues, we collected data from 281 published scientists. We assessed whether scientists overestimate the negative reputational effects of a failed replication in a scenario-based study. Second, we assessed the reputational consequences of admitting wrongness (versus not) as an original scientist of an effect that has failed to replicate. Our data suggests that scientists overestimate the negative reputational impact of a hypothetical failed replication effort. We also show that admitting wrongness about a non-replicated finding is less harmful to one’s reputation than not admitting. Finally, we discovered a hint of evidence that feelings about the replication movement can be affected by whether replication efforts are aimed one’s own work versus the work of another. Given these findings, we then present potential ways forward in these discussions.","tags":["Reproducibility Crisis and Credibility Revolution","Open Science"],"title":"The Reputational Consequences of Failed Replications and Wrongness Admission among Scientists","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"d268e461cc89b924eaaee2ccf5dc6627","permalink":"https://forrt.org/curated_resources/the-role-of-libraries-in-the-age-of-comp/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/the-role-of-libraries-in-the-age-of-comp/","section":"curated_resources","summary":"A lighting talk at csv,conf,4 about how libraries and librarians are helping researchers with reproducibility.","tags":["Librarians","Organizational Change","Reproducibility","Researchers"],"title":"The Role of Libraries in the Age of Computational Reproducibility","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"76bd9e7df50aa312e788ba1ad57d8665","permalink":"https://forrt.org/curated_resources/the-role-of-replication-research-in-adva/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/the-role-of-replication-research-in-adva/","section":"curated_resources","summary":"The analysis of longitudinal observational data can take many forms and requires many decisions, with research findings and conclusions often found to differ across independent longitudinal studies addressing the same question. Differences in measurements, sample composition (e.g., age, cohort, country/culture), and statistical models (e.g., change/time function, covariate set, centering, treatment of incomplete data) can affect the replicability of results. The central aim of the Integrative Analysis of Longitudinal Studies of Aging (IALSA) research network (NIH/NIA P01AG043362) is to optimize opportunities for replication and cross-validation across heterogeneous sources of longitudinal data by evaluating comparable conceptual and statistical models at the construct-level. We will provide an overview of the methodological challenges associated with comparative longitudinal and international research, including the comparability of alternative models of change, measurement harmonization and construct-level comparison, retest effects, distinguishing and contrasting between-person and within-person effects across studies, and evaluation of alternative models for change over time. These methodological challenges and recommended approaches will be discussed within the context of reproducible and replication research focused on longitudinal studies.","tags":["Aging Research","Longitudinal Studies","Replication"],"title":"The Role Of Replication Research In Advancing Gerontological Science: Trajectories, Transitions, And Typologies","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1689122843,"objectID":"74e922138172f937dd5937d65421338b","permalink":"https://forrt.org/curated_resources/the-rules-of-the-game-called-psychologic/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/the-rules-of-the-game-called-psychologic/","section":"curated_resources","summary":"If science were a game, a dominant rule would probably be to collect results that are statistically significant. Several reviews of the psychological literature have shown that around 96% of papers involving the use of null hypothesis significance testing report significant outcomes for their main results but that the typical studies are insufficiently powerful for such a track record. We explain this paradox by showing that the use of several small underpowered samples often represents a more efficient research strategy (in terms of finding p \u003c .05) than does the use of one larger (more powerful) sample. Publication bias and the most efficient strategy lead to inflated effects and high rates of false positives, especially when researchers also resorted to questionable research practices, such as adding participants after intermediate testing. We provide simulations that highlight the severity of such biases in meta-analyses. We consider 13 meta-analyses covering 281 primary studies in various fields of psychology and find indications of biases and/or an excess of significant results in seven. These results highlight the need for sufficiently powerful replications and changes in journal policies.","tags":["Reproducibility Crisis and Credibility Revolution","Open Science"],"title":"The rules of the game called psychological science","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"d6f8bd127aa9eaad854122fc48c994e0","permalink":"https://forrt.org/curated_resources/the-secret-to-writing-a-great-nasa-propo/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/the-secret-to-writing-a-great-nasa-propo/","section":"curated_resources","summary":"Use a real NASA proposal as a roadmap and follow these tips for clearly presenting your research ideas. I’m a 100% soft money-funded research scientist primarily funded by NASA research grants. I teamed with the Jet Propulsion Laboratory (JPL) to lead a $190M NASA proposal. JPL’s mission formulation group provided a lot of help and guidance to our team. What I learned applies to most proposals, whether they are for $100K or $100M.","tags":["NASA","Grant Writing"],"title":"The secret to writing a great NASA proposal","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1689122843,"objectID":"cdb7213c2a60324f87ae195ef6547b18","permalink":"https://forrt.org/curated_resources/the-state-of-social-and-personality-scie/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/the-state-of-social-and-personality-scie/","section":"curated_resources","summary":"The scientific quality of social and personality psychology has been debated at great length in recent years. Despite research on the prevalence of Questionable Research Practices (QRPs) and the replicability of particular findings, the impact of the current discussion on research practices is unknown. The current studies examine whether and how practices have changed, if at all, over the last 10 years. In Study 1, we surveyed 1,166 social and personality psychologists about how the current debate has affected their perceptions of their own and the field’s research practices. In Study 2, we coded the research practices and critical test statistics from social and personality psychology articles published in 2003–2004 and 2013–2014. Together, these studies suggest that (a) perceptions of the current state of the field are more pessimistic than optimistic; (b) the discussion has increased researchers’ intentions to avoid QRPs and adopt proposed best practices, (c) the estimated replicability of research published in 2003–2004 may not be as bad as many feared, and (d) research published in 2013–2014 shows some improvement over research published in 2003–2004, a result that suggests the field is evolving in a positive direction.","tags":["Reproducibility Crisis and Credibility Revolution","Open Science"],"title":"The State of Social and Personality Science: Rotten to the Core, Not So Bad, Getting Better, or Getting Worse?","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1689122843,"objectID":"3f2ece0e5e902568354805611f2e7c08","permalink":"https://forrt.org/curated_resources/the-statistical-power-of-abnormal-social/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/the-statistical-power-of-abnormal-social/","section":"curated_resources","summary":"An article about statistical power of abnormal and social psychology","tags":[""],"title":"The statistical power of abnormal-social psychological research: A review. ","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"5522a48b3b73a0eac6ba731708d54900","permalink":"https://forrt.org/curated_resources/the-stem-education-hub-helps-your-resear/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/the-stem-education-hub-helps-your-resear/","section":"curated_resources","summary":"OSF-specific screenshots and walk-throughs are very helpful, preregistration also discusses qualitative preregistration","tags":["Research"],"title":"The STEM Education Hub Helps Your Research Workflow with Open Tools","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1689122843,"objectID":"8cd5ff58be3a83866cec1a6bf508c70f","permalink":"https://forrt.org/curated_resources/the-superego-the-ego-and-the-id-in-stati/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/the-superego-the-ego-and-the-id-in-stati/","section":"curated_resources","summary":"Statistical reasoning is an art and so demands both mathematical knowledge and informed judgment. When it is mechanized, as with the institutionalized hybrid logic, it becomes ritual, not reasoning. Many experts have argued that it is not going to be easy to get researchers in psychology and other sociobiomedical sciences to drop this comforting crutch unless one offers an easy-to-use substitute. This chapter argues that this should be avoided — the substitution of one mechanistic dogma for another. At the very least, this chapter can serve as a tool in arguments with people who think they have to defend a ritualistic dogma instead of good statistical reasoning. Making and winning such arguments is indispensable to good science.","tags":[""],"title":"The Superego, the Ego, and the Id in Statistical Reasoning","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1689122843,"objectID":"bc1e9f2476890be5fa0df17325d61ba8","permalink":"https://forrt.org/curated_resources/the-t-distritbution-and-its-normal-appro/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/the-t-distritbution-and-its-normal-appro/","section":"curated_resources","summary":"I just published a new interactive visualization in my series of basic statistical concepts and techniques. This time I am trying to show how the t-distribution and the normal distribution differs, and how they become very similar for larger sample sizes","tags":["Blog","Interaction","Simulation","Tutorial"],"title":"The t-distritbution and its normal approximation","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1689122843,"objectID":"c94c2678671822fbab33dd8b994bdadd","permalink":"https://forrt.org/curated_resources/the-test-of-insufficient-variance-tiva-a/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/the-test-of-insufficient-variance-tiva-a/","section":"curated_resources","summary":"It has been known for decades that published results tend to be biased (Sterling, 1959). For most of the past decades this inconvenient truth has been ignored. In the past years, there have been many suggestions and initiatives to increase the replicability of reported scientific findings (Asendorpf et al., 2013). One approach is to examine published research results for evidence of questionable research practices (see Schimmack, 2014, for a discussion of existing tests). This blog post introduces a new test of bias in reported research findings, namely the Test of Insufficient Variance (TIVA).","tags":["Blog"],"title":"The Test of Insufficient Variance (TIVA): A New Tool for the Detection of Questionable Research Practices","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"fbab6cc55a797af8f6dc95e1cc9f1a97","permalink":"https://forrt.org/curated_resources/the-tone-debate-knowledge-self-and-socia/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/the-tone-debate-knowledge-self-and-socia/","section":"curated_resources","summary":"In the replication crisis in psychology, a “tone debate” has developed. It concerns the question of how to conduct scientific debate effectively and ethically. How should scientists give critique without unnecessarily damaging relations? The increasing use of Facebook and Twitter by researchers has made this issue especially pressing, as these social technologies have greatly expanded the possibilities for conversation between academics, but there is little formal control over the debate. In this article, we show that psychologists have tried to solve this issue with various codes of conduct, with an appeal to virtues such as humility, and with practices of self-transformation. We also show that the polemical style of debate, popular in many scientific communities, is itself being questioned by psychologists. Following Shapin and Schaffer’s analysis of the ethics of Robert Boyle’s experimental philosophy in the 17th century, we trace the connections between knowledge, social order, and subjectivity as they are debated and revised by present-day psychologists.","tags":["Tone Debate","Replication Crisis","Psychology","Social Media","Code of Conduct","Debate"],"title":"The Tone Debate: Knowledge, Self, and Social Order","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"c06538ce3b2a6311c3f7f08c576cc9e8","permalink":"https://forrt.org/curated_resources/the-transparency-of-quantitative-empiric/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/the-transparency-of-quantitative-empiric/","section":"curated_resources","summary":"Background: Scientists are increasingly concerned with making their work easy to verify and build upon. Associated practices include sharing data, materials, and analytic scripts, and preregistering protocols. This shift towards increased transparency and rigor has been referred to as a “credibility revolution.” The credibility of empirical legal research has been questioned in the past due to its distinctive peer review system and because the legal background of its researchers means that many often are not trained in study design or statistics. Still, there has been no systematic study of transparency and credibility-related characteristics of published empirical legal research.\nMethods: To fill this gap and provide an estimate of current practices that can be tracked as the field evolves, we assessed 300 empirical articles from highly ranked law journals including both faculty-edited journals and student-edited journals.\nResults: We found high levels of article accessibility, especially among student-edited journals. Few articles stated that a study’s data are available. Preregistration and availability of analytic scripts were very uncommon.\nConclusion: We suggest that empirical legal researchers and the journals that publish their work cultivate norms and practices to encourage research credibility. Our estimates may be revisited to track the field’s progress in the coming years.","tags":["Metaresearch","Open Science","Transparency","Credibility","Empirical Legal Research"],"title":"The transparency of quantitative empirical legal research published in highly ranked law journals (2018–2020): an observational study","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1721390377,"objectID":"edcf8a8296c7e4653dd88143aee2aa13","permalink":"https://forrt.org/glossary/english/the_troubling_trio/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/english/the_troubling_trio/","section":"glossary","summary":"","tags":null,"title":"The Troubling Trio","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1635961284,"objectID":"602ded714082d60b985786e405a1bbea","permalink":"https://forrt.org/glossary/vbeta/the-troubling-trio/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/vbeta/the-troubling-trio/","section":"glossary","summary":"","tags":null,"title":"The Troubling Trio","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1720865851,"objectID":"2c44211befac79c894a36f50f0c28f33","permalink":"https://forrt.org/glossary/german/the_troubling_trio/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/german/the_troubling_trio/","section":"glossary","summary":"","tags":null,"title":"The Troubling Trio (Die dreisten Drei)","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"8bbe0f52d5f7d05bb3324c3679523fcc","permalink":"https://forrt.org/curated_resources/the-trustworthiness-of-the-cumulative-kn/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/the-trustworthiness-of-the-cumulative-kn/","section":"curated_resources","summary":"The goal of industrial/organizational (IO) psychology, is to build and organize trustworthy knowledge about people-related phenomena in the workplace. Unfortunately, as with other scientific disciplines, our discipline may be experiencing a “crisis of confidence” stemming from the lack of reproducibility and replicability of many of our field's research findings, which would suggest that much of our research may be untrustworthy. If a scientific discipline's research is deemed untrustworthy, it can have dire consequences, including the withdraw of funding for future research. In this focal article, we review the current state of reproducibility and replicability in IO psychology and related fields. As part of this review, we discuss factors that make it less likely that research findings will be trustworthy, including the prevalence of scientific misconduct, questionable research practices (QRPs), and errors. We then identify some root causes of these issues and provide several potential remedies. In particular, we highlight the need for improved research methods and statistics training as well as a re-alignment of the incentive structure in academia. To accomplish this, we advocate for changes in the reward structure, improvements to the peer review process, and the implementation of open science practices. Overall, addressing the current “crisis of confidence” in IO psychology requires individual researchers, academic institutions, and publishers to embrace system-wide change.","tags":["Reproducibility","Replicability","Scientific Misconduct","Questionable Research Practices","Trustworthiness of our Scientific Knowledge","Open Science Practices"],"title":"The trustworthiness of the cumulative knowledge in industrial/organizational psychology: The current state of affairs and a path forward","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"40f58df9d4a2bfa0b3106eeb242a68a0","permalink":"https://forrt.org/curated_resources/the-unix-shell/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/the-unix-shell/","section":"curated_resources","summary":"Software Carpentry lesson on how to use the shell to navigate the filesystem and write simple loops and scripts. The Unix shell has been around longer than most of its users have been alive. It has survived so long because it's a power tool that allows people to do complex things with just a few keystrokes. More importantly, it helps them combine existing programs in new ways and automate repetitive tasks so they aren't typing the same things over and over again. Use of the shell is fundamental to using a wide range of other powerful tools and computing resources (including â€œhigh-performance computingâ€ supercomputers). These lessons will start you on a path towards using these resources effectively.","tags":["Analysis","Data","Education","Open Scholarship Tools and Technologies","Reproducibility","Research Data Management Tools","Researchers","Shell"],"title":"The Unix Shell","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1689122843,"objectID":"dc2a1d2eb296ea152eb0d234dc0c1f0f","permalink":"https://forrt.org/curated_resources/the-value-of-direct-replication/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/the-value-of-direct-replication/","section":"curated_resources","summary":"Reproducibility is the cornerstone of science. If an effect is reliable, any competent researcher should be able to obtain it when using the same procedures with adequate statistical power. Two of the articles in this special section question the value of direct replication by other laboratories. In this commentary, I discuss the problematic implications of some of their assumptions and argue that direct replication by multiple laboratories is the only way to verify the reliability of an effect.","tags":["Reproducibility Crisis and Credibility Revolution","Open Science"],"title":"The value of direct replication.","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"4a8b5b7c3137b36a4f8fb3822f49feb0","permalink":"https://forrt.org/curated_resources/the-value-of-preregistration-for-psychol/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/the-value-of-preregistration-for-psychol/","section":"curated_resources","summary":"For over two centuries researchers have been criticized for using research practices that makes it easier to present data in line with what they wish to be true. With the rise of the internet it has become easier to preregister the theoretical and empirical basis for predictions, the experimental design, the materials, and the analysis code. Whether the practice of preregistration is valuable depends on your philosophy of science. Here, I provide a conceptual analysis of the value of preregistration for psychological science from an error statistical philosophy (Mayo, 2018). Preregistration has the goal to allow others to transparently evaluate the capacity of a test to falsify a prediction, or the severity of a test. Researchers who aim to test predictions with severity should find value in the practice of preregistration. I differentiate the goal of preregistration from positive externalities, discuss how preregistration itself does not make a study better or worse compared to a non-preregistered study, and highlight the importance of evaluating the usefulness of a tool such as preregistration based on an explicit consideration of your philosophy of science.","tags":["Preregistration","Registered Reports","Severity","Hypothesis Testing","Metascience"],"title":"The value of preregistration for psychological science: A conceptual analysis","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"ed079e1d259c1d1ec12bb33f43cb65a2","permalink":"https://forrt.org/curated_resources/the-weak-spots-in-contemporary-science-a/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/the-weak-spots-in-contemporary-science-a/","section":"curated_resources","summary":"In this review, the author discusses several of the weak spots in contemporary science, including scientific misconduct, the problems of post hoc hypothesizing (HARKing), outcome switching, theoretical bloopers in formulating research questions and hypotheses, selective reading of the literature, selective citing of previous results, improper blinding and other design failures, p-hacking or researchers’ tendency to analyze data in many different ways to find positive (typically significant) results, errors and biases in the reporting of results, and publication bias. The author presents some empirical results highlighting problems that lower the trustworthiness of reported results in scientific literatures, including that of animal welfare studies. Some of the underlying causes of these biases are discussed based on the notion that researchers are only human and hence are not immune to confirmation bias, hindsight bias, and minor ethical transgressions. The author discusses solutions in the form of enhanced transparency, sharing of data and materials, (post-publication) peer review, pre-registration, registered reports, improved training, reporting guidelines, replication, dealing with publication bias, alternative inferential techniques, power, and other statistical tools.","tags":["Meta-research","Questionable Research Practices","Replicability","Reproducibility","Validity"],"title":"The Weak Spots in Contemporary Science (and How to Fix Them)","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1689122843,"objectID":"3c261cdd2d167cb34c8ba9a8e40a5fe5","permalink":"https://forrt.org/curated_resources/the-what-why-and-how-of-born-open-data/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/the-what-why-and-how-of-born-open-data/","section":"curated_resources","summary":"Although many researchers agree that scientific data should be open to scrutiny to ferret out poor analyses and outright fraud, most raw data sets are not available on demand. There are many reasons researchers do not open their data, and one is technical. It is often time consuming to prepare and archive data. In response, my laboratory has automated the process such that our data are archived the night they are created without any human approval or action. All data are versioned, logged, time stamped, and uploaded including aborted runs and data from pilot subjects. The archive is GitHub, github.com, the world’s largest collection of open-source materials. Data archived in this manner are called born open. In this paper, I discuss the benefits of born-open data and provide a brief technical overview of the process. I also address some of the common concerns about opening data before publication.","tags":[""],"title":"The what, why, and how of born-open data","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"d131eaa5d60337f0393e1dbb12bb7e3b","permalink":"https://forrt.org/curated_resources/the-what-why-and-how-of-preregistration/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/the-what-why-and-how-of-preregistration/","section":"curated_resources","summary":"More researchers are preregistering their studies as a way to combat publication bias and improve the credibility of research findings. Preregistration is at its core designed to distinguish between confirmatory and exploratory results. Both are important to the progress of science, but when they are conflated, problems arise. In this webinar, we discuss the What, Why, and How of preregistration and what it means for the future of science. Visit cos.io/prereg for additional resources.","tags":["Analysis","Data","Education","Materials","Open Scholarship Tools and Technologies","OSF","Preregistation","Publishing","Reproducibility","Research Data Management Tools","Researchers"],"title":"The What, Why, and How of Preregistration","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1689122843,"objectID":"426ed080bde7565632d5e06e15aa3d0a","permalink":"https://forrt.org/curated_resources/theoretical-risks-and-tabular-asterisks/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/theoretical-risks-and-tabular-asterisks/","section":"curated_resources","summary":"Theories in \"soft\" areas of psychology (e.g., clinical, counseling, social, personality, school, and community) lack the cumulative character of scientific knowledge because they tend neither to be refuted nor corroborated, but instead merely fade away as people lose interest. Even though intrinsic subject matter difficulties (20 are listed) contribute to this, the excessive reliance on significance testing is partly responsible (Ronald A. Fisher). Karl Popper's approach, with modifications, would be prophylactic. Since the null hypothesis is quasi-always false, tables summarizing research in terms of patterns of \"significant differences\" are little more than complex, causally uninterpretable outcomes of statistical power functions. Multiple paths to estimating numerical point values (\"consistency tests\") are better, even if approximate with rough tolerances; and lacking this, ranges, orderings, 2nd-order differences, curve peaks and valleys, and function forms should be used. Such methods are usual in developed sciences that seldom report statistical significance. Consistency tests of a conjectural taxometric model yielded 94% success with no false negatives. ","tags":["Reproducibility Crisis and Credibility Revolution","Open Science"],"title":"Theoretical risks and tabular asterisks: Sir Karl, Sir Ronald, and the slow progress of soft psychology.","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1721390377,"objectID":"2367c273474520cfd61dd99de3eeff9d","permalink":"https://forrt.org/glossary/english/theory/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/english/theory/","section":"glossary","summary":"","tags":null,"title":"Theory","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1635961284,"objectID":"95e3e23343d4b9e92897344bba8e2c62","permalink":"https://forrt.org/glossary/vbeta/theory/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/vbeta/theory/","section":"glossary","summary":"","tags":null,"title":"Theory ","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1720865851,"objectID":"74b89b543e0376ad27cee7c71498dd5e","permalink":"https://forrt.org/glossary/german/theory/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/german/theory/","section":"glossary","summary":"","tags":null,"title":"Theory (Theorie)","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1721390377,"objectID":"7d017f35cb824dece84d65af3262121a","permalink":"https://forrt.org/glossary/english/theory_building/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/english/theory_building/","section":"glossary","summary":"","tags":null,"title":"Theory building","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1635961284,"objectID":"af245618c71a1e52306bce79ca8a5b20","permalink":"https://forrt.org/glossary/vbeta/theory-building/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/vbeta/theory-building/","section":"glossary","summary":"","tags":null,"title":"Theory building ","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1720865851,"objectID":"b0d74edf81152cd0181c2db95d80be9f","permalink":"https://forrt.org/glossary/german/theory_building/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/german/theory_building/","section":"glossary","summary":"","tags":null,"title":"Theory building (Theoriebildung)","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1689122843,"objectID":"fde86925e34b77aa7549c0427b44f347","permalink":"https://forrt.org/curated_resources/theory-construction-and-model-building-s/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/theory-construction-and-model-building-s/","section":"curated_resources","summary":"Meeting a crucial need for graduate students and newly minted researchers, this innovative text provides hands-on tools for generating ideas and translating them into formal theories. It is illustrated with numerous practical examples drawn from multiple social science disciplines and research settings. The authors offer clear guidance for defining constructs, thinking through relationships and processes that link constructs, and deriving new theoretical models (or building on existing ones) based on those relationships. Step by step, they show readers how to use causal analysis, mathematical modeling, simulations, and grounded and emergent approaches to theory construction. A chapter on writing about theories contains invaluable advice on crafting effective papers and grant applications. ","tags":["Book"],"title":"Theory Construction and Model-Building Skills: A Practical Guide for Social Scientists","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1689122843,"objectID":"668af6059910564c713b4fa07986a770","permalink":"https://forrt.org/curated_resources/theory-testing-in-psychology-and-physics/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/theory-testing-in-psychology-and-physics/","section":"curated_resources","summary":"Because physical theories typically predict numerical values, an improvement in experimental precision reduces the tolerance range and hence increases corroborability. In most psychological research, improved power of a statistical design leads to a prior probability approaching 1/2 of finding a significant difference in the theoretically predicted direction. Hence the corroboration yielded by \"success\" is very weak, and becomes weaker with increased precision. \"Statistical significance\" plays a logical role in psychology precisely the reverse of its role in physics. This problem is worsened by certain unhealthy tendencies prevalent among psychologists, such as a premium placed on experimental \"cuteness\" and a free reliance upon ad hoc explanations to avoid refutation.","tags":["Reproducibility Crisis and Credibility Revolution","Open Science"],"title":"Theory-Testing in Psychology and Physics: A Methodological Paradox","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1689122843,"objectID":"6d06b2c60f848d12cac42f66589b9271","permalink":"https://forrt.org/curated_resources/things-i-have-learned-so-far/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/things-i-have-learned-so-far/","section":"curated_resources","summary":" This is an account of what I have learned (so far) about the application of statistics to psychology and the other sociobiomedical sciences. It includes the principles \"less is more\" (fewer variables, more highly targeted issues, sharp rounding off), \"simple is better\" (graphic representation, unit weighting for linear composites), and \"some things you learn aren't so.\" I have learned to avoid the many misconceptions that surround Fisherian null hypothesis testing. I have also learned the importance of power analysis and the determination of just how big (rather than how statistically significant) are the effects that we study. Finally, I have learned that there is no royal road to statistical induction, that the informed judgment of the investigator is the crucial element in the interpretation of data, and that things take time.","tags":[""],"title":"Things I have learned (so far)","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"efd8052d4d2c24c03c45251c6bf4d32e","permalink":"https://forrt.org/curated_resources/tie-my-hands-loosely-pre-analysis-plans/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/tie-my-hands-loosely-pre-analysis-plans/","section":"curated_resources","summary":"In this short article, I want to provide some of my thoughts on these developments from the perspective of someone who writes PAPs and reads them as a reviewer, as well as from the perspective of a journal editor.","tags":["Preregistration","Political Science"],"title":"Tie my hands loosely: Pre-analysis plans in political science","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1689122843,"objectID":"9f001b12217f623aa25e72c4c54c5d56","permalink":"https://forrt.org/curated_resources/too-true-to-be-bad-when-sets-of-studies/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/too-true-to-be-bad-when-sets-of-studies/","section":"curated_resources","summary":"Psychology journals rarely publish nonsignificant results. At the same time, it is often very unlikely (or “too good to be true”) that a set of studies yields exclusively significant results. Here, we use likelihood ratios to explain when sets of studies that contain a mix of significant and nonsignificant results are likely to be true or “too true to be bad.” As we show, mixed results are not only likely to be observed in lines of research but also, when observed, often provide evidence for the alternative hypothesis, given reasonable levels of statistical power and an adequately controlled low Type 1 error rate. Researchers should feel comfortable submitting such lines of research with an internal meta-analysis for publication. A better understanding of probabilities, accompanied by more realistic expectations of what real sets of studies look like, might be an important step in mitigating publication bias in the scientific literature.","tags":[""],"title":"Too true to be bad: When sets of studies with significant and nonsignificant findings are probably true","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1689122843,"objectID":"2de530306c176e4f4fa66c6dcd40155a","permalink":"https://forrt.org/curated_resources/tools-for-de-identification-of-personal/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/tools-for-de-identification-of-personal/","section":"curated_resources","summary":"This report identifies useful and available tools and techniques for the deidentification of personal information from interoperable electronic health records and health information related data warehouses. Section 1 contains a general introduction and defines some of the terms commonly used when discussing de-identification. Section 2 describes the principles of de-identification, including two basic models of how data warehouse are operated; the distinction between record-level and aggregate data; the distinction between direct and indirect identifiers; a description of the types of secondary uses that data warehouses support (health research, health system planning, public health surveillance, and generation of deidentified data for system testing); an explanation of k-anonymity as a measure of de-identification; a discussion of the special problems inherent in free-text data; a discussion of the special problems posed by genetic information; and guidance to prevent unintended disclosures through lapses in security. Section 3 describes the various approaches to de-identification, including a flow diagram of when to use each approach. Record-level data can be de-identified through data reduction (including removal of direct identifiers, reduction in the detail of the data, and sampling), data modification (random addition of noise to the data, randomization of data values, and data swapping), and data suppression. Each approach is briefly described and examples are given. Pseudonymisation is described; including the distinction between reversible and irreversible pseudonymisation, and the two basic ways in which pseudonymisation is carried out. Aggregate data has its own approaches to de-identification, including restriction-based methods (cell suppression and changing the classification scheme for the data) and heuristics. These are described with examples. Section 5 contains some best practices for de-identification. These include how direct identifiers should be handled in data warehouses; how date variables should be presented in released datasets; how location data such as postal codes should be handled in released datasets; special guidelines for diagnostic imaging data; how pseudonymous IDs should be handled in released datasets to prevent unintended data linkages to other datasets; and elements of contractual agreements on use and disclosure of datasets. Section 5 contains a description of readily available tools for de-identification of both record-level data and aggregate data. Tools described for handling direct identifiers in record-level data include Oracle Data Masking Pack, Camouflage, Informatica Data Privacy, and Data Masker. Tools for handling indirect identifiers in record-level data include PARAT, μ-Argus, and the Cornell Anonymization Toolkit. Additional tools for aggregate data include τ-ARGUS. Additional tools are also discussed for postal code conversion. Third-party evaluation of de-identification (or the lack thereof) is also briefly discussed. Section 6 briefly discusses re-identification risks. The report concludes with two observations: that the tools described can significantly reduce the risk of reidentification but only when sensibly combined with administrative controls such as end-user agreements and good security practices; and that the de-identified data will only be of value to the end-users if the approach to de-identification supports the intended use.","tags":[""],"title":"Tools for De-Identification of Personal Health Information ","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"9071ef900d7e34d0004c91756a5a4875","permalink":"https://forrt.org/curated_resources/tools-for-reproducible-research/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/tools-for-reproducible-research/","section":"curated_resources","summary":"Course summary\nA minimal standard for data analysis and other scientific computations is that they be reproducible: that the code and data are assembled in a way so that another group can re-create all of the results (e.g., the figures in a paper). The importance of such reproducibility is now widely recognized, but it is still not so widely practiced as it should be, in large part because many computational scientists (and particularly statisticians) have not fully adopted the required tools for reproducible research.\n\nIn this course, we will discuss general principles for reproducible research but will focus primarily on the use of relevant tools (particularly make, git, and knitr), with the goal that the students leave the course ready and willing to ensure that all aspects of their computational research (software, data analyses, papers, presentations, posters) are reproducible.","tags":["Documentation","Literate Programming","Reproducibility","Version Control"],"title":"Tools for Reproducible Research","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"eb9047e03355f3ea5110594b4673cd9e","permalink":"https://forrt.org/curated_resources/top-guidelines/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/top-guidelines/","section":"curated_resources","summary":"The Transparency and Openness Promotion guidelines include eight modular standards, each with three levels of increasing stringency. Journals select which of the eight transparency standards they wish to implement and select a level of implementation for each. These features provide flexibility for adoption depending on disciplinary variation, but simultaneously establish community standards.","tags":["Funders","Open Scholarship Policy","Open Standards","Policy","Policy Makers","Publishers","Publishing"],"title":"TOP Guidelines","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"ddcd8a35eaea82bebc83947d8cf27926","permalink":"https://forrt.org/curated_resources/topicos-especiais-em-biotecnologia-plane/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/topicos-especiais-em-biotecnologia-plane/","section":"curated_resources","summary":"A crise de confiança na ciência. Práticas e condutas questionáveis na pesquisa. Delineamento experimental adequado. Definição de variáveis. Armadilhas à validade. Cálculo amostral. Transparência na pesquisa. Pré-registro. Dados e código aberto.","tags":["Reproducibility Crisis and Credibility Revolution","Open Science"],"title":"Tópicos Especiais em Biotecnologia: Planejamento e Otimização de Experimentos","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"0a44144bf9e95b5c7529af9ea7bd1d3b","permalink":"https://forrt.org/curated_resources/topics-in-social-psychology-and-personal/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/topics-in-social-psychology-and-personal/","section":"curated_resources","summary":"This seminar class will focus on the theme of Reproducibility in Social Psychology. We will discuss issues surrounding open science as well as the “replication crisis” in social psychology.","tags":["Reproducibility Crisis and Credibility Revolution","Open Science"],"title":"Topics in Social Psychology and Personality","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"e244f231da994e7e15f7adb70980ed70","permalink":"https://forrt.org/curated_resources/toward-reproducible-computational-resear/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/toward-reproducible-computational-resear/","section":"curated_resources","summary":"Journal policy on research data and code availability is an important part of the ongoing shift toward publishing reproducible computational science. This article extends the literature by studying journal data sharing policies by year (for both 2011 and 2012) for a referent set of 170 journals. We make a further contribution by evaluating code sharing policies, supplemental materials policies, and open access status for these 170 journals for each of 2011 and 2012. We build a predictive model of open data and code policy adoption as a function of impact factor and publisher and find higher impact journals more likely to have open data and code policies and scientific societies more likely to have open data and code policies than commercial publishers. We also find open data policies tend to lead open code policies, and we find no relationship between open data and code policies and either supplemental material policies or open access journal status. Of the journals in this study, 38% had a data policy, 22% had a code policy, and 66% had a supplemental materials policy as of June 2012. This reflects a striking one year increase of 16% in the number of data policies, a 30% increase in code policies, and a 7% increase in the number of supplemental materials policies. We introduce a new dataset to the community that categorizes data and code sharing, supplemental materials, and open access policies in 2011 and 2012 for these 170 journals.","tags":["Bibliometrics","Computational Biology","Data","Data Management","Open Access Publishing","Open Data","Policy","Publishing","Reproducibility","Science Policy","Scientific Publishing"],"title":"Toward Reproducible Computational Research: An Empirical Analysis of Data and Code Policy Adoption by Journals","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"2bc3aec9259cb04f74a9059d379af2f7","permalink":"https://forrt.org/curated_resources/towards-inclusive-funding-practices-for/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/towards-inclusive-funding-practices-for/","section":"curated_resources","summary":"Securing research funding is a challenge faced by most scientists in academic institutions worldwide. Funding success rates for all career stages are low, but the burden falls most heavily on early career researchers (ECRs) - young investigators in training and new principal investigators - who have a shorter track record and are dependent on funding to establish their academic career. The low number of career development awards and the lack of sustained research funding results in the loss of ECR talent in academia. Several steps in the current funding process, from grant conditions to the review process, play significant roles in the distribution of funds. Furthermore, there is an imbalance among certain research disciplines and labs of influential researchers that receive more funding. As a group of ECRs with global representation, we examined funding practices, barriers, facilitators, and alternatives to the current funding systems to diversify risk or award grants on a partly random basis. Based on our discussions, research, and collective opinions, we detail recommendations for funding agencies and grant reviewers to improve ECR funding prospects worldwide and promote a fairer and more inclusive funding landscape for ECRs.","tags":["Funding"],"title":"Towards inclusive funding practices for early career researchers","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"22fcd0e30973714170eea8d0afc54d44","permalink":"https://forrt.org/curated_resources/towards-reproducible-radiomics-research/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/towards-reproducible-radiomics-research/","section":"curated_resources","summary":"Objectives\nTo investigate the model-, code-, and data-sharing practices in the current radiomics research landscape and to introduce a radiomics research database.\n\nMethods\nA total of 1254 articles published between January 1, 2021, and December 31, 2022, in leading radiology journals (European Radiology, European Journal of Radiology, Radiology, Radiology: Artificial Intelligence, Radiology: Cardiothoracic Imaging, Radiology: Imaging Cancer) were retrospectively screened, and 257 original research articles were included in this study. The categorical variables were compared using Fisher’s exact tests or chi-square test and numerical variables using Student’s t test with relation to the year of publication.\n\nResults\nHalf of the articles (128 of 257) shared the model by either including the final model formula or reporting the coefficients of selected radiomics features. A total of 73 (28%) models were validated on an external independent dataset. Only 16 (6%) articles shared the data or used publicly available open datasets. Similarly, only 20 (7%) of the articles shared the code. A total of 7 (3%) articles both shared code and data. All collected data in this study is presented in a radiomics research database (RadBase) and could be accessed at https://github.com/EuSoMII/RadBase.\n\nConclusion\nAccording to the results of this study, the majority of published radiomics models were not technically reproducible since they shared neither model nor code and data. There is still room for improvement in carrying out reproducible and open research in the field of radiomics.\n\nClinical relevance statement\nTo date, the reproducibility of radiomics research and open science practices within the radiomics research community are still very low. Ensuring reproducible radiomics research with model-, code-, and data-sharing practices will facilitate faster clinical translation.\n\nKey Points\n• There is a discrepancy between the number of published radiomics papers and the clinical implementation of these published radiomics models.\n\n• The main obstacle to clinical implementation is the lack of model-, code-, and data-sharing practices.\n\n• In order to translate radiomics research into clinical practice, the radiomics research community should adopt open science practices.","tags":["Multiomics","Radiomics","Artificial intelligence","Reproducibility of results"],"title":"Towards reproducible radiomics research: introduction of a database for radiomics studies","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1689122843,"objectID":"8d8f46a9fbd9728edbb209d855971ed7","permalink":"https://forrt.org/curated_resources/tracking-replicability-as-a-method-of-po/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/tracking-replicability-as-a-method-of-po/","section":"curated_resources","summary":"Recent reports have suggested that many published results are unreliable. To increase the reliability and accuracy of published papers, multiple changes have been proposed, such as changes in statistical methods. We support such reforms. However, we believe that the incentive structure of scientific publishing must change for such reforms to be successful. Under the current system, the quality of individual scientists is judged on the basis of their number of publications and citations, with journals similarly judged via numbers of citations. Neither of these measures takes into account the replicability of the published findings, as false or controversial results are often particularly widely cited. We propose tracking replications as a means of post-publication evaluation, both to help researchers identify reliable findings and to incentivize the publication of reliable results. Tracking replications requires a database linking published studies that replicate one another. As any such database is limited by the number of replication attempts published, we propose establishing an open-access journal dedicated to publishing replication attempts. Data quality of both the database and the affiliated journal would be ensured through a combination of crowd-sourcing and peer review. As reports in the database are aggregated, ultimately it will be possible to calculate replicability scores, which may be used alongside citation counts to evaluate the quality of work published in individual journals. In this paper, we lay out a detailed description of how this system could be implemented, including mechanisms for compiling the information, ensuring data quality, and incentivizing the research community to participate.","tags":["Reproducibility Crisis and Credibility Revolution","Open Science"],"title":"Tracking replicability as a method of post-publication open evaluation","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"c42fad52e0233de4c60e89a05abaf758","permalink":"https://forrt.org/curated_resources/trainer-space-for-the-introduction-to-op/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/trainer-space-for-the-introduction-to-op/","section":"curated_resources","summary":"Central location housing curriculum materials and planning tools for trainers of the COS Introduction to Open and Reproducible Research workshop.","tags":["Education","Librarians","Reproducibility","Research Data Management Tools","Researchers"],"title":"Trainer Space for the Introduction to Open and Reproducible Research Workshop","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1721390377,"objectID":"9c25d32d12cdeb9347efd246d9154b5c","permalink":"https://forrt.org/glossary/english/transparency/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/english/transparency/","section":"glossary","summary":"","tags":null,"title":"Transparency","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1635961284,"objectID":"a06a138859ff9ffac7cc66b59cc35b5f","permalink":"https://forrt.org/glossary/vbeta/transparency/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/vbeta/transparency/","section":"glossary","summary":"","tags":null,"title":"Transparency","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1720865851,"objectID":"002d879a19e654e0856b5c2a32f5cf96","permalink":"https://forrt.org/glossary/german/transparency/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/german/transparency/","section":"glossary","summary":"","tags":null,"title":"Transparency (Transparenz)","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"fd191ca8fc25444fc217a9c1c2bf2fdc","permalink":"https://forrt.org/curated_resources/transparency-and-open-science-symposium/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/transparency-and-open-science-symposium/","section":"curated_resources","summary":"The past decade has seen rapid growth in conversations around and progress towards fostering a more transparent, open, and cumulative science. Best practices are being codified and established across fields relevant to gerontology from cancer science to psychological science. Many of the areas currently under development are of particular relevance to gerontologists such as best practices in balancing open science with participant confidentiality or best practices for preregistering archival, longitudinal data analysis. The present panel showcases one of the particular strengths of the open science movement - the contribution that early career researchers are making to these ongoing conversations on best practices. Early career researchers have the opportunity to blend their expertise with technology, their knowledge of their disciplines, and their vision for the future in shaping these conversations. In this panel, three early career researchers share their insights. Pfund presents an introduction to preregistration and the value of preregistration from the perspective of “growing up” within the open science movement. Seaman discusses efforts in and tools for transparency and reproducibility in neuroimaging of aging research. Ludwig introduces the idea of registered reports as a particularly useful form of publication for researchers who use longitudinal methods and/or those who work with hard-to-access samples. The symposium will include time for the audience to engage the panel in questions and discussion about current efforts in and future directions for transparent, open, and cumulative science efforts in gerontology.","tags":["Aging","Aging Science","Gerontology","Open Science"],"title":"Transparency and Open Science Symposium GSA 2019","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1721390377,"objectID":"c9a2a2b8b6c12b044542a881c027eb7a","permalink":"https://forrt.org/glossary/english/transparency_checklist/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/english/transparency_checklist/","section":"glossary","summary":"","tags":null,"title":"Transparency Checklist","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1635961284,"objectID":"2baeae3c543bcc5c7e6da7dd35a8bf20","permalink":"https://forrt.org/glossary/vbeta/transparency-checklist/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/vbeta/transparency-checklist/","section":"glossary","summary":"","tags":null,"title":"Transparency Checklist","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1720865851,"objectID":"68f3ab2c70a5b857cf2db11d81b019c9","permalink":"https://forrt.org/glossary/german/transparency_checklist/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/german/transparency_checklist/","section":"glossary","summary":"","tags":null,"title":"Transparency Checklist (Transparenz-Checkliste)","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"592d4f15b4249b4ee714d5ae1de34c1d","permalink":"https://forrt.org/curated_resources/transparency-of-chi-research-artifacts-r/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/transparency-of-chi-research-artifacts-r/","section":"curated_resources","summary":"Several fields of science are experiencing a \"replication crisis\" that has negatively impacted their credibility. Assessing the validity of a contribution via replicability of its experimental evidence and reproducibility of its analyses requires access to relevant study materials, data, and code. Failing to share them limits the ability to scrutinize or build-upon the research, ultimately hindering scientific progress.Understanding how the diverse research artifacts in HCI impact sharing can help produce informed recommendations for individual researchers and policy-makers in HCI. Therefore, we surveyed authors of CHI 2018â€“2019 papers, asking if they share their papers' research materials and data, how they share them, and why they do not. The results (N = 460/1356, 34% response rate) show that sharing is uncommon, partly due to misunderstandings about the purpose of sharing and reliable hosting. We conclude with recommendations for fostering open research practices.This paper and all data and materials are freely available at https://osf.io/csy8q","tags":["Data","Reproducibility"],"title":"Transparency of CHI Research Artifacts: Results of a Self-Reported Survey","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"3333b88ba88c38b3c201cb8a1f0374e2","permalink":"https://forrt.org/curated_resources/transparency-replicability-and-discovery/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/transparency-replicability-and-discovery/","section":"curated_resources","summary":"Healthy aging is associated with deficits in performance on episodic memory tasks. Popular verbal theories of the mechanisms underlying this decrement have primarily focused on inferred changes in associative memory. However, performance on any task is the result of interactions between different neurocognitive mechanisms, such as perceptuomotor, memory, and decision-making processes. As a result, age-related differences in performance could arise from multiple processes, which could lead to incomplete or incorrect conclusions about the sources of aging effects. In addition, standard statistical comparisons of group-level summary statistics, such as mean accuracy, may not provide sufficient information to allow detailed mechanistic explanations of age-related change. We argue that these and other drawbacks of relying exclusively on verbal theories can hamper replicability, transparency, and scientific progress in aging research and psychological science more generally, and that computational modeling is a tool that can address many of these limitations. Computational models make mathematically transparent claims about how latent processes give rise to observed behavior and decompose an individual’s performance into model parameters governing hypothesized mechanisms. In this work, we present a short memory task designed for and analyzed with mechanistic model-based approaches. We provide an example of a computational model and fit the model to data from young and older adults with hierarchical Bayesian techniques in order to (a) detect differences in latent cognitive processes between young and older adults (as well as individual participants), (b) quantitatively compare models to assess different processes that could underlie performance, and (c) simulate data to make predictions for future experiments based on model mechanisms. We argue that computational modeling is a powerful tool to examine age differences in latent processes, make theories more transparent, and facilitate discovery in cognitive aging research.","tags":["Transparency","Replicability","Discovery","Cognitive Aging","Computational Modeling"],"title":"Transparency, replicability, and discovery in cognitive aging research: A computational modeling approach","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1689122843,"objectID":"0762a8a6f817a254e8d11f3234cb4f84","permalink":"https://forrt.org/curated_resources/transparent-and-open-social-science-rese/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/transparent-and-open-social-science-rese/","section":"curated_resources","summary":"Demand is growing for evidence-based policy making, but there is also growing recognition in the social science community that limited transparency and openness in research have contributed to widespread problems. With this course, you can explore the causes of limited transparency in social science research, as well as tools to make your own work more open and reproducible.","tags":["Course","Video","Reproducibility Knowledge"],"title":"Transparent and Open Social Science Research course","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1689122843,"objectID":"5e6302e52d92f880be4012541f3509f8","permalink":"https://forrt.org/curated_resources/transparent-science-a-more-credible-repr/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/transparent-science-a-more-credible-repr/","section":"curated_resources","summary":"This is an exciting time to be a psychological scientist. There is a major new movement that seeks to promote the credibility and replicability of psychological research by enhancing its transparency, with scholarly societies promoting the principles (http://www.psychologicalscience.org/publications/open-science) and groups formed specifically to advance that mission (see http://improvingpsych.org/ and https://cos.io for two examples). While relatively low rates of replicability among scientific findings (Begley \u0026 Ellis, 2012; OSC, 2015; Chang \u0026 Li 2015) inspired the existence of these groups, in this chapter we describe how striving to maximize transparency in your research can benefit both science and your career.","tags":["Reproducibility Crisis and Credibility Revolution","Open Science"],"title":"Transparent science: A more credible, reproducible, and publishable way to do science","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"a2f61792ea0629ba99bc03df6c71245c","permalink":"https://forrt.org/curated_resources/transparent-reproducible-and-open-scienc/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/transparent-reproducible-and-open-scienc/","section":"curated_resources","summary":"Background: Reproducible research is a foundational component for scientific advancements, yet little is known regarding the extent of reproducible research within the dermatology literature. Objective: This study aimed to determine the quality and transparency of the literature in dermatology journals by evaluating for the presence of 8 indicators of reproducible and transparent research practices. Methods: By implementing a cross-sectional study design, we conducted an advanced search of publications in dermatology journals from the National Library of Medicine catalog. Our search included articles published between January 1, 2014, and December 31, 2018. After generating a list of eligible dermatology publications, we then searched for full text PDF versions by using Open Access Button, Google Scholar, and PubMed. Publications were analyzed for 8 indicators of reproducibility and transparency—availability of materials, data, analysis scripts, protocol, preregistration, conflict of interest statement, funding statement, and open access—using a pilot-tested Google Form. Results: After exclusion, 127 studies with empirical data were included in our analysis. Certain indicators were more poorly reported than others. We found that most publications (113, 88.9%) did not provide unmodified, raw data used to make computations, 124 (97.6%) failed to make the complete protocol available, and 126 (99.2%) did not include step-by-step analysis scripts. Conclusions: Our sample of studies published in dermatology journals do not appear to include sufficient detail to be accurately and successfully reproduced in their entirety. Solutions to increase the quality, reproducibility, and transparency of dermatology research are warranted. More robust reporting of key methodological details, open data sharing, and stricter standards journals impose on authors regarding disclosure of study materials might help to better the climate of reproducible research in dermatology. [JMIR Dermatol 2019;2(1):e16078]","tags":["Data","Dermatology","Reproducibility"],"title":"Transparent, Reproducible, and Open Science Practices of Published Literature in Dermatology Journals: Cross-Sectional Analysis","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"862da4a36d125c2c18745033fe3808ee","permalink":"https://forrt.org/curated_resources/trial-publication-after-registration-in/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/trial-publication-after-registration-in/","section":"curated_resources","summary":"Background ClinicalTrials.gov is a publicly accessible, Internet-based registry of clinical trials managed by the US National Library of Medicine that has the potential to address selective trial publication. Our objectives were to examine completeness of registration within ClinicalTrials.gov and to determine the extent and correlates of selective publication. Methods and Findings We examined reporting of registration information among a cross-section of trials that had been registered at ClinicalTrials.gov after December 31, 1999 and updated as having been completed by June 8, 2007, excluding phase I trials. We then determined publication status among a random 10% subsample by searching MEDLINE using a systematic protocol, after excluding trials completed after December 31, 2005 to allow at least 2 y for publication following completion. Among the full sample of completed trials (n = 7,515), nearly 100% reported all data elements mandated by ClinicalTrials.gov, such as intervention and sponsorship. Optional data element reporting varied, with 53% reporting trial end date, 66% reporting primary outcome, and 87% reporting trial start date. Among the 10% subsample, less than half (311 of 677, 46%) of trials were published, among which 96 (31%) provided a citation within ClinicalTrials.gov of a publication describing trial results. Trials primarily sponsored by industry (40%, 144 of 357) were less likely to be published when compared with nonindustry/nongovernment sponsored trials (56%, 110 of 198; p\u003c0.001), but there was no significant difference when compared with government sponsored trials (47%, 57 of 122; p = 0.22). Among trials that reported an end date, 75 of 123 (61%) completed prior to 2004, 50 of 96 (52%) completed during 2004, and 62 of 149 (42%) completed during 2005 were published (p = 0.006). Conclusions Reporting of optional data elements varied and publication rates among completed trials registered within ClinicalTrials.gov were low. Without greater attention to reporting of all data elements, the potential for ClinicalTrials.gov to address selective publication of clinical trials will be limited.","tags":["Clinical Trials","Citation Analysis","Lung and Intrathoracic Tumors","United States","Phase I Clinical Investigation","Safety Studies","Drug Administration","Medical Journals"],"title":"Trial Publication after Registration in ClinicalTrials.Gov: A Cross-Sectional Analysis","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"78d0e4aca61dc32d6f9c1c7ad51401fe","permalink":"https://forrt.org/curated_resources/trial-registration-10-years-on/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/trial-registration-10-years-on/","section":"curated_resources","summary":"This month marks the tenth anniversary of the landmark decision by the International Committee of Medical Journal Editors to make journals require “registration of any clinical trials in a public trials registry at or before the time of first patient enrolment as a condition of consideration for publication.”","tags":["Clinical Research","Preregistration"],"title":"Trial registration 10 years on","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1720865851,"objectID":"71551088287e2b2d74facd677a790d12","permalink":"https://forrt.org/glossary/german/trim_and_fill_method/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/german/trim_and_fill_method/","section":"glossary","summary":"","tags":null,"title":"Trim-and-fill method","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1721390377,"objectID":"a5a73132a618249899ed6ea700a14700","permalink":"https://forrt.org/glossary/english/triple_blind_peer_review/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/english/triple_blind_peer_review/","section":"glossary","summary":"","tags":null,"title":"Triple-blind peer review","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1635961284,"objectID":"6a381f4af02789008c279091279d4f3b","permalink":"https://forrt.org/glossary/vbeta/triple-blind-peer-review/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/vbeta/triple-blind-peer-review/","section":"glossary","summary":"","tags":null,"title":"Triple-blind peer review","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1720865851,"objectID":"eb75e6de2217a03536d3669902810b6d","permalink":"https://forrt.org/glossary/german/triple_blind_peer_review/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/german/triple_blind_peer_review/","section":"glossary","summary":"","tags":null,"title":"Triple-blind peer review (Dreifach-blindes Peer Review)","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1721390377,"objectID":"3a9565cc2d62a733d04b002a6db46602","permalink":"https://forrt.org/glossary/english/trust_principles/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/english/trust_principles/","section":"glossary","summary":"","tags":null,"title":"TRUST Principles","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1635961284,"objectID":"193cd1dd09996dc5c413692749b1549a","permalink":"https://forrt.org/glossary/vbeta/trust-principles/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/vbeta/trust-principles/","section":"glossary","summary":"","tags":null,"title":"TRUST Principles","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1720865851,"objectID":"cc8daab277b32aa3a6ef183eeb9e0784","permalink":"https://forrt.org/glossary/german/trust_principles/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/german/trust_principles/","section":"glossary","summary":"","tags":null,"title":"TRUST Principles (TRUST Prinzipien)","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1689122843,"objectID":"581a85d3438fcc60c6949b6aa114446f","permalink":"https://forrt.org/curated_resources/trust-your-science-open-your-data-and-co/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/trust-your-science-open-your-data-and-co/","section":"curated_resources","summary":"A paper about Open Your Data and Code","tags":["Reproducibility Crisis and Credibility Revolution","Open Science"],"title":"Trust your science? Open your data and code.","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1689122843,"objectID":"80f385bb7ee471c7540bc37d11193a3f","permalink":"https://forrt.org/curated_resources/tutorial-r-code-for-creating-funnel-fore/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/tutorial-r-code-for-creating-funnel-fore/","section":"curated_resources","summary":"Meta-analyses are often accompanied by two popular forms of data visualization: forest plots and funnel plots. In this post, I’ll show how quick-and-dirty forest and funnel plots can be created with the metafor package. After, I’ll show how we can instead use the ggplot2 package to create forest plots and use the ggplot2 package to create funnel plots, so that we can have pretty plots that are easy to change/stylize, and that can be produced regardless of which meta-analysis package for R that you elect to use.","tags":["Blog","Tutorial"],"title":"Tutorial/R code for creating funnel/forest plots","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1689122843,"objectID":"f56caccd7957860fa7851f80d8d2de5b","permalink":"https://forrt.org/curated_resources/tutorial-r-code-for-creating-plots-for-2/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/tutorial-r-code-for-creating-plots-for-2/","section":"curated_resources","summary":"ggplot2, as I’ve already made clear, is one of my favourite packages for R. And since that original post about ggplot2 remains one of my most frequently visited, I thought I would proceed with starting a series of posts called “Make It Pretty”, all about sharing ways of visualizing data that I think are attractive/effective/comprehensive. So with this inaugural MIP post, I will be covering how to plot 2-way interactions using ggplot2. 2-way interactions can come in one of three general forms, and I will be providing code for plotting each. This will be a pretty lengthy post (lots of code/explanation), so if you’re only interested in learning how to plot a particular form, just click the the one below. Oh, and each uses an APA format theme that I’ve shared before, but just click here to quickly flip to the code if you need it.","tags":["Blog","Tutorial"],"title":"Tutorial/R code for creating plots for 2-way interactions","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1689122843,"objectID":"c64eab2f0f85aaf6605987d7d94a0d2c","permalink":"https://forrt.org/curated_resources/tutorial-r-code-for-creating-scree-paral/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/tutorial-r-code-for-creating-scree-paral/","section":"curated_resources","summary":"With this post, I’m going to be showing how you can use the psych package in conjunction with ggplot2 in order to create a prettier scree plot with parallel analysis–a very useful visualization when conducting exploratory factor analysis.","tags":["Blog","Tutorial"],"title":"Tutorial/R code for creating scree/parallel analysis plots","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"1ed439d79722daae100429ee68bc02cc","permalink":"https://forrt.org/curated_resources/two-years-later-journals-are-not-yet-enf/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/two-years-later-journals-are-not-yet-enf/","section":"curated_resources","summary":"A study by David Baker and colleagues reveals poor quality of reporting in pre-clinical animal research and a failure of journals to implement the ARRIVE guidelines. There is growing concern that poor experimental design and lack of transparent reporting contribute to the frequent failure of pre-clinical animal studies to translate into treatments for human disease. In 2010, the Animal Research: Reporting of In Vivo Experiments (ARRIVE) guidelines were introduced to help improve reporting standards. They were published in PLOS Biology and endorsed by funding agencies and publishers and their journals, including PLOS, Nature research journals, and other top-tier journals. Yet our analysis of papers published in PLOS and Nature journals indicates that there has been very little improvement in reporting standards since then. This suggests that authors, referees, and editors generally are ignoring guidelines, and the editorial endorsement is yet to be effectively implemented.","tags":["Analysis","Animal Models","Animal Studies","Data","Experimental Design","Publication Ethics","Publishing","Research Laboratories","Research Reporting Guidelines","Scientific Publishing","Statistical Data","Statistics"],"title":"Two Years Later: Journals Are Not Yet Enforcing the ARRIVE Guidelines on Reporting Standards for Pre-Clinical Animal Studies","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1721390377,"objectID":"3cf406eccdf46393d4f47a7e08285a47","permalink":"https://forrt.org/glossary/english/type_i_error/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/english/type_i_error/","section":"glossary","summary":"","tags":null,"title":"Type I error","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1635961284,"objectID":"297feedc7fa709e67925e0f60daa9048","permalink":"https://forrt.org/glossary/vbeta/type-i-error/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/vbeta/type-i-error/","section":"glossary","summary":"","tags":null,"title":"Type I error","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1720865851,"objectID":"26c5e7e94d3d8a70c20d879f46e440aa","permalink":"https://forrt.org/glossary/german/type_i_error/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/german/type_i_error/","section":"glossary","summary":"","tags":null,"title":"Type I error (Typ-I-Fehler)","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1721390377,"objectID":"c2cccc26249bf487a27e3799c4c7295a","permalink":"https://forrt.org/glossary/english/type_ii_error/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/english/type_ii_error/","section":"glossary","summary":"","tags":null,"title":"Type II error","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1635961284,"objectID":"8eb19ded042c78123223386a59bb5905","permalink":"https://forrt.org/glossary/vbeta/type-ii-error/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/vbeta/type-ii-error/","section":"glossary","summary":"","tags":null,"title":"Type II error","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1720865851,"objectID":"c183f3d8e63e90daecb9ae5f74aeb6b2","permalink":"https://forrt.org/glossary/german/type_ii_error/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/german/type_ii_error/","section":"glossary","summary":"","tags":null,"title":"Type II error (Typ-II-Fehler)","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1721390377,"objectID":"97e297c08db70a8d00ae525a3e58e0a0","permalink":"https://forrt.org/glossary/english/type_m_error/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/english/type_m_error/","section":"glossary","summary":"","tags":null,"title":"Type M error","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1635961284,"objectID":"a0b60fa45aa2fabbcbaa42e2e8a11459","permalink":"https://forrt.org/glossary/vbeta/type-m-error/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/vbeta/type-m-error/","section":"glossary","summary":"","tags":null,"title":"Type M error","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1720865851,"objectID":"0f2a57d90922de01f44fec63c43f16c1","permalink":"https://forrt.org/glossary/german/type_m_error/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/german/type_m_error/","section":"glossary","summary":"","tags":null,"title":"Type M error (Typ-M-Fehler)","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1721390377,"objectID":"93ef864c6a8260867b91bb764361c52d","permalink":"https://forrt.org/glossary/english/type_s_error/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/english/type_s_error/","section":"glossary","summary":"","tags":null,"title":"Type S error","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1635961284,"objectID":"d79f55d7d28c15adb1a88d6f9099c8ca","permalink":"https://forrt.org/glossary/vbeta/type-s-error/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/vbeta/type-s-error/","section":"glossary","summary":"","tags":null,"title":"Type S error","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1720865851,"objectID":"48eddbb780374fe59c4defe39da307a2","permalink":"https://forrt.org/glossary/german/type_s_error/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/german/type_s_error/","section":"glossary","summary":"","tags":null,"title":"Type S error (Typ S Fehler)","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"f4534286fa91adc5e988afc8a04bf7e5","permalink":"https://forrt.org/curated_resources/ukrn/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/ukrn/","section":"curated_resources","summary":"The UKRN primer series is designed to introduce a broad audience to important topics in open and reproducible scholarship. Each primer includes an overview of the topic in the introductory “What?” section, reasons for undertaking these practices in the “Why?” section, followed by a longer “How?” section that provides guidance on how to do that open research behaviour practically. Throughout the primers there are embedded explanatory weblinks, and at the end of each is a collated list of links to useful further resources.","tags":["Reproducibility","Research Administration","Researchers"],"title":"UKRN","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"3d6f9d2b731df8c53876df82c1f05be0","permalink":"https://forrt.org/curated_resources/ukrn-open-research-primers/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/ukrn-open-research-primers/","section":"curated_resources","summary":"Open Research Action Plan, Data Sharing, Open Access, Open Code \u0026 Software, Open Resarch Awards, Preprints, Preregistration \u0026 Registered Reports","tags":["Research"],"title":"UKRN Open Research Primers","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"ca9b88a49572dc5e111c2510b4f5569b","permalink":"https://forrt.org/curated_resources/ulysses-pact-or-ulysses-raft-using-pre-a/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/ulysses-pact-or-ulysses-raft-using-pre-a/","section":"curated_resources","summary":"Economists have recently adopted pre-analysis plans in response to concerns about robustness and transparency in research. The increased use of registered pre-analysis plans has raised competing concerns that detailed plans are costly to create, overly restrictive, and limit the type of inspiration that stems from exploratory analysis. We consider these competing views of pre-analysis plans, and make a careful distinction between the roles of pre-analysis plans and registries, which provide a record of all planned research. We propose a flexible “packraft” pre-analysis plan approach that offers benefits for a wide variety of experimental and nonexperimental applications in applied economics.","tags":["Hypothesis Registry","Pre-Analysis Plan","Preregistration","Research Ethics","Transparency"],"title":"Ulysses' pact or Ulysses' raft: Using pre-analysis plans in experimental and nonexperimental research","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1721390377,"objectID":"ec11d8ee195916fb60accc3858ace666","permalink":"https://forrt.org/glossary/english/under_representation/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/english/under_representation/","section":"glossary","summary":"","tags":null,"title":"Under-representation","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1635961284,"objectID":"5cce4d45f2ce0491ff9bafdb88536a48","permalink":"https://forrt.org/glossary/vbeta/under-representation/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/vbeta/under-representation/","section":"glossary","summary":"","tags":null,"title":"Under-representation","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1720865851,"objectID":"19cf50ad4ce9742e13a7d97d7f9c1e25","permalink":"https://forrt.org/glossary/german/under_representation/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/german/under_representation/","section":"glossary","summary":"","tags":null,"title":"Under-representation (Unterrepräsentation)","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1689122843,"objectID":"b2c41de62c36f8c6363adf3bf76ec3fb","permalink":"https://forrt.org/curated_resources/underreporting-in-psychology-experiments/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/underreporting-in-psychology-experiments/","section":"curated_resources","summary":"Many scholars have raised concerns about the credibility of empirical findings in psychology, arguing that the proportion of false positives reported in the published literature dramatically exceeds the rate implied by standard significance levels. A major contributor of false positives is the practice of reporting a subset of the potentially relevant statistical analyses pertaining to a research project. This study is the first to provide direct evidence of selective underreporting in psychology experiments. To overcome the problem that the complete experimental design and full set of measured variables are not accessible for most published research, we identify a population of published psychology experiments from a competitive grant program for which questionnaires and data are made publicly available because of an institutional rule. We find that about 40% of studies fail to fully report all experimental conditions and about 70% of studies do not report all outcome variables included in the questionnaire. Reported effect sizes are about twice as large as unreported effect sizes and are about 3 times more likely to be statistically significant.","tags":["Reproducibility Crisis and Credibility Revolution","Open Science"],"title":"Underreporting in Psychology Experiments: Evidence from a Study Registry. ","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1689122843,"objectID":"35baf19c645f9b78b931ccdf7651b066","permalink":"https://forrt.org/curated_resources/understanding-bayes-a-look-at-the-likeli/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/understanding-bayes-a-look-at-the-likeli/","section":"curated_resources","summary":"A blog about bayesian statistics","tags":["Blog"],"title":"Understanding Bayes: A Look at the Likelihood","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1689122843,"objectID":"d5d192b1dc7da4a3486edbaa2f1bc5bd","permalink":"https://forrt.org/curated_resources/understanding-bayes-visualization-of-the/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/understanding-bayes-visualization-of-the/","section":"curated_resources","summary":"An abstract about Understanding Bayes and visualising Bayes Factor","tags":["Blog","R code"],"title":"Understanding Bayes: Visualization of the Bayes Factor","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1689122843,"objectID":"0fd2676c2479c968aed991ab0129ad9c","permalink":"https://forrt.org/curated_resources/understanding-psychology-as-a-science-an/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/understanding-psychology-as-a-science-an/","section":"curated_resources","summary":"How can we objectively define categories of truth in scientific thinking? How can we reliably measure the results of research? In this ground-breaking text, Dienes undertakes a comprehensive historical analysis of the dominant schools of thought, key theories and influential thinkers that have progressed the foundational principles and characteristics that typify scientific research methodology today. This book delivers a masterfully simple, ‘though not simplistic’, introduction to the core arguments surrounding Popper, Kuhn and Lakatos, Fisher and Royall, Neyman and Pearson and Bayes. Subsequently, this book clarifies the prevalent misconceptions that surround such theoretical perspectives in psychology today, providing an especially accessible critique for student readers. ","tags":["Book"],"title":"Understanding psychology as a science: An introduction to scientific and statistical inference","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1689122843,"objectID":"06e2eee58e4497881f40f4f4223919b4","permalink":"https://forrt.org/curated_resources/understanding-statistical-power-and-sign/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/understanding-statistical-power-and-sign/","section":"curated_resources","summary":"Much has been said about significance testing – most of it negative. Methodologists constantly point out that researchers misinterpret p-values. Some say that it is at best a meaningless exercise and at worst an impediment to scientific discoveries. Consequently, I believe it is extremely important that students and researchers correctly interpret statistical tests. This visualization is meant as an aid for students when they are learning about statistical hypothesis testing. The visualization is based on a one-sample Z-test. You can vary the sample size, power, significance level and the effect size using the sliders to see how the sampling distributions change.","tags":["Blog","Interaction","Simulation","Tutorial"],"title":"Understanding Statistical Power and Significance Testing: an interactive visualization","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1721390377,"objectID":"ace190c284a5e47d89dae919cd386a6e","permalink":"https://forrt.org/glossary/english/universal_design_for_learning/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/english/universal_design_for_learning/","section":"glossary","summary":"","tags":null,"title":"Universal design for learning (UDL)","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1720865851,"objectID":"592bd7cdaa95aeffe8554340d7c4e490","permalink":"https://forrt.org/glossary/german/universal_design_for_learning/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/german/universal_design_for_learning/","section":"glossary","summary":"","tags":null,"title":"Universal design for learning (UDL)","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1635961284,"objectID":"59774812c15a0e67540b3152da6ab221","permalink":"https://forrt.org/glossary/vbeta/universal-design-for-learning-udl/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/vbeta/universal-design-for-learning-udl/","section":"glossary","summary":"","tags":null,"title":"Universal design for learning (UDL)","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1689122843,"objectID":"40296f2938ea6b3db9961b08b8703cd7","permalink":"https://forrt.org/curated_resources/unlikely-results/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/unlikely-results/","section":"curated_resources","summary":"Why most published scientific research is probably false","tags":["Video"],"title":"Unlikely Results","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"f5024b449e2827d8eb06d6ee2f6490b7","permalink":"https://forrt.org/curated_resources/up-front-and-open-shrouded-in-secrecy-or/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/up-front-and-open-shrouded-in-secrecy-or/","section":"curated_resources","summary":"OBJECTIVE: To investigate open science practices in research published in the top five sports medicine journals from 01 May 2022 and 01 October 2022.\n\nDESIGN: A meta-research systematic review\n\nLITERATURE SEARCH: Open science practices were searched in MEDLINE.\n\nSTUDY SELECTION CRITERIA: We included original scientific research published in one of the identified top-five sports medicine journals in 2022 as ranked by Clarivate ((1) British Journal of Sports Medicine, (2) Journal of Sport and Health Science, (3) American Journal of Sports Medicine, (4) Medicine Science Sport and Exercise, and (5) Sports Medicine-Open). Studies were excluded if they were systematic reviews, qualitative research, grey literature, or animal or cadaver models.\n\nDATA SYNTHESIS: Open science practices were extracted in accordance with the Transparency and Openness Promotion (TOP) guidelines and patient and public involvement (PPI).\n\nRESULTS: 243 studies were included. The median number of open science practices in each study was 2, out of a maximum of 12 (Range: 0-8; IQR: 2). 234 studies (96%, 95% CI: 94-99%) provided an author conflict of interest statement and 163 (67%, 95% CI: 62-73%) reported funding. 21 studies (9%, 95% CI: 5-12%) provided open access data. Fifty-four studies (22%, 95% CI: 17-27%) included a data availability statement and 3 (1%, 95% CI: 0-3%) made code available. Seventy-six studies (32%, 95% CI: 25-37%) had transparent materials and 30 (12%, 95% CI: 8-16) used a reporting guideline. Twenty-eight studies (12%, 95% CI: 8-16%) were pre-registered. Six studies (3%, 95% CI: 1-4%) published a protocol. Four studies (2%, 95% CI: 0-3%) reported an analysis plan a priori. Seven studies (3%, 95% CI: 1-5%) reported patient and public involvement.\n\nCONCLUSION: Open science practices in the sports medicine field are extremely limited. The least followed practices were sharing code, data, and analysis plans.","tags":["Open Access","Open Code","Study Protocol","Reporting Guidelines"],"title":"Up front and open, shrouded in secrecy, or somewhere in between? A Meta Research Systematic Review of Open Science Practices in Sport Medicine Research","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"b6aa4f60f324dfd124e2d24964e2478b","permalink":"https://forrt.org/curated_resources/update-on-the-endorsement-of-consort-by/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/update-on-the-endorsement-of-consort-by/","section":"curated_resources","summary":"The CONsolidated Standards Of Reporting Trials (CONSORT) Statement provides a minimum standard set of items to be reported in published clinical trials; it has received widespread recognition within the biomedical publishing community. This research aims to provide an update on the endorsement of CONSORT by high impact medical journals. Methods We performed a cross-sectional examination of the online “Instructions to Authors” of 168 high impact factor (2012) biomedical journals between July and December 2014. We assessed whether the text of the “Instructions to Authors” mentioned the CONSORT Statement and any CONSORT extensions, and we quantified the extent and nature of the journals’ endorsements of these. These data were described by frequencies. We also determined whether journals mentioned trial registration and the International Committee of Medical Journal Editors (ICMJE; other than in regards to trial registration) and whether either of these was associated with CONSORT endorsement (relative risk and 95 % confidence interval). We compared our findings to the two previous iterations of this survey (in 2003 and 2007). We also identified the publishers of the included journals. Results Sixty-three percent (106/168) of the included journals mentioned CONSORT in their “Instructions to Authors.” Forty-four endorsers (42 %) explicitly stated that authors “must” use CONSORT to prepare their trial manuscript, 38 % required an accompanying completed CONSORT checklist as a condition of submission, and 39 % explicitly requested the inclusion of a flow diagram with the submission. CONSORT extensions were endorsed by very few journals. One hundred and thirty journals (77 %) mentioned ICMJE, and 106 (63 %) mentioned trial registration. Conclusions The endorsement of CONSORT by high impact journals has increased over time; however, specific instructions on how CONSORT should be used by authors are inconsistent across journals and publishers. Publishers and journals should encourage authors to use CONSORT and set clear expectations for authors about compliance with CONSORT.","tags":["Data","Publishing","Reporting Guidelines"],"title":"Update on the endorsement of CONSORT by high impact factor journals: a survey of journal “Instructions to Authors” in 2014","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"4aabe388088e2d4f40c3ca68e68db482","permalink":"https://forrt.org/curated_resources/update-on-trial-registration-11-years-af/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/update-on-trial-registration-11-years-af/","section":"curated_resources","summary":"Laws and policies to establish a global trial reporting system have greatly increased the transparency and accountability of the clinical research enterprise. The three components of the trial reporting system are trial registration, reporting of aggregate results, and sharing of individual participant data.1 Trial registration is foundational to our understanding and interpretation of trial results, because it requires that information be provided about all relevant clinical trials (to put results in a broad context) and their prespecified protocol details (to ensure adherence to the scientific plan).\n\nIn this article, we describe the current trial registration landscape and summarize evidence of its effect on the clinical research enterprise to date. We then present the results of analyses that were performed with the use of ClinicalTrials.gov data to provide additional evidence regarding the degree to which current practices are fulfilling certain key goals initially envisioned for trial registration. Finally, we identify challenges and suggest potential responses for the next decade.","tags":["Trial Registration","Medicine","Preregistration"],"title":"Update on Trial Registration 11 Years after the ICMJE Policy Was Established","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"5cfe4182fabae63b15a5d713e1141dc5","permalink":"https://forrt.org/curated_resources/use-of-the-journal-impact-factor-in-acad/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/use-of-the-journal-impact-factor-in-acad/","section":"curated_resources","summary":"The Journal Impact Factor (JIF) was originally designed to aid libraries in deciding which journals to index and purchase for their collections. Over the past few decades, however, it has become a relied upon metric used to evaluate research articles based on journal rank. Surveyed faculty often report feeling pressure to publish in journals with high JIFs and mention reliance on the JIF as one problem with current academic evaluation systems. While faculty reports are useful, information is lacking on how often and in what ways the JIF is currently used for review, promotion, and tenure (RPT). We therefore collected and analyzed RPT documents from a representative sample of 129 universities from the United States and Canada and 381 of their academic units. We found that 40% of doctoral, research-intensive (R-type) institutions and 18% of master's, or comprehensive (M-type) institutions explicitly mentioned the JIF, or closely related terms, in their RPT documents. Undergraduate, or baccalaureate (B-type) institutions did not mention it at all. A detailed reading of these documents suggests that institutions may also be using a variety of terms to indirectly refer to the JIF. Our qualitative analysis shows that 87% of the institutions that mentioned the JIF supported the metric's use in at least one of their RPT documents, while 13% of institutions expressed caution about the JIF's use in evaluations. None of the RPT documents we analyzed heavily criticized the JIF or prohibited its use in evaluations. Of the institutions that mentioned the JIF, 63% associated it with quality, 40% with impact, importance, or significance, and 20% with prestige, reputation, or status. In sum, our results show that the use of the JIF is encouraged in RPT evaluations, especially at research-intensive universities, and indicates there is work to be done to improve evaluation processes to avoid the potential misuse of metrics like the JIF.","tags":["And Tenure","Journal Impact Factor","Promotion","Publishing","Review"],"title":"Use of the Journal Impact Factor in academic review, promotion, and tenure evaluations","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"7b96e1f83a3aa45aa5e5f21b1f05d58c","permalink":"https://forrt.org/curated_resources/using-artificial-intelligence-to-create/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/using-artificial-intelligence-to-create/","section":"curated_resources","summary":"Introduction\nMedical case vignettes play a crucial role in medical education, yet they often fail to authentically represent diverse patients. Moreover, these vignettes tend to oversimplify the complex relationship between patient characteristics and medical conditions, leading to biased and potentially harmful perspectives among students. Displaying aspects of patient diversity, such as ethnicity, in written cases proves challenging. Additionally, creating these cases places a significant burden on teachers in terms of labor and time. Our objective is to explore the potential of AI-assisted computer-generated clinical cases to expedite case creation and enhance diversity, along with AI-generated patient photographs for more lifelike portrayal.\n\nMethods\nIn this study, we employed chatGPT (OpenAI, GPT 3.5) to develop diverse and inclusive medical case vignettes. We evaluated various approaches and identified a set of eight consecutive prompts that can be readily customized to accommodate local contexts and specific assignments. To enhance visual representation, we utilized Adobe Firefly beta for image generation.\n\nResults\nUsing the described prompts, we consistently generated cases for various assignments, producing sets of 30 cases at a time. We ensured the inclusion of mandatory checks and formatting, completing the process within approximately 60 minutes per set.\n\nDiscussion\nOur approach significantly accelerated case creation and improved diversity, though prioritizing maximum diversity compromised representativeness to some extent. While the optimized prompts are easily reusable, the process itself demands computer skills not all educators possess. To address this, we aim to share all created patients as Open Educational Resources (OER), empowering educators to create cases independently.","tags":["Artificial Intelligence","chatGPT","Diversity","Inclusivity"],"title":"Using artificial intelligence to create diverse and inclusive medical case vignettes for education","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1689122843,"objectID":"cda5ebbaeec1de03e5c01b0ee40d72d8","permalink":"https://forrt.org/curated_resources/using-osf-to-share-data-a-step-by-step-g/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/using-osf-to-share-data-a-step-by-step-g/","section":"curated_resources","summary":"Sharing data, materials, and analysis scripts with reviewers and readers is valued in psychological science. To facilitate this sharing, files should be stored in a stable location, referenced with unique identifiers, and cited in published work associated with them. This Tutorial provides a step-by-step guide to using OSF to meet the needs for sharing psychological data.","tags":["Reproducibility Crisis and Credibility Revolution","Open Science"],"title":"Using OSF to Share Data: A Step-by-Step Guide","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"d4330e584890a65c42783c60f58aa5f0","permalink":"https://forrt.org/curated_resources/using-pre-analysis-plans-in-qualitative/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/using-pre-analysis-plans-in-qualitative/","section":"curated_resources","summary":"In the last decade, there has been a significant push for greater transparency in the social sciences. For example, epistemological and methodological debates have addressed the scope, meaning, and appropriateness of research transparency, and scholars have developed tools and practices to facilitate the process. One such approach is preregistration, the practice of recording a priori a study’s design and its plan of analysis in open and public repositories (Haven et al. 2020). While it is a standard practice in experimental social science, it has been a matter of contested debate in observational work, both  quantitative  and  qualitative.  Arguments  in  favor of using this practice in qualitative inquiry, as well as opposing views, have recently been published (Büthe et al. 2015; Elman and Kapiszewski 2014; Elman and Lupia 2016; Kern and Gleditsch 2017; Haven et al. 2020; Jacobs et al. 2021; Kapiszewski and Karcher 2020; Moravcsik 2014; Piñeiro and Rosenblatt 2016). ","tags":["Pre-Analysis Plan","Preregistration","Qualitative Research"],"title":"Using Pre-Analysis Plans in Qualitative Research","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1689122843,"objectID":"7162adefd362743feafdc79d8ee7f55f","permalink":"https://forrt.org/curated_resources/using-prediction-markets-to-estimate-the/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/using-prediction-markets-to-estimate-the/","section":"curated_resources","summary":"Concerns about a lack of reproducibility of statistically significant results have recently been raised in many fields, and it has been argued that this lack comes at substantial economic costs. We here report the results from prediction markets set up to quantify the reproducibility of 44 studies published in prominent psychology journals and replicated in the Reproducibility Project: Psychology. The prediction markets predict the outcomes of the replications well and outperform a survey of market participants’ individual forecasts. This shows that prediction markets are a promising tool for assessing the reproducibility of published scientific results. The prediction markets also allow us to estimate probabilities for the hypotheses being true at different testing stages, which provides valuable information regarding the temporal dynamics of scientific discovery. We find that the hypotheses being tested in psychology typically have low prior probabilities of being true (median, 9%) and that a “statistically significant” finding needs to be confirmed in a well-powered replication to have a high probability of being true. We argue that prediction markets could be used to obtain speedy information about reproducibility at low cost and could potentially even be used to determine which studies to replicate to optimally allocate limited resources into replications.","tags":["Reproducibility Crisis and Credibility Revolution","Open Science"],"title":"Using prediction markets to estimate the reproducibility of scientific research","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1689122843,"objectID":"bac7ffc7a2882979ec2d72cb05745fe1","permalink":"https://forrt.org/curated_resources/using-r-and-lme-lmer-to-fit-different-tw/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/using-r-and-lme-lmer-to-fit-different-tw/","section":"curated_resources","summary":"I often get asked how to fit different multilevel models (or individual growth models, hierarchical linear models or linear mixed-models, etc.) in R. In this guide I have compiled some of the more common and/or useful models (at least common in clinical psychology), and how to fit them using nlme::lme() and lme4::lmer(). I will cover the common two-level random intercept-slope model, and three-level models when subjects are clustered due to some higher level grouping (such as therapists), partially nested models were there are clustering in one group but not the other, and different level 1 residual covariances (such as AR(1)). The point of this post is to show how to fit these longitudinal models in R, not to cover the statistical theory behind them, or how to interpret them.","tags":["Blog","Tutorial"],"title":"Using R and lme/lmer to fit different two- and three- level longitudinal models","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1689122843,"objectID":"45a5c2d15f10a085a2e03592bdffa53d","permalink":"https://forrt.org/curated_resources/using-science-and-psychology-to-improve/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/using-science-and-psychology-to-improve/","section":"curated_resources","summary":"Here I outline some of what science can tell us about the problems in psychological publishing and how to best address those problems. First, the motivation behind questionable research practices is examined (the desire to get ahead or, at least, not fall behind). Next, behavior modification strategies are discussed, pointing out that reward works better than punishment. Humans are utility seekers and the implementation of current change initiatives is hindered by high initial buy-in costs and insufficient expected utility. Open science tools interested in improving science should team up, to increase utility while lowering the cost and risk associated with engagement. The best way to realign individual and group motives will probably be to create one, centralized, easy to use, platform, with a profile, a feed of targeted science stories based upon previous system interaction, a sophisticated (public) discussion section, and impact metrics which use the associated data. These measures encourage high quality review and other prosocial activities while inhibiting self-serving behavior. Some advantages of centrally digitizing communications are outlined, including ways the data could be used to improve the peer review process. Most generally, it seems that decisions about change design and implementation should be theory and data driven.","tags":["Reproducibility Crisis and Credibility Revolution","Open Science"],"title":"Using science and psychology to improve the dissemination and evaluation of scientific work","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1721390377,"objectID":"e9119cc12656138721e10deb115647cc","permalink":"https://forrt.org/glossary/english/validity/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/english/validity/","section":"glossary","summary":"","tags":null,"title":"Validity","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1635961284,"objectID":"9ec099ef9bda84c10191127c0de04236","permalink":"https://forrt.org/glossary/vbeta/validity/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/vbeta/validity/","section":"glossary","summary":"","tags":null,"title":"Validity","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1720865851,"objectID":"b936f2f926b1ca539b3b372e99efa587","permalink":"https://forrt.org/glossary/german/validity/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/german/validity/","section":"glossary","summary":"","tags":null,"title":"Validity (Validität)","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1721390377,"objectID":"05ce44605ea43861b5da468c07cf912e","permalink":"https://forrt.org/glossary/english/version_control/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/english/version_control/","section":"glossary","summary":"","tags":null,"title":"Version control","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1635961284,"objectID":"d308051498eaae92783ab86b443c4e22","permalink":"https://forrt.org/glossary/vbeta/version-control/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/vbeta/version-control/","section":"glossary","summary":"","tags":null,"title":"Version control","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1720865851,"objectID":"59eab963434f5dec4e45e8f8626d4d23","permalink":"https://forrt.org/glossary/german/version_control/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/german/version_control/","section":"glossary","summary":"","tags":null,"title":"Version control (Versionskontrolle)","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"2704f0348d769d1d900ddb522f47f270","permalink":"https://forrt.org/curated_resources/version-control-with-the-osf/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/version-control-with-the-osf/","section":"curated_resources","summary":"This webinar will introduce the concept of version control and the version control features that are built into the Open Science Framework (OSF; https://osf.io). The OSF is a free, open source web application built to help researchers manage their workflows. The OSF is part collaboration tool, part version control software, and part data archive. The OSF connects to popular tools researchers already use, like Dropbox, Box, Github and Mendeley, to streamline workflows and increase efficiency. This webinar will discuss how keeping track of the different file versions is important for efficient reproducible research practices, how version control works on the OSF, and how researchers can view and download previous versions of files.","tags":["Analysis","Data","Education","Materials","Open Scholarship Tools and Technologies","OSF","Publishing","Reproducibility","Research Data Management Tools","Researchers"],"title":"Version control with the OSF","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1689122843,"objectID":"9fd3f80e42b35a63e5ad4deb85538cdd","permalink":"https://forrt.org/curated_resources/visualizing-a-one-way-anova-using-d3-js/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/visualizing-a-one-way-anova-using-d3-js/","section":"curated_resources","summary":"A blog post and tutorial on visualizing one-way ANOVA","tags":["Blog","Interactive","Tutorial"],"title":"Visualizing a One-Way ANOVA using D3.js","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"9a9542e36a47059e6c908162f9816e18","permalink":"https://forrt.org/curated_resources/want-to-speed-up-scientific-progress-fir/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/want-to-speed-up-scientific-progress-fir/","section":"curated_resources","summary":"Researchers and policymakers often exist in different worlds and speak different languages. Here are three ways to bridge the divide.","tags":["Policy","Funding","Scientific Communication"],"title":"Want to speed up scientific progress? First understand how science policy works","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1689122843,"objectID":"2f4b7d39c950a3b98436f30ce530d997","permalink":"https://forrt.org/curated_resources/we-have-to-break-up/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/we-have-to-break-up/","section":"curated_resources","summary":"Three mostly positive developments in academic psychology—the cognitive revolution, the virtual requirement for multiple study reports in our top journals, and the prioritization of mediational evidence in our data—have had the unintended effect of making field research on naturally occurring behavior less suited to publication in the leading outlets of the discipline. Two regrettable consequences have ensued. The first is a reduction in the willingness of researchers, especially those young investigators confronting hiring and promotion issues, to undertake such field work. The second is a reduction in the clarity with which nonacademic audiences (e.g., citizens and legislators) can see the relevance of academic psychology to their lives and self-interest, which has contributed to a concomitant reduction in the availability of federal funds for basic behavioral science. Suggestions are offered for countering this problem","tags":["Reproducibility Crisis and Credibility Revolution","Open Science"],"title":"We have to break up","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1689122843,"objectID":"f24d84b239601bcfe1609fe781e725b6","permalink":"https://forrt.org/curated_resources/we-knew-the-future-all-along-scientific/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/we-knew-the-future-all-along-scientific/","section":"curated_resources","summary":"A critique about Daryl Bem's (2011) paper and reproducibility ","tags":["Reproducibility Crisis and Credibility Revolution","Open Science"],"title":"We Knew the Future All Along: Scientific Hypothesizing Is Much More Accurate Than Other Forms of Precognition-A Satire in One Part","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1721390377,"objectID":"e8a4dd1f8df0e78afe3104817365657a","permalink":"https://forrt.org/glossary/english/webometrics/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/english/webometrics/","section":"glossary","summary":"","tags":null,"title":"Webometrics","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1635961284,"objectID":"96da5c4ad34a22cddfe9e6b95b5c2c46","permalink":"https://forrt.org/glossary/vbeta/webometrics/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/vbeta/webometrics/","section":"glossary","summary":"","tags":null,"title":"Webometrics ","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1720865851,"objectID":"53262754b5eee0a775baa90544e8eaed","permalink":"https://forrt.org/glossary/german/webometrics/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/german/webometrics/","section":"glossary","summary":"","tags":null,"title":"Webometrics (Webometrie)","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1721390377,"objectID":"0e0d007ea2c41906cb9a0c54d6fa4898","permalink":"https://forrt.org/glossary/english/weird/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/english/weird/","section":"glossary","summary":"","tags":null,"title":"WEIRD","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1720865851,"objectID":"7858a21f9f6173d19514fa0457d79a65","permalink":"https://forrt.org/glossary/german/weird/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/german/weird/","section":"glossary","summary":"","tags":null,"title":"WEIRD","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1635961284,"objectID":"dde9c6d2b2cd4a82b0dc74de5532d05a","permalink":"https://forrt.org/glossary/vbeta/weird/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/vbeta/weird/","section":"glossary","summary":"","tags":null,"title":"WEIRD","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1689122843,"objectID":"7aef60ca7c3ab7134f7d1cf7f978572b","permalink":"https://forrt.org/curated_resources/welcoming-quality-in-non-significance-an/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/welcoming-quality-in-non-significance-an/","section":"curated_resources","summary":"The self-correcting nature of psychological and educational science has been seriously questioned. Recent special issues of Perspectives on Psychological Science and Psychology of Aesthetics, Creativity, and the Arts have roundly condemned current organizational models of research and dissemination and have criticized the perverse incentive structure that tempts researchers into generating and publishing false positive findings. At the same time, replications are rarely attempted, allowing untruths to persist in the literature unchallenged. In this article, the editors of the Journal of Advanced Academics consider this situation and announce new policies for quantitative submissions. They are (a) an explicit call for replication studies; (b) new instructions directing reviewers to base their evaluation of a study’s merit on the quality of the research design, execution, and written description, rather than on the statistical significance of its results; and (c) an invitation to omit statistical hypothesis tests in favor of reporting effect sizes and their confidence limits.","tags":["Reproducibility Crisis and Credibility Revolution","Open Science"],"title":"Welcoming Quality in Non-Significance and Replication Work, but Moving Beyond the p-Value: Announcing New Editorial Policies for Quantitative Research in JOAA","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"838057d3434d456153eb07c988d9ad08","permalink":"https://forrt.org/curated_resources/what-does-research-reproducibility-mean/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/what-does-research-reproducibility-mean/","section":"curated_resources","summary":"The language and conceptual framework of “research reproducibility” are nonstandard and unsettled across the sciences. In this Perspective, we review an array of explicit and implicit definitions of reproducibility and related terminology, and discuss how to avoid potential misunderstandings when these terms are used as a surrogate for “truth.”","tags":["Reproducibility Crisis and Credibility Revolution","Open Science"],"title":"What does research reproducibility mean?","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"573379ca4fda388ea1ca1f711b8f7cf9","permalink":"https://forrt.org/curated_resources/what-incentives-increase-data-sharing-in/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/what-incentives-increase-data-sharing-in/","section":"curated_resources","summary":"The foundation of health and medical research is data. Data sharing facilitates the progress of research and strengthens science. Data sharing in research is widely discussed in the literature; however, there are seemingly no evidence-based incentives that promote data sharing. Methods A systematic review (registration: doi.org/10.17605/OSF.IO/6PZ5E) of the health and medical research literature was used to uncover any evidence-based incentives, with pre- and post-empirical data that examined data sharing rates. We were also interested in quantifying and classifying the number of opinion pieces on the importance of incentives, the number observational studies that analysed data sharing rates and practices, and strategies aimed at increasing data sharing rates. Results Only one incentive (using open data badges) has been tested in health and medical research that examined data sharing rates. The number of opinion pieces (n = 85) out-weighed the number of article-testing strategies (n = 76), and the number of observational studies exceeded them both (n = 106). Conclusions Given that data is the foundation of evidence-based health and medical research, it is paradoxical that there is only one evidence-based incentive to promote data sharing. More well-designed studies are needed in order to increase the currently low rates of data sharing.","tags":["Data","Data Sharing","Open Data"],"title":"What incentives increase data sharing in health and medical research? A systematic review","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1689122843,"objectID":"a5cde263c143edeb59097a6eee5f7594","permalink":"https://forrt.org/curated_resources/what-is-a-p-value/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/what-is-a-p-value/","section":"curated_resources","summary":"Blog post going over the p value, misnomers and what p \u003c .05 means","tags":["Blog","Code","Reproducibility Crisis and Credibility Revolution"],"title":"What is a p-value?","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"cc6b9a7e8eacba8c3dd8ddfd9f35dd87","permalink":"https://forrt.org/curated_resources/what-is-replication/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/what-is-replication/","section":"curated_resources","summary":"Replications are inevitably different from the original studies. How do we decide whether something is a replication? The answer shifts the conception of replication from a boring, uncreative, housekeeping activity to an exciting, generative, vital contributor to research progress.","tags":["Reproducibility","Researchers"],"title":"What is replication?","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"083b8c2532256091bcdef7cd8e0de1c1","permalink":"https://forrt.org/curated_resources/what-is-statistical-power/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/what-is-statistical-power/","section":"curated_resources","summary":"This video is the first in a series of videos related to the basics of power analyses. All materials shown in the video, as well as content from the other videos in the power analysis series can be found here: https://osf.io/a4xhr/","tags":["Analysis","Data","Education","Materials","Open Scholarship Tools and Technologies","OSF","Power","Publishing","Reproducibility","Research Data Management Tools","Researchers","Statistics"],"title":"What is statistical power","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"5f635c5d6b889139403c3e163320ec54","permalink":"https://forrt.org/curated_resources/what-is-this-thing-called-open-science/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/what-is-this-thing-called-open-science/","section":"curated_resources","summary":"A general introduction to open scholarship.","tags":["Data","Funders","History of Science","Metascience","Open Data","Open Scholarship Policy","Policy","Policy Makers","Publishing","Publishing Models","Reproducibility","Research Administration","Researcher Degrees of Freedom","Researchers","Research Integrity","Statistics","Transparency"],"title":"What is this thing called open science?","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"d336ef3a0f1e54a6de933aa25cdae306","permalink":"https://forrt.org/curated_resources/what-is-universal-design-for-learning-ud/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/what-is-universal-design-for-learning-ud/","section":"curated_resources","summary":"Universal Design for Learning (UDL) is a way of thinking about teaching and learning that helps give all students an equal opportunity to succeed. This approach offers flexibility in the ways students access material, engage with it and show what they know. Developing lesson plans this way helps all kids, but it may be especially helpful for kids with learning and thinking difference.","tags":["Diversity","Equity","Inclusion","Neurodiversity"],"title":"What is Universal Design for Learning (UDL)?","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"4957df5ee97d845b137e70591c95b3c2","permalink":"https://forrt.org/curated_resources/what-should-a-preregistration-contain/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/what-should-a-preregistration-contain/","section":"curated_resources","summary":"A large amount of variation exists in beliefs about the purpose and benefits of preregistration, making it difficult to implement and evaluate, and limiting its usefulness. Additionally, no single resource exists to describe what a preregistration should contain or how it should be used. In this paper, I describe what an effective preregistration should contain and when it should be used. Specifically, preregistration should 1) restrict as many researcher degrees of freedom as possible, 2) detail all aspects of a study’s method and analysis, 3) detail information on decisions made during the planning stages, and 4) specify how the results will be used and interpreted. Further, a preregistration must be publicly verifiable and permanent. Finally, I argue that pre-registration should be used in any situation where researchers intend to collect data in order to make a claim, description, decision, or inference based on that data. I also note that preregistrations which do not address each of these points do more harm than good by falsely signalling credibility and quality.","tags":["Hypothesis Testing","Metascience","Open Science","Planning","Preregistration","Research Methods. Transparency"],"title":"What should a preregistration contain?","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1689122843,"objectID":"bc233f4359b26975b3f3406870b023b9","permalink":"https://forrt.org/curated_resources/what-you-see-is-what-you-get-enhancing-m/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/what-you-see-is-what-you-get-enhancing-m/","section":"curated_resources","summary":"We review the literature on evidence-based best practices on how to enhance methodological transparency, which is the degree of detail and disclosure about the specific steps, decisions, and judgment calls made during a scientific study. We conceptualize lack of transparency as a “research performance problem” because it masks fraudulent acts, serious errors, and questionable research practices, and therefore precludes inferential and results reproducibility. Our recommendations for authors provide guidance on how to increase transparency at each stage of the research process: (1) theory, (2) design, (3) measurement, (4) analysis, and (5) reporting of results. We also offer recommendations for journal editors, reviewers, and publishers on how to motivate authors to be more transparent. We group these recommendations into the following categories: (1) manuscript submission forms requiring authors to certify they have taken actions to enhance transparency, (2) manuscript evaluation forms including additional items to encourage reviewers to assess the degree of transparency, and (3) review process improvements to enhance transparency. Taken together, our recommendations provide a resource for doctoral education and training; researchers conducting empirical studies; journal editors and reviewers evaluating submissions; and journals, publishers, and professional organizations interested in enhancing the credibility and trustworthiness of research.","tags":["Reproducibility Crisis and Credibility Revolution","Open Science"],"title":"What You See Is What You Get? Enhancing Methodological Transparency in Management Research","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1689122843,"objectID":"6e956555f3b2f32adf0251fe66fcb682","permalink":"https://forrt.org/curated_resources/what-s-wrong-with-psychology-anyway/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/what-s-wrong-with-psychology-anyway/","section":"curated_resources","summary":"This chapter considers various factors that have been responsible for the comparatively slow development of psychology into a cumulative empirical science. Special attention is devoted to correctable methodological mistakes, the over-reliance upon significance testing (and the fact that, in psychology, the null hypothesis is almost always false), and an analysis of the concept of replication.","tags":["Reproducibility Crisis and Credibility Revolution","Open Science","Transparency"],"title":"What's wrong with Psychology, anyway?","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1689122843,"objectID":"85b78ab634fb679f3942a0e1c3e4b709","permalink":"https://forrt.org/curated_resources/what-s-wrong-with-statistical-tests-and/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/what-s-wrong-with-statistical-tests-and/","section":"curated_resources","summary":"This chapter considers problems with null hypothesis significance testing (NHST). The literature in this area is quite large. D. Anderson, Burnham, and W. Thompson (2000) recently found more than 300 articles in different disciplines about the indiscriminate use of NHST, and W. Thompson (2001) lists more than 400 references about this topic. As a consequence, it is possible to cite only a few representative works. After review of the debate about NHST, the author argues that the criticisms have sufficient merit to support the minimization or elimination of NHST in the behavioral sciences. The author offers specific suggestions along these lines. Some concern alternatives that may replace or supplement NHST and thus are directed at researchers. Others concern editorial policies or educational curricula. Few of the recommendations given are original in that many have been made over the years by various authors. However, as a set they deal with issues often considered in separate works. For simplicity, the context for NHST assumed is reject-support (RS) instead of accept-support (AS). The RS context is more common, and many of the arguments can be reframed for the AS context.","tags":[""],"title":"What’s wrong with statistical tests – and where do we go from here?","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"85c70c3192f60127f87c8cf57ff5f071","permalink":"https://forrt.org/curated_resources/when-does-harking-hurt-identifying-when/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/when-does-harking-hurt-identifying-when/","section":"curated_resources","summary":"Hypothesizing after the results are known, or HARKing, occurs when researchers check their research results and then add or remove hypotheses on the basis of those results without acknowledging this process in their research report (Kerr, 1998). In the present article, I discuss 3 forms of HARKing: (a) using current results to construct post hoc hypotheses that are then reported as if they were a priori hypotheses; (b) retrieving hypotheses from a post hoc literature search and reporting them as a priori hypotheses; and (c) failing to report a priori hypotheses that are unsupported by the current results. These 3 types of HARKing are often characterized as being bad for science and a potential cause of the current replication crisis. In the present article, I use insights from the philosophy of science to present a more nuanced view. Specifically, I identify the conditions under which each of these 3 types of HARKing is most and least likely to be bad for science. I conclude with a brief discussion about the ethics of each type of HARKing.","tags":["Accommodation","Falsification","HARKing","Prediction","Replication Crisis"],"title":"When Does HARKing Hurt? Identifying When Different Types of Undisclosed Post Hoc Hypothesizing Harm Scientific Progress","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"53ccd2bfafbc503c5cc4d935ddb9e2ce","permalink":"https://forrt.org/curated_resources/when-great-minds-think-unalike-inside-sc/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/when-great-minds-think-unalike-inside-sc/","section":"curated_resources","summary":"A podcast about replication crisis","tags":["Podcast","Reproducibility Knowledge","Reproducibility Crisis and Credibility Revolution"],"title":"When Great Minds Think Unalike: Inside Science's 'Replication Crisis'","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"e6a5c090431c1286d38dcc3978637ac4","permalink":"https://forrt.org/curated_resources/when-is-science-un-reliable/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/when-is-science-un-reliable/","section":"curated_resources","summary":"In this course, we will explore the so‐called “reproducibility crisis” that has struck fields from psychology and economics to ecology and cancer biology. You will learn statistical principles at the heart of the reproducibility crisis, how disregard for those principles undermines the reliability of scientific inference, and how such disregard has been incentivized by various institutions. You will learn to recognize problematic research practices and will critically evaluate scientific claims both in the scientific literature and in the popular press. Further, you will evaluate and debate proposals for institutional policies designed to reduce bias and improve reproducibility.","tags":["Reproducibility Crisis and Credibility Revolution","Open Science","Conceptual and statistical knowledge"],"title":"When is science (un)reliable?","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1689122843,"objectID":"fc910a36785c0d902320eac1e36a3861","permalink":"https://forrt.org/curated_resources/when-power-analyses-based-on-pilot-data/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/when-power-analyses-based-on-pilot-data/","section":"curated_resources","summary":"When designing a study, the planned sample size is often based on power analyses. One way to choose an effect size for power analyses is by relying on pilot data. A-priori power analyses are only accurate when the effect size estimate is accurate. In this paper we highlight two sources of bias when performing a-priori power analyses for between-subject designs based on pilot data. First, we examine how the choice of the effect size index (η2, ω2 and ε2) affects the sample size and power of the main study. Based on our observations, we recommend against the use of η2 in a-priori power analyses. Second, we examine how the maximum sample size researchers are willing to collect in a main study (e.g. due to time or financial constraints) leads to overestimated effect size estimates in the studies that are performed. Determining the required sample size exclusively based on the effect size estimates from pilot data, and following up on pilot studies only when the sample size estimate for the main study is considered feasible, creates what we term follow-up bias. We explain how follow-up bias leads to underpowered main studies. Our simulations show that designing main studies based on effect sizes estimated from small pilot studies does not yield desired levels of power due to accuracy bias and follow-up bias, even when publication bias is not an issue. We urge researchers to consider alternative approaches to determining the sample size of their studies, and discuss several options.","tags":[""],"title":"When power analyses based on pilot data are biased: Inaccurate effect size estimators and follow-up bias","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1689122843,"objectID":"4350e88ebcc96bf60a1a3022afdfb983","permalink":"https://forrt.org/curated_resources/which-is-the-correct-statistical-test-to/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/which-is-the-correct-statistical-test-to/","section":"curated_resources","summary":"This paper explains how to select the correct statistical test for a research project, clinical trial, or other investigation. The first step is to decide in what scale of measurement your data are as this will affect your decision—nominal, ordinal, or interval. The next stage is to consider the purpose of the analysis—for example, are you comparing independent or paired groups? Several statistical tests are discussed with an explanation of when it is appropriate to use each one; relevant examples of each are provided. If an incorrect test is used, then invalid results and misleading conclusions may be drawn from the study","tags":[""],"title":"Which is the correct statistical test to use?","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"34d4f9ba003dad73ee9dc0659871eb78","permalink":"https://forrt.org/curated_resources/who-re-uses-data-a-bibliometric-analysis/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/who-re-uses-data-a-bibliometric-analysis/","section":"curated_resources","summary":"Open data is receiving increased attention and support in academic environments, with one justification being that shared data may be re-used in further research. But what evidence exists for such re-use, and what is the relationship between the producers of shared datasets and researchers who use them? Using a sample of data citations from OpenAlex, this study investigates the relationship between creators and citers of datasets at the individual, institutional, and national levels. We find that the vast majority of datasets have no recorded citations, and that most cited datasets only have a single citation. Rates of self-citation by individuals and institutions tend towards the low end of previous findings and vary widely across disciplines. At the country level, the United States is by far the most prominent exporter of re-used datasets, while importation is more evenly distributed. Understanding where and how the sharing of data between researchers, institutions, and countries takes place is essential to developing open research practices.","tags":["Open Data","Citations"],"title":"Who Re-Uses Data? A Bibliometric Analysis of Dataset Citations","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1689122843,"objectID":"092302da1ab7f8fad666e833bea80927","permalink":"https://forrt.org/curated_resources/why-an-entire-field-of-psychology-is-in/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/why-an-entire-field-of-psychology-is-in/","section":"curated_resources","summary":"A video about psychology being trouble","tags":["Video"],"title":"Why an Entire Field of Psychology Is in Trouble","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"25b725b265a2f548ec5258bdce6825f8","permalink":"https://forrt.org/curated_resources/why-and-how-to-use-pre-analysis-plans/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/why-and-how-to-use-pre-analysis-plans/","section":"curated_resources","summary":"We describe what is a pre-analysis plan (PAP) and why you should use one. We emphasize the potential political uses of PAPs and, in particular, how the PAP is in this respect a uniquely powerful tool for increasing the likelihood that evidence informs policy-making.","tags":["Pre-analysis plans","Preregistration","Open Science","Motivated Reasoning","Policy"],"title":"Why and How to Use Pre-Analysis Plans","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"7eab0b778933918666e86e040d49009f","permalink":"https://forrt.org/curated_resources/why-hypothesis-testers-should-spend-less/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/why-hypothesis-testers-should-spend-less/","section":"curated_resources","summary":"For almost half a century, Paul Meehl educated psychologists about how the mindless use of null-hypothesis significance tests made research on theories in the social sciences basically uninterpretable. In response to the replication crisis, reforms in psychology have focused on formalizing procedures for testing hypotheses. These reforms were necessary and influential. However, as an unexpected consequence, psychological scientists have begun to realize that they may not be ready to test hypotheses. Forcing researchers to prematurely test hypotheses before they have established a sound “derivation chain” between test and theory is counterproductive. Instead, various nonconfirmatory research activities should be used to obtain the inputs necessary to make hypothesis tests informative. Before testing hypotheses, researchers should spend more time forming concepts, developing valid measures, establishing the causal relationships between concepts and the functional form of those relationships, and identifying boundary conditions and auxiliary assumptions. Providing these inputs should be recognized and incentivized as a crucial goal in itself. In this article, we discuss how shifting the focus to nonconfirmatory research can tie together many loose ends of psychology’s reform movement and help us to develop strong, testable theories, as Paul Meehl urged.","tags":["Exploratory Research","Hypothesis Testing","Replication Crisis"],"title":"Why Hypothesis Testers Should Spend Less Time Testing Hypotheses","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1689122843,"objectID":"31d02aa3bfb0964ba4d0f9c42e4d377a","permalink":"https://forrt.org/curated_resources/why-most-discovered-true-associations-ar/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/why-most-discovered-true-associations-ar/","section":"curated_resources","summary":"Newly discovered true (non-null) associations often have inflated effects compared with the true effect sizes. I discuss here the main reasons for this inflation. First, theoretical considerations prove that when true discovery is claimed based on crossing a threshold of statistical significance and the discovery study is underpowered, the observed effects are expected to be inflated. This has been demonstrated in various fields ranging from early stopped clinical trials to genome-wide associations. Second, flexible analyses coupled with selective reporting may inflate the published discovered effects. The vibration ratio (the ratio of the largest vs. smallest effect on the same association approached with different analytic choices) can be very large. Third, effects may be inflated at the stage of interpretation due to diverse conflicts of interest. Discovered effects are not always inflated, and under some circumstances may be deflated-for example, in the setting of late discovery of associations in sequentially accumulated overpowered evidence, in some types of misclassification from measurement error, and in conflicts causing reverse biases. Finally, I discuss potential approaches to this problem. These include being cautious about newly discovered effect sizes, considering some rational down-adjustment, using analytical methods that correct for the anticipated inflation, ignoring the magnitude of the effect (if not necessary), conducting large studies in the discovery phase, using strict protocols for analyses, pursuing complete and transparent reporting of all results, placing emphasis on replication, and being fair with interpretation of results","tags":["Reproducibility Crisis and Credibility Revolution","Open Science"],"title":"Why Most Discovered True Associations Are Inflated","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1689122843,"objectID":"90d95f95336aa8b6ccb82aee32658765","permalink":"https://forrt.org/curated_resources/why-most-of-psychology-is-statistically/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/why-most-of-psychology-is-statistically/","section":"curated_resources","summary":"Low power in experimental psychology is an oft-discussed problem. We show in the context of the Replicability Project: Psychology (Open Science Collaboration, 2015) that sample sizes are so small in psychology that often one cannot detect even large differences between studies. High-powered replications cannot answer this problem, because the power to find differences in results from a previous study is limited by the sample size in the original study. This is not simply a problem with replications; cumulative science, which critically depends on assessing differences between results published in the literature, is practically impossible with typical sample sizes in experimental psychology. We diagnose misconceptions about power and suggest a solution to increase the resolution of published results","tags":[""],"title":"Why most of psychology is statistically unfalsifiable","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1689122843,"objectID":"776ecb391e08f80072413aba2b8a4684","permalink":"https://forrt.org/curated_resources/why-most-published-research-findings-are/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/why-most-published-research-findings-are/","section":"curated_resources","summary":"There is increasing concern that most current published research findings are false. The probability that a research claim is true may depend on study power and bias, the number of other studies on the same question, and, importantly, the ratio of true to no relationships among the relationships probed in each scientific field. In this framework, a research finding is less likely to be true when the studies conducted in a field are smaller; when effect sizes are smaller; when there is a greater number and lesser preselection of tested relationships; where there is greater flexibility in designs, definitions, outcomes, and analytical modes; when there is greater financial and other interest and prejudice; and when more teams are involved in a scientific field in chase of statistical significance. Simulations show that for most study designs and settings, it is more likely for a research claim to be false than true. Moreover, for many current scientific fields, claimed research findings may often be simply accurate measures of the prevailing bias. In this essay, I discuss the implications of these problems for the conduct and interpretation of research.","tags":[""],"title":"Why most published research findings are false","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"8f84fd50815dca8d05269e67a01fabba","permalink":"https://forrt.org/curated_resources/why-nasa-and-federal-agencies-are-declar/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/why-nasa-and-federal-agencies-are-declar/","section":"curated_resources","summary":"I’m thrilled to be the Transform to Open Science lead for NASA, which has a 60-year legacy of pushing the limits of how science is used to understand the Universe, planetary systems and life on Earth. Much of NASA’s success can be attributed to a culture of openness for the public good. Since the 1990s, the agency has been a leading advocate for full and open access to data and algorithms.\n\nThat culture is needed now more than ever. Humanity is facing many intersecting challenges, from the COVID-19 pandemic to climate change and food and water insecurity. To combat them, we must find breakthroughs faster, increase interdisciplinary expertise and improve how we translate research findings into action. This will require a fundamental shift: from simply sharing results in journal articles to collaborating openly, publishing reproducible results and implementing full inclusivity and transparency.\n\nTo catalyse this shift, on 11 January the US White House — joined by 10 federal agencies, a coalition of more than 85 universities, and other organizations — declared 2023 to be the Year of Open Science.","tags":["Year of Open Science","NASA","Federal Research"],"title":"Why NASA and federal agencies are declaring this the Year of Open Science","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1689122843,"objectID":"9226546452a0fc0bd051607c852e2315","permalink":"https://forrt.org/curated_resources/why-p-048-should-be-rare-and-why-this-fe/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/why-p-048-should-be-rare-and-why-this-fe/","section":"curated_resources","summary":"This post discusses the why p value around .048 should be rare","tags":["Blog"],"title":"why p = .048 should be rare (and why this feels counterintuitive)","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"b5d17a40e91f06b1e820c4cbc942928b","permalink":"https://forrt.org/curated_resources/why-preregistration-makes-me-nervous/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/why-preregistration-makes-me-nervous/","section":"curated_resources","summary":"I must admit that when I first heard of the effort to get psychological scientists to preregister their studies (that is, to submit to a journal a study’s hypotheses and a plan for how the data will be analyzed before that study has been run), I had a moment of panic. It seemed, on the surface, entirely too regulated for my tastes. I have since calmed down and now see the usefulness of preregistration — indeed, APS has been at the forefront of encouraging preregistration to make our science more transparent and reliable. ","tags":["Preregistration","Badges"],"title":"Why Preregistration Makes Me Nervous","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1689122843,"objectID":"f76978b90537646c67a620a9421827c8","permalink":"https://forrt.org/curated_resources/why-psychologists-must-change-the-way-th/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/why-psychologists-must-change-the-way-th/","section":"curated_resources","summary":"Does psi exist? D. J. Bem (2011) conducted 9 studies with over 1,000 participants in an attempt to demonstrate that future events retroactively affect people's responses. Here we discuss several limitations of Bem's experiments on psi; in particular, we show that the data analysis was partly exploratory and that one-sided p values may overstate the statistical evidence against the null hypothesis. We reanalyze Bem's data with a default Bayesian t test and show that the evidence for psi is weak to nonexistent. We argue that in order to convince a skeptical audience of a controversial claim, one needs to conduct strictly confirmatory studies and analyze the results with statistical tests that are conservative rather than liberal. We conclude that Bem's p values do not indicate evidence in favor of precognition; instead, they indicate that experimental psychologists need to change the way they conduct their experiments and analyze their data","tags":["Reproducibility Crisis and Credibility Revolution","Open Science"],"title":"Why psychologists must change the way they analyze their data: The case of psi: Comment on Bem (2011).","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1689122843,"objectID":"95d5488384640e2292d096103e40b184","permalink":"https://forrt.org/curated_resources/why-psychologists-food-fight-matters/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/why-psychologists-food-fight-matters/","section":"curated_resources","summary":"“Important findings” haven’t been replicated, and science may have to change its ways.","tags":["Blog","Open Science","Reproducibility Crisis and Credibility Revolution"],"title":"Why Psychologists’ Food Fight Matters","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1689122843,"objectID":"1a464224a477b6388aa8d691b9a699c8","permalink":"https://forrt.org/curated_resources/why-summaries-of-research-on-psychologic/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/why-summaries-of-research-on-psychologic/","section":"curated_resources","summary":"Null hypothesis testing of correlational predictions from weak substantive theories in soft psychology is subject to the influence of ten obfuscating factors whose effects are usually (1) sizeable, (2) opposed, (3) variable, and (4) unknown. The net epistemic effect of these ten obfuscating influences is that the usual research literature review is well-nigh uninterpretable. Major changes in graduate education, conduct of research, and editorial policy are proposed.","tags":["Reproducibility Crisis and Credibility Revolution","Open Science"],"title":"Why Summaries of Research on Psychological Theories are Often Uninterpretable","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"74ee318e98be3d8e6e6a2bd1faf91849","permalink":"https://forrt.org/curated_resources/wide-open-accelerating-public-data-relea/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/wide-open-accelerating-public-data-relea/","section":"curated_resources","summary":"Open data is a vital pillar of open science and a key enabler for reproducibility, data reuse, and novel discoveries. Enforcement of open-data policies, however, largely relies on manual efforts, which invariably lag behind the increasingly automated generation of biological data. To address this problem, we developed a general approach to automatically identify datasets overdue for public release by applying text mining to identify dataset references in published articles and parse query results from repositories to determine if the datasets remain private. We demonstrate the effectiveness of this approach on 2 popular National Center for Biotechnology Information (NCBI) repositories: Gene Expression Omnibus (GEO) and Sequence Read Archive (SRA). Our Wide-Open system identified a large number of overdue datasets, which spurred administrators to respond directly by releasing 400 datasets in one week.","tags":["Archives","Biotechnology","Data","Data Mining","Gene Expression","Open Data","Sequence Databases","Text Mining","Web-based Applications"],"title":"Wide-Open: Accelerating public data release by automating detection of overdue datasets","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1689122843,"objectID":"98e50e06e9d207bb8d1f9952333fca10","permalink":"https://forrt.org/curated_resources/willingness-to-share-research-data-is-re/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/willingness-to-share-research-data-is-re/","section":"curated_resources","summary":"Background: The widespread reluctance to share published research data is often hypothesized to be due to the authors’ fear that reanalysis may expose errors in their work or may produce conclusions that contradict their own. However, these hypotheses have not previously been studied systematically. Methods and Findings: We related the reluctance to share research data for reanalysis to 1148 statistically significant results reported in 49 papers published in two major psychology journals. We found the reluctance to share data to be associated with weaker evidence (against the null hypothesis of no effect) and a higher prevalence of apparent errors in the reporting of statistical results. The unwillingness to share data was particularly clear when reporting errors had a bearing on statistical significance. Conclusions: Our findings on the basis of psychological papers suggest that statistical results are particularly hard to verify when reanalysis is more likely to lead to contrasting conclusions. This highlights the importance of establishing mandatory data archiving policies.","tags":[""],"title":"Willingness to share research data is related to the strength of the evidence and the quality of reporting of statistical results","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"7c3d531090758d14952075608bdd3b0d","permalink":"https://forrt.org/curated_resources/workflow-for-awarding-badges/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/workflow-for-awarding-badges/","section":"curated_resources","summary":"Badges are a great way to signal that a journal values transparent research practices. Readers see the papers that have underlying data or methods available, colleagues see that norms are changing within a community and have ample opportunities to emulate better practices, and authors get recognition for taking a step into new techniques. In this webinar, Professor Stephen Lindsay of University of Victoria discusses the workflow of a badging program, eligibility for badge issuance, and the pitfalls to avoid in launching a badging program. Visit cos.io/badges to learn more.","tags":["Analysis","Data","Education","Materials","Open Scholarship Tools and Technologies","OSF","Publishing","Reproducibility","Research Data Management Tools","Researchers"],"title":"Workflow for Awarding Badges","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"d8b9563609133bdec395f8919569bffb","permalink":"https://forrt.org/curated_resources/writing-a-data-management-plan-for-grant/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/writing-a-data-management-plan-for-grant/","section":"curated_resources","summary":"A class covering the basics of writing a successful data management plan for federal funding agencies such as the NEH, NSF, NIH, NASA, and others.","tags":["Data","Open Scholarship Guidelines","Research Data Management","Researchers"],"title":"Writing a Data Management Plan for Grant Applications","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"76fb2133e50932a9cdfe0d828d7b37f9","permalink":"https://forrt.org/curated_resources/writing-reproducible-geoscience-papers-u/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/writing-reproducible-geoscience-papers-u/","section":"curated_resources","summary":"Reproducibility is unquestionably at the heart of science. Scientists face numerous challenges in this context, not least the lack of concepts, tools, and workflows for reproducible research in today's curricula.This short course introduces established and powerful tools that enable reproducibility of computational geoscientific research, statistical analyses, and visualisation of results using R (http://www.r-project.org/) in two lessons:1. Reproducible Research with R MarkdownOpen Data, Open Source, Open Reviews and Open Science are important aspects of science today. In the first lesson, basic motivations and concepts for reproducible research touching on these topics are briefly introduced. During a hands-on session the course participants write R Markdown (http://rmarkdown.rstudio.com/) documents, which include text and code and can be compiled to static documents (e.g. HTML, PDF).R Markdown is equally well suited for day-to-day digital notebooks as it is for scientific publications when using publisher templates.2. GitLab and DockerIn the second lesson, the R Markdown files are published and enriched on an online collaboration platform. Participants learn how to save and version documents using GitLab (http://gitlab.com/) and compile them using Docker containers (https://docker.com/). These containers capture the full computational environment and can be transported, executed, examined, shared and archived. Furthermore, GitLab's collaboration features are explored as an environment for Open Science.Prerequisites: Participants should install required software (R, RStudio, a current browser) and register on GitLab (https://gitlab.com) before the course.This short course is especially relevant for early career scientists (ECS).Participants are welcome to bring their own data and R scripts to work with during the course.All material by the conveners will be shared publicly via OSF (https://osf.io/qd9nf/).","tags":["Analysis","Docker","Geosciences","GitLab","Open Scholarship Tools and Technologies","Publishing","R","Reproducibility","Research Data Management Tools","Researchers","RMarkdown","RStudio"],"title":"Writing reproducible geoscience papers using R Markdown, Docker, and GitLab","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1689122843,"objectID":"1b03e7e32a421d273201c66a71bc49d4","permalink":"https://forrt.org/curated_resources/you-are-not-so-smart/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/you-are-not-so-smart/","section":"curated_resources","summary":"Psychology is working on the hardest problems in all of science. Physics, astronomy, geology — those are easy, by comparison. Understanding consciousness, willpower, ideology, social change – there’s a larger-than-Large-Hadron-Collider level of difficulty to each one of these, but since these are more relatable ideas than quarks and bosons and mass coronal ejections — this a science about our minds and selves — it’s easier to create eye-catching headlines and, well, to make podcasts about them. This is the problem. Because the system for distributing the findings of science is based on publication within journals, which themselves are often depend on the interest of the general media, all the biases that come with that system and media consumption in general are now causing the sciences that are most interesting to the public to get tainted by that interest. As you will hear in this episode, one of the most famous and most talked-about phenomena in recent psychological history, ego depletion, hasn’t been doing so well in replication attempts. In the show, journalist Daniel Engber who wrote an article for Slate about the failure to replicate many of the famous ego depletion experiments will detail what this means for the science and the scientists involved. Also, you’ll hear from psychologist Brain Nosek, who says, “Science is wrong about everything, but you can trust it more than anything.” Nosek is director of the Center for Open Science, an organization working to correct what they see as the temporarily wayward path of psychology. Nosek recently lead a project in which 270 scientists sought to replicate 100 different studies in psychology, all published in 2008 — 97 of which claimed to have found significant results — and in the end, two-thirds failed to replicate. Clearly, some sort of course correction is in order. There is now a massive effort underway sort out what is being called the replication crisis. Much of the most headline-producing research in the last 20 years isn’t standing up to attempts to reproduce its findings. Nosek wants to clean up the processes that have lead to this situation, and in this episode, you’ll learn how he and others plan to do so.","tags":["Podcast","Reproducibility Crisis and Credibility Revolution"],"title":"You are not so smart","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1689122843,"objectID":"c241da241d193633e7e94a7127ce5aa3","permalink":"https://forrt.org/curated_resources/you-cannot-step-into-the-same-river-twic/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/you-cannot-step-into-the-same-river-twic/","section":"curated_resources","summary":"Statistical power depends on the size of the effect of interest. However, effect sizes are rarely fixed in psychological research: Study design choices, such as the operationalization of the dependent variable or the treatment manipulation, the social context, the subject pool, or the time of day, typically cause systematic variation in the effect size. Ignoring this between-study variation, as standard power formulae do, results in assessments of power that are too optimistic. Consequently, when researchers attempting replication set sample sizes using these formulae, their studies will be underpowered and will thus fail at a greater than expected rate. We illustrate this with both hypothetical examples and data on several well-studied phenomena in psychology. We provide formulae that account for between-study variation and suggest that researchers set sample sizes with respect to our generally more conservative formulae. Our formulae generalize to settings in which there are multiple effects of interest. We also introduce an easy-to-use website that implements our approach to setting sample sizes. Finally, we conclude with recommendations for quantifying between-study variation.","tags":[""],"title":"You Cannot Step Into the Same River Twice: When Power Analyses Are Optimistic","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"ce21bfd058ea5c2933043395741b7e1b","permalink":"https://forrt.org/curated_resources/your-questions-answered-how-to-retain-co/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/curated_resources/your-questions-answered-how-to-retain-co/","section":"curated_resources","summary":"In this webinar, a panel discusses licensing options, fundamentals in choosing a license for your research, and answers questions about licensing scholarship. The panel consists of moderator Joanna Schimizzi, Professional Learning Specialist at the Institute for the Study of Knowledge Management in Education, along with panelists Brandon Butler, Director of Information Policy, University of Virginia Library and Becca Neel, Assistant Director for Resource Management \u0026 User Experience, University of Southern Indiana for an informative discussion on licensing your research. Accessible and further resources for this event are available on OSF: https://osf.io/s4wdf/","tags":["Licensing","Open Science","OSF","Scholarly Communication","Scholarship"],"title":"Your Questions Answered: How to Retain Copyright While Others Distribute and Build Upon Your Work","type":"curated_resources"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1721390377,"objectID":"54fcc70de0242f493d3a951246df8b7a","permalink":"https://forrt.org/glossary/english/z_curve/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/english/z_curve/","section":"glossary","summary":"","tags":null,"title":"Z-Curve","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1635961284,"objectID":"7b1252f39a647b9084395163032e0a1b","permalink":"https://forrt.org/glossary/vbeta/z-curve/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/vbeta/z-curve/","section":"glossary","summary":"","tags":null,"title":"Z-Curve","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1720865851,"objectID":"9d9aed2e9871c1569ddce53fa1fddd9d","permalink":"https://forrt.org/glossary/german/z_curve/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/german/z_curve/","section":"glossary","summary":"","tags":null,"title":"Z-Curve (Z-Kurve)","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1721390377,"objectID":"86368efba3e4a713917b6d0ae8d091d9","permalink":"https://forrt.org/glossary/english/zenodo/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/english/zenodo/","section":"glossary","summary":"","tags":null,"title":"Zenodo","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1720865851,"objectID":"3c07cc9a74cc9596ecf3042a0db261b8","permalink":"https://forrt.org/glossary/german/zenodo/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/german/zenodo/","section":"glossary","summary":"","tags":null,"title":"Zenodo","type":"glossary"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1635961284,"objectID":"978bdc092b89f65a83915ed3be76152d","permalink":"https://forrt.org/glossary/vbeta/zenodo/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/glossary/vbeta/zenodo/","section":"glossary","summary":"","tags":null,"title":"Zenodo ","type":"glossary"}]