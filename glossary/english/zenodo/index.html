<!doctype html><html lang=en-us><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=generator content="Source Themes Academic 4.8.0"><link rel=stylesheet href=/academicons/css/academicons.min.css><meta name=description content='An open science repository where researchers can deposit research papers, reports, data sets, research software, and any other research-related digital artifacts. Zenodo creates a persistent digital object identifier (DOI) for each submission to make it '><link rel=alternate hreflang=en-us href=https://forrt.org/glossary/english/zenodo/><meta name=theme-color content="#004055"><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.7.2/css/all.min.css integrity="sha512-Evv84Mr4kqVGRNSgIGL/F/aIDqQb7xQ2vcrdIwxfjThSH8CSR7PBEakCr51Ck+w+/U6swU2Im1vVX0SVk9ABhg==" crossorigin=anonymous><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/fancybox/3.5.7/jquery.fancybox.min.css integrity="sha256-Vzbj7sDDS/woiFS3uNKo8eIuni59rjyNGtXfstRzStA=" crossorigin=anonymous><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.2/styles/github.min.css crossorigin=anonymous title=hl-light><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.2/styles/dracula.min.css crossorigin=anonymous title=hl-dark disabled><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/leaflet/1.5.1/leaflet.css integrity="sha256-SHMGCYmST46SoyGgo4YR/9AlK1vf3ff84Aq9yK4hdqM=" crossorigin=anonymous><script src=https://cdnjs.cloudflare.com/ajax/libs/lazysizes/5.1.2/lazysizes.min.js integrity="sha256-Md1qLToewPeKjfAHU1zyPwOutccPAm5tahnaw7Osw0A=" crossorigin=anonymous async></script><link rel=stylesheet href="https://fonts.googleapis.com/css?family=Domine:400,400italic,700&display=swap" type=text/css><link rel=stylesheet href=/css/academic.css><script async src="https://www.googletagmanager.com/gtag/js?id=G-2BTFVS6SPM"></script><script>window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-2BTFVS6SPM")</script><link rel=manifest href=/index.webmanifest><link rel=icon type=image/png href=/images/icon_hu63989008d5fc2bc2d6ffdc2b032b1215_17560_32x32_fill_q100_h2_lanczos_center_2.webp><link rel=apple-touch-icon type=image/png href=/images/icon_hu63989008d5fc2bc2d6ffdc2b032b1215_17560_192x192_fill_q100_h2_lanczos_center_2.webp><link rel=canonical href=https://forrt.org/glossary/english/zenodo/><meta property="twitter:card" content="summary_large_image"><meta property="twitter:site" content="@FORRTproject"><meta property="twitter:creator" content="@FORRTproject"><meta property="og:site_name" content="FORRT - Framework for Open and Reproducible Research Training"><meta property="og:url" content="https://forrt.org/glossary/english/zenodo/"><meta property="og:title" content="Zenodo"><meta property="og:description" content='An open science repository where researchers can deposit research papers, reports, data sets, research software, and any other research-related digital artifacts. Zenodo creates a persistent digital object identifier (DOI) for each submission to make it '><meta property="og:image" content="https://forrt.org/img/FORRT_banner.svg"><meta property="twitter:image" content="https://forrt.org/img/FORRT_banner.svg"><meta property="og:locale" content="en-us"><meta property="article:modified_time" content="2025-02-07T18:11:11+08:00"><script src=https://cdnjs.cloudflare.com/ajax/libs/cookieconsent2/3.1.1/cookieconsent.min.js integrity="sha256-5VhCqFam2Cn+yjw61zbBNrbHVJ6SRydPeKopYlngbiQ=" crossorigin=anonymous></script><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/cookieconsent2/3.1.1/cookieconsent.min.css integrity="sha256-zQ0LblD/Af8vOppw18+2anxsuaz3pWYyVWi+bTvTH8Q=" crossorigin=anonymous><script>window.addEventListener("load",function(){window.cookieconsent.initialise({palette:{popup:{background:"#004055",text:"#fefdf6"},button:{background:"#fefdf6",text:"#004055"}},theme:"classic",content:{message:"This website uses cookies to ensure you get the best experience on our website.",dismiss:"Got it!",link:"Learn more",href:"https://www.cookiesandyou.com"}})})</script><title>Zenodo | FORRT - Framework for Open and Reproducible Research Training</title></head><body id=top data-spy=scroll data-offset=70 data-target=#TableOfContents><script>const isSiteThemeDark=!1</script><script src=/js/load-theme.js></script><aside class=search-results id=search><div class=container><section class=search-header><div class="row no-gutters justify-content-between mb-3"><div class=col-6><h1>Search</h1></div><div class="col-6 col-search-close"><a class=js-search href=#><i class="fas fa-times-circle text-muted" aria-hidden=true></i></a></div></div><div id=search-box><label for=search-query class=sr-only>Search: </label><input name=q id=search-query placeholder=Search... autocapitalize=off autocomplete=off autocorrect=off spellcheck=false type=search></div></section><section class=section-search-results><div id=search-hits></div></section></div></aside><nav class="navbar navbar-expand-lg navbar-light compensate-for-scrollbar" id=navbar-main><div class=container><div class="d-none d-lg-inline-flex"><a class=navbar-brand href=/><img src=/images/logo.svg alt="FORRT - Framework for Open and Reproducible Research Training Logo. Link to homepage"></a></div><button type=button class=navbar-toggler data-toggle=collapse data-target=#navbar-content aria-controls=navbar aria-expanded=false aria-label="Toggle navigation">
<span><i class="fas fa-bars"></i></span></button><div class="navbar-brand-mobile-wrapper d-inline-flex d-lg-none"><a class=navbar-brand href=/><img src=/images/logo.svg alt="FORRT - Framework for Open and Reproducible Research Training"></a></div><div class="navbar-collapse main-menu-item collapse justify-content-center" id=navbar-content><ul class="navbar-nav d-md-inline-flex font-weight-bold text-uppercase"><li class="nav-item dropdown px-2"><a href=javascript:void(0) class="nav-link dropdown-toggle" data-toggle=dropdown aria-haspopup=true aria-expanded=false><span>About FORRT</span><span class=caret></span></a><div class=dropdown-menu><a class=dropdown-item href=/about/us><span>About FORRT</span></a>
<a class=dropdown-item href=/awards><span>Awards</span></a>
<a class=dropdown-item href=/about/get-involved/#calendar><span>Calendar</span></a>
<a class=dropdown-item href=/about/cite_us/><span>Cite us</span></a>
<a class=dropdown-item href=/about/charity/><span>Charity</span></a>
<a class=dropdown-item href=/coc><span>Code of Conduct</span></a>
<a class=dropdown-item href=/about/community><span>Community</span></a>
<a class=dropdown-item href=/community-map><span>Community Map</span></a>
<a class=dropdown-item href=/contributors><span>Contributors</span></a>
<a class=dropdown-item href=/cv><span>Curriculum Vitae</span></a>
<a class=dropdown-item href=/feedback><span>Feedback</span></a>
<a class=dropdown-item href=/about/get-involved><span>Get Involved</span></a>
<a class=dropdown-item href=/about/mission/><span>Mission & Advocacy</span></a>
<a class=dropdown-item href=/newsletters><span>Newsletters</span></a>
<a class=dropdown-item href=/about/partnerships/><span>Partners</span></a>
<a class=dropdown-item href=/about/principles/><span>Principles</span></a>
<a class=dropdown-item href=/talk/><span>Talks</span></a>
<a class=dropdown-item href=/about/teams/><span>Teams</span></a>
<a class=dropdown-item href=/testimonials/><span>Testimonials</span></a>
<a class=dropdown-item href=/about/visitors><span>Visitors</span></a></div></li><li class="nav-item dropdown px-2"><a href=javascript:void(0) class="nav-link dropdown-toggle" data-toggle=dropdown aria-haspopup=true aria-expanded=false><span>Educational NEXUS</span><span class=caret></span></a><div class=dropdown-menu><a class=dropdown-item href=/nexus><span>About Educational Nexus</span></a>
<a class=dropdown-item href=/adopting><span>Adopting Principled Education</span></a>
<a class=dropdown-item href=/citation-politics><span>Citation Politics</span></a>
<a class=dropdown-item href=/resources><span>Curated Resources</span></a>
<a class=dropdown-item href=/os-developing-world><span>Developing Countries & OS</span></a>
<a class=dropdown-item href=/educators-corner><span>Educators' Corner</span></a>
<a class=dropdown-item href=/equityinos><span>Equity in Open Science</span></a>
<a class=dropdown-item href=/clusters><span>Clusters</span></a>
<a class=dropdown-item href=/glossary><span>Glossary</span></a>
<a class=dropdown-item href=/impact><span>Impact of OS on students</span></a>
<a class=dropdown-item href=/lesson-plans/><span>Lesson Plans</span></a>
<a class=dropdown-item href=/mapping_os><span>Mapping OS Communities</span></a>
<a class=dropdown-item href=/neurodiversity><span>Neurodiversity Team</span></a>
<a class=dropdown-item href=/pedagogies><span>Pedagogies</span></a>
<a class=dropdown-item href=/self-assessment><span>Self-Assessment</span></a>
<a class=dropdown-item href=/dei><span>Social Justice Initiatives</span></a>
<a class=dropdown-item href=/summaries><span>Summaries</span></a>
<a class=dropdown-item href=/syllabus><span>Syllabi</span></a>
<a class=dropdown-item href=/teaching_os><span>Teaching OS</span></a>
<a class=dropdown-item href=/awop><span>Wheel of Privilege</span></a></div></li><li class="nav-item dropdown px-2"><a href=javascript:void(0) class="nav-link dropdown-toggle" data-toggle=dropdown aria-haspopup=true aria-expanded=false><span>Replication Hub</span><span class=caret></span></a><div class=dropdown-menu><a class=dropdown-item href=/replication-hub><span>About Replication Hub</span></a>
<a class=dropdown-item href=/apps/fred_annotator.html target=_blank rel=noopener><span>FReD Annotator</span></a>
<a class=dropdown-item href=/apps/fred_explorer.html target=_blank rel=noopener><span>FReD Explorer</span></a>
<a class=dropdown-item href=/FReD/index.html target=_blank rel=noopener><span>FReD R-Package</span></a>
<a class=dropdown-item href=/replication-hub/large-scale-replication-projects><span>Large-Scale Replication Projects</span></a>
<a class=dropdown-item href=/open-social-psychology/ target=_blank rel=noopener><span>Open Social Psychology by Rima-Maria Rahal</span></a>
<a class=dropdown-item href=/positive-changes-replication-crisis><span>Positive Changes from the Replication Crisis</span></a>
<a class=dropdown-item href=/reversals><span>Replications & Reversals</span></a>
<a class=dropdown-item href=https://replicationresearch.org target=_blank rel=noopener><span>Replication Research Journal</span></a>
<a class=dropdown-item href=https://osf.io/brxtd/ target=_blank rel=noopener><span>Replication Manuscript Templates</span></a>
<a class=dropdown-item href=/replication-hub/submit><span>Submit a replication to FReD</span></a></div></li><li class="nav-item px-2"><a class=nav-link href=/publications/><span>Publications</span></a></li><li class=nav-item><a href=/about/get-involved/ class="nav-link btn btn-primary nav-btn">🚀&nbsp;Get&nbsp;involved!</a></li></ul></div><ul class="nav-icons navbar-nav flex-row ml-auto d-flex pl-md-2"><li class=nav-item><a class="nav-link js-search" href=# alt="Opens search bar"><i class="fas fa-search" aria-hidden=true></i></a></li><li class=nav-item><a class=nav-link href=https://github.com/forrtproject/forrt alt="Link to FORRT's GitHub repository"><i class="fab fa-github" aria-hidden=true></i></a></li></ul></div></nav><style>.nav-link.active{font-weight:700;color:#007bff}.language-switcher{margin-top:20px}.language-switcher ul{list-style:disc;margin-left:20px;padding-left:0}.language-switcher li{margin-bottom:5px}.language-switcher a{text-decoration:none;color:#007bff;font-weight:700}.language-switcher a:hover{text-decoration:underline}</style><script>document.addEventListener("DOMContentLoaded",function(){const t=document.querySelectorAll("#TableOfContents .nav-link"),n=document.querySelectorAll("#TableOfContentsMobile .nav-link"),e=window.location.href;t.forEach(t=>{t.href===e&&(t.classList.add("active"),t.scrollIntoView({behavior:"auto",block:"start"}),window.scrollBy(0,-document.querySelector(".docs-toc").offsetHeight))}),n.forEach(t=>{t.href===e&&t.classList.add("active")})})</script><div class="container-fluid docs"><div class="row flex-xl-nowrap"><div class="d-none d-xl-block col-xl-3 docs-toc" style=order:1><ul class="nav toc-top"><li><a href=# id=back_to_top class=docs-toc-title>List of terms</a></li></ul><nav id=TableOfContents class="nav flex-column"><ul><li class=nav-item data-toggle=tooltip data-placement=right title="The tendency to report only significant results in the abstract, while reporting non-significant results within the main body of the manuscript (not reporting non-significant results altogether would constitute selective reporting). The consequence of abstract bias is that studies reporting non-significant results may not be captured with standard meta-analytic search procedures (which rely on information in the title, abstract and keywords) and thus biasing the results of meta-analyses."><a href=https://forrt.org/glossary/english/abstract_bias/ class=nav-link>Abstract Bias</a></li><li class=nav-item data-toggle=tooltip data-placement=right title="The contribution that a research output (e.g., published manuscript) makes in shifting understanding and advancing scientific theory, method, and application, across and within disciplines. Impact can also refer to the degree to which an output or research programme influences change outside of academia, e.g. societal and economic impact (cf. ESRC: https://esrc.ukri.org/research/impact-toolkit/what-is-impact/)."><a href=https://forrt.org/glossary/english/academic_impact/ class=nav-link>Academic Impact</a></li><li class=nav-item data-toggle=tooltip data-placement=right title="Accessibility refers to the ease of access and re-use of materials (e.g., data, code, outputs, publications) for academic purposes, particularly the ease of access is afforded to people with a chronic illness, disability and/or neurodivergence. These groups face numerous financial, legal and/or technical barriers within research, including (but not limited to) the acquisition of appropriately formatted materials and physical access to spaces. Accessibility also encompasses structural concerns about diversity, equity, inclusion, and representation (Pownall et al., 2021). Interfaces, events and spaces should be designed with accessibility in mind to ensure full participation, such as by ensuring that web-based images are colorblind friendly and have alternative text, or by using live captions at events (Brown et al., 2018; Pollet & Bond, 2021; World Wide Web Consortium, 2021)."><a href=https://forrt.org/glossary/english/accessibility/ class=nav-link>Accessibility</a></li><li class=nav-item data-toggle=tooltip data-placement=right title="From Latin meaning “to the person”; Judgment of an argument or piece of work influenced by the characteristics of the person who forwarded it, not the characteristics of the argument itself. Ad hominem bias can be negative, as when work from a competitor or target of personal animosity is viewed more critically than the quality of the work merits, or positive, as when work from a friend benefits from overly favorable evaluation."><a href=https://forrt.org/glossary/english/ad_hominem_bias/ class=nav-link>Ad hominem bias</a></li><li class=nav-item data-toggle=tooltip data-placement=right title="A commentary in which the original authors of a work and critics of said work collaborate to draft a consensus statement. The aim is to draft a commentary that is free of ad hominem attacks and communicates a common understanding or at least identifies where both parties agree and disagree. In doing so, it provides a clear take-home message and path forward, rather than leaving the reader to decide between opposing views conveyed in separate commentaries."><a href=https://forrt.org/glossary/english/adversarial/ class=nav-link>Adversarial (collaborative) commentary</a></li><li class=nav-item data-toggle=tooltip data-placement=right title="A collaboration where two or more researchers with opposing or contradictory theoretical views —and likely diverging predictions about study results— work together on one project. The aim is to minimise biases and methodological weaknesses as well as to establish a shared base of facts for which competing theories must account."><a href=https://forrt.org/glossary/english/adversarial_collaboration/ class=nav-link>Adversarial collaboration</a></li><li class=nav-item data-toggle=tooltip data-placement=right title="This bias occurs when one’s opinions or judgements about the quality of research are influenced by the affiliation of the author(s). When publishing manuscripts, a potential example of an affiliation bias could be when editors prefer to publish work from prestigious institutions (Tvina et al., 2019)."><a href=https://forrt.org/glossary/english/affiliation_bias/ class=nav-link>Affiliation bias</a></li><li class=nav-item data-toggle=tooltip data-placement=right title="Variability in outcomes due to unknowable or inherently random factors. The stochastic component of outcome uncertainty that cannot be reduced through additional sources of information. For example, when flipping a coin, uncertainty about whether it will land on heads or tails."><a href=https://forrt.org/glossary/english/aleatoric_uncertainty/ class=nav-link>Aleatoric uncertainty</a></li><li class=nav-item data-toggle=tooltip data-placement=right title="Departing from traditional citation measures, altmetrics (short for “alternative metrics”) provide an assessment of the attention and broader impact of research work based on diverse sources such as social media (e.g. Twitter), digital news media, number of preprint downloads, etc. Altmetrics have been criticized in that sensational claims usually receive more attention than serious research (Ali, 2021)."><a href=https://forrt.org/glossary/english/altmetrics/ class=nav-link>Altmetrics</a></li><li class=nav-item data-toggle=tooltip data-placement=right title="AMNESIA is a free anonymization tool to remove identifying information from data. After uploading a dataset that contains personal data, the original dataset is transformed by the tool, resulting in a dataset that is anonymized regarding personal and sensitive data."><a href=https://forrt.org/glossary/english/amnesia/ class=nav-link>AMNESIA</a></li><li class=nav-item data-toggle=tooltip data-placement=right title="Analytic flexibility is a type of researcher degrees of freedom (Simmons, Nelson, & Simonsohn, 2011\) that refers specifically to the large number of choices made during data preprocessing and statistical analysis. “\[T\]he range of analysis outcomes across different acceptable analysis methods” (Carp, 2012, p. 1). Analytic flexibility can be problematic, as this variability in analytic strategies can translate into variability in research outcomes, particularly when several strategies are applied, but not transparently reported (Masur, 2021)."><a href=https://forrt.org/glossary/english/analytic_flexibility/ class=nav-link>Analytic Flexibility</a></li><li class=nav-item data-toggle=tooltip data-placement=right title="Anonymising data refers to removing, generalising, aggregating or distorting any information which may potentially identify participants, peer-reviewers, and authors, among others. Data should be anonymised so that participants are not personally identifiable. The most basic level of anonymisation is to replace participants’ names with pseudonyms (fake names) and remove references to specific places. Anonymity is particularly important for open data and data may not be made open for anonymity concerns. Anonymity and open data has been discussed within qualitative research which often focuses on personal experiences and opinions, and in quantitative research that includes participants from clinical populations."><a href=https://forrt.org/glossary/english/anonymity/ class=nav-link>Anonymity</a></li><li class=nav-item data-toggle=tooltip data-placement=right title="The ARRIVE guidelines (Animal Research: Reporting of In Vivo Experiments) are a checklist-based set of reporting guidelines developed to improve reporting standards, and enhance replicability, within living (i.e. *in vivo*) animal research. The second generation ARRIVE guidelines, ARRIVE 2.0, were released in 2020\. In these new guidelines, the clarity has been improved, items have been prioritised and new information has been added with an accompanying “Explanation” and “Elaboration” document to provide a rationale for each item and a recommended set to add context to the study being described."><a href=https://forrt.org/glossary/english/arrive_guidelines/ class=nav-link>ARRIVE Guidelines</a></li><li class=nav-item data-toggle=tooltip data-placement=right title="An article (sometimes author) processing charge (APC) is a fee charged to authors by a publisher in exchange for publishing and hosting an open access article. APCs are often intended to compensate for a potential loss of revenue the journal may experience when moving from traditional publication models, such as subscription services or pay-per-view, to open access. While some journals charge only about US$300, APCs vary widely, from US$1000 (Advances in Methods and Practice in Psychological Science) or less to over US$10,000 (Nature). While some publishers offer waivers for researchers from certain regions of the world or who lack funds, some APCs have been criticized for being disproportionate compared to actual processing and hosting costs (Grossmann & Brembs, 2021\) and for creating possible inequities with regard to which scientists can afford to make their works freely available (Smith et al. 2020)."><a href=https://forrt.org/glossary/english/article_processing_charge/ class=nav-link>Article Processing Charge (APC)</a></li><li class=nav-item data-toggle=tooltip data-placement=right title="Authorship assigns credit for research outputs (e.g. manuscripts, data, and software) and accountability for content (McNutt et al. 2018; Patience et al. 2019). Conventions differ across disciplines, cultures, and even research groups, in their expectations of what efforts earn authorship, what the order of authorship signifies (if anything), how much accountability for the research the corresponding author assumes, and the extent to which authors are accountable for aspects of the work that they did not personally conduct."><a href=https://forrt.org/glossary/english/authorship/ class=nav-link>Authorship</a></li><li class=nav-item data-toggle=tooltip data-placement=right title="All theories contain assumptions about the nature of constructs and how they can be measured. However, not all predictions are derived from theories and assumptions can sometimes be drawn from other premises. Additional assumptions that are made to deduce a prediction and tested by making links to observable data. These auxiliary hypotheses are sometimes invoked to explain why a replication attempt has failed."><a href=https://forrt.org/glossary/english/auxiliary_hypothesis/ class=nav-link>Auxiliary Hypothesis</a></li><li class=nav-item data-toggle=tooltip data-placement=right title="Badges are symbols that editorial teams add to published manuscripts to acknowledge open science practices and act as incentives for researchers to share data, materials, or to embed study preregistration. As clearly-visible symbols, they are intended to signal to the reader that content has met the standard of open research required to receive the badge (typically from that journal). Different badges may be assigned for different practices, such as research having been made available and accessible in a persistent location (“open material badge” and “open data badge”), or study preregistration (“preregistration badge”)."><a href=https://forrt.org/glossary/english/badges/ class=nav-link>Badges (Open Science)</a></li><li class=nav-item data-toggle=tooltip data-placement=right title="A continuous statistical measure for model selection used in Bayesian inference, describing the *relative* evidence for one model over another, regardless of whether the models are correct. Bayes factors (BF) range from 0 to infinity, indicating the relative strength of the evidence, and where 1 is a neutral point of no evidence. In contrast to *p*\-values, Bayes factors allow for 3 types of conclusions: a) evidence for the alternative hypothesis, b) evidence for the null hypothesis, and c) no sufficient evidence for either. Thus, BF are typically expressed as BF10 for evidence regarding the alternative compared to the null hypothesis, and as BF01 for evidence regarding the null compared to the alternative hypothesis."><a href=https://forrt.org/glossary/english/bayes_factor/ class=nav-link>Bayes Factor</a></li><li class=nav-item data-toggle=tooltip data-placement=right title="A method of statistical inference based upon Bayes’ theorem, which makes use of epistemological (un)certainty using the mathematical language of probability. Bayesian inference is based on allocating (and reallocating, based on newly-observed data or evidence) credibility across possibilities. Two existing approaches to Bayesian inference include “Bayes factors” (BF) and Bayesian parameter estimation."><a href=https://forrt.org/glossary/english/bayesian_inference/ class=nav-link>Bayesian Inference</a></li><li class=nav-item data-toggle=tooltip data-placement=right title="A Bayesian approach to estimating parameter values by updating a prior belief about model parameters (i.e., prior distribution) with new evidence (i.e., observed data) via a likelihood function, resulting in a posterior distribution. The posterior distribution may be summarised in a number of ways including: point estimates (mean/mode/median of a posterior probability distribution), intervals of defined boundaries, and intervals of defined mass (typically referred to as a credible interval). In turn, a posterior distribution may become a prior distribution in a subsequent estimation. A posterior distribution can also be sampled using Monte-Carlo Markov Chain methods which can be used to determine complex model uncertainties (e.g. Foreman-Mackey et al., 2013)."><a href=https://forrt.org/glossary/english/bayesian_parameter_estimation/ class=nav-link>Bayesian Parameter Estimation</a></li><li class=nav-item data-toggle=tooltip data-placement=right title="The Brain Imaging Data Structure (BIDS) describes a simple and easy-to-adopt way of organizing neuroimaging, electrophysiological, and behavioral data (i.e., file formats, folder structures). BIDS is a community effort developed *by* the community *for* the community and was inspired by the format used internally by the OpenfMRI repository known as [OpenNeuro](https://openneuro.org). Having initially been developed for fMRI data, the BIDS data structure has been extended for many other measures, such as EEG (Pernet et al., 2019)."><a href=https://forrt.org/glossary/english/bids_data_structure/ class=nav-link>BIDS data structure</a></li><li class=nav-item data-toggle=tooltip data-placement=right title="This acronym refers to Barren, Institutional, Zoo, and other Rare Rearing Environments (BIZARRE). Most research for chimpanzees is conducted on this specific sample. This limits the generalizability of a large number of research findings in the chimpanzee population. The BIZARRE has been argued to reflect the universal concept of what is a chimpanzee (see also WEIRD, which has been argued to be a universal concept for what is a human)."><a href=https://forrt.org/glossary/english/bizarre/ class=nav-link>BIZARRE</a></li><li class=nav-item data-toggle=tooltip data-placement=right title='Within academic culture, an approach focusing on the intrinsic interest of academics to improve the quality of research and research culture, for instance by making it supportive, collaborative, creative and inclusive. Usually indicates leadership from early-career researchers acting as the changemakers driving shifts and change in scientific methodology through enthusiasm and innovation, compared to a “top-down” approach initiated by more senior researchers "Bottom-up approaches take into account the specific local circumstances of the case itself, often using empirical data, lived experience, personal accounts, and circumstances as the starting point for developing policy solutions."'><a href=https://forrt.org/glossary/english/bottom_up_approach/ class=nav-link>Bottom-up approach (to Open Scholarship)</a></li><li class=nav-item data-toggle=tooltip data-placement=right title="Bracketing interviews are commonly used within qualitative approaches. During these interviews researchers explore their personal subjectivities and assumptions surrounding their ongoing research. This allows researchers to be aware of their own interests and helps them to become both more reflective and critical about their research, considering how their own experiences may impact the research process. Bracketing interviews can also be subject to qualitative analysis."><a href=https://forrt.org/glossary/english/bracketing_interviews/ class=nav-link>Bracketing Interviews</a></li><li class=nav-item data-toggle=tooltip data-placement=right title="A tongue-in-cheek expression intended to raise awareness of the lack of diverse voices in open science (Bahlai, Bartlett, Burgio et al. 2019; Onie, 2020), in addition to the presence of behavior and communication styles that can be toxic or exclusionary. Importantly, not all bros are men; rather, they are individuals who demonstrate rigid thinking, lack self-awareness, and tend towards hostility, unkindness, and exclusion (Pownall et al., 2021; Whitaker & Guest, 2020). They generally belong to dominant groups who benefit from structural privileges. To address \#bropenscience, researchers should examine and address structural inequalities within academic systems and institutions."><a href=https://forrt.org/glossary/english/bropenscience/ class=nav-link>Bropenscience</a></li><li class=nav-item data-toggle=tooltip data-placement=right title="Critiquing After the Results are Known (CARKing) refers to presenting a criticism of a design as one that you would have made in advance of the results being known. It usually forms a reaction or criticism to unwelcome or unfavourable results, results whether the critic is conscious of this fact or not."><a href=https://forrt.org/glossary/english/carking/ class=nav-link>CARKing</a></li><li class=nav-item data-toggle=tooltip data-placement=right title="A non-profit technology organization based in Charlottesville, Virginia with the mission “to increase openness, integrity, and reproducibility of research.” Among other resources, the COS hosts the Open Science Framework (OSF) and the Open Scholarship Knowledge Base."><a href=https://forrt.org/glossary/english/center_for_open_science/ class=nav-link>Center for Open Science (COS)</a></li><li class=nav-item data-toggle=tooltip data-placement=right title="A biased selection of papers or authors cited and included in the references section. When citation bias is present, it is often in a way which would benefit the author(s) or reviewers, over-represents statistically significant studies, or reflects pervasive gender or racial biases (Brooks, 1985; Jannot et al., 2013; Zurn et al., 2020). One proposed solution is the use of Citation Diversity Statements, in which authors reflect on their citation practices and identify biases which may have emerged (Zurn et al., 2020)."><a href=https://forrt.org/glossary/english/citation_bias/ class=nav-link>Citation bias</a></li><li class=nav-item data-toggle=tooltip data-placement=right title="A current effort trying to increase awareness and mitigate the citation bias in relation to gender and race is the Citation Diversity Statement, a short paragraph where “the authors consider their own bias and quantify the equitability of their reference lists. It states: (i) the importance of citation diversity, (ii) the percentage breakdown (or other diversity indicators) of citations in the paper, (iii) the method by which percentages were assessed and its limitations, and (iv) a commitment to improving equitable practices in science” (Zurn et al., 2020, p. 669)."><a href=https://forrt.org/glossary/english/citation_diversity_statement/ class=nav-link>Citation Diversity Statement</a></li><li class=nav-item data-toggle=tooltip data-placement=right title="Citizen science refers to projects that actively involve the general public in the scientific endeavour, with the goal of democratizing science. Citizen scientists can be involved in all stages of research, acting as collaborators, contributors or project leaders. An example of a major citizen science project involved individuals identifying astronomical bodies (Lintott, 2008)."><a href=https://forrt.org/glossary/english/citizen_science/ class=nav-link>Citizen Science</a></li><li class=nav-item data-toggle=tooltip data-placement=right title="The Comprehensive Knowledge Archive Network (CKAN) is an open-source data platform and free software that aims to provide tools to streamline publishing and data sharing. CKAN supports governments, research institutions and other organizations in managing and publishing large amounts of data."><a href=https://forrt.org/glossary/english/ckan/ class=nav-link>CKAN</a></li><li class=nav-item data-toggle=tooltip data-placement=right title="An approach to research where stakeholders who are not traditionally involved in the research process are empowered to collaborate, either at the start of the project or throughout the research lifecycle. For example, co-produced health research may involve health professionals and patients, while co-produced education research may involve teaching staff and pupils/students. This is motivated by principles such as respecting and valuing the experiences of non-researchers, addressing power dynamics, and building mutually beneficial relationships."><a href=https://forrt.org/glossary/english/co_production/ class=nav-link>Co-production</a></li><li class=nav-item data-toggle=tooltip data-placement=right title="A framework which identifies best practices for scientific repositories and evaluation criteria for these practices. Its flexible and multidimensional approach means that it can be applied to different types of repositories, including those which host publications or data, across geographical and thematic contexts."><a href=https://forrt.org/glossary/english/coar_community_framework_for_good_practices_in_repositories/ class=nav-link>COAR Community Framework for Good Practices in Repositories</a></li><li class=nav-item data-toggle=tooltip data-placement=right title="The process of checking another researcher's programming (specifically, computer source code) including but not limited to statistical code and data modelling. This process is designed to detect and resolve mistakes, thereby improving code quality. In practice, a modern peer review process may take place via a hosted online repository such as GitHub, GitLab or SourceForge.Related terms: Reproducibility; Version control"><a href=https://forrt.org/glossary/english/code_review/ class=nav-link>Code review</a></li><li class=nav-item data-toggle=tooltip data-placement=right title="A codebook is a high-level summary that describes the contents, structure, nature and layout of a data set. A well-documented codebook contains information intended to be complete and self-explanatory for each variable in a data file, such as the wording and coding of the item, and the underlying construct. It provides transparency to researchers who may be unfamiliar with the data but wish to reproduce analyses or reuse the data."><a href=https://forrt.org/glossary/english/codebook/ class=nav-link>Codebook</a></li><li class=nav-item data-toggle=tooltip data-placement=right title="The Collaborative Replication and Education Project (CREP) is an initiative designed to organize and structure replication efforts of highly-cited empirical studies in psychology to satisfy the dual needs for more high-quality direct replications and more training in empirical research techniques for psychology students. CREP aims to address the need for replications of highly cited studies, and to provide training, support and professional growth opportunities for academics completing replication projects."><a href=https://forrt.org/glossary/english/collaborative_replication_and_education_project/ class=nav-link>Collaborative Replication and Education Project (CREP)</a></li><li class=nav-item data-toggle=tooltip data-placement=right title="The Organization for Human Brain Mapping (OHBM) neuroimaging community has developed a guideline for best practices in neuroimaging data acquisition, analysis, reporting, and sharing of both data and analysis code. It contains eight elements that should be included when writing up or submitting a manuscript in order to improve reporting methods and the resulting neuroimages in order to optimize transparency and reproducibility. Alternative definition: (if applicable) Checklist for data analysis and sharing"><a href=https://forrt.org/glossary/english/committee_on_best_practices_in_data_analysis_and_sharing/ class=nav-link>Committee on Best Practices in Data Analysis and Sharing (COBIDAS)</a></li><li class=nav-item data-toggle=tooltip data-placement=right title="The common ownership of scientific results and methods and the consequent imperative to share both freely. Communality is based on the fact that every scientific finding is seen as a product of the effort of a number of agents. This norm is followed when scientists openly share their new findings with colleagues."><a href=https://forrt.org/glossary/english/communality/ class=nav-link>Communality</a></li><li class=nav-item data-toggle=tooltip data-placement=right title="Collaborative projects that involve researchers from different career levels, disciplines, institutions or countries. Projects may have different goals including peer support and learning, conducting research, teaching and education. They can be short-term (e.g., conference events or hackathons) or long-term (e.g., journal clubs or consortium-led research projects). Collaborative culture and community building are key to achieving project goals."><a href=https://forrt.org/glossary/english/community_projects/ class=nav-link>Community Projects</a></li><li class=nav-item data-toggle=tooltip data-placement=right title="A collection of files prepared by a researcher to support a report or publication that include the data, metadata, programming code, software dependencies, licenses, and other instructions necessary for another researcher to independently reproduce the findings presented in the report or publication."><a href=https://forrt.org/glossary/english/compendium/ class=nav-link>Compendium</a></li><li class=nav-item data-toggle=tooltip data-placement=right title="Ability to recreate the same results as the original study (including tables, figures, and quantitative findings), using the same input data, computational methods, and conditions of analysis. The availability of code and data facilitates computational reproducibility, as does preparation of these materials (annotating data, delineating software versions used, sharing computational environments, etc). Ideally, computational reproducibility should be achievable by another second researcher (or the original researcher, at a future time), using only a set of files and written instructions. Also referred to as analytic reproducibility (LeBel et al., 2018)."><a href=https://forrt.org/glossary/english/computational_reproducibility/ class=nav-link>Computational reproducibility</a></li><li class=nav-item data-toggle=tooltip data-placement=right title="A replication attempt whereby the primary effect of interest is the same but tested in a different sample and captured in a different way to that originally reported (i.e., using different operationalisations, data processing and statistical approaches and/or different constructs; LeBel et al., 2018). The purpose of a conceptual replication is often to explore what conditions limit the extent to which an effect can be observed and generalised (e.g., only within certain contexts, with certain samples, using certain measurement approaches) towards evaluating and advancing theory (Hüffmeier et al., 2016)."><a href=https://forrt.org/glossary/english/conceptual_replication/ class=nav-link>Conceptual replication</a></li><li class=nav-item data-toggle=tooltip data-placement=right title="The tendency to seek out, interpret, favor and recall information in a way that supports one’s prior values, beliefs, expectations, or hypothesis."><a href=https://forrt.org/glossary/english/confirmation_bias/ class=nav-link>Confirmation bias</a></li><li class=nav-item data-toggle=tooltip data-placement=right title="Part of the confirmatory-exploratory distinction (Wagenmakers et al., 2012), where confirmatory analyses refer to analyses that were set *a priori* and test existent hypotheses. The lack of this distinction within published research findings has been suggested to explain replicability issues and is suggested to be overcome through study preregistration which clearly distinguishes confirmatory from exploratory analyses. Other researchers have questioned these terms and recommended a replacement with ‘discovery-oriented’ and ‘theory-testing research’ (Oberauer & Lewandowsky, 2019; see also Szollosi & Donkin, 2019)."><a href=https://forrt.org/glossary/english/confirmatory_analyses/ class=nav-link>Confirmatory analyses</a></li><li class=nav-item data-toggle=tooltip data-placement=right title="A conflict of interest (COI, also ‘competing interest’) is a financial or non-financial relationship, activity or other interest that might compromise objectivity or professional judgement on the part of an author, reviewer, editor, or editorial staff. The *Principles of Transparency and Best Practice in Scholarly Publishing* by the Committee on Publication Ethics (COPE), the Directory of Open Access Journals (DOAJ), the Open Access Scholarly Publishers Association (OASPA), and the World Association of Medical Editors (WAME) states that journals should have policies on publication ethics, including policies on COI (DOAJ, 2018). COIs should be made transparent so that readers can properly evaluate research and assess for potential or actual bias(es). Outside publishing, academic presenters, panel members and educators should also declare COIs. Purposeful failure to disclose a COI may be considered a form of misconduct."><a href=https://forrt.org/glossary/english/conflict_of_interest/ class=nav-link>Conflict of interest</a></li><li class=nav-item data-toggle=tooltip data-placement=right title="Only the name of the consortium or organization appears in the author column, and the individuals' names do not appear in the literature: For example, ‘FORRT’ as an author. This can be seen in the products of collaborative projects with a very large number of collaborators and/or contributors. Depending on the journal policy, individual researchers may be recorded as one of the authors of the product in literature databases such as ORCID and Scopus. Consortium authorship can also be termed group, corporate, organisation/organization or collective authorship (e.g. [https://www.bmj.com/about-bmj/resources-authors/article-submission/authorship-contributorship](https://www.bmj.com/about-bmj/resources-authors/article-submission/authorship-contributorship)), or collaborative authorship (e.g. [https://support.jmir.org/hc/en-us/articles/115001449591-What-is-a-group-author-collaborative-author-and-does-it-need-an-ORCID](https://support.jmir.org/hc/en-us/articles/115001449591-What-is-a-group-author-collaborative-author-and-does-it-need-an-ORCID))"><a href=https://forrt.org/glossary/english/consortium_authorship/ class=nav-link>Consortium authorship</a></li><li class=nav-item data-toggle=tooltip data-placement=right title="A statement that explicitly identifies and justifies the target population, and conditions, for the reported findings. Researchers should be explicit about potential boundary conditions for their generalisations (Simons et al., 2017). Researchers should provide detailed descriptions of the sampled population and/or contextual factors that might have affected the results such that future replication attempts can take these factors into account (Brandt et al., 2014). Conditions not explicitly listed are assumed not to have theoretical relevance to the replicability of the effect."><a href=https://forrt.org/glossary/english/constraints_on_generality/ class=nav-link>Constraints on Generality (COG)</a></li><li class=nav-item data-toggle=tooltip data-placement=right title="When used in the context of measurement and testing, construct validity refers to the degree to which a test measures what it claims to be measuring. In fields that study hypothetical unobservable entities, construct validation is essentially theory testing, because it involves determining whether an objective measure (a questionnaire, lab task, etc.) is a valid representation of a hypothetical construct (i.e., conforms to a theory). When used in a broader sense about a research study, or a claim, conclusion, or observed effect in a research study, construct validity concerns the extent to which the sampling particulars used in the study (participants, settings, treatments, and dependent variables) map onto the higher order constructs the study, the claim, the conclusion is about. According to Shadish et al. (2002), construct validity can be defined as “the degree to which inferences are warranted from the observed persons, settings, and cause and effect operations included in a study to the constructs that these instances might represent” (p. 38)."><a href=https://forrt.org/glossary/english/construct_validity/ class=nav-link>Construct validity</a></li><li class=nav-item data-toggle=tooltip data-placement=right title="The degree to which a measurement includes all aspects of the concept that the researcher claims to measure; “A qualitative type of validity where the domain of the concept is made clear and the analyst judges whether the measures fully represent the domain” (Bollen, 1989, p.185). It is a component of *construct validity* and can be established using both quantitative and qualitative methods, often involving expert assessment."><a href=https://forrt.org/glossary/english/content_validity/ class=nav-link>Content validity</a></li><li class=nav-item data-toggle=tooltip data-placement=right title><a href=https://forrt.org/glossary/english/contribution/ class=nav-link>Contribution</a></li><li class=nav-item data-toggle=tooltip data-placement=right title="A corrigendum (pl. corrigenda, Latin: 'to correct') documents one or multiple errors within a published work that do not alter the central claim or conclusions and thus does not rise to the standard of requiring a retraction of the work. Corrigenda are typically available alongside the original work to aid transparency. Some publishers refer to this document as an erratum (pl. errata, Latin: 'error'), while others draw a distinction between the two (corrigenda as author-errors and errata as publisher-errors)."><a href=https://forrt.org/glossary/english/corrigendum/ class=nav-link>Corrigendum</a></li><li class=nav-item data-toggle=tooltip data-placement=right title="A set of free and easy-to-use copyright licences that define the rights of the authors and users of open data and materials in a standardized way. CC licenses enable authors or creators to share copyright-law-protected work with the public and come in different varieties with more or less clauses. For example, the Attribution-NonCommercial-ShareAlike 4.0 International (CC BY-NC-SA 4.0) allows you to share and adapt the material, under the condition that you; give credit to the original creators, indicate if changes were made, and share under the same license as the original, and you cannot use the material for commercial purposes."><a href=https://forrt.org/glossary/english/creative_commons/ class=nav-link>Creative Commons (CC) license</a></li><li class=nav-item data-toggle=tooltip data-placement=right title="Replication efforts should seek not just to support or question the original findings, but also to replace them with revised, stronger theories with greater explanatory power. This approach therefore involves ‘pruning’ existing theories, comparing all the alternative theories, and making replication efforts more generative and engaged in theory-building (Tierney et al. 2020, 2021)."><a href=https://forrt.org/glossary/english/creative_destruction_approach_to_replication/ class=nav-link>Creative destruction approach to replication</a></li><li class=nav-item data-toggle=tooltip data-placement=right title="The problems and the solutions resulting from a growing distrust in scientific findings, following concerns about the credibility of scientific claims (e.g., low replicability). The term has been proposed as a more positive alternative to the term replicability crisis, and includes the many solutions to improve the credibility of research, such as preregistration, transparency, and replication."><a href=https://forrt.org/glossary/english/credibility_revolution/ class=nav-link>Credibility revolution</a></li><li class=nav-item data-toggle=tooltip data-placement=right title="The Contributor Roles Taxonomy (CRediT; [https://casrai.org/credit/](https://casrai.org/credit/)) is a high-level taxonomy used to indicate the roles typically adopted by contributors to scientific scholarly output. There are currently 14 roles that describe each contributor’s specific contribution to the scholarly output. They can be assigned multiple times to different authors and one author can also be assigned multiple roles. CRediT includes the following roles: Conceptualization, Data curation, Formal Analysis, Funding acquisition, Investigation, Methodology, Project administration, Resources, Software, Supervision, Validation, Visualization, Writing – original draft, Writing – review & editing. A description of the different roles can be found in the work of Brand et al., (2015)."><a href=https://forrt.org/glossary/english/credit/ class=nav-link>CRediT</a></li><li class=nav-item data-toggle=tooltip data-placement=right title="The degree to which a measure corresponds to other valid measures of the same concept. Criterion validity is usually established by calculating regression coefficients or bivariate correlations estimating the direction and strength of relation between test measure and criterion measure. It is often confused with *construct validity* although it differs from it in intent (merely predictive rather than theoretical) and interest (predicting an observable outcome rather than a latent construct). Unreliability in either test or criterion scores usually diminishes criterion validity. Also called criterion-related or concrete validity."><a href=https://forrt.org/glossary/english/criterion_validity/ class=nav-link>Criterion validity</a></li><li class=nav-item data-toggle=tooltip data-placement=right title="Crowdsourced research is a model of the social organisation of research as a large-scale collaboration in which one or more research projects are conducted by multiple teams in an independent yet coordinated manner. Crowdsourced research aims at achieving efficiency and scalability gains by pooling resources, promoting transparency and social inclusion, as well as increasing the rigor, reliability, and trustworthiness by enhancing statistical power and mutual social vetting. It stands in contrast to the traditional model of academic research production, which is dominated by the independent work of individual or small groups of researchers (‘small science’). Examples of crowdsourced research include so-called ‘many labs replication’ studies (Klein et al., 2018), ‘many analysts, one dataset’ studies (Silberzahn et al., 2018), distributive collaborative networks (Moshontz et al., 2018\) and open collaborative writing projects such as Massively Open Online Papers (MOOPs) (Himmelstein et al., 2019; Tennant et al., 2019). Alternatively, crowdsourced research can refer to the use of a large number of research “crowdworkers” in data collection hired through online labor markets like Amazon Mechanical Turk or Prolific, for example in content analysis (Benoit et al., 2016; Lind et al., 2017\) or experimental research (Peer et al., 2017). Crowdsourced research that is both open for participation and open through shared intermediate outputs has been referred to as *crowd science* (Franzoni & Sauermann, 2014)."><a href=https://forrt.org/glossary/english/crowdsourced_research/ class=nav-link>Crowdsourced Research</a></li><li class=nav-item data-toggle=tooltip data-placement=right title="The additional labor expected or demanded of members of underrepresented or marginalized minority groups, particularly scholars of color. This labor often comes from service roles providing ethnic, cultural, or gender representation and diversity. These roles can be formal or informal, and are generally unrewarded or uncompensated. Such labor includes providing expertise on matters of diversity, educating members of majority groups, acting as a liaison to minority communities, and formal and informal roles as mentor and support system for minority students."><a href=https://forrt.org/glossary/english/cultural_taxation/ class=nav-link>Cultural taxation</a></li><li class=nav-item data-toggle=tooltip data-placement=right title="Goal of any empirical science, it is the pursuit of “the construction of a cumulative base of knowledge upon which the future of the science may be built” (Curran, 2009, p. 1). The idea that science will create more complete and accurate theories as a function of the amount of evidence and data that has been collected. Cumulative science develops in gradual and incremental steps, as opposed to one abrupt discovery. While revolutionary science occurs scarcely, cumulative science is the most common form of science."><a href=https://forrt.org/glossary/english/cumulative_science/ class=nav-link>Cumulative science</a></li><li class=nav-item data-toggle=tooltip data-placement=right title="Data Access and Research Transparency ([DA-RT](https://www.dartstatement.org/)) is an initiative aimed at increasing data access and research transparency in the social sciences. It is a multi-epistemic and multi-method initiative, created in 2014 by the Council of the American Political Science Association (APSA), to bolster the rigor of empirical social inquiry. In addition to other activities, DA-RT developed the Journal Editors' Transparency Statement (JETS), which requires subscribing journals to (a) making relevant data publicly available if the study is published, (b) following a strict data citation policy, (c) transparently describing the analytical procedures and, if possible, providing public access to analytical code, and (d) updating their journal style guides, codes of ethics to include improved data access and research transparency requirements."><a href=https://forrt.org/glossary/english/data_access_and_research_transparency/ class=nav-link>Data Access and Research Transparency (DA-RT)</a></li><li class=nav-item data-toggle=tooltip data-placement=right title="A structured document that describes the process of data acquisition, analysis, management and storage during a research project. It also describes data ownership and how the data will be preserved and shared during and upon completion of a project. Data management templates also provide guidance on how to make research data FAIR and where possible, openly available."><a href=https://forrt.org/glossary/english/data_management_plan/ class=nav-link>Data management plan (DMP)</a></li><li class=nav-item data-toggle=tooltip data-placement=right title="collection of practices, technologies, cultural elements and legal frameworks that are relevant to the practice of making data used for scholarly research available to other investigators. Gollwitzer et al. (2020) describe two types of data sharing: Type 1: Data that is necessary to reproduce the findings of a published research article. Type 2: data that have been collected in a research project but have not (or only partly) been analysed or reported after the completion of the project and are hence typically shared under a specified embargo period."><a href=https://forrt.org/glossary/english/data_sharing/ class=nav-link>Data sharing</a></li><li class=nav-item data-toggle=tooltip data-placement=right title="Graphical representation of data or information. Data visualisation takes advantage of humans’ well-developed visual processing capacity to convey insight and communicate key information. Data visualisations often display the raw data, descriptive statistics, and/or inferential statistics."><a href=https://forrt.org/glossary/english/data_visualisation/ class=nav-link>Data visualisation</a></li><li class=nav-item data-toggle=tooltip data-placement=right title="Coloniality can be described as the naturalisation of concepts such as imperialism, capitalism, and nationalism. Together these concepts can be thought of as a matrix of power (and power relations) that can be traced to the colonial period. Decoloniality seeks to break down and decentralize those power relations, with the aim to understand their persistence and to reconstruct the norms and values of a given domain. In an academic setting, decolonisation refers to the rethinking of the lens through which we teach, research, and co-exist, so that the lens generalises beyond Western-centred and colonial perspectives. Decolonising academia involves reconstructing the historical and cultural frameworks being used, redistributing a sense of belonging in universities, and empowering and including voices and knowledge types that have historically been excluded from academia. This is done when people engage with their past, present, and future whilst holding a perspective that is separate from the socially dominant perspective. Also, by including, not rejecting, an individuals’ internalised norms and taboos from the specific colony."><a href=https://forrt.org/glossary/english/decolonisation/ class=nav-link>Decolonisation</a></li><li class=nav-item data-toggle=tooltip data-placement=right title="A criterion for distinguishing science from non-science which aims to indicate an optimal way for knowledge of the world to grow. In a Popperian approach, the demarcation criterion was falsifiability and the application of a falsificationist attitude. Alternative approaches include that of Kuhn, who believed that the criterion was puzzle solving with the aim of understanding nature, and Lakatos, who argued that science is marked by working within a progressive research programme."><a href=https://forrt.org/glossary/english/demarcation_criterion/ class=nav-link>Demarcation criterion</a></li><li class=nav-item data-toggle=tooltip data-placement=right title="As ‘direct replication’ does not have a widely-agreed technical meaning nor there is no clear cut distinction between a direct and conceptual replication, below we list several contributions towards a consensus. Rather than debating the ‘exactness’ of a replication, it is more helpful to discuss the relevant differences between a replication and its target, and their implications for the reliability and generality of the target’s results. Generally, direct replication refers to a new data collection that attempts to replicate original studies’ methods as closely as possible. A replication attempt that “seek(s) to duplicate the necessary elements that produced the original finding.” (Cruwell et al., 2019; p.243). The purpose of a direct replication can be to identify type 1 errors and/or experimenter effects, determine the replicability of an effect using the same or improved practices, or to create more specific estimates of effect size (Hűffmeier et al., 2016). Directness of replication is a continuum between repeating specific observations (data) and observing generalised effects (phenomena). How closely a replication replicates an original study is often a matter for debate, often with differences being cited as hidden moderators of effects. Furthermore, there can be debate over the relevant importance of technical equivalence (i.e., using identical materials) versus psychological equivalence (i.e., realizing the identical psychological conditions) to the original study (Schwarz and Strack, 2014). For example, consider a study on Trust in the US- President conducted in 2018\. A technical equivalent replication would use Trump as stimulus (he was president in 2018\) a psychological equivalent study would use Biden (he is the current president)."><a href=https://forrt.org/glossary/english/direct_replication/ class=nav-link>Direct replication</a></li><li class=nav-item data-toggle=tooltip data-placement=right title="Diversity refers to between-person (i.e., interindividual) variation in humans, e.g. ability, age, beliefs, cognition, country, disability, ethnicity, gender, language, race, religion or sexual orientation. Diversity can refer to diversity of researchers (who do the research), the diversity of participant samples (who is included in the study), and diversity of perspectives (the views and beliefs researchers bring into their work; Syed & Kathawalla, 2020)."><a href=https://forrt.org/glossary/english/diversity/ class=nav-link>Diversity</a></li><li class=nav-item data-toggle=tooltip data-placement=right title="Digital Object Identifiers (DOI) are alpha-numeric strings that can be assigned to any entity, including: publications (including preprints), materials, datasets, and feature films \- the use of DOIs is not restricted to just scholarly or academic material. DOIs “provides a system for persistent and actionable identification and interoperable exchange of managed information on digital networks.” ([https://doi.org/hb.html](https://doi.org/hb.html)). There are many different DOI registration agencies that operate DOIs, but the two that researchers would most likely encounter are [Crossref](https://www.crossref.org/) and [Datacite](https://datacite.org/)."><a href=https://forrt.org/glossary/english/doi/ class=nav-link>DOI (digital object identifier)</a></li><li class=nav-item data-toggle=tooltip data-placement=right title="The San Francisco Declaration on Research Assessment (DORA) is a global initiative aiming to reduce dependence on journal-based metrics (e.g. journal impact factor and citation counts) and, instead, promote a culture which emphasises the intrinsic value of research. The DORA declaration targets research funders, publishers, research institutes and researchers and signing it represents a commitment to aligning research practices and procedures with the declaration’s principles."><a href=https://forrt.org/glossary/english/dora/ class=nav-link>DORA</a></li><li class=nav-item data-toggle=tooltip data-placement=right title="An identity confusion, as the individual feels like they have two distinct identities. One is to assimilate to the dominant culture at university when the individual is with colleagues and professors, while the other is when the individual is with their families. This continuous shift may cause a lack of certainty about the individual’s identity and a belief that the individual does not fully belong anywhere. This lack of belonging can lead to poor social integration within the academic culture that can manifest in less opportunities and more mental health issues in the individual (Rubin, 2021; Rubin et al., 2019)."><a href=https://forrt.org/glossary/english/double_consciousness/ class=nav-link>Double consciousness</a></li><li class=nav-item data-toggle=tooltip data-placement=right title="Evaluation of research products by qualified experts where both the author(s) and reviewer(s) are anonymous to each other. “This approach conceals the identity of the authors and their affiliations from reviewers and would, in theory, remove biases of professional reputation, gender, race, and institutional affiliation, allowing the reviewer to avoid bias and to focus on the manuscript’s merit alone.” (Tvina et al., 2019, 1082). Like all types of peer-review, double-blind peer review is not without flaws. Anonymity can be difficult, if not impossible, to achieve for certain researchers working in a niche area."><a href=https://forrt.org/glossary/english/double_blind_peer_review/ class=nav-link>Double-blind peer review</a></li><li class=nav-item data-toggle=tooltip data-placement=right title="A label given to researchers who “range from senior doctoral students to postdoctoral workers who may have up to 10 years postdoctoral education; the latter group may therefore include early career or junior academics” (Eley et al., 2012, p. 3). What specifically (e.g. age, time since PhD inclusive or exclusive of career breaks and leave, title, funding awarded) constitutes an ECR can vary across funding bodies, academic organisations, and countries."><a href=https://forrt.org/glossary/english/early_career_researchers/ class=nav-link>Early career researchers (ECRs)</a></li><li class=nav-item data-toggle=tooltip data-placement=right title="The contribution a research item makes to the broader economy and society. It also captures the benefits of research to individuals, organisations, and/or nations."><a href=https://forrt.org/glossary/english/economic_and_societal_impact/ class=nav-link>Economic and societal impact</a></li><li class=nav-item data-toggle=tooltip data-placement=right title="Applied to Open Scholarship, in academic publishing, the period of time after an article has been published and before it can be made available as Open Access. If an author decides to self-archive their article (e.g., in an Open Access repository) they need to observe any embargo period a publisher might have in place. Embargo periods vary from instantaneous up to 48 months, with 6 and 12 months being common (Laakso & Björk, 2013). Embargo periods may also apply to pre-registrations, materials, and data, when authors decide to only make these available to the public after a certain period of time, for instance upon publication or even later when they have additional publication plans and want to avoid being scooped (Klein et al., 2018)."><a href=https://forrt.org/glossary/english/embargo_period/ class=nav-link>Embargo Period</a></li><li class=nav-item data-toggle=tooltip data-placement=right title="Systematic uncertainty due to limited data, measurement precision, model or process specification, or lack of knowledge. That is, uncertainty due to lack of knowledge that could, in theory, be reduced through conducting additional research to increase understanding. Such uncertainty is said to be personal, since knowledge differs across scientists, and temporary since it can change as new data become available."><a href=https://forrt.org/glossary/english/epistemic_uncertainty/ class=nav-link>Epistemic uncertainty</a></li><li class=nav-item data-toggle=tooltip data-placement=right title="Alongside ethics, logic, and metaphysics, epistemology is one of the four main branches of philosophy. Epistemology is largely concerned with nature, origin, and scope of knowledge, as well as the rationality of beliefs."><a href=https://forrt.org/glossary/english/epistemology/ class=nav-link>Epistemology</a></li><li class=nav-item data-toggle=tooltip data-placement=right title="Different individuals have different starting positions (cf. “opportunity gaps”) and needs. Whereas equal treatment focuses on treating all individuals *equally*, equitable treatment aims to level the playing field by actively increasing opportunities for under-represented minorities. Equitable treatment aims to attain equality through “fairness”: taking into account different needs for support for different individuals, instead of focusing merely on the needs of the majority."><a href=https://forrt.org/glossary/english/equity/ class=nav-link>Equity</a></li><li class=nav-item data-toggle=tooltip data-placement=right title="Equivalence tests statistically assess the null hypothesis that a given effect exceeds a minimum criterion to be considered meaningful. Thus, rejection of the null hypothesis provides evidence of a lack of (meaningful) effect. Based upon frequentist statistics, equivalence tests work by specifying equivalence bounds: a lower and upper value that reflect the smallest effect size of interest. Two one-sided *t*\-tests are then conducted against each of these equivalence bounds to assess whether effects that are deemed meaningful can be *rejected* (see Schuirmann, 1972; Lakens et al., 2018; 2020)."><a href=https://forrt.org/glossary/english/equivalence_testing/ class=nav-link>Equivalence Testing</a></li><li class=nav-item data-toggle=tooltip data-placement=right title="Broadly refers to examining research data and manuscripts for mistakes or inconsistencies in reporting. Commonly discussed approaches include: checking inconsistencies in descriptive statistics (e.g. summary statistics that are not possible given the sample size and measure characteristics; Brown & Heathers, 2017; Heathers et al. 2018), inconsistencies in reported statistics (e.g. *p*\-values that do not match the reported *F* statistics and accompanying degrees of freedom; Epskamp, & Nuijten, 2016; Nuijten et al. 2016), and image manipulation (Bik et al., 2016). Error detection is one motivation for data and analysis code to be openly available, so that peer review can confirm a manuscript’s findings, or if already published, the record can be corrected. Detected errors can result in corrections or retractions of published articles, though these actions are often delayed, long after erroneous findings have influenced and impacted further research."><a href=https://forrt.org/glossary/english/error_detection/ class=nav-link>Error detection</a></li><li class=nav-item data-toggle=tooltip data-placement=right title="This is a type of research method which aims to draw general conclusions to address a research question on a certain topic, phenomenon or effect by reviewing research outcomes and information from a range of different sources. Information which is subject to synthesis can be extracted from both qualitative and quantitative studies. The method used to synthesise the gathered information can be qualitative (narrative synthesis), quantitative (meta-analysis) or mixed (meta-synthesis, systematic mapping). Evidence synthesis has many applications and is often used in the context of healthcare, public policy as well as understanding and advancement of specific research fields."><a href=https://forrt.org/glossary/english/evidence_synthesis/ class=nav-link>Evidence Synthesis</a></li><li class=nav-item data-toggle=tooltip data-placement=right title="Exploratory Data Analysis (EDA) is a well-established statistical tradition that provides conceptual and computational tools for discovering patterns in data to foster hypothesis development and refinement. These tools and attitudes complement the use of hypothesis tests used in confirmatory data analysis (CDA). Even when well-specified theories are held, EDA helps one interpret the results of CDA and may reveal unexpected or misleading patterns in the data."><a href=https://forrt.org/glossary/english/exploratory_data_analysis/ class=nav-link>Exploratory data analysis</a></li><li class=nav-item data-toggle=tooltip data-placement=right title="Whether the findings of a scientific study can be generalized to other contexts outside the study context (different measures, settings, people, places, and times). Statistically, threats to external validity may reflect interactions whereby the effect of one factor (the independent variable) depends on another factor (a confounding variable). External validity may also be limited by the study design (e.g., an artificial laboratory setting or a non-representative sample)."><a href=https://forrt.org/glossary/english/external_validity/ class=nav-link>External Validity</a></li><li class=nav-item data-toggle=tooltip data-placement=right title="A subjective judgement of how suitable a measure appears to be on the surface, that is, how well a measure is operationalized. For example, judging whether questionnaire items should relate to a construct of interest at face value. Face validity is related to construct validity, but since it is subjective/informal, it is considered an easy but weak form of validity."><a href=https://forrt.org/glossary/english/face_validity/ class=nav-link>Face validity</a></li><li class=nav-item data-toggle=tooltip data-placement=right title="Describes making scholarly materials Findable, Accessible, Interoperable and Reusable (FAIR). ‘Findable’ and ‘Accessible’ are concerned with where materials are stored (e.g. in data repositories), while ‘Interoperable’ and ‘Reusable’ focus on the importance of data formats and how such formats might change in the future."><a href=https://forrt.org/glossary/english/fair_principles/ class=nav-link>FAIR principles</a></li><li class=nav-item data-toggle=tooltip data-placement=right title="With a particular focus on gender and sexuality, feminist psychology is inherently concerned with representation, diversity, inclusion, accessibility, and equality. Feminist psychology initially grew out out of a concern for representing the lived experiences of girls and women, but has since evolved into a more nuanced, intersectional and comprehensive concern for all aspects of equality (e.g., Eagly & Riger, 2014). Feminist psychologists have advocated for more rigorous consideration of equality, diversity, and inclusion within Open Science spaces (Pownall et al., 2021)."><a href=https://forrt.org/glossary/english/feminist_psychology/ class=nav-link>Feminist psychology</a></li><li class=nav-item data-toggle=tooltip data-placement=right title="An authorship system that assigns the order of authorship depending on the contributions of a given author while simultaneously valuing the first and last position of the authorship order most. According to this system, the two main authors are indicated as the first and last author \- the order of the authors between the first and last position is determined by contribution in a descending order."><a href=https://forrt.org/glossary/english/first_last_author_emphasis_norm/ class=nav-link>First-last-author-emphasis norm (FLAE)</a></li><li class=nav-item data-toggle=tooltip data-placement=right title="Framework of Open Reproducible Research and Teaching. It aims to provide a pedagogical infrastructure designed to recognize and support the teaching and mentoring of open and reproducible research in tandem with prototypical subject matters in higher education. FORRT strives to be an effective, evolving, and community-driven organization raising awareness of the pedagogical implications of open and reproducible science and its associated challenges (i.e., curricular reform, epistemological uncertainty, methods of education). FORRT also advocates for the opening of teaching and mentoring materials as a means to facilitate access, discovery, and learning to those who otherwise would be educationally disenfranchised."><a href=https://forrt.org/glossary/english/forrt/ class=nav-link>FORRT</a></li><li class=nav-item data-toggle=tooltip data-placement=right title="A collective action platform aiming to support the open science movement by obtaining pledges from researchers that they will implement certain research practices (e.g., pre-registration, pre-print). Initially pledges will be anonymous until a sufficient number of people pledge, upon which names of pledges will be released. The initiative is a grassroots movement instigated by early career researchers."><a href=https://forrt.org/glossary/english/free_our_knowledge_platform/ class=nav-link>Free Our Knowledge Platform</a></li><li class=nav-item data-toggle=tooltip data-placement=right title="Adopting questionable research practices (QRPs, e.g., salami slicing of an academic paper) that would align with academic incentive structures that benefit the academic (e.g. in prestige, hiring, or promotion) regardless of whether they support the process of scholarship. If systems rely on metrics to determine an outcome (e.g. academic credit) those metrics can be subject to intentional manipulation (Naudet et al., 2018\) or “gamed”. Where promotions, hiring, and tenure are based on flawed metrics they may disfavor openness, rigor, and transparent work (Naudet et al., 2018\) \- for example favoring “quantity over quality” \- and exacerbate existing inequalities."><a href=https://forrt.org/glossary/english/gaming/ class=nav-link>Gaming (the system)</a></li><li class=nav-item data-toggle=tooltip data-placement=right title="The typically-invisible decision tree traversed during operationalization and statistical analysis given that ‘there is a one-to-many mapping from scientific to statistical hypotheses' (Gelman and Loken, 2013, p. 6). In other words, even in absence of p-hacking or fishing expeditions and when the research hypothesis was posited ahead of time, there can be a plethora of statistical results that can appear to be supported by theory given data. “The problem is there can be a large number of potential comparisons when the details of data analysis are highly contingent on data, without the researcher having to perform any conscious procedure of fishing or examining multiple p-values” (Gelman and Loken, 2013, p. 1). The term aims to highlight the uncertainty ensuing from idiosyncratic analytical and statistical choices in mapping theory-to-test, and contrasting intentional (and unethical) questionable research practices (e.g. p-hacking and fishing expeditions) versus non-intentional research practices that can, potentially, have the same effect despite not having intent to corrupt their results. The garden of forking paths refers to the decisions during the scientific process that inflate the false-positive rate as a consequence of the potential paths which could have been taken (had other decisions been made)."><a href=https://forrt.org/glossary/english/garden_of_forking_paths/ class=nav-link>Garden of forking paths</a></li><li class=nav-item data-toggle=tooltip data-placement=right title="A legal framework of seven principles implemented across the European Union (EU) that aims to safeguard individuals’ information. The framework seeks to commission citizens with control over their personal data, whilst regulating the parties involved in storing and processing these data. This set of legislation dictates the free movement of individuals’ personal information both within and outside the EU and must be considered by researchers when designing and running studies."><a href=https://forrt.org/glossary/english/general_data_protection_regulation/ class=nav-link>General Data Protection Regulation (GDPR)</a></li><li class=nav-item data-toggle=tooltip data-placement=right title="Generalizability refers to how applicable a study’s results are to broader groups of people, settings, or situations they study and how the findings relate to this wider context (Frey, 2018; Kukull & Ganguli, 2012)."><a href=https://forrt.org/glossary/english/generalizability/ class=nav-link>Generalizability</a></li><li class=nav-item data-toggle=tooltip data-placement=right title="The inclusion in an article’s author list of individuals who do not meet the criteria for authorship. As authorship is associated with benefits including peer recognition and financial rewards, there are incentives for inclusion as an author on published research. Gifting authorship, or extending authorship credit to an individual who does not merit such recognition, can be intended to help the gift recipient, repay favors (including reciprocal gift authorship), maintain personal and professional relationships, and enhance chances of publication. Gift authorship is widely considered an unethical practice."><a href=https://forrt.org/glossary/english/gift/ class=nav-link>Gift (or Guest) Authorship</a></li><li class=nav-item data-toggle=tooltip data-placement=right title="A software package for tracking changes in a local set of files (local version control), initially developed by Linus Torvalds. In general, it is used by programmers to track and develop computer source code within a set directory, folder or a file system. Git can access remote repository hosting services (e.g. GitHub) for remote version control that enables collaborative software development by uploading contributions from a local system. This process found its way into the scientific process to enable open data, open code and reproducible analyses."><a href=https://forrt.org/glossary/english/git/ class=nav-link>Git</a></li><li class=nav-item data-toggle=tooltip data-placement=right title="A term coined by economist Charles Goodhart to refer to the observation that measuring something inherently changes user behaviour. In relation to examination performance, Strathern (1997) stated that “when a measure becomes a target, it ceases to be a good measure” (p. 308). Applied to open scholarship, and the structure of incentives in academia, Goodhart’s Law would predict that metrics of scientific evaluation will likely be abused and exploited, as evidenced by Muller (2019)"><a href=https://forrt.org/glossary/english/goodhart_s_law/ class=nav-link>Goodhart’s Law</a></li><li class=nav-item data-toggle=tooltip data-placement=right title="Free to use statistical software for performing power analyses. The user specifies the desired statistical test (e.g. t-test, regression, ANOVA), and three of the following: the number of groups/observations, effect size, significance level, or power, in order to calculate the unspecified aspect."><a href=https://forrt.org/glossary/english/gpower/ class=nav-link>GPower</a></li><li class=nav-item data-toggle=tooltip data-placement=right title="Hirsch’s index, abbreviated as H-index, intends to measure both productivity and research impact by combining the number of publications and the number of citations to these publications. Hirsch (2005) defined the index as “the number of papers with citation number ≥ *h*” (p. 16569). That is, the greatest number such that an author (or journal) has published at least that many papers that have been cited at least that many times. The index is perceived as a superior measure to measures that only assess, for instance, the number of citations and number of publications but this index has been criticised for the purpose of researcher assessment (e.g. Wendl, 2007)."><a href=https://forrt.org/glossary/english/h_index/ class=nav-link>H-index</a></li><li class=nav-item data-toggle=tooltip data-placement=right title="An organized event where experts, designers, or researchers collaborate for a relatively short amount of time to work intensively on a project or problem. The term is originally borrowed from computer programmer and software development events whose goal is to create a fully fledged product (resources, research, software, hardware) by the end of the event, which can last several hours to several days."><a href=https://forrt.org/glossary/english/hackathon/ class=nav-link>Hackathon</a></li><li class=nav-item data-toggle=tooltip data-placement=right title="A questionable research practice termed ‘Hypothesizing After the Results are Known’ (HARKing). “HARKing is defined as presenting a post hoc hypothesis (i.e., one based on or informed by one's results) in a research report as if it was, in fact, *a priori*” (Kerr, 1998, p. 196). For example, performing subgroup analyses, finding an effect in one subgroup, and writing the introduction with a ‘hypothesis’ that matches these results."><a href=https://forrt.org/glossary/english/harking/ class=nav-link>HARKing</a></li><li class=nav-item data-toggle=tooltip data-placement=right title="Contextual conditions that can, unbeknownst to researchers, make the results of a replication attempt deviate from those of the original study. Hidden moderators are sometimes invoked to explain (away) failed replications. Also called hidden assumptions."><a href=https://forrt.org/glossary/english/hidden_moderators/ class=nav-link>Hidden Moderators</a></li><li class=nav-item data-toggle=tooltip data-placement=right title="A hypothesis is an unproven statement relating the connection between variables (Glass & Hall, 2008\) and can be based on prior experiences, scientific knowledge, preliminary observations, theory and/or logic. In scientific testing, a hypothesis can be usually formulated with (e.g. a positive correlation) or without a direction (e.g. there will be a correlation). Popper (1959) posits that hypotheses must be falsifiable, that is, it must be conceivably possible to prove the hypothesis false. However, hypothesis testing based on falsification has been argued to be vague, as it is contingent on many other untested assumptions in the hypothesis (i.e., auxiliary hypotheses). Longino (1990, 1992\) argued that ontological heterogeneity should be valued more than ontological simplicity for the biological sciences, which considers we should investigate differences between and within biological organisms."><a href=https://forrt.org/glossary/english/hypothesis/ class=nav-link>Hypothesis</a></li><li class=nav-item data-toggle=tooltip data-placement=right title="A research metric created by Google Scholar that represents the number of publications a researcher has with at least 10 citations."><a href=https://forrt.org/glossary/english/i10_index/ class=nav-link>i10-index</a></li><li class=nav-item data-toggle=tooltip data-placement=right title="The idea that pre-existing opinions about the quality of research can depend on the ideological views of the author(s). One of the many biases in the peer review process, it expects that favourable opinions towards the research would be more likely if friends, collaborators, or scientists agree with an editor or reviewer’s political viewpoints (Tvina et al. 2019). This could potentially lead to a variety of conflicts of interest that undermine diverse perspectives, for example: speeding or delaying peer-review, or influencing the chances of an individual being invited to present their research, thus promoting their work."><a href=https://forrt.org/glossary/english/ideological_bias/ class=nav-link>Ideological bias</a></li><li class=nav-item data-toggle=tooltip data-placement=right title="The set of evaluation and reward mechanisms (explicit and implicit) for scientists and their work. Incentivised areas within the broader structure include hiring and promotion practices, track record for awarding funding, and prestige indicators such as publication in journals with high impact factors, invited presentations, editorships, and awards. It is commonly believed that these criteria are often misaligned with the telos of science, and therefore do not promote rigorous scientific output. Initiatives like DORA aim to reduce the field’s dependency on evaluation criteria such as journal impact factors in favor of assessments based on the intrinsic quality of research outputs."><a href=https://forrt.org/glossary/english/incentive_structure/ class=nav-link>Incentive structure</a></li><li class=nav-item data-toggle=tooltip data-placement=right title="Inclusion, or inclusivity, refers to a sense of welcome and respect within a given collaborative project or environment (such as academia) where *diversity* simply indicates a wide range of backgrounds, perspectives, and experiences, efforts to increase *inclusion* go further to promote engagement and equal valuation among diverse individuals, who might otherwise be marginalized. Increasing inclusivity often involves minimising the impact of, or even removing, systemic barriers to accessibility and engagement."><a href=https://forrt.org/glossary/english/inclusion/ class=nav-link>Inclusion</a></li><li class=nav-item data-toggle=tooltip data-placement=right title="“Reasoning by drawing a conclusion not guaranteed by the premises; for example, by inferring a general rule from a limited number of observations. Popper believed that there was no such logical process; we may guess general rules but such guesses are not rendered even more probable by any number of observations. By contrast, Bayesians inductively work out the increase in probability of a hypothesis that follows from the observations.” Dienes (p. 164, 2008\)"><a href=https://forrt.org/glossary/english/induction/ class=nav-link>Induction</a></li><li class=nav-item data-toggle=tooltip data-placement=right title="A statistical error in which a formal test is not conducted to assess the difference between a significant and non-significant correlation (or other measures, such as Odds Ratio). This fallacy occurs when a significant and non-significant correlation coefficient are assumed to represent a statistically significant difference but the comparison itself is not explicitly tested."><a href=https://forrt.org/glossary/english/interaction_fallacy/ class=nav-link>Interaction Fallacy</a></li><li class=nav-item data-toggle=tooltip data-placement=right title="An analysis at the core of intersectionality to analyse power, inequality and exclusion, as efforts to reform academic culture cannot be completed by investigating only one avenue in isolation (e.g. race, gender or ability) but by considering all the systems of exclusion. In contrast to intersectionality (which refers to the individual having multiple social identities), interlocking is usually used to describe the systems that combine to serve as oppressive measures toward the individual based on these identities."><a href=https://forrt.org/glossary/english/interlocking/ class=nav-link>Interlocking</a></li><li class=nav-item data-toggle=tooltip data-placement=right title="An indicator of the extent to which a study’s findings are representative of the true effect in the population of interest and not due to research confounds, such as methodological shortcomings. In other words, whether the observed evidence or covariation between the independent (predictor) and dependent (criterion) variables can be taken as a bona fide relationship and not a spurious effect owing to uncontrolled aspects of the study’s set up. Since it involves the quality of the study itself, internal validity is a priority for scientific research."><a href=https://forrt.org/glossary/english/internal_validity/ class=nav-link>Internal Validity</a></li><li class=nav-item data-toggle=tooltip data-placement=right title="A term which derives from Black feminist thought and broadly describes how social identities exist within ‘interlocking systems of oppression’ and structures of (in)equalities (Crenshaw, 1989\). Intersectionality offers a perspective on the way multiple forms of inequality operate together to compound or exacerbate each other. Multiple concurrent forms of identity can have a multiplicative effect and are not merely the sum of the component elements. One implication is that identity cannot be adequately understood through examining a single axis (e.g., race, gender, sexual orientation, class) at a time in isolation, but requires simultaneous consideration of overlapping forms of identity."><a href=https://forrt.org/glossary/english/intersectionality/ class=nav-link>Intersectionality</a></li><li class=nav-item data-toggle=tooltip data-placement=right title="An open-sourced, cross-platform citation and reference management tool that is available free of charge. It allows editing BibTeX files, importing data from online scientific databases, and managing and searching BibTeX files."><a href=https://forrt.org/glossary/english/jabref/ class=nav-link>JabRef</a></li><li class=nav-item data-toggle=tooltip data-placement=right title="Free and open source software for data analysis based on the R language. The software has a graphical user interface and provides the R code to the analyses. Jamovi supports computational reproducibility by saving the data, code, analyses, and results in a single file."><a href=https://forrt.org/glossary/english/jamovi/ class=nav-link>Jamovi</a></li><li class=nav-item data-toggle=tooltip data-placement=right title="Named after Sir Harold Jeffreys, JASP stands for Jeffrey’s Amazing Statistics Program. It is a free and open source software for data analysis. JASP relies on a user interface and offers both null hypothesis tests and their Bayesian counterparts. JASP supports computational reproducibility by saving the data, code, analyses, and results in a single file."><a href=https://forrt.org/glossary/english/jasp/ class=nav-link>JASP</a></li><li class=nav-item data-toggle=tooltip data-placement=right title="An online resource that collects and presents open access policies from publishers, from across the world, providing summaries of individual journal's copyright and open access archiving policies."><a href=https://forrt.org/glossary/english/sherpa_romeo/ class=nav-link>JISC Open Policy Finder (formerly Sherpa services)</a></li><li class=nav-item data-toggle=tooltip data-placement=right title="The mean number of citations to research articles in that journal over the preceding two years. It is a proprietary and opaque calculation marketed by Clarivate™. Journal Impact Factors are not associated with the content quality or the peer review process."><a href=https://forrt.org/glossary/english/journal_impact_factor_/ class=nav-link>Journal Impact Factor™</a></li><li class=nav-item data-toggle=tooltip data-placement=right title="JavaScript Object Notation (JSON) is a data format for structured data that can be used to represent attribute-value pairs. Values thereby can contain further JSON notation (i.e., nested information). JSON files can be formally encoded as strings of text and thus are human-readable. Beyond storing information this feature makes them suitable for annotating other content. For example, JSON files are used in Brain Imaging Data Structure (BIDS) for describing the metadata dataset by following a standardized format (dataset\_description.json)."><a href=https://forrt.org/glossary/english/json_file/ class=nav-link>JSON file</a></li><li class=nav-item data-toggle=tooltip data-placement=right title="The process by which the mind decodes or extracts, stores, and relates new information to existing information in long term memory. Given the complex structure and nature of knowledge, this process is studied in the philosophical field of epistemology, as well as the psychological field of learning and memory."><a href=https://forrt.org/glossary/english/knowledge_acquisition/ class=nav-link>Knowledge acquisition</a></li><li class=nav-item data-toggle=tooltip data-placement=right title="A statistical model of the data used in frequentist and Bayesian analyses, defined up to a constant of proportionality. A likelihood function represents the likeliness of different parameters for your distribution given the data. Given that probability distributions have unknown population parameters, the likelihood function indicates how well the sample data summarise these parameters. As such, the likelihood function gives an idea of the goodness of fit of a model to the sample data for a given set of values of the unknown population parameters."><a href=https://forrt.org/glossary/english/likelihood_function/ class=nav-link>Likelihood function</a></li><li class=nav-item data-toggle=tooltip data-placement=right title="The notion that all information relevant to inference contained in data is provided by the likelihood. The principle suggests that the likelihood function can be used to compare the plausibility of various parameter values. While Bayesians and likelihood theorists subscribe to the likelihood principle, Neyman-Pearson theorists do not, as significance tests violate the likelihood principle because they take into account information not in the likelihood."><a href=https://forrt.org/glossary/english/likelihood_principle/ class=nav-link>Likelihood Principle</a></li><li class=nav-item data-toggle=tooltip data-placement=right title="Researchers often review research records on a given topic to better understand effects and phenomena of interest before embarking on a new research project, to understand how theory links to evidence or to investigate common themes and directions of existing study results and claims. Different types of reviews can be conducted depending on the research question and literature scope. To determine the scope and key concepts in a given field, researchers may want to conduct a scoping literature review. Systematic reviews aim to access and review all available records for the most accurate and unbiased representation of existing literature. Non-systematic or focused literature reviews synthesise information from a selection of studies relevant to the research question although they are uncommon due to susceptibility to biases (e.g. researcher bias; Siddaway et al., 2019)."><a href=https://forrt.org/glossary/english/literature_review/ class=nav-link>Literature Review</a></li><li class=nav-item data-toggle=tooltip data-placement=right title="Portmanteau for ‘male panel’, usually to refer to speaker panels at conferences entirely composed of (usually caucasian) males. Typically discussed in the context of gender disparities in academia (e.g., women being less likely to be recognised as experts by their peers and, subsequently, having fewer opportunities for career development)."><a href=https://forrt.org/glossary/english/manel/ class=nav-link>Manel</a></li><li class=nav-item data-toggle=tooltip data-placement=right title="Large-scale collaborative projects involving tens or hundreds of authors from different institutions. This kind of approach has become increasingly common in psychology and other sciences in recent years as opposed to research carried out by small teams of authors, following earlier trends which have been observed e.g. for high-energy physics or biomedical research in the 1990s. These large international scientific consortia work on a research project to bring together a broader range of expertise and work collaboratively to produce manuscripts."><a href=https://forrt.org/glossary/english/many_authors/ class=nav-link>Many authors</a></li><li class=nav-item data-toggle=tooltip data-placement=right title="A crowdsourcing initiative led by the Open Science Collaboration (2015) whereby several hundred separate research groups from various universities run replication studies of published effects. This initiative is also known as “Many Labs I” and was subsequently followed by a “Many Labs II” project that assessed variation in replication results across samples and settings. Similar projects include ManyBabies, EEGManyLabs, and the Psychological Science Accelerator."><a href=https://forrt.org/glossary/english/many_labs/ class=nav-link>Many Labs</a></li><li class=nav-item data-toggle=tooltip data-placement=right title="Exclusively online courses which are accessible to any learner at any time, are typically free to access (while not necessarily openly licensed), and provide video-based instructions and downloadable data sets and exercises. The “massive” aspect describes the high volume of students that can access the course at any one time due to their flexibility, low or no cost, and online nature of the materials."><a href=https://forrt.org/glossary/english/massive_open_online_courses/ class=nav-link>Massive Open Online Courses (MOOCs)</a></li><li class=nav-item data-toggle=tooltip data-placement=right title="Unlike the traditional collaborative article, a MOOP follows an open participatory and dynamic model that is not restricted by a predetermined list of contributors."><a href=https://forrt.org/glossary/english/massively_open_online_papers/ class=nav-link>Massively Open Online Papers (MOOPs)</a></li><li class=nav-item data-toggle=tooltip data-placement=right title="Named for the ‘rich get richer; poor get poorer’ paraphrase of the Gospel of Matthew. Eminent scientists and early-career researchers with a prestigious fellowship are disproportionately attributed greater levels of credit and funding for their contributions to science while relatively unknown or early-career researchers without a prestigious fellowship tend to get disproportionately little credit for comparable contributions. The impact is a substantial cumulative advantage that results from modest initial comparative advantages (and vice versa)."><a href=https://forrt.org/glossary/english/matthew_effect/ class=nav-link>Matthew effect (in science)</a></li><li class=nav-item data-toggle=tooltip data-placement=right title="A meta-analysis is a statistical synthesis of results from a series of studies examining the same phenomenon. A variety of meta-analytic approaches exist, including random or fixed effects models or meta-regressions, which allow for an examination of moderator effects. By aggregating data from multiple studies, a meta-analysis could provide a more precise estimate for a phenomenon (e.g. type of treatment) than individual studies. Results are usually visualized in a forest plot. Meta-analyses can also help examine heterogeneity across study results. Meta-analyses are often carried out in conjunction with systematic reviews and similarly require a systematic search and screening of studies. Publication bias is also commonly examined in the context of a meta-analysis and is typically visually presented via a funnel plot."><a href=https://forrt.org/glossary/english/meta_analysis/ class=nav-link>Meta-analysis</a></li><li class=nav-item data-toggle=tooltip data-placement=right title="The scientific study of science itself with the aim to describe, explain, evaluate and/or improve scientific practices. Meta-science typically investigates scientific methods, analyses, the reporting and evaluation of data, the reproducibility and replicability of research results, and research incentives."><a href=https://forrt.org/glossary/english/meta_science_or_meta_research/ class=nav-link>Meta-science or Meta-research</a></li><li class=nav-item data-toggle=tooltip data-placement=right title="Structured data that describes and synthesises other data. Metadata can help find, organize, and understand data. Examples of metadata include creator, title, contributors, keywords, tags, as well as any kind of information necessary to verify and understand the results and conclusions of a study such as codebook on data labels, descriptions, the sample and data collection process."><a href=https://forrt.org/glossary/english/metadata/ class=nav-link>Metadata</a></li><li class=nav-item data-toggle=tooltip data-placement=right title="The process by which a verbal description is formalised to remove ambiguity, while also constraining the dimensions a theory can span. The model is thus data derived. “Many scientific models are representational models: they represent a selected part or aspect of the world, which is the model’s target system” (Frigg & Hartman, 2020)."><a href=https://forrt.org/glossary/english/model/ class=nav-link>Model (philosophy)</a></li><li class=nav-item data-toggle=tooltip data-placement=right title="In typical empirical studies, a single researcher or research team conducts the analysis, which creates uncertainty about the extent to which the choice of analysis influences the results. In multi-analyst studies, two or more researchers independently analyse the same research question or hypothesis on the same dataset. According to Aczel and colleagues (2021), a multi-analyst approach may be beneficial in increasing our confidence in a particular finding; uncovering the impact of analytical preferences across research teams; and highlighting the variability in such analytical approaches."><a href=https://forrt.org/glossary/english/multi_analyst_studies/ class=nav-link>Multi-Analyst Studies</a></li><li class=nav-item data-toggle=tooltip data-placement=right title="Potential inflation of Type I error rates (incorrectly rejecting the null hypothesis) because of multiple statistical testing, for example, multiple outcomes, multiple follow-up time points, or multiple subgroup analyses. To overcome issues with multiplicity, researchers will often apply controlling procedures (e.g., Bonferroni, Holm-Bonferroni; Tukey) that correct the alpha value to control for inflated Type I errors. However, by controlling for Type I errors, one can increase the possibility of Type II errors (i.e., incorrectly accepting the null hypothesis)."><a href=https://forrt.org/glossary/english/multiplicity/ class=nav-link>Multiplicity</a></li><li class=nav-item data-toggle=tooltip data-placement=right title="Multiverse analyses are based on all potentially equally justifiable data processing and statistical analysis pipelines that can be employed to test a single hypothesis. In a data multiverse analysis, a single set of raw data is processed into a multiverse of data sets by applying all possible combinations of justifiable preprocessing choices. Model multiverse analyses apply equally justifiable statistical models to the same data to answer the same hypothesis. The statistical analysis is then conducted on all data sets in the multiverse and all results are reported which enhances promoting transparency and illustrates the robustness of results against different data processing (data multiverse) or statistical (model multiverse) pipelines). Multiverse analysis differs from Specification curve analysis with regards to the graphical displays (a histogram and tile plota rather than a specification curve plot)."><a href=https://forrt.org/glossary/english/multiverse_analysis/ class=nav-link>Multiverse analysis</a></li><li class=nav-item data-toggle=tooltip data-placement=right title="An attribution issue arising from two related problems: authors may use multiple names or monikers to publish work, and multiple authors in a single field may share full names. This makes accurate identification of authors on names and specialisms alone a difficult task. This can be addressed through the creation and use of unique digital identifiers that act akin to digital fingerprints such as ORCID."><a href=https://forrt.org/glossary/english/name_ambiguity_problem/ class=nav-link>Name Ambiguity Problem</a></li><li class=nav-item data-toggle=tooltip data-placement=right title="A free, open-source anonymisation software that identifies and modifies named entities (e.g. persons, locations, times, dates). Its key feature is that it preserves critical context needed for secondary analyses. The aim is to assist researchers in sharing their raw text data, while adhering to research ethics."><a href=https://forrt.org/glossary/english/named_entity_based_text_anonymization_for_open_science/ class=nav-link>Named entity-based Text Anonymization for Open Science (NETANOS)</a></li><li class=nav-item data-toggle=tooltip data-placement=right title="A comprehensive set of tools to facilitate the development, preregistration and dissemination of systematic literature reviews for non-intervention research. Part A represents detailed guidelines for creating and preregistering a systematic review protocol in the context of non-intervention research whilst preparing for transparency. Part B represents guidelines for writing up the completed systematic review, with a focus on enhancing reproducibility."><a href=https://forrt.org/glossary/english/non_intervention_reproducible_and_open_systematic_reviews/ class=nav-link>Non-Intervention, Reproducible, and Open Systematic Reviews (NIRO-SR)</a></li><li class=nav-item data-toggle=tooltip data-placement=right title="A frequentist approach to inference used to test the probability of an observed effect against the null hypothesis of no effect/relationship (Pernet, 2015). Such a conclusion is arrived at through use of an index called the *p*\-value. Specifically, researchers will conclude an effect is present when an a priori alpha threshold, set by the researchers, is satisfied; this determines the acceptable level of uncertainty and is closely related to Type I error."><a href=https://forrt.org/glossary/english/null_hypothesis_significance_testing/ class=nav-link>Null Hypothesis Significance Testing (NHST)</a></li><li class=nav-item data-toggle=tooltip data-placement=right title="The idea that scientific claims, methods, results and scientists themselves should remain value-free and unbiased, and thus not be affected by cultural, political, racial or religious bias as well as any personal interests (Merton, 1942)."><a href=https://forrt.org/glossary/english/objectivity/ class=nav-link>Objectivity</a></li><li class=nav-item data-toggle=tooltip data-placement=right title="A set of axioms in a subject area that help classify and explain the nature of the entities under study and the relationships between them."><a href=https://forrt.org/glossary/english/ontology/ class=nav-link>Ontology (Artificial Intelligence)</a></li><li class=nav-item data-toggle=tooltip data-placement=right title="“Free availability of scholarship on the public internet, permitting any users to read, download, copy, distribute, print, search, or link to the full texts of these research articles, crawl them for indexing, pass them as data to software, or use them for any other lawful purpose, without financial, legal, or technical barriers other than those inseparable from gaining access to the internet itself” (Boai, 2002). Different methods of achieving open access (OA) are often referred to by color, including Green Open Access (when the work is openly accessible from a public repository), Gold Open Access (when the work is immediately openly accessible upon publication via a journal website), and Platinum (or Diamond) Open Access (a subset of Gold OA in which all works in the journal are immediately accessible after publication from the journal website without the authors needing to pay an article processing fee \[APC\])."><a href=https://forrt.org/glossary/english/open_access/ class=nav-link>Open access</a></li><li class=nav-item data-toggle=tooltip data-placement=right title="Making computer code (e.g., programming, analysis code, stimuli generation) freely and publicly available in order to make research methodology and analysis transparent and allow for reproducibility and collaboration. Code can be made available via open code websites, such as GitHub, the Open Science Framework, and Codeshare (to name a few), enabling others to evaluate and correct errors and re-use and modify the code for subsequent research."><a href=https://forrt.org/glossary/english/open_code/ class=nav-link>Open Code</a></li><li class=nav-item data-toggle=tooltip data-placement=right title="Open data refers to data that is freely available and readily accessible for use by others without restriction, “Open data and content can be freely used, modified, and shared by anyone for any purpose” ([https://opendefinition.org/](https://opendefinition.org/)). Open data are subject to the requirement to attribute and share alike, thus it is important to consider appropriate Open Licenses. Sensitive or time-sensitive datasets can be embargoed or shared with more selective access options to ensure data integrity is upheld."><a href=https://forrt.org/glossary/english/open_data/ class=nav-link>Open Data</a></li><li class=nav-item data-toggle=tooltip data-placement=right title="OER Commons (with OER standing for open educational resources) is a freely accessible online library allowing teachers to create, share and remix educational resources. The goal of the OER movement is to stimulate “collaborative teaching and learning” ([https://www.oercommons.org/about](https://www.oercommons.org/about)) and provide high-quality educational resources that are accessible for everyone."><a href=https://forrt.org/glossary/english/open_educational_resources/ class=nav-link>Open Educational Resources (OER) Commons</a></li><li class=nav-item data-toggle=tooltip data-placement=right title="Open licenses are provided with open data and open software (e.g., analysis code) to define how others can (re)use the licensed material. In setting out the permissions and restrictions, open licenses often permit the unrestricted access, reuse and retribution of an author’s original work. Datasets are typically licensed under a type of open licence known as a Creative Commons license (e.g., MIT, Apache, and GPL). These can differ in relatively subtle ways with GPL licenses (and their variants) being Copyleft licenses that require that any derivative work is licensed under the same terms as the original."><a href=https://forrt.org/glossary/english/open_licenses/ class=nav-link>Open Licenses</a></li><li class=nav-item data-toggle=tooltip data-placement=right title="Author’s public sharing of materials that were used in a study, “such as survey items, stimulus materials, and experiment programs” (Kidwell et al., 2016, p. 3). Digitally-shareable materials are posted on open access repositories, which makes them publicly available and accessible. Depending on licensing, the material can be reused by other authors for their own studies. Components that are not digitally-shareable (e.g. biological materials, equipment) must be described in sufficient detail to allow reproducibility."><a href=https://forrt.org/glossary/english/open_material/ class=nav-link>Open Material</a></li><li class=nav-item data-toggle=tooltip data-placement=right title="A scholarly review mechanism providing disclosure of any combination of author and referee identities, as well as peer-review reports and editorial decision letters, to one another or publicly at any point during or after the peer review or publication process. It may also refer to the removal of restrictions on who can participate in peer review and the platforms for doing so. Note that ‘open peer review’ has been used interchangeably to refer to any, or all, of the above practices."><a href=https://forrt.org/glossary/english/open_peer_review/ class=nav-link>Open Peer Review</a></li><li class=nav-item data-toggle=tooltip data-placement=right title="‘Open scholarship’ is often used synonymously with ‘open science’, but extends to all disciplines, drawing in those which might not traditionally identify as science-based. It reflects the idea that knowledge of all kinds should be openly shared, transparent, rigorous, reproducible, replicable, accumulative, and inclusive (allowing for all knowledge systems). Open scholarship includes all scholarly activities that are not solely limited to research such as teaching and pedagogy."><a href=https://forrt.org/glossary/english/open_scholarship/ class=nav-link>Open Scholarship</a></li><li class=nav-item data-toggle=tooltip data-placement=right title="The Open Scholarship Knowledge Base (OSKB) is a collaborative initiative to share knowledge on the what, why and how of open scholarship to make this knowledge easy to find and apply. Information is curated and created by the community. The OSKB is a community under the Center for Open Science (COS)."><a href=https://forrt.org/glossary/english/open_scholarship_knowledge_base/ class=nav-link>Open Scholarship Knowledge Base</a></li><li class=nav-item data-toggle=tooltip data-placement=right title="An umbrella term reflecting the idea that scientific knowledge of all kinds, where appropriate, should be openly accessible, transparent, rigorous, reproducible, replicable, accumulative, and inclusive, all which are considered fundamental features of the scientific endeavour. Open science consists of principles and behaviors that promote transparent, credible, reproducible, and accessible science. Open science has six major aspects: open data, open methodology, open source, open access, open peer review, and open educational resources."><a href=https://forrt.org/glossary/english/open_science/ class=nav-link>Open Science</a></li><li class=nav-item data-toggle=tooltip data-placement=right title="A free and open source platform for researchers to organize and share their research project and to encourage collaboration. Often used as an open repository for research code, data and materials, preprints and preregistrations, while managing a more efficient workflow. Created and maintained by the Center for Open Science."><a href=https://forrt.org/glossary/english/open_science_framework/ class=nav-link>Open Science Framework</a></li><li class=nav-item data-toggle=tooltip data-placement=right title="A type of computer software in which source code is released under a license that permits others to use, change, and distribute the software to anyone and for any purpose. Open source is more than openly accessible: the distribution terms of open-source software must comply with 10 specific criteria (see: [https://opensource.org/osd](https://opensource.org/osd))."><a href=https://forrt.org/glossary/english/open_source_software/ class=nav-link>Open Source software</a></li><li class=nav-item data-toggle=tooltip data-placement=right title="Open washing, termed after “greenwashing”, refers to the act of claiming openness to secure perceptions of rigor or prestige associated with open practices. It has been used to characterise the marketing strategy of software companies that have the appearance of open-source and open-licensing, while engaging in proprietary practices. Open washing is a growing concern for those adopting open science practices as their actions are undermined by misleading uses of the practices, and actions designed to facilitate progressive developments are reduced to ‘ticking the box’ without clear quality control."><a href=https://forrt.org/glossary/english/open_washing/ class=nav-link>Open washing</a></li><li class=nav-item data-toggle=tooltip data-placement=right title="A free platform where researchers can freely and openly share, browse, download and re-use brain imaging data (e.g., MRI, MEG, EEG, iEEG, ECoG, ASL, and PET data)."><a href=https://forrt.org/glossary/english/openneuro/ class=nav-link>OpenNeuro</a></li><li class=nav-item data-toggle=tooltip data-placement=right title="The practice of (repeatedly) analyzing data during the data collection process and deciding to stop data collection if a statistical criterion (e.g. *p*\-value, or bayes factor) reaches a specified threshold. If appropriate methodological precautions are taken to control the type 1 error rate, this can be an efficient analysis procedure (e.g. Lakens, 2014). However, without transparent reporting or appropriate error control the type 1 error can increase greatly and optional stopping could be considered a Questionable Research Practice (QRP) or a form of p-hacking."><a href=https://forrt.org/glossary/english/optional_stopping/ class=nav-link>Optional Stopping</a></li><li class=nav-item data-toggle=tooltip data-placement=right title="A organisation that provides a registry of persistent unique identifiers (ORCID iDs) for researchers and scholars, allowing these users to link their digital research documents and other contributions to their ORCID record. This avoids the name ambiguity problem in scholarly communication. ORCID iDs provide unique, persistent identifiers connecting researchers and their scholarly work. It is free to register for an ORCID iD at https://orcid.org/register."><a href=https://forrt.org/glossary/english/orcid/ class=nav-link>ORCID (Open Researcher and Contributor ID)</a></li><li class=nav-item data-toggle=tooltip data-placement=right title="Open access electronic journals that collect and curate articles available from other sources (typically preprint servers, such as arXiv). Article curation may include (post-publication) peer review or editorial selection. Overlay journals do not publish novel material; rather, they organize and collate articles available in existing repositories."><a href=https://forrt.org/glossary/english/overlay_journal/ class=nav-link>Overlay Journal</a></li><li class=nav-item data-toggle=tooltip data-placement=right title="P-curve is a tool for identifying potential publication bias and makes use of the distribution of significant *p*\-values in a series of independent findings. The deviation from the expected right-skewed distribution can be used to assess the existence and degree of publication bias: if the curve is right-skewed, there are more low, highly significant *p*\-values, reflecting an underlying true effect. If the curve is left-skewed, there are many barely significant results just under the 0.05-threshold. This suggests that the studies lack evidential value and may be underpinned by questionable research practices (QRPs; e.g., p-hacking). In the case of no true effect present (true null hypothesis) and unbiased *p*\-value reporting, the *p*\-curve should be a flat, horizontal line, representing the typical distribution of *p*\-values."><a href=https://forrt.org/glossary/english/p_curve/ class=nav-link>P-curve</a></li><li class=nav-item data-toggle=tooltip data-placement=right title="Exploiting techniques that may artificially increase the likelihood of obtaining a statistically significant result by meeting the standard statistical significance criterion (typically α \= .05). For example, performing multiple analyses and reporting only those at *p* \< .05, selectively removing data until *p* \< .05, selecting variables for use in analyses based on whether those parameters are statistically significant."><a href=https://forrt.org/glossary/english/p_hacking/ class=nav-link>p*-hacking</a></li><li class=nav-item data-toggle=tooltip data-placement=right title="A statistic used to evaluate the outcome of a hypothesis test in Null Hypothesis Significance Testing (NHST). It refers to the probability of observing an effect, or more extreme effect, assuming the null hypothesis is true (Lakens, 2021b). The American Statistical Association’s statement on p-values (Wasserstein & Lazar, 2016\) notes that p-values are not an indicator of the truth of the null hypothesis and instead defines p-values in this way: “Informally, a p-value is the probability under a specified statistical model that a statistical summary of the data (e.g., the sample mean difference between two compared groups) would be equal to or more extreme than its observed value” (p. 131)."><a href=https://forrt.org/glossary/english/p_value/ class=nav-link>p*-value</a></li><li class=nav-item data-toggle=tooltip data-placement=right title="An organization that is engaged in scientific misconduct wherein multiple papers are produced by falsifying or fabricating data, e.g. by editing figures or numerical data or plagiarizing written text. Papermills are “alleged to offer products ranging from research data through to ghostwritten fraudulent or fabricated manuscripts and submission services” (Byrne & Christopher, 2020, p. 583). A papermill relates to the fast production and dissemination of multiple allegedly new papers. These are often not detected in the scientific publishing process and therefore either never found or retracted if discovered (e.g. through plagiarism software)."><a href=https://forrt.org/glossary/english/papermill/ class=nav-link>Papermill</a></li><li class=nav-item data-toggle=tooltip data-placement=right title="Data that are captured about the characteristics and context of primary data collected from an individual \- distinct from metadata. Paradata can be used to investigate a respondent’s interaction with a survey or an experiment on a micro-level. They can be most easily collected during computer mediated surveys but are not limited to them. Examples include response times to survey questions, repeated patterns of responses such as choosing the same answer for all questions, contextual characteristics of the participant such as injuries that prevent good performance on tasks, the number of premature responses to stimuli in an experiment. Paradata have been used for the investigation and adjustment of measurement and sampling errors."><a href=https://forrt.org/glossary/english/paradata/ class=nav-link>Paradata</a></li><li class=nav-item data-toggle=tooltip data-placement=right title='PARKing (preregistering after results are known) is defined as the practice where researchers complete an experiment (possibly with infinite re-experimentation) before preregistering. This practice invalidates the purpose of preregistration, and is one of the QRPs (or, even scientific misconduct) that try to gain only "credibility that it has been preregistered."'><a href=https://forrt.org/glossary/english/parking/ class=nav-link>PARKing</a></li><li class=nav-item data-toggle=tooltip data-placement=right title="Participatory research refers to incorporating the views of people from relevant communities in the entire research process to achieve shared goals between researchers and the communities. This approach takes a collaborative stance that seeks to reduce the power imbalance between the researcher and those researched through a “systematic cocreation of new knowledge” (Andersson, 2018)."><a href=https://forrt.org/glossary/english/participatory_research/ class=nav-link>Participatory Research</a></li><li class=nav-item data-toggle=tooltip data-placement=right title="Active research collaboration with the population of interest, as opposed to conducting research “about” them. Researchers can incorporate the lived experience and expertise of patients and the public at all stages of the research process. For example, patients can help to develop a set of research questions, review the suitability of a study design, approve plain English summaries for grant/ethics applications and dissemination, collect and analyse data, and assist with writing up a project for publication. This is becoming highly recommended and even required by funders (Boivin et al., 2018)."><a href=https://forrt.org/glossary/english/patient_and_public_involvement/ class=nav-link>Patient and Public Involvement (PPI)</a></li><li class=nav-item data-toggle=tooltip data-placement=right title="A technological barrier that permits access to information only to individuals who have paid \- either personally, or via an organisation \- a designated fee or subscription."><a href=https://forrt.org/glossary/english/paywall/ class=nav-link>Paywall</a></li><li class=nav-item data-toggle=tooltip data-placement=right title="PCI is a non-profit organisation that creates communities of researchers who review and recommend unpublished preprints based upon high-quality peer review from at least two researchers in their field. These preprints are then assigned a DOI, similarly to a journal article. PCI was developed to establish a free, transparent and public scientific publication system based on the review and recommendation of preprints."><a href=https://forrt.org/glossary/english/pci/ class=nav-link>PCI (Peer Community In)</a></li><li class=nav-item data-toggle=tooltip data-placement=right title="An initiative launched in 2021 dedicated to receiving, reviewing, and recommending Registered Reports (RRs) across the full spectrum of Science, technology, engineering, and mathematics (STEM), medicine, social sciences and humanities. Peer Community In (PCI) RRs are overseen by a ‘Recommender’ (equivalent to an Action Editor) and reviewed by at least two experts in the relevant field. It provides free and transparent pre- (Stage 1\) and post-study (Stage 2\) reviews across research fields. A network of PCI RR-friendly journals endorse the PCI RR review criteria and commit to accepting, without further peer review, RRs that receive a positive final recommendation from PCI RR."><a href=https://forrt.org/glossary/english/pci_registered_reports/ class=nav-link>PCI Registered Reports</a></li><li class=nav-item data-toggle=tooltip data-placement=right title="Plan S is an initiative, launched in September 2018 by cOAlition S, a consortium of research funding organisations, which aims to accelerate the transition to full and immediate Open Access. Participating funders require recipients of research grants to publish their research in compliant Open Access journals or platforms, or make their work openly and immediately available in an Open Access repository, from 2021 onwards. cOAlition S funders have commited to not financially support ‘hybrid’ Open Access publication fees in subscription venues. However, authors can comply with plan S through publishing Open Access in a subscription journal under a “transformative arrangement” as further described in the implementation guidance. The “S” in Plan S stands for shock."><a href=https://forrt.org/glossary/english/plan_s/ class=nav-link>Plan S</a></li><li class=nav-item data-toggle=tooltip data-placement=right title="The contextualization of both the research environment and the researcher, to define the boundaries within the research was produced (Jaraf, 2018). Positionality is typically centred and celebrated in qualitative research, but there have been recent calls for it to also be used in quantitative research as well. Positionality statements, whereby a researcher outlines their background and ‘position’ within and towards the research, have been suggested as one method of recognising and centring researcher bias."><a href=https://forrt.org/glossary/english/positionality/ class=nav-link>Positionality</a></li><li class=nav-item data-toggle=tooltip data-placement=right title="A reflexive tool for practicing explicit positionality in critical qualitative research. The map is to be used “as a flexible starting point to guide researchers to reflect and be reflexive about their social location. The map involves three tiers: the identification of social identities (Tier 1), how these positions impact our life (Tier 2), and details that may be tied to the particularities of our social identity (Tier 3).” (Jacobson and Mustafa 2019, p. 1). The aim of the map is “for researchers to be able to better identify and understand their social locations and how they may pose challenges and aspects of ease within the qualitative research process.”"><a href=https://forrt.org/glossary/english/positionality_map/ class=nav-link>Positionality Map</a></li><li class=nav-item data-toggle=tooltip data-placement=right title="Post hoc is borrowed from Latin, meaning “after this”. In statistics, post hoc (or post hoc analysis) refers to the testing of hypotheses not specified prior to data analysis. In frequentist statistics, the procedure differs based on whether the analysis was planned or post-hoc, for example by applying more stringent error control. In contrast, Bayesian and likelihood approaches do not differ as a function of when the hypothesis was specified."><a href=https://forrt.org/glossary/english/post_hoc/ class=nav-link>Post Hoc</a></li><li class=nav-item data-toggle=tooltip data-placement=right title="Peer review that takes place after research has been published. It is typically posted on a dedicated platform (e.g., PubPeer). It is distinct from the traditional commentary which is published in the same journal and which is itself usually peer reviewed."><a href=https://forrt.org/glossary/english/post_publication_peer_review/ class=nav-link>Post Publication Peer Review</a></li><li class=nav-item data-toggle=tooltip data-placement=right title="A way to summarize one’s updated knowledge in Bayesian inference, balancing prior knowledge with observed data. In statistical terms, posterior distributions are proportional to the product of the likelihood function and the prior. A posterior probability distribution captures (un)certainty about a given parameter value."><a href=https://forrt.org/glossary/english/posterior_distribution/ class=nav-link>Posterior distribution</a></li><li class=nav-item data-toggle=tooltip data-placement=right title="Predatory (sometimes “vanity”) publishing describes a range of business practices in which publishers seek to profit, primarily by collecting article processing charges (APCs), from publishing scientific works without necessarily providing legitimate quality checks (e.g., peer review) or editorial services. In its most extreme form, predatory publishers will publish any work, so long as charges are paid. Other less extreme strategies, such as sending out high numbers of unsolicited requests for editing or publishing in fee-driven special issues, have also been accused as predatory (Crosetto, 2021)."><a href=https://forrt.org/glossary/english/predatory_publishing/ class=nav-link>Predatory Publishing</a></li><li class=nav-item data-toggle=tooltip data-placement=right title="The PREPARE guidelines and checklist (Planning Research and Experimental Procedures on Animals: Recommendations for Excellence) aim to help the planning of animal research, and support adherence to the 3Rs (Replacement, Reduction or Refinement) and facilitate the reproducibility of animal research."><a href=https://forrt.org/glossary/english/prepare_guidelines/ class=nav-link>PREPARE Guidelines</a></li><li class=nav-item data-toggle=tooltip data-placement=right title="A publicly available version of any type of scientific manuscript/research output preceding formal publication, considered a form of Green Open Access. Preprints are usually hosted on a repository (e.g. arXiv) that facilitates dissemination by sharing research results more quickly than through traditional publication. Preprint repositories typically provide persistent identifiers (e.g. DOIs) to preprints. Preprints can be published at any point during the research cycle, but are most commonly published upon submission (i.e., before peer-review). Accepted and peer-reviewed versions of articles are also often uploaded to preprint servers, and are called postprints."><a href=https://forrt.org/glossary/english/preprint/ class=nav-link>Preprint</a></li><li class=nav-item data-toggle=tooltip data-placement=right title="The practice of publishing the plan for a study, including research questions/hypotheses, research design, data analysis before the data has been collected or examined. It is also possible to preregister secondary data analyses (Merten & Krypotos, 2019). A preregistration document is time-stamped and typically registered with an independent party (e.g., a repository) so that it can be publicly shared with others (possibly after an embargo period). Preregistration provides a transparent documentation of what was planned at a certain time point, and allows third parties to assess what changes may have occurred afterwards. The more detailed a preregistration is, the better third parties can assess these changes and with that the validity of the performed analyses. Preregistration aims to clearly distinguish confirmatory from exploratory research."><a href=https://forrt.org/glossary/english/preregistration/ class=nav-link>Preregistration</a></li><li class=nav-item data-toggle=tooltip data-placement=right title="In a “collective action in support of open and reproducible research practices'', the preregistration pledge is a campaign from the Project Free Our Knowledge that asks a researcher to commit to preregistering at least one study in the next two years ([https://freeourknowledge.org/about/](https://freeourknowledge.org/about/)). The project is a grassroots movement initiated by early career researchers (ECRs)."><a href=https://forrt.org/glossary/english/preregistration_pledge/ class=nav-link>Preregistration Pledge</a></li><li class=nav-item data-toggle=tooltip data-placement=right title="Beliefs held by researchers about the parameters in a statistical model before further evidence is taken into account. A ‘prior’ is expressed as a probability distribution and can be determined in a number of ways (e.g., previous research, subjective assessment, principles such as maximising entropy given constraints), and is typically combined with the likelihood function using Bayes’ theorem to obtain a posterior distribution."><a href=https://forrt.org/glossary/english/prior_distribution/ class=nav-link>Prior distribution</a></li><li class=nav-item data-toggle=tooltip data-placement=right title="The agreement made by several academics that they will not provide a peer review of a manuscript unless certain conditions are met. Specifically, the manuscript authors should ensure the data and materials will be made publically available (or give a justification as to why they are not freely available or shared), provide documentation detailing how to interpret and run any files or code and detail where these files can be located via the manuscript itself."><a href=https://forrt.org/glossary/english/pro/ class=nav-link>PRO (peer review openness) initiative</a></li><li class=nav-item data-toggle=tooltip data-placement=right title="Pseudonymisation refers to a technique that involves replacing or removing any information that could lead to identification of research subjects’ identity whilst still being able to make them identifiable through the use of the combination of code number and identifiers. This process comprises the following steps: removal of all identifiers from the research dataset; attribution of a specific identifier (pseudonym) for each participant and using it to label each research record; and maintenance of a cipher that links the code number to the participant in a document physically separate from the dataset. Pseudonymisation is typically a minimum requirement from ethical committees when conducting research, especially on human participants or involving confidential information, in order to ensure upholding of data privacy."><a href=https://forrt.org/glossary/english/pseudonymisation/ class=nav-link>Pseudonymisation</a></li><li class=nav-item data-toggle=tooltip data-placement=right title="When there is a lack of statistical independence presented in the data and thus artificially inflating the number of samples (i.e. replicates). For instance, collecting more than one data point from the same experimental unit (e.g. participant or crops). Numerous methods can overcome this, such as averaging across replicates (e.g., taking the mean RT for a participant) or implementing mixed effects models with the random effects structure accounting for the pseudoreplication (e.g., specifying each individual RT as belonging to the same subject). Note, the former option would be associated with a loss of information and statistical power."><a href=https://forrt.org/glossary/english/pseudoreplication/ class=nav-link>Pseudoreplication</a></li><li class=nav-item data-toggle=tooltip data-placement=right title="Psychometric meta-analyses aim to correct for attenuation of the effect sizes of interest due to measurement error and other artifacts by using procedures based on psychometric principles, e.g. reliability of the measures. These procedures should be implemented before using the synthesised effect sizes in correlational or experimental meta-analysis, as making these corrections tends to lead to larger and less variable effect sizes."><a href=https://forrt.org/glossary/english/psychometric_meta_analysis/ class=nav-link>Psychometric meta-analysis</a></li><li class=nav-item data-toggle=tooltip data-placement=right title="Trust in the knowledge, guidelines and recommendations that has been produced or provided by scientists to the benefit of civil society (Hendriks et al., 2016). These may also refer to trust in scientific-based recommendations on public health (e.g., universal health-care, stem cell research, federal funds for women’s reproductive rights, preventive measures of contagious diseases, and vaccination), climate change, economic policies (e.g., welfare, inequality- and poverty-control) and their intersections. The trust a member of the public has in science has been shown to be influenced by a vast number of factors such as age (Anderson et al., 2012), gender (Von Roten, 2004), rejection of scientific norms (Lewandowsky & Oberauer, 2021), political ideology (Azevedo & Jost, 2021; Brewer & Ley, 2012; Leiserowitz et al., 2010), 	right-wing authoritarianism and social dominance (Kerr & Wilson, 2021), education (Bak, 2001; Hayes & Tariq, 2000), income (Anderson et al., 2012), science knowledge (Evans & Durant, 1995; Nisbet et al., 2002), social media use (Huber et al., 2019), and religiosity (Azevedo, 2021; Brewer & Ley, 2013; Liu & Priest, 2009)."><a href=https://forrt.org/glossary/english/public_trust_in_science/ class=nav-link>Public Trust in Science</a></li><li class=nav-item data-toggle=tooltip data-placement=right title='The failure to publish results based on the "direction or strength of the study findings" (Dickersin & Min, 1993, p. 135). The bias arises when the evaluation of a study’s publishability disproportionately hinges on the outcome of the study, often with the inclination that novel and significant results are worth publishing more than replications and null results. This bias typically materializes through a disproportionate number of significant findings and inflated effect sizes. This process leads to the published scientific literature not being representative of the full extent of all research, and specifically underrepresents null finding. Such findings, in turn, land in the so called “file drawer”, where they are never published and have no findable documentation.'><a href=https://forrt.org/glossary/english/publication_bias/ class=nav-link>Publication bias (File Drawer Problem)</a></li><li class=nav-item data-toggle=tooltip data-placement=right title="An aphorism describing the pressure researchers feel to publish academic manuscripts, often in high prestige academic journals, in order to have a successful academic career. This pressure to publish a high quantity of manuscripts can go at the expense of the quality of the manuscripts. This institutional pressure is exacerbated by hiring procedures and funding decisions strongly focusing on the number and impact of publications."><a href=https://forrt.org/glossary/english/publish_or_perish/ class=nav-link>Publish or Perish</a></li><li class=nav-item data-toggle=tooltip data-placement=right title="A website that allows users to post anonymous peer reviews of research that has been published (i.e. post-publication peer review)."><a href=https://forrt.org/glossary/english/pubpeer/ class=nav-link>PubPeer</a></li><li class=nav-item data-toggle=tooltip data-placement=right title="An interpreted general-purpose programming language, intended to be user-friendly and easily readable, originally created by Guido van Rossum in 1991\. Python has an extensive library of additional features with accessible documentation for tasks ranging from data analysis to experiment creation. It is a popular programming language in data science, machine learning and web development. Similar to R Markdown, Python can be presented in an interactive online format called a Jupyter notebook, combining code, data, and text."><a href=https://forrt.org/glossary/english/python/ class=nav-link>Python</a></li><li class=nav-item data-toggle=tooltip data-placement=right title="Research which uses non-numerical data, such as textual responses, images, videos or other artefacts, to explore in-depth concepts, theories, or experiences. There are a wide range of qualitative approaches, from micro-detailed exploration of language or focusing on personal subjective experiences, to those which explore macro-level social experiences and opinions."><a href=https://forrt.org/glossary/english/qualitative_research/ class=nav-link>Qualitative research</a></li><li class=nav-item data-toggle=tooltip data-placement=right title="Quantitative research encompasses a diverse range of methods to systematically investigate a range of phenomena via the use of numerical data which can be analysed with statistics."><a href=https://forrt.org/glossary/english/quantitative_research/ class=nav-link>Quantitative research</a></li><li class=nav-item data-toggle=tooltip data-placement=right title="Decisions researchers make that raise doubts about the validity of measures used in a study, and ultimately the study’s final conclusions (Flake & Fried, 2020). Issues arise from a lack of transparency in reporting measurement practices, a failure to address construct validity, negligence, ignorance, or deliberate misrepresentation of information."><a href=https://forrt.org/glossary/english/questionable_measurement_practices/ class=nav-link>Questionable Measurement Practices (QMP)</a></li><li class=nav-item data-toggle=tooltip data-placement=right title="A range of activities that intentionally or unintentionally distort data in favour of a researcher’s own hypotheses \- or omissions in reporting such practices \- including; selective inclusion of data, hypothesising after the results are known (HARKing), and *p*\-hacking. Popularized by John et al. (2012)."><a href=https://forrt.org/glossary/english/questionable_research_practices_or_questionable_reporting_practices/ class=nav-link>Questionable Research Practices or Questionable Reporting Practices (QRPs)</a></li><li class=nav-item data-toggle=tooltip data-placement=right title="R is a free, open-source programming language and software environment that can be used to conduct statistical analyses and plot data. R was created by Ross Ihaka and Robert Gentleman at the University of Auckland. R enables authors to share reproducible analysis scripts, which increases the transparency of a study. Often, R is used in conjunction with an integrated development environment (IDE) which simplifies working with the language, for example RStudio or Visual Studio Code, or Tinn-R ."><a href=https://forrt.org/glossary/english/r/ class=nav-link>R</a></li><li class=nav-item data-toggle=tooltip data-placement=right title="An approach that integrates external criticism by colleagues and peers into the research process. Red teams are based on the idea that research that is more critically and widely evaluated is more reliable. The term originates from a military practice: One group (the red team) attacks something, and another group (the blue team) defends it. The practice has been applied to open science, by giving a red team (designated critical individuals) financial incentives to find errors in or identify improvements to the materials or content of a research project (in the materials, code, writing, etc.; Coles et al., 2020)."><a href=https://forrt.org/glossary/english/red_teams/ class=nav-link>Red Teams</a></li><li class=nav-item data-toggle=tooltip data-placement=right title="The process of reflexivity refers to critically considering the knowledge that we produce through research, how it is produced, and our own role as researchers in producing this knowledge. There are different forms of reflexivity; personal reflexivity whereby researchers consider the impact of their own personal experiences, and functional whereby researchers consider the way in which our research tools and methods may have impacted knowledge production. Reflexivity aims to bring attention to underlying factors which may impact the research process, including development of research questions, data collection, and the analysis."><a href=https://forrt.org/glossary/english/reflexivity/ class=nav-link>Reflexivity</a></li><li class=nav-item data-toggle=tooltip data-placement=right title="A scientific publishing format that includes an initial round of peer review of the background and methods (study design, measurement, and analysis plan); sufficiently high quality manuscripts are accepted for in-principle acceptance (IPA) at this stage. Typically, this stage 1 review occurs before data collection, however secondary data analyses are possible in this publishing format. Following data analyses and write up of results and discussion sections, the stage 2 review assesses whether authors sufficiently followed their study plan and reported deviations from it (and remains indifferent to the results). This shifts the focus of the review to the study’s proposed research question and methodology and away from the perceived interest in the study’s results."><a href=https://forrt.org/glossary/english/registered_report/ class=nav-link>Registered Report</a></li><li class=nav-item data-toggle=tooltip data-placement=right title="A global registry of research data repositories from different academic disciplines. It includes repositories that enable permanent storage of, description via metadata and access to, data sets by researchers, funding bodies, publishers, and scholarly institutions."><a href=https://forrt.org/glossary/english/registry_of_research_data_repositories/ class=nav-link>Registry of Research Data Repositories</a></li><li class=nav-item data-toggle=tooltip data-placement=right title="The extent to which repeated measurements lead to the same results. In psychometrics, reliability refers to the extent to which respondents have similar scores when they take a questionnaire on multiple occasions. Noteworthy, reliability does not imply validity. Furthermore, additional types of reliability besides internal consistency exist, including: test-retest reliability, parallel forms reliability and interrater reliability."><a href=https://forrt.org/glossary/english/reliability/ class=nav-link>Reliability</a></li><li class=nav-item data-toggle=tooltip data-placement=right title="Synonymous with *test-retest* *reliability*. It refers to the agreement between the results of successive measurements of the same measure. Repeatability requires the same experimental tools, the same observer, the same measuring instrument administered under the same conditions, the same location, repetition over a short period of time, and the same objectives (Joint Committee for Guidelines in Metrology, 2008\)"><a href=https://forrt.org/glossary/english/repeatability/ class=nav-link>Repeatability</a></li><li class=nav-item data-toggle=tooltip data-placement=right title="An umbrella term, used differently across fields, covering concepts of: direct and conceptual replication, computational reproducibility/replicability, generalizability analysis and robustness analyses. Some of the definitions used previously include: a different team arriving at the same results using the original author's artifacts (Barba 2018); a study arriving at the same conclusion after collecting new data (Claerbout and Karrenbach, 1992); as well as studies for which any outcome would be considered diagnostic evidence about a claim from prior research (Nosek & Errington, 2020)."><a href=https://forrt.org/glossary/english/replicability/ class=nav-link>Replicability</a></li><li class=nav-item data-toggle=tooltip data-placement=right title="A replication market is an environment where users bet on the replicability of certain effects. Forecasters are incentivized to make accurate predictions and the top successful forecasters receive monetary compensation or contributorship for their bets. The rationale behind a replication market is that it leverages the collective wisdom of the scientific community to predict which effect will most likely replicate, thus encouraging researchers to channel their limited resources to replicating these effects."><a href=https://forrt.org/glossary/english/replication_markets/ class=nav-link>Replication Markets</a></li><li class=nav-item data-toggle=tooltip data-placement=right title="Collaborative Assessment for Trustworthy Science. The repliCATS project’s aim is to crowdsource predictions about the reliability and replicability of published research in eight social science fields: business research, criminology, economics, education, political science, psychology, public administration, and sociology."><a href=https://forrt.org/glossary/english/replicats_project/ class=nav-link>RepliCATs project</a></li><li class=nav-item data-toggle=tooltip data-placement=right title="A reporting guideline is a “checklist, flow diagram, or structured text to guide authors in reporting a specific type of research, developed using explicit methodology.” (EQUATOR Network, n.d.). Reporting guidelines provide the minimum guidance required to ensure that research findings can be appropriately interpreted, appraised, synthesized and replicated. Their use often differs per scientific journal or publisher."><a href=https://forrt.org/glossary/english/reporting_guideline/ class=nav-link>Reporting Guideline</a></li><li class=nav-item data-toggle=tooltip data-placement=right title="An online archive for the storage of digital objects including research outputs, manuscripts, analysis code and/or data. Examples include preprint servers such as bioRxiv, MetaArXiv, PsyArXiv, institutional research repositories, as well as data repositories that collect and store datasets including zenodo.org, PsychData, and code repositories such as Github, or more general repositories for all kinds of research data, such as the Open Science Framework (OSF). Digital objects stored in repositories are typically described through metadata which enables discovery across different storage locations."><a href=https://forrt.org/glossary/english/repository/ class=nav-link>Repository</a></li><li class=nav-item data-toggle=tooltip data-placement=right title="A grassroots initiative that helps researchers create local journal clubs at their universities to discuss a range of topics relating to open research and scholarship. Each meeting usually centres around a specific paper that discusses, for example, reproducibility, research practice, research quality, social justice and inclusion, and ideas for improving science."><a href=https://forrt.org/glossary/english/reproducibilitea/ class=nav-link>ReproducibiliTea</a></li><li class=nav-item data-toggle=tooltip data-placement=right title='A minimum standard on a spectrum of activities ("reproducibility spectrum") for assessing the value or accuracy of scientific claims based on the original methods, data, and code. For instance, where the original researcher&#39;s data and computer codes are used to regenerate the results (Barba, 2018), often referred to as computational reproducibility. Reproducibility does not guarantee the quality, correctness, or validity of the published results (Peng, 2011). In some fields, this meaning is, instead, associated with the term “replicability” or ‘repeatability’.'><a href=https://forrt.org/glossary/english/reproducibility/ class=nav-link>Reproducibility</a></li><li class=nav-item data-toggle=tooltip data-placement=right title="The finding, and related shift in academic culture and thinking, that a large proportion of scientific studies published across disciplines do not replicate (e.g. Open Science Collaboration, 2015). This is considered to be due to a lack of quality and integrity of research and publication practices, such as publication bias, QRPs and a lack of transparency, leading to an inflated rate of false positive results. Others have described this process as a ‘Credibility revolution’ towards improving these practices."><a href=https://forrt.org/glossary/english/reproducibility_crisis/ class=nav-link>Reproducibility crisis (aka Replicability or replication crisis)</a></li><li class=nav-item data-toggle=tooltip data-placement=right title="A reproducibility network is a consortium of open research working groups, often peer-led. The groups operate on a wheel-and-spoke model across a particular country, in which the network connects local cross-disciplinary researchers, groups, and institutions with a central steering group, who also connect with external stakeholders in the research ecosystem. The goals of reproducibility networks include; advocating for greater awareness, promoting training activities, and disseminating best-practices at grassroots, institutional, and research ecosystem levels. Such networks exist in the UK, Germany, Switzerland, Slovakia, and Australia (as of March 2021)."><a href=https://forrt.org/glossary/english/reproducibility_network/ class=nav-link>Reproducibility Network</a></li><li class=nav-item data-toggle=tooltip data-placement=right title="Type of semantometric measure assessing similarity of publications connected in a citation network. This method uses a simple formula to assess authors’ contributions. Publication *p* can be estimated based on the semantic distance from the publications cited by *p* to publications citing *p*."><a href=https://forrt.org/glossary/english/research_contribution_metric/ class=nav-link>Research Contribution Metric (*p*)</a></li><li class=nav-item data-toggle=tooltip data-placement=right title="Describes the circular process of conducting scientific research, with “researchers working at various stages of inquiry, from more tentative and exploratory investigations to the testing of more definitive and well-supported claims” (Lieberman, 2020, p. 42). The cycle includes literature research and hypothesis generation, data collection and analysis, as well as dissemination of results (e.g. through publication in peer-reviewed journals), which again informs theory and new hypotheses/research."><a href=https://forrt.org/glossary/english/research_cycle/ class=nav-link>Research Cycle</a></li><li class=nav-item data-toggle=tooltip data-placement=right title="Research Data Management (RDM) is a broad concept that includes processes undertaken to create organized, documented, accessible, and reusable quality research data. Adequate research data management provides many benefits including, but not limited to, reduced likelihood of data loss, greater visibility and collaborations due to data sharing, demonstration of research integrity and accountability."><a href=https://forrt.org/glossary/english/research_data_management/ class=nav-link>Research Data Management</a></li><li class=nav-item data-toggle=tooltip data-placement=right title="Research integrity is defined by a set of good research practices based on fundamental principles: honesty, reliability, respect and accountability (ALLEA, 2017). Good research practices —which are based on fundamental principles of research integrity and should guide researchers in their work as well as in their engagement with the practical, ethical and intellectual challenges inherent in research— refer to areas such as: research environment (e.g., research institutions and organisations promote awareness and ensure a prevailing culture of research integrity), training, supervision and mentoring (e.g., Research institutions and organisations develop appropriate and adequate training in ethics and research integrity to ensure that all concerned are made aware of the relevant codes and regulations), research procedures (e.g., researchers report their results in a way that is compatible with the standards of the discipline and, where applicable, can be verified and reproduced), safeguards (e.g., researchers have due regard for the health, safety and welfare of the community, of collaborators and others connected with their research), data practices and management (e.g., researchers, research institutions and organisations provide transparency about how to access or make use of their data and research materials), collaborative working, publication and dissemination (e.g., authors and publishers consider negative results to be as valid as positive findings for publication and dissemination), reviewing, evaluating and editing (e.g., researchers review and evaluate submissions for publication, funding, appointment, promotion or reward in a transparent and justifiable manner)."><a href=https://forrt.org/glossary/english/research_integrity/ class=nav-link>Research integrity</a></li><li class=nav-item data-toggle=tooltip data-placement=right title="A detailed document prepared before conducting a study, often written as part of ethics and funding applications. The protocol should include information relating to the background, rationale and aims of the study, as well as hypotheses which reflect the researchers’ expectations. The protocol should also provide a “recipe” for conducting the study, including methodological details and clear analysis plans. Best practice guidelines for creating a study protocol should be used for specific methodologies and fields. It is possible to publically share research protocols to attract new collaborators or facilitate efficient collaboration across labs (e.g. [https://www.protocols.io/](https://www.protocols.io/)). In medical and educational fields, protocols are often a separate article type suitable for publication in journals. Where protocol sharing or publication is not common practice, researchers can choose preregistration."><a href=https://forrt.org/glossary/english/research_protocol/ class=nav-link>Research Protocol</a></li><li class=nav-item data-toggle=tooltip data-placement=right title="The process of conducting research from conceptualisation to dissemination. A typical workflow may look like the following: Starting with conceptualisation to identify a research question and design a study. After study design, researchers need to gain ethical approval (if necessary) and may decide to preregister the final version. Researchers then collect and analyse their data. Finally, the process ends with dissemination; moving between pre-print and post-print stages as the manuscript is submitted to a journal."><a href=https://forrt.org/glossary/english/research_workflow/ class=nav-link>Research workflow</a></li><li class=nav-item data-toggle=tooltip data-placement=right title="refers to the flexibility often inherent in the scientific process, from hypothesis generation, designing and conducting a research study to processing the data and analyzing as well as interpreting and reporting results. Due to a lack of precisely defined theories and/or empirical evidence, multiple decisions are often equally justifiable. The term is sometimes used to refer to the opportunistic (ab-)use of this flexibility aiming to achieve desired results —e.g., when in- or excluding certain data— albeit the fact that technically the term is not inherently value-laden."><a href=https://forrt.org/glossary/english/researcher_degrees_of_freedom/ class=nav-link>Researcher degrees of freedom</a></li><li class=nav-item data-toggle=tooltip data-placement=right title="An approach that considers societal implications and expectations, relating to research and innovation, with the aim to foster inclusivity and sustainability. It accounts for the fact that scientific endeavours are not isolated from their wider effects and that research is motivated by factors beyond the pursuit of knowledge. As such, many parties are important in fostering responsible research, including funding bodies, research teams, stakeholders, activists, and members of the public."><a href=https://forrt.org/glossary/english/responsible_research_and_innovation/ class=nav-link>Responsible Research and Innovation</a></li><li class=nav-item data-toggle=tooltip data-placement=right title="Exploiting researcher degrees of freedom during statistical analysis in order to increase the likelihood of accepting the null hypothesis (for instance, *p* \> .05)."><a href=https://forrt.org/glossary/english/reverse_p_hacking/ class=nav-link>Reverse p-hacking</a></li><li class=nav-item data-toggle=tooltip data-placement=right title="The RIOT Science Club is a multi-site seminar series that raises awareness and provides training in Reproducible, Interpretable, Open & Transparent science practices. It provides regular talks, workshops and conferences, all of which are openly available and rewatchable on the respective location’s websites and Youtube."><a href=https://forrt.org/glossary/english/riot_science_club/ class=nav-link>RIOT Science Club</a></li><li class=nav-item data-toggle=tooltip data-placement=right title="The persistence of support for a hypothesis under perturbations of the methodological/analytical pipeline In other words, applying different methods/analysis pipelines to examine if the same conclusion is supported under analytical different conditions."><a href=https://forrt.org/glossary/english/robustness/ class=nav-link>Robustness (analyses)</a></li><li class=nav-item data-toggle=tooltip data-placement=right title="A questionable research/reporting practice strategy, often done *post hoc*, to increase the number of publishable manuscripts by ‘slicing’ up the data from a single study \- one example of a method of ‘gaming the system’ of academic incentives. For instance, this may involve publishing multiple studies based on a single dataset, or publishing multiple studies from different data collection sites without transparently stating where the data originally derives from. Such practices distort the literature, and particularly meta-analyses, because it is unclear that the findings were obtained from the same dataset, thereby concealing the dependencies across the separately published papers."><a href=https://forrt.org/glossary/english/salami_slicing/ class=nav-link>Salami slicing</a></li><li class=nav-item data-toggle=tooltip data-placement=right title="The act of reporting or publishing a novel finding prior to another researcher/team. Survey-based research indicates that fear of being scooped is an important fear-related barrier for data sharing in psychology, and agent-based models suggest that competition for priority harms scientific reliability (Tiokhin et al. 2021)."><a href=https://forrt.org/glossary/english/scooping/ class=nav-link>Scooping</a></li><li class=nav-item data-toggle=tooltip data-placement=right title="A class of metrics for evaluating research using full publication text to measure semantic similarity of publications and highlighting an article’s contribution to the progress of scholarly discussion. It is an extension of tools such as bibliometrics, webometrics, and altmetrics."><a href=https://forrt.org/glossary/english/semantometrics/ class=nav-link>Semantometrics</a></li><li class=nav-item data-toggle=tooltip data-placement=right title="Research that poses a threat to those who are or have been involved in it, including the researchers, the participants, and the wider society. This threat can be physical danger (e.g. suicide) or a negative emotional response (e.g. depression) to those who are involved in the research process. For instance, research conducted on victims of suicide, the researcher might be emotionally traumatised by the descriptions of the suicidal behaviours. Indeed, the communication with the victims might also make them re-experience the traumatic memories, leading to negative psychological responses."><a href=https://forrt.org/glossary/english/sensitive_research/ class=nav-link>Sensitive research</a></li><li class=nav-item data-toggle=tooltip data-placement=right title="An authorship system that assigns authorship order based on the contribution of each author. The names of the authors are listed according to their contribution in descending order with the most contributing author first and the least contributing author last."><a href=https://forrt.org/glossary/english/sequence_determines_credit_approach/ class=nav-link>Sequence-determines-credit approach (SDC)</a></li><li class=nav-item data-toggle=tooltip data-placement=right title="Evaluation of research products by qualified experts where the reviewer(s) knows the identity of the author(s), but the reviewer(s) remains anonymous to the author(s)."><a href=https://forrt.org/glossary/english/single_blind_peer_review/ class=nav-link>Single-blind peer review</a></li><li class=nav-item data-toggle=tooltip data-placement=right title="Adopting Open Scholarship practices leads to a longer research process overall, with more focus on transparency, reproducibility, replicability and quality, over the quantity of outputs. Slow Science opposes publish-or-perish culture and describes an academic system that allows time and resources to produce fewer higher-quality and transparent outputs, for instance prioritising researcher time towards collecting more data, more time to read the literature, think about how their findings fit the literature and documenting and sharing research materials instead of running additional studies."><a href=https://forrt.org/glossary/english/slow_science/ class=nav-link>Slow science</a></li><li class=nav-item data-toggle=tooltip data-placement=right title="Social class is usually measured using both objective and subjective measurements, as recommended by the American Psychological Association (American Psychological Association,Task Force on Socioeconomic Status, 2007). Unlike the conventional concept, which only considers one factor, either education or income (e.g., economic variables), an individual's social class is considered to be a combination of their education, income, occupational prestige, subjective social status, and self-identified social class. Social class is partly a cultural variable, as it is a stable variable and likely to change slowly over the years. Social class can have important implications to academic outcomes. An individual may have a high socio-economic status yet identify as a working class individual. Working class students tend to have different life circumstances and often more restrictive commitments than middle-class students, which make their integration with other students more difficult (Rubin, 2021). The lack of time and money is obstructive to their social experience at university. Working class students are more likely to work to support themselves, resulting in less time for academic activities and for socializing with other students as well as less money to purchase items linked to social experiences (e.g. food)."><a href=https://forrt.org/glossary/english/social_class/ class=nav-link>Social class</a></li><li class=nav-item data-toggle=tooltip data-placement=right title="Social integration is a multi-dimensional construct. In an academic context, social integration is related to the quantity and quality of the social interactions with staff and students, as well as the sense of connection and belonging to the university and the people within the institute. To be more specific, social support, trust, and connectedness are all variables that contribute to social integration. Social integration has important implications for academic outcomes and mental wellbeing (Evans & Rubin, 2021). Working class students are less likely to integrate with other students, since they have differing social and economic backgrounds and less disposable income. Thus they are not able to experience as many educational and fiscal opportunities than others. In turn, this can lead to poor mental health and feelings of ostracism (Rubin, 2021)."><a href=https://forrt.org/glossary/english/social_integration/ class=nav-link>Social integration</a></li><li class=nav-item data-toggle=tooltip data-placement=right title="SORTEE ([https://www.sortee.org/](https://www.sortee.org/)) is an international society with the aim of improving the transparency and reliability of research results in the fields of ecology, evolution, and related disciplines through cultural and institutional changes. SORTEE was launched in December 2020 to anyone interested in improving research in these disciplines, regardless of experience. The society is international in scope, membership, and objectives. As of May 2021, SORTEE comprises of over 600 members."><a href=https://forrt.org/glossary/english/society_for_open_reliable_and_transparent_ecology_and_evolutionary_biology/ class=nav-link>Society for Open, Reliable, and Transparent Ecology and Evolutionary biology (SORTEE)</a></li><li class=nav-item data-toggle=tooltip data-placement=right title="A membership society founded to further promote improved methods and practices in the psychological research field. The society aims to complete its mission statement by enhancing the training of psychological researchers; by promoting research cultures that are more conducive to better quality research; by quantifying and empirically assessing the impact of such reforms; and by leading outreach events within and outside psychology to better the current state of research norms."><a href=https://forrt.org/glossary/english/society_for_the_improvement_of_psychological_science/ class=nav-link>Society for the Improvement of Psychological Science (SIPS)</a></li><li class=nav-item data-toggle=tooltip data-placement=right title="An analytic approach that consists of identifying, calculating, visualising and interpreting results (through inferential statistics) for *all* reasonable specifications for a particular research question (see Simonsohn et al. 2015). Specification curve analysis helps make transparent the influence of presumably arbitrary decisions during the scientific progress (e.g., experimental design, construct operationalization, statistical models or several of these) made by a researcher by comprehensively reporting all non-redundant, sensible tests of the research question. Voracek et al. (2019) suggest that SCA differs from multiverse analysis with regards to the graphical displays (a specification curve plot rather than a histogram and tile plot) and the use of inferential statistics to interpret findings."><a href=https://forrt.org/glossary/english/specification_curve_analysis/ class=nav-link>Specification Curve Analysis</a></li><li class=nav-item data-toggle=tooltip data-placement=right title="Analytical approaches and models assume certain characteristics of one’s data (e.g., statistical independence, random samples, normality, equal variance,...). Before running an analysis, these assumptions should be checked since their violation can change the results and conclusion of a study. Good practice in open and reproducible science is to report assumption testing in terms of the assumptions verified and the results of such checks or corrections applied."><a href=https://forrt.org/glossary/english/statistical_assumptions/ class=nav-link>Statistical Assumptions</a></li><li class=nav-item data-toggle=tooltip data-placement=right title="Statistical power is the long-run probability that a statistical test correctly rejects the null hypothesis if the alternative hypothesis is true. It ranges from 0 to 1, but is often expressed as a percentage. Power can be estimated using the significance criterion (alpha), effect size, and sample size used for a specific analysis technique. There are two main applications of statistical power. A priori power where the researcher asks the question “given an effect size, how many participants would I need for X% power?”. Sensitivity power asks the question “given a known sample size, what effect size could I detect with X% power?”."><a href=https://forrt.org/glossary/english/statistical_power/ class=nav-link>Statistical power</a></li><li class=nav-item data-toggle=tooltip data-placement=right title="A property of a result using Null Hypothesis Significance Testing (NHST) that, given a significance level, is deemed unlikely to have occurred given the null hypothesis. Tenny and Abdelgawad (2017) defined it as “a measure of the probability of obtaining your data or more extreme data assuming the null hypothesis is true, compared to a pre-selected acceptable level of uncertainty regarding the true answer” (p. 1). Conventions for determining the threshold vary between applications and disciplines but ultimately depend on the considerations of the researcher about an appropriate error margin. The American Statistical Association’s statement (Wasserstein & Lazar, 2016\) notes that “Researchers often wish to turn a p-value into a statement about the truth of a null hypothesis, or about the probability that random chance produced the observed data. The p-value is neither. It is a statement about data in relation to a specified hypothetical explanation, and is not a statement about the explanation itself” (p. 131)."><a href=https://forrt.org/glossary/english/statistical_significance/ class=nav-link>Statistical significance</a></li><li class=nav-item data-toggle=tooltip data-placement=right title="The extent to which conclusions from a statistical test are accurate and reflective of the true effect found in nature. In other words, whether or not a relationship exists between two variables and can be accurately detected with the conducted analyses. Threats to statistical validity include low power, violation of assumptions, reliability of measures, etc, affecting the reliability and generality of the conclusions."><a href=https://forrt.org/glossary/english/statistical_validity/ class=nav-link>Statistical validity</a></li><li class=nav-item data-toggle=tooltip data-placement=right title="The STRANGE “framework” is a proposal and series of questions to help animal behaviour researchers consider sampling biases when planning, performing and interpreting research with animals. STRANGE is an acronym highlighting several possible sources of sampling bias in animal research, such as the animals’ Social background; Trappability and self-selection; Rearing history; Acclimation and habituation; Natural changes in responsiveness; Genetic make-up, and Experience."><a href=https://forrt.org/glossary/english/strange/ class=nav-link>STRANGE</a></li><li class=nav-item data-toggle=tooltip data-placement=right title="A free online platform through which researchers post brief descriptions of research projects or resources that are available for use (“haves”) or that they require and another researcher may have (“needs”). StudySwap is a crowdsourcing approach to research which can ensure that fewer research resources go unused and more researchers have access to the resources they need."><a href=https://forrt.org/glossary/english/studyswap/ class=nav-link>StudySwap</a></li><li class=nav-item data-toggle=tooltip data-placement=right title="A form of literature review and evidence synthesis. A systematic review will usually include a thorough, repeatable (reproducible) search strategy including key terms and databases in order to find relevant literature on a given topic or research question. Systematic reviewers follow a process of screening the papers found through their search, until they have filtered down to a set of papers that fit their predefined inclusion criteria. These papers can then be synthesised in a written review which may optionally include statistical synthesis in the form of a meta-analysis as well. A systematic review should follow a standard set of guidelines to ensure that bias is kept to a minimum for example PRISMA (Moher et al., 2009; Page et al., 2021), Cochrane Systematic Reviews (Higgins et al., 2019), or NIRO-SR (Topor et al., 2021)."><a href=https://forrt.org/glossary/english/systematic_review/ class=nav-link>Systematic Review</a></li><li class=nav-item data-toggle=tooltip data-placement=right title="tenzing* is an online webapp and R package that helps researchers to track and report the contributions of each team member using the CRediT taxonomy in an efficient way. Team members of a research project can indicate their contributions to each CRediT role using an online spreadsheet template, and provide any additional authors' information (e.g., name, affiliation, order in publication, email address, and ORCID iD). Upon writing the manuscript, *tenzing* can automatically create a list of contributors belonging to each CRediT role to be included in the contributions section and create the manuscript’s title page."><a href=https://forrt.org/glossary/english/tenzing/ class=nav-link>Tenzing</a></li><li class=nav-item data-toggle=tooltip data-placement=right title="Described as a combination of low statistical power, a surprising result, and a *p*\-value only slightly lower than .05."><a href=https://forrt.org/glossary/english/the_troubling_trio/ class=nav-link>The Troubling Trio</a></li><li class=nav-item data-toggle=tooltip data-placement=right title="A theory is a unifying explanation or description of a process or phenomenon, which is amenable to repeated testing and verifiable through scientific investigation, using various experiments led by several independent researchers. Theories may be rejected or deemed an unsatisfactory explanation of a phenomenon after rigorous testing of a new hypothesis that explains the phenomena better or seems to contradict them but is more generalisable to a wider array of findings."><a href=https://forrt.org/glossary/english/theory/ class=nav-link>Theory</a></li><li class=nav-item data-toggle=tooltip data-placement=right title="The process of creating and developing a statement of concepts and their interrelationships to show how and/or why a phenomenon occurs. Theory building leads to theory testing."><a href=https://forrt.org/glossary/english/theory_building/ class=nav-link>Theory building</a></li><li class=nav-item data-toggle=tooltip data-placement=right title="Having one’s actions open and accessible for external evaluation. Transparency pertains to researchers being honest about theoretical, methodological, and analytical decisions made throughout the research cycle. Transparency can be usefully differentiated into “scientifically relevant transparency” and “socially relevant transparency”. While the former has been the focus of early Open Science discourses, the latter is needed to provide scientific information in ways that are relevant to decision makers and members of the public (Elliott & Resnik, 2019)."><a href=https://forrt.org/glossary/english/transparency/ class=nav-link>Transparency</a></li><li class=nav-item data-toggle=tooltip data-placement=right title="The transparency checklist is a consensus-based, comprehensive checklist that contains 36 items that cover the prepregistration, methods, results and discussion and data, code and materials availability. A shortened 12-item version of the checklist is also available. Checklist responses can be submitted alongside a manuscript for review. While the checklist can also work for educational purposes, it mainly aims to support researchers to identify concrete actions that can increase the transparency of their research while a disclosed checklist can help the readers and reviewers gain critical information about different aspects of transparency of the submitted research."><a href=https://forrt.org/glossary/english/transparency_checklist/ class=nav-link>Transparency Checklist</a></li><li class=nav-item data-toggle=tooltip data-placement=right title="Evaluation of research products by qualified experts where the author(s) are anonymous to both the reviewer(s) and editor(s). “Blinding of the authors and their affiliations to both editors and reviewers. This approach aims to eliminate institutional, personal, and gender biases” (Tvina et al., 2019, p. 1082)."><a href=https://forrt.org/glossary/english/triple_blind_peer_review/ class=nav-link>Triple-blind peer review</a></li><li class=nav-item data-toggle=tooltip data-placement=right title="A set of guiding principles that consider Transparency, Responsibility, User focus, Sustainability, and Technology (TRUST) as the essential components for assessing, developing, and sustaining the trustworthiness of digital data repositories (especially those that store research data). They are complementary to the FAIR Data Principles."><a href=https://forrt.org/glossary/english/trust_principles/ class=nav-link>TRUST Principles</a></li><li class=nav-item data-toggle=tooltip data-placement=right title="“Incorrect rejection of a null hypothesis” (Simmons et al., 2011, p. 1359), i.e. finding evidence to reject the null hypothesis that there is no effect when the evidence is actually in favouring of retaining the null that there is no effect (For example, a judge imprisoning an innocent person). Concluding that there is a significant effect and rejecting the null hypothesis when your findings actually occured by chance."><a href=https://forrt.org/glossary/english/type_i_error/ class=nav-link>Type I error</a></li><li class=nav-item data-toggle=tooltip data-placement=right title="A false negative result occurs when the alternative hypothesis is true in the population but the null hypothesis is accepted as part of the analysis (Hartgerink et al., 2017). That is, finding a non-significant statistical result when the effect is true (For example, a judge passing an innocent verdict on a guilty person). False negatives are less likely to be the subject of replications than positive results (Fiedler et al., 2012), and remain an unresolved issue in scientific research (Hartgerink et al., 2017)."><a href=https://forrt.org/glossary/english/type_ii_error/ class=nav-link>Type II error</a></li><li class=nav-item data-toggle=tooltip data-placement=right title="A Type M error occurs when a researcher concludes that an effect was observed with magnitude lower or higher than the real one. For example, a type M error occurs when a researcher claims that an effect of small magnitude was observed when it is large in truth or vice versa."><a href=https://forrt.org/glossary/english/type_m_error/ class=nav-link>Type M error</a></li><li class=nav-item data-toggle=tooltip data-placement=right title="A Type S error occurs when a researcher concludes that an effect was observed with an opposite sign than real one. For example, a type S error occurs when a researcher claims that a positive effect was observed when it is negative in reality or vice versa."><a href=https://forrt.org/glossary/english/type_s_error/ class=nav-link>Type S error</a></li><li class=nav-item data-toggle=tooltip data-placement=right title="Not all voices, perspectives, and members of the community are adequately represented. Under-representation typically occurs when the voices or perspectives of one group dominate, resulting in the marginalization of another. This often affects groups who are a minority in relation to certain personal characteristics."><a href=https://forrt.org/glossary/english/under_representation/ class=nav-link>Under-representation</a></li><li class=nav-item data-toggle=tooltip data-placement=right title="A framework for improving learning and optimising teaching based upon scientific insights of how humans learn. It aims to make learning inclusive and transformative for all people in which the focus is on catering to the differing needs of different students. It is often regarded as an evidence-based and scientifically valid framework to guide educational practice, consisting of three key principles: engagement, representation, and action and expression. In addition, UDL is included in the Higher Education Opportunity Act of 2008 (Edyburn, 2010)."><a href=https://forrt.org/glossary/english/universal_design_for_learning/ class=nav-link>Universal design for learning (UDL)</a></li><li class=nav-item data-toggle=tooltip data-placement=right title="Validity refers to the application of statistical principles to arrive at well-founded —i.e., likely corresponding accurately to the real world— concepts, conclusions or measurement. In psychometrics, validity refers to the extent to which something measures what it intends to or claims to measure. Under this generic term, there are different types of validity (e.g., internal validity, construct validity, face validity, criterion validity, diagnostic validity, discriminant validity, concurrent validity, convergent validity, predictive validity, external validity)."><a href=https://forrt.org/glossary/english/validity/ class=nav-link>Validity</a></li><li class=nav-item data-toggle=tooltip data-placement=right title="The practice of managing and recording changes to digital resources (e.g. files, websites, programmes, etc.) over time so that you can recall specific versions later. Version control systems are designed to record the history of changes (who, what and when), and help to avoid human errors (e.g. working on the wrong version). For example, the Git version control system is a widely used software tool that originally helped software developers to version control shared code and is now used across many scientific disciplines to manage and share files."><a href=https://forrt.org/glossary/english/version_control/ class=nav-link>Version control</a></li><li class=nav-item data-toggle=tooltip data-placement=right title="Webometrics involves the study of online content. Webometrics focuses on the numbers and types of hyperlinks between different online sites. Such approaches have been considered as a type of altmetrics. “The study of the quantitative aspects of the construction and use of information resources, structures and technologies on the Web drawing on [bibliometric](https://en.wikipedia.org/wiki/Bibliometrics) and [informetric](https://en.wikipedia.org/wiki/Informetrics) approaches” (Björneborn & Ingwersen, 2004)."><a href=https://forrt.org/glossary/english/webometrics/ class=nav-link>Webometrics</a></li><li class=nav-item data-toggle=tooltip data-placement=right title><a href=https://forrt.org/glossary/english/weird/ class=nav-link>WEIRD</a></li><li class=nav-item data-toggle=tooltip data-placement=right title="Computing a Z-score is a statistical approach mainly used to obtain the ‘Estimated Replication Rate’ (ERR) and ‘Expected Discovery Rate’ (EDR) for a set of reported studies. Calculating a *z*\-curve for a set of statistically significant studies involves converting reported *p*\-values to *z*\-scores, fitting a finite mixture model to the distribution of *z*\-scores, and estimating mean power based on the mixture model. The Z-curve analysis can be performed in R through a dedicated package \- https://cran.r-project.org/web/packages/zcurve/index.html."><a href=https://forrt.org/glossary/english/z_curve/ class=nav-link>Z-Curve</a></li><li class=nav-item data-toggle=tooltip data-placement=right title="An open science repository where researchers can deposit research papers, reports, data sets, research software, and any other research-related digital artifacts. Zenodo creates a persistent digital object identifier (DOI) for each submission to make it citable. This platform was developed under the European OpenAIRE program and operated by CERN."><a href=https://forrt.org/glossary/english/zenodo/ class=nav-link>Zenodo</a></li></ul></nav></div><main class="col-12 col-xl-8 pl-md-5 docs-content" role=main><article class=article><div class="docs-article-container mx-auto"><div class="article-container pt-3"><h1>Zenodo</h1><div class=article-metadata><span class=article-date>Last updated on
Feb 7, 2025</span></div></div><div class=language-switcher><strong>Also available in: </strong><a href=/glossary/arabic/zenodo/>Arabic</a> |
<a href=/glossary/german/zenodo/>German</a> |<br>&nbsp;</div><p class="pub-abstract border-bottom pb-1"><b>Definition: </b>An open science repository where researchers can deposit research papers, reports, data sets, research software, and any other research-related digital artifacts. Zenodo creates a persistent digital object identifier (DOI) for each submission to make it citable. This platform was developed under the European OpenAIRE program and operated by CERN.</p><div class=article-style><p class="mb-1 pr-2"><strong>Related terms: </strong><a href=/glossary/english/doi/>DOI (digital object identifier)</a>, figshare, <a href=/glossary/english/open_data/>Open data</a>, <a href=/glossary/english/open_science_framework/>Open Science Framework</a>, <a href=/glossary/english/preprint/>Preprint</a></p><p class="mb-1 pr-2"><b>Reference: </b><a href=https://www.zenodo.org target=_blank rel=noopener>www.zenodo.org</a></p><p class="mb-1 pr-2"><b>Drafted and Reviewed by: </b>Ali H. Al-Hoorie, Sara Middleton #### # {#heading} # References {#references} Abele-Brehm, A. E., Gollwitzer, M., Steinberg, U., & Schönbrodt, F. D. (2019). Attitudes toward open science and public data sharing. <em>Social Psychology, 50</em>, 252-260.
<a href=https://doi.org/10.1027/1864-9335/a000384 target=_blank rel=noopener>https://doi.org/10.1027/1864-9335/a000384</a> Aczel, B., Szaszi, B., Nilsonne, G., Van den Akker, O., Albers, C. J., van Assen, M. A. L. M., … Wagenmakers, E. (2021, April 21). Guidance for Multi-Analyst Studies.
<a href=https://doi.org/10.31222/osf.io/5ecnh target=_blank rel=noopener>https://doi.org/10.31222/osf.io/5ecnh</a> Aczel, B., Szaszi, B., Sarafoglou, A., Kekecs, Z., Kucharský, Š., Benjamin, D., &mldr; & Wagenmakers, E. J. (2020). A consensus-based transparency checklist. <em>Nature Human Behaviour, 4</em>(1), 4-6.
<a href=https://doi.org/10.1038/s41562-019-0772-6 target=_blank rel=noopener>https://doi.org/10.1038/s41562-019-0772-6</a> Albayrak, N. (2018a). Diversity helps but decolonisation is the key to equality in higher education.
<a href=https://lsepgcertcitl.wordpress.com/2018/04/16/diversity-helps-but-decolonisation-is-the-key-to-equality-in-higher-education/ target=_blank rel=noopener>https://lsepgcertcitl.wordpress.com/2018/04/16/diversity-helps-but-decolonisation-is-the-key-to-equality-in-higher-education/</a> Albayrak, N. (2018b). Academics’ role on the future of higher education: Important but unrecognised.
<a href=https://lsepgcertcitl.wordpress.com/2018/11/29/academics-role-on-the-future-of-higher-education-important-but-unrecognised/ target=_blank rel=noopener>https://lsepgcertcitl.wordpress.com/2018/11/29/academics-role-on-the-future-of-higher-education-important-but-unrecognised/</a> Albayrak, N., & Okoroji, C. (2019). Facing the challenges of postgraduate study as a minority student. <em>A Guide for Psychology Postgraduates</em>, 63. Albayrak-Aydemir, N. (2020). The hidden costs of being a scholar from the global south. <em>Higher Education Across Borders (LSE Blog).</em>
<a href=https://blogs.lse.ac.uk/highereducation/2020/02/20/the-hidden-costs-of-being-a-scholar-from-the-global-south/ target=_blank rel=noopener>https://blogs.lse.ac.uk/highereducation/2020/02/20/the-hidden-costs-of-being-a-scholar-from-the-global-south/</a> ALLEA - All European Academies (2017). The European Code of Conduct for Research Integrity. Revised Edition. Available at:
<a href=https://allea.org/code-of-conduct/ target=_blank rel=noopener>https://allea.org/code-of-conduct/</a> Allen, L., & McGonagle-O’Connell, A. (n.d.). <em>CRediT – Contributor Roles Taxonomy.</em> CASRAI.
<a href=https://casrai.org/credit/ target=_blank rel=noopener>https://casrai.org/credit/</a> Anderson, M.S., Ronning, E.A., Devries, R., & Martinson, B.C. (2010). Extending the Mertonian norms: Scientists’ subscription to norms of research. <em>Journal of Higher Education, 81</em>(3), 366–393.
<a href=https://doi.org/10.1353/jhe.0.0095 target=_blank rel=noopener>https://doi.org/10.1353/jhe.0.0095</a>. Andersson, N. (2018). Participatory research—a modernizing science for primary health care. <em>Journal of General and Family Medicine</em>, <em>19</em>(5): 154–159.
<a href=https://doi.org/10.1002/jgf2.187 target=_blank rel=noopener>https://doi.org/10.1002/jgf2.187</a> Angrist, J. D., & Pischke, J. S. (2010). The credibility revolution in empirical economics: How better research design is taking the con out of econometrics. <em>Journal of Economic Perspectives, 24</em>, 3-30.
<a href=https://doi.org/10.1257/jep.24.2.3 target=_blank rel=noopener>https://doi.org/10.1257/jep.24.2.3</a>. Anon (n.d.). <em>About CC Licenses.</em> Creative Commons.
<a href=https://creativecommons.org/about/cclicenses/ target=_blank rel=noopener>https://creativecommons.org/about/cclicenses/</a> Anon. (n.d.). <em>Ckan</em>.
<a href=https://ckan.org/ target=_blank rel=noopener>https://ckan.org/</a> Anon. (2006). Correction or retraction?. <em>Nature, 444</em>, 123–124.
<a href=https://doi.org/10.1038/444123b target=_blank rel=noopener>https://doi.org/10.1038/444123b</a> Anon (n.d.). <em>Datacite Metadata Schema.</em> Datacite Schema.
<a href=https://schema.datacite.org/ target=_blank rel=noopener>https://schema.datacite.org/</a> Anon. (n.d.). Domov | SKRN (Slovak Reproducibility network). SKRN.
<a href=https://slovakrn.wixsite.com/skrn target=_blank rel=noopener>https://slovakrn.wixsite.com/skrn</a> Anon (n.d.). <em>Home | re3data.org</em>. Registry of Research Data Repositories. Retrieved 6 June 2021, from
<a href=https://www.re3data.org/ target=_blank rel=noopener>https://www.re3data.org/</a> Anon (n.d.). <em>INVOLVE – INVOLVE Supporting public involvement in NHS, public health and social care research</em>. INVOLVE.
<a href=https://www.invo.org.uk/ target=_blank rel=noopener>https://www.invo.org.uk/</a> Anon. (n.d.). <em>Licenses & Standards | Open Source Initiative</em>. OpenSource.Com.
<a href=https://opensource.org/licenses target=_blank rel=noopener>https://opensource.org/licenses</a> Anon (n.d.). <em>Open Source in Open Science | FOSTER</em>. Foster.
<a href=https://www.fosteropenscience.eu/foster-taxonomy/open-source-open-science target=_blank rel=noopener>https://www.fosteropenscience.eu/foster-taxonomy/open-source-open-science</a> Anon. (2019). <em>The DOI Handbook.</em> DOI.
<a href=https://www.doi.org/hb.html target=_blank rel=noopener>https://www.doi.org/hb.html</a> Anon (n.d.). <em>Welcome to Sherpa Romeo - v2.sherpa</em>. Sherpa Romeo.
<a href=https://v2.sherpa.ac.uk/romeo/ target=_blank rel=noopener>https://v2.sherpa.ac.uk/romeo/</a> Anon (n.d.). <em>What is a codebook?</em> ICPSR.
<a href=https://www.icpsr.umich.edu/icpsrweb/content/shared/ICPSR/faqs/what-is-a-codebook.html target=_blank rel=noopener>https://www.icpsr.umich.edu/icpsrweb/content/shared/ICPSR/faqs/what-is-a-codebook.html</a> Anon.(2009-2020). <em>What is a digital object identifier, or DOI?</em> American Psychological Association. h
<a href=https://apastyle.apa.org/learn/faqs/what-is-doi target=_blank rel=noopener>ttps://apastyle.apa.org/learn/faqs/what-is-doi</a> Anon. (n.d.). <em>What is a reporting guideline?</em> Equator Network.
<a href=https://www.equator-network.org/about-us/what-is-a-reporting-guideline/ target=_blank rel=noopener>https://www.equator-network.org/about-us/what-is-a-reporting-guideline/</a> Anon (2021). <em>What is impact?</em> The Economic and Social Research Council.
<a href=https://esrc.ukri.org/research/impact-toolkit/what-is-impact/ target=_blank rel=noopener>https://esrc.ukri.org/research/impact-toolkit/what-is-impact/</a> Anon. (n.d.). <em>What is open education?</em> Opensource.Com.
<a href=https://opensource.com/resources/what-open-education target=_blank rel=noopener>https://opensource.com/resources/what-open-education</a> Arslan, R. C. (2019). How to Automatically Document Data With the codebook Package to Facilitate Data Reuse. <em>Advances in Methods and Practices in Psychological Science, 2</em>(2), 169–187.
<a href=https://doi.org/10.1177/2515245919838783 target=_blank rel=noopener>https://doi.org/10.1177/2515245919838783</a> Arts and Humanities Research Council. (n.d.). <em>Definition of eligibility for funding</em>. Arts and Humanities Research Council. Available at:
<a href=https://ahrc.ukri.org/skills/earlycareerresearchers/definitionofeligibility/ target=_blank rel=noopener>https://ahrc.ukri.org/skills/earlycareerresearchers/definitionofeligibility/</a> Aspers, P., & Corte, U. (2019). What is qualitative in qualitative research. <em>Qualitative Sociology, 42</em>(2), 139-160.
<a href=https://doi.org/10.1007/s11133-019-9413-7 target=_blank rel=noopener>https://doi.org/10.1007/s11133-019-9413-7</a> AusRN. (n.d.). <em>Australian Reproducibility Network.</em> Retrieved 5 June 2021, from
<a href=https://www.aus-rn.org/ target=_blank rel=noopener>https://www.aus-rn.org/</a> Banks, G. C., Rogelberg, S. G., Woznyj, H. M., Landis, R. S., & Rupp, D. E. (2016). Editorial: Evidence on questionable research practices: The good, the bad, and the ugly. <em>Journal of Business and Psychology, 31</em>(3), 323–338.
<a href=https://doi.org/10.1007/s10869-016-9456-7 target=_blank rel=noopener>https://doi.org/10.1007/s10869-016-9456-7</a> Barba, L. A. (2018). Terminologies for reproducible research. arXiv preprint arXiv:1802.03311. Bardsley, N. (2018) What lessons does the “replication crisis” in psychology hold for experimental economics? In: <em>Handbook of Psychology and Economic Behaviour. 2nd edition. Cambridge Handbooks in Psychology.</em> Cambridge University Press. ISBN 9781107161399 Available at
<a href=http://centaur.reading.ac.uk/69874/ target=_blank rel=noopener>http://centaur.reading.ac.uk/69874/</a> Bartoš, F., & Schimmack, U. (2020). Z-Curve 2.0: Estimating replication rates and discovery rates.
<a href=https://doi.org/10.31234/osf.io/urgtn target=_blank rel=noopener>https://doi.org/10.31234/osf.io/urgtn</a> Bateman, I., Kahneman, D., Munro, A., Starmer, C., & Sugden, R. (2005). Testing competing models of loss aversion: An adversarial collaboration. <em>Journal of Public Economics, 89</em>(8), 1561-1580.
<a href=https://doi.org/10.1016/j.jpubeco.2004.06.013 target=_blank rel=noopener>https://doi.org/10.1016/j.jpubeco.2004.06.013</a> Baturay, M. H. (2015). An overview of the world of MOOCs. <em>Procedia-Social and Behavioral Sciences, 174,</em> 427-433.
<a href=https://doi.org/10.1016/j.sbspro.2015.01.685 target=_blank rel=noopener>https://doi.org/10.1016/j.sbspro.2015.01.685</a> Bazeley, P. (2003). Defining &lsquo;Early Career&rsquo; in Research. <em>Higher Education 45</em>, 257–279
<a href=https://doi.org/10.1023/A:1022698529612 target=_blank rel=noopener>https://doi.org/10.1023/A:1022698529612</a> Beffara Bret, B., Beffara Bret, A., & Nalborczyk, L. (2021). A fully automated, transparent, reproducible, and blind protocol for sequential analyses. Meta-Psychology, 5.
<a href=https://doi.org/10.15626/MP.2018.869 target=_blank rel=noopener>https://doi.org/10.15626/MP.2018.869</a> Behrens, J. T. (1997). Principles and procedures of exploratory data analysis. <em>Psychological Methods, 2</em>(2), 131-160.
<a href=https://doi.org/10.1037/1082-989X.2.2.131 target=_blank rel=noopener>https://doi.org/10.1037/1082-989X.2.2.131</a> Beller, S., & Bender, A. (2017). Theory, the final frontier? A corpus-based analysis of the role of theory in psychological articles. <em>Frontiers in Psychology, 8</em>, 951.
<a href=https://doi.org/10.3389/fpsyg.2017.00951 target=_blank rel=noopener>https://doi.org/10.3389/fpsyg.2017.00951</a> Benoit, K., Conway, D., Lauderdale, B. E., Laver, M., & Mikhaylov, S. (2016). Crowd-sourced text analysis: Reproducible and agile production of political data. A<em>merican Political Science Review, 110</em>(2), 278–295.
<a href=https://doi.org/10.1017/S0003055416000058 target=_blank rel=noopener>https://doi.org/10.1017/S0003055416000058</a> Bhopal, R., Rankin, J., McColl, E., Thomas, L., Kaner, E., Stacy, R., Pearson, P., Vernon, B., & Rodgers, H. (1997). The vexed question of authorship: views of researchers in a British medical faculty. <em>BMJ, 314,</em> 1009-1012.
<a href=https://doi.org/10.1136/bmj.314.7086.1009 target=_blank rel=noopener>https://doi.org/10.1136/bmj.314.7086.1009</a> BIDS (n.d.). <em>Modality agnostic files.</em> Brain Imaging Data Structure.
<a href=https://bids-specification.readthedocs.io/en/stable/03-modality-agnostic-files.html target=_blank rel=noopener>https://bids-specification.readthedocs.io/en/stable/03-modality-agnostic-files.html</a> BIDS. (2020). <em>About BIDS</em>. Brain Imaging Data Structure.
<a href=https://bids.neuroimaging.io/ target=_blank rel=noopener>https://bids.neuroimaging.io</a> Bilder, G. (2013). <em>DOIs unambiguously and persistently identify published, trustworthy, citable online scholarly literature. Right?</em> Crossref.
<a href=https://www.crossref.org/blog/dois-unambiguously-and-persistently-identify-published-trustworthy-citable-online-scholarly-literature-right/ target=_blank rel=noopener>https://www.crossref.org/blog/dois-unambiguously-and-persistently-identify-published-trustworthy-citable-online-scholarly-literature-right/</a> Bishop, D. V. (2020). The psychology of experimental psychologists: Overcoming cognitive constraints to improve research: The 47th Sir Frederic Bartlett Lecture. <em>Quarterly Journal of Experimental Psychology, 73</em>(1), 1-19.
<a href=https://doi.org/10.1177/1747021819886519 target=_blank rel=noopener>https://doi.org/10.1177/1747021819886519</a> Björneborn, L., & Ingwersen, P. (2004). Toward a basic framework for webometrics. <em>Journal of the American society for information science and technology, 55</em>(14), 1216-1227.https://doi.org/10.1002/asi.20077 Blohowiak, B. B., Cohoon, J., de-Wit, L., Eich, E., Farach, F. J., Hasselman, F., … Riss, C. (2020, July 4). Badges to Acknowledge Open Practices. Retrieved from osf.io/tvyxz BMJ. (2015). <em>Introducing ‘How to write and publish a Study Protocol’ using BMJ’s new eLearning programme: Research to Publication.</em> Retrieved, March 2021, from:
<a href=https://blogs.bmj.com/bmjopen/2015/09/22/introducing-how-to-write-and-publish-a-study-protocol-using-bmjs-new-elearning-programme-research-to-publication/ target=_blank rel=noopener>https://blogs.bmj.com/bmjopen/2015/09/22/introducing-how-to-write-and-publish-a-study-protocol-using-bmjs-new-elearning-programme-research-to-publication/</a> Boivin, A., Richards, T., Forsythe, L., Gregoire, A., L’Esperance, A., Abelson, J., & Carman, K.L. (2018). Evaluating the patient and public involvement in research. <em>British Medical Journal, 363</em>, k5147.
<a href=https://doi.org/10.1136/bmj.k5147 target=_blank rel=noopener>https://doi.org/10.1136/bmj.k5147</a> Bol, T., de Vaan, M., & van de Rijt, A. (2018). The Matthew effect in science funding. <em>Proceedings of the National Academy of Sciences, 115</em>(19), 4887-4890.
<a href=https://doi.org/10.1073/pnas.1719557115 target=_blank rel=noopener>https://doi.org/10.1073/pnas.1719557115</a> Bollen, K. A. (1989). <em>Structural Equations with Latent Variables</em> (pp. 179-225). John Wiley & Sons. Borenstein, M., Hedges, L. V., Higgins, J. P., & Rothstein, H. R. (2011). <em>Introduction to meta-analysis.</em> John Wiley & Sons. Bornmann, L., Ganser, C., Tekles, A., & Leydesdorff, L. (2019). Does the $ h_\alpha $ index reinforce the Matthew effect in science? Agent-based simulations using Stata and R. <em>arXiv preprint arXiv:1905.11052.</em> Borsboom, D., Mellenbergh, G. J., & Van Heerden, J. (2004). The concept of validity. <em>Psychological review, 111</em>(4), 1061.
<a href=https://doi.org/10.1037/0033-295X.111.4.1061 target=_blank rel=noopener>https://doi.org/10.1037/0033-295X.111.4.1061</a> Borsboom, D., van der Maas, H., Dalege, J., Kievit, R., & Haig, B. (2020, February 29). Theory Construction Methodology: A practical framework for theory formation in psychology.
<a href=https://doi.org/10.31234/osf.io/w5tp8 target=_blank rel=noopener>https://doi.org/10.31234/osf.io/w5tp8</a> Bourne, P. E., Polka, J. K., Vale, R. D., & Kiley, R. (2017). Ten simple rules to consider regarding preprint submission.<em>PLoS Computational Biology, 13</em>(5), e1005473.
<a href=https://doi.org/10.1371/journal.pcbi.1005473 target=_blank rel=noopener>https://doi.org/10.1371/journal.pcbi.1005473</a> Box, G.E. P. (1976). Science and statistics. J<em>ournal of the American Statistical Association 71</em>(356), 791–799. Bouvy, J. C., & Mujoomdar, M. (2019). All-Male Panels and Gender Diversity of Issue Panels and Plenary Sessions at ISPOR Europe. <em>PharmacoEconomics-open</em>, <em>3</em>(3), 419-422.
<a href=https://doi.org/10.1007/s41669-019-0153-0 target=_blank rel=noopener>https://doi.org/10.1007/s41669-019-0153-0</a> Bramoullé, Y., & Saint-Paul, G. (2010). Research cycles. <em>Journal of economic theory, 145</em>(5), 1890-1920.
<a href=https://doi.org/10.2139/ssrn.965816 target=_blank rel=noopener>https://doi.org/10.2139/ssrn.965816</a> Brand, A., Allen, L., Altman, M., Hlava, M., & Scott, J. (2015). Beyond authorship: attribution, contribution, collaboration, and credit. <em>Learned Publishing, 28</em>(2), 151-155.
<a href=https://doi.org/10.1087/20150211 target=_blank rel=noopener>https://doi.org/10.1087/20150211</a> Brandt, M. J., IJzerman, H., Dijksterhuis, A., Farach, F. J., Geller, J., Giner-Sorolla, R., &mldr; & Van&rsquo;t Veer, A. (2014). The replication recipe: What makes for a convincing replication?. <em>Journal of Experimental Social Psychology, 50</em>, 217-224.
<a href=https://doi.org/10.1016/j.jesp.2013.10.005 target=_blank rel=noopener>https://doi.org/10.1016/j.jesp.2013.10.005</a> Braun, V., & Clarke, V. (2013) <em>Successful Qualitative Research.</em> SAGE Publications. Brembs, B., Button, K., & Munafò, M. (2013). Deep impact: unintended consequences of journal rank. <em>Frontiers in Human Neuroscience, 7</em>, 291.
<a href=https://doi.org/10.3389/fnhum.2013.00291 target=_blank rel=noopener>https://doi.org/10.3389/fnhum.2013.00291</a> Breznau, N. (2021). I saw you in the crowd: Credibility, reproducibility, and meta-utility. <em>PS: Political Science & Politics, 54</em>(2), 309-313.
<a href=https://doi.org/10.1017/S1049096520000980 target=_blank rel=noopener>https://doi.org/10.1017/S1049096520000980</a> Brod, M., Tesler, L., & Christensen, T. (2009). Qualitative research and content validity: Developing best practices based on science and experience. <em>Quality of Life Research, 18</em>(9), 1263–1278.
<a href=https://doi.org/10.1007/s11136-009-9540-9 target=_blank rel=noopener>https://doi.org/10.1007/s11136-009-9540-9</a> Brooks, T. A. (1985). Private acts and public objects: An investigation of citer motivations. <em>Journal of the American Society for Information Science, 36</em>(4), 223-229.
<a href=https://doi.org/10.1002/asi.4630360402 target=_blank rel=noopener>https://doi.org/10.1002/asi.4630360402</a> Brunner, J., & Schimmack, U. (2020). Estimating population mean power under conditions of heterogeneity and selection for significance. <em>Meta-Psychology, 4</em>, MP.2018.874.
<a href=https://doi.org/1[0.15626/MP.2018.874]%28https://doi.org/10.15626/MP.2018.874%29 target=_blank rel=noopener>https://doi.org/1[0.15626/MP.2018.874](https://doi.org/10.15626/MP.2018.874)</a> Bruns, S. B., & Ioannidis, J. P. (2016). P-curve and p-hacking in observational research. <em>PLoS ONE, 11</em>(2), e0149144.
<a href=https://doi.org/10.1371/journal.pone.0149144 target=_blank rel=noopener>https://doi.org/10.1371/journal.pone.0149144</a> Budapest Open Access Initiative (2002) <em>Read the Budapest open access initiative.</em> Budapest, Hungary. Available from:
<a href=https://www.budapestopenaccessinitiative.org/read target=_blank rel=noopener>https://www.budapestopenaccessinitiative.org/read</a> Burnette, M., Williams, S., & Imker, H. (2016). From Plan to Action: Successful Data Management Plan Implementation in a Multidisciplinary Project. <em>Journal of eScience librarianship, 5</em>(1), e1101.
<a href=https://doi.org/10.7191/jeslib.2016.1101 target=_blank rel=noopener>https://doi.org/10.7191/jeslib.2016.1101</a> Button, K. S., Chambers, C. D., Lawrence, N., & Munafò, M. R. (2020). Grassroots training for reproducible science: a consortium-based approach to the empirical dissertation. <em>Psychology Learning & Teaching, 19</em>(1), 77-90.
<a href=https://doi.org/10.1177/1475725719857659 target=_blank rel=noopener>https://doi.org/10.1177/1475725719857659</a> Button, K. S., Lawrence, N. S., Chambers, C. D., & Munafò, M. R. (2016). Instilling scientific rigour at the grassroots. <em>Psychologist, 29</em>(3), 158-159. Byrne J. A. & Christopher J. (2020). Digital magic, or the dark arts of the 21st century—how can journals and peer reviewers detect manuscripts and publications from paper mills? <em>FEBS Lett, 594</em>(4), 583-589.
<a href=https://doi.org/10.1002/1873-3468.13747 target=_blank rel=noopener>https://doi.org/10.1002/1873-3468.13747</a> Campbell, D. T., & Stanley, J.C. (1966) <em>Experimental and Quasi Experimental Designs.</em> Rand McNally. Carp, J. (2012). On the plurality of (methodological) worlds: estimating the analytic flexibility of FMRI experiments. <em>Frontiers in Neuroscience, 6</em>, 149.
<a href=https://doi.org/10.3389/fnins.2012.00149 target=_blank rel=noopener>https://doi.org/10.3389/fnins.2012.00149</a> Carsey, T. M. (2014). Making DA-RT a reality. <em>PS: Political Science & Politics, 47</em>(1), 72–77.
<a href=https://doi.org/10.1017/S1049096513001753 target=_blank rel=noopener>https://doi.org/10.1017/S1049096513001753</a> Carter, A., Tilling, K., & Munafo, M. R. (2021, January 26). Considerations of sample size and power calculations given a range of analytical scenarios.
<a href=https://doi.org/10.31234/osf.io/tcqrn target=_blank rel=noopener>https://doi.org/10.31234/osf.io/tcqrn</a> Case, C. M. (1928). Scholarship in Sociology. <em>Sociology and Social Research, 12</em>, 323–340.
<a href=http://www.sudoc.fr/036493414 target=_blank rel=noopener>http://www.sudoc.fr/036493414</a> Cassidy, S. A., Dimova, R., Giguère, B., Spence, J. R., & Stanley, D. J. (2019). Failing grade: 89% of introduction-to-psychology textbooks that define or explain statistical significance do so incorrectly. <em>Advances in Methods and Practices in Psychological Science, 2</em>(3), 233-239.
<a href=https://doi.org/10.1177/2515245919858072 target=_blank rel=noopener>https://doi.org/10.1177/2515245919858072</a> Centre for Evaluation. (n.d.). <em>Evidence Synthesis.</em>
<a href=https://www.lshtm.ac.uk/research/centres/centre-evaluation/evidence-synthesis target=_blank rel=noopener>https://www.lshtm.ac.uk/research/centres/centre-evaluation/evidence-synthesis</a> Centre for Open Science. (2011-2021) <em>Open Science Framework</em>. Centre for Open Science.
<a href=https://osf.io/ target=_blank rel=noopener>https://osf.io/</a> Centre for Open Science. (n.d.). S<em>how Your Work. Share Your Work.</em> Advance Science. That&rsquo;s Open Science. The Centre for Open Science.
<a href=https://www.cos.io/ target=_blank rel=noopener>https://www.cos.io/</a> CESSDA Training Team (2017 - 2020). C<em>ESSDA Data Management Expert Guide. B</em>ergen, Norway: CESSDA ERIC. Retrieved from
<a href=https://www.cessda.eu/DMGuide target=_blank rel=noopener>https://www.cessda.eu/DMGuide</a> Chambers, C. D. (2013). Registered reports: a new publishing initiative at Cortex. C<em>ortex, 49</em>(3), 609-610.
<a href=https://doi.org/10.1016/j.cortex.2012.12.016 target=_blank rel=noopener>https://doi.org/10.1016/j.cortex.2012.12.016</a>. Chambers, C. D., Dienes, Z., McIntosh, R. D., Rotshtein, P., & Willmes, K. (2015). Registered reports: realigning incentives in scientific publishing. <em>Cortex, 66</em>, A1-A2.
<a href=https://doi.org/10.1016/j.cortex.2015.03.022 target=_blank rel=noopener>https://doi.org/10.1016/j.cortex.2015.03.022</a>. Chambers, C. D., & Tzavella, L. (2020, February 10). Registered Reports: Past, Present and Future.
<a href=https://doi.org/10.31222/osf.io/43298 target=_blank rel=noopener>https://doi.org/10.31222/osf.io/43298</a> Chartier, C. R., Riegelman, A., & McCarthy, R. J. (2018). StudySwap: A platform for interlab replication, collaboration, and resource exchange. <em>Advances in Methods and Practices in Psychological Science, 1</em>(4), 574-579.
<a href=https://doi.org/10.1177/2515245918808767 target=_blank rel=noopener>https://doi.org/10.1177/2515245918808767</a> Chuard, P. J. C., Vrtilek, M., Head, M. L., & Jennions, M. D. (2019). Evidence that non-significant results are sometimes preferred: Reverse P-hacking or selective reporting? <em>PLoS Biol 17</em>(1), e3000127.
<a href=https://doi.org/10.1371/journal.pbio.3000127 target=_blank rel=noopener>https://doi.org/10.1371/journal.pbio.3000127</a> Citizen Science Association (2015). <em>Who We Are.</em> Citizen Science.
<a href=https://www.citizenscience.org/about-3/ target=_blank rel=noopener>https://www.citizenscience.org/about-3/</a> Claerbout, J. F., & Karrenbach, M. (1992). Electronic documents give reproducible research a new meaning. In <em>SEG Technical Program Expanded Abstracts 1992</em> (pp. 601-604). Society of Exploration Geophysicists. Available at
<a href="http://sepwww.stanford.edu/doku.php?id=sep:research:reproducible:seg92" target=_blank rel=noopener>http://sepwww.stanford.edu/doku.php?id=sep:research:reproducible:seg92</a> Clark, H., Elsherif, M. M., & Leavens, D. A. (2019). Ontogeny vs. phylogeny in primate/canid comparisons: a meta-analysis of the object choice task. <em>Neuroscience & Biobehavioral Reviews, 105,</em> 178-189.
<a href=https://doi.org/10.1016/j.neubiorev.2019.06.001 target=_blank rel=noopener>https://doi.org/10.1016/j.neubiorev.2019.06.001</a> Cohen, J. (1962). The statistical power of abnormal-social psychological research: A review. <em>The Journal of Abnormal and Social Psychology, 65</em>(3), 145–153.
<a href=https://doi.org/10.1037/h0045186 target=_blank rel=noopener>https://doi.org/10.1037/h0045186</a> Cohen, J. (1969). <em>Statistical power analysis for the behavioral sciences.</em> Academic Press. Cohn, J. P. (2008). Citizen science: Can volunteers do real research?. <em>BioScience, 58</em>(3), 192-197.
<a href=https://doi.org/10.1641/B580303 target=_blank rel=noopener>https://doi.org/10.1641/B580303</a> Coles, N. A., Tiokhin, L., Arslan, R., Forscher, P., Scheel, A., & Lakens, D. (2020, May 11). <em>Red Team Challenge.</em>
<a href=http://daniellakens.blogspot.com/2020/05/red-team-challenge.html target=_blank rel=noopener>http://daniellakens.blogspot.com/2020/05/red-team-challenge.html</a> Committee on Reproducibility and Replicability in Science, Board on Behavioral, Cognitive, and Sensory Sciences, Committee on National Statistics, Division of Behavioral and Social Sciences and Education, Nuclear and Radiation Studies Board, Division on Earth and Life Studies, … National Academies of Sciences, Engineering, and Medicine. (2019). Reproducibility and Replicability in Science (p. 25303). National Academies Press.
<a href=https://doi.org/10.17226/25303 target=_blank rel=noopener>https://doi.org/10.17226/25303</a> Cook, T. D., & Campbell, D. T. (1979). <em>Quasi-Experimentation.</em> Rand McNally. Coproduction Collective (2021). <em>Our approach.</em>
<a href=https://www.coproductioncollective.co.uk/what-is-co-production/our-approach target=_blank rel=noopener>https://www.coproductioncollective.co.uk/what-is-co-production/our-approach</a> Corley, K. G., & Gioia, D. A. (2011). Building theory about theory building: what constitutes a theoretical contribution?. <em>Academy of management review, 36</em>(1), 12-32.
<a href=https://doi.org/10.5465/amr.2009.0486 target=_blank rel=noopener>https://doi.org/10.5465/amr.2009.0486</a> Cornell University (2020). <em>Measuring your research impact: i10 index.</em> Cornell University Library.
<a href=https://guides.library.cornell.edu/impact/author-impact-10 target=_blank rel=noopener>https://guides.library.cornell.edu/impact/author-impact-10</a> Corti, L., Van den Eynden, V., Bishop, L., & Woollard, M. (2019). <em>Managing and sharing research data: a guide to good practice.</em> Sage. Cowan, N., Belletier, C., Doherty, J. M., Jaroslawska, A. J., Rhodes, S., Forsberg, A., &mldr; & Logie, R. H. (2020). How do scientific views change? Notes from an extended adversarial collaboration. <em>Perspectives on Psychological Science, 15</em>(4), 1011-1025.
<a href=https://doi.org/10.1177/1745691620906415 target=_blank rel=noopener>https://doi.org/10.1177/1745691620906415</a> Crenshaw, K. W. (1989). Demarginalizing the Intersection of Race and Sex: A Black Feminist Critique of Antidiscrimination Doctrine. <em>University of Chicago Legal Forum, 1989</em> (8), 139–168. Cronbach, L. J., & Meehl, P. E. (1955). Construct validity in psychological tests. P<em>sychological Bulletin, 52</em>(4), 281–302.
<a href=https://psycnet.apa.org/doi/10.1037/h0040957 target=_blank rel=noopener>https://doi.org/10.1037/h0040957</a> Cronin, B. (2001). Hyperauthorship: A postmodern perversion or evidence of a structural shift in scholarly communication practices? <em>Journal of the American Society for Information Science and Technology</em>, 52(7), 558–569.
<a href=https://doi.org/10.1002/asi.1097 target=_blank rel=noopener>https://doi.org/10.1002/asi.1097</a> Crowdsourcing Week. (2021, April 29). <em>What is Crowdsourcing?</em>
<a href=https://crowdsourcingweek.com/what-is-crowdsourcing/ target=_blank rel=noopener>https://crowdsourcingweek.com/what-is-crowdsourcing/</a> Crutzen, R., Ygram Peters, G. J., & Mondschein, C. (2019). Why and how we should care about the General Data Protection Regulation. <em>Psychology & health</em>, <em>34</em>(11), 1347-1357.
<a href=https://doi.org/10.1080/08870446.2019.1606222 target=_blank rel=noopener>https://doi.org/10.1080/08870446.2019.1606222</a> Crüwell, S., van Doorn, J., Etz, A., Makel, M. C., Moshontz, H., Niebaum, J. C., Orben, A., Parsons, S., & Schulte-Mecklenbeck, M. (2019). Seven Easy Steps to Open Science: An Annotated Reading List. Zeitschrift Für Psychologie, 227(4), 237–248.
<a href=https://doi.org/10.1027/2151-2604/a000387 target=_blank rel=noopener>https://doi.org/10.1027/2151-2604/a000387</a> Curry, S. (2012) <em>Sick of impact factors.</em> [blogpost]
<a href=http://occamstypewriter.org/scurry/2012/08/13/sick-of-impact-factors/ target=_blank rel=noopener>http://occamstypewriter.org/scurry/2012/08/13/sick-of-impact-factors/</a> d’Espagnat, B. (2008). Is science cumulative? A physicist viewpoint. In <em>Rethinking Scientific Change and Theory Comparison</em> (pp. 145-151). Springer, Dordrecht.
<a href=https://doi.org/10.1007/978-1-4020-6279-7_10 target=_blank rel=noopener>https://doi.org/10.1007/978-1-4020-6279-7_10</a> Davies, G. M., & Gray, A. (2015). Don’t let spurious accusations of pseudoreplication limit our ability to learn from natural experiments (and other messy kinds of ecological monitoring). <em>Ecology and Evolution, 5</em>(22), 5295–5304.
<a href=https://doi.org/10.1002/ece3.1782 target=_blank rel=noopener>https://doi.org/10.1002/ece3.1782</a> Del Giudice, M., & Gangestad, S. W. (2021). A traveler’s guide to the multiverse: Promises, pitfalls, and a framework for the evaluation of analytic decisions. <em>Advances in Methods and Practices in Psychological Science, 4</em>(1), 2515245920954925.
<a href=https://doi.org/10.1177/2515245920954925 target=_blank rel=noopener>https://doi.org/10.1177/2515245920954925</a> Der Kiureghian, A., & Ditlevsen, O. (2009). Aleatory or epistemic? Does it matter?. <em>Structural Safety, 31</em>(2), 105-112.
<a href=https://doi.org/10.1016/j.strusafe.2008.06.020 target=_blank rel=noopener>https://doi.org/10.1016/j.strusafe.2008.06.020</a> DeVellis, R. F. (2017). <em>Scale development: Theory and applications</em> (4th ed.). Sage. Devito, N., & Goldacre, B. (2019). Publication bias. Catalogue Of Bias
<a href=https://catalogofbias.org/biases/publication-bias/ target=_blank rel=noopener>https://catalogofbias.org/biases/publication-bias/</a> Dickersin, K., & Min, Y. (1993). Publication Bias: The problem that wont go away. <em>Annals New York Academy of Sciences, 703</em>(1), 135-148.
<a href=https://doi.org/10.1111/j.1749-6632.1993.tb26343.x target=_blank rel=noopener>https://doi.org/10.1111/j.1749-6632.1993.tb26343.x</a> Dienes, Z. (2011). Bayesian versus orthodox statistics: Which side are you on?. <em>Perspectives on Psychological Science, 6</em>(3), 274-290.https://doi.org/10.1177/1745691611406920 Dienes, Z. (2014). Using Bayes to get the most out of non-significant results. <em>Frontiers in psychology, 5</em>, 781.
<a href=https://doi.org/10.3389/fpsyg.2014.00781 target=_blank rel=noopener>https://doi.org/10.3389/fpsyg.2014.00781</a> Dienes, Z. (2016). How Bayes factors change scientific practice. <em>Journal of Mathematical Psychology, 72</em>, 78-89.
<a href=https://doi.org/10.1016/j.jmp.2015.10.003 target=_blank rel=noopener>https://doi.org/10.1016/j.jmp.2015.10.003</a> Doll, R., & Hill, A. B. (1954). The mortality of doctors in relation to their smoking habits, a preliminary report. <em>British Medical Journal, 1</em> (4877), 1451–1455. doi:10.1136/bmj.1.4877.1451 Drost, E. A. (2011). Validity and reliability in social science research. E<em>ducation Research and Perspectives, 38</em>(1), 105-123. Du Bois, W.E.B. (1968). <em>The souls of black folk, essays and sketches.</em> Chicago, A.G. McClurg, 1903. New York: Johnson Reprint Corp. Duval, S., & Tweedie, R. (2000a). A nonparametric “trim and fill” method of accounting for publication bias in meta-analysis. <em>Journal of the American Statistical Association, 95</em>, 89–98.
<a href=https://doi.org/10.2307/2669529 target=_blank rel=noopener>https://doi.org/10.2307/2669529</a> Duval, S., & Tweedie, R. (2000b). Trim and fill: A simple funnel-plot–based method of testing and adjusting for publication bias in meta-analysis. <em>Biometrics, 56</em>, 455–463.
<a href=https://doi.org/10.1111/j.0006-341x.2000.00455.x target=_blank rel=noopener>https://doi.org/10.1111/j.0006-341x.2000.00455.x</a>. Eagly, A. H., & Riger, S. (2014). Feminism and psychology: Critiques of methods and epistemology. <em>American Psychologist, 69</em>(7), 685–702.
<a href=https://doi.org/10.1037/a0037372 target=_blank rel=noopener>https://doi.org/10.1037/a0037372</a> Easterbrook, S. M. (2014). Open code for open science? <em>Nature Geoscience, 7,</em> 779-781.
<a href=https://doi.org/10.1038/ngeo2283 target=_blank rel=noopener>https://doi.org/10.1038/ngeo2283</a> Ebersole, C. R., Atherton, O. E., Belanger, A. L., Skulborstad, H. M., Allen, J. M., Banks, J. B., … Nosek, B. A. (2016). Many Labs 3: Evaluating participant pool quality across the academic semester via replication. <em>Journal of Experimental Social Psychology, 67,</em> 68–82.
<a href=https://doi.org/10.1016/j.jesp.2015.10.012 target=_blank rel=noopener>https://doi.org/10.1016/j.jesp.2015.10.012</a> Edyburn, D. L. (2010). Would you recognize universal design for learning if you saw it? Ten propositions for new directions for the second decade of UDL. L<em>earning Disability Quarterly, 33</em>(1), 33-41.
<a href=https://doi.org/10.1177/073194871003300103 target=_blank rel=noopener>https://doi.org/10.1177/073194871003300103</a> Ellemers, N. (2021). Science as collaborative knowledge generation. <em>British Journal of Social Psychology, 60</em> (1), 1-28.https://doi.org/10.1111/bjso.12430 Eley, A. R. (2012). <em>Becoming a successful early career researcher.</em> Routledge.
<a href=http://www.worldcat.org/oclc/934369360 target=_blank rel=noopener>http://www.worldcat.org/oclc/934369360</a> f Elliott, K. C., & Resnik, D. B. (2019). Making open science work for science and society. <em>Environmental Health Perspectives, 127</em>(7).
<a href=https://doi.org/10.1289/EHP4808 target=_blank rel=noopener>https://doi.org/10.1289/EHP4808</a> Esterling, K., Brady, D., & Schwitzgebel, E. (2021, January 27). <em>The Necessity of Construct and External Validity for Generalized Causal Claims.</em>
<a href=https://doi.org/10.31219/osf.io/2s8w5 target=_blank rel=noopener>https://doi.org/10.31219/osf.io/2s8w5</a> European Commission (2021, January 17th). <em>Responsible research & innovation.</em> Horizon 2020.
<a href=https://ec.europa.eu/programmes/horizon2020/en/h2020-section/responsible-research-innovation target=_blank rel=noopener>https://ec.europa.eu/programmes/horizon2020/en/h2020-section/responsible-research-innovation</a> F. (2019, December 13). <em>Introducing a Framework for Open and Reproducible Research Training (FORRT).</em>
<a href=https://doi.org/10.31219/osf.io/bnh7p target=_blank rel=noopener>https://doi.org/10.31219/osf.io/bnh7p</a> Fanelli, D. (2010). Do Pressures to Publish Increase Scientists&rsquo; Bias? An Empirical Support from US States Data. <em>PLOS ONE. 5</em> (4), e10271.
<a href=https://doi.org/10.1371/journal.pone.0010271 target=_blank rel=noopener>https://doi.org/10.1371/journal.pone.0010271</a> Fanelli, D. (2018). Opinion: Is science really facing a reproducibility crisis, and do we need it to?. <em>Proceedings of the National Academy of Sciences, 115</em>(11), 2628-2631.
<a href=https://doi.org/10.1073/pnas.1708272114 target=_blank rel=noopener>https://doi.org/10.1073/pnas.1708272114</a> Farrow, R. (2017). Open education and critical pedagogy. <em>Learning, Media and Technology</em>, <em>42</em>(2), 130-146.
<a href=https://doi.org/10.1080/17439884.2016.1113991 target=_blank rel=noopener>https://doi.org/10.1080/17439884.2016.1113991</a> Faul, F., Erdfelder, E., Lang, A.-G., & Buchner, A. (2007). G*Power 3: A flexible statistical power analysis program for the social, behavioral, and biomedical sciences. B<em>ehavior Research Methods, 39</em>, 175-191.
<a href=https://doi.org/10.3758/BF03193146 target=_blank rel=noopener>https://doi.org/10.3758/BF03193146</a> Faul, F., Erdfelder, E., Buchner, A., & Lang, A.-G. (2009). Statistical power analyses using G*Power 3.1: Tests for correlation and regression analyses. <em>Behavior Research Methods, 41</em>, 1149-1160.
<a href=https://doi.org/10.3758/BRM.41.4.1149 target=_blank rel=noopener>https://doi.org/10.3758/BRM.41.4.1149</a> Ferson, S., Joslyn, C. A., Helton, J. C., Oberkampf, W. L., & Sentz, K. (2004). Summary from the epistemic uncertainty workshop: consensus amid diversity. <em>Reliability Engineering & System Safety, 85</em>(1-3), 355-369.
<a href=https://doi.org/10.1016/j.ress.2004.03.023 target=_blank rel=noopener>https://doi.org/10.1016/j.ress.2004.03.023</a> Fiedler K., Kutzner F., Krueger J. I.. (2012). The long way from α-error control to validity proper: Problems with a short-sighted false-positive debate. Perspectives on <em>Psychological Science, 7</em>(6), 661-669.
<a href=https://doi.org/ target=_blank rel=noopener>https://doi.org/</a> 10.1177/1745691612462587. Fiedler, K., & Schwarz, N. (2016). Questionable research practices revisited. Social <em>Psychological and Personality Science, 7</em>(1), 45–52.
<a href=https://doi.org/10.1177/1948550615612150 target=_blank rel=noopener>https://doi.org/10.1177/1948550615612150</a> Filipe, A., Renedo, A., & Marston, C. (2017). The co-production of what? Knowledge, values, and social relations in health care. <em>PLoS biology, 15</em>(5), e2001403.
<a href=https://doi.org/10.1371/journal.pbio.2001403 target=_blank rel=noopener>https://doi.org/10.1371/journal.pbio.2001403</a> Fillon, A.A., Feldman, G., Yeung, S. K., Protzko, J., Elsherif, M. M., Xiao, Q., Nanakdewa, K. & Brick, C. (2021). <em>Correlational Meta-Analysis Registered Report Template.</em> [Manuscript in preparation]. Findley, M. G., Jensen, N. M., Malesky, E. J., & Pepinsky, T. B. (2016). Can results-free review reduce publication bias? The results and implications of a pilot study. <em>Comparative Political Studies, 49</em>(13), 1667–1703.
<a href=https://doi.org/10.1177/0010414016655539 target=_blank rel=noopener>https://doi.org/10.1177/0010414016655539</a> Finlay, L., & Gough, B. (Eds.). (2008). <em>Reflexivity: A practical guide for researchers in health and social sciences.</em> John Wiley & Sons. Flake, J. K., & Fried, E. I. (2020). Measurement schmeasurement: Questionable measurement practices and how to avoid them. <em>Advances in Methods and Practices in Psychological Science, 3</em>(4), 456-465.
<a href=https://doi.org/10.1177%2F2515245920952393 target=_blank rel=noopener>https://doi.org/10.1177%2F2515245920952393</a> Fletcher-Watson, S., Adams, J., Brook, K., Charman, T., Crane, L., Cusack, J., Leekam, S., Milton, D., Parr, J. R., & Pellicano, E. (2019). Making the future together: Shaping autism research through meaningful participation. <em>Autism</em>, <em>23</em>(4), 943–953 FORRT. (2021). <em>Welcome to FORRT.</em> Framework for Open and Reproducible Research Training.
<a href=https://forrt.org target=_blank rel=noopener>https://forrt.org</a> Foster, E. D., & Deardorff, A. (2017). Open science framework (OSF). <em>Journal of the Medical Library Association: JMLA, 105</em>(2), 203.
<a href=https://doi.org/ target=_blank rel=noopener>https://doi.org/</a> 10.5195/jmla.2017.88 Franco, A., Malhotra, N., & Simonovits, G. (2014). Publication bias in the social sciences: Unlocking the file drawer. <em>Science, 345</em>(6203), 1502-1505.
<a href=https://doi.org/10.1126/science.1255484 target=_blank rel=noopener>https://doi.org/10.1126/science.1255484</a> Frank, M.C., Bergelson, E., Bergmann, C., Cristia, A., Floccia, C., Gervain, J., Hamlin, J.K., Hannon, E.E., Kline, M., Levelt, C., Lew-Williams, C., Nazzi, T., Panneton, R., Rabagliati, H., Soderstrom, M., Sullivan, J., Waxman, S. and Yurovsky, D. (2017). A Collaborative Approach to Infant Research: Promoting Reproducibility, Best Practices, and Theory-Building. <em>Infancy, 22,</em> 421-435.
<a href=https://doi.org/10.1111/infa.12182 target=_blank rel=noopener>https://doi.org/10.1111/infa.12182</a> Franzoni, C., & Sauermann, H. (2014). Crowd science: The organization of scientific research in open collaborative projects. <em>Research Policy, 43</em>(1), 1–20.
<a href=https://doi.org/10.1016/j.respol.2013.07.005 target=_blank rel=noopener>https://doi.org/10.1016/j.respol.2013.07.005</a> Fraser, H., Bush, M., Wintle, B., Mody, F., Smith, E., Hanea, A., &mldr; & Fidler, F. (2021). <em>Predicting reliability through structured expert elicitation with repliCATS</em> (Collaborative Assessments for Trustworthy Science). Free Our Knowledge. (n.d.). <em>About</em>. Free Our Knowledge.
<a href=https://freeourknowledge.org/about/ target=_blank rel=noopener>https://freeourknowledge.org/about/</a>. Frith, U. (2020). Fast lane to slow science. <em>Trends in Cognitive Sciences, 24</em>(1), 1-2.https://doi.org/10.1016/j.tics.2019.10.007 Galligan, F., & Dyas-Correia, S. (2013). Altmetrics: rethinking the way we measure. <em>Serials Review, 39</em>(1), 56-61.
<a href=https://doi.org/10.1016/j.serrev.2013.01.003 target=_blank rel=noopener>https://doi.org/10.1016/j.serrev.2013.01.003</a> Gelman, A., & Loken, E. (2013). The garden of forking paths: Why multiple comparisons can be a problem, even when there is no “fishing expedition” or “p-hacking” and the research hypothesis was posited ahead of time. <em>Department of Statistics, Columbia University, 348.</em>
<a href=http://www.stat.columbia.edu/ target=_blank rel=noopener>http://www.stat.columbia.edu/</a>~gelman/research/unpublished/p_hacking.pdf Gelman, A., & Carlin, J. (2014). Beyond Power Calculations: Assessing Type S (Sign) and Type M (Magnitude) Errors. <em>Perspectives on Psychological Science, 9</em>(6), 641-651.
<a href=https://doi.org/ target=_blank rel=noopener>https://doi.org/</a> 10.1177/1745691614551642 Gentleman, R. (2005). Reproducible Research: A Bioinformatics Case Study. <em>Statistical Applications in Genetics and Molecular Biology, 4</em>, 1034.
<a href=https://doi.org/10.2202/1544-6115.1034 target=_blank rel=noopener>https://doi.org/10.2202/1544-6115.1034</a> German Research Foundation (2019). Guidelines for Safeguarding Good Research Practice. Code of Conduct.
<a href=http://doi.org/10.5281/zenodo.3923602 target=_blank rel=noopener>http://doi.org/10.5281/zenodo.3923602</a> Gilroy, P. (1993). <em>The black Atlantic: Modernity and double consciousness</em>. New York: Harvard University Press. Giner-Sorolla, R., Aberson, C. L., Bostyn, D. H., Carpenter, T., Conrique, B. G., Lewis, N. A., & Soderberg, C. (2019). Power to detect what? Considerations for planning and evaluating sample size [Preprint].
<a href=https://osf.io/jnmya/ target=_blank rel=noopener>https://osf.io/jnmya/</a> Ginsparg, P. (1997). Winners and losers in the global research village, <em>The Serials Librarian, 30</em>(3-4), 83-95.
<a href=https://doi.org/10.1300/J123v30n03 target=_blank rel=noopener>https://doi.org/10.1300/J123v30n03</a>_13 Ginsparg, P. (2001). Creating a global knowledge network. In <em>Second Joint ICSU Press-UNESCO Expert Conference on Electronic Publishing in Scienc</em>e (pp. 19-23). Gioia, D. A., & Pitre, E. (1990). Multiparadigm perspectives on theory building. Academy of management review, 15(4), 584-602.
<a href=https://doi.org/10.5465/amr.1990.4310758 target=_blank rel=noopener>https://doi.org/10.5465/amr.1990.4310758</a> Glass, D. J., & Hall, N. (2008). A brief history of the hypothesis. <em>Cell, 134</em>(3), 378-381.
<a href=https://doi.org/10.1016/j.cell.2008.07.033 target=_blank rel=noopener>https://doi.org/10.1016/j.cell.2008.07.033</a> Goertzen, M.J. (2017). <em>Introduction to Quantitative Research and Data.</em> Library Technology Reports. 53(4), 12–18. Gollwitzer, M., Abele-Brehm, A., Fiebach, C., Ramthun, R., Scheel, A. M., Schönbrodt, F. D., & Steinberg, U. (2020, September 10). Data Management and Data Sharing in Psychological Science: Revision of the DGPs Recommendations.
<a href=https://doi.org/10.31234/osf.io/24ncs target=_blank rel=noopener>https://doi.org/10.31234/osf.io/24ncs</a> Goodman, S. N., Fanelli, D., & Ioannidis, J. P. A. (2016). What does research reproducibility mean? <em>Science Translational Medicine, 8</em>(341), 341ps12-341ps12.
<a href=https://doi.org/10.1126/scitranslmed.aaf5027 target=_blank rel=noopener>https://doi.org/10.1126/scitranslmed.aaf5027</a> Goodman, S. W., & Pepinsky, T. B. (2019). Gender Representation and Strategies for Panel Diversity: Lessons from the APSA Annual Meeting. <em>PS: Political Science & Politics</em>, <em>52</em>(4), 669-676.
<a href=https://doi.org/10.1017/S1049096519000908 target=_blank rel=noopener>https://doi.org/10.1017/S1049096519000908</a> Gorgolewski, K., Auer, T., Calhoun, V. et al. (2016). The brain imaging data structure, a format for organizing and describing outputs of neuroimaging experiments. S<em>cientific Data, 3,</em> 160044.
<a href=https://doi.org/10.1038/sdata.2016.44 target=_blank rel=noopener>https://doi.org/10.1038/sdata.2016.44</a> Graham, I. D., McCutcheon, C., & Kothari, A. (2019). Exploring the frontiers of research co-production: the Integrated Knowledge Translation Research Network concept papers. <em>Health Research Policy and Systems, 17</em>, 88.
<a href=https://doi.org/10.1186/s12961-019-0501-7 target=_blank rel=noopener>https://doi.org/10.1186/s12961-019-0501-7</a> GRN Â· German Reproducibility Network. (n.d.). <em>A German Reproducibility Network.</em> Retrieved 5 June 2021, from
<a href=https://reproducibilitynetwork.de/ target=_blank rel=noopener>https://reproducibilitynetwork.de/</a> Grossmann, A., & Brembs, B. (2021). Current market rates for scholarly publishing services. <em>F1000Research, 10</em>(20), 20.
<a href=https://doi.org/10.12688/f1000research.27468.1 target=_blank rel=noopener>https://doi.org/10.12688/f1000research.27468.1</a> Grzanka, P. R. (2020). From buzzword to critical psychology: An invitation to take intersectionality seriously. <em>Women & Therapy, 43</em>(3-4), 244-261. Guest, O. [@o_guest]. (2017, June 5). <em>Thanks! Hopefully this thread & many other similar discussions & blogs will help make it less Bropen Science and more Open Science. *hides\ [Tweet]. Twitter.
<a href=https://twitter.com/o target=_blank rel=noopener>https://twitter.com/o</a>_guest/status/871675631062458368 Guest, O., & Martin, A. E. (2020). How computational modeling can force theory building in psychological science. <em>Perspectives on Psychological Science.</em>
<a href=https://doi.org/10.1177/1745691620970585 target=_blank rel=noopener>https://doi.org/10.1177/1745691620970585</a> Haak, L. L., Fenner, M., Paglione, L., Pentz, E., & Ratner, H. (2012). ORCID: A system to uniquely identify researchers. <em>Learned Publishing, 25</em>(4), 259-264. doi:10.1087/20120404 Hackett, R., & Kelly, S. (2020). Publishing ethics in the era of paper mills. <em>Biology Open, 9</em>(10), bio056556.
<a href=https://doi.org/10.1242/bio.056556 target=_blank rel=noopener>https://doi.org/10.1242/bio.056556</a> Hardwicke, T. E., Jameel, L., Jones, M., Walczak, E. J., & Weinberg, L. M. (2014). Only human: Scientists, systems, and suspect statistics. <em>Opticon1826, 16,</em> 25. DOI:10.5334/OPT.CH Hardwicke, T. E., Bohn, M., MacDonald, K., Hembacher, E., Nuijten, M. B., Peloquin, B. N., &mldr; & Frank, M. C. (2020). Analytic reproducibility in articles receiving open data badges at the journal Psychological Science: an observational study. <em>Royal Society Open Science, 8</em>(1), 201494.
<a href=https://doi.org/10.1098/rsos.201494 target=_blank rel=noopener>https://doi.org/10.1098/rsos.201494</a> Hart, D. D., & Silka, L. (2020). Rebuilding the Ivory Tower: A Bottom-Up Experiment in Aligning Research with Societal Needs. <em>Issues in Science and Technology,</em> 79-85.
<a href=https://issues.org/aligning-research-with-societal-needs/ target=_blank rel=noopener>https://issues.org/aligning-research-with-societal-needs/</a> Hartgerink, C. H., Wicherts, J. M., & Van Assen, M. A. L. M. (2017). Too good to be false: Nonsignificant results revisited. <em>Collabra: Psychology, 3</em>(1).
<a href=https://doi.org/10.1525/collabra.71 target=_blank rel=noopener>https://doi.org/10.1525/collabra.71</a> Haven, T. L., & van Grootel, L. (2019). Preregistering qualitative research. <em>Accountability in Research, 26</em>(3), 229–244.
<a href=https://doi.org/10.1080/08989621.2019.1580147 target=_blank rel=noopener>https://doi.org/10.1080/08989621.2019.1580147</a> Haynes, S. N., Richard, D. C. S., & Kubany, E. S. (1995). Content validity in psychological assessment: A functional approach to concepts and methods. <em>Psychological Assessment, 7</em>(3), 238–247.
<a href=https://doi.org/10.1037/1040-3590.7.3.238 target=_blank rel=noopener>https://doi.org/10.1037/1040-3590.7.3.238</a> Health Research Board (n.d.) <em>Declaration on Research Assessment.</em> Available from:
<a href=https://www.hrb.ie/funding/funding-schemes/before-you-apply/how-we-assess-applications/declaration-on-research-assessment/ target=_blank rel=noopener>https://www.hrb.ie/funding/funding-schemes/before-you-apply/how-we-assess-applications/declaration-on-research-assessment/</a> Healy, K. (2018). <em>Data visualization: A practical introduction.</em> Princeton University Press. Hendriks, F., Kienhues, D., & Bromme, R. (2016). Trust in science and the science of trust. In <em>Trust and communication in a digitized world</em> (S. 143–159). Springer. Henrich, J. (2020). T</em>he weirdest people in the world: How the west became psychologically peculiar and particularly prosperous.* Farrar, Straus and Giroux. Henrich, J., Heine, S. J., & Norenzayan, A. (2010). The weirdest people in the world?. <em>Behavioral and brain sciences, 33</em>(2-3), 61-83.
<a href=https://doi.org/10.1017/S0140525X0999152X target=_blank rel=noopener>https://doi.org/10.1017/S0140525X0999152X</a> Herrmannova, D., & Knoth, P. (2016). <em>Semantometrics Towards Full text-based Research Evaluation.</em>
<a href=https://arxiv.org/pdf/1605.04180.pdf target=_blank rel=noopener>https://arxiv.org/pdf/1605.04180.pdf</a> Higgins, J.P.T., Thomas, J., Chandler, J., Cumpston, M., Li, T., Page, M.J., Welch, V.A. (Eds). (2019). <em>Cochrane Handbook for Systematic Reviews of Interventions.</em> 2nd Edition. Chichester, UK: John Wiley & Sons. Himmelstein, D. S., Rubinetti, V., Slochower, D. R., Hu, D., Malladi, V. S., Greene, C. S., & Gitter, A. (2019). Open collaborative writing with Manubot. <em>PLOS Computational Biology, 15</em>(6), e1007128.
<a href=https://doi.org/10.1371/journal.pcbi.1007128 target=_blank rel=noopener>https://doi.org/10.1371/journal.pcbi.1007128</a> Hirsch, J. E. (2005). An index to quantify an individual&rsquo;s scientific research output. <em>Proceedings of the National Academy of Sciences, 102</em>(46), 16569-16572.
<a href=https://doi.org/10.1073/pnas.0507655102 target=_blank rel=noopener>https://doi.org/10.1073/pnas.0507655102</a> Hitchcock, C., Meyer, A., Rose, D., & Jackson, R. (2002). Providing new access to the general curriculum: Universal design for learning. <em>Teaching exceptional children, 35</em>(2), 8-17.
<a href="https://www.proquest.com/scholarly-journals/providing-new-access-general-curriculum/docview/201139970/se-2?accountid=8630" target=_blank rel=noopener>https://www.proquest.com/scholarly-journals/providing-new-access-general-curriculum/docview/201139970/se-2?accountid=8630</a> Hoekstra, R., Kiers, H., & Johnson, A. (2012). Are assumptions of well-known statistical techniques checked, and why (not)?. <em>Frontiers in Psychology, 3</em>(137), 1-9.
<a href=https://doi.org/10.3389/fpsyg.2012.00137 target=_blank rel=noopener>https://doi.org/10.3389/fpsyg.2012.00137</a> Hoijtink, H., Mulder, J., van Lissa, C., & Gu, X. (2019). A tutorial on testing hypotheses using the Bayes factor. <em>Psychological Methods, 24</em>(5), 539–556.
<a href=https://doi.org/10.1037/met0000201 target=_blank rel=noopener>https://doi.org/10.1037/met0000201</a> Holcombe, A. O. (2019). Contributorship, not authorship: Use CRediT to indicate who did what. <em>Publications, 7</em>(3), 48.
<a href=https://doi.org/10.3390/publications7030048 target=_blank rel=noopener>https://doi.org/10.3390/publications7030048</a> Holcombe, A. O., Kovacs, M., Aust, F., & Aczel, B. (2020). Documenting contributions to scholarly articles using CRediT and tenzing. <em>Plos one, 15</em>(12), e0244611. Homepage. (n.d.). <em>Open Science MOOC.</em> Retrieved 5 June 2021, from
<a href=https://opensciencemooc.eu/ target=_blank rel=noopener>https://opensciencemooc.eu/</a> Houtkoop, B. L., Chambers, C., Macleod, M. Bishop, D. V. M. Nichols, T. E., & Wagenmekers, E.-J. (2018). Data sharing in psychology: A survey on barriers and preconditions. <em>Advances in Methods and Practices in Psychological Science, 1</em>(1), 70.85.
<a href=https://doi.org/10.1177/2515245917751886 target=_blank rel=noopener>https://doi.org/10.1177/2515245917751886</a> Huber, B., Barnidge, M., Gil de Zúñiga, H., & Liu, J. (2019). Fostering public trust in science: The role of social media. <em>Public understanding of science, 28</em>(7), 759-777.
<a href=https://doi.org/10.1177/0963662519869097 target=_blank rel=noopener>https://doi.org/10.1177/0963662519869097</a> Huelin, R., Iheanacho, I., Payne, K., & Sandman, K. (2015). What’s in a name? Systematic and non-systematic literature reviews, and why the distinction matters. <em>The evidence Forum</em>, 34-37. Retrieved from:
<a href=https://www.evidera.com/wp-content/uploads/2015/06/Whats-in-a-Name-Systematic-and-Non-Systematic-Literature-Reviews-and-Why-the-Distinction-Matters.pdf target=_blank rel=noopener>https://www.evidera.com/wp-content/uploads/2015/06/Whats-in-a-Name-Systematic-and-Non-Systematic-Literature-Reviews-and-Why-the-Distinction-Matters.pdf</a> Hüffmeier, J., Mazei, J., & Schultze, T. (2016). Reconceptualizing replication as a sequence of different studies: A replication typology. <em>Journal of Experimental Social Psychology, 66</em>, 81-92.
<a href=https://doi.org/10.1016/j.jesp.2015.09.009 target=_blank rel=noopener>https://doi.org/10.1016/j.jesp.2015.09.009</a> Hultsch, D. F., MacDonald, S. W., & Dixon, R. A. (2002). Variability in reaction time performance of younger and older adults. <em>The Journals of Gerontology Series B: Psychological Sciences and Social Sciences, 57</em>(2), P101-P115.
<a href=https://doi.org/10.1093/geronb/57.2.P101 target=_blank rel=noopener>https://doi.org/10.1093/geronb/57.2.P101</a> Hurlbert, S. H. (1984). Pseudoreplication and the Design of Ecological Field Experiments. <em>Ecological Monographs, 54</em>(2), 187–211.
<a href=https://doi.org/10.2307/1942661 target=_blank rel=noopener>https://doi.org/10.2307/1942661</a> Ikeda, A., Xu, H., Fuji, N., Zhu, S., & Yamada, Y. (2019). Questionable research practices following pre-registration. <em>Japanese Psychological Review, 62</em>, 281–295. International Committee of Medical Journal Editors [ICMJE]. (2019)<em>. Recommendations for the conduct, reporting, eduting, and publication of scholarly work in medical journals.</em>
<a href=http://www.icmje.org/icmje-recommendations.pdf target=_blank rel=noopener>http://www.icmje.org/icmje-recommendations.pdf</a> ISO. (1993). <em>Guide to the Expression of Uncertainty in Measuremen</em>t. 1st ed. Geneva: International Organization for Standardization. Ioannidis, J. P. (2005). Why most published research findings are false. <em>PLoS medicine, 2</em>(8), e124.https://doi.org/10.1371/journal.pmed.0020124 Ioannidis, J. P., Fanelli, D., Dunne, D. D., & Goodman, S. N. (2015). Meta-research: evaluation and improvement of research methods and practices. <em>PLoS Biology, 13</em>(10), e1002264.
<a href=https://doi.org/10.1371/journal.pbio.1002264 target=_blank rel=noopener>https://doi.org/10.1371/journal.pbio.1002264</a> JabRef Development Team (2021). <em>JabRef - An open-source, cross-platform citation and reference management software</em>.
<a href=https://www.jabref.org target=_blank rel=noopener>https://www.jabref.org</a> Jacobson, D., & Mustafa, N. (2019). Social Identity Map: A Reflexivity Tool for Practicing Explicit Positionality in Critical Qualitative Research. I<em>nternational Journal of Qualitative Methods, 18</em>, 1609406919870075.
<a href=https://doi.org/10.1177/1609406919870075 target=_blank rel=noopener>https://doi.org/10.1177/1609406919870075</a> Jafar, A. J. N. (2018). What is positionality and should it be expressed in quantitative studies? <em>Emergency Medicine Journal, 35</em>(5), 323–324.
<a href=https://doi.org/10.1136/emermed-2017-207158 target=_blank rel=noopener>https://doi.org/10.1136/emermed-2017-207158</a> James, K. L., Randall, N. P., & Haddaway, N. R. (2016). A methodology for systematic mapping in environmental sciences. <em>Environmental evidence, 5</em>(1), 1-13.
<a href=https://doi.org/10.1242/bio.056556 target=_blank rel=noopener>https://doi.org/</a>
<a href=https://doi.org/10.1186/s13750-016-0059-6 target=_blank rel=noopener>10.1186/s13750-016-0059-6</a> Jannot, A. S., Agoritsas, T., Gayet-Ageron, A., & Perneger, T. V. (2013). Citation bias favoring statistically significant studies was present in medical research. J<em>ournal of clinical epidemiology, 66</em>(3), 296-301.
<a href=https://doi.org/10.1016/j.jclinepi.2012.09.015 target=_blank rel=noopener>https://doi.org/10.1016/j.jclinepi.2012.09.015</a>. JASP Team (2020). <em>JASP</em> (Version 0.14.1)[Computer software] John, L. K., Loewenstein, G., & Prelec, D. (2012). Measuring the prevalence of questionable research practices with incentives for truth telling. <em>Psychological Science, 23</em>(5), 524–532.
<a href=https://doi.org/10.1177/0956797611430953 target=_blank rel=noopener>https://doi.org/10.1177/0956797611430953</a> Jones, A., Dr, Duckworth, J., & Christiansen, P. (2020, June 29). May I have your attention, please? Methodological and Analytical Flexibility in the Addiction Stroop.
<a href=https://doi.org/10.31234/osf.io/ws8xp target=_blank rel=noopener>https://doi.org/10.31234/osf.io/ws8xp</a> Joseph, T. D., & Hirshfield, L. E. (2011). ‘Why don&rsquo;t you get somebody new to do it?’Race and cultural taxation in the academy. <em>Ethnic and Racial Studies, 34</em>(1), 121-141.
<a href=https://doi.org/10.1080/01419870.2010.496489 target=_blank rel=noopener>https://doi.org/10.1080/01419870.2010.496489</a> Kalliamvakou, E., Gousios, G., Blincoe, K., Singer, L., German, D. M., & Damian, D. (2014). The promises and perils of mining github. In <em>Proceedings of the 11th working conference on mining software repositories</em> (pp. 92-101). Kathawalla, U., Silverstein, P., & Syed, M. (2020). Easing into Open Science: A Guide for Graduate Students and Their Advisors*. Collabra: Psychology.*
<a href=https://psyarxiv.com/vzjdp target=_blank rel=noopener>https://psyarxiv.com/vzjdp</a> Kelley, T. L. (1927). <em>Interpretation of educational measurements</em>. New York: Macmillan. Kerr, N. L. (1998). HARKing: Hypothesizing after the results are known. <em>Personality and social psychology review, 2</em>(3), 196-217.
<a href=https://doi.org/10.1207/s15327957pspr0203_4 target=_blank rel=noopener>https://doi.org/10.1207/s15327957pspr0203_4</a> Kerr, N. L., Ao, X., Hogg, M. A., & Zhang, J. (2018). Addressing replicability concerns via adversarial collaboration: Discovering hidden moderators of the minimal intergroup discrimination effect. <em>Journal of Experimental Social Psychology, 78</em>, 66-76.
<a href=https://doi.org/10.1016/j.jesp.2018.05.001 target=_blank rel=noopener>https://doi.org/10.1016/j.jesp.2018.05.001</a> Kidwell, M. C., Lazarević, L. B., Baranski, E., Hardwicke, T. E., Piechowski, S., Falkenberg, L. S., &mldr; & Nosek, B. A. (2016). Badges to acknowledge open practices: A simple, low-cost, effective method for increasing transparency. <em>PLoS biology, 14</em>(5), e1002456.
<a href=https://doi.org/10.1371/journal.pbio.1002456 target=_blank rel=noopener>https://doi.org/10.1371/journal.pbio.1002456</a> Kienzler, H., & Fontanesi, C. (2017). Learning through inquiry: A global health hackathon. <em>Teaching in Higher Education, 22</em>(2), 129-142.
<a href=https://doi.org/10.1080/13562517.2016.1221805 target=_blank rel=noopener>https://doi.org/10.1080/13562517.2016.1221805</a> Kiernan, C. (1999). Participation in research by people with learning disability: Origins and issues. British Journal of Learning Disabilities, 27(2), 43–47.
<a href=https://doi.org/10.1111/j.1468-3156.1999.tb00084.x target=_blank rel=noopener>https://doi.org/10.1111/j.1468-3156.1999.tb00084.x</a> King, G. (1995). Replication, replication. <em>PS: Political Science & Politics, 28</em>(3), 444–452.
<a href=https://doi.org/10.2307/420301 target=_blank rel=noopener>https://doi.org/10.2307/420301</a> Kitzes, J., Turek, D., Deniz, F. (2017). <em>The practice of reproducible research: Case studies and lessons from the data-intensive sciences.</em> University of California Press. Klein, R. A., Ratliff, K. A., Vianello, M., Adams, R. B., Bahník, Š., Bernstein, M. J., et al. (2014). Investigating variation in replicability: A “many labs” replication project. S<em>ocial Psychology, 45</em>, 142–152.
<a href=https://doi.org/10.1027/1864-9335/a000178 target=_blank rel=noopener>https://doi.org/10.1027/1864-9335/a000178</a> Klein, R. A., Vianello, M., Hasselman, F., Adams, B. G., Adams, R. B., Alper, S., … Nosek, B. A. (2018). Many Labs 2: Investigating Variation in Replicability Across Samples and Settings. Advances in Methods and Practices in Psychological <em>Science, 1</em>(4), 443–490.
<a href=https://doi.org/10.1177/2515245918810225 target=_blank rel=noopener>https://doi.org/10.1177/2515245918810225</a> Kleinberg, B., Mozes, M., van der Toolen, Y., & verschuere, B. (2017, June 3). NETANOS - Named entity-based Text Anonymization for Open Science. Retrieved from
<a href=https://osf.io/w9nhb/ target=_blank rel=noopener>https://osf.io/w9nhb/</a> Knoth, P., & Herrmannova, D. (2014). Towards semantometrics: A new semantic similarity based measure for assessing a research publication’s contribution. <em>D-Lib Magazine, 20</em>(11), 8.
<a href=https://doi.org/10.1045/november14-knoth target=_blank rel=noopener>https://doi.org/10.1045/november14-knoth</a> Knowledge, F. O. (2020, December 3). Preregistration Pledge. Free Our Knowledge.
<a href=https://freeourknowledge.org/2020-12-03-preregistration-pledge/ target=_blank rel=noopener>https://freeourknowledge.org/2020-12-03-preregistration-pledge/</a> Koole, S. L., & Lakens, D. (2012). Rewarding replications: A sure and simple way to improve psychological science. <em>Perspectives on Psychological Science, 7</em>(6), 608-614 .https://doi.org/10.1177/1745691612462586 Kreuter, F. (Ed.). (2013). <em>Improving Surveys with Paradata.</em> doi:10.1002/9781118596869 Kruschke, J. K. (2015). <em>Doing Bayesian data analysis: A tutorial with R, JAGS, and Stan</em> (2nd ed.). Academic Press. Kuhn, T. (1962). <em>The Structure of Scientific Revolutions.</em> University of Chicago Press. ISBN 978-0226458083. Kukull, W.A. & Ganguli, M. (2012). Generalizability: The trees, the forest, and the low-hanging fruit. <em>Neurology, 78</em>(23), 1886-1891.
<a href=https://doi.org/10.1212/WNL.0b013e318258f812 target=_blank rel=noopener>https://doi.org/10.1212/WNL.0b013e318258f812</a> Lakens, D. (2014). Performing high-powered studies efficiently with sequential analyses. <em>European Journal of Social Psychology, 44</em>(7), 701–710.
<a href=https://doi.org/10.1002/ejsp.2023 target=_blank rel=noopener>https://doi.org/10.1002/ejsp.2023</a> Lakens, D. (2020). Pandemic researchers — recruit your own best critics. <em>Nature, 581</em>, 121. Lakens, D. (2021a, January 4). Sample Size Justification.
<a href=https://doi.org/10.31234/osf.io/9d3yf target=_blank rel=noopener>https://doi.org/10.31234/osf.io/9d3yf</a> Lakens, D. (2021b). The practical alternative to the p-value is the correctly used p-value. Lakens, D., Scheel, A. M., & Isager, P. M. (2018). Equivalence testing for psychological research: A tutorial. <em>Advances in Methods and Practices in Psychological Science, 1</em>(2), 259-269.
<a href=https://doi.org/10.1177/2515245918770963 target=_blank rel=noopener>https://doi.org/10.1177/2515245918770963</a> Lakens, D., McLatchie, N., Isager, P. M., Scheel, A. M., & Dienes, Z. (2020). Improving inferences about null effects with Bayes factors and equivalence tests. <em>The Journals of Gerontology: Series B, 75</em>(1), 45-57.
<a href=https://doi.org/10.1093/geronb/gby065 target=_blank rel=noopener>https://doi.org/10.1093/geronb/gby065</a> Laine, H. (2017) Afraid of scooping – Case study on researcher strategies against fear of scooping in the context of open science. <em>Data Science Journal, 16</em>(29), 1–14.
<a href=https://doi.org/10.5334/dsj-2017-029 target=_blank rel=noopener>https://doi.org/10.5334/dsj-2017-029</a> Largent, E. A., & Snodgrass, R. T. (2016). Blind peer review by academic journals. In C. T. Robertson and A. S. Kesselheim (Eds.) <em>Blinding as a solution to bias: Strengthening biomedical science, forensic science, and law</em>, (pp. 75-95). Academic Press.
<a href=https://doi.org/10.1016/B978-0-12-802460-7.00005-X target=_blank rel=noopener>https://doi.org/10.1016/B978-0-12-802460-7.00005-X</a> Lazic, S. E. (2019). <em>Genuine replication and pseudoreplication: What’s the difference?</em> BMJ Open Science.
<a href=https://blogs.bmj.com/openscience/2019/09/16/genuine-replication-and-pseudoreplication-whats-the-difference/ target=_blank rel=noopener>https://blogs.bmj.com/openscience/2019/09/16/genuine-replication-and-pseudoreplication-whats-the-difference/</a> Leavy, P. (2017). <em>Research design: Quantitative, qualitative, mixed methods, arts-based, and community-based participatory research approaches.</em> The Guilford Press. Leavens, D. A., Bard, K. A., & Hopkins, W. D. (2010). BIZARRE chimpanzees do not represent “the chimpanzee”. <em>Behavioral and Brain Sciences, 33(<em>2-3), 100-101.
<a href=https://doi.org/10.1017/S0140525X10000166 target=_blank rel=noopener>https://doi.org/10.1017/S0140525X10000166</a> LeBel, E. P., Vanpaemel, W., Cheung, I., & Campbell, L. (2017). A brief guide to evaluate replications. <em>Meta-Psychology, 3</em>.
<a href=https://doi.org/10.15626/MP.2018.843 target=_blank rel=noopener>https://doi.org/10.15626/MP.2018.843</a> LeBel, E. P., McCarthy, R. J., Earp, B. D., Elson, M., & Vanpaemel, W. (2018). A unified framework to quantify the credibility of scientific findings. A</em>dvances in Methods and Practices in Psychological Science, 1</em>(3), 389-402.
<a href=https://doi.org/10.1177/2515245918787489 target=_blank rel=noopener>https://doi.org/10.1177/2515245918787489</a> Ledgerwood, A., Hudson, S. T. J., Lewis, N. A., Jr., Maddox, K. B., Pickett, C., Remedios, J. D., … Wilkins, C. L. (2021, January 11). The Pandemic as a Portal: Reimagining Psychological Science as Truly Open and Inclusive.
<a href=https://doi.org/10.31234/osf.io/gdzue target=_blank rel=noopener>https://doi.org/10.31234/osf.io/gdzue</a> Lee, R.M. (1993). <em>Doing research on sensitive topics.</em> London: Sage. Levitt, H. M., Motulsky, S. L., Wertz, F. J., Morrow, S. L., & Ponterotto, J. G. (2017). Recommendations for designing and reviewing qualitative research in psychology: Promoting methodological integrity. <em>Qualitative psychology, 4</em>(1), 2.
<a href=https://doi.org/10.1037/qup0000082 target=_blank rel=noopener>https://doi.org/10.1037/qup0000082</a> Lewandowsky, S., & Bishop, D. (2016). Research integrity: Don&rsquo;t let transparency damage science. <em>Nature News, 529</em>(7587), 459.
<a href=https://doi.org/10.1038/529459a target=_blank rel=noopener>https://doi.org/10.1038/529459a</a> LibGuides. (n.d.). <em>Measuring your research impact: i10-Index</em>.
<a href=https://guides.library.cornell.edu/impact/author-impact-10 target=_blank rel=noopener>https://guides.library.cornell.edu/impact/author-impact-10</a>. Lieberman, E. (2020). Research Cycles. In C. Elman, J. Gerring, & J. Mahoney (Eds.), <em>The Production of Knowledge: Enhancing Progress in Social Science</em> (Strategies for Social Inquiry, pp. 42-70). Cambridge: Cambridge University Press.
<a href=https://doi.org10.1017/9781108762519.003 target=_blank rel=noopener>https://doi.org10.1017/9781108762519.003</a> Lind, F., Gruber, M., & Boomgaarden, H. G. (2017). Content analysis by the crowd: Assessing the usability of crowdsourcing for coding latent constructs. <em>Communication Methods and Measures, 11</em>(3), 191–209.
<a href=https://doi.org/10.1080/19312458.2017.1317338 target=_blank rel=noopener>https://doi.org/10.1080/19312458.2017.1317338</a> Lindsay, D. S. (2015). Replication in Psychological Science [Editorial]. <em>Psychological Science, 26</em>(12), 1827-1832.
<a href=https://doi.org/10.1177/0956797615616374 target=_blank rel=noopener>https://doi.org/10.1177/0956797615616374</a> Lindsay, D. S. (2020). Seven steps toward transparency and replicability in psychological science. <em>Canadian Psychology/Psychologie canadienne., 61</em>(4), 310–317.
<a href=https://doi.org/10.1037/cap0000222 target=_blank rel=noopener>https://doi.org/10.1037/cap0000222</a> Liu, Y. et al. (2020). Replication markets: Results, lessons, challenges and opportunities in AI replication. arXiv:2005.04543 Lu, J., Qiu, Y., & Deng, A. (2018). A note on Type S/M errors in hypothesis testing. <em>British Journal of Mathematical and Statistical Psychology, 72</em>(1), 1-17.
<a href=https://doi.org/10.1111/bmsp.12132 target=_blank rel=noopener>https://doi.org/10.1111/bmsp.12132</a> Lüdtke, O., Ulitzsch, E., & Robitzsch, A. (2020). <em>A Comparison of Penalized Maximum Likelihood Estimation and Markov Chain Monte Carlo Techniques for Estimating Confirmatory Factor Analysis Models with Small Sample Sizes</em> [Preprint]. PsyArXiv.
<a href=https://doi.org/10.31234/osf.io/u3qag target=_blank rel=noopener>https://doi.org/10.31234/osf.io/u3qag</a> Lutz, M. (2001). <em>Programming python.</em> O&rsquo;Reilly Media, Inc. Lyon, L. (2016) Transparency: The Emerging Third Dimension of Open Science and Open Data. <em>LIBER Quarterly, 25</em>(4), 153-171.
<a href=http://doi.org/10.18352/lq.10113 target=_blank rel=noopener>http://doi.org/10.18352/lq.10113</a> Makowski, D., Ben-Shachar, M. S., Chen, S. H. A., & Lüdecke, D. (2019). Indices of Effect Existence and Significance in the Bayesian Framework. Retrieved from 10.3389/fpsyg.2019.02767 Marwick, B., Boettiger, C., & Mullen, L. (2018). Packaging data analytical work reproducibly using R (and friends). <em>The American Statistician, 72</em>(1), 80-88.
<a href=https://doi.org/10.1080/00031305.2017.1375986 target=_blank rel=noopener>https://doi.org/10.1080/00031305.2017.1375986</a> McElreath, R. (2020). <em>Statistical rethinking: A Bayesian course with examples in R and Stan</em> (2nd ed.). Taylor and Francis, CRC Press. McNutt, M. K., Bradford, M., Drazen, J. M., Hanson, B., Howard, B., Jamieson, K. H., Kiermer, V., Marcus, E., Pope, B. K., Schekman, R., Swaminathan, S., Stang, P. J., and Verma, I. M. (2018). Transparency in authors’ contributions and responsibilities to promote integrity in scientific publication. <em>Proceedings of the National Academy of Sciences of the United States of America, 115(<em>11), 2557-2560.
<a href=https://doi.org/10.1073/pnas.1715374115 target=_blank rel=noopener>https://doi.org/10.1073/pnas.1715374115</a> Medin, D. L. (2012). Rigor without rigor mortis: The APS Board discusses research integrity. <em>APS Observer, 25</em> (5-9), 27-28.
<a href=https://www.psychologicalscience.org/observer/scientific-rigor target=_blank rel=noopener>https://www.psychologicalscience.org/observer/scientific-rigor</a> Mellers, B., Hertwig, R., & Kahneman, D. (2001). Do frequency representations eliminate conjunction effects? An exercise in adversarial collaboration. P</em>sychological Science, 12</em>(4), 269-275.
<a href=https://doi.org/10.1111/1467-9280.00350 target=_blank rel=noopener>https://doi.org/10.1111/1467-9280.00350</a> Mertens, G., & Krypotos, A. M. (2019). Preregistration of analyses of preexisting data. <em>Psychologica Belgica, 59</em>(1), 338. Merton, R.K. (1938). Science and the social order. <em>Philosophy of Science, 5</em>(3), 321–337
<a href=https://doi.org/10.1086/286513 target=_blank rel=noopener>https://doi.org/10.1086/286513</a> Merton, R. K. (1942). A note on science and democracy. <em>Journal of Legal and Political Sociology, 1</em>, 115–126.
<a href=https://doi.org/10.1515/9783110375008-013 target=_blank rel=noopener>https://doi.org/10.1515/9783110375008-013</a> Merton, R.K. (1968). The Matthew Effect in Science. <em>Science, 159</em> (3810), 56–63. https:/doi.org/10.1126/science.159.3810.56 Messick, S. (1995). Standards of validity and the validity of standards in performance assessment. <em>Educational measurement: Issues and practice, 14</em>(4), 5-8.
<a href=https://doi.org/10.1111/j.1745-3992.1995.tb00881.x target=_blank rel=noopener>https://doi.org/10.1111/j.1745-3992.1995.tb00881.x</a> Michener W.K. (2015). Ten simple rules for creating a good data management plan. <em>PLoS Computational Biology, 11</em>(10), e1004525. https:/doi.org/10.1371/journal.pcbi.1004525 Moher, D., Bouter, L., Kleinert, S., Glasziou, P., Sham, M. H., Barbour, V., &mldr; & Dirnagl, U. (2020). The Hong Kong Principles for assessing researchers: Fostering research integrity. <em>PLoS Biology, 18</em>(7), e3000737.
<a href=https://doi.org/10.1371/journal.pbio.3000737 target=_blank rel=noopener>https://doi.org/10.1371/journal.pbio.3000737</a> Moher, D., Liberati, A., Tetzlaff, J., & Altman, D. (2009). Preferred Reporting Items for Systematic Reviews and Meta-Analyses: The PRISMA Statement. <em>PLoS Medicine, 6</em>(7), e1000097.
<a href=https://doi.org/10.1371/journal.pmed.1000097 target=_blank rel=noopener>https://doi.org/10.1371/journal.pmed.1000097</a> Moher, D., Naudet, F., Cristea, I. A., Miedema, F., Ioannidis, J. P. A., & Goodman, S. N. (2018). Assessing scientists for hiring, promotion, and tenure. <em>PLOS Biology, 16</em>(3), e2004089.
<a href=https://doi.org/10.1371/journal.pbio.2004089 target=_blank rel=noopener>https://doi.org/10.1371/journal.pbio.2004089</a> Moshontz, H., Campbell, L., Ebersole, C. R., IJzerman, H., Urry, H. L., Forscher, P. S., &mldr; & Chartier, C. R. (2018). The Psychological Science Accelerator: Advancing psychology through a distributed collaborative network. <em>Advances in Methods and Practices in Psychological Science, 1</em>(4), 501-515.
<a href=https://doi.org/10.1177/2515245918797607 target=_blank rel=noopener>https://doi.org/10.1177/2515245918797607</a> Monroe, K. R. (2018). The rush to transparency: DA-RT and the potential dangers for qualitative research. <em>Perspectives on Politics, 16</em>(1), 141–148.
<a href=https://doi.org/10.1017/S153759271700336X target=_blank rel=noopener>https://doi.org/10.1017/S153759271700336X</a> Moretti, M. (2020, August 25). <em>Beyond Open-washing: Are Narratives the Future of Open Data Portals?</em> Medium.
<a href=https://medium.com/nightingale/beyond-open-washing-are-stories-and-narratives-the-future-of-open-data-portals-93228d8882f3 target=_blank rel=noopener>https://medium.com/nightingale/beyond-open-washing-are-stories-and-narratives-the-future-of-open-data-portals-93228d8882f3</a> Morgan, C. (1998). The DOI (Digital Object Identifier). <em>Serials, 11</em>(1), pp.47–51.
<a href=http://doi.org/10.1629/1147 target=_blank rel=noopener>http://doi.org/10.1629/1147</a> Munn, Z., Peters, M. D., Stern, C., Tufanaru, C., McArthur, A., & Aromataris, E. (2018). Systematic review or scoping review? Guidance for authors when choosing between a systematic or scoping review approach. <em>BMC medical research methodology, 18</em>(1), 1-7.
<a href=https://doi.org/10.1186/s12874-018-0611-x target=_blank rel=noopener>https://doi.org/10.1186/s12874-018-0611-x</a> Muthukrishna, M., Bell, A. V., Henrich, J., Curtin, C. M., Gedranovich, A., McInerney, J., & Thue, B. (2020). Beyond Western, Educated, Industrial, Rich, and Democratic (WEIRD) psychology: Measuring and mapping scales of cultural and psychological distance. <em>Psychological Science, 31</em>, 678-701.
<a href=https://doi.org/10.1177/0956797620916782 target=_blank rel=noopener>https://doi.org/10.1177/0956797620916782</a> National Academies of Sciences, Engineering, and Medicine, Policy and Global Affairs, Committee on Science, Engineering, Medicine, and Public Policy, Board on Research Data and Information, Division on Engineering and Physical Sciences, Committee on Applied and Theoretical Statistics, Board on Mathematical Sciences and Analytics, Division on Earth and Life Studies, Nuclear and Radiation Studies Board, Division of Behavioral and Social Sciences and Education, Committee on National Statistics, Board on Behavioral, Cognitive, and Sensory Sciences, Committee on Reproducibility and Replicability in Science. (2019). Reproducibility and Replicability in Science. In <em>Understanding Reproducibility and Replicability</em>. Washington (DC): National Academies Press (US), Available from:
<a href=https://www.ncbi.nlm.nih.gov/books/NBK547546/ target=_blank rel=noopener>https://www.ncbi.nlm.nih.gov/books/NBK547546/</a> Nature (n.d.). <em>Recommended Data Repositories.</em> Scientific Data.
<a href=https://www.nature.com/sdata/policies/repositories target=_blank rel=noopener>https://www.nature.com/sdata/policies/repositories</a> Naudet, F., Ioannidis, J., Miedema, F., Cristea, I. A., Goodman, S. N., & Moher, D. (2018). <em>Six principles for assessing scientists for hiring, promotion, and tenure.</em> Impact of Social Sciences Blog. Navarro, D. (2020). <em>Paths in strange spaces: A comment on preregistration.</em> Nelson, L.D., Simmons, J.P. & Simonsohn, U. (2012) Let&rsquo;s Publish Fewer Papers, <em>Psychological Inquiry, 23</em> (3), 291-293,
<a href=https://doi.org/10.1080/1047840X.2012.705245 target=_blank rel=noopener>https://doi.org/10.1080/1047840X.2012.705245</a> Neuroskeptic. (2012). The nine circles of scientific hell. (2012). <em>Perspectives on Psychological Science, 7</em>(6), 643–644.
<a href=https://doi.org/10.1177/1745691612459519 target=_blank rel=noopener>https://doi.org/10.1177/1745691612459519</a> Nichols, T. E., Das, S., Eickhoff, S. B., Evans, A. C., Glatard, T., Hanke, M., &mldr; & Yeo, B. T. (2017). Best practices in data analysis and sharing in neuroimaging using MRI. N<em>ature neuroscience, 20</em>(3), 299-303.
<a href=https://doi.org/10.1038/nn.4500 target=_blank rel=noopener>https://doi.org/10.1038/nn.4500</a> Nickerson, R. S. (1998). Confirmation bias: A ubiquitous phenomenon in many guises. <em>Review of general psychology, 2</em>(2), 175-220.
<a href=https://doi.org/10.1037/1089-2680.2.2.175 target=_blank rel=noopener>https://doi.org/10.1037/1089-2680.2.2.175</a> Nieuwenhuis, S., Forstmann, B. U., & Wagenmakers, E. J. (2011). Erroneous analyses of interactions in neuroscience: a problem of significance. <em>Nature Neuroscience</em>, <em>14</em>(9), 1105-1107.
<a href=https://doi.org/10.1038/nn.2886 target=_blank rel=noopener>https://doi.org/10.1038/nn.2886</a> NIHR (2021)
<a href="https://www.learningforinvolvement.org.uk/?opportunity=nihr-guidance-on-co-producing-a-research-project" target=_blank rel=noopener>https://www.learningforinvolvement.org.uk/?opportunity=nihr-guidance-on-co-producing-a-research-project</a> Nittrouer, C., Hebl, M., Ashburn-Nardo, L., Trump-Steele, R., Lane, D., Valian, V. (2018). Gender disparities in colloquium speakers. <em>Proceedings of the National Academy of Sciences Jan, 115</em> (1) 104-108, DOI: 10.1073/pnas.1708414115 Nosek, B. A., & Bar-Anan, Y. (2012). Scientific utopia: I. Opening scientific communication. <em>Psychological Inquiry, 23</em>(3), 217-243.
<a href=https://doi.org/10.1080/1047840X.2012.692215 target=_blank rel=noopener>https://doi.org/10.1080/1047840X.2012.692215</a> Nosek, B. A., Ebersole, C. R., DeHaven, A. C., & Mellor, D. T. (2018). The preregistration revolution. <em>Proceedings of the National Academy of Sciences, 115</em>(11), 2600-2606.
<a href=https://doi.org/10.1073/pnas.1708274114 target=_blank rel=noopener>https://doi.org/10.1073/pnas.1708274114</a> Nosek, B.A. & Errington, T.M. (2020) What is replication? <em>PlosBiology, 18</em>(3), e3000691.
<a href=https://doi.org/10.1371/journal.pbio.3000691 target=_blank rel=noopener>https://doi.org/10.1371/journal.pbio.3000691</a> Nosek, B. A., & Lakens, D. (2014). Registered Reports. <em>Social Psychology, 45,</em> 137-141.
<a href=https://doi.org/10.1027/1864-9335/a000192 target=_blank rel=noopener>https://doi.org/10.1027/1864-9335/a000192</a>. Nosek, B. A., Spies, J. R., & Motyl, M. (2012). Scientific utopia: II. Restructuring incentives and practices to promote truth over publishability. <em>Perspectives on Psychological Science, 7</em>(6), 615-631.
<a href=https://doi.org/10.1177%2F1745691612459058 target=_blank rel=noopener>https://doi.org/10.1177%2F1745691612459058</a> Noy, N. F., & McGuinness, D. L. (2001). <em>Ontology development 101: A guide to creating your first ontology</em> .
<a href=https://corais.org/sites/default/files/ontology_development_101_aguide_to_creating_your_first_ontology.pdf target=_blank rel=noopener>https://corais.org/sites/default/files/ontology_development_101_aguide_to_creating_your_first_ontology.pdf</a> Nüst, D. C., Boettiger, C., & Marwick, B. (2018) How to Read a Research Compendium. arXiv preprint arXiv:1806.09525 O’Dea, R. E., Parker, T. H., Chee, Y. E., Culina, A., Drobniak, S. M., Duncan, D. H., Fidler, F., Gould, E., Ihle, M., Kelly, C. D., Lagisz, M., Roche, D. G., Sánchez-Tójar, A., Wilkinson, D. P., Wintle, B. C., & Nakagawa, S. (2021). Towards open, reliable, and transparent ecology and evolutionary biology. <em>BMC Biology, 19</em>(1).
<a href=https://doi.org/10.1186/s12915-021-01006-3 target=_blank rel=noopener>https://doi.org/10.1186/s12915-021-01006-3</a> O’Grady (2020) <em>Psychology’s replication crisis inspires ecologists to push for more reliable research.</em> Science.
<a href=https://doi.org/10.1126/science.abg0894 target=_blank rel=noopener>https://doi.org/10.1126/science.abg0894</a> Obels, P., Lakens, D., Coles, N. A., Gottfried, J., & Green, S. A. (2020). Analysis of open data and computational reproducibility in registered reports in psychology. <em>Advances in Methods and Practices in Psychological Science, 3</em>(2), 229-237. Oberauer K., & Lewandowsky S. (2019). Addressing the theory crisis in psychology. <em>Psychonomic bulletin & review. 26</em>(5),1596–1618.
<a href=https://doi.org/10.3758/s13423-019-01645-2 target=_blank rel=noopener>https://doi.org/10.3758/s13423-019-01645-2</a> Open Science Collaboration. (2015). Estimating the reproducibility of psychological science. <em>Science, 349</em>(6251), aac4716.
<a href=https://doi.org/10.1126/science.aac4716 target=_blank rel=noopener>https://doi.org/10.1126/science.aac4716</a> Open Aire. (2020). <em>High accuracy Data anonymisation</em>. Amnesia.
<a href=https://amnesia.openaire.eu/ target=_blank rel=noopener>https://amnesia.openaire.eu/</a> Open Source Initiative (n.d.). <em>The Open Source Definition.</em> Open Source Initiative.
<a href=https://opensource.org/osd target=_blank rel=noopener>https://opensource.org/osd</a> Orben, A. (2019). A journal club to fix science. <em>Nature, 573</em>(7775), 465-466.
<a href=https://doi.org/10.1038/d41586-019-02842-8 target=_blank rel=noopener>https://doi.org/10.1038/d41586-019-02842-8</a> Ottmann, G., Laragy, C., Allen, J., Feldman, P. (2011). Coproduction in practice: Participatory action research to develop a model of community aged care. Systemic Practice and Action Research, 24, 413–427.
<a href=https://doi.org/10.1007/s11213-011-9192-x target=_blank rel=noopener>https://doi.org/10.1007/s11213-011-9192-x</a> Oxford Dictionaries. (2017). <em>Bias—definition of bias in English.</em>
<a href=https://en.oxforddictionaries.com/definition/bias target=_blank rel=noopener>https://en.oxforddictionaries.com/definition/bias</a> Oxford Reference. (2017). <em>Reflexivity</em>.
<a href=https://www.oxfordreference.com/view/10.1093/acref/9780199599868.001.0001/acref-9780199599868-e-1530 target=_blank rel=noopener>https://www.oxfordreference.com/view/10.1093/acref/9780199599868.001.0001/acref-9780199599868-e-1530</a> Padilla, A. M. (1994). Research news and comment: Ethnic minority scholars, research, and mentoring: Current and future issues. <em>Educational Researcher, 23</em>(4), 24-27.
<a href=https://doi.org/10.3102/0013189X023004024 target=_blank rel=noopener>https://doi.org/10.3102/0013189X023004024</a> Page, M. J., Moher, D., Bossuyt, P. M., Boutron, I., Hoffmann, T. C., Mulrow, C. D., &mldr; & McKenzie, J. E. (2021). PRISMA 2020 explanation and elaboration: updated guidance and exemplars for reporting systematic reviews. <em>British Medical Journal, 372</em>.
<a href=https://doi.org/ target=_blank rel=noopener>https://doi.org/</a> 10.1136/bmj.n160 Patience, G. S., Galli, F., Patience, P. A., & Boffito, D. C. (2019). Intellectual contributions meriting authorship: Survey results from the top cited authors across all science categories. <em>PLoS One, 14</em>(1), e0198117.
<a href=https://doi.org/10.1371/journal.pone.0198117 target=_blank rel=noopener>https://doi.org/10.1371/journal.pone.0198117</a> Pavlov, Y. G., Adamian, N., Appelhoff, S., Arvaneh, M., Benwell, C., Ph.D., Beste, C., … Mushtaq, F. (2020, November 27). #EEGManyLabs: Investigating the Replicability of Influential EEG Experiments.
<a href=https://doi.org/10.31234/osf.io/528nr target=_blank rel=noopener>https://doi.org/10.31234/osf.io/528nr</a> PCI (n.d.). <em>PCI IN A FEW LINES.</em> Peer community in.
<a href=https://peercommunityin.org/ target=_blank rel=noopener>https://peercommunityin.org/</a> Peer, E., Brandimarte, L., Samat, S., & Acquisti, A. (2017). Beyond the Turk: Alternative platforms for crowdsourcing behavioral research. <em>Journal of Experimental Social Psychology, 70</em>, 153–163.
<a href=https://doi.org/10.1016/j.jesp.2017.01.006 target=_blank rel=noopener>https://doi.org/10.1016/j.jesp.2017.01.006</a> Peng, R. D. (2011). Reproducible Research in Computational Science. <em>Science, 334</em>(6060), 1226–1227.
<a href=https://doi.org/10.1126/science.1213847 target=_blank rel=noopener>https://doi.org/10.1126/science.1213847</a> Percie du Sert, N., Hurst, V., Ahluwalia, A., Alam, S., Avey, M. T., Baker, M., &mldr; & Würbel, H. (2020). The ARRIVE guidelines 2.0: Updated guidelines for reporting animal research. <em>Journal of Cerebral Blood Flow & Metabolism, 40</em>(9), 1769-1777.
<a href=https://doi.org/10.1371/journal.pbio.3000410 target=_blank rel=noopener>https://doi.org/10.1371/journal.pbio.3000410</a> Pernet, C. R. (2015). Null hypothesis significance testing: a short tutorial. <em>F1000Research, 4,</em> 621. https:/doi.org/10.12688/f1000research.6963.3 Pernet, C. R., Appelhoff, S., Gorgolewski, K. J., Flandin, G., Phillips, C., Delorme, A., & Oostenveld, R. (2019). EEG-BIDS, an extension to the brain imaging data structure for electroencephalography. <em>Scientific Data, 6</em>(1), 103.
<a href=https://doi.org/10.1038/s41597-019-0104-8 target=_blank rel=noopener>https://doi.org/10.1038/s41597-019-0104-8</a> Pernet, C., Garrido, M. I., Gramfort, A., Maurits, N., Michel, C. M., Pang, E., &mldr; & Puce, A. (2020). Issues and recommendations from the OHBM COBIDAS MEEG committee for reproducible EEG and MEG research. <em>Nature Neuroscience, 23</em>(12), 1473-1483.
<a href=https://doi.org/10.1038/s41593-020-00709-0 target=_blank rel=noopener>https://doi.org/10.1038/s41593-020-00709-0</a> Peterson, D., & Panofsky, A. (2020, August 4). Metascience as a scientific social movement.
<a href=https://doi.org/10.31235/osf.io/4dsqa target=_blank rel=noopener>https://doi.org/10.31235/osf.io/4dsqa</a> Petre, M., & Wilson, G. (2014). Code review for and by scientists. arXiv preprint arXiv:1407.5648. Popper, K. (1959). <em>The logic of scientific discovery.</em> London, United Kingdom: Routledge. Posselt, J. R. (2020). <em>Equity in Science: Representation, Culture, and the Dynamics of Change in Graduate Education.</em> Stanford University Press.
<a href="https://books.google.de/books?id=2CjwDwAAQBAJ" target=_blank rel=noopener>https://books.google.de/books?id=2CjwDwAAQBAJ</a> Pownall, M., Talbot, C. V., Henschel, A., Lautarescu, A., Lloyd, K., Hartmann, H., … Siegel, J. A. (2020, October 13). Navigating Open Science as Early Career Feminist Researchers.
<a href=https://doi.org/10.31234/osf.io/f9m47 target=_blank rel=noopener>https://doi.org/10.31234/osf.io/f9m47</a> Psychological Science Accelerator. (n.d.).
<a href=https://psysciacc.org/ target=_blank rel=noopener>https://psysciacc.org/</a>. R Core Team (2020). <em>R: A language and environment for statistical computing</em>. R Foundation for Statistical Computing, Vienna, Austria. URL
<a href=https://www.R-project.org/ target=_blank rel=noopener>https://www.R-project.org/</a> Rakow, T., Thompson, V., Ball, L., & Markovits, H. (2014). Rationale and guidelines for empirical adversarial collaboration: A Thinking & Reasoning initiative. <em>Thinking & Reasoning, 21</em>(2), 167–175. doi:10.1080/13546783.2015.975405 RepliCATS project. (2020). <em>Collaborative Assessment for Trustworthy Science.</em> The University of Melbourne.
<a href=https://replicats.research.unimelb.edu.au/ target=_blank rel=noopener>https://replicats.research.unimelb.edu.au/</a> ReproducibiliTea. (n.d.). <em>Welcome to ReproducibiliTea.</em> ReproducibiliTea.
<a href=https://reproducibilitea.org/ target=_blank rel=noopener>https://reproducibilitea.org/</a> Research Data Alliance (2020). <em>Data management plan (DMP) common standard.</em> Available from:
<a href=https://github.com/RDA-DMP-Common/RDA-DMP-Common-Standard target=_blank rel=noopener>https://github.com/RDA-DMP-Common/RDA-DMP-Common-Standard</a> RIOT Science Club. (2021, May 28). <em>RIOT Science Club.</em>
<a href=http://riotscience.co.uk/ target=_blank rel=noopener>http://riotscience.co.uk/</a> Rodriguez, J. K., & Günther, E. A. (2020, Oct 14). <em>What’s wrong with manels and what can we do about it.</em> The Conversation.
<a href=https://theconversation.com/whats-wrong-with-manels-and-what-can-we-do-about-them-148068 target=_blank rel=noopener>https://theconversation.com/whats-wrong-with-manels-and-what-can-we-do-about-them-148068</a> Rolls, L., & Relf, M. (2006). Bracketing interviews: addressing methodological challenges in qualitative interviewing in bereavement and palliative care, <em>Mortality, 11</em> (3), 286-305,
<a href=https://doi.org/10.1080/13576270600774893 target=_blank rel=noopener>https://doi.org/10.1080/13576270600774893</a> Rose, D. (2000). Universal design for learning. J<em>ournal of Special Education Technology, 15</em>(3), 45-49.
<a href=https://doi.org/10.1177/016264340001500307 target=_blank rel=noopener>https://doi.org/10.1177/016264340001500307</a> Rose, D. (2018). Participatory research: real or imagined. <em>Social Psychiatry and Psychiatric Epidemiology</em>. <em>53</em>, 765–771.
<a href=https://doi.org/10.1007/s00127-018-1549-3 target=_blank rel=noopener>https://doi.org/10.1007/s00127-018-1549-3</a> Rose, D. H., & Meyer, A. (2002). <em>Teaching every student in the digital age: Universal design for learning.</em> Association for Supervision and Curriculum Development, 1703 N. Beauregard St., Alexandria, VA 22311-1714 (Product no. 101042: $22.95 ASCD members, $26.95 nonmembers). Ross-Hellauer, T. (2017). What is open peer review? A systematic review [version 2, peer review: 4 approved]. <em>F1000Research, 6,</em> 588 (
<a href=https://doi.org/10.12688/f1000research.11369.2 target=_blank rel=noopener>https://doi.org/10.12688/f1000research.11369.2</a> Rossner, M., Van Epps, H., & Hill, E. (2008). <em>Show me the data.</em>
<a href=https://doi.org/10.1083/jcb.200711140 target=_blank rel=noopener>https://doi.org/10.1083/jcb.200711140</a> Rothstein, H. R., Sutton, A. J., & Borenstein, M. (2005). Publication bias in meta-analysis. In Rothstein, A. J. Sutton, & M. Borenstein (Eds.).P<em>ublication bias in meta-analysis: Prevention, assessment and adjustments</em> (pp. 1-7). John Wiley & Sons, Ltd.
<a href=https://doi.org/10.1002/0470870168.ch1 target=_blank rel=noopener>https://doi.org/10.1002/0470870168.ch1</a> Rowhani-Farid, A., Aldcroft, A., & Barnett, A. G. (2020). Did awarding badges increase data sharing in BMJ Open? A randomized controlled trial. <em>Royal Society open science, 7</em>(3), 191818.
<a href=https://doi.org/10.1098/rsos.191818 target=_blank rel=noopener>https://doi.org/10.1098/rsos.191818</a> Rubin, M. (2021). Explaining the association between subjective social status and mental health among university students using an impact ratings approach. S<em>N Social Sciences, 1</em>(1), 1-21.
<a href=https://doi.org/10.1007/s43545-020-00031-3 target=_blank rel=noopener>https://doi.org/10.1007/s43545-020-00031-3</a> Rubin, M., Evans, O., & McGuffog, R. (2019). Social class differences in social integration at university: Implications for academic outcomes and mental health. In J. Jetten, & K. Peters (Eds.), <em>The social psychology of inequality</em> (pp. 87-102). Springer.
<a href=https://doi.org/10.1007/978-3-030-28856-3_6 target=_blank rel=noopener>https://doi.org/10.1007/978-3-030-28856-3_6</a> S. (2021, June 5). OSF | StudySwap: A platform for interlab replication, collaboration, and research resource exchange. OSF.
<a href=https://osf.io/meetings/StudySwap/ target=_blank rel=noopener>https://osf.io/meetings/StudySwap/</a> Sagarin, B. J., Ambler, J. K., & Lee, E. M. (2014). An ethical approach to peeking at data. <em>Perspectives on Psychological Science, 9</em>(3), 293-304.
<a href=https://doi.org/10.1177/1745691614528214 target=_blank rel=noopener>https://doi.org/10.1177/1745691614528214</a> San Francisco Declaration on Research Assessment (DORA).
<a href=https://sfdora.org/ target=_blank rel=noopener>https://sfdora.org/</a> Retrieved February 18th 2021. Sato, T. (1996). Type I and Type II error in multiple comparisons. <em>The Journal of Psychology, 130</em>(3), 293-302.
<a href=https://doi.org/10.1080/00223980.1996.9915010 target=_blank rel=noopener>https://doi.org/10.1080/00223980.1996.9915010</a> Schafersman, S.D. (1997). <em>An Introduction to Science.</em> Available from:
<a href=https://www.geo.sunysb.edu/esp/files/scientific-method.html target=_blank rel=noopener>https://www.geo.sunysb.edu/esp/files/scientific-method.html</a> Schmidt, F. L., & Hunter, J. E. (2014). <em>Methods of meta-analysis: Correcting error and bias in research findings</em> (3rd ed.). Thousand Oaks, CA: Sage. Schmidt, R.H. (1987) A worksheet for authorship of scientific articles. <em>The Bulletin of the Ecological Society of America, 68</em>, 8–10. Retrieved March 4, 2021, from
<a href=http://www.jstor.org/stable/20166549 target=_blank rel=noopener>http://www.jstor.org/stable/20166549</a> Schneider, J., Merk, S., & Rosman, T. (2019). (Re)Building Trust? Investigating the effects of open science badges on perceived trustworthiness in journal articles.
<a href=https://doi.org/10.17605/OSF.IO/VGBRS target=_blank rel=noopener>https://doi.org/10.17605/OSF.IO/VGBRS</a> Schönbrodt, F. D., Wagenmakers, E.-J., Zehetleitner, M., & Perugini, M. (2017). Sequential hypothesis testing with Bayes factors: Efficiently testing mean differences. <em>Psychological Methods, 22</em>(2), 322–339.
<a href=https://doi.org/10.1037/met0000061 target=_blank rel=noopener>https://doi.org/10.1037/met0000061</a> Schönbrodt, F. (2019). Training students for the Open Science future. <em>Nature human behaviour, 3</em>(10), 1031-1031.
<a href=https://doi.org/10.1038/s41562-019-0726-z target=_blank rel=noopener>https://doi.org/10.1038/s41562-019-0726-z</a> Schuirmann, D. J. (1987). A comparison of the two one-sided tests procedure and the power approach for assessing the equivalence of average bioavailability. <em>Journal of Pharmacokinetics and Biopharmaceutics, 15</em>, 657–680.
<a href=https://doi.org/10.1007/BF01068419 target=_blank rel=noopener>https://doi.org/10.1007/BF01068419</a> Schulz, K. F., & Grimes, D. A. (2005). Multiplicity in randomised trials I: endpoints and treatments. <em>The Lancet, 365</em>(9470), 1591-1595.
<a href=https://doi.org/10.1016/S0140-6736%5c%2805%5c%2966461-6 target=_blank rel=noopener>https://doi.org/10.1016/S0140-6736(05)66461-6</a> Schulz, K. F., Altman, D. G., & Moher, D. (2010). CONSORT 2010 statement: updated guidelines for reporting parallel group randomised trials. <em>Trials, 11</em>(1), 1-8.
<a href=https://doi.org/10.1186/1745-6215-11-32 target=_blank rel=noopener>https://doi.org/10.1186/1745-6215-11-32</a> Schwarz, N., & Strack, F. (2014). Does Merely Going Through the Same Moves Make for a “Direct” Replication?: Concepts, Contexts, and Operationalizations. S<em>ocial Psychology, 45</em>(4), 305-306. Science, C. (n.d.). Open science badges. Retrieved February 08, 2021, from
<a href=https://www.cos.io/initiatives/badges target=_blank rel=noopener>https://www.cos.io/initiatives/badges</a> Sert, N. P. du, Hurst, V., Ahluwalia, A., Alam, S., Avey, M. T., Baker, M., Browne, W. J., Clark, A., Cuthill, I. C., Dirnagl, U., Emerson, M., Garner, P., Holgate, S. T., Howells, D. W., Karp, N. A., Lazic, S. E., Lidster, K., MacCallum, C. J., Macleod, M., … Würbel, H. (2020). The ARRIVE guidelines 2.0: Updated guidelines for reporting animal research. <em>PLOS Biology, 18</em>(7), e3000410.
<a href=https://doi.org/10.1371/journal.pbio.3000410 target=_blank rel=noopener>https://doi.org/10.1371/journal.pbio.3000410</a> Shadish, W. R., Cook, T. D., & Campbell, D. T. (2002). <em>Experimental and quasi-experimental designs for generalized causal inference.</em> Houghton, Mifflin and Company. Sharma, M., Sarin, A., Gupta, P., Sachdeva, S., & Desai, A. (2014). Journal impact factor: its use, significance and limitations. <em>World journal of nuclear medicine, 13</em>(2), 146.
<a href=https://doi.org/10.4103/1450-1147.139151 target=_blank rel=noopener>https://doi.org/10.4103/1450-1147.139151</a> Siddaway, A. P., Wood, A. M., & Hedges, L. V. (2019). How to do a systematic review: a best practice guide for conducting and reporting narrative reviews, meta-analyses, and meta-syntheses. <em>Annual review of psychology, 70</em>, 747-770.
<a href=https://doi.org/[10.1146/annurev-psych-010418-102803]%28https://doi-org.surrey.idm.oclc.org/10.1146/annurev-psych-010418-102803%29 target=_blank rel=noopener>https://doi.org/[10.1146/annurev-psych-010418-102803](https://doi-org.surrey.idm.oclc.org/10.1146/annurev-psych-010418-102803)</a> Sijtsma, K. (2016). Playing with data—Or how to discourage questionable research practices and stimulate researchers to do things right. <em>Psychometrika, 81</em>(1), 1–15.
<a href=https://doi.org/10.1007/s11336-015-9446-0 target=_blank rel=noopener>https://doi.org/10.1007/s11336-015-9446-0</a> Silberzahn, R., Uhlmann, E. L., Martin, D. P., Anselmi, P., Aust, F., Awtrey, E., Bahník, Š., Bai, F., Bannard, C., Bonnier, E., Carlsson, R., Cheung, F., Christensen, G., Clay, R., Craig, M. A., Dalla Rosa, A., Dam, L., Evans, M. H., Flores Cervantes, I., … Nosek, B. A. (2018). Many Analysts, One Data Set: Making Transparent How Variations in Analytic Choices Affect Results. <em>Advances in Methods and Practices in Psychological Science,</em> 337–356.
<a href=https://doi.org/10.1177/2515245917747646 target=_blank rel=noopener>https://doi.org/10.1177/2515245917747646</a> Simons, D. J., Shoda, Y., & Lindsay, D. S. (2017). Constraints on generality (COG): A proposed addition to all empirical papers. <em>Perspectives on Psychological Science, 12</em>(6), 1123-1128.
<a href=https://doi.org/10.1177/1745691617708630 target=_blank rel=noopener>https://doi.org/10.1177/1745691617708630</a> Simmons, J. P., Nelson, L. D., & Simonsohn, U. (2011). False-positive psychology: Undisclosed flexibility in data collection and analysis allows presenting anything as significant. <em>Psychological Science, 22</em>(11), 1359-1366.
<a href=https://doi.org/10.1177/0956797611417632 target=_blank rel=noopener>https://doi.org/10.1177/0956797611417632</a> Simmons, J., Nelson, L., & Simonsohn, U. (2021). Pre‐registration: Why and how. <em>Journal of Consumer Psychology, 31</em>(1), 151–162.
<a href=https://doi.org/10.1002/jcpy.1208 target=_blank rel=noopener>https://doi.org/10.1002/jcpy.1208</a> Simonsohn, U., Nelson, L. D., & Simmons, J. P. (2014). P-curve: a key to the file-drawer. <em>Journal of experimental psychology: General, 143</em>(2), 534. Simonsohn, U., Simmons, J. P., & Nelson, L. D. (2015). Specification curve: Descriptive and inferential statistics on all reasonable specifications. Retrieved from
<a href=http://sticerd.lse.ac.uk/seminarpapers/psyc16022016.pdf target=_blank rel=noopener>http://sticerd.lse.ac.uk/seminarpapers/psyc16022016.pdf</a>. Simonsohn, U., Nelson, L. D., & Simmons, J. P. (2014). p-curve and effect size: Correcting for publication bias using only significant results. <em>Perspectives on Psychological Science, 9</em>(6), 666–681.
<a href=https://doi.org/10.1177/1745691614553988 target=_blank rel=noopener>https://doi.org/10.1177/1745691614553988</a> Simonsohn, U., Nelson, L. D., & Simmons, J. P. (2019). P-curve won’t do your laundry, but it will distinguish replicable from non-replicable findings in observational research: Comment on Bruns & Ioannidis (2016). <em>PLoS ONE, 14</em>(3), e0213454.
<a href=https://doi.org/10.1371/journal.pone.0213454 target=_blank rel=noopener>https://doi.org/10.1371/journal.pone.0213454</a> Simonsohn, U., Simmons, J. P., & Nelson, L. D. (2020). Specification curve analysis. <em>Nature Human Behaviour, 4</em>(11), 1208-1214.
<a href=https://doi.org/10.1038/s41562-020-0912-z target=_blank rel=noopener>https://doi.org/10.1038/s41562-020-0912-z</a> Slow Science Academy. (2010). <em>The Slow Science Manifesto</em>. Slow Science.
<a href=http://slow-science.org/ target=_blank rel=noopener>http://slow-science.org/</a>. SIPS. (2021). <em>The Society for the Improvement of Psychological Science.</em> SIPS.
<a href=https://improvingpsych.org/ target=_blank rel=noopener>https://improvingpsych.org/</a> Smaldino, P. E., & McElreath, R. (2016). The natural selection of bad science. <em>Royal Society Open Science, 3</em>(9), 160384.
<a href=https://doi.org/10.1098/rsos.160384 target=_blank rel=noopener>https://doi.org/10.1098/rsos.160384</a> Smith, G. T. (2005). On Construct Validity: Issues of Method and Measurement. <em>Psychological Assessment, 17</em>(4), 396–408.
<a href=https://psycnet.apa.org/doi/10.1037/1040-3590.17.4.396 target=_blank rel=noopener>https://doi.org/10.1037/1040-3590.17.4.396</a> Smith, A. C., Merz, L., Borden, J. B., Gulick, C., Kshirsagar, A. R., & Bruna, E. M. (2020, September 2). Assessing the effect of article processing charges on the geographic diversity of authors using Elsevier’s ‘Mirror Journal’ system.
<a href=https://doi.org/10.31222/osf.io/s7cx4 target=_blank rel=noopener>https://doi.org/10.31222/osf.io/s7cx4</a> Smith, A.J., Clutton, R.E., Lilley, E., Hansen K.E.A., Brattelid, T. (2018): PREPARE: Guidelines for planning animal research and testing. <em>Laboratory Animals, 52</em>(2), 135-141. https:/doi.org/10.1177/0023677217724823 Sorsa, M.A., Kiikkala, I., & Åstedt-Kurki, P. (2015). Bracketing as a skill in conducting unstructured qualitative interviews. <em>Nursing Research, 22</em>(4), 8-12. https:/doi.org/10.7748/nr.22.4.8.e1317. Society for Open, Reliable and Transparent Ecology and Evolutionary biology (n.d.). SORTEE. Retrieved 5 June 2021, from
<a href=https://www.sortee.org/ target=_blank rel=noopener>https://www.sortee.org/</a> Spence, J. R., & Stanley, D. J. (2018). Concise, simple, and not wrong: In search of a short-hand interpretation of statistical significance. <em>Frontiers in psychology, 9,</em> 2185. https:/doi.org/10.3389/fpsyg.2018.02185 Stanford Libraries. (n.d.). <em>Data management plans.</em>
<a href="https://library.stanford.edu/research/data-management-services/data-management-plans#:~:text=A%20data%20management%20plan%20" target=_blank rel=noopener>https://library.stanford.edu/research/data-management-services/data-management-plans#:~:text=A%20data%20management%20plan%20</a>(DMP,share%20and%20preserve%20your%20data. Steup, M., & Neta, R. (2020, April 11). <em>Epistemology.</em> Stanford Encyclopedia of Philosophy.
<a href=https://plato.stanford.edu/entries/epistemology/ target=_blank rel=noopener>https://plato.stanford.edu/entries/epistemology/</a>. Steegen, S., Tuerlinckx, F., , Gelman, A. & Vanpaemel, W. (2016). Increasing Transparency through a Multiverse Analysis. <em>Perspectives on Psychological Science, 11,</em> 702-712.
<a href=https://doi.org/10.1177/1745691616658637 target=_blank rel=noopener>https://doi.org/10.1177/1745691616658637</a> Stewart, N., Chandler, J., & Paolacci, G. (2017). Crowdsourcing samples in cognitive science. <em>Trends in cognitive sciences, 21</em>(10), 736-748.
<a href=https://doi.org/10.1016/j.tics.2017.06.007 target=_blank rel=noopener>https://doi.org/10.1016/j.tics.2017.06.007</a> Stodden, V. C. (2011). Trust your science? Open your data and code. Strathern, M. (1997). ‘Improving ratings’: audit in the British University system. <em>European review, 5</em>(3), 305-321.
<a href=https://doi.org/10.1002/%28SICI%291234-981X%28199707%295:3 target=_blank rel=noopener>https://doi.org/10.1002/(SICI)1234-981X(199707)5:3</a>&lt;305::AID-EURO184>3.0.CO;2-4https://doi.org/10.1002/(SICI)1234-981X(199707)5:3&lt;305::AID-EURO184>3.0.CO;2-4 Suber, P. (2004). The primacy of authors in achieving Open Access. <em>Nature.</em> June 10, 2004. (previous, unabridged version:
<a href=http://dash.harvard.edu/handle/1/4391161 target=_blank rel=noopener>http://dash.harvard.edu/handle/1/4391161</a>) Suber, P. (2015). <em>Open Access Overview.</em> Available from:
<a href=http://legacy.earlham.edu/~peters/fos/overview.htm target=_blank rel=noopener>http://legacy.earlham.edu/~peters/fos/overview.htm</a> SwissRN. (n.d.). Swiss Reproducibility Network. Retrieved 5 June 2021, from
<a href=https://www.swissrn.org/ target=_blank rel=noopener>https://www.swissrn.org/</a>
<a href=https://psyarxiv.com/cteyb/ target=_blank rel=noopener>Syed, M. (2019). The Open Science Movement is for all of us. PsyArXiv.</a> Syed, M., & Kathawalla, U. (2020, February 25). Cultural Psychology, Diversity, and Representation in Open Science.
<a href=https://doi.org/10.31234/osf.io/t7hp2 target=_blank rel=noopener>https://doi.org/10.31234/osf.io/t7hp2</a> Szollosi, A., & Donkin, C., (2019). Arrested theory development: The misguided distinction between exploratory and confirmatory research. PsyArXiv. Tenney, S., & Abdelgawad, I. (2019). Statistical significance. In <em>StatsPearls</em>. Treasure Island (FL), StatPearls Publishing. Tscharntke, T., Hochberg, M. E., Rand, T. A., Resh, V. H., & Krauss, J. (2007). Author sequence and credit for contributions in multiauthored publications. <em>PLoS Biology, 5</em>(1), e18.
<a href=https://doi.org/10.1371/journal.pbio.0050018 target=_blank rel=noopener>https://doi.org/10.1371/journal.pbio.0050018</a> Tennant, J., Bielczyk, N. Z., Cheplygina, V., Greshake Tzovaras, B., Hartgerink, C. H. J., Havemann, J., Masuzzo, P., & Steiner, T. (2019). Ten simple rules for researchers collaborating on Massively Open Online Papers (MOOPs) [Preprint]. MetaArXiv.
<a href=https://doi.org/10.31222/osf.io/et8ak target=_blank rel=noopener>https://doi.org/10.31222/osf.io/et8ak</a> The jamovi project (2020). <em>jamovi</em> (Version 1.2) [Computer Software]. Retrieved from
<a href=https://www.jamovi.org target=_blank rel=noopener>https://www.jamovi.org</a> The Nine Circles of Scientific Hell. (2012). <em>Perspectives on Psychological Science, 7</em>(6), 643–644.
<a href=https://doi.org/10.1177/1745691612459519 target=_blank rel=noopener>https://doi.org/10.1177/1745691612459519</a> The R Foundation (n.d.). <em>The R Project for Statistical Computing.</em> The R Foundation.
<a href=https://www.r-project.org/ target=_blank rel=noopener>https://www.r-project.org</a>/ Thombs, B. D., Levis, A. W., Razykov, I., Syamchandra, A., Leentjens, A. F., Levenson, J. L., & Lumley, M. A. (2015). Potentially coercive self-citation by peer reviewers: a cross-sectional study. <em>Journal of Psychosomatic Research, 78</em>(1), 1-6.
<a href=https://doi.org/10.1016/j.jpsychores.2014.09.015 target=_blank rel=noopener>https://doi.org/10.1016/j.jpsychores.2014.09.015</a> Tierney, W., Hardy III, J. H., Ebersole, C. R., Leavitt, K., Viganola, D., Clemente, E. G., &mldr; & Hiring Decisions Forecasting Collaboration. (2020). Creative destruction in science. <em>Organizational Behavior and Human Decision Processes, 161,</em> 291-309.
<a href=https://doi.org/10.1016/j.obhdp.2020.07.002 target=_blank rel=noopener>https://doi.org/10.1016/j.obhdp.2020.07.002</a> Tierney, W., Hardy III, J., Ebersole, C. R., Viganola, D., Clemente, E. G., Gordon, M., &mldr; & Culture & Work Morality Forecasting Collaboration. (2021). A creative destruction approach to replication: Implicit work and sex morality across cultures. <em>Journal of Experimental Social Psychology, 93</em>, 104060.
<a href=https://doi.org/10.1016/j.jesp.2020.104060 target=_blank rel=noopener>https://doi.org/10.1016/j.jesp.2020.104060</a> Tiokhin, L., Yan, M., & Horgan, T. J. H. (2021). Competition for priority harms the reliability of science, but reforms can help. <em>Nature Human Behaviour.</em>
<a href=https://doi.org/10.1038/s41562-020-01040-1 target=_blank rel=noopener>https://doi.org/10.1038/s41562-020-01040-1</a> Topor, M., Pickering, J. S., Barbosa Mendes, A., Bishop, D. V. M., Büttner, F. C., Elsherif, M. M., … Westwood, S. J. (2021, March 15). An integrative framework for planning and conducting Non-Intervention, Reproducible, and Open Systematic Reviews (NIRO-SR).
<a href=https://doi.org/10.31222/osf.io/8gu5z target=_blank rel=noopener>https://doi.org/10.31222/osf.io/8gu5z</a> Tufte, E. R. (1983). <em>The visual display of quantitative information.</em> Graphics Press. Tukey, J.W. (1977). <em>Exploratory data analysis.</em> Reading, MA: Addison-Wesley. Tvina, A., Spellecy, R., & Palatnik, A. (2019). Bias in the peer review process: can we do better?. <em>Obstetrics & Gynecology, 133</em>(6), 1081-1083.
<a href=https://doi.org/10.1097/AOG.0000000000003260 target=_blank rel=noopener>https://doi.org/10.1097/AOG.0000000000003260</a> Uhlmann, E. L., Ebersole, C. R., Chartier, C. R., Errington, T. M., Kidwell, M. C., Lai, C. K., McCarthy, R. J., Riegelman, A., Silberzahn, R., & Nosek, B. A. (2019). Scientific utopia III: Crowdsourcing science. <em>Perspectives on Psychological Science, 14</em>(5), 711–733.
<a href=https://doi.org/10.1177/1745691619850561 target=_blank rel=noopener>https://doi.org/10.1177/1745691619850561</a> van de Schoot, R., Depaoli, S., King, R., Kramer, B., Märtens, K., Tadesse, M. G., Vannucci, M., Gelman, A., Veen, D., Willemsen, J., & Yau, C. (2021). Bayesian statistics and modelling. <em>Nature Reviews Methods Primers, 1</em>(1), 1–26.
<a href=https://doi.org/10.1038/s43586-020-00001-2 target=_blank rel=noopener>https://doi.org/10.1038/s43586-020-00001-2</a> Vazire, S. (2018). Implications of the Credibility Revolution for Productivity, Creativity, and Progress. <em>Perspectives on Psychological Science, 13</em>(4), 411–417.
<a href=https://doi.org/10.1177/1745691617751884 target=_blank rel=noopener>https://doi.org/10.1177/1745691617751884</a> Vazire, S., Schiavone, S. R., & Bottesini, J. G. (2020). Credibility Beyond Replicability: Improving the Four Validities in Psychological Science.
<a href=https://doi.org/10.31234/osf.io/bu4d3 target=_blank rel=noopener>https://doi.org/10.31234/osf.io/bu4d3</a> Villum, C. (2016, July 2). <em>“Open-washing” – The difference between opening your data and simply making them available</em>. Open Knowledge Foundation Blog.
<a href=https://blog.okfn.org/2014/03/10/open-washing-the-difference-between-opening-your-data-and-simply-making-them-available/ target=_blank rel=noopener>https://blog.okfn.org/2014/03/10/open-washing-the-difference-between-opening-your-data-and-simply-making-them-available/</a> Vlaeminck, S., & Podkrajac, F. (2017). Journals in Economic Sciences: Paying Lip Service to Reproducible Research? Paying Lip Service to Reproducible Research?. <em>IASSIST Quarterly, 41</em>(1-4), 16.
<a href=https://doi.org/10.29173/iq6 target=_blank rel=noopener>https://doi.org/10.29173/iq6</a> Von Elm, E., Altman, D. G., Egger, M., Pocock, S. J., Gøtzsche, P. C., & Vandenbroucke, J. P. (2007). The Strengthening the Reporting of Observational Studies in Epidemiology (STROBE) statement: guidelines for reporting observational studies. <em>Annals of internal medicine, 147</em>(8), 573-577.
<a href=https://doi.org/10.1136/bmj.39335.541782.AD target=_blank rel=noopener>https://doi.org/10.1136/bmj.39335.541782.AD</a> Voracek, M., Kossmeier, M., & Tran, U. S. (2019). Which Data to Meta-Analyze, and How?. <em>Zeitschrift für Psychologie.</em>
<a href=https://doi.org/10.1027/2151-2604/a000357 target=_blank rel=noopener>https://doi.org/10.1027/2151-2604/a000357</a> Vuorre, M., & Curley, J. P. (2018). Curating research assets: A tutorial on the Git version control system. <em>Advances in methods and Practices in Psychological Science, 1</em>(2), 219-236.
<a href=https://doi.org/10.1177/2515245918754826 target=_blank rel=noopener>https://doi.org/10.1177/2515245918754826</a> Wacker, J. (1998). A definition of theory: research guidelines for different theory-building research methods in operations management. <em>Journal of Operations Management, 16</em>(4), 361–385. doi:10.1016/s0272-6963(98)00019-9 Wagenmakers, E. J., Wetzels, R., Borsboom, D., van der Maas, H. L., & Kievit, R. A. (2012). An agenda for purely confirmatory research. <em>Perspectives on Psychological Science, 7</em>(6), 632-638.
<a href=https://doi.org/10.1177/1745691612463078 target=_blank rel=noopener>https://doi.org/10.1177/1745691612463078</a> Wagenmakers, E.-J., Marsman, M., Jamil, T., Ly, A., Verhagen, J., Love, J., Selker, R., Gronau, Q. F., Šmíra, M., Epskamp, S., Matzke, D., Rouder, J. N., & Morey, R. D. (2018). Bayesian inference for psychology. Part I: Theoretical advantages and practical ramifications. <em>Psychonomic Bulletin & Review, 25</em>(1), 35–57.
<a href=https://doi.org/10.3758/s13423-017-1343-3 target=_blank rel=noopener>https://doi.org/10.3758/s13423-017-1343-3</a> Wagge, J. R., Baciu, C., Banas, K., Nadler, J. T., Schwarz, S., Weisberg, Y., &mldr; & McCarthy, R. (2019). A demonstration of the collaborative replication and education project: Replication attempts of the red-romance effect. <em>Collabra: Psychology, 5</em>(1).
<a href=https://doi.org/10.1525/collabra.177 target=_blank rel=noopener>https://doi.org/10.1525/collabra.177</a> Wason, P. C. (1960). On the failure to eliminate hypotheses in a conceptual task. <em>Quarterly Journal of Experimental Psychology, 12</em>(3), 129-140.
<a href=https://doi.org/10.1080/17470216008416717 target=_blank rel=noopener>https://doi.org/10.1080/17470216008416717</a> Wasserstein, R. L., & Lazar, N. A. (2016). The ASA statement on <em>p</em>-values: Context, process, and purpose. <em>The American Statistician, 70,</em> 129-133.
<a href=https://doi.org/10.1080/00031305.2016.1154108 target=_blank rel=noopener>https://doi.org/10.1080/00031305.2016.1154108</a> Webster, M.M., & Rutz, C. (2020). How STRANGE are your study animals? <em>Nature, 582,</em> 337–40.
<a href=https://doi.org/10.1038/d41586-020-01751-5 target=_blank rel=noopener>https://doi.org/10.1038/d41586-020-01751-5</a> Wendl, M. C. (2007). H-index: however ranked, citations need context. *Nature, 449(*7161), 403-403.
<a href=https://doi.org/10.1038/449403b target=_blank rel=noopener>https://doi.org/10.1038/449403b</a> Whitaker, K., & Guest, O. (2020). #bropenscience is broken science. <em>The Psychologist, 33,</em> 34-37. Wicherts, J. M., Veldkamp, C. L., Augusteijn, H. E., Bakker, M., Van Aert, R., & Van Assen, M. A. (2016). Degrees of freedom in planning, running, analyzing, and reporting psychological studies: A checklist to avoid p-hacking. <em>Frontiers in psychology, 7,</em> 1832.
<a href=https://doi.org/10.3389/fpsyg.2016.01832 target=_blank rel=noopener>https://doi.org/10.3389/fpsyg.2016.01832</a> Wilkinson, M. D., Dumontier, M., Aalbersberg, I. J., Appleton, G., Axton, M., Baak, A., &mldr; & Mons, B. (2016). The FAIR Guiding Principles for scientific data management and stewardship. <em>Scientific data, 3</em>(1), 1-9.
<a href=https://doi.org/10.1038/sdata.2016.18 target=_blank rel=noopener>https://doi.org/10.1038/sdata.2016.18</a> Wilson, B. & Fenner, M. (2012) Open Researcher & Contributor ID (ORCID): Solving the Name Ambiguity Problem. <em>Educause Review - E-Content 47</em>(3), 54-55.
<a href=https://er.educause.edu/articles/2012/5/open-researcher--contributor-id-orcid-solving-the-name-ambiguity-problem target=_blank rel=noopener>https://er.educause.edu/articles/2012/5/open-researcher&ndash;contributor-id-orcid-solving-the-name-ambiguity-problem</a> Wilson, R. C., & Collins, A. G. (2019). Ten simple rules for the computational modeling of behavioral data. <em>Elife, 8,</em> e49547.
<a href=https://doi.org/10.7554/eLife.49547 target=_blank rel=noopener>https://doi.org/10.7554/eLife.49547</a> Wingen, T., Berkessel, J. B., & Englich, B. (2020). No Replication, No Trust? How Low Replicability Influences Trust in Psychology. <em>Social Psychological and Personality Science, 11</em>(4).
<a href=https://doi.org/10.1177/1948550619877412 target=_blank rel=noopener>https://doi.org/10.1177/1948550619877412</a> Woelfe, M., Olliaro, P., & Todd, M. H. (2011). Open science is a research accelerator. <em>Nature chemistry, 3</em>(10), 745-748.
<a href=https://doi.org/10.1038/nchem.1149 target=_blank rel=noopener>https://doi.org/10.1038/nchem.1149</a> Wren, J. D., Valencia, A., & Kelso, J. (2019). Reviewer-coerced citation: case report, update on journal policy and suggestions for future prevention. <em>Bioinformatics, 35</em> (18), 3217-3218.
<a href=https://doi.org/10.1093/bioinformatics/btz071 target=_blank rel=noopener>https://doi.org/10.1093/bioinformatics/btz071</a> Wuchty, S., Jones, B. F., & Uzzi, B. (2007). The increasing dominance of teams in production of knowledge. <em>Science, 316</em>(5827), 1036-1039.
<a href=https://doi.org/10.1126/science.1136099 target=_blank rel=noopener>https://doi.org/10.1126/science.1136099</a> Xia, J., Harmon, J. L., Connolly, K. G., Donnelly, R. M., Anderson, M. R., & Howard, H. A. (2015). Who publishes in “predatory” journals?. <em>Journal of the Association for Information Science and Technology, 66</em>(7), 1406-1417.
<a href=https://doi.org/10.1002/asi.23265 target=_blank rel=noopener>https://doi.org/10.1002/asi.23265</a> Yamada, Y. (2018). How to crack pre-registration: Toward transparent and open science. <em>Frontiers in Psychology, 9</em>, 1831.
<a href=https://doi.org/10.3389/fpsyg.2018.01831 target=_blank rel=noopener>https://doi.org/10.3389/fpsyg.2018.01831</a> Yarkoni, T. (2020). The generalizability crisis. <em>Behavioral and Brain Sciences,</em> 1-37.
<a href=https://doi.org/10.1017/S0140525X20001685 target=_blank rel=noopener>https://doi.org/10.1017/S0140525X20001685</a> Yeung, S. K., Feldman, G., Fillon, A., Protzko, J., Elsherif, M. M., Xiao, Q., & Pickering, J. (2020a). Experimental Studies Meta-Analysis Registered Report Templates. [Manuscript in preparation]. Zurn, P., Bassett, D. S., & Rust, N. C. (2020). The Citation Diversity Statement: A Practice of Transparency, A Way of Life. <em>Trends in Cognitive Sciences, 24</em>(9), 669-672.
<a href=https://doi.org/10.1016/j.tics.2020.06.009 target=_blank rel=noopener>https://doi.org/10.1016/j.tics.2020.06.009</a></p><div class="alert alert-info mt-5"><div>We are currently working to link the references directly. For now, the complete reference list can be viewed <a href=https://forrt.org/glossary/references>here</a>.</div></div><div class=article-widget><div class="post-nav d-sm-flex justify-content-around"><div class="post-nav-item d-inline"><a href=/glossary/english/z_curve/ rel=next><div class=meta-nav>Previous</div><p>Z-Curve</p></a></div></div></div></div></div></article><div class="d-flex d-xl-none col-12 docs-toc"><nav id=TableOfContentsMobile class="nav flex-column"><ul><li class=nav-item data-toggle=tooltip data-placement=right title="The tendency to report only significant results in the abstract, while reporting non-significant results within the main body of the manuscript (not reporting non-significant results altogether would constitute selective reporting). The consequence of abstract bias is that studies reporting non-significant results may not be captured with standard meta-analytic search procedures (which rely on information in the title, abstract and keywords) and thus biasing the results of meta-analyses."><a href=https://forrt.org/glossary/english/abstract_bias/ class=nav-link>Abstract Bias</a></li><li class=nav-item data-toggle=tooltip data-placement=right title="The contribution that a research output (e.g., published manuscript) makes in shifting understanding and advancing scientific theory, method, and application, across and within disciplines. Impact can also refer to the degree to which an output or research programme influences change outside of academia, e.g. societal and economic impact (cf. ESRC: https://esrc.ukri.org/research/impact-toolkit/what-is-impact/)."><a href=https://forrt.org/glossary/english/academic_impact/ class=nav-link>Academic Impact</a></li><li class=nav-item data-toggle=tooltip data-placement=right title="Accessibility refers to the ease of access and re-use of materials (e.g., data, code, outputs, publications) for academic purposes, particularly the ease of access is afforded to people with a chronic illness, disability and/or neurodivergence. These groups face numerous financial, legal and/or technical barriers within research, including (but not limited to) the acquisition of appropriately formatted materials and physical access to spaces. Accessibility also encompasses structural concerns about diversity, equity, inclusion, and representation (Pownall et al., 2021). Interfaces, events and spaces should be designed with accessibility in mind to ensure full participation, such as by ensuring that web-based images are colorblind friendly and have alternative text, or by using live captions at events (Brown et al., 2018; Pollet & Bond, 2021; World Wide Web Consortium, 2021)."><a href=https://forrt.org/glossary/english/accessibility/ class=nav-link>Accessibility</a></li><li class=nav-item data-toggle=tooltip data-placement=right title="From Latin meaning “to the person”; Judgment of an argument or piece of work influenced by the characteristics of the person who forwarded it, not the characteristics of the argument itself. Ad hominem bias can be negative, as when work from a competitor or target of personal animosity is viewed more critically than the quality of the work merits, or positive, as when work from a friend benefits from overly favorable evaluation."><a href=https://forrt.org/glossary/english/ad_hominem_bias/ class=nav-link>Ad hominem bias</a></li><li class=nav-item data-toggle=tooltip data-placement=right title="A commentary in which the original authors of a work and critics of said work collaborate to draft a consensus statement. The aim is to draft a commentary that is free of ad hominem attacks and communicates a common understanding or at least identifies where both parties agree and disagree. In doing so, it provides a clear take-home message and path forward, rather than leaving the reader to decide between opposing views conveyed in separate commentaries."><a href=https://forrt.org/glossary/english/adversarial/ class=nav-link>Adversarial (collaborative) commentary</a></li><li class=nav-item data-toggle=tooltip data-placement=right title="A collaboration where two or more researchers with opposing or contradictory theoretical views —and likely diverging predictions about study results— work together on one project. The aim is to minimise biases and methodological weaknesses as well as to establish a shared base of facts for which competing theories must account."><a href=https://forrt.org/glossary/english/adversarial_collaboration/ class=nav-link>Adversarial collaboration</a></li><li class=nav-item data-toggle=tooltip data-placement=right title="This bias occurs when one’s opinions or judgements about the quality of research are influenced by the affiliation of the author(s). When publishing manuscripts, a potential example of an affiliation bias could be when editors prefer to publish work from prestigious institutions (Tvina et al., 2019)."><a href=https://forrt.org/glossary/english/affiliation_bias/ class=nav-link>Affiliation bias</a></li><li class=nav-item data-toggle=tooltip data-placement=right title="Variability in outcomes due to unknowable or inherently random factors. The stochastic component of outcome uncertainty that cannot be reduced through additional sources of information. For example, when flipping a coin, uncertainty about whether it will land on heads or tails."><a href=https://forrt.org/glossary/english/aleatoric_uncertainty/ class=nav-link>Aleatoric uncertainty</a></li><li class=nav-item data-toggle=tooltip data-placement=right title="Departing from traditional citation measures, altmetrics (short for “alternative metrics”) provide an assessment of the attention and broader impact of research work based on diverse sources such as social media (e.g. Twitter), digital news media, number of preprint downloads, etc. Altmetrics have been criticized in that sensational claims usually receive more attention than serious research (Ali, 2021)."><a href=https://forrt.org/glossary/english/altmetrics/ class=nav-link>Altmetrics</a></li><li class=nav-item data-toggle=tooltip data-placement=right title="AMNESIA is a free anonymization tool to remove identifying information from data. After uploading a dataset that contains personal data, the original dataset is transformed by the tool, resulting in a dataset that is anonymized regarding personal and sensitive data."><a href=https://forrt.org/glossary/english/amnesia/ class=nav-link>AMNESIA</a></li><li class=nav-item data-toggle=tooltip data-placement=right title="Analytic flexibility is a type of researcher degrees of freedom (Simmons, Nelson, & Simonsohn, 2011\) that refers specifically to the large number of choices made during data preprocessing and statistical analysis. “\[T\]he range of analysis outcomes across different acceptable analysis methods” (Carp, 2012, p. 1). Analytic flexibility can be problematic, as this variability in analytic strategies can translate into variability in research outcomes, particularly when several strategies are applied, but not transparently reported (Masur, 2021)."><a href=https://forrt.org/glossary/english/analytic_flexibility/ class=nav-link>Analytic Flexibility</a></li><li class=nav-item data-toggle=tooltip data-placement=right title="Anonymising data refers to removing, generalising, aggregating or distorting any information which may potentially identify participants, peer-reviewers, and authors, among others. Data should be anonymised so that participants are not personally identifiable. The most basic level of anonymisation is to replace participants’ names with pseudonyms (fake names) and remove references to specific places. Anonymity is particularly important for open data and data may not be made open for anonymity concerns. Anonymity and open data has been discussed within qualitative research which often focuses on personal experiences and opinions, and in quantitative research that includes participants from clinical populations."><a href=https://forrt.org/glossary/english/anonymity/ class=nav-link>Anonymity</a></li><li class=nav-item data-toggle=tooltip data-placement=right title="The ARRIVE guidelines (Animal Research: Reporting of In Vivo Experiments) are a checklist-based set of reporting guidelines developed to improve reporting standards, and enhance replicability, within living (i.e. *in vivo*) animal research. The second generation ARRIVE guidelines, ARRIVE 2.0, were released in 2020\. In these new guidelines, the clarity has been improved, items have been prioritised and new information has been added with an accompanying “Explanation” and “Elaboration” document to provide a rationale for each item and a recommended set to add context to the study being described."><a href=https://forrt.org/glossary/english/arrive_guidelines/ class=nav-link>ARRIVE Guidelines</a></li><li class=nav-item data-toggle=tooltip data-placement=right title="An article (sometimes author) processing charge (APC) is a fee charged to authors by a publisher in exchange for publishing and hosting an open access article. APCs are often intended to compensate for a potential loss of revenue the journal may experience when moving from traditional publication models, such as subscription services or pay-per-view, to open access. While some journals charge only about US$300, APCs vary widely, from US$1000 (Advances in Methods and Practice in Psychological Science) or less to over US$10,000 (Nature). While some publishers offer waivers for researchers from certain regions of the world or who lack funds, some APCs have been criticized for being disproportionate compared to actual processing and hosting costs (Grossmann & Brembs, 2021\) and for creating possible inequities with regard to which scientists can afford to make their works freely available (Smith et al. 2020)."><a href=https://forrt.org/glossary/english/article_processing_charge/ class=nav-link>Article Processing Charge (APC)</a></li><li class=nav-item data-toggle=tooltip data-placement=right title="Authorship assigns credit for research outputs (e.g. manuscripts, data, and software) and accountability for content (McNutt et al. 2018; Patience et al. 2019). Conventions differ across disciplines, cultures, and even research groups, in their expectations of what efforts earn authorship, what the order of authorship signifies (if anything), how much accountability for the research the corresponding author assumes, and the extent to which authors are accountable for aspects of the work that they did not personally conduct."><a href=https://forrt.org/glossary/english/authorship/ class=nav-link>Authorship</a></li><li class=nav-item data-toggle=tooltip data-placement=right title="All theories contain assumptions about the nature of constructs and how they can be measured. However, not all predictions are derived from theories and assumptions can sometimes be drawn from other premises. Additional assumptions that are made to deduce a prediction and tested by making links to observable data. These auxiliary hypotheses are sometimes invoked to explain why a replication attempt has failed."><a href=https://forrt.org/glossary/english/auxiliary_hypothesis/ class=nav-link>Auxiliary Hypothesis</a></li><li class=nav-item data-toggle=tooltip data-placement=right title="Badges are symbols that editorial teams add to published manuscripts to acknowledge open science practices and act as incentives for researchers to share data, materials, or to embed study preregistration. As clearly-visible symbols, they are intended to signal to the reader that content has met the standard of open research required to receive the badge (typically from that journal). Different badges may be assigned for different practices, such as research having been made available and accessible in a persistent location (“open material badge” and “open data badge”), or study preregistration (“preregistration badge”)."><a href=https://forrt.org/glossary/english/badges/ class=nav-link>Badges (Open Science)</a></li><li class=nav-item data-toggle=tooltip data-placement=right title="A continuous statistical measure for model selection used in Bayesian inference, describing the *relative* evidence for one model over another, regardless of whether the models are correct. Bayes factors (BF) range from 0 to infinity, indicating the relative strength of the evidence, and where 1 is a neutral point of no evidence. In contrast to *p*\-values, Bayes factors allow for 3 types of conclusions: a) evidence for the alternative hypothesis, b) evidence for the null hypothesis, and c) no sufficient evidence for either. Thus, BF are typically expressed as BF10 for evidence regarding the alternative compared to the null hypothesis, and as BF01 for evidence regarding the null compared to the alternative hypothesis."><a href=https://forrt.org/glossary/english/bayes_factor/ class=nav-link>Bayes Factor</a></li><li class=nav-item data-toggle=tooltip data-placement=right title="A method of statistical inference based upon Bayes’ theorem, which makes use of epistemological (un)certainty using the mathematical language of probability. Bayesian inference is based on allocating (and reallocating, based on newly-observed data or evidence) credibility across possibilities. Two existing approaches to Bayesian inference include “Bayes factors” (BF) and Bayesian parameter estimation."><a href=https://forrt.org/glossary/english/bayesian_inference/ class=nav-link>Bayesian Inference</a></li><li class=nav-item data-toggle=tooltip data-placement=right title="A Bayesian approach to estimating parameter values by updating a prior belief about model parameters (i.e., prior distribution) with new evidence (i.e., observed data) via a likelihood function, resulting in a posterior distribution. The posterior distribution may be summarised in a number of ways including: point estimates (mean/mode/median of a posterior probability distribution), intervals of defined boundaries, and intervals of defined mass (typically referred to as a credible interval). In turn, a posterior distribution may become a prior distribution in a subsequent estimation. A posterior distribution can also be sampled using Monte-Carlo Markov Chain methods which can be used to determine complex model uncertainties (e.g. Foreman-Mackey et al., 2013)."><a href=https://forrt.org/glossary/english/bayesian_parameter_estimation/ class=nav-link>Bayesian Parameter Estimation</a></li><li class=nav-item data-toggle=tooltip data-placement=right title="The Brain Imaging Data Structure (BIDS) describes a simple and easy-to-adopt way of organizing neuroimaging, electrophysiological, and behavioral data (i.e., file formats, folder structures). BIDS is a community effort developed *by* the community *for* the community and was inspired by the format used internally by the OpenfMRI repository known as [OpenNeuro](https://openneuro.org). Having initially been developed for fMRI data, the BIDS data structure has been extended for many other measures, such as EEG (Pernet et al., 2019)."><a href=https://forrt.org/glossary/english/bids_data_structure/ class=nav-link>BIDS data structure</a></li><li class=nav-item data-toggle=tooltip data-placement=right title="This acronym refers to Barren, Institutional, Zoo, and other Rare Rearing Environments (BIZARRE). Most research for chimpanzees is conducted on this specific sample. This limits the generalizability of a large number of research findings in the chimpanzee population. The BIZARRE has been argued to reflect the universal concept of what is a chimpanzee (see also WEIRD, which has been argued to be a universal concept for what is a human)."><a href=https://forrt.org/glossary/english/bizarre/ class=nav-link>BIZARRE</a></li><li class=nav-item data-toggle=tooltip data-placement=right title='Within academic culture, an approach focusing on the intrinsic interest of academics to improve the quality of research and research culture, for instance by making it supportive, collaborative, creative and inclusive. Usually indicates leadership from early-career researchers acting as the changemakers driving shifts and change in scientific methodology through enthusiasm and innovation, compared to a “top-down” approach initiated by more senior researchers "Bottom-up approaches take into account the specific local circumstances of the case itself, often using empirical data, lived experience, personal accounts, and circumstances as the starting point for developing policy solutions."'><a href=https://forrt.org/glossary/english/bottom_up_approach/ class=nav-link>Bottom-up approach (to Open Scholarship)</a></li><li class=nav-item data-toggle=tooltip data-placement=right title="Bracketing interviews are commonly used within qualitative approaches. During these interviews researchers explore their personal subjectivities and assumptions surrounding their ongoing research. This allows researchers to be aware of their own interests and helps them to become both more reflective and critical about their research, considering how their own experiences may impact the research process. Bracketing interviews can also be subject to qualitative analysis."><a href=https://forrt.org/glossary/english/bracketing_interviews/ class=nav-link>Bracketing Interviews</a></li><li class=nav-item data-toggle=tooltip data-placement=right title="A tongue-in-cheek expression intended to raise awareness of the lack of diverse voices in open science (Bahlai, Bartlett, Burgio et al. 2019; Onie, 2020), in addition to the presence of behavior and communication styles that can be toxic or exclusionary. Importantly, not all bros are men; rather, they are individuals who demonstrate rigid thinking, lack self-awareness, and tend towards hostility, unkindness, and exclusion (Pownall et al., 2021; Whitaker & Guest, 2020). They generally belong to dominant groups who benefit from structural privileges. To address \#bropenscience, researchers should examine and address structural inequalities within academic systems and institutions."><a href=https://forrt.org/glossary/english/bropenscience/ class=nav-link>Bropenscience</a></li><li class=nav-item data-toggle=tooltip data-placement=right title="Critiquing After the Results are Known (CARKing) refers to presenting a criticism of a design as one that you would have made in advance of the results being known. It usually forms a reaction or criticism to unwelcome or unfavourable results, results whether the critic is conscious of this fact or not."><a href=https://forrt.org/glossary/english/carking/ class=nav-link>CARKing</a></li><li class=nav-item data-toggle=tooltip data-placement=right title="A non-profit technology organization based in Charlottesville, Virginia with the mission “to increase openness, integrity, and reproducibility of research.” Among other resources, the COS hosts the Open Science Framework (OSF) and the Open Scholarship Knowledge Base."><a href=https://forrt.org/glossary/english/center_for_open_science/ class=nav-link>Center for Open Science (COS)</a></li><li class=nav-item data-toggle=tooltip data-placement=right title="A biased selection of papers or authors cited and included in the references section. When citation bias is present, it is often in a way which would benefit the author(s) or reviewers, over-represents statistically significant studies, or reflects pervasive gender or racial biases (Brooks, 1985; Jannot et al., 2013; Zurn et al., 2020). One proposed solution is the use of Citation Diversity Statements, in which authors reflect on their citation practices and identify biases which may have emerged (Zurn et al., 2020)."><a href=https://forrt.org/glossary/english/citation_bias/ class=nav-link>Citation bias</a></li><li class=nav-item data-toggle=tooltip data-placement=right title="A current effort trying to increase awareness and mitigate the citation bias in relation to gender and race is the Citation Diversity Statement, a short paragraph where “the authors consider their own bias and quantify the equitability of their reference lists. It states: (i) the importance of citation diversity, (ii) the percentage breakdown (or other diversity indicators) of citations in the paper, (iii) the method by which percentages were assessed and its limitations, and (iv) a commitment to improving equitable practices in science” (Zurn et al., 2020, p. 669)."><a href=https://forrt.org/glossary/english/citation_diversity_statement/ class=nav-link>Citation Diversity Statement</a></li><li class=nav-item data-toggle=tooltip data-placement=right title="Citizen science refers to projects that actively involve the general public in the scientific endeavour, with the goal of democratizing science. Citizen scientists can be involved in all stages of research, acting as collaborators, contributors or project leaders. An example of a major citizen science project involved individuals identifying astronomical bodies (Lintott, 2008)."><a href=https://forrt.org/glossary/english/citizen_science/ class=nav-link>Citizen Science</a></li><li class=nav-item data-toggle=tooltip data-placement=right title="The Comprehensive Knowledge Archive Network (CKAN) is an open-source data platform and free software that aims to provide tools to streamline publishing and data sharing. CKAN supports governments, research institutions and other organizations in managing and publishing large amounts of data."><a href=https://forrt.org/glossary/english/ckan/ class=nav-link>CKAN</a></li><li class=nav-item data-toggle=tooltip data-placement=right title="An approach to research where stakeholders who are not traditionally involved in the research process are empowered to collaborate, either at the start of the project or throughout the research lifecycle. For example, co-produced health research may involve health professionals and patients, while co-produced education research may involve teaching staff and pupils/students. This is motivated by principles such as respecting and valuing the experiences of non-researchers, addressing power dynamics, and building mutually beneficial relationships."><a href=https://forrt.org/glossary/english/co_production/ class=nav-link>Co-production</a></li><li class=nav-item data-toggle=tooltip data-placement=right title="A framework which identifies best practices for scientific repositories and evaluation criteria for these practices. Its flexible and multidimensional approach means that it can be applied to different types of repositories, including those which host publications or data, across geographical and thematic contexts."><a href=https://forrt.org/glossary/english/coar_community_framework_for_good_practices_in_repositories/ class=nav-link>COAR Community Framework for Good Practices in Repositories</a></li><li class=nav-item data-toggle=tooltip data-placement=right title="The process of checking another researcher's programming (specifically, computer source code) including but not limited to statistical code and data modelling. This process is designed to detect and resolve mistakes, thereby improving code quality. In practice, a modern peer review process may take place via a hosted online repository such as GitHub, GitLab or SourceForge.Related terms: Reproducibility; Version control"><a href=https://forrt.org/glossary/english/code_review/ class=nav-link>Code review</a></li><li class=nav-item data-toggle=tooltip data-placement=right title="A codebook is a high-level summary that describes the contents, structure, nature and layout of a data set. A well-documented codebook contains information intended to be complete and self-explanatory for each variable in a data file, such as the wording and coding of the item, and the underlying construct. It provides transparency to researchers who may be unfamiliar with the data but wish to reproduce analyses or reuse the data."><a href=https://forrt.org/glossary/english/codebook/ class=nav-link>Codebook</a></li><li class=nav-item data-toggle=tooltip data-placement=right title="The Collaborative Replication and Education Project (CREP) is an initiative designed to organize and structure replication efforts of highly-cited empirical studies in psychology to satisfy the dual needs for more high-quality direct replications and more training in empirical research techniques for psychology students. CREP aims to address the need for replications of highly cited studies, and to provide training, support and professional growth opportunities for academics completing replication projects."><a href=https://forrt.org/glossary/english/collaborative_replication_and_education_project/ class=nav-link>Collaborative Replication and Education Project (CREP)</a></li><li class=nav-item data-toggle=tooltip data-placement=right title="The Organization for Human Brain Mapping (OHBM) neuroimaging community has developed a guideline for best practices in neuroimaging data acquisition, analysis, reporting, and sharing of both data and analysis code. It contains eight elements that should be included when writing up or submitting a manuscript in order to improve reporting methods and the resulting neuroimages in order to optimize transparency and reproducibility. Alternative definition: (if applicable) Checklist for data analysis and sharing"><a href=https://forrt.org/glossary/english/committee_on_best_practices_in_data_analysis_and_sharing/ class=nav-link>Committee on Best Practices in Data Analysis and Sharing (COBIDAS)</a></li><li class=nav-item data-toggle=tooltip data-placement=right title="The common ownership of scientific results and methods and the consequent imperative to share both freely. Communality is based on the fact that every scientific finding is seen as a product of the effort of a number of agents. This norm is followed when scientists openly share their new findings with colleagues."><a href=https://forrt.org/glossary/english/communality/ class=nav-link>Communality</a></li><li class=nav-item data-toggle=tooltip data-placement=right title="Collaborative projects that involve researchers from different career levels, disciplines, institutions or countries. Projects may have different goals including peer support and learning, conducting research, teaching and education. They can be short-term (e.g., conference events or hackathons) or long-term (e.g., journal clubs or consortium-led research projects). Collaborative culture and community building are key to achieving project goals."><a href=https://forrt.org/glossary/english/community_projects/ class=nav-link>Community Projects</a></li><li class=nav-item data-toggle=tooltip data-placement=right title="A collection of files prepared by a researcher to support a report or publication that include the data, metadata, programming code, software dependencies, licenses, and other instructions necessary for another researcher to independently reproduce the findings presented in the report or publication."><a href=https://forrt.org/glossary/english/compendium/ class=nav-link>Compendium</a></li><li class=nav-item data-toggle=tooltip data-placement=right title="Ability to recreate the same results as the original study (including tables, figures, and quantitative findings), using the same input data, computational methods, and conditions of analysis. The availability of code and data facilitates computational reproducibility, as does preparation of these materials (annotating data, delineating software versions used, sharing computational environments, etc). Ideally, computational reproducibility should be achievable by another second researcher (or the original researcher, at a future time), using only a set of files and written instructions. Also referred to as analytic reproducibility (LeBel et al., 2018)."><a href=https://forrt.org/glossary/english/computational_reproducibility/ class=nav-link>Computational reproducibility</a></li><li class=nav-item data-toggle=tooltip data-placement=right title="A replication attempt whereby the primary effect of interest is the same but tested in a different sample and captured in a different way to that originally reported (i.e., using different operationalisations, data processing and statistical approaches and/or different constructs; LeBel et al., 2018). The purpose of a conceptual replication is often to explore what conditions limit the extent to which an effect can be observed and generalised (e.g., only within certain contexts, with certain samples, using certain measurement approaches) towards evaluating and advancing theory (Hüffmeier et al., 2016)."><a href=https://forrt.org/glossary/english/conceptual_replication/ class=nav-link>Conceptual replication</a></li><li class=nav-item data-toggle=tooltip data-placement=right title="The tendency to seek out, interpret, favor and recall information in a way that supports one’s prior values, beliefs, expectations, or hypothesis."><a href=https://forrt.org/glossary/english/confirmation_bias/ class=nav-link>Confirmation bias</a></li><li class=nav-item data-toggle=tooltip data-placement=right title="Part of the confirmatory-exploratory distinction (Wagenmakers et al., 2012), where confirmatory analyses refer to analyses that were set *a priori* and test existent hypotheses. The lack of this distinction within published research findings has been suggested to explain replicability issues and is suggested to be overcome through study preregistration which clearly distinguishes confirmatory from exploratory analyses. Other researchers have questioned these terms and recommended a replacement with ‘discovery-oriented’ and ‘theory-testing research’ (Oberauer & Lewandowsky, 2019; see also Szollosi & Donkin, 2019)."><a href=https://forrt.org/glossary/english/confirmatory_analyses/ class=nav-link>Confirmatory analyses</a></li><li class=nav-item data-toggle=tooltip data-placement=right title="A conflict of interest (COI, also ‘competing interest’) is a financial or non-financial relationship, activity or other interest that might compromise objectivity or professional judgement on the part of an author, reviewer, editor, or editorial staff. The *Principles of Transparency and Best Practice in Scholarly Publishing* by the Committee on Publication Ethics (COPE), the Directory of Open Access Journals (DOAJ), the Open Access Scholarly Publishers Association (OASPA), and the World Association of Medical Editors (WAME) states that journals should have policies on publication ethics, including policies on COI (DOAJ, 2018). COIs should be made transparent so that readers can properly evaluate research and assess for potential or actual bias(es). Outside publishing, academic presenters, panel members and educators should also declare COIs. Purposeful failure to disclose a COI may be considered a form of misconduct."><a href=https://forrt.org/glossary/english/conflict_of_interest/ class=nav-link>Conflict of interest</a></li><li class=nav-item data-toggle=tooltip data-placement=right title="Only the name of the consortium or organization appears in the author column, and the individuals' names do not appear in the literature: For example, ‘FORRT’ as an author. This can be seen in the products of collaborative projects with a very large number of collaborators and/or contributors. Depending on the journal policy, individual researchers may be recorded as one of the authors of the product in literature databases such as ORCID and Scopus. Consortium authorship can also be termed group, corporate, organisation/organization or collective authorship (e.g. [https://www.bmj.com/about-bmj/resources-authors/article-submission/authorship-contributorship](https://www.bmj.com/about-bmj/resources-authors/article-submission/authorship-contributorship)), or collaborative authorship (e.g. [https://support.jmir.org/hc/en-us/articles/115001449591-What-is-a-group-author-collaborative-author-and-does-it-need-an-ORCID](https://support.jmir.org/hc/en-us/articles/115001449591-What-is-a-group-author-collaborative-author-and-does-it-need-an-ORCID))"><a href=https://forrt.org/glossary/english/consortium_authorship/ class=nav-link>Consortium authorship</a></li><li class=nav-item data-toggle=tooltip data-placement=right title="A statement that explicitly identifies and justifies the target population, and conditions, for the reported findings. Researchers should be explicit about potential boundary conditions for their generalisations (Simons et al., 2017). Researchers should provide detailed descriptions of the sampled population and/or contextual factors that might have affected the results such that future replication attempts can take these factors into account (Brandt et al., 2014). Conditions not explicitly listed are assumed not to have theoretical relevance to the replicability of the effect."><a href=https://forrt.org/glossary/english/constraints_on_generality/ class=nav-link>Constraints on Generality (COG)</a></li><li class=nav-item data-toggle=tooltip data-placement=right title="When used in the context of measurement and testing, construct validity refers to the degree to which a test measures what it claims to be measuring. In fields that study hypothetical unobservable entities, construct validation is essentially theory testing, because it involves determining whether an objective measure (a questionnaire, lab task, etc.) is a valid representation of a hypothetical construct (i.e., conforms to a theory). When used in a broader sense about a research study, or a claim, conclusion, or observed effect in a research study, construct validity concerns the extent to which the sampling particulars used in the study (participants, settings, treatments, and dependent variables) map onto the higher order constructs the study, the claim, the conclusion is about. According to Shadish et al. (2002), construct validity can be defined as “the degree to which inferences are warranted from the observed persons, settings, and cause and effect operations included in a study to the constructs that these instances might represent” (p. 38)."><a href=https://forrt.org/glossary/english/construct_validity/ class=nav-link>Construct validity</a></li><li class=nav-item data-toggle=tooltip data-placement=right title="The degree to which a measurement includes all aspects of the concept that the researcher claims to measure; “A qualitative type of validity where the domain of the concept is made clear and the analyst judges whether the measures fully represent the domain” (Bollen, 1989, p.185). It is a component of *construct validity* and can be established using both quantitative and qualitative methods, often involving expert assessment."><a href=https://forrt.org/glossary/english/content_validity/ class=nav-link>Content validity</a></li><li class=nav-item data-toggle=tooltip data-placement=right title><a href=https://forrt.org/glossary/english/contribution/ class=nav-link>Contribution</a></li><li class=nav-item data-toggle=tooltip data-placement=right title="A corrigendum (pl. corrigenda, Latin: 'to correct') documents one or multiple errors within a published work that do not alter the central claim or conclusions and thus does not rise to the standard of requiring a retraction of the work. Corrigenda are typically available alongside the original work to aid transparency. Some publishers refer to this document as an erratum (pl. errata, Latin: 'error'), while others draw a distinction between the two (corrigenda as author-errors and errata as publisher-errors)."><a href=https://forrt.org/glossary/english/corrigendum/ class=nav-link>Corrigendum</a></li><li class=nav-item data-toggle=tooltip data-placement=right title="A set of free and easy-to-use copyright licences that define the rights of the authors and users of open data and materials in a standardized way. CC licenses enable authors or creators to share copyright-law-protected work with the public and come in different varieties with more or less clauses. For example, the Attribution-NonCommercial-ShareAlike 4.0 International (CC BY-NC-SA 4.0) allows you to share and adapt the material, under the condition that you; give credit to the original creators, indicate if changes were made, and share under the same license as the original, and you cannot use the material for commercial purposes."><a href=https://forrt.org/glossary/english/creative_commons/ class=nav-link>Creative Commons (CC) license</a></li><li class=nav-item data-toggle=tooltip data-placement=right title="Replication efforts should seek not just to support or question the original findings, but also to replace them with revised, stronger theories with greater explanatory power. This approach therefore involves ‘pruning’ existing theories, comparing all the alternative theories, and making replication efforts more generative and engaged in theory-building (Tierney et al. 2020, 2021)."><a href=https://forrt.org/glossary/english/creative_destruction_approach_to_replication/ class=nav-link>Creative destruction approach to replication</a></li><li class=nav-item data-toggle=tooltip data-placement=right title="The problems and the solutions resulting from a growing distrust in scientific findings, following concerns about the credibility of scientific claims (e.g., low replicability). The term has been proposed as a more positive alternative to the term replicability crisis, and includes the many solutions to improve the credibility of research, such as preregistration, transparency, and replication."><a href=https://forrt.org/glossary/english/credibility_revolution/ class=nav-link>Credibility revolution</a></li><li class=nav-item data-toggle=tooltip data-placement=right title="The Contributor Roles Taxonomy (CRediT; [https://casrai.org/credit/](https://casrai.org/credit/)) is a high-level taxonomy used to indicate the roles typically adopted by contributors to scientific scholarly output. There are currently 14 roles that describe each contributor’s specific contribution to the scholarly output. They can be assigned multiple times to different authors and one author can also be assigned multiple roles. CRediT includes the following roles: Conceptualization, Data curation, Formal Analysis, Funding acquisition, Investigation, Methodology, Project administration, Resources, Software, Supervision, Validation, Visualization, Writing – original draft, Writing – review & editing. A description of the different roles can be found in the work of Brand et al., (2015)."><a href=https://forrt.org/glossary/english/credit/ class=nav-link>CRediT</a></li><li class=nav-item data-toggle=tooltip data-placement=right title="The degree to which a measure corresponds to other valid measures of the same concept. Criterion validity is usually established by calculating regression coefficients or bivariate correlations estimating the direction and strength of relation between test measure and criterion measure. It is often confused with *construct validity* although it differs from it in intent (merely predictive rather than theoretical) and interest (predicting an observable outcome rather than a latent construct). Unreliability in either test or criterion scores usually diminishes criterion validity. Also called criterion-related or concrete validity."><a href=https://forrt.org/glossary/english/criterion_validity/ class=nav-link>Criterion validity</a></li><li class=nav-item data-toggle=tooltip data-placement=right title="Crowdsourced research is a model of the social organisation of research as a large-scale collaboration in which one or more research projects are conducted by multiple teams in an independent yet coordinated manner. Crowdsourced research aims at achieving efficiency and scalability gains by pooling resources, promoting transparency and social inclusion, as well as increasing the rigor, reliability, and trustworthiness by enhancing statistical power and mutual social vetting. It stands in contrast to the traditional model of academic research production, which is dominated by the independent work of individual or small groups of researchers (‘small science’). Examples of crowdsourced research include so-called ‘many labs replication’ studies (Klein et al., 2018), ‘many analysts, one dataset’ studies (Silberzahn et al., 2018), distributive collaborative networks (Moshontz et al., 2018\) and open collaborative writing projects such as Massively Open Online Papers (MOOPs) (Himmelstein et al., 2019; Tennant et al., 2019). Alternatively, crowdsourced research can refer to the use of a large number of research “crowdworkers” in data collection hired through online labor markets like Amazon Mechanical Turk or Prolific, for example in content analysis (Benoit et al., 2016; Lind et al., 2017\) or experimental research (Peer et al., 2017). Crowdsourced research that is both open for participation and open through shared intermediate outputs has been referred to as *crowd science* (Franzoni & Sauermann, 2014)."><a href=https://forrt.org/glossary/english/crowdsourced_research/ class=nav-link>Crowdsourced Research</a></li><li class=nav-item data-toggle=tooltip data-placement=right title="The additional labor expected or demanded of members of underrepresented or marginalized minority groups, particularly scholars of color. This labor often comes from service roles providing ethnic, cultural, or gender representation and diversity. These roles can be formal or informal, and are generally unrewarded or uncompensated. Such labor includes providing expertise on matters of diversity, educating members of majority groups, acting as a liaison to minority communities, and formal and informal roles as mentor and support system for minority students."><a href=https://forrt.org/glossary/english/cultural_taxation/ class=nav-link>Cultural taxation</a></li><li class=nav-item data-toggle=tooltip data-placement=right title="Goal of any empirical science, it is the pursuit of “the construction of a cumulative base of knowledge upon which the future of the science may be built” (Curran, 2009, p. 1). The idea that science will create more complete and accurate theories as a function of the amount of evidence and data that has been collected. Cumulative science develops in gradual and incremental steps, as opposed to one abrupt discovery. While revolutionary science occurs scarcely, cumulative science is the most common form of science."><a href=https://forrt.org/glossary/english/cumulative_science/ class=nav-link>Cumulative science</a></li><li class=nav-item data-toggle=tooltip data-placement=right title="Data Access and Research Transparency ([DA-RT](https://www.dartstatement.org/)) is an initiative aimed at increasing data access and research transparency in the social sciences. It is a multi-epistemic and multi-method initiative, created in 2014 by the Council of the American Political Science Association (APSA), to bolster the rigor of empirical social inquiry. In addition to other activities, DA-RT developed the Journal Editors' Transparency Statement (JETS), which requires subscribing journals to (a) making relevant data publicly available if the study is published, (b) following a strict data citation policy, (c) transparently describing the analytical procedures and, if possible, providing public access to analytical code, and (d) updating their journal style guides, codes of ethics to include improved data access and research transparency requirements."><a href=https://forrt.org/glossary/english/data_access_and_research_transparency/ class=nav-link>Data Access and Research Transparency (DA-RT)</a></li><li class=nav-item data-toggle=tooltip data-placement=right title="A structured document that describes the process of data acquisition, analysis, management and storage during a research project. It also describes data ownership and how the data will be preserved and shared during and upon completion of a project. Data management templates also provide guidance on how to make research data FAIR and where possible, openly available."><a href=https://forrt.org/glossary/english/data_management_plan/ class=nav-link>Data management plan (DMP)</a></li><li class=nav-item data-toggle=tooltip data-placement=right title="collection of practices, technologies, cultural elements and legal frameworks that are relevant to the practice of making data used for scholarly research available to other investigators. Gollwitzer et al. (2020) describe two types of data sharing: Type 1: Data that is necessary to reproduce the findings of a published research article. Type 2: data that have been collected in a research project but have not (or only partly) been analysed or reported after the completion of the project and are hence typically shared under a specified embargo period."><a href=https://forrt.org/glossary/english/data_sharing/ class=nav-link>Data sharing</a></li><li class=nav-item data-toggle=tooltip data-placement=right title="Graphical representation of data or information. Data visualisation takes advantage of humans’ well-developed visual processing capacity to convey insight and communicate key information. Data visualisations often display the raw data, descriptive statistics, and/or inferential statistics."><a href=https://forrt.org/glossary/english/data_visualisation/ class=nav-link>Data visualisation</a></li><li class=nav-item data-toggle=tooltip data-placement=right title="Coloniality can be described as the naturalisation of concepts such as imperialism, capitalism, and nationalism. Together these concepts can be thought of as a matrix of power (and power relations) that can be traced to the colonial period. Decoloniality seeks to break down and decentralize those power relations, with the aim to understand their persistence and to reconstruct the norms and values of a given domain. In an academic setting, decolonisation refers to the rethinking of the lens through which we teach, research, and co-exist, so that the lens generalises beyond Western-centred and colonial perspectives. Decolonising academia involves reconstructing the historical and cultural frameworks being used, redistributing a sense of belonging in universities, and empowering and including voices and knowledge types that have historically been excluded from academia. This is done when people engage with their past, present, and future whilst holding a perspective that is separate from the socially dominant perspective. Also, by including, not rejecting, an individuals’ internalised norms and taboos from the specific colony."><a href=https://forrt.org/glossary/english/decolonisation/ class=nav-link>Decolonisation</a></li><li class=nav-item data-toggle=tooltip data-placement=right title="A criterion for distinguishing science from non-science which aims to indicate an optimal way for knowledge of the world to grow. In a Popperian approach, the demarcation criterion was falsifiability and the application of a falsificationist attitude. Alternative approaches include that of Kuhn, who believed that the criterion was puzzle solving with the aim of understanding nature, and Lakatos, who argued that science is marked by working within a progressive research programme."><a href=https://forrt.org/glossary/english/demarcation_criterion/ class=nav-link>Demarcation criterion</a></li><li class=nav-item data-toggle=tooltip data-placement=right title="As ‘direct replication’ does not have a widely-agreed technical meaning nor there is no clear cut distinction between a direct and conceptual replication, below we list several contributions towards a consensus. Rather than debating the ‘exactness’ of a replication, it is more helpful to discuss the relevant differences between a replication and its target, and their implications for the reliability and generality of the target’s results. Generally, direct replication refers to a new data collection that attempts to replicate original studies’ methods as closely as possible. A replication attempt that “seek(s) to duplicate the necessary elements that produced the original finding.” (Cruwell et al., 2019; p.243). The purpose of a direct replication can be to identify type 1 errors and/or experimenter effects, determine the replicability of an effect using the same or improved practices, or to create more specific estimates of effect size (Hűffmeier et al., 2016). Directness of replication is a continuum between repeating specific observations (data) and observing generalised effects (phenomena). How closely a replication replicates an original study is often a matter for debate, often with differences being cited as hidden moderators of effects. Furthermore, there can be debate over the relevant importance of technical equivalence (i.e., using identical materials) versus psychological equivalence (i.e., realizing the identical psychological conditions) to the original study (Schwarz and Strack, 2014). For example, consider a study on Trust in the US- President conducted in 2018\. A technical equivalent replication would use Trump as stimulus (he was president in 2018\) a psychological equivalent study would use Biden (he is the current president)."><a href=https://forrt.org/glossary/english/direct_replication/ class=nav-link>Direct replication</a></li><li class=nav-item data-toggle=tooltip data-placement=right title="Diversity refers to between-person (i.e., interindividual) variation in humans, e.g. ability, age, beliefs, cognition, country, disability, ethnicity, gender, language, race, religion or sexual orientation. Diversity can refer to diversity of researchers (who do the research), the diversity of participant samples (who is included in the study), and diversity of perspectives (the views and beliefs researchers bring into their work; Syed & Kathawalla, 2020)."><a href=https://forrt.org/glossary/english/diversity/ class=nav-link>Diversity</a></li><li class=nav-item data-toggle=tooltip data-placement=right title="Digital Object Identifiers (DOI) are alpha-numeric strings that can be assigned to any entity, including: publications (including preprints), materials, datasets, and feature films \- the use of DOIs is not restricted to just scholarly or academic material. DOIs “provides a system for persistent and actionable identification and interoperable exchange of managed information on digital networks.” ([https://doi.org/hb.html](https://doi.org/hb.html)). There are many different DOI registration agencies that operate DOIs, but the two that researchers would most likely encounter are [Crossref](https://www.crossref.org/) and [Datacite](https://datacite.org/)."><a href=https://forrt.org/glossary/english/doi/ class=nav-link>DOI (digital object identifier)</a></li><li class=nav-item data-toggle=tooltip data-placement=right title="The San Francisco Declaration on Research Assessment (DORA) is a global initiative aiming to reduce dependence on journal-based metrics (e.g. journal impact factor and citation counts) and, instead, promote a culture which emphasises the intrinsic value of research. The DORA declaration targets research funders, publishers, research institutes and researchers and signing it represents a commitment to aligning research practices and procedures with the declaration’s principles."><a href=https://forrt.org/glossary/english/dora/ class=nav-link>DORA</a></li><li class=nav-item data-toggle=tooltip data-placement=right title="An identity confusion, as the individual feels like they have two distinct identities. One is to assimilate to the dominant culture at university when the individual is with colleagues and professors, while the other is when the individual is with their families. This continuous shift may cause a lack of certainty about the individual’s identity and a belief that the individual does not fully belong anywhere. This lack of belonging can lead to poor social integration within the academic culture that can manifest in less opportunities and more mental health issues in the individual (Rubin, 2021; Rubin et al., 2019)."><a href=https://forrt.org/glossary/english/double_consciousness/ class=nav-link>Double consciousness</a></li><li class=nav-item data-toggle=tooltip data-placement=right title="Evaluation of research products by qualified experts where both the author(s) and reviewer(s) are anonymous to each other. “This approach conceals the identity of the authors and their affiliations from reviewers and would, in theory, remove biases of professional reputation, gender, race, and institutional affiliation, allowing the reviewer to avoid bias and to focus on the manuscript’s merit alone.” (Tvina et al., 2019, 1082). Like all types of peer-review, double-blind peer review is not without flaws. Anonymity can be difficult, if not impossible, to achieve for certain researchers working in a niche area."><a href=https://forrt.org/glossary/english/double_blind_peer_review/ class=nav-link>Double-blind peer review</a></li><li class=nav-item data-toggle=tooltip data-placement=right title="A label given to researchers who “range from senior doctoral students to postdoctoral workers who may have up to 10 years postdoctoral education; the latter group may therefore include early career or junior academics” (Eley et al., 2012, p. 3). What specifically (e.g. age, time since PhD inclusive or exclusive of career breaks and leave, title, funding awarded) constitutes an ECR can vary across funding bodies, academic organisations, and countries."><a href=https://forrt.org/glossary/english/early_career_researchers/ class=nav-link>Early career researchers (ECRs)</a></li><li class=nav-item data-toggle=tooltip data-placement=right title="The contribution a research item makes to the broader economy and society. It also captures the benefits of research to individuals, organisations, and/or nations."><a href=https://forrt.org/glossary/english/economic_and_societal_impact/ class=nav-link>Economic and societal impact</a></li><li class=nav-item data-toggle=tooltip data-placement=right title="Applied to Open Scholarship, in academic publishing, the period of time after an article has been published and before it can be made available as Open Access. If an author decides to self-archive their article (e.g., in an Open Access repository) they need to observe any embargo period a publisher might have in place. Embargo periods vary from instantaneous up to 48 months, with 6 and 12 months being common (Laakso & Björk, 2013). Embargo periods may also apply to pre-registrations, materials, and data, when authors decide to only make these available to the public after a certain period of time, for instance upon publication or even later when they have additional publication plans and want to avoid being scooped (Klein et al., 2018)."><a href=https://forrt.org/glossary/english/embargo_period/ class=nav-link>Embargo Period</a></li><li class=nav-item data-toggle=tooltip data-placement=right title="Systematic uncertainty due to limited data, measurement precision, model or process specification, or lack of knowledge. That is, uncertainty due to lack of knowledge that could, in theory, be reduced through conducting additional research to increase understanding. Such uncertainty is said to be personal, since knowledge differs across scientists, and temporary since it can change as new data become available."><a href=https://forrt.org/glossary/english/epistemic_uncertainty/ class=nav-link>Epistemic uncertainty</a></li><li class=nav-item data-toggle=tooltip data-placement=right title="Alongside ethics, logic, and metaphysics, epistemology is one of the four main branches of philosophy. Epistemology is largely concerned with nature, origin, and scope of knowledge, as well as the rationality of beliefs."><a href=https://forrt.org/glossary/english/epistemology/ class=nav-link>Epistemology</a></li><li class=nav-item data-toggle=tooltip data-placement=right title="Different individuals have different starting positions (cf. “opportunity gaps”) and needs. Whereas equal treatment focuses on treating all individuals *equally*, equitable treatment aims to level the playing field by actively increasing opportunities for under-represented minorities. Equitable treatment aims to attain equality through “fairness”: taking into account different needs for support for different individuals, instead of focusing merely on the needs of the majority."><a href=https://forrt.org/glossary/english/equity/ class=nav-link>Equity</a></li><li class=nav-item data-toggle=tooltip data-placement=right title="Equivalence tests statistically assess the null hypothesis that a given effect exceeds a minimum criterion to be considered meaningful. Thus, rejection of the null hypothesis provides evidence of a lack of (meaningful) effect. Based upon frequentist statistics, equivalence tests work by specifying equivalence bounds: a lower and upper value that reflect the smallest effect size of interest. Two one-sided *t*\-tests are then conducted against each of these equivalence bounds to assess whether effects that are deemed meaningful can be *rejected* (see Schuirmann, 1972; Lakens et al., 2018; 2020)."><a href=https://forrt.org/glossary/english/equivalence_testing/ class=nav-link>Equivalence Testing</a></li><li class=nav-item data-toggle=tooltip data-placement=right title="Broadly refers to examining research data and manuscripts for mistakes or inconsistencies in reporting. Commonly discussed approaches include: checking inconsistencies in descriptive statistics (e.g. summary statistics that are not possible given the sample size and measure characteristics; Brown & Heathers, 2017; Heathers et al. 2018), inconsistencies in reported statistics (e.g. *p*\-values that do not match the reported *F* statistics and accompanying degrees of freedom; Epskamp, & Nuijten, 2016; Nuijten et al. 2016), and image manipulation (Bik et al., 2016). Error detection is one motivation for data and analysis code to be openly available, so that peer review can confirm a manuscript’s findings, or if already published, the record can be corrected. Detected errors can result in corrections or retractions of published articles, though these actions are often delayed, long after erroneous findings have influenced and impacted further research."><a href=https://forrt.org/glossary/english/error_detection/ class=nav-link>Error detection</a></li><li class=nav-item data-toggle=tooltip data-placement=right title="This is a type of research method which aims to draw general conclusions to address a research question on a certain topic, phenomenon or effect by reviewing research outcomes and information from a range of different sources. Information which is subject to synthesis can be extracted from both qualitative and quantitative studies. The method used to synthesise the gathered information can be qualitative (narrative synthesis), quantitative (meta-analysis) or mixed (meta-synthesis, systematic mapping). Evidence synthesis has many applications and is often used in the context of healthcare, public policy as well as understanding and advancement of specific research fields."><a href=https://forrt.org/glossary/english/evidence_synthesis/ class=nav-link>Evidence Synthesis</a></li><li class=nav-item data-toggle=tooltip data-placement=right title="Exploratory Data Analysis (EDA) is a well-established statistical tradition that provides conceptual and computational tools for discovering patterns in data to foster hypothesis development and refinement. These tools and attitudes complement the use of hypothesis tests used in confirmatory data analysis (CDA). Even when well-specified theories are held, EDA helps one interpret the results of CDA and may reveal unexpected or misleading patterns in the data."><a href=https://forrt.org/glossary/english/exploratory_data_analysis/ class=nav-link>Exploratory data analysis</a></li><li class=nav-item data-toggle=tooltip data-placement=right title="Whether the findings of a scientific study can be generalized to other contexts outside the study context (different measures, settings, people, places, and times). Statistically, threats to external validity may reflect interactions whereby the effect of one factor (the independent variable) depends on another factor (a confounding variable). External validity may also be limited by the study design (e.g., an artificial laboratory setting or a non-representative sample)."><a href=https://forrt.org/glossary/english/external_validity/ class=nav-link>External Validity</a></li><li class=nav-item data-toggle=tooltip data-placement=right title="A subjective judgement of how suitable a measure appears to be on the surface, that is, how well a measure is operationalized. For example, judging whether questionnaire items should relate to a construct of interest at face value. Face validity is related to construct validity, but since it is subjective/informal, it is considered an easy but weak form of validity."><a href=https://forrt.org/glossary/english/face_validity/ class=nav-link>Face validity</a></li><li class=nav-item data-toggle=tooltip data-placement=right title="Describes making scholarly materials Findable, Accessible, Interoperable and Reusable (FAIR). ‘Findable’ and ‘Accessible’ are concerned with where materials are stored (e.g. in data repositories), while ‘Interoperable’ and ‘Reusable’ focus on the importance of data formats and how such formats might change in the future."><a href=https://forrt.org/glossary/english/fair_principles/ class=nav-link>FAIR principles</a></li><li class=nav-item data-toggle=tooltip data-placement=right title="With a particular focus on gender and sexuality, feminist psychology is inherently concerned with representation, diversity, inclusion, accessibility, and equality. Feminist psychology initially grew out out of a concern for representing the lived experiences of girls and women, but has since evolved into a more nuanced, intersectional and comprehensive concern for all aspects of equality (e.g., Eagly & Riger, 2014). Feminist psychologists have advocated for more rigorous consideration of equality, diversity, and inclusion within Open Science spaces (Pownall et al., 2021)."><a href=https://forrt.org/glossary/english/feminist_psychology/ class=nav-link>Feminist psychology</a></li><li class=nav-item data-toggle=tooltip data-placement=right title="An authorship system that assigns the order of authorship depending on the contributions of a given author while simultaneously valuing the first and last position of the authorship order most. According to this system, the two main authors are indicated as the first and last author \- the order of the authors between the first and last position is determined by contribution in a descending order."><a href=https://forrt.org/glossary/english/first_last_author_emphasis_norm/ class=nav-link>First-last-author-emphasis norm (FLAE)</a></li><li class=nav-item data-toggle=tooltip data-placement=right title="Framework of Open Reproducible Research and Teaching. It aims to provide a pedagogical infrastructure designed to recognize and support the teaching and mentoring of open and reproducible research in tandem with prototypical subject matters in higher education. FORRT strives to be an effective, evolving, and community-driven organization raising awareness of the pedagogical implications of open and reproducible science and its associated challenges (i.e., curricular reform, epistemological uncertainty, methods of education). FORRT also advocates for the opening of teaching and mentoring materials as a means to facilitate access, discovery, and learning to those who otherwise would be educationally disenfranchised."><a href=https://forrt.org/glossary/english/forrt/ class=nav-link>FORRT</a></li><li class=nav-item data-toggle=tooltip data-placement=right title="A collective action platform aiming to support the open science movement by obtaining pledges from researchers that they will implement certain research practices (e.g., pre-registration, pre-print). Initially pledges will be anonymous until a sufficient number of people pledge, upon which names of pledges will be released. The initiative is a grassroots movement instigated by early career researchers."><a href=https://forrt.org/glossary/english/free_our_knowledge_platform/ class=nav-link>Free Our Knowledge Platform</a></li><li class=nav-item data-toggle=tooltip data-placement=right title="Adopting questionable research practices (QRPs, e.g., salami slicing of an academic paper) that would align with academic incentive structures that benefit the academic (e.g. in prestige, hiring, or promotion) regardless of whether they support the process of scholarship. If systems rely on metrics to determine an outcome (e.g. academic credit) those metrics can be subject to intentional manipulation (Naudet et al., 2018\) or “gamed”. Where promotions, hiring, and tenure are based on flawed metrics they may disfavor openness, rigor, and transparent work (Naudet et al., 2018\) \- for example favoring “quantity over quality” \- and exacerbate existing inequalities."><a href=https://forrt.org/glossary/english/gaming/ class=nav-link>Gaming (the system)</a></li><li class=nav-item data-toggle=tooltip data-placement=right title="The typically-invisible decision tree traversed during operationalization and statistical analysis given that ‘there is a one-to-many mapping from scientific to statistical hypotheses' (Gelman and Loken, 2013, p. 6). In other words, even in absence of p-hacking or fishing expeditions and when the research hypothesis was posited ahead of time, there can be a plethora of statistical results that can appear to be supported by theory given data. “The problem is there can be a large number of potential comparisons when the details of data analysis are highly contingent on data, without the researcher having to perform any conscious procedure of fishing or examining multiple p-values” (Gelman and Loken, 2013, p. 1). The term aims to highlight the uncertainty ensuing from idiosyncratic analytical and statistical choices in mapping theory-to-test, and contrasting intentional (and unethical) questionable research practices (e.g. p-hacking and fishing expeditions) versus non-intentional research practices that can, potentially, have the same effect despite not having intent to corrupt their results. The garden of forking paths refers to the decisions during the scientific process that inflate the false-positive rate as a consequence of the potential paths which could have been taken (had other decisions been made)."><a href=https://forrt.org/glossary/english/garden_of_forking_paths/ class=nav-link>Garden of forking paths</a></li><li class=nav-item data-toggle=tooltip data-placement=right title="A legal framework of seven principles implemented across the European Union (EU) that aims to safeguard individuals’ information. The framework seeks to commission citizens with control over their personal data, whilst regulating the parties involved in storing and processing these data. This set of legislation dictates the free movement of individuals’ personal information both within and outside the EU and must be considered by researchers when designing and running studies."><a href=https://forrt.org/glossary/english/general_data_protection_regulation/ class=nav-link>General Data Protection Regulation (GDPR)</a></li><li class=nav-item data-toggle=tooltip data-placement=right title="Generalizability refers to how applicable a study’s results are to broader groups of people, settings, or situations they study and how the findings relate to this wider context (Frey, 2018; Kukull & Ganguli, 2012)."><a href=https://forrt.org/glossary/english/generalizability/ class=nav-link>Generalizability</a></li><li class=nav-item data-toggle=tooltip data-placement=right title="The inclusion in an article’s author list of individuals who do not meet the criteria for authorship. As authorship is associated with benefits including peer recognition and financial rewards, there are incentives for inclusion as an author on published research. Gifting authorship, or extending authorship credit to an individual who does not merit such recognition, can be intended to help the gift recipient, repay favors (including reciprocal gift authorship), maintain personal and professional relationships, and enhance chances of publication. Gift authorship is widely considered an unethical practice."><a href=https://forrt.org/glossary/english/gift/ class=nav-link>Gift (or Guest) Authorship</a></li><li class=nav-item data-toggle=tooltip data-placement=right title="A software package for tracking changes in a local set of files (local version control), initially developed by Linus Torvalds. In general, it is used by programmers to track and develop computer source code within a set directory, folder or a file system. Git can access remote repository hosting services (e.g. GitHub) for remote version control that enables collaborative software development by uploading contributions from a local system. This process found its way into the scientific process to enable open data, open code and reproducible analyses."><a href=https://forrt.org/glossary/english/git/ class=nav-link>Git</a></li><li class=nav-item data-toggle=tooltip data-placement=right title="A term coined by economist Charles Goodhart to refer to the observation that measuring something inherently changes user behaviour. In relation to examination performance, Strathern (1997) stated that “when a measure becomes a target, it ceases to be a good measure” (p. 308). Applied to open scholarship, and the structure of incentives in academia, Goodhart’s Law would predict that metrics of scientific evaluation will likely be abused and exploited, as evidenced by Muller (2019)"><a href=https://forrt.org/glossary/english/goodhart_s_law/ class=nav-link>Goodhart’s Law</a></li><li class=nav-item data-toggle=tooltip data-placement=right title="Free to use statistical software for performing power analyses. The user specifies the desired statistical test (e.g. t-test, regression, ANOVA), and three of the following: the number of groups/observations, effect size, significance level, or power, in order to calculate the unspecified aspect."><a href=https://forrt.org/glossary/english/gpower/ class=nav-link>GPower</a></li><li class=nav-item data-toggle=tooltip data-placement=right title="Hirsch’s index, abbreviated as H-index, intends to measure both productivity and research impact by combining the number of publications and the number of citations to these publications. Hirsch (2005) defined the index as “the number of papers with citation number ≥ *h*” (p. 16569). That is, the greatest number such that an author (or journal) has published at least that many papers that have been cited at least that many times. The index is perceived as a superior measure to measures that only assess, for instance, the number of citations and number of publications but this index has been criticised for the purpose of researcher assessment (e.g. Wendl, 2007)."><a href=https://forrt.org/glossary/english/h_index/ class=nav-link>H-index</a></li><li class=nav-item data-toggle=tooltip data-placement=right title="An organized event where experts, designers, or researchers collaborate for a relatively short amount of time to work intensively on a project or problem. The term is originally borrowed from computer programmer and software development events whose goal is to create a fully fledged product (resources, research, software, hardware) by the end of the event, which can last several hours to several days."><a href=https://forrt.org/glossary/english/hackathon/ class=nav-link>Hackathon</a></li><li class=nav-item data-toggle=tooltip data-placement=right title="A questionable research practice termed ‘Hypothesizing After the Results are Known’ (HARKing). “HARKing is defined as presenting a post hoc hypothesis (i.e., one based on or informed by one's results) in a research report as if it was, in fact, *a priori*” (Kerr, 1998, p. 196). For example, performing subgroup analyses, finding an effect in one subgroup, and writing the introduction with a ‘hypothesis’ that matches these results."><a href=https://forrt.org/glossary/english/harking/ class=nav-link>HARKing</a></li><li class=nav-item data-toggle=tooltip data-placement=right title="Contextual conditions that can, unbeknownst to researchers, make the results of a replication attempt deviate from those of the original study. Hidden moderators are sometimes invoked to explain (away) failed replications. Also called hidden assumptions."><a href=https://forrt.org/glossary/english/hidden_moderators/ class=nav-link>Hidden Moderators</a></li><li class=nav-item data-toggle=tooltip data-placement=right title="A hypothesis is an unproven statement relating the connection between variables (Glass & Hall, 2008\) and can be based on prior experiences, scientific knowledge, preliminary observations, theory and/or logic. In scientific testing, a hypothesis can be usually formulated with (e.g. a positive correlation) or without a direction (e.g. there will be a correlation). Popper (1959) posits that hypotheses must be falsifiable, that is, it must be conceivably possible to prove the hypothesis false. However, hypothesis testing based on falsification has been argued to be vague, as it is contingent on many other untested assumptions in the hypothesis (i.e., auxiliary hypotheses). Longino (1990, 1992\) argued that ontological heterogeneity should be valued more than ontological simplicity for the biological sciences, which considers we should investigate differences between and within biological organisms."><a href=https://forrt.org/glossary/english/hypothesis/ class=nav-link>Hypothesis</a></li><li class=nav-item data-toggle=tooltip data-placement=right title="A research metric created by Google Scholar that represents the number of publications a researcher has with at least 10 citations."><a href=https://forrt.org/glossary/english/i10_index/ class=nav-link>i10-index</a></li><li class=nav-item data-toggle=tooltip data-placement=right title="The idea that pre-existing opinions about the quality of research can depend on the ideological views of the author(s). One of the many biases in the peer review process, it expects that favourable opinions towards the research would be more likely if friends, collaborators, or scientists agree with an editor or reviewer’s political viewpoints (Tvina et al. 2019). This could potentially lead to a variety of conflicts of interest that undermine diverse perspectives, for example: speeding or delaying peer-review, or influencing the chances of an individual being invited to present their research, thus promoting their work."><a href=https://forrt.org/glossary/english/ideological_bias/ class=nav-link>Ideological bias</a></li><li class=nav-item data-toggle=tooltip data-placement=right title="The set of evaluation and reward mechanisms (explicit and implicit) for scientists and their work. Incentivised areas within the broader structure include hiring and promotion practices, track record for awarding funding, and prestige indicators such as publication in journals with high impact factors, invited presentations, editorships, and awards. It is commonly believed that these criteria are often misaligned with the telos of science, and therefore do not promote rigorous scientific output. Initiatives like DORA aim to reduce the field’s dependency on evaluation criteria such as journal impact factors in favor of assessments based on the intrinsic quality of research outputs."><a href=https://forrt.org/glossary/english/incentive_structure/ class=nav-link>Incentive structure</a></li><li class=nav-item data-toggle=tooltip data-placement=right title="Inclusion, or inclusivity, refers to a sense of welcome and respect within a given collaborative project or environment (such as academia) where *diversity* simply indicates a wide range of backgrounds, perspectives, and experiences, efforts to increase *inclusion* go further to promote engagement and equal valuation among diverse individuals, who might otherwise be marginalized. Increasing inclusivity often involves minimising the impact of, or even removing, systemic barriers to accessibility and engagement."><a href=https://forrt.org/glossary/english/inclusion/ class=nav-link>Inclusion</a></li><li class=nav-item data-toggle=tooltip data-placement=right title="“Reasoning by drawing a conclusion not guaranteed by the premises; for example, by inferring a general rule from a limited number of observations. Popper believed that there was no such logical process; we may guess general rules but such guesses are not rendered even more probable by any number of observations. By contrast, Bayesians inductively work out the increase in probability of a hypothesis that follows from the observations.” Dienes (p. 164, 2008\)"><a href=https://forrt.org/glossary/english/induction/ class=nav-link>Induction</a></li><li class=nav-item data-toggle=tooltip data-placement=right title="A statistical error in which a formal test is not conducted to assess the difference between a significant and non-significant correlation (or other measures, such as Odds Ratio). This fallacy occurs when a significant and non-significant correlation coefficient are assumed to represent a statistically significant difference but the comparison itself is not explicitly tested."><a href=https://forrt.org/glossary/english/interaction_fallacy/ class=nav-link>Interaction Fallacy</a></li><li class=nav-item data-toggle=tooltip data-placement=right title="An analysis at the core of intersectionality to analyse power, inequality and exclusion, as efforts to reform academic culture cannot be completed by investigating only one avenue in isolation (e.g. race, gender or ability) but by considering all the systems of exclusion. In contrast to intersectionality (which refers to the individual having multiple social identities), interlocking is usually used to describe the systems that combine to serve as oppressive measures toward the individual based on these identities."><a href=https://forrt.org/glossary/english/interlocking/ class=nav-link>Interlocking</a></li><li class=nav-item data-toggle=tooltip data-placement=right title="An indicator of the extent to which a study’s findings are representative of the true effect in the population of interest and not due to research confounds, such as methodological shortcomings. In other words, whether the observed evidence or covariation between the independent (predictor) and dependent (criterion) variables can be taken as a bona fide relationship and not a spurious effect owing to uncontrolled aspects of the study’s set up. Since it involves the quality of the study itself, internal validity is a priority for scientific research."><a href=https://forrt.org/glossary/english/internal_validity/ class=nav-link>Internal Validity</a></li><li class=nav-item data-toggle=tooltip data-placement=right title="A term which derives from Black feminist thought and broadly describes how social identities exist within ‘interlocking systems of oppression’ and structures of (in)equalities (Crenshaw, 1989\). Intersectionality offers a perspective on the way multiple forms of inequality operate together to compound or exacerbate each other. Multiple concurrent forms of identity can have a multiplicative effect and are not merely the sum of the component elements. One implication is that identity cannot be adequately understood through examining a single axis (e.g., race, gender, sexual orientation, class) at a time in isolation, but requires simultaneous consideration of overlapping forms of identity."><a href=https://forrt.org/glossary/english/intersectionality/ class=nav-link>Intersectionality</a></li><li class=nav-item data-toggle=tooltip data-placement=right title="An open-sourced, cross-platform citation and reference management tool that is available free of charge. It allows editing BibTeX files, importing data from online scientific databases, and managing and searching BibTeX files."><a href=https://forrt.org/glossary/english/jabref/ class=nav-link>JabRef</a></li><li class=nav-item data-toggle=tooltip data-placement=right title="Free and open source software for data analysis based on the R language. The software has a graphical user interface and provides the R code to the analyses. Jamovi supports computational reproducibility by saving the data, code, analyses, and results in a single file."><a href=https://forrt.org/glossary/english/jamovi/ class=nav-link>Jamovi</a></li><li class=nav-item data-toggle=tooltip data-placement=right title="Named after Sir Harold Jeffreys, JASP stands for Jeffrey’s Amazing Statistics Program. It is a free and open source software for data analysis. JASP relies on a user interface and offers both null hypothesis tests and their Bayesian counterparts. JASP supports computational reproducibility by saving the data, code, analyses, and results in a single file."><a href=https://forrt.org/glossary/english/jasp/ class=nav-link>JASP</a></li><li class=nav-item data-toggle=tooltip data-placement=right title="An online resource that collects and presents open access policies from publishers, from across the world, providing summaries of individual journal's copyright and open access archiving policies."><a href=https://forrt.org/glossary/english/sherpa_romeo/ class=nav-link>JISC Open Policy Finder (formerly Sherpa services)</a></li><li class=nav-item data-toggle=tooltip data-placement=right title="The mean number of citations to research articles in that journal over the preceding two years. It is a proprietary and opaque calculation marketed by Clarivate™. Journal Impact Factors are not associated with the content quality or the peer review process."><a href=https://forrt.org/glossary/english/journal_impact_factor_/ class=nav-link>Journal Impact Factor™</a></li><li class=nav-item data-toggle=tooltip data-placement=right title="JavaScript Object Notation (JSON) is a data format for structured data that can be used to represent attribute-value pairs. Values thereby can contain further JSON notation (i.e., nested information). JSON files can be formally encoded as strings of text and thus are human-readable. Beyond storing information this feature makes them suitable for annotating other content. For example, JSON files are used in Brain Imaging Data Structure (BIDS) for describing the metadata dataset by following a standardized format (dataset\_description.json)."><a href=https://forrt.org/glossary/english/json_file/ class=nav-link>JSON file</a></li><li class=nav-item data-toggle=tooltip data-placement=right title="The process by which the mind decodes or extracts, stores, and relates new information to existing information in long term memory. Given the complex structure and nature of knowledge, this process is studied in the philosophical field of epistemology, as well as the psychological field of learning and memory."><a href=https://forrt.org/glossary/english/knowledge_acquisition/ class=nav-link>Knowledge acquisition</a></li><li class=nav-item data-toggle=tooltip data-placement=right title="A statistical model of the data used in frequentist and Bayesian analyses, defined up to a constant of proportionality. A likelihood function represents the likeliness of different parameters for your distribution given the data. Given that probability distributions have unknown population parameters, the likelihood function indicates how well the sample data summarise these parameters. As such, the likelihood function gives an idea of the goodness of fit of a model to the sample data for a given set of values of the unknown population parameters."><a href=https://forrt.org/glossary/english/likelihood_function/ class=nav-link>Likelihood function</a></li><li class=nav-item data-toggle=tooltip data-placement=right title="The notion that all information relevant to inference contained in data is provided by the likelihood. The principle suggests that the likelihood function can be used to compare the plausibility of various parameter values. While Bayesians and likelihood theorists subscribe to the likelihood principle, Neyman-Pearson theorists do not, as significance tests violate the likelihood principle because they take into account information not in the likelihood."><a href=https://forrt.org/glossary/english/likelihood_principle/ class=nav-link>Likelihood Principle</a></li><li class=nav-item data-toggle=tooltip data-placement=right title="Researchers often review research records on a given topic to better understand effects and phenomena of interest before embarking on a new research project, to understand how theory links to evidence or to investigate common themes and directions of existing study results and claims. Different types of reviews can be conducted depending on the research question and literature scope. To determine the scope and key concepts in a given field, researchers may want to conduct a scoping literature review. Systematic reviews aim to access and review all available records for the most accurate and unbiased representation of existing literature. Non-systematic or focused literature reviews synthesise information from a selection of studies relevant to the research question although they are uncommon due to susceptibility to biases (e.g. researcher bias; Siddaway et al., 2019)."><a href=https://forrt.org/glossary/english/literature_review/ class=nav-link>Literature Review</a></li><li class=nav-item data-toggle=tooltip data-placement=right title="Portmanteau for ‘male panel’, usually to refer to speaker panels at conferences entirely composed of (usually caucasian) males. Typically discussed in the context of gender disparities in academia (e.g., women being less likely to be recognised as experts by their peers and, subsequently, having fewer opportunities for career development)."><a href=https://forrt.org/glossary/english/manel/ class=nav-link>Manel</a></li><li class=nav-item data-toggle=tooltip data-placement=right title="Large-scale collaborative projects involving tens or hundreds of authors from different institutions. This kind of approach has become increasingly common in psychology and other sciences in recent years as opposed to research carried out by small teams of authors, following earlier trends which have been observed e.g. for high-energy physics or biomedical research in the 1990s. These large international scientific consortia work on a research project to bring together a broader range of expertise and work collaboratively to produce manuscripts."><a href=https://forrt.org/glossary/english/many_authors/ class=nav-link>Many authors</a></li><li class=nav-item data-toggle=tooltip data-placement=right title="A crowdsourcing initiative led by the Open Science Collaboration (2015) whereby several hundred separate research groups from various universities run replication studies of published effects. This initiative is also known as “Many Labs I” and was subsequently followed by a “Many Labs II” project that assessed variation in replication results across samples and settings. Similar projects include ManyBabies, EEGManyLabs, and the Psychological Science Accelerator."><a href=https://forrt.org/glossary/english/many_labs/ class=nav-link>Many Labs</a></li><li class=nav-item data-toggle=tooltip data-placement=right title="Exclusively online courses which are accessible to any learner at any time, are typically free to access (while not necessarily openly licensed), and provide video-based instructions and downloadable data sets and exercises. The “massive” aspect describes the high volume of students that can access the course at any one time due to their flexibility, low or no cost, and online nature of the materials."><a href=https://forrt.org/glossary/english/massive_open_online_courses/ class=nav-link>Massive Open Online Courses (MOOCs)</a></li><li class=nav-item data-toggle=tooltip data-placement=right title="Unlike the traditional collaborative article, a MOOP follows an open participatory and dynamic model that is not restricted by a predetermined list of contributors."><a href=https://forrt.org/glossary/english/massively_open_online_papers/ class=nav-link>Massively Open Online Papers (MOOPs)</a></li><li class=nav-item data-toggle=tooltip data-placement=right title="Named for the ‘rich get richer; poor get poorer’ paraphrase of the Gospel of Matthew. Eminent scientists and early-career researchers with a prestigious fellowship are disproportionately attributed greater levels of credit and funding for their contributions to science while relatively unknown or early-career researchers without a prestigious fellowship tend to get disproportionately little credit for comparable contributions. The impact is a substantial cumulative advantage that results from modest initial comparative advantages (and vice versa)."><a href=https://forrt.org/glossary/english/matthew_effect/ class=nav-link>Matthew effect (in science)</a></li><li class=nav-item data-toggle=tooltip data-placement=right title="A meta-analysis is a statistical synthesis of results from a series of studies examining the same phenomenon. A variety of meta-analytic approaches exist, including random or fixed effects models or meta-regressions, which allow for an examination of moderator effects. By aggregating data from multiple studies, a meta-analysis could provide a more precise estimate for a phenomenon (e.g. type of treatment) than individual studies. Results are usually visualized in a forest plot. Meta-analyses can also help examine heterogeneity across study results. Meta-analyses are often carried out in conjunction with systematic reviews and similarly require a systematic search and screening of studies. Publication bias is also commonly examined in the context of a meta-analysis and is typically visually presented via a funnel plot."><a href=https://forrt.org/glossary/english/meta_analysis/ class=nav-link>Meta-analysis</a></li><li class=nav-item data-toggle=tooltip data-placement=right title="The scientific study of science itself with the aim to describe, explain, evaluate and/or improve scientific practices. Meta-science typically investigates scientific methods, analyses, the reporting and evaluation of data, the reproducibility and replicability of research results, and research incentives."><a href=https://forrt.org/glossary/english/meta_science_or_meta_research/ class=nav-link>Meta-science or Meta-research</a></li><li class=nav-item data-toggle=tooltip data-placement=right title="Structured data that describes and synthesises other data. Metadata can help find, organize, and understand data. Examples of metadata include creator, title, contributors, keywords, tags, as well as any kind of information necessary to verify and understand the results and conclusions of a study such as codebook on data labels, descriptions, the sample and data collection process."><a href=https://forrt.org/glossary/english/metadata/ class=nav-link>Metadata</a></li><li class=nav-item data-toggle=tooltip data-placement=right title="The process by which a verbal description is formalised to remove ambiguity, while also constraining the dimensions a theory can span. The model is thus data derived. “Many scientific models are representational models: they represent a selected part or aspect of the world, which is the model’s target system” (Frigg & Hartman, 2020)."><a href=https://forrt.org/glossary/english/model/ class=nav-link>Model (philosophy)</a></li><li class=nav-item data-toggle=tooltip data-placement=right title="In typical empirical studies, a single researcher or research team conducts the analysis, which creates uncertainty about the extent to which the choice of analysis influences the results. In multi-analyst studies, two or more researchers independently analyse the same research question or hypothesis on the same dataset. According to Aczel and colleagues (2021), a multi-analyst approach may be beneficial in increasing our confidence in a particular finding; uncovering the impact of analytical preferences across research teams; and highlighting the variability in such analytical approaches."><a href=https://forrt.org/glossary/english/multi_analyst_studies/ class=nav-link>Multi-Analyst Studies</a></li><li class=nav-item data-toggle=tooltip data-placement=right title="Potential inflation of Type I error rates (incorrectly rejecting the null hypothesis) because of multiple statistical testing, for example, multiple outcomes, multiple follow-up time points, or multiple subgroup analyses. To overcome issues with multiplicity, researchers will often apply controlling procedures (e.g., Bonferroni, Holm-Bonferroni; Tukey) that correct the alpha value to control for inflated Type I errors. However, by controlling for Type I errors, one can increase the possibility of Type II errors (i.e., incorrectly accepting the null hypothesis)."><a href=https://forrt.org/glossary/english/multiplicity/ class=nav-link>Multiplicity</a></li><li class=nav-item data-toggle=tooltip data-placement=right title="Multiverse analyses are based on all potentially equally justifiable data processing and statistical analysis pipelines that can be employed to test a single hypothesis. In a data multiverse analysis, a single set of raw data is processed into a multiverse of data sets by applying all possible combinations of justifiable preprocessing choices. Model multiverse analyses apply equally justifiable statistical models to the same data to answer the same hypothesis. The statistical analysis is then conducted on all data sets in the multiverse and all results are reported which enhances promoting transparency and illustrates the robustness of results against different data processing (data multiverse) or statistical (model multiverse) pipelines). Multiverse analysis differs from Specification curve analysis with regards to the graphical displays (a histogram and tile plota rather than a specification curve plot)."><a href=https://forrt.org/glossary/english/multiverse_analysis/ class=nav-link>Multiverse analysis</a></li><li class=nav-item data-toggle=tooltip data-placement=right title="An attribution issue arising from two related problems: authors may use multiple names or monikers to publish work, and multiple authors in a single field may share full names. This makes accurate identification of authors on names and specialisms alone a difficult task. This can be addressed through the creation and use of unique digital identifiers that act akin to digital fingerprints such as ORCID."><a href=https://forrt.org/glossary/english/name_ambiguity_problem/ class=nav-link>Name Ambiguity Problem</a></li><li class=nav-item data-toggle=tooltip data-placement=right title="A free, open-source anonymisation software that identifies and modifies named entities (e.g. persons, locations, times, dates). Its key feature is that it preserves critical context needed for secondary analyses. The aim is to assist researchers in sharing their raw text data, while adhering to research ethics."><a href=https://forrt.org/glossary/english/named_entity_based_text_anonymization_for_open_science/ class=nav-link>Named entity-based Text Anonymization for Open Science (NETANOS)</a></li><li class=nav-item data-toggle=tooltip data-placement=right title="A comprehensive set of tools to facilitate the development, preregistration and dissemination of systematic literature reviews for non-intervention research. Part A represents detailed guidelines for creating and preregistering a systematic review protocol in the context of non-intervention research whilst preparing for transparency. Part B represents guidelines for writing up the completed systematic review, with a focus on enhancing reproducibility."><a href=https://forrt.org/glossary/english/non_intervention_reproducible_and_open_systematic_reviews/ class=nav-link>Non-Intervention, Reproducible, and Open Systematic Reviews (NIRO-SR)</a></li><li class=nav-item data-toggle=tooltip data-placement=right title="A frequentist approach to inference used to test the probability of an observed effect against the null hypothesis of no effect/relationship (Pernet, 2015). Such a conclusion is arrived at through use of an index called the *p*\-value. Specifically, researchers will conclude an effect is present when an a priori alpha threshold, set by the researchers, is satisfied; this determines the acceptable level of uncertainty and is closely related to Type I error."><a href=https://forrt.org/glossary/english/null_hypothesis_significance_testing/ class=nav-link>Null Hypothesis Significance Testing (NHST)</a></li><li class=nav-item data-toggle=tooltip data-placement=right title="The idea that scientific claims, methods, results and scientists themselves should remain value-free and unbiased, and thus not be affected by cultural, political, racial or religious bias as well as any personal interests (Merton, 1942)."><a href=https://forrt.org/glossary/english/objectivity/ class=nav-link>Objectivity</a></li><li class=nav-item data-toggle=tooltip data-placement=right title="A set of axioms in a subject area that help classify and explain the nature of the entities under study and the relationships between them."><a href=https://forrt.org/glossary/english/ontology/ class=nav-link>Ontology (Artificial Intelligence)</a></li><li class=nav-item data-toggle=tooltip data-placement=right title="“Free availability of scholarship on the public internet, permitting any users to read, download, copy, distribute, print, search, or link to the full texts of these research articles, crawl them for indexing, pass them as data to software, or use them for any other lawful purpose, without financial, legal, or technical barriers other than those inseparable from gaining access to the internet itself” (Boai, 2002). Different methods of achieving open access (OA) are often referred to by color, including Green Open Access (when the work is openly accessible from a public repository), Gold Open Access (when the work is immediately openly accessible upon publication via a journal website), and Platinum (or Diamond) Open Access (a subset of Gold OA in which all works in the journal are immediately accessible after publication from the journal website without the authors needing to pay an article processing fee \[APC\])."><a href=https://forrt.org/glossary/english/open_access/ class=nav-link>Open access</a></li><li class=nav-item data-toggle=tooltip data-placement=right title="Making computer code (e.g., programming, analysis code, stimuli generation) freely and publicly available in order to make research methodology and analysis transparent and allow for reproducibility and collaboration. Code can be made available via open code websites, such as GitHub, the Open Science Framework, and Codeshare (to name a few), enabling others to evaluate and correct errors and re-use and modify the code for subsequent research."><a href=https://forrt.org/glossary/english/open_code/ class=nav-link>Open Code</a></li><li class=nav-item data-toggle=tooltip data-placement=right title="Open data refers to data that is freely available and readily accessible for use by others without restriction, “Open data and content can be freely used, modified, and shared by anyone for any purpose” ([https://opendefinition.org/](https://opendefinition.org/)). Open data are subject to the requirement to attribute and share alike, thus it is important to consider appropriate Open Licenses. Sensitive or time-sensitive datasets can be embargoed or shared with more selective access options to ensure data integrity is upheld."><a href=https://forrt.org/glossary/english/open_data/ class=nav-link>Open Data</a></li><li class=nav-item data-toggle=tooltip data-placement=right title="OER Commons (with OER standing for open educational resources) is a freely accessible online library allowing teachers to create, share and remix educational resources. The goal of the OER movement is to stimulate “collaborative teaching and learning” ([https://www.oercommons.org/about](https://www.oercommons.org/about)) and provide high-quality educational resources that are accessible for everyone."><a href=https://forrt.org/glossary/english/open_educational_resources/ class=nav-link>Open Educational Resources (OER) Commons</a></li><li class=nav-item data-toggle=tooltip data-placement=right title="Open licenses are provided with open data and open software (e.g., analysis code) to define how others can (re)use the licensed material. In setting out the permissions and restrictions, open licenses often permit the unrestricted access, reuse and retribution of an author’s original work. Datasets are typically licensed under a type of open licence known as a Creative Commons license (e.g., MIT, Apache, and GPL). These can differ in relatively subtle ways with GPL licenses (and their variants) being Copyleft licenses that require that any derivative work is licensed under the same terms as the original."><a href=https://forrt.org/glossary/english/open_licenses/ class=nav-link>Open Licenses</a></li><li class=nav-item data-toggle=tooltip data-placement=right title="Author’s public sharing of materials that were used in a study, “such as survey items, stimulus materials, and experiment programs” (Kidwell et al., 2016, p. 3). Digitally-shareable materials are posted on open access repositories, which makes them publicly available and accessible. Depending on licensing, the material can be reused by other authors for their own studies. Components that are not digitally-shareable (e.g. biological materials, equipment) must be described in sufficient detail to allow reproducibility."><a href=https://forrt.org/glossary/english/open_material/ class=nav-link>Open Material</a></li><li class=nav-item data-toggle=tooltip data-placement=right title="A scholarly review mechanism providing disclosure of any combination of author and referee identities, as well as peer-review reports and editorial decision letters, to one another or publicly at any point during or after the peer review or publication process. It may also refer to the removal of restrictions on who can participate in peer review and the platforms for doing so. Note that ‘open peer review’ has been used interchangeably to refer to any, or all, of the above practices."><a href=https://forrt.org/glossary/english/open_peer_review/ class=nav-link>Open Peer Review</a></li><li class=nav-item data-toggle=tooltip data-placement=right title="‘Open scholarship’ is often used synonymously with ‘open science’, but extends to all disciplines, drawing in those which might not traditionally identify as science-based. It reflects the idea that knowledge of all kinds should be openly shared, transparent, rigorous, reproducible, replicable, accumulative, and inclusive (allowing for all knowledge systems). Open scholarship includes all scholarly activities that are not solely limited to research such as teaching and pedagogy."><a href=https://forrt.org/glossary/english/open_scholarship/ class=nav-link>Open Scholarship</a></li><li class=nav-item data-toggle=tooltip data-placement=right title="The Open Scholarship Knowledge Base (OSKB) is a collaborative initiative to share knowledge on the what, why and how of open scholarship to make this knowledge easy to find and apply. Information is curated and created by the community. The OSKB is a community under the Center for Open Science (COS)."><a href=https://forrt.org/glossary/english/open_scholarship_knowledge_base/ class=nav-link>Open Scholarship Knowledge Base</a></li><li class=nav-item data-toggle=tooltip data-placement=right title="An umbrella term reflecting the idea that scientific knowledge of all kinds, where appropriate, should be openly accessible, transparent, rigorous, reproducible, replicable, accumulative, and inclusive, all which are considered fundamental features of the scientific endeavour. Open science consists of principles and behaviors that promote transparent, credible, reproducible, and accessible science. Open science has six major aspects: open data, open methodology, open source, open access, open peer review, and open educational resources."><a href=https://forrt.org/glossary/english/open_science/ class=nav-link>Open Science</a></li><li class=nav-item data-toggle=tooltip data-placement=right title="A free and open source platform for researchers to organize and share their research project and to encourage collaboration. Often used as an open repository for research code, data and materials, preprints and preregistrations, while managing a more efficient workflow. Created and maintained by the Center for Open Science."><a href=https://forrt.org/glossary/english/open_science_framework/ class=nav-link>Open Science Framework</a></li><li class=nav-item data-toggle=tooltip data-placement=right title="A type of computer software in which source code is released under a license that permits others to use, change, and distribute the software to anyone and for any purpose. Open source is more than openly accessible: the distribution terms of open-source software must comply with 10 specific criteria (see: [https://opensource.org/osd](https://opensource.org/osd))."><a href=https://forrt.org/glossary/english/open_source_software/ class=nav-link>Open Source software</a></li><li class=nav-item data-toggle=tooltip data-placement=right title="Open washing, termed after “greenwashing”, refers to the act of claiming openness to secure perceptions of rigor or prestige associated with open practices. It has been used to characterise the marketing strategy of software companies that have the appearance of open-source and open-licensing, while engaging in proprietary practices. Open washing is a growing concern for those adopting open science practices as their actions are undermined by misleading uses of the practices, and actions designed to facilitate progressive developments are reduced to ‘ticking the box’ without clear quality control."><a href=https://forrt.org/glossary/english/open_washing/ class=nav-link>Open washing</a></li><li class=nav-item data-toggle=tooltip data-placement=right title="A free platform where researchers can freely and openly share, browse, download and re-use brain imaging data (e.g., MRI, MEG, EEG, iEEG, ECoG, ASL, and PET data)."><a href=https://forrt.org/glossary/english/openneuro/ class=nav-link>OpenNeuro</a></li><li class=nav-item data-toggle=tooltip data-placement=right title="The practice of (repeatedly) analyzing data during the data collection process and deciding to stop data collection if a statistical criterion (e.g. *p*\-value, or bayes factor) reaches a specified threshold. If appropriate methodological precautions are taken to control the type 1 error rate, this can be an efficient analysis procedure (e.g. Lakens, 2014). However, without transparent reporting or appropriate error control the type 1 error can increase greatly and optional stopping could be considered a Questionable Research Practice (QRP) or a form of p-hacking."><a href=https://forrt.org/glossary/english/optional_stopping/ class=nav-link>Optional Stopping</a></li><li class=nav-item data-toggle=tooltip data-placement=right title="A organisation that provides a registry of persistent unique identifiers (ORCID iDs) for researchers and scholars, allowing these users to link their digital research documents and other contributions to their ORCID record. This avoids the name ambiguity problem in scholarly communication. ORCID iDs provide unique, persistent identifiers connecting researchers and their scholarly work. It is free to register for an ORCID iD at https://orcid.org/register."><a href=https://forrt.org/glossary/english/orcid/ class=nav-link>ORCID (Open Researcher and Contributor ID)</a></li><li class=nav-item data-toggle=tooltip data-placement=right title="Open access electronic journals that collect and curate articles available from other sources (typically preprint servers, such as arXiv). Article curation may include (post-publication) peer review or editorial selection. Overlay journals do not publish novel material; rather, they organize and collate articles available in existing repositories."><a href=https://forrt.org/glossary/english/overlay_journal/ class=nav-link>Overlay Journal</a></li><li class=nav-item data-toggle=tooltip data-placement=right title="P-curve is a tool for identifying potential publication bias and makes use of the distribution of significant *p*\-values in a series of independent findings. The deviation from the expected right-skewed distribution can be used to assess the existence and degree of publication bias: if the curve is right-skewed, there are more low, highly significant *p*\-values, reflecting an underlying true effect. If the curve is left-skewed, there are many barely significant results just under the 0.05-threshold. This suggests that the studies lack evidential value and may be underpinned by questionable research practices (QRPs; e.g., p-hacking). In the case of no true effect present (true null hypothesis) and unbiased *p*\-value reporting, the *p*\-curve should be a flat, horizontal line, representing the typical distribution of *p*\-values."><a href=https://forrt.org/glossary/english/p_curve/ class=nav-link>P-curve</a></li><li class=nav-item data-toggle=tooltip data-placement=right title="Exploiting techniques that may artificially increase the likelihood of obtaining a statistically significant result by meeting the standard statistical significance criterion (typically α \= .05). For example, performing multiple analyses and reporting only those at *p* \< .05, selectively removing data until *p* \< .05, selecting variables for use in analyses based on whether those parameters are statistically significant."><a href=https://forrt.org/glossary/english/p_hacking/ class=nav-link>p*-hacking</a></li><li class=nav-item data-toggle=tooltip data-placement=right title="A statistic used to evaluate the outcome of a hypothesis test in Null Hypothesis Significance Testing (NHST). It refers to the probability of observing an effect, or more extreme effect, assuming the null hypothesis is true (Lakens, 2021b). The American Statistical Association’s statement on p-values (Wasserstein & Lazar, 2016\) notes that p-values are not an indicator of the truth of the null hypothesis and instead defines p-values in this way: “Informally, a p-value is the probability under a specified statistical model that a statistical summary of the data (e.g., the sample mean difference between two compared groups) would be equal to or more extreme than its observed value” (p. 131)."><a href=https://forrt.org/glossary/english/p_value/ class=nav-link>p*-value</a></li><li class=nav-item data-toggle=tooltip data-placement=right title="An organization that is engaged in scientific misconduct wherein multiple papers are produced by falsifying or fabricating data, e.g. by editing figures or numerical data or plagiarizing written text. Papermills are “alleged to offer products ranging from research data through to ghostwritten fraudulent or fabricated manuscripts and submission services” (Byrne & Christopher, 2020, p. 583). A papermill relates to the fast production and dissemination of multiple allegedly new papers. These are often not detected in the scientific publishing process and therefore either never found or retracted if discovered (e.g. through plagiarism software)."><a href=https://forrt.org/glossary/english/papermill/ class=nav-link>Papermill</a></li><li class=nav-item data-toggle=tooltip data-placement=right title="Data that are captured about the characteristics and context of primary data collected from an individual \- distinct from metadata. Paradata can be used to investigate a respondent’s interaction with a survey or an experiment on a micro-level. They can be most easily collected during computer mediated surveys but are not limited to them. Examples include response times to survey questions, repeated patterns of responses such as choosing the same answer for all questions, contextual characteristics of the participant such as injuries that prevent good performance on tasks, the number of premature responses to stimuli in an experiment. Paradata have been used for the investigation and adjustment of measurement and sampling errors."><a href=https://forrt.org/glossary/english/paradata/ class=nav-link>Paradata</a></li><li class=nav-item data-toggle=tooltip data-placement=right title='PARKing (preregistering after results are known) is defined as the practice where researchers complete an experiment (possibly with infinite re-experimentation) before preregistering. This practice invalidates the purpose of preregistration, and is one of the QRPs (or, even scientific misconduct) that try to gain only "credibility that it has been preregistered."'><a href=https://forrt.org/glossary/english/parking/ class=nav-link>PARKing</a></li><li class=nav-item data-toggle=tooltip data-placement=right title="Participatory research refers to incorporating the views of people from relevant communities in the entire research process to achieve shared goals between researchers and the communities. This approach takes a collaborative stance that seeks to reduce the power imbalance between the researcher and those researched through a “systematic cocreation of new knowledge” (Andersson, 2018)."><a href=https://forrt.org/glossary/english/participatory_research/ class=nav-link>Participatory Research</a></li><li class=nav-item data-toggle=tooltip data-placement=right title="Active research collaboration with the population of interest, as opposed to conducting research “about” them. Researchers can incorporate the lived experience and expertise of patients and the public at all stages of the research process. For example, patients can help to develop a set of research questions, review the suitability of a study design, approve plain English summaries for grant/ethics applications and dissemination, collect and analyse data, and assist with writing up a project for publication. This is becoming highly recommended and even required by funders (Boivin et al., 2018)."><a href=https://forrt.org/glossary/english/patient_and_public_involvement/ class=nav-link>Patient and Public Involvement (PPI)</a></li><li class=nav-item data-toggle=tooltip data-placement=right title="A technological barrier that permits access to information only to individuals who have paid \- either personally, or via an organisation \- a designated fee or subscription."><a href=https://forrt.org/glossary/english/paywall/ class=nav-link>Paywall</a></li><li class=nav-item data-toggle=tooltip data-placement=right title="PCI is a non-profit organisation that creates communities of researchers who review and recommend unpublished preprints based upon high-quality peer review from at least two researchers in their field. These preprints are then assigned a DOI, similarly to a journal article. PCI was developed to establish a free, transparent and public scientific publication system based on the review and recommendation of preprints."><a href=https://forrt.org/glossary/english/pci/ class=nav-link>PCI (Peer Community In)</a></li><li class=nav-item data-toggle=tooltip data-placement=right title="An initiative launched in 2021 dedicated to receiving, reviewing, and recommending Registered Reports (RRs) across the full spectrum of Science, technology, engineering, and mathematics (STEM), medicine, social sciences and humanities. Peer Community In (PCI) RRs are overseen by a ‘Recommender’ (equivalent to an Action Editor) and reviewed by at least two experts in the relevant field. It provides free and transparent pre- (Stage 1\) and post-study (Stage 2\) reviews across research fields. A network of PCI RR-friendly journals endorse the PCI RR review criteria and commit to accepting, without further peer review, RRs that receive a positive final recommendation from PCI RR."><a href=https://forrt.org/glossary/english/pci_registered_reports/ class=nav-link>PCI Registered Reports</a></li><li class=nav-item data-toggle=tooltip data-placement=right title="Plan S is an initiative, launched in September 2018 by cOAlition S, a consortium of research funding organisations, which aims to accelerate the transition to full and immediate Open Access. Participating funders require recipients of research grants to publish their research in compliant Open Access journals or platforms, or make their work openly and immediately available in an Open Access repository, from 2021 onwards. cOAlition S funders have commited to not financially support ‘hybrid’ Open Access publication fees in subscription venues. However, authors can comply with plan S through publishing Open Access in a subscription journal under a “transformative arrangement” as further described in the implementation guidance. The “S” in Plan S stands for shock."><a href=https://forrt.org/glossary/english/plan_s/ class=nav-link>Plan S</a></li><li class=nav-item data-toggle=tooltip data-placement=right title="The contextualization of both the research environment and the researcher, to define the boundaries within the research was produced (Jaraf, 2018). Positionality is typically centred and celebrated in qualitative research, but there have been recent calls for it to also be used in quantitative research as well. Positionality statements, whereby a researcher outlines their background and ‘position’ within and towards the research, have been suggested as one method of recognising and centring researcher bias."><a href=https://forrt.org/glossary/english/positionality/ class=nav-link>Positionality</a></li><li class=nav-item data-toggle=tooltip data-placement=right title="A reflexive tool for practicing explicit positionality in critical qualitative research. The map is to be used “as a flexible starting point to guide researchers to reflect and be reflexive about their social location. The map involves three tiers: the identification of social identities (Tier 1), how these positions impact our life (Tier 2), and details that may be tied to the particularities of our social identity (Tier 3).” (Jacobson and Mustafa 2019, p. 1). The aim of the map is “for researchers to be able to better identify and understand their social locations and how they may pose challenges and aspects of ease within the qualitative research process.”"><a href=https://forrt.org/glossary/english/positionality_map/ class=nav-link>Positionality Map</a></li><li class=nav-item data-toggle=tooltip data-placement=right title="Post hoc is borrowed from Latin, meaning “after this”. In statistics, post hoc (or post hoc analysis) refers to the testing of hypotheses not specified prior to data analysis. In frequentist statistics, the procedure differs based on whether the analysis was planned or post-hoc, for example by applying more stringent error control. In contrast, Bayesian and likelihood approaches do not differ as a function of when the hypothesis was specified."><a href=https://forrt.org/glossary/english/post_hoc/ class=nav-link>Post Hoc</a></li><li class=nav-item data-toggle=tooltip data-placement=right title="Peer review that takes place after research has been published. It is typically posted on a dedicated platform (e.g., PubPeer). It is distinct from the traditional commentary which is published in the same journal and which is itself usually peer reviewed."><a href=https://forrt.org/glossary/english/post_publication_peer_review/ class=nav-link>Post Publication Peer Review</a></li><li class=nav-item data-toggle=tooltip data-placement=right title="A way to summarize one’s updated knowledge in Bayesian inference, balancing prior knowledge with observed data. In statistical terms, posterior distributions are proportional to the product of the likelihood function and the prior. A posterior probability distribution captures (un)certainty about a given parameter value."><a href=https://forrt.org/glossary/english/posterior_distribution/ class=nav-link>Posterior distribution</a></li><li class=nav-item data-toggle=tooltip data-placement=right title="Predatory (sometimes “vanity”) publishing describes a range of business practices in which publishers seek to profit, primarily by collecting article processing charges (APCs), from publishing scientific works without necessarily providing legitimate quality checks (e.g., peer review) or editorial services. In its most extreme form, predatory publishers will publish any work, so long as charges are paid. Other less extreme strategies, such as sending out high numbers of unsolicited requests for editing or publishing in fee-driven special issues, have also been accused as predatory (Crosetto, 2021)."><a href=https://forrt.org/glossary/english/predatory_publishing/ class=nav-link>Predatory Publishing</a></li><li class=nav-item data-toggle=tooltip data-placement=right title="The PREPARE guidelines and checklist (Planning Research and Experimental Procedures on Animals: Recommendations for Excellence) aim to help the planning of animal research, and support adherence to the 3Rs (Replacement, Reduction or Refinement) and facilitate the reproducibility of animal research."><a href=https://forrt.org/glossary/english/prepare_guidelines/ class=nav-link>PREPARE Guidelines</a></li><li class=nav-item data-toggle=tooltip data-placement=right title="A publicly available version of any type of scientific manuscript/research output preceding formal publication, considered a form of Green Open Access. Preprints are usually hosted on a repository (e.g. arXiv) that facilitates dissemination by sharing research results more quickly than through traditional publication. Preprint repositories typically provide persistent identifiers (e.g. DOIs) to preprints. Preprints can be published at any point during the research cycle, but are most commonly published upon submission (i.e., before peer-review). Accepted and peer-reviewed versions of articles are also often uploaded to preprint servers, and are called postprints."><a href=https://forrt.org/glossary/english/preprint/ class=nav-link>Preprint</a></li><li class=nav-item data-toggle=tooltip data-placement=right title="The practice of publishing the plan for a study, including research questions/hypotheses, research design, data analysis before the data has been collected or examined. It is also possible to preregister secondary data analyses (Merten & Krypotos, 2019). A preregistration document is time-stamped and typically registered with an independent party (e.g., a repository) so that it can be publicly shared with others (possibly after an embargo period). Preregistration provides a transparent documentation of what was planned at a certain time point, and allows third parties to assess what changes may have occurred afterwards. The more detailed a preregistration is, the better third parties can assess these changes and with that the validity of the performed analyses. Preregistration aims to clearly distinguish confirmatory from exploratory research."><a href=https://forrt.org/glossary/english/preregistration/ class=nav-link>Preregistration</a></li><li class=nav-item data-toggle=tooltip data-placement=right title="In a “collective action in support of open and reproducible research practices'', the preregistration pledge is a campaign from the Project Free Our Knowledge that asks a researcher to commit to preregistering at least one study in the next two years ([https://freeourknowledge.org/about/](https://freeourknowledge.org/about/)). The project is a grassroots movement initiated by early career researchers (ECRs)."><a href=https://forrt.org/glossary/english/preregistration_pledge/ class=nav-link>Preregistration Pledge</a></li><li class=nav-item data-toggle=tooltip data-placement=right title="Beliefs held by researchers about the parameters in a statistical model before further evidence is taken into account. A ‘prior’ is expressed as a probability distribution and can be determined in a number of ways (e.g., previous research, subjective assessment, principles such as maximising entropy given constraints), and is typically combined with the likelihood function using Bayes’ theorem to obtain a posterior distribution."><a href=https://forrt.org/glossary/english/prior_distribution/ class=nav-link>Prior distribution</a></li><li class=nav-item data-toggle=tooltip data-placement=right title="The agreement made by several academics that they will not provide a peer review of a manuscript unless certain conditions are met. Specifically, the manuscript authors should ensure the data and materials will be made publically available (or give a justification as to why they are not freely available or shared), provide documentation detailing how to interpret and run any files or code and detail where these files can be located via the manuscript itself."><a href=https://forrt.org/glossary/english/pro/ class=nav-link>PRO (peer review openness) initiative</a></li><li class=nav-item data-toggle=tooltip data-placement=right title="Pseudonymisation refers to a technique that involves replacing or removing any information that could lead to identification of research subjects’ identity whilst still being able to make them identifiable through the use of the combination of code number and identifiers. This process comprises the following steps: removal of all identifiers from the research dataset; attribution of a specific identifier (pseudonym) for each participant and using it to label each research record; and maintenance of a cipher that links the code number to the participant in a document physically separate from the dataset. Pseudonymisation is typically a minimum requirement from ethical committees when conducting research, especially on human participants or involving confidential information, in order to ensure upholding of data privacy."><a href=https://forrt.org/glossary/english/pseudonymisation/ class=nav-link>Pseudonymisation</a></li><li class=nav-item data-toggle=tooltip data-placement=right title="When there is a lack of statistical independence presented in the data and thus artificially inflating the number of samples (i.e. replicates). For instance, collecting more than one data point from the same experimental unit (e.g. participant or crops). Numerous methods can overcome this, such as averaging across replicates (e.g., taking the mean RT for a participant) or implementing mixed effects models with the random effects structure accounting for the pseudoreplication (e.g., specifying each individual RT as belonging to the same subject). Note, the former option would be associated with a loss of information and statistical power."><a href=https://forrt.org/glossary/english/pseudoreplication/ class=nav-link>Pseudoreplication</a></li><li class=nav-item data-toggle=tooltip data-placement=right title="Psychometric meta-analyses aim to correct for attenuation of the effect sizes of interest due to measurement error and other artifacts by using procedures based on psychometric principles, e.g. reliability of the measures. These procedures should be implemented before using the synthesised effect sizes in correlational or experimental meta-analysis, as making these corrections tends to lead to larger and less variable effect sizes."><a href=https://forrt.org/glossary/english/psychometric_meta_analysis/ class=nav-link>Psychometric meta-analysis</a></li><li class=nav-item data-toggle=tooltip data-placement=right title="Trust in the knowledge, guidelines and recommendations that has been produced or provided by scientists to the benefit of civil society (Hendriks et al., 2016). These may also refer to trust in scientific-based recommendations on public health (e.g., universal health-care, stem cell research, federal funds for women’s reproductive rights, preventive measures of contagious diseases, and vaccination), climate change, economic policies (e.g., welfare, inequality- and poverty-control) and their intersections. The trust a member of the public has in science has been shown to be influenced by a vast number of factors such as age (Anderson et al., 2012), gender (Von Roten, 2004), rejection of scientific norms (Lewandowsky & Oberauer, 2021), political ideology (Azevedo & Jost, 2021; Brewer & Ley, 2012; Leiserowitz et al., 2010), 	right-wing authoritarianism and social dominance (Kerr & Wilson, 2021), education (Bak, 2001; Hayes & Tariq, 2000), income (Anderson et al., 2012), science knowledge (Evans & Durant, 1995; Nisbet et al., 2002), social media use (Huber et al., 2019), and religiosity (Azevedo, 2021; Brewer & Ley, 2013; Liu & Priest, 2009)."><a href=https://forrt.org/glossary/english/public_trust_in_science/ class=nav-link>Public Trust in Science</a></li><li class=nav-item data-toggle=tooltip data-placement=right title='The failure to publish results based on the "direction or strength of the study findings" (Dickersin & Min, 1993, p. 135). The bias arises when the evaluation of a study’s publishability disproportionately hinges on the outcome of the study, often with the inclination that novel and significant results are worth publishing more than replications and null results. This bias typically materializes through a disproportionate number of significant findings and inflated effect sizes. This process leads to the published scientific literature not being representative of the full extent of all research, and specifically underrepresents null finding. Such findings, in turn, land in the so called “file drawer”, where they are never published and have no findable documentation.'><a href=https://forrt.org/glossary/english/publication_bias/ class=nav-link>Publication bias (File Drawer Problem)</a></li><li class=nav-item data-toggle=tooltip data-placement=right title="An aphorism describing the pressure researchers feel to publish academic manuscripts, often in high prestige academic journals, in order to have a successful academic career. This pressure to publish a high quantity of manuscripts can go at the expense of the quality of the manuscripts. This institutional pressure is exacerbated by hiring procedures and funding decisions strongly focusing on the number and impact of publications."><a href=https://forrt.org/glossary/english/publish_or_perish/ class=nav-link>Publish or Perish</a></li><li class=nav-item data-toggle=tooltip data-placement=right title="A website that allows users to post anonymous peer reviews of research that has been published (i.e. post-publication peer review)."><a href=https://forrt.org/glossary/english/pubpeer/ class=nav-link>PubPeer</a></li><li class=nav-item data-toggle=tooltip data-placement=right title="An interpreted general-purpose programming language, intended to be user-friendly and easily readable, originally created by Guido van Rossum in 1991\. Python has an extensive library of additional features with accessible documentation for tasks ranging from data analysis to experiment creation. It is a popular programming language in data science, machine learning and web development. Similar to R Markdown, Python can be presented in an interactive online format called a Jupyter notebook, combining code, data, and text."><a href=https://forrt.org/glossary/english/python/ class=nav-link>Python</a></li><li class=nav-item data-toggle=tooltip data-placement=right title="Research which uses non-numerical data, such as textual responses, images, videos or other artefacts, to explore in-depth concepts, theories, or experiences. There are a wide range of qualitative approaches, from micro-detailed exploration of language or focusing on personal subjective experiences, to those which explore macro-level social experiences and opinions."><a href=https://forrt.org/glossary/english/qualitative_research/ class=nav-link>Qualitative research</a></li><li class=nav-item data-toggle=tooltip data-placement=right title="Quantitative research encompasses a diverse range of methods to systematically investigate a range of phenomena via the use of numerical data which can be analysed with statistics."><a href=https://forrt.org/glossary/english/quantitative_research/ class=nav-link>Quantitative research</a></li><li class=nav-item data-toggle=tooltip data-placement=right title="Decisions researchers make that raise doubts about the validity of measures used in a study, and ultimately the study’s final conclusions (Flake & Fried, 2020). Issues arise from a lack of transparency in reporting measurement practices, a failure to address construct validity, negligence, ignorance, or deliberate misrepresentation of information."><a href=https://forrt.org/glossary/english/questionable_measurement_practices/ class=nav-link>Questionable Measurement Practices (QMP)</a></li><li class=nav-item data-toggle=tooltip data-placement=right title="A range of activities that intentionally or unintentionally distort data in favour of a researcher’s own hypotheses \- or omissions in reporting such practices \- including; selective inclusion of data, hypothesising after the results are known (HARKing), and *p*\-hacking. Popularized by John et al. (2012)."><a href=https://forrt.org/glossary/english/questionable_research_practices_or_questionable_reporting_practices/ class=nav-link>Questionable Research Practices or Questionable Reporting Practices (QRPs)</a></li><li class=nav-item data-toggle=tooltip data-placement=right title="R is a free, open-source programming language and software environment that can be used to conduct statistical analyses and plot data. R was created by Ross Ihaka and Robert Gentleman at the University of Auckland. R enables authors to share reproducible analysis scripts, which increases the transparency of a study. Often, R is used in conjunction with an integrated development environment (IDE) which simplifies working with the language, for example RStudio or Visual Studio Code, or Tinn-R ."><a href=https://forrt.org/glossary/english/r/ class=nav-link>R</a></li><li class=nav-item data-toggle=tooltip data-placement=right title="An approach that integrates external criticism by colleagues and peers into the research process. Red teams are based on the idea that research that is more critically and widely evaluated is more reliable. The term originates from a military practice: One group (the red team) attacks something, and another group (the blue team) defends it. The practice has been applied to open science, by giving a red team (designated critical individuals) financial incentives to find errors in or identify improvements to the materials or content of a research project (in the materials, code, writing, etc.; Coles et al., 2020)."><a href=https://forrt.org/glossary/english/red_teams/ class=nav-link>Red Teams</a></li><li class=nav-item data-toggle=tooltip data-placement=right title="The process of reflexivity refers to critically considering the knowledge that we produce through research, how it is produced, and our own role as researchers in producing this knowledge. There are different forms of reflexivity; personal reflexivity whereby researchers consider the impact of their own personal experiences, and functional whereby researchers consider the way in which our research tools and methods may have impacted knowledge production. Reflexivity aims to bring attention to underlying factors which may impact the research process, including development of research questions, data collection, and the analysis."><a href=https://forrt.org/glossary/english/reflexivity/ class=nav-link>Reflexivity</a></li><li class=nav-item data-toggle=tooltip data-placement=right title="A scientific publishing format that includes an initial round of peer review of the background and methods (study design, measurement, and analysis plan); sufficiently high quality manuscripts are accepted for in-principle acceptance (IPA) at this stage. Typically, this stage 1 review occurs before data collection, however secondary data analyses are possible in this publishing format. Following data analyses and write up of results and discussion sections, the stage 2 review assesses whether authors sufficiently followed their study plan and reported deviations from it (and remains indifferent to the results). This shifts the focus of the review to the study’s proposed research question and methodology and away from the perceived interest in the study’s results."><a href=https://forrt.org/glossary/english/registered_report/ class=nav-link>Registered Report</a></li><li class=nav-item data-toggle=tooltip data-placement=right title="A global registry of research data repositories from different academic disciplines. It includes repositories that enable permanent storage of, description via metadata and access to, data sets by researchers, funding bodies, publishers, and scholarly institutions."><a href=https://forrt.org/glossary/english/registry_of_research_data_repositories/ class=nav-link>Registry of Research Data Repositories</a></li><li class=nav-item data-toggle=tooltip data-placement=right title="The extent to which repeated measurements lead to the same results. In psychometrics, reliability refers to the extent to which respondents have similar scores when they take a questionnaire on multiple occasions. Noteworthy, reliability does not imply validity. Furthermore, additional types of reliability besides internal consistency exist, including: test-retest reliability, parallel forms reliability and interrater reliability."><a href=https://forrt.org/glossary/english/reliability/ class=nav-link>Reliability</a></li><li class=nav-item data-toggle=tooltip data-placement=right title="Synonymous with *test-retest* *reliability*. It refers to the agreement between the results of successive measurements of the same measure. Repeatability requires the same experimental tools, the same observer, the same measuring instrument administered under the same conditions, the same location, repetition over a short period of time, and the same objectives (Joint Committee for Guidelines in Metrology, 2008\)"><a href=https://forrt.org/glossary/english/repeatability/ class=nav-link>Repeatability</a></li><li class=nav-item data-toggle=tooltip data-placement=right title="An umbrella term, used differently across fields, covering concepts of: direct and conceptual replication, computational reproducibility/replicability, generalizability analysis and robustness analyses. Some of the definitions used previously include: a different team arriving at the same results using the original author's artifacts (Barba 2018); a study arriving at the same conclusion after collecting new data (Claerbout and Karrenbach, 1992); as well as studies for which any outcome would be considered diagnostic evidence about a claim from prior research (Nosek & Errington, 2020)."><a href=https://forrt.org/glossary/english/replicability/ class=nav-link>Replicability</a></li><li class=nav-item data-toggle=tooltip data-placement=right title="A replication market is an environment where users bet on the replicability of certain effects. Forecasters are incentivized to make accurate predictions and the top successful forecasters receive monetary compensation or contributorship for their bets. The rationale behind a replication market is that it leverages the collective wisdom of the scientific community to predict which effect will most likely replicate, thus encouraging researchers to channel their limited resources to replicating these effects."><a href=https://forrt.org/glossary/english/replication_markets/ class=nav-link>Replication Markets</a></li><li class=nav-item data-toggle=tooltip data-placement=right title="Collaborative Assessment for Trustworthy Science. The repliCATS project’s aim is to crowdsource predictions about the reliability and replicability of published research in eight social science fields: business research, criminology, economics, education, political science, psychology, public administration, and sociology."><a href=https://forrt.org/glossary/english/replicats_project/ class=nav-link>RepliCATs project</a></li><li class=nav-item data-toggle=tooltip data-placement=right title="A reporting guideline is a “checklist, flow diagram, or structured text to guide authors in reporting a specific type of research, developed using explicit methodology.” (EQUATOR Network, n.d.). Reporting guidelines provide the minimum guidance required to ensure that research findings can be appropriately interpreted, appraised, synthesized and replicated. Their use often differs per scientific journal or publisher."><a href=https://forrt.org/glossary/english/reporting_guideline/ class=nav-link>Reporting Guideline</a></li><li class=nav-item data-toggle=tooltip data-placement=right title="An online archive for the storage of digital objects including research outputs, manuscripts, analysis code and/or data. Examples include preprint servers such as bioRxiv, MetaArXiv, PsyArXiv, institutional research repositories, as well as data repositories that collect and store datasets including zenodo.org, PsychData, and code repositories such as Github, or more general repositories for all kinds of research data, such as the Open Science Framework (OSF). Digital objects stored in repositories are typically described through metadata which enables discovery across different storage locations."><a href=https://forrt.org/glossary/english/repository/ class=nav-link>Repository</a></li><li class=nav-item data-toggle=tooltip data-placement=right title="A grassroots initiative that helps researchers create local journal clubs at their universities to discuss a range of topics relating to open research and scholarship. Each meeting usually centres around a specific paper that discusses, for example, reproducibility, research practice, research quality, social justice and inclusion, and ideas for improving science."><a href=https://forrt.org/glossary/english/reproducibilitea/ class=nav-link>ReproducibiliTea</a></li><li class=nav-item data-toggle=tooltip data-placement=right title='A minimum standard on a spectrum of activities ("reproducibility spectrum") for assessing the value or accuracy of scientific claims based on the original methods, data, and code. For instance, where the original researcher&#39;s data and computer codes are used to regenerate the results (Barba, 2018), often referred to as computational reproducibility. Reproducibility does not guarantee the quality, correctness, or validity of the published results (Peng, 2011). In some fields, this meaning is, instead, associated with the term “replicability” or ‘repeatability’.'><a href=https://forrt.org/glossary/english/reproducibility/ class=nav-link>Reproducibility</a></li><li class=nav-item data-toggle=tooltip data-placement=right title="The finding, and related shift in academic culture and thinking, that a large proportion of scientific studies published across disciplines do not replicate (e.g. Open Science Collaboration, 2015). This is considered to be due to a lack of quality and integrity of research and publication practices, such as publication bias, QRPs and a lack of transparency, leading to an inflated rate of false positive results. Others have described this process as a ‘Credibility revolution’ towards improving these practices."><a href=https://forrt.org/glossary/english/reproducibility_crisis/ class=nav-link>Reproducibility crisis (aka Replicability or replication crisis)</a></li><li class=nav-item data-toggle=tooltip data-placement=right title="A reproducibility network is a consortium of open research working groups, often peer-led. The groups operate on a wheel-and-spoke model across a particular country, in which the network connects local cross-disciplinary researchers, groups, and institutions with a central steering group, who also connect with external stakeholders in the research ecosystem. The goals of reproducibility networks include; advocating for greater awareness, promoting training activities, and disseminating best-practices at grassroots, institutional, and research ecosystem levels. Such networks exist in the UK, Germany, Switzerland, Slovakia, and Australia (as of March 2021)."><a href=https://forrt.org/glossary/english/reproducibility_network/ class=nav-link>Reproducibility Network</a></li><li class=nav-item data-toggle=tooltip data-placement=right title="Type of semantometric measure assessing similarity of publications connected in a citation network. This method uses a simple formula to assess authors’ contributions. Publication *p* can be estimated based on the semantic distance from the publications cited by *p* to publications citing *p*."><a href=https://forrt.org/glossary/english/research_contribution_metric/ class=nav-link>Research Contribution Metric (*p*)</a></li><li class=nav-item data-toggle=tooltip data-placement=right title="Describes the circular process of conducting scientific research, with “researchers working at various stages of inquiry, from more tentative and exploratory investigations to the testing of more definitive and well-supported claims” (Lieberman, 2020, p. 42). The cycle includes literature research and hypothesis generation, data collection and analysis, as well as dissemination of results (e.g. through publication in peer-reviewed journals), which again informs theory and new hypotheses/research."><a href=https://forrt.org/glossary/english/research_cycle/ class=nav-link>Research Cycle</a></li><li class=nav-item data-toggle=tooltip data-placement=right title="Research Data Management (RDM) is a broad concept that includes processes undertaken to create organized, documented, accessible, and reusable quality research data. Adequate research data management provides many benefits including, but not limited to, reduced likelihood of data loss, greater visibility and collaborations due to data sharing, demonstration of research integrity and accountability."><a href=https://forrt.org/glossary/english/research_data_management/ class=nav-link>Research Data Management</a></li><li class=nav-item data-toggle=tooltip data-placement=right title="Research integrity is defined by a set of good research practices based on fundamental principles: honesty, reliability, respect and accountability (ALLEA, 2017). Good research practices —which are based on fundamental principles of research integrity and should guide researchers in their work as well as in their engagement with the practical, ethical and intellectual challenges inherent in research— refer to areas such as: research environment (e.g., research institutions and organisations promote awareness and ensure a prevailing culture of research integrity), training, supervision and mentoring (e.g., Research institutions and organisations develop appropriate and adequate training in ethics and research integrity to ensure that all concerned are made aware of the relevant codes and regulations), research procedures (e.g., researchers report their results in a way that is compatible with the standards of the discipline and, where applicable, can be verified and reproduced), safeguards (e.g., researchers have due regard for the health, safety and welfare of the community, of collaborators and others connected with their research), data practices and management (e.g., researchers, research institutions and organisations provide transparency about how to access or make use of their data and research materials), collaborative working, publication and dissemination (e.g., authors and publishers consider negative results to be as valid as positive findings for publication and dissemination), reviewing, evaluating and editing (e.g., researchers review and evaluate submissions for publication, funding, appointment, promotion or reward in a transparent and justifiable manner)."><a href=https://forrt.org/glossary/english/research_integrity/ class=nav-link>Research integrity</a></li><li class=nav-item data-toggle=tooltip data-placement=right title="A detailed document prepared before conducting a study, often written as part of ethics and funding applications. The protocol should include information relating to the background, rationale and aims of the study, as well as hypotheses which reflect the researchers’ expectations. The protocol should also provide a “recipe” for conducting the study, including methodological details and clear analysis plans. Best practice guidelines for creating a study protocol should be used for specific methodologies and fields. It is possible to publically share research protocols to attract new collaborators or facilitate efficient collaboration across labs (e.g. [https://www.protocols.io/](https://www.protocols.io/)). In medical and educational fields, protocols are often a separate article type suitable for publication in journals. Where protocol sharing or publication is not common practice, researchers can choose preregistration."><a href=https://forrt.org/glossary/english/research_protocol/ class=nav-link>Research Protocol</a></li><li class=nav-item data-toggle=tooltip data-placement=right title="The process of conducting research from conceptualisation to dissemination. A typical workflow may look like the following: Starting with conceptualisation to identify a research question and design a study. After study design, researchers need to gain ethical approval (if necessary) and may decide to preregister the final version. Researchers then collect and analyse their data. Finally, the process ends with dissemination; moving between pre-print and post-print stages as the manuscript is submitted to a journal."><a href=https://forrt.org/glossary/english/research_workflow/ class=nav-link>Research workflow</a></li><li class=nav-item data-toggle=tooltip data-placement=right title="refers to the flexibility often inherent in the scientific process, from hypothesis generation, designing and conducting a research study to processing the data and analyzing as well as interpreting and reporting results. Due to a lack of precisely defined theories and/or empirical evidence, multiple decisions are often equally justifiable. The term is sometimes used to refer to the opportunistic (ab-)use of this flexibility aiming to achieve desired results —e.g., when in- or excluding certain data— albeit the fact that technically the term is not inherently value-laden."><a href=https://forrt.org/glossary/english/researcher_degrees_of_freedom/ class=nav-link>Researcher degrees of freedom</a></li><li class=nav-item data-toggle=tooltip data-placement=right title="An approach that considers societal implications and expectations, relating to research and innovation, with the aim to foster inclusivity and sustainability. It accounts for the fact that scientific endeavours are not isolated from their wider effects and that research is motivated by factors beyond the pursuit of knowledge. As such, many parties are important in fostering responsible research, including funding bodies, research teams, stakeholders, activists, and members of the public."><a href=https://forrt.org/glossary/english/responsible_research_and_innovation/ class=nav-link>Responsible Research and Innovation</a></li><li class=nav-item data-toggle=tooltip data-placement=right title="Exploiting researcher degrees of freedom during statistical analysis in order to increase the likelihood of accepting the null hypothesis (for instance, *p* \> .05)."><a href=https://forrt.org/glossary/english/reverse_p_hacking/ class=nav-link>Reverse p-hacking</a></li><li class=nav-item data-toggle=tooltip data-placement=right title="The RIOT Science Club is a multi-site seminar series that raises awareness and provides training in Reproducible, Interpretable, Open & Transparent science practices. It provides regular talks, workshops and conferences, all of which are openly available and rewatchable on the respective location’s websites and Youtube."><a href=https://forrt.org/glossary/english/riot_science_club/ class=nav-link>RIOT Science Club</a></li><li class=nav-item data-toggle=tooltip data-placement=right title="The persistence of support for a hypothesis under perturbations of the methodological/analytical pipeline In other words, applying different methods/analysis pipelines to examine if the same conclusion is supported under analytical different conditions."><a href=https://forrt.org/glossary/english/robustness/ class=nav-link>Robustness (analyses)</a></li><li class=nav-item data-toggle=tooltip data-placement=right title="A questionable research/reporting practice strategy, often done *post hoc*, to increase the number of publishable manuscripts by ‘slicing’ up the data from a single study \- one example of a method of ‘gaming the system’ of academic incentives. For instance, this may involve publishing multiple studies based on a single dataset, or publishing multiple studies from different data collection sites without transparently stating where the data originally derives from. Such practices distort the literature, and particularly meta-analyses, because it is unclear that the findings were obtained from the same dataset, thereby concealing the dependencies across the separately published papers."><a href=https://forrt.org/glossary/english/salami_slicing/ class=nav-link>Salami slicing</a></li><li class=nav-item data-toggle=tooltip data-placement=right title="The act of reporting or publishing a novel finding prior to another researcher/team. Survey-based research indicates that fear of being scooped is an important fear-related barrier for data sharing in psychology, and agent-based models suggest that competition for priority harms scientific reliability (Tiokhin et al. 2021)."><a href=https://forrt.org/glossary/english/scooping/ class=nav-link>Scooping</a></li><li class=nav-item data-toggle=tooltip data-placement=right title="A class of metrics for evaluating research using full publication text to measure semantic similarity of publications and highlighting an article’s contribution to the progress of scholarly discussion. It is an extension of tools such as bibliometrics, webometrics, and altmetrics."><a href=https://forrt.org/glossary/english/semantometrics/ class=nav-link>Semantometrics</a></li><li class=nav-item data-toggle=tooltip data-placement=right title="Research that poses a threat to those who are or have been involved in it, including the researchers, the participants, and the wider society. This threat can be physical danger (e.g. suicide) or a negative emotional response (e.g. depression) to those who are involved in the research process. For instance, research conducted on victims of suicide, the researcher might be emotionally traumatised by the descriptions of the suicidal behaviours. Indeed, the communication with the victims might also make them re-experience the traumatic memories, leading to negative psychological responses."><a href=https://forrt.org/glossary/english/sensitive_research/ class=nav-link>Sensitive research</a></li><li class=nav-item data-toggle=tooltip data-placement=right title="An authorship system that assigns authorship order based on the contribution of each author. The names of the authors are listed according to their contribution in descending order with the most contributing author first and the least contributing author last."><a href=https://forrt.org/glossary/english/sequence_determines_credit_approach/ class=nav-link>Sequence-determines-credit approach (SDC)</a></li><li class=nav-item data-toggle=tooltip data-placement=right title="Evaluation of research products by qualified experts where the reviewer(s) knows the identity of the author(s), but the reviewer(s) remains anonymous to the author(s)."><a href=https://forrt.org/glossary/english/single_blind_peer_review/ class=nav-link>Single-blind peer review</a></li><li class=nav-item data-toggle=tooltip data-placement=right title="Adopting Open Scholarship practices leads to a longer research process overall, with more focus on transparency, reproducibility, replicability and quality, over the quantity of outputs. Slow Science opposes publish-or-perish culture and describes an academic system that allows time and resources to produce fewer higher-quality and transparent outputs, for instance prioritising researcher time towards collecting more data, more time to read the literature, think about how their findings fit the literature and documenting and sharing research materials instead of running additional studies."><a href=https://forrt.org/glossary/english/slow_science/ class=nav-link>Slow science</a></li><li class=nav-item data-toggle=tooltip data-placement=right title="Social class is usually measured using both objective and subjective measurements, as recommended by the American Psychological Association (American Psychological Association,Task Force on Socioeconomic Status, 2007). Unlike the conventional concept, which only considers one factor, either education or income (e.g., economic variables), an individual's social class is considered to be a combination of their education, income, occupational prestige, subjective social status, and self-identified social class. Social class is partly a cultural variable, as it is a stable variable and likely to change slowly over the years. Social class can have important implications to academic outcomes. An individual may have a high socio-economic status yet identify as a working class individual. Working class students tend to have different life circumstances and often more restrictive commitments than middle-class students, which make their integration with other students more difficult (Rubin, 2021). The lack of time and money is obstructive to their social experience at university. Working class students are more likely to work to support themselves, resulting in less time for academic activities and for socializing with other students as well as less money to purchase items linked to social experiences (e.g. food)."><a href=https://forrt.org/glossary/english/social_class/ class=nav-link>Social class</a></li><li class=nav-item data-toggle=tooltip data-placement=right title="Social integration is a multi-dimensional construct. In an academic context, social integration is related to the quantity and quality of the social interactions with staff and students, as well as the sense of connection and belonging to the university and the people within the institute. To be more specific, social support, trust, and connectedness are all variables that contribute to social integration. Social integration has important implications for academic outcomes and mental wellbeing (Evans & Rubin, 2021). Working class students are less likely to integrate with other students, since they have differing social and economic backgrounds and less disposable income. Thus they are not able to experience as many educational and fiscal opportunities than others. In turn, this can lead to poor mental health and feelings of ostracism (Rubin, 2021)."><a href=https://forrt.org/glossary/english/social_integration/ class=nav-link>Social integration</a></li><li class=nav-item data-toggle=tooltip data-placement=right title="SORTEE ([https://www.sortee.org/](https://www.sortee.org/)) is an international society with the aim of improving the transparency and reliability of research results in the fields of ecology, evolution, and related disciplines through cultural and institutional changes. SORTEE was launched in December 2020 to anyone interested in improving research in these disciplines, regardless of experience. The society is international in scope, membership, and objectives. As of May 2021, SORTEE comprises of over 600 members."><a href=https://forrt.org/glossary/english/society_for_open_reliable_and_transparent_ecology_and_evolutionary_biology/ class=nav-link>Society for Open, Reliable, and Transparent Ecology and Evolutionary biology (SORTEE)</a></li><li class=nav-item data-toggle=tooltip data-placement=right title="A membership society founded to further promote improved methods and practices in the psychological research field. The society aims to complete its mission statement by enhancing the training of psychological researchers; by promoting research cultures that are more conducive to better quality research; by quantifying and empirically assessing the impact of such reforms; and by leading outreach events within and outside psychology to better the current state of research norms."><a href=https://forrt.org/glossary/english/society_for_the_improvement_of_psychological_science/ class=nav-link>Society for the Improvement of Psychological Science (SIPS)</a></li><li class=nav-item data-toggle=tooltip data-placement=right title="An analytic approach that consists of identifying, calculating, visualising and interpreting results (through inferential statistics) for *all* reasonable specifications for a particular research question (see Simonsohn et al. 2015). Specification curve analysis helps make transparent the influence of presumably arbitrary decisions during the scientific progress (e.g., experimental design, construct operationalization, statistical models or several of these) made by a researcher by comprehensively reporting all non-redundant, sensible tests of the research question. Voracek et al. (2019) suggest that SCA differs from multiverse analysis with regards to the graphical displays (a specification curve plot rather than a histogram and tile plot) and the use of inferential statistics to interpret findings."><a href=https://forrt.org/glossary/english/specification_curve_analysis/ class=nav-link>Specification Curve Analysis</a></li><li class=nav-item data-toggle=tooltip data-placement=right title="Analytical approaches and models assume certain characteristics of one’s data (e.g., statistical independence, random samples, normality, equal variance,...). Before running an analysis, these assumptions should be checked since their violation can change the results and conclusion of a study. Good practice in open and reproducible science is to report assumption testing in terms of the assumptions verified and the results of such checks or corrections applied."><a href=https://forrt.org/glossary/english/statistical_assumptions/ class=nav-link>Statistical Assumptions</a></li><li class=nav-item data-toggle=tooltip data-placement=right title="Statistical power is the long-run probability that a statistical test correctly rejects the null hypothesis if the alternative hypothesis is true. It ranges from 0 to 1, but is often expressed as a percentage. Power can be estimated using the significance criterion (alpha), effect size, and sample size used for a specific analysis technique. There are two main applications of statistical power. A priori power where the researcher asks the question “given an effect size, how many participants would I need for X% power?”. Sensitivity power asks the question “given a known sample size, what effect size could I detect with X% power?”."><a href=https://forrt.org/glossary/english/statistical_power/ class=nav-link>Statistical power</a></li><li class=nav-item data-toggle=tooltip data-placement=right title="A property of a result using Null Hypothesis Significance Testing (NHST) that, given a significance level, is deemed unlikely to have occurred given the null hypothesis. Tenny and Abdelgawad (2017) defined it as “a measure of the probability of obtaining your data or more extreme data assuming the null hypothesis is true, compared to a pre-selected acceptable level of uncertainty regarding the true answer” (p. 1). Conventions for determining the threshold vary between applications and disciplines but ultimately depend on the considerations of the researcher about an appropriate error margin. The American Statistical Association’s statement (Wasserstein & Lazar, 2016\) notes that “Researchers often wish to turn a p-value into a statement about the truth of a null hypothesis, or about the probability that random chance produced the observed data. The p-value is neither. It is a statement about data in relation to a specified hypothetical explanation, and is not a statement about the explanation itself” (p. 131)."><a href=https://forrt.org/glossary/english/statistical_significance/ class=nav-link>Statistical significance</a></li><li class=nav-item data-toggle=tooltip data-placement=right title="The extent to which conclusions from a statistical test are accurate and reflective of the true effect found in nature. In other words, whether or not a relationship exists between two variables and can be accurately detected with the conducted analyses. Threats to statistical validity include low power, violation of assumptions, reliability of measures, etc, affecting the reliability and generality of the conclusions."><a href=https://forrt.org/glossary/english/statistical_validity/ class=nav-link>Statistical validity</a></li><li class=nav-item data-toggle=tooltip data-placement=right title="The STRANGE “framework” is a proposal and series of questions to help animal behaviour researchers consider sampling biases when planning, performing and interpreting research with animals. STRANGE is an acronym highlighting several possible sources of sampling bias in animal research, such as the animals’ Social background; Trappability and self-selection; Rearing history; Acclimation and habituation; Natural changes in responsiveness; Genetic make-up, and Experience."><a href=https://forrt.org/glossary/english/strange/ class=nav-link>STRANGE</a></li><li class=nav-item data-toggle=tooltip data-placement=right title="A free online platform through which researchers post brief descriptions of research projects or resources that are available for use (“haves”) or that they require and another researcher may have (“needs”). StudySwap is a crowdsourcing approach to research which can ensure that fewer research resources go unused and more researchers have access to the resources they need."><a href=https://forrt.org/glossary/english/studyswap/ class=nav-link>StudySwap</a></li><li class=nav-item data-toggle=tooltip data-placement=right title="A form of literature review and evidence synthesis. A systematic review will usually include a thorough, repeatable (reproducible) search strategy including key terms and databases in order to find relevant literature on a given topic or research question. Systematic reviewers follow a process of screening the papers found through their search, until they have filtered down to a set of papers that fit their predefined inclusion criteria. These papers can then be synthesised in a written review which may optionally include statistical synthesis in the form of a meta-analysis as well. A systematic review should follow a standard set of guidelines to ensure that bias is kept to a minimum for example PRISMA (Moher et al., 2009; Page et al., 2021), Cochrane Systematic Reviews (Higgins et al., 2019), or NIRO-SR (Topor et al., 2021)."><a href=https://forrt.org/glossary/english/systematic_review/ class=nav-link>Systematic Review</a></li><li class=nav-item data-toggle=tooltip data-placement=right title="tenzing* is an online webapp and R package that helps researchers to track and report the contributions of each team member using the CRediT taxonomy in an efficient way. Team members of a research project can indicate their contributions to each CRediT role using an online spreadsheet template, and provide any additional authors' information (e.g., name, affiliation, order in publication, email address, and ORCID iD). Upon writing the manuscript, *tenzing* can automatically create a list of contributors belonging to each CRediT role to be included in the contributions section and create the manuscript’s title page."><a href=https://forrt.org/glossary/english/tenzing/ class=nav-link>Tenzing</a></li><li class=nav-item data-toggle=tooltip data-placement=right title="Described as a combination of low statistical power, a surprising result, and a *p*\-value only slightly lower than .05."><a href=https://forrt.org/glossary/english/the_troubling_trio/ class=nav-link>The Troubling Trio</a></li><li class=nav-item data-toggle=tooltip data-placement=right title="A theory is a unifying explanation or description of a process or phenomenon, which is amenable to repeated testing and verifiable through scientific investigation, using various experiments led by several independent researchers. Theories may be rejected or deemed an unsatisfactory explanation of a phenomenon after rigorous testing of a new hypothesis that explains the phenomena better or seems to contradict them but is more generalisable to a wider array of findings."><a href=https://forrt.org/glossary/english/theory/ class=nav-link>Theory</a></li><li class=nav-item data-toggle=tooltip data-placement=right title="The process of creating and developing a statement of concepts and their interrelationships to show how and/or why a phenomenon occurs. Theory building leads to theory testing."><a href=https://forrt.org/glossary/english/theory_building/ class=nav-link>Theory building</a></li><li class=nav-item data-toggle=tooltip data-placement=right title="Having one’s actions open and accessible for external evaluation. Transparency pertains to researchers being honest about theoretical, methodological, and analytical decisions made throughout the research cycle. Transparency can be usefully differentiated into “scientifically relevant transparency” and “socially relevant transparency”. While the former has been the focus of early Open Science discourses, the latter is needed to provide scientific information in ways that are relevant to decision makers and members of the public (Elliott & Resnik, 2019)."><a href=https://forrt.org/glossary/english/transparency/ class=nav-link>Transparency</a></li><li class=nav-item data-toggle=tooltip data-placement=right title="The transparency checklist is a consensus-based, comprehensive checklist that contains 36 items that cover the prepregistration, methods, results and discussion and data, code and materials availability. A shortened 12-item version of the checklist is also available. Checklist responses can be submitted alongside a manuscript for review. While the checklist can also work for educational purposes, it mainly aims to support researchers to identify concrete actions that can increase the transparency of their research while a disclosed checklist can help the readers and reviewers gain critical information about different aspects of transparency of the submitted research."><a href=https://forrt.org/glossary/english/transparency_checklist/ class=nav-link>Transparency Checklist</a></li><li class=nav-item data-toggle=tooltip data-placement=right title="Evaluation of research products by qualified experts where the author(s) are anonymous to both the reviewer(s) and editor(s). “Blinding of the authors and their affiliations to both editors and reviewers. This approach aims to eliminate institutional, personal, and gender biases” (Tvina et al., 2019, p. 1082)."><a href=https://forrt.org/glossary/english/triple_blind_peer_review/ class=nav-link>Triple-blind peer review</a></li><li class=nav-item data-toggle=tooltip data-placement=right title="A set of guiding principles that consider Transparency, Responsibility, User focus, Sustainability, and Technology (TRUST) as the essential components for assessing, developing, and sustaining the trustworthiness of digital data repositories (especially those that store research data). They are complementary to the FAIR Data Principles."><a href=https://forrt.org/glossary/english/trust_principles/ class=nav-link>TRUST Principles</a></li><li class=nav-item data-toggle=tooltip data-placement=right title="“Incorrect rejection of a null hypothesis” (Simmons et al., 2011, p. 1359), i.e. finding evidence to reject the null hypothesis that there is no effect when the evidence is actually in favouring of retaining the null that there is no effect (For example, a judge imprisoning an innocent person). Concluding that there is a significant effect and rejecting the null hypothesis when your findings actually occured by chance."><a href=https://forrt.org/glossary/english/type_i_error/ class=nav-link>Type I error</a></li><li class=nav-item data-toggle=tooltip data-placement=right title="A false negative result occurs when the alternative hypothesis is true in the population but the null hypothesis is accepted as part of the analysis (Hartgerink et al., 2017). That is, finding a non-significant statistical result when the effect is true (For example, a judge passing an innocent verdict on a guilty person). False negatives are less likely to be the subject of replications than positive results (Fiedler et al., 2012), and remain an unresolved issue in scientific research (Hartgerink et al., 2017)."><a href=https://forrt.org/glossary/english/type_ii_error/ class=nav-link>Type II error</a></li><li class=nav-item data-toggle=tooltip data-placement=right title="A Type M error occurs when a researcher concludes that an effect was observed with magnitude lower or higher than the real one. For example, a type M error occurs when a researcher claims that an effect of small magnitude was observed when it is large in truth or vice versa."><a href=https://forrt.org/glossary/english/type_m_error/ class=nav-link>Type M error</a></li><li class=nav-item data-toggle=tooltip data-placement=right title="A Type S error occurs when a researcher concludes that an effect was observed with an opposite sign than real one. For example, a type S error occurs when a researcher claims that a positive effect was observed when it is negative in reality or vice versa."><a href=https://forrt.org/glossary/english/type_s_error/ class=nav-link>Type S error</a></li><li class=nav-item data-toggle=tooltip data-placement=right title="Not all voices, perspectives, and members of the community are adequately represented. Under-representation typically occurs when the voices or perspectives of one group dominate, resulting in the marginalization of another. This often affects groups who are a minority in relation to certain personal characteristics."><a href=https://forrt.org/glossary/english/under_representation/ class=nav-link>Under-representation</a></li><li class=nav-item data-toggle=tooltip data-placement=right title="A framework for improving learning and optimising teaching based upon scientific insights of how humans learn. It aims to make learning inclusive and transformative for all people in which the focus is on catering to the differing needs of different students. It is often regarded as an evidence-based and scientifically valid framework to guide educational practice, consisting of three key principles: engagement, representation, and action and expression. In addition, UDL is included in the Higher Education Opportunity Act of 2008 (Edyburn, 2010)."><a href=https://forrt.org/glossary/english/universal_design_for_learning/ class=nav-link>Universal design for learning (UDL)</a></li><li class=nav-item data-toggle=tooltip data-placement=right title="Validity refers to the application of statistical principles to arrive at well-founded —i.e., likely corresponding accurately to the real world— concepts, conclusions or measurement. In psychometrics, validity refers to the extent to which something measures what it intends to or claims to measure. Under this generic term, there are different types of validity (e.g., internal validity, construct validity, face validity, criterion validity, diagnostic validity, discriminant validity, concurrent validity, convergent validity, predictive validity, external validity)."><a href=https://forrt.org/glossary/english/validity/ class=nav-link>Validity</a></li><li class=nav-item data-toggle=tooltip data-placement=right title="The practice of managing and recording changes to digital resources (e.g. files, websites, programmes, etc.) over time so that you can recall specific versions later. Version control systems are designed to record the history of changes (who, what and when), and help to avoid human errors (e.g. working on the wrong version). For example, the Git version control system is a widely used software tool that originally helped software developers to version control shared code and is now used across many scientific disciplines to manage and share files."><a href=https://forrt.org/glossary/english/version_control/ class=nav-link>Version control</a></li><li class=nav-item data-toggle=tooltip data-placement=right title="Webometrics involves the study of online content. Webometrics focuses on the numbers and types of hyperlinks between different online sites. Such approaches have been considered as a type of altmetrics. “The study of the quantitative aspects of the construction and use of information resources, structures and technologies on the Web drawing on [bibliometric](https://en.wikipedia.org/wiki/Bibliometrics) and [informetric](https://en.wikipedia.org/wiki/Informetrics) approaches” (Björneborn & Ingwersen, 2004)."><a href=https://forrt.org/glossary/english/webometrics/ class=nav-link>Webometrics</a></li><li class=nav-item data-toggle=tooltip data-placement=right title><a href=https://forrt.org/glossary/english/weird/ class=nav-link>WEIRD</a></li><li class=nav-item data-toggle=tooltip data-placement=right title="Computing a Z-score is a statistical approach mainly used to obtain the ‘Estimated Replication Rate’ (ERR) and ‘Expected Discovery Rate’ (EDR) for a set of reported studies. Calculating a *z*\-curve for a set of statistically significant studies involves converting reported *p*\-values to *z*\-scores, fitting a finite mixture model to the distribution of *z*\-scores, and estimating mean power based on the mixture model. The Z-curve analysis can be performed in R through a dedicated package \- https://cran.r-project.org/web/packages/zcurve/index.html."><a href=https://forrt.org/glossary/english/z_curve/ class=nav-link>Z-Curve</a></li><li class=nav-item data-toggle=tooltip data-placement=right title="An open science repository where researchers can deposit research papers, reports, data sets, research software, and any other research-related digital artifacts. Zenodo creates a persistent digital object identifier (DOI) for each submission to make it citable. This platform was developed under the European OpenAIRE program and operated by CERN."><a href=https://forrt.org/glossary/english/zenodo/ class=nav-link>Zenodo</a></li></ul></nav></div></main></div></div><script src=https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js integrity="sha256-9/aliU8dGd2tb6OSsuzixeV4y/faTqgFtohetphbbj0=" crossorigin=anonymous></script><script src=https://cdnjs.cloudflare.com/ajax/libs/jquery.imagesloaded/4.1.4/imagesloaded.pkgd.min.js integrity="sha256-lqvxZrPLtfffUl2G/e7szqSvPBILGbwmsGE1MKlOi0Q=" crossorigin=anonymous></script><script src=https://cdnjs.cloudflare.com/ajax/libs/jquery.isotope/3.0.6/isotope.pkgd.min.js integrity="sha256-CBrpuqrMhXwcLLUd5tvQ4euBHCdh7wGlDfNz8vbu/iI=" crossorigin=anonymous></script><script src=https://cdnjs.cloudflare.com/ajax/libs/fancybox/3.5.7/jquery.fancybox.min.js integrity="sha256-yt2kYMy0w8AbtF89WXb2P1rfjcP/HTHLT7097U8Y5b8=" crossorigin=anonymous></script><script src=https://cdnjs.cloudflare.com/ajax/libs/instant.page/5.1.0/instantpage.min.js integrity="sha512-1+qUtKoh9XZW7j+6LhRMAyOrgSQKenQ4mluTR+cvxXjP1Z54RxZuzstR/H9kgPXQsVB8IW7DMDFUJpzLjvhGSQ==" crossorigin=anonymous></script><script src=https://cdnjs.cloudflare.com/ajax/libs/mermaid/8.4.8/mermaid.min.js integrity="sha256-lyWCDMnMeZiXRi7Zl54sZGKYmgQs4izcT7+tKc+KUBk=" crossorigin=anonymous title=mermaid></script><script src=https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.2/highlight.min.js integrity="sha512-7t8APmYpzEsZP7CYoA7RfMPV9Bb+PJHa9x2WiUnDXZx3XHveuyWUtvNOexhkierl5flZ3tr92dP1mMS+SGlD+A==" crossorigin=anonymous></script><script src=https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.2/languages/r.min.js></script><script src=https://cdnjs.cloudflare.com/ajax/libs/leaflet/1.5.1/leaflet.js integrity="sha256-EErZamuLefUnbMBQbsEqu1USa+btR2oIlCpBJbyD4/g=" crossorigin=anonymous></script><script>const code_highlighting=!0</script><script>const search_config={indexURI:"/index.json",minLength:1,threshold:.3},i18n={no_results:"No results found",placeholder:"Search...",results:"results found"},content_type={post:"Posts",project:"Projects",publication:"Publications",talk:"Talks",slides:"Slides"}</script><script id=search-hit-fuse-template type=text/x-template>
      <div class="search-hit" id="summary-{{key}}">
      <div class="search-hit-content">
        <div class="search-hit-name">
          <a href="{{relpermalink}}">{{title}}</a>
          <div class="article-metadata search-hit-type">{{type}}</div>
          <p class="search-hit-description">{{snippet}}</p>
        </div>
      </div>
      </div>
    </script><script src=https://cdnjs.cloudflare.com/ajax/libs/fuse.js/3.2.1/fuse.min.js integrity="sha256-VzgmKYmhsGNNN4Ph1kMW+BjoYJM2jV5i4IlFoeZA9XI=" crossorigin=anonymous></script><script src=https://cdnjs.cloudflare.com/ajax/libs/mark.js/8.11.1/jquery.mark.min.js integrity="sha256-4HLtjeVgH0eIB3aZ9mLYF6E8oU5chNdjU6p6rrXpl9U=" crossorigin=anonymous></script><script src=/js/academic.min.5367584a2dab5698b5d631d722b877ef.js></script><div class=container><style>footer{margin:.5rem;padding:.5rem;float:left}</style><footer class=site-footer><p class=powered-by><a class="btn btn-primary" href=/about/cite_us/ role=button>Cite FORRT</a>
<span style=margin-left:2rem></span>
<a href=/imprint/>Imprint</a></p><p class=powered-by>© 2025 - FORRT > Framework for Open and Reproducible Research Training</p><p class="powered-by copyright-license-text">Except where otherwise noted, content on this site is licensed under a <a href=https://creativecommons.org/licenses/by-nc-sa/4.0 rel="noopener noreferrer" target=_blank>CC BY NC SA 4.0</a> license</p><div class="d-flex justify-content-center powered-by footer-license-icons pb-2"><a rel=license href=http://creativecommons.org/licenses/by-nc-sa/4.0/><img alt="Creative Commons License" style=border-width:0 src=https://i.creativecommons.org/l/by-nc-sa/4.0/88x31.png></a></div><p class=powered-by>This website is published using two great open source tools:
<a href=https://gohugo.io target=_blank rel=noopener>Hugo</a> & the
    <a href=https://hugoblox.com/ target=_blank rel=noopener>Academic theme.</a>
<span class=float-right aria-hidden=true><a href=# class=back-to-top><span class=button_icon><i class="fas fa-chevron-up fa-2x"></i></span></a></span></p></footer></div><div id=modal class="modal fade" role=dialog><div class=modal-dialog><div class=modal-content><div class=modal-header><h5 class=modal-title>Cite</h5><button type=button class=close data-dismiss=modal aria-label=Close>
<span aria-hidden=true>&#215;</span></button></div><div class=modal-body><pre><code class="tex hljs"></code></pre></div><div class=modal-footer><a class="btn btn-outline-primary my-1 js-copy-cite" href=# target=_blank><i class="fas fa-copy"></i> Copy
</a><a class="btn btn-outline-primary my-1 js-download-cite" href=# target=_blank><i class="fas fa-download"></i> Download</a><div id=modal-error></div></div></div></div></div></body></html>