name: Data Processing

# This workflow is triggered daily at midnight and can also be manually triggered.
# It processes data from various scripts and uploads the processed data as an artifact.
# The data is used to update the website's content.

on:
  schedule:
    - cron: '0 0 * * *'  # Daily at midnight
  workflow_dispatch:

jobs:
  process-data:
    name: Process Data
    runs-on: ubuntu-22.04
    permissions:
      contents: read
    env:
      PYTHON_VERSION: "3.11"
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: ${{ env.PYTHON_VERSION }}
          cache: 'pip'

      - name: Setup r2u
        uses: eddelbuettel/github-actions/r2u-setup@master
      - uses: r-lib/actions/setup-pandoc@v2
      - name: Install tenzing R dependencies
        run: Rscript -e 'install.packages(c("rmarkdown","ggplot2", "readxl", "dplyr", "googlesheets4", "stringr", "gridExtra", "glue", "tidygraph", "ggraph", "igraph", "visNetwork"))'

      - name: Run Contributor Analysis
        continue-on-error: true  # Continue even if this step fails
        run: |
          echo "üöÄ Rendering R Markdown..."
          Rscript -e "rmarkdown::render('content/contributor-analysis/index.Rmd')"
          echo "‚úÖ Done"

      - name: Install Python dependencies
        run: python3 -m pip install -r ./requirements.txt

      - name: Run Tenzing script
        continue-on-error: true  # Continue even if this step fails
        run: python3 scripts/forrt_contribs/tenzing.py

      - name: Run Curated Resources script
        continue-on-error: true  # Continue even if this step fails
        run: python3 content/resources/resource.py

      - name: Move and validate Tenzing output
        continue-on-error: true  # Continue even if this step fails
        run: |
          mv scripts/forrt_contribs/tenzing.md content/contributors/tenzing.md
          if [ ! -f content/contributors/tenzing.md ]; then
            echo "tenzing.md not found"
            exit 1
          fi

      - name: Validate curated resources
        continue-on-error: true  # Continue even if this step fails
        run: |
          for file in content/curated_resources/*; do
            if [ ! -f "$file" ]; then
              echo "Non-markdown file found: $file"
              exit 1
            fi
          done

      - name: Download GA Data
        continue-on-error: true  # Continue even if this step fails
        env:
          GA_API_CREDENTIALS: ${{ secrets.GA_API_CREDENTIALS }}
          GA_PROPERTY_ID: ${{ secrets.GA_PROPERTY_ID }}
        run: |
          echo "Starting GA data download..."
          if [ -z "$GA_API_CREDENTIALS" ]; then
            echo "‚ùå GA_API_CREDENTIALS is not set"
            exit 1
          fi
          if [ -z "$GA_PROPERTY_ID" ]; then
            echo "‚ùå GA_PROPERTY_ID is not set"
            exit 1
          fi
          echo "‚úÖ Credentials are set, running script..."
          
          # Delete old GA data file before creating new one
          echo "=== Cleaning up old GA data file ==="
          if [ -f "data/ga_data.json" ]; then
            echo "Found old file: data/ga_data.json"
            echo "Old file size: $(wc -c < data/ga_data.json) bytes"
            echo "Old file modified: $(stat -c %y data/ga_data.json)"
            echo "Removing old file..."
            rm -f data/ga_data.json
            echo "‚úÖ Old file removed"
          else
            echo "No old file found at data/ga_data.json"
          fi
          
          # Also clean up any nested files
          if [ -d "data/ga_data" ]; then
            echo "Found old nested directory: data/ga_data/"
            echo "Removing nested directory..."
            rm -rf data/ga_data/
            echo "‚úÖ Nested directory removed"
          fi
          
          echo "=== Running GA data download script ==="
          python scripts/download_ga_data.py
          echo "‚úÖ GA data download completed"
          
          # Verify the file was created
          if [ -f "data/ga_data.json" ]; then
            echo "‚úÖ GA data file created successfully"
            echo "File size: $(wc -c < data/ga_data.json) bytes"
          else
            echo "‚ùå GA data file was not created"
            exit 1
          fi

      # - name: Commit and push GA data to master
      #   if: github.event_name != 'pull_request'
      #   run: |
      #     echo "=== Committing GA data to master branch ==="
          
      #     # Configure git
      #     git config --local user.email "action@github.com"
      #     git config --local user.name "GitHub Action"
          
      #     # Checkout ga branch
      #     git checkout ga
          
      #     # Add and commit the GA data file directly (no removal needed)
      #     echo "Adding GA data file..."
      #     git add data/ga_data.json
      #     git commit -m "Update GA data - $(date -u +'%Y-%m-%d %H:%M:%S UTC')"
          
      #     # Push to ga branch
      #     git push origin ga
      #     echo "‚úÖ GA data committed and pushed to ga branch"

      - name: Run Google Scholar script
        continue-on-error: true  # Continue even if this step fails
        run: python3 scripts/gs-cite/google_scholar.py
        env:
          SERPAPI: ${{ secrets.SERPAPI }}

      - name: Upload data artifact
        id: upload-artifact
        uses: actions/upload-artifact@v4
        with:
          name: data-artifact
          path: |
            content/contributors/tenzing.md
            content/curated_resources/
            data/
            content/contributor-analysis/
            content/publications/citation_chart.webp
          retention-days: 1
