name: Data Processing

# This workflow is triggered daily at midnight and can also be manually triggered.
# It processes data from various scripts and uploads the processed data as an artifact.
# The data is used to update the website's content.

on:
  schedule:
    - cron: '0 0 * * *'  # Daily at midnight
  workflow_dispatch:

jobs:
  process-data:
    name: Process Data
    runs-on: ubuntu-22.04
    permissions:
      contents: write
    env:
      PYTHON_VERSION: "3.11"
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: ${{ env.PYTHON_VERSION }}
          cache: 'pip'

      - name: Setup r2u
        uses: eddelbuettel/github-actions/r2u-setup@master
      - uses: r-lib/actions/setup-pandoc@v2
      - name: Install tenzing R dependencies
        run: Rscript -e 'install.packages(c("rmarkdown","ggplot2", "readxl", "dplyr", "googlesheets4", "stringr", "gridExtra", "glue", "tidygraph", "ggraph", "igraph", "visNetwork"))'

      # - name: Run Tenzing analysis
      #   continue-on-error: true  # Continue even if this step fails
      #   run: |
      #     Rscript -e "rmarkdown::render('scripts/contributor-analysis/contributor_analysis.rmd')"

      # - name: Move Tenzing analysis
      #   continue-on-error: true  # Continue even if this step fails
      #   run: |
      #     mv scripts/contributor-analysis/contributor_analysis.md content/contributor-analysis/index.md
      #     mv scripts/contributor-analysis/*.png content/contributor-analysis/
      #     rm -rf content/contributor-analysis/htmlwidgets_libs
      #     mv scripts/contributor-analysis/htmlwidgets_libs content/contributor-analysis/
      #     sed -i.bak -e '/^```{=html}$/d' -e '/^```$/d' content/contributor-analysis/index.md && rm content/contributor-analysis/index.md.bak

      - name: Install Python dependencies
        run: python3 -m pip install -r ./requirements.txt

      # - name: Run Tenzing script
      #   continue-on-error: true  # Continue even if this step fails
      #   run: python3 scripts/forrt_contribs/tenzing.py

      # - name: Run Curated Resources script
      #   continue-on-error: true  # Continue even if this step fails
      #   run: python3 content/resources/resource.py

      # - name: Move and validate Tenzing output
      #   continue-on-error: true  # Continue even if this step fails
      #   run: |
      #     mv scripts/forrt_contribs/tenzing.md content/contributors/tenzing.md
      #     if [ ! -f content/contributors/tenzing.md ]; then
      #       echo "tenzing.md not found"
      #       exit 1
      #     fi

      # - name: Validate curated resources
      #   continue-on-error: true  # Continue even if this step fails
      #   run: |
      #     for file in content/curated_resources/*; do
      #       if [ ! -f "$file" ]; then
      #         echo "Non-markdown file found: $file"
      #         exit 1
      #       fi
      #     done

      - name: Download GA Data
        continue-on-error: true  # Continue even if this step fails
        env:
          GA_API_CREDENTIALS: ${{ secrets.GA_API_CREDENTIALS }}
          GA_PROPERTY_ID: ${{ secrets.GA_PROPERTY_ID }}
        run: |
          echo "Starting GA data download..."
          if [ -z "$GA_API_CREDENTIALS" ]; then
            echo "❌ GA_API_CREDENTIALS is not set"
            exit 1
          fi
          if [ -z "$GA_PROPERTY_ID" ]; then
            echo "❌ GA_PROPERTY_ID is not set"
            exit 1
          fi
          echo "✅ Credentials are set, running script..."
          python scripts/download_ga_data.py
          echo "✅ GA data download completed"
          
          # Verify the file was created
          if [ -f "data/ga_data.json" ]; then
            echo "✅ GA data file created successfully"
            echo "File size: $(wc -c < data/ga_data.json) bytes"
          else
            echo "❌ GA data file was not created"
            exit 1
          fi

      - name: Run Google Scholar script
        continue-on-error: true  # Continue even if this step fails
        run: python3 scripts/gs-cite/google_scholar.py
        env:
          SERPAPI: ${{ secrets.SERPAPI }}

      - name: Commit and push GA data to ga branch
        if: github.event_name != 'pull_request'
        run: |
          echo "=== Committing GA data to master branch ==="
          
          # Configure git
          git config --local user.email "action@github.com"
          git config --local user.name "GitHub Action"
          
          # Checkout master branch
          git checkout ga
          
          # Remove old GA data file if it exists (force removal)
          if [ -f "data/ga_data.json" ]; then
            echo "Removing old GA data file..."
            git rm -f data/ga_data.json || rm -f data/ga_data.json
          fi
          
          # Copy the new GA data file from the working directory
          echo "Adding new GA data file..."
          cp data/ga_data.json data/ga_data.json.new
          mv data/ga_data.json.new data/ga_data.json
          
          # Add and commit the file
          git add data/ga_data.json
          git commit -m "Update GA data - $(date -u +'%Y-%m-%d %H:%M:%S UTC')"
          
          # Push to master
          git push origin ga
          echo "✅ GA data committed and pushed to master branch"

      - name: Verify GA data file exists
        run: |
          echo "=== Verifying GA data file ==="
          
          # Clean up any old nested files
          if [ -f "data/ga_data/ga_data.json" ]; then
            echo "Found old nested file, removing..."
            rm -rf data/ga_data/
          fi
          
          # Check for the correct file
          if [ -f "data/ga_data.json" ]; then
            echo "✅ GA data file exists in data/ga_data.json"
            echo "File size: $(wc -c < data/ga_data.json) bytes"
            echo "File modified: $(stat -c %y data/ga_data.json)"
            echo "File contents preview:"
            head -3 data/ga_data.json
          else
            echo "❌ GA data file missing from data/ga_data.json"
            echo "Available files in data/:"
            ls -la data/ || echo "data/ directory does not exist"
            exit 1
          fi

      - name: Debug before artifact upload
        run: |
          echo "=== Debug before artifact upload ==="
          echo "Current directory: $(pwd)"
          echo "Files to be uploaded:"
          echo "1. content/contributors/tenzing.md"
          ls -la content/contributors/tenzing.md 2>/dev/null || echo "❌ tenzing.md not found"
          echo "2. data/ directory:"
          ls -la data/ 2>/dev/null || echo "❌ data/ directory not found"
          echo "3. content/curated_resources/ directory:"
          ls -la content/curated_resources/ 2>/dev/null || echo "❌ curated_resources/ directory not found"
          echo "4. content/contributor-analysis/ directory:"
          ls -la content/contributor-analysis/ 2>/dev/null || echo "❌ contributor-analysis/ directory not found"
          echo "5. content/publications/citation_chart.webp"
          ls -la content/publications/citation_chart.webp 2>/dev/null || echo "❌ citation_chart.webp not found"

      - name: Upload data artifact
        id: upload-artifact
        uses: actions/upload-artifact@v4
        with:
          name: data-artifact
          path: |
            content/contributors/tenzing.md
            content/curated_resources/
            data/
            content/contributor-analysis/
            content/publications/citation_chart.webp
          retention-days: 1
